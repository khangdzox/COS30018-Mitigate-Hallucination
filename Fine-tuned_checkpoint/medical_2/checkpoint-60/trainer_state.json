{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.14962593516209477,
  "eval_steps": 500,
  "global_step": 60,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0024937655860349127,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.2226,
      "step": 1
    },
    {
      "epoch": 0.004987531172069825,
      "grad_norm": 3.2931673526763916,
      "learning_rate": 4e-05,
      "loss": 2.415,
      "step": 2
    },
    {
      "epoch": 0.007481296758104738,
      "grad_norm": 3.348045825958252,
      "learning_rate": 8e-05,
      "loss": 2.4549,
      "step": 3
    },
    {
      "epoch": 0.00997506234413965,
      "grad_norm": 3.341059446334839,
      "learning_rate": 0.00012,
      "loss": 2.4492,
      "step": 4
    },
    {
      "epoch": 0.012468827930174564,
      "grad_norm": 3.572349786758423,
      "learning_rate": 0.00016,
      "loss": 2.37,
      "step": 5
    },
    {
      "epoch": 0.014962593516209476,
      "grad_norm": 4.048288822174072,
      "learning_rate": 0.0002,
      "loss": 2.584,
      "step": 6
    },
    {
      "epoch": 0.017456359102244388,
      "grad_norm": 3.9006106853485107,
      "learning_rate": 0.00019636363636363636,
      "loss": 2.4178,
      "step": 7
    },
    {
      "epoch": 0.0199501246882793,
      "grad_norm": 3.685194492340088,
      "learning_rate": 0.00019272727272727274,
      "loss": 2.3593,
      "step": 8
    },
    {
      "epoch": 0.022443890274314215,
      "grad_norm": 4.407322883605957,
      "learning_rate": 0.0001890909090909091,
      "loss": 2.2502,
      "step": 9
    },
    {
      "epoch": 0.02493765586034913,
      "grad_norm": NaN,
      "learning_rate": 0.0001890909090909091,
      "loss": 2.1372,
      "step": 10
    },
    {
      "epoch": 0.02743142144638404,
      "grad_norm": 5.746457099914551,
      "learning_rate": 0.00018545454545454545,
      "loss": 2.2637,
      "step": 11
    },
    {
      "epoch": 0.029925187032418952,
      "grad_norm": NaN,
      "learning_rate": 0.00018545454545454545,
      "loss": 2.1285,
      "step": 12
    },
    {
      "epoch": 0.032418952618453865,
      "grad_norm": 6.581241130828857,
      "learning_rate": 0.00018181818181818183,
      "loss": 2.3517,
      "step": 13
    },
    {
      "epoch": 0.034912718204488775,
      "grad_norm": 10.81906509399414,
      "learning_rate": 0.0001781818181818182,
      "loss": 2.2592,
      "step": 14
    },
    {
      "epoch": 0.03740648379052369,
      "grad_norm": 9.294864654541016,
      "learning_rate": 0.00017454545454545454,
      "loss": 2.1451,
      "step": 15
    },
    {
      "epoch": 0.0399002493765586,
      "grad_norm": 10.160326957702637,
      "learning_rate": 0.0001709090909090909,
      "loss": 2.0207,
      "step": 16
    },
    {
      "epoch": 0.04239401496259352,
      "grad_norm": 9.862997055053711,
      "learning_rate": 0.00016727272727272728,
      "loss": 1.7727,
      "step": 17
    },
    {
      "epoch": 0.04488778054862843,
      "grad_norm": 11.778581619262695,
      "learning_rate": 0.00016363636363636366,
      "loss": 1.4437,
      "step": 18
    },
    {
      "epoch": 0.04738154613466334,
      "grad_norm": 8.759937286376953,
      "learning_rate": 0.00016,
      "loss": 1.4551,
      "step": 19
    },
    {
      "epoch": 0.04987531172069826,
      "grad_norm": 10.25264835357666,
      "learning_rate": 0.00015636363636363637,
      "loss": 1.7991,
      "step": 20
    },
    {
      "epoch": 0.05236907730673317,
      "grad_norm": 9.687920570373535,
      "learning_rate": 0.00015272727272727275,
      "loss": 1.5888,
      "step": 21
    },
    {
      "epoch": 0.05486284289276808,
      "grad_norm": 9.174581527709961,
      "learning_rate": 0.0001490909090909091,
      "loss": 1.248,
      "step": 22
    },
    {
      "epoch": 0.057356608478802994,
      "grad_norm": 7.364667892456055,
      "learning_rate": 0.00014545454545454546,
      "loss": 1.4964,
      "step": 23
    },
    {
      "epoch": 0.059850374064837904,
      "grad_norm": 7.0927042961120605,
      "learning_rate": 0.00014181818181818184,
      "loss": 1.3091,
      "step": 24
    },
    {
      "epoch": 0.06234413965087282,
      "grad_norm": 7.054991245269775,
      "learning_rate": 0.0001381818181818182,
      "loss": 1.1583,
      "step": 25
    },
    {
      "epoch": 0.06483790523690773,
      "grad_norm": 5.518060684204102,
      "learning_rate": 0.00013454545454545455,
      "loss": 0.97,
      "step": 26
    },
    {
      "epoch": 0.06733167082294264,
      "grad_norm": 6.1669511795043945,
      "learning_rate": 0.00013090909090909093,
      "loss": 0.9958,
      "step": 27
    },
    {
      "epoch": 0.06982543640897755,
      "grad_norm": 5.432255268096924,
      "learning_rate": 0.00012727272727272728,
      "loss": 0.7501,
      "step": 28
    },
    {
      "epoch": 0.07231920199501247,
      "grad_norm": 7.3836188316345215,
      "learning_rate": 0.00012363636363636364,
      "loss": 1.1808,
      "step": 29
    },
    {
      "epoch": 0.07481296758104738,
      "grad_norm": 6.018070220947266,
      "learning_rate": 0.00012,
      "loss": 0.8594,
      "step": 30
    },
    {
      "epoch": 0.0773067331670823,
      "grad_norm": 6.286722183227539,
      "learning_rate": 0.00011636363636363636,
      "loss": 0.8439,
      "step": 31
    },
    {
      "epoch": 0.0798004987531172,
      "grad_norm": 12.981318473815918,
      "learning_rate": 0.00011272727272727272,
      "loss": 1.3547,
      "step": 32
    },
    {
      "epoch": 0.08229426433915212,
      "grad_norm": 9.337541580200195,
      "learning_rate": 0.00010909090909090909,
      "loss": 1.4039,
      "step": 33
    },
    {
      "epoch": 0.08478802992518704,
      "grad_norm": 8.237483978271484,
      "learning_rate": 0.00010545454545454545,
      "loss": 1.5847,
      "step": 34
    },
    {
      "epoch": 0.08728179551122195,
      "grad_norm": 7.522390365600586,
      "learning_rate": 0.00010181818181818181,
      "loss": 1.3661,
      "step": 35
    },
    {
      "epoch": 0.08977556109725686,
      "grad_norm": 7.48240852355957,
      "learning_rate": 9.818181818181818e-05,
      "loss": 1.5181,
      "step": 36
    },
    {
      "epoch": 0.09226932668329177,
      "grad_norm": 8.385061264038086,
      "learning_rate": 9.454545454545455e-05,
      "loss": 1.325,
      "step": 37
    },
    {
      "epoch": 0.09476309226932668,
      "grad_norm": 6.641849994659424,
      "learning_rate": 9.090909090909092e-05,
      "loss": 1.3641,
      "step": 38
    },
    {
      "epoch": 0.09725685785536159,
      "grad_norm": 5.49578857421875,
      "learning_rate": 8.727272727272727e-05,
      "loss": 1.2377,
      "step": 39
    },
    {
      "epoch": 0.09975062344139651,
      "grad_norm": 8.38901424407959,
      "learning_rate": 8.363636363636364e-05,
      "loss": 1.276,
      "step": 40
    },
    {
      "epoch": 0.10224438902743142,
      "grad_norm": 6.207622528076172,
      "learning_rate": 8e-05,
      "loss": 1.2109,
      "step": 41
    },
    {
      "epoch": 0.10473815461346633,
      "grad_norm": 8.724374771118164,
      "learning_rate": 7.636363636363637e-05,
      "loss": 0.9411,
      "step": 42
    },
    {
      "epoch": 0.10723192019950124,
      "grad_norm": 6.710389137268066,
      "learning_rate": 7.272727272727273e-05,
      "loss": 1.198,
      "step": 43
    },
    {
      "epoch": 0.10972568578553615,
      "grad_norm": 8.932501792907715,
      "learning_rate": 6.90909090909091e-05,
      "loss": 0.9567,
      "step": 44
    },
    {
      "epoch": 0.11221945137157108,
      "grad_norm": 7.179162502288818,
      "learning_rate": 6.545454545454546e-05,
      "loss": 0.8511,
      "step": 45
    },
    {
      "epoch": 0.11471321695760599,
      "grad_norm": 5.972395420074463,
      "learning_rate": 6.181818181818182e-05,
      "loss": 1.1047,
      "step": 46
    },
    {
      "epoch": 0.1172069825436409,
      "grad_norm": 7.919640064239502,
      "learning_rate": 5.818181818181818e-05,
      "loss": 1.0319,
      "step": 47
    },
    {
      "epoch": 0.11970074812967581,
      "grad_norm": 7.371321201324463,
      "learning_rate": 5.4545454545454546e-05,
      "loss": 1.107,
      "step": 48
    },
    {
      "epoch": 0.12219451371571072,
      "grad_norm": 13.366811752319336,
      "learning_rate": 5.090909090909091e-05,
      "loss": 1.2798,
      "step": 49
    },
    {
      "epoch": 0.12468827930174564,
      "grad_norm": 36.2297248840332,
      "learning_rate": 4.7272727272727275e-05,
      "loss": 1.9962,
      "step": 50
    },
    {
      "epoch": 0.12718204488778054,
      "grad_norm": 5.591128826141357,
      "learning_rate": 4.3636363636363636e-05,
      "loss": 1.1875,
      "step": 51
    },
    {
      "epoch": 0.12967581047381546,
      "grad_norm": 6.944619655609131,
      "learning_rate": 4e-05,
      "loss": 1.2241,
      "step": 52
    },
    {
      "epoch": 0.13216957605985039,
      "grad_norm": 7.493984699249268,
      "learning_rate": 3.6363636363636364e-05,
      "loss": 1.3716,
      "step": 53
    },
    {
      "epoch": 0.13466334164588528,
      "grad_norm": 5.233179569244385,
      "learning_rate": 3.272727272727273e-05,
      "loss": 0.9636,
      "step": 54
    },
    {
      "epoch": 0.1371571072319202,
      "grad_norm": 4.853445053100586,
      "learning_rate": 2.909090909090909e-05,
      "loss": 1.1714,
      "step": 55
    },
    {
      "epoch": 0.1396508728179551,
      "grad_norm": 5.2549967765808105,
      "learning_rate": 2.5454545454545454e-05,
      "loss": 1.5147,
      "step": 56
    },
    {
      "epoch": 0.14214463840399003,
      "grad_norm": 6.309887409210205,
      "learning_rate": 2.1818181818181818e-05,
      "loss": 1.0882,
      "step": 57
    },
    {
      "epoch": 0.14463840399002495,
      "grad_norm": 5.99778938293457,
      "learning_rate": 1.8181818181818182e-05,
      "loss": 1.1286,
      "step": 58
    },
    {
      "epoch": 0.14713216957605985,
      "grad_norm": 6.160606384277344,
      "learning_rate": 1.4545454545454545e-05,
      "loss": 1.0118,
      "step": 59
    },
    {
      "epoch": 0.14962593516209477,
      "grad_norm": 4.481682300567627,
      "learning_rate": 1.0909090909090909e-05,
      "loss": 1.3062,
      "step": 60
    }
  ],
  "logging_steps": 1,
  "max_steps": 60,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2083725513326592.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
