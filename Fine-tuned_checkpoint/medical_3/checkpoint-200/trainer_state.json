{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.021878845890879257,
  "eval_steps": 20,
  "global_step": 200,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00010939422945439628,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.3339,
      "step": 1
    },
    {
      "epoch": 0.00021878845890879256,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.3259,
      "step": 2
    },
    {
      "epoch": 0.0003281826883631888,
      "grad_norm": 4.484958648681641,
      "learning_rate": 2e-05,
      "loss": 2.2579,
      "step": 3
    },
    {
      "epoch": 0.00043757691781758513,
      "grad_norm": NaN,
      "learning_rate": 2e-05,
      "loss": 1.6797,
      "step": 4
    },
    {
      "epoch": 0.0005469711472719814,
      "grad_norm": 4.766985893249512,
      "learning_rate": 4e-05,
      "loss": 2.1602,
      "step": 5
    },
    {
      "epoch": 0.0006563653767263776,
      "grad_norm": 4.781851291656494,
      "learning_rate": 6e-05,
      "loss": 1.9977,
      "step": 6
    },
    {
      "epoch": 0.000765759606180774,
      "grad_norm": NaN,
      "learning_rate": 6e-05,
      "loss": 2.202,
      "step": 7
    },
    {
      "epoch": 0.0008751538356351703,
      "grad_norm": 4.407110691070557,
      "learning_rate": 8e-05,
      "loss": 2.2842,
      "step": 8
    },
    {
      "epoch": 0.0009845480650895666,
      "grad_norm": NaN,
      "learning_rate": 8e-05,
      "loss": 2.268,
      "step": 9
    },
    {
      "epoch": 0.0010939422945439629,
      "grad_norm": 6.511317729949951,
      "learning_rate": 0.0001,
      "loss": 1.8839,
      "step": 10
    },
    {
      "epoch": 0.0012033365239983592,
      "grad_norm": NaN,
      "learning_rate": 0.0001,
      "loss": 2.0544,
      "step": 11
    },
    {
      "epoch": 0.0013127307534527553,
      "grad_norm": 22.712112426757812,
      "learning_rate": 9.948717948717949e-05,
      "loss": 2.4443,
      "step": 12
    },
    {
      "epoch": 0.0014221249829071516,
      "grad_norm": NaN,
      "learning_rate": 9.948717948717949e-05,
      "loss": 1.9898,
      "step": 13
    },
    {
      "epoch": 0.001531519212361548,
      "grad_norm": 74.88016510009766,
      "learning_rate": 9.897435897435898e-05,
      "loss": 2.1009,
      "step": 14
    },
    {
      "epoch": 0.0016409134418159442,
      "grad_norm": 45.14767074584961,
      "learning_rate": 9.846153846153848e-05,
      "loss": 2.0719,
      "step": 15
    },
    {
      "epoch": 0.0017503076712703405,
      "grad_norm": 7.815644264221191,
      "learning_rate": 9.794871794871795e-05,
      "loss": 2.0592,
      "step": 16
    },
    {
      "epoch": 0.0018597019007247368,
      "grad_norm": 21.244565963745117,
      "learning_rate": 9.743589743589744e-05,
      "loss": 1.945,
      "step": 17
    },
    {
      "epoch": 0.001969096130179133,
      "grad_norm": 9.883245468139648,
      "learning_rate": 9.692307692307692e-05,
      "loss": 1.6772,
      "step": 18
    },
    {
      "epoch": 0.0020784903596335292,
      "grad_norm": 6.345090866088867,
      "learning_rate": 9.641025641025641e-05,
      "loss": 2.0886,
      "step": 19
    },
    {
      "epoch": 0.0021878845890879258,
      "grad_norm": 5.566249847412109,
      "learning_rate": 9.589743589743591e-05,
      "loss": 1.7744,
      "step": 20
    },
    {
      "epoch": 0.0021878845890879258,
      "eval_loss": 1.971915364265442,
      "eval_runtime": 1072.3805,
      "eval_samples_per_second": 0.391,
      "eval_steps_per_second": 0.391,
      "step": 20
    },
    {
      "epoch": 0.002297278818542322,
      "grad_norm": 5.731481075286865,
      "learning_rate": 9.53846153846154e-05,
      "loss": 1.8491,
      "step": 21
    },
    {
      "epoch": 0.0024066730479967184,
      "grad_norm": 9.742348670959473,
      "learning_rate": 9.487179487179487e-05,
      "loss": 2.0106,
      "step": 22
    },
    {
      "epoch": 0.0025160672774511145,
      "grad_norm": 10.340167045593262,
      "learning_rate": 9.435897435897436e-05,
      "loss": 1.6825,
      "step": 23
    },
    {
      "epoch": 0.0026254615069055106,
      "grad_norm": 12.777979850769043,
      "learning_rate": 9.384615384615386e-05,
      "loss": 1.8419,
      "step": 24
    },
    {
      "epoch": 0.002734855736359907,
      "grad_norm": 40.64524841308594,
      "learning_rate": 9.333333333333334e-05,
      "loss": 1.4741,
      "step": 25
    },
    {
      "epoch": 0.002844249965814303,
      "grad_norm": 44.06110763549805,
      "learning_rate": 9.282051282051283e-05,
      "loss": 1.3513,
      "step": 26
    },
    {
      "epoch": 0.0029536441952686997,
      "grad_norm": 39.88823318481445,
      "learning_rate": 9.230769230769232e-05,
      "loss": 1.6031,
      "step": 27
    },
    {
      "epoch": 0.003063038424723096,
      "grad_norm": 10.8026704788208,
      "learning_rate": 9.179487179487179e-05,
      "loss": 1.4564,
      "step": 28
    },
    {
      "epoch": 0.0031724326541774923,
      "grad_norm": 14.014619827270508,
      "learning_rate": 9.128205128205129e-05,
      "loss": 1.449,
      "step": 29
    },
    {
      "epoch": 0.0032818268836318884,
      "grad_norm": 17.41488265991211,
      "learning_rate": 9.076923076923078e-05,
      "loss": 1.4332,
      "step": 30
    },
    {
      "epoch": 0.0033912211130862845,
      "grad_norm": 39.79304885864258,
      "learning_rate": 9.025641025641026e-05,
      "loss": 1.3786,
      "step": 31
    },
    {
      "epoch": 0.003500615342540681,
      "grad_norm": 9.940396308898926,
      "learning_rate": 8.974358974358975e-05,
      "loss": 1.2201,
      "step": 32
    },
    {
      "epoch": 0.003610009571995077,
      "grad_norm": 12.508027076721191,
      "learning_rate": 8.923076923076924e-05,
      "loss": 1.0721,
      "step": 33
    },
    {
      "epoch": 0.0037194038014494737,
      "grad_norm": 11.322312355041504,
      "learning_rate": 8.871794871794872e-05,
      "loss": 1.2694,
      "step": 34
    },
    {
      "epoch": 0.0038287980309038698,
      "grad_norm": 20.57086181640625,
      "learning_rate": 8.820512820512821e-05,
      "loss": 1.177,
      "step": 35
    },
    {
      "epoch": 0.003938192260358266,
      "grad_norm": 13.632572174072266,
      "learning_rate": 8.76923076923077e-05,
      "loss": 1.1393,
      "step": 36
    },
    {
      "epoch": 0.004047586489812662,
      "grad_norm": 24.75382423400879,
      "learning_rate": 8.717948717948718e-05,
      "loss": 1.0651,
      "step": 37
    },
    {
      "epoch": 0.0041569807192670585,
      "grad_norm": 11.879859924316406,
      "learning_rate": 8.666666666666667e-05,
      "loss": 1.111,
      "step": 38
    },
    {
      "epoch": 0.0042663749487214546,
      "grad_norm": 13.72707462310791,
      "learning_rate": 8.615384615384617e-05,
      "loss": 1.0284,
      "step": 39
    },
    {
      "epoch": 0.0043757691781758515,
      "grad_norm": 18.78465461730957,
      "learning_rate": 8.564102564102564e-05,
      "loss": 0.9349,
      "step": 40
    },
    {
      "epoch": 0.0043757691781758515,
      "eval_loss": 1.211974024772644,
      "eval_runtime": 1040.5135,
      "eval_samples_per_second": 0.403,
      "eval_steps_per_second": 0.403,
      "step": 40
    },
    {
      "epoch": 0.004485163407630248,
      "grad_norm": 17.923494338989258,
      "learning_rate": 8.512820512820513e-05,
      "loss": 0.7253,
      "step": 41
    },
    {
      "epoch": 0.004594557637084644,
      "grad_norm": 61.87374496459961,
      "learning_rate": 8.461538461538461e-05,
      "loss": 0.922,
      "step": 42
    },
    {
      "epoch": 0.00470395186653904,
      "grad_norm": 48.602813720703125,
      "learning_rate": 8.410256410256411e-05,
      "loss": 0.7427,
      "step": 43
    },
    {
      "epoch": 0.004813346095993437,
      "grad_norm": 55.67683029174805,
      "learning_rate": 8.35897435897436e-05,
      "loss": 0.8096,
      "step": 44
    },
    {
      "epoch": 0.004922740325447833,
      "grad_norm": 57.62361145019531,
      "learning_rate": 8.307692307692309e-05,
      "loss": 0.6219,
      "step": 45
    },
    {
      "epoch": 0.005032134554902229,
      "grad_norm": 51.996944427490234,
      "learning_rate": 8.256410256410256e-05,
      "loss": 0.893,
      "step": 46
    },
    {
      "epoch": 0.005141528784356625,
      "grad_norm": 43.99436950683594,
      "learning_rate": 8.205128205128205e-05,
      "loss": 0.6848,
      "step": 47
    },
    {
      "epoch": 0.005250923013811021,
      "grad_norm": 29.442716598510742,
      "learning_rate": 8.153846153846155e-05,
      "loss": 0.6646,
      "step": 48
    },
    {
      "epoch": 0.005360317243265418,
      "grad_norm": 47.01970291137695,
      "learning_rate": 8.102564102564103e-05,
      "loss": 0.5347,
      "step": 49
    },
    {
      "epoch": 0.005469711472719814,
      "grad_norm": 34.356658935546875,
      "learning_rate": 8.051282051282052e-05,
      "loss": 0.6303,
      "step": 50
    },
    {
      "epoch": 0.00557910570217421,
      "grad_norm": 14.098906517028809,
      "learning_rate": 8e-05,
      "loss": 1.5808,
      "step": 51
    },
    {
      "epoch": 0.005688499931628606,
      "grad_norm": 11.818120002746582,
      "learning_rate": 7.948717948717948e-05,
      "loss": 1.7167,
      "step": 52
    },
    {
      "epoch": 0.0057978941610830025,
      "grad_norm": 5.494206428527832,
      "learning_rate": 7.897435897435898e-05,
      "loss": 1.6244,
      "step": 53
    },
    {
      "epoch": 0.005907288390537399,
      "grad_norm": 6.470988750457764,
      "learning_rate": 7.846153846153847e-05,
      "loss": 1.5264,
      "step": 54
    },
    {
      "epoch": 0.0060166826199917955,
      "grad_norm": 16.045801162719727,
      "learning_rate": 7.794871794871795e-05,
      "loss": 1.5607,
      "step": 55
    },
    {
      "epoch": 0.006126076849446192,
      "grad_norm": 6.920595645904541,
      "learning_rate": 7.743589743589744e-05,
      "loss": 1.377,
      "step": 56
    },
    {
      "epoch": 0.006235471078900588,
      "grad_norm": 4.807570934295654,
      "learning_rate": 7.692307692307693e-05,
      "loss": 1.5108,
      "step": 57
    },
    {
      "epoch": 0.006344865308354985,
      "grad_norm": 13.891671180725098,
      "learning_rate": 7.641025641025641e-05,
      "loss": 1.4076,
      "step": 58
    },
    {
      "epoch": 0.006454259537809381,
      "grad_norm": 20.367530822753906,
      "learning_rate": 7.58974358974359e-05,
      "loss": 1.3211,
      "step": 59
    },
    {
      "epoch": 0.006563653767263777,
      "grad_norm": 6.634157180786133,
      "learning_rate": 7.538461538461539e-05,
      "loss": 1.4957,
      "step": 60
    },
    {
      "epoch": 0.006563653767263777,
      "eval_loss": 1.1332652568817139,
      "eval_runtime": 1197.9552,
      "eval_samples_per_second": 0.35,
      "eval_steps_per_second": 0.35,
      "step": 60
    },
    {
      "epoch": 0.006673047996718173,
      "grad_norm": 5.716446399688721,
      "learning_rate": 7.487179487179487e-05,
      "loss": 1.3239,
      "step": 61
    },
    {
      "epoch": 0.006782442226172569,
      "grad_norm": 10.176459312438965,
      "learning_rate": 7.435897435897436e-05,
      "loss": 1.4496,
      "step": 62
    },
    {
      "epoch": 0.006891836455626966,
      "grad_norm": 12.113444328308105,
      "learning_rate": 7.384615384615386e-05,
      "loss": 1.2329,
      "step": 63
    },
    {
      "epoch": 0.007001230685081362,
      "grad_norm": 7.665979862213135,
      "learning_rate": 7.333333333333333e-05,
      "loss": 1.1668,
      "step": 64
    },
    {
      "epoch": 0.007110624914535758,
      "grad_norm": 4.4889326095581055,
      "learning_rate": 7.282051282051282e-05,
      "loss": 1.2525,
      "step": 65
    },
    {
      "epoch": 0.007220019143990154,
      "grad_norm": 5.258130073547363,
      "learning_rate": 7.23076923076923e-05,
      "loss": 1.3472,
      "step": 66
    },
    {
      "epoch": 0.007329413373444551,
      "grad_norm": 6.186871528625488,
      "learning_rate": 7.17948717948718e-05,
      "loss": 1.1945,
      "step": 67
    },
    {
      "epoch": 0.007438807602898947,
      "grad_norm": 5.130664825439453,
      "learning_rate": 7.128205128205129e-05,
      "loss": 1.2352,
      "step": 68
    },
    {
      "epoch": 0.007548201832353343,
      "grad_norm": 5.792457580566406,
      "learning_rate": 7.076923076923078e-05,
      "loss": 1.2344,
      "step": 69
    },
    {
      "epoch": 0.0076575960618077395,
      "grad_norm": 9.226359367370605,
      "learning_rate": 7.025641025641025e-05,
      "loss": 1.2825,
      "step": 70
    },
    {
      "epoch": 0.007766990291262136,
      "grad_norm": 7.169330596923828,
      "learning_rate": 6.974358974358974e-05,
      "loss": 1.4968,
      "step": 71
    },
    {
      "epoch": 0.007876384520716533,
      "grad_norm": 6.753794193267822,
      "learning_rate": 6.923076923076924e-05,
      "loss": 1.1092,
      "step": 72
    },
    {
      "epoch": 0.007985778750170928,
      "grad_norm": 6.269320011138916,
      "learning_rate": 6.871794871794872e-05,
      "loss": 1.1899,
      "step": 73
    },
    {
      "epoch": 0.008095172979625325,
      "grad_norm": 6.423154830932617,
      "learning_rate": 6.820512820512821e-05,
      "loss": 1.1608,
      "step": 74
    },
    {
      "epoch": 0.008204567209079722,
      "grad_norm": 5.285358428955078,
      "learning_rate": 6.76923076923077e-05,
      "loss": 1.2983,
      "step": 75
    },
    {
      "epoch": 0.008313961438534117,
      "grad_norm": 6.726363658905029,
      "learning_rate": 6.717948717948718e-05,
      "loss": 1.0411,
      "step": 76
    },
    {
      "epoch": 0.008423355667988514,
      "grad_norm": 8.142300605773926,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.8049,
      "step": 77
    },
    {
      "epoch": 0.008532749897442909,
      "grad_norm": 6.694406032562256,
      "learning_rate": 6.615384615384616e-05,
      "loss": 0.976,
      "step": 78
    },
    {
      "epoch": 0.008642144126897306,
      "grad_norm": 6.640763282775879,
      "learning_rate": 6.564102564102564e-05,
      "loss": 1.0338,
      "step": 79
    },
    {
      "epoch": 0.008751538356351703,
      "grad_norm": 8.952377319335938,
      "learning_rate": 6.512820512820513e-05,
      "loss": 0.8463,
      "step": 80
    },
    {
      "epoch": 0.008751538356351703,
      "eval_loss": 1.1106057167053223,
      "eval_runtime": 986.8784,
      "eval_samples_per_second": 0.425,
      "eval_steps_per_second": 0.425,
      "step": 80
    },
    {
      "epoch": 0.008860932585806098,
      "grad_norm": 6.775707721710205,
      "learning_rate": 6.461538461538462e-05,
      "loss": 1.191,
      "step": 81
    },
    {
      "epoch": 0.008970326815260495,
      "grad_norm": 8.40322208404541,
      "learning_rate": 6.410256410256412e-05,
      "loss": 0.8757,
      "step": 82
    },
    {
      "epoch": 0.00907972104471489,
      "grad_norm": 6.987175941467285,
      "learning_rate": 6.358974358974359e-05,
      "loss": 1.069,
      "step": 83
    },
    {
      "epoch": 0.009189115274169287,
      "grad_norm": 7.514222145080566,
      "learning_rate": 6.307692307692308e-05,
      "loss": 0.9346,
      "step": 84
    },
    {
      "epoch": 0.009298509503623684,
      "grad_norm": 10.383159637451172,
      "learning_rate": 6.256410256410256e-05,
      "loss": 1.2346,
      "step": 85
    },
    {
      "epoch": 0.00940790373307808,
      "grad_norm": 7.0505242347717285,
      "learning_rate": 6.205128205128206e-05,
      "loss": 0.9226,
      "step": 86
    },
    {
      "epoch": 0.009517297962532477,
      "grad_norm": 7.67656946182251,
      "learning_rate": 6.153846153846155e-05,
      "loss": 0.8798,
      "step": 87
    },
    {
      "epoch": 0.009626692191986874,
      "grad_norm": 8.691095352172852,
      "learning_rate": 6.1025641025641035e-05,
      "loss": 0.9801,
      "step": 88
    },
    {
      "epoch": 0.009736086421441269,
      "grad_norm": 10.50149154663086,
      "learning_rate": 6.0512820512820515e-05,
      "loss": 0.8751,
      "step": 89
    },
    {
      "epoch": 0.009845480650895666,
      "grad_norm": 5.603986740112305,
      "learning_rate": 6e-05,
      "loss": 0.6351,
      "step": 90
    },
    {
      "epoch": 0.009954874880350061,
      "grad_norm": 8.604676246643066,
      "learning_rate": 5.948717948717949e-05,
      "loss": 0.6807,
      "step": 91
    },
    {
      "epoch": 0.010064269109804458,
      "grad_norm": 8.050175666809082,
      "learning_rate": 5.897435897435898e-05,
      "loss": 0.6743,
      "step": 92
    },
    {
      "epoch": 0.010173663339258855,
      "grad_norm": 7.4699482917785645,
      "learning_rate": 5.846153846153847e-05,
      "loss": 0.5835,
      "step": 93
    },
    {
      "epoch": 0.01028305756871325,
      "grad_norm": 7.724609375,
      "learning_rate": 5.7948717948717954e-05,
      "loss": 0.5362,
      "step": 94
    },
    {
      "epoch": 0.010392451798167647,
      "grad_norm": 7.871286869049072,
      "learning_rate": 5.7435897435897434e-05,
      "loss": 0.5475,
      "step": 95
    },
    {
      "epoch": 0.010501846027622042,
      "grad_norm": 8.501078605651855,
      "learning_rate": 5.692307692307692e-05,
      "loss": 0.6534,
      "step": 96
    },
    {
      "epoch": 0.01061124025707644,
      "grad_norm": 6.883724689483643,
      "learning_rate": 5.6410256410256414e-05,
      "loss": 0.5173,
      "step": 97
    },
    {
      "epoch": 0.010720634486530836,
      "grad_norm": 5.911402702331543,
      "learning_rate": 5.58974358974359e-05,
      "loss": 0.4013,
      "step": 98
    },
    {
      "epoch": 0.010830028715985231,
      "grad_norm": 6.238540172576904,
      "learning_rate": 5.538461538461539e-05,
      "loss": 0.4681,
      "step": 99
    },
    {
      "epoch": 0.010939422945439628,
      "grad_norm": 7.63104248046875,
      "learning_rate": 5.487179487179488e-05,
      "loss": 0.4644,
      "step": 100
    },
    {
      "epoch": 0.010939422945439628,
      "eval_loss": 1.0773200988769531,
      "eval_runtime": 986.7684,
      "eval_samples_per_second": 0.425,
      "eval_steps_per_second": 0.425,
      "step": 100
    },
    {
      "epoch": 0.011048817174894024,
      "grad_norm": 3.6275203227996826,
      "learning_rate": 5.435897435897436e-05,
      "loss": 1.6328,
      "step": 101
    },
    {
      "epoch": 0.01115821140434842,
      "grad_norm": 3.739474058151245,
      "learning_rate": 5.384615384615385e-05,
      "loss": 1.7633,
      "step": 102
    },
    {
      "epoch": 0.011267605633802818,
      "grad_norm": 5.1768083572387695,
      "learning_rate": 5.333333333333333e-05,
      "loss": 1.5887,
      "step": 103
    },
    {
      "epoch": 0.011376999863257213,
      "grad_norm": 4.208120346069336,
      "learning_rate": 5.2820512820512826e-05,
      "loss": 1.5314,
      "step": 104
    },
    {
      "epoch": 0.01148639409271161,
      "grad_norm": 3.6310036182403564,
      "learning_rate": 5.230769230769231e-05,
      "loss": 1.6239,
      "step": 105
    },
    {
      "epoch": 0.011595788322166005,
      "grad_norm": 4.483188629150391,
      "learning_rate": 5.17948717948718e-05,
      "loss": 1.4318,
      "step": 106
    },
    {
      "epoch": 0.011705182551620402,
      "grad_norm": 3.8945775032043457,
      "learning_rate": 5.128205128205128e-05,
      "loss": 1.4815,
      "step": 107
    },
    {
      "epoch": 0.011814576781074799,
      "grad_norm": 4.298191547393799,
      "learning_rate": 5.0769230769230766e-05,
      "loss": 1.0956,
      "step": 108
    },
    {
      "epoch": 0.011923971010529194,
      "grad_norm": 6.030786037445068,
      "learning_rate": 5.025641025641026e-05,
      "loss": 1.3685,
      "step": 109
    },
    {
      "epoch": 0.012033365239983591,
      "grad_norm": 5.169377326965332,
      "learning_rate": 4.9743589743589746e-05,
      "loss": 1.045,
      "step": 110
    },
    {
      "epoch": 0.012142759469437988,
      "grad_norm": 4.228280067443848,
      "learning_rate": 4.923076923076924e-05,
      "loss": 1.3943,
      "step": 111
    },
    {
      "epoch": 0.012252153698892383,
      "grad_norm": 4.2246413230896,
      "learning_rate": 4.871794871794872e-05,
      "loss": 1.3398,
      "step": 112
    },
    {
      "epoch": 0.01236154792834678,
      "grad_norm": 4.539516925811768,
      "learning_rate": 4.8205128205128205e-05,
      "loss": 1.0678,
      "step": 113
    },
    {
      "epoch": 0.012470942157801175,
      "grad_norm": 4.086266040802002,
      "learning_rate": 4.76923076923077e-05,
      "loss": 1.0541,
      "step": 114
    },
    {
      "epoch": 0.012580336387255572,
      "grad_norm": 5.213820457458496,
      "learning_rate": 4.717948717948718e-05,
      "loss": 1.1928,
      "step": 115
    },
    {
      "epoch": 0.01268973061670997,
      "grad_norm": 4.659237861633301,
      "learning_rate": 4.666666666666667e-05,
      "loss": 1.1639,
      "step": 116
    },
    {
      "epoch": 0.012799124846164365,
      "grad_norm": 4.345372200012207,
      "learning_rate": 4.615384615384616e-05,
      "loss": 1.1933,
      "step": 117
    },
    {
      "epoch": 0.012908519075618761,
      "grad_norm": 4.450069427490234,
      "learning_rate": 4.5641025641025645e-05,
      "loss": 1.4664,
      "step": 118
    },
    {
      "epoch": 0.013017913305073157,
      "grad_norm": 4.419053077697754,
      "learning_rate": 4.512820512820513e-05,
      "loss": 1.1883,
      "step": 119
    },
    {
      "epoch": 0.013127307534527554,
      "grad_norm": 4.220300197601318,
      "learning_rate": 4.461538461538462e-05,
      "loss": 1.0873,
      "step": 120
    },
    {
      "epoch": 0.013127307534527554,
      "eval_loss": 1.0745468139648438,
      "eval_runtime": 984.2229,
      "eval_samples_per_second": 0.426,
      "eval_steps_per_second": 0.426,
      "step": 120
    },
    {
      "epoch": 0.01323670176398195,
      "grad_norm": 6.05254602432251,
      "learning_rate": 4.4102564102564104e-05,
      "loss": 1.1484,
      "step": 121
    },
    {
      "epoch": 0.013346095993436346,
      "grad_norm": 5.207534313201904,
      "learning_rate": 4.358974358974359e-05,
      "loss": 1.1022,
      "step": 122
    },
    {
      "epoch": 0.013455490222890743,
      "grad_norm": 5.023882865905762,
      "learning_rate": 4.3076923076923084e-05,
      "loss": 1.1316,
      "step": 123
    },
    {
      "epoch": 0.013564884452345138,
      "grad_norm": 6.8112263679504395,
      "learning_rate": 4.2564102564102564e-05,
      "loss": 1.0583,
      "step": 124
    },
    {
      "epoch": 0.013674278681799535,
      "grad_norm": 5.036733627319336,
      "learning_rate": 4.205128205128206e-05,
      "loss": 0.9304,
      "step": 125
    },
    {
      "epoch": 0.013783672911253932,
      "grad_norm": 6.390748023986816,
      "learning_rate": 4.1538461538461544e-05,
      "loss": 1.1457,
      "step": 126
    },
    {
      "epoch": 0.013893067140708327,
      "grad_norm": 4.712496757507324,
      "learning_rate": 4.1025641025641023e-05,
      "loss": 1.0676,
      "step": 127
    },
    {
      "epoch": 0.014002461370162724,
      "grad_norm": 4.978355407714844,
      "learning_rate": 4.051282051282052e-05,
      "loss": 0.8452,
      "step": 128
    },
    {
      "epoch": 0.01411185559961712,
      "grad_norm": 4.939945220947266,
      "learning_rate": 4e-05,
      "loss": 1.0318,
      "step": 129
    },
    {
      "epoch": 0.014221249829071516,
      "grad_norm": 5.492319583892822,
      "learning_rate": 3.948717948717949e-05,
      "loss": 0.8365,
      "step": 130
    },
    {
      "epoch": 0.014330644058525913,
      "grad_norm": 4.541593074798584,
      "learning_rate": 3.8974358974358976e-05,
      "loss": 0.7761,
      "step": 131
    },
    {
      "epoch": 0.014440038287980309,
      "grad_norm": 6.086540222167969,
      "learning_rate": 3.846153846153846e-05,
      "loss": 0.9869,
      "step": 132
    },
    {
      "epoch": 0.014549432517434705,
      "grad_norm": 4.753077983856201,
      "learning_rate": 3.794871794871795e-05,
      "loss": 0.8607,
      "step": 133
    },
    {
      "epoch": 0.014658826746889102,
      "grad_norm": 6.113520622253418,
      "learning_rate": 3.7435897435897436e-05,
      "loss": 0.9749,
      "step": 134
    },
    {
      "epoch": 0.014768220976343498,
      "grad_norm": 10.06386661529541,
      "learning_rate": 3.692307692307693e-05,
      "loss": 0.8312,
      "step": 135
    },
    {
      "epoch": 0.014877615205797895,
      "grad_norm": 5.757160663604736,
      "learning_rate": 3.641025641025641e-05,
      "loss": 0.9066,
      "step": 136
    },
    {
      "epoch": 0.01498700943525229,
      "grad_norm": 5.959140777587891,
      "learning_rate": 3.58974358974359e-05,
      "loss": 0.7287,
      "step": 137
    },
    {
      "epoch": 0.015096403664706687,
      "grad_norm": 6.50175142288208,
      "learning_rate": 3.538461538461539e-05,
      "loss": 0.7821,
      "step": 138
    },
    {
      "epoch": 0.015205797894161084,
      "grad_norm": 5.402103424072266,
      "learning_rate": 3.487179487179487e-05,
      "loss": 0.7581,
      "step": 139
    },
    {
      "epoch": 0.015315192123615479,
      "grad_norm": 4.151427745819092,
      "learning_rate": 3.435897435897436e-05,
      "loss": 0.61,
      "step": 140
    },
    {
      "epoch": 0.015315192123615479,
      "eval_loss": 1.070582628250122,
      "eval_runtime": 985.006,
      "eval_samples_per_second": 0.425,
      "eval_steps_per_second": 0.425,
      "step": 140
    },
    {
      "epoch": 0.015424586353069876,
      "grad_norm": 5.631662845611572,
      "learning_rate": 3.384615384615385e-05,
      "loss": 0.5258,
      "step": 141
    },
    {
      "epoch": 0.015533980582524271,
      "grad_norm": 7.345868110656738,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.6723,
      "step": 142
    },
    {
      "epoch": 0.015643374811978666,
      "grad_norm": 7.129330635070801,
      "learning_rate": 3.282051282051282e-05,
      "loss": 0.4642,
      "step": 143
    },
    {
      "epoch": 0.015752769041433065,
      "grad_norm": 9.119987487792969,
      "learning_rate": 3.230769230769231e-05,
      "loss": 0.6441,
      "step": 144
    },
    {
      "epoch": 0.01586216327088746,
      "grad_norm": 6.687685489654541,
      "learning_rate": 3.1794871794871795e-05,
      "loss": 0.5445,
      "step": 145
    },
    {
      "epoch": 0.015971557500341856,
      "grad_norm": 5.7076849937438965,
      "learning_rate": 3.128205128205128e-05,
      "loss": 0.442,
      "step": 146
    },
    {
      "epoch": 0.016080951729796254,
      "grad_norm": 6.322701930999756,
      "learning_rate": 3.0769230769230774e-05,
      "loss": 0.4821,
      "step": 147
    },
    {
      "epoch": 0.01619034595925065,
      "grad_norm": 6.417416572570801,
      "learning_rate": 3.0256410256410257e-05,
      "loss": 0.5263,
      "step": 148
    },
    {
      "epoch": 0.016299740188705045,
      "grad_norm": 6.137234687805176,
      "learning_rate": 2.9743589743589744e-05,
      "loss": 0.4141,
      "step": 149
    },
    {
      "epoch": 0.016409134418159443,
      "grad_norm": 6.400210857391357,
      "learning_rate": 2.9230769230769234e-05,
      "loss": 0.5371,
      "step": 150
    },
    {
      "epoch": 0.01651852864761384,
      "grad_norm": 3.5638186931610107,
      "learning_rate": 2.8717948717948717e-05,
      "loss": 1.5332,
      "step": 151
    },
    {
      "epoch": 0.016627922877068234,
      "grad_norm": 3.3323793411254883,
      "learning_rate": 2.8205128205128207e-05,
      "loss": 1.5397,
      "step": 152
    },
    {
      "epoch": 0.016737317106522633,
      "grad_norm": 3.290487766265869,
      "learning_rate": 2.7692307692307694e-05,
      "loss": 1.4648,
      "step": 153
    },
    {
      "epoch": 0.016846711335977028,
      "grad_norm": 3.2670376300811768,
      "learning_rate": 2.717948717948718e-05,
      "loss": 1.3784,
      "step": 154
    },
    {
      "epoch": 0.016956105565431423,
      "grad_norm": 3.9720394611358643,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 1.4931,
      "step": 155
    },
    {
      "epoch": 0.017065499794885818,
      "grad_norm": 3.593386650085449,
      "learning_rate": 2.6153846153846157e-05,
      "loss": 1.6121,
      "step": 156
    },
    {
      "epoch": 0.017174894024340217,
      "grad_norm": 3.9180796146392822,
      "learning_rate": 2.564102564102564e-05,
      "loss": 1.4529,
      "step": 157
    },
    {
      "epoch": 0.017284288253794612,
      "grad_norm": 3.570199966430664,
      "learning_rate": 2.512820512820513e-05,
      "loss": 1.2853,
      "step": 158
    },
    {
      "epoch": 0.017393682483249007,
      "grad_norm": 3.473426580429077,
      "learning_rate": 2.461538461538462e-05,
      "loss": 1.1388,
      "step": 159
    },
    {
      "epoch": 0.017503076712703406,
      "grad_norm": 5.983969211578369,
      "learning_rate": 2.4102564102564103e-05,
      "loss": 1.3442,
      "step": 160
    },
    {
      "epoch": 0.017503076712703406,
      "eval_loss": 1.0637599229812622,
      "eval_runtime": 985.5284,
      "eval_samples_per_second": 0.425,
      "eval_steps_per_second": 0.425,
      "step": 160
    },
    {
      "epoch": 0.0176124709421578,
      "grad_norm": 4.162994861602783,
      "learning_rate": 2.358974358974359e-05,
      "loss": 1.0901,
      "step": 161
    },
    {
      "epoch": 0.017721865171612197,
      "grad_norm": 4.652478218078613,
      "learning_rate": 2.307692307692308e-05,
      "loss": 1.2905,
      "step": 162
    },
    {
      "epoch": 0.017831259401066595,
      "grad_norm": 4.568570613861084,
      "learning_rate": 2.2564102564102566e-05,
      "loss": 1.4552,
      "step": 163
    },
    {
      "epoch": 0.01794065363052099,
      "grad_norm": 4.473881244659424,
      "learning_rate": 2.2051282051282052e-05,
      "loss": 1.1999,
      "step": 164
    },
    {
      "epoch": 0.018050047859975386,
      "grad_norm": 5.650711536407471,
      "learning_rate": 2.1538461538461542e-05,
      "loss": 1.4661,
      "step": 165
    },
    {
      "epoch": 0.01815944208942978,
      "grad_norm": 5.624884128570557,
      "learning_rate": 2.102564102564103e-05,
      "loss": 1.2638,
      "step": 166
    },
    {
      "epoch": 0.01826883631888418,
      "grad_norm": 5.06972074508667,
      "learning_rate": 2.0512820512820512e-05,
      "loss": 1.1753,
      "step": 167
    },
    {
      "epoch": 0.018378230548338575,
      "grad_norm": 5.99013090133667,
      "learning_rate": 2e-05,
      "loss": 0.9661,
      "step": 168
    },
    {
      "epoch": 0.01848762477779297,
      "grad_norm": 5.574107646942139,
      "learning_rate": 1.9487179487179488e-05,
      "loss": 0.9779,
      "step": 169
    },
    {
      "epoch": 0.01859701900724737,
      "grad_norm": 5.504761695861816,
      "learning_rate": 1.8974358974358975e-05,
      "loss": 1.1244,
      "step": 170
    },
    {
      "epoch": 0.018706413236701764,
      "grad_norm": 6.332094192504883,
      "learning_rate": 1.8461538461538465e-05,
      "loss": 1.1273,
      "step": 171
    },
    {
      "epoch": 0.01881580746615616,
      "grad_norm": 5.083357810974121,
      "learning_rate": 1.794871794871795e-05,
      "loss": 1.1459,
      "step": 172
    },
    {
      "epoch": 0.018925201695610558,
      "grad_norm": 4.905461311340332,
      "learning_rate": 1.7435897435897434e-05,
      "loss": 0.9969,
      "step": 173
    },
    {
      "epoch": 0.019034595925064953,
      "grad_norm": 5.390684127807617,
      "learning_rate": 1.6923076923076924e-05,
      "loss": 0.87,
      "step": 174
    },
    {
      "epoch": 0.01914399015451935,
      "grad_norm": 4.717048645019531,
      "learning_rate": 1.641025641025641e-05,
      "loss": 1.0022,
      "step": 175
    },
    {
      "epoch": 0.019253384383973747,
      "grad_norm": 7.455872535705566,
      "learning_rate": 1.5897435897435897e-05,
      "loss": 1.0492,
      "step": 176
    },
    {
      "epoch": 0.019362778613428142,
      "grad_norm": 5.427576541900635,
      "learning_rate": 1.5384615384615387e-05,
      "loss": 0.76,
      "step": 177
    },
    {
      "epoch": 0.019472172842882537,
      "grad_norm": 5.696281433105469,
      "learning_rate": 1.4871794871794872e-05,
      "loss": 1.052,
      "step": 178
    },
    {
      "epoch": 0.019581567072336933,
      "grad_norm": 6.543529510498047,
      "learning_rate": 1.4358974358974359e-05,
      "loss": 0.9648,
      "step": 179
    },
    {
      "epoch": 0.01969096130179133,
      "grad_norm": 6.250631809234619,
      "learning_rate": 1.3846153846153847e-05,
      "loss": 0.8028,
      "step": 180
    },
    {
      "epoch": 0.01969096130179133,
      "eval_loss": 1.0645173788070679,
      "eval_runtime": 984.6078,
      "eval_samples_per_second": 0.426,
      "eval_steps_per_second": 0.426,
      "step": 180
    },
    {
      "epoch": 0.019800355531245727,
      "grad_norm": 8.584641456604004,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.9884,
      "step": 181
    },
    {
      "epoch": 0.019909749760700122,
      "grad_norm": 5.364982604980469,
      "learning_rate": 1.282051282051282e-05,
      "loss": 0.8658,
      "step": 182
    },
    {
      "epoch": 0.02001914399015452,
      "grad_norm": 5.168489933013916,
      "learning_rate": 1.230769230769231e-05,
      "loss": 1.0123,
      "step": 183
    },
    {
      "epoch": 0.020128538219608916,
      "grad_norm": 5.976719379425049,
      "learning_rate": 1.1794871794871795e-05,
      "loss": 0.8525,
      "step": 184
    },
    {
      "epoch": 0.02023793244906331,
      "grad_norm": 5.689727783203125,
      "learning_rate": 1.1282051282051283e-05,
      "loss": 0.7915,
      "step": 185
    },
    {
      "epoch": 0.02034732667851771,
      "grad_norm": 5.8017802238464355,
      "learning_rate": 1.0769230769230771e-05,
      "loss": 0.7471,
      "step": 186
    },
    {
      "epoch": 0.020456720907972105,
      "grad_norm": 7.842677116394043,
      "learning_rate": 1.0256410256410256e-05,
      "loss": 0.7504,
      "step": 187
    },
    {
      "epoch": 0.0205661151374265,
      "grad_norm": 6.577317714691162,
      "learning_rate": 9.743589743589744e-06,
      "loss": 0.7545,
      "step": 188
    },
    {
      "epoch": 0.020675509366880895,
      "grad_norm": 5.939279556274414,
      "learning_rate": 9.230769230769232e-06,
      "loss": 0.7952,
      "step": 189
    },
    {
      "epoch": 0.020784903596335294,
      "grad_norm": 7.310387134552002,
      "learning_rate": 8.717948717948717e-06,
      "loss": 0.7516,
      "step": 190
    },
    {
      "epoch": 0.02089429782578969,
      "grad_norm": 5.365866184234619,
      "learning_rate": 8.205128205128205e-06,
      "loss": 0.5161,
      "step": 191
    },
    {
      "epoch": 0.021003692055244085,
      "grad_norm": 5.392921447753906,
      "learning_rate": 7.692307692307694e-06,
      "loss": 0.5849,
      "step": 192
    },
    {
      "epoch": 0.021113086284698483,
      "grad_norm": 5.9878644943237305,
      "learning_rate": 7.179487179487179e-06,
      "loss": 0.5481,
      "step": 193
    },
    {
      "epoch": 0.02122248051415288,
      "grad_norm": 6.456686019897461,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.5367,
      "step": 194
    },
    {
      "epoch": 0.021331874743607274,
      "grad_norm": 5.142548561096191,
      "learning_rate": 6.153846153846155e-06,
      "loss": 0.5021,
      "step": 195
    },
    {
      "epoch": 0.021441268973061672,
      "grad_norm": 6.684848785400391,
      "learning_rate": 5.641025641025641e-06,
      "loss": 0.4625,
      "step": 196
    },
    {
      "epoch": 0.021550663202516068,
      "grad_norm": 7.31300163269043,
      "learning_rate": 5.128205128205128e-06,
      "loss": 0.5229,
      "step": 197
    },
    {
      "epoch": 0.021660057431970463,
      "grad_norm": 5.655838489532471,
      "learning_rate": 4.615384615384616e-06,
      "loss": 0.5757,
      "step": 198
    },
    {
      "epoch": 0.02176945166142486,
      "grad_norm": 4.487000942230225,
      "learning_rate": 4.102564102564103e-06,
      "loss": 0.4228,
      "step": 199
    },
    {
      "epoch": 0.021878845890879257,
      "grad_norm": 6.0791335105896,
      "learning_rate": 3.5897435897435896e-06,
      "loss": 0.4318,
      "step": 200
    },
    {
      "epoch": 0.021878845890879257,
      "eval_loss": 1.0660185813903809,
      "eval_runtime": 983.1971,
      "eval_samples_per_second": 0.426,
      "eval_steps_per_second": 0.426,
      "step": 200
    }
  ],
  "logging_steps": 1,
  "max_steps": 200,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 8439646727061504.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
