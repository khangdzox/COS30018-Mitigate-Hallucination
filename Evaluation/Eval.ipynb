{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4fCxKQJ_No-"
      },
      "source": [
        "### **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "4Jf1x_85vHE3"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from textwrap import dedent\n",
        "from typing import Dict, List\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.colors as colors\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from colored import Back, Fore, Style\n",
        "from datasets import Dataset, load_dataset\n",
        "from matplotlib.ticker import PercentFormatter\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftModel,\n",
        "    TaskType,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    PeftConfig\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from trl import DataCollatorForCompletionOnlyLM, SFTConfig, SFTTrainer\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk import word_tokenize\n",
        "from rouge import rouge\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "COLORS = [\"#bae1ff\", \"#ffb3ba\", \"#ffdfba\", \"#ffffba\", \"#baffc9\"]\n",
        "\n",
        "sns.set( style = \"whitegrid\", palette = \"muted\", font_scale = 1.2 )\n",
        "sns.set_palette(sns.color_palette(COLORS))\n",
        "\n",
        "cmap = colors.LinearSegmentedColormap.from_list(\"custom_cmap\", COLORS[:2])\n",
        "\n",
        "MY_STYLE = {\n",
        "    \"figure.facecolor\": \"black\",\n",
        "    \"axes.facecolor\": \"black\",\n",
        "    \"axes.edgecolor\": \"white\",\n",
        "    \"axes.labelcolor\": \"white\",\n",
        "    \"text.color\": \"white\",\n",
        "    \"axes.linewidth\": 0.5,\n",
        "    \"xtick.color\": \"white\",\n",
        "    \"ytick.color\": \"white\",\n",
        "    \"grid.color\": \"gray\",\n",
        "    \"grid.linestyle\": \"--\",\n",
        "    \"grid.linewidth\":  0.5,\n",
        "    \"axes.grid\": True,\n",
        "    \"xtick.labelsize\": \"medium\",\n",
        "    \"ytick.labelsize\": \"medium\",\n",
        "    \"axes.titlesize\": \"large\",\n",
        "    \"axes.labelsize\": \"large\",\n",
        "    \"lines.color\": COLORS[0],\n",
        "    \"patch.edgecolor\": \"white\",\n",
        "}\n",
        "\n",
        "mpl.rcParams.update( MY_STYLE )\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7d0EVN4_J-e"
      },
      "source": [
        "### **Trained Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7AWr6ts_NcGY",
        "outputId": "f712e17b-a083-4ff5-b944-011180001b37"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'COS30018-Mitigate-Hallucination' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/khangdzox/COS30018-Mitigate-Hallucination.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "--ZwzKMiUit4"
      },
      "outputs": [],
      "source": [
        "seed_everything(SEED)\n",
        "PAD_TOKEN = \"<|pad|>\"\n",
        "TMODEL_NAME = \"COS30018-Mitigate-Hallucination/Finetuning/QLoRA/6\"\n",
        "NEW_MODEL = \"Llama-3-8B-Project\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "reg8eOiOwKMQ"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "424e5c8d1e304fd79a9b24826e720b71",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 11.99 GiB of which 0 bytes is free. Of the allocated memory 42.08 GiB is allocated by PyTorch, and 63.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[67], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m get_device_map()\n\u001b[0;32m      7\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\pink\\anaconda3\\envs\\LLama3-hallucinate\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\pink\\anaconda3\\envs\\LLama3-hallucinate\\Lib\\site-packages\\transformers\\modeling_utils.py:3941\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3932\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   3934\u001b[0m     (\n\u001b[0;32m   3935\u001b[0m         model,\n\u001b[0;32m   3936\u001b[0m         missing_keys,\n\u001b[0;32m   3937\u001b[0m         unexpected_keys,\n\u001b[0;32m   3938\u001b[0m         mismatched_keys,\n\u001b[0;32m   3939\u001b[0m         offload_index,\n\u001b[0;32m   3940\u001b[0m         error_msgs,\n\u001b[1;32m-> 3941\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[0;32m   3945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3948\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3949\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3952\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3953\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3960\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[0;32m   3961\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
            "File \u001b[1;32mc:\\Users\\pink\\anaconda3\\envs\\LLama3-hallucinate\\Lib\\site-packages\\transformers\\modeling_utils.py:4415\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[0;32m   4411\u001b[0m                 set_module_tensor_to_device(\n\u001b[0;32m   4412\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   4413\u001b[0m                 )\n\u001b[0;32m   4414\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4415\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4416\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4417\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4418\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4419\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4420\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4421\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4422\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4423\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4424\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4425\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4426\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4427\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4428\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4429\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4430\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4431\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4432\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[0;32m   4433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4434\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\pink\\anaconda3\\envs\\LLama3-hallucinate\\Lib\\site-packages\\transformers\\modeling_utils.py:936\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[1;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[0;32m    925\u001b[0m     state_dict_index \u001b[38;5;241m=\u001b[39m offload_weight(param, param_name, state_dict_folder, state_dict_index)\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m is_quantized\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m hf_quantizer\u001b[38;5;241m.\u001b[39mrequires_parameters_quantization)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m ):\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[1;32m--> 936\u001b[0m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    938\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
            "File \u001b[1;32mc:\\Users\\pink\\anaconda3\\envs\\LLama3-hallucinate\\Lib\\site-packages\\accelerate\\utils\\modeling.py:416\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[1;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[0;32m    414\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 416\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    418\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
            "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 11.99 GiB of which 0 bytes is free. Of the allocated memory 42.08 GiB is allocated by PyTorch, and 63.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "# Load device\n",
        "def get_device_map() -> str:\n",
        "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "device = get_device_map()\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_id, device_map='auto')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-wCWJiMa_v_"
      },
      "outputs": [],
      "source": [
        "model = PeftModel.from_pretrained(base_model, TMODEL_NAME, device_map = \"cuda\", torch_dtype = torch.bfloat16)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuMmZuBEUqFC"
      },
      "source": [
        "###Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RJs1bSNNC9V"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvThuZ8RNJdp"
      },
      "outputs": [],
      "source": [
        "# import zipfile\n",
        "# import os\n",
        "\n",
        "# # Replace 'your_zip_file_path' with the path to your zip file in Google Drive\n",
        "# zip_path = '/content/drive/MyDrive/5.zip'\n",
        "# extract_path = '/content/extracted_files'\n",
        "\n",
        "# # Create the directory if it doesn't exist\n",
        "# os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# # Extract the zip file\n",
        "# zip_ref = zipfile.ZipFile(zip_path, 'r')\n",
        "# zip_ref.extractall(extract_path)\n",
        "# zip_ref.close()\n",
        "\n",
        "# # Verify the files are extracted\n",
        "# print(os.listdir(extract_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mS301quuNK47"
      },
      "outputs": [],
      "source": [
        "# seed_everything(SEED)\n",
        "# PAD_TOKEN = \"<|pad|>\"\n",
        "# TMODEL_PATH = \"/content/extracted_files/5\"  # Update this with your model path\n",
        "# NEW_MODEL = \"Llama-3-8B-Project\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaSyf_fONM0V"
      },
      "outputs": [],
      "source": [
        "# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "# base_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
        "\n",
        "# model = PeftModel.from_pretrained(base_model, TMODEL_PATH, device_map=\"cuda\", torch_dtype=torch.bfloat16)\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "# tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
        "# tokenizer.padding_side = \"right\"\n",
        "\n",
        "# model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE_Z-ODr-_Ra"
      },
      "source": [
        "### **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VpSSDzWzsgS"
      },
      "outputs": [],
      "source": [
        "dataset=load_dataset(\"PatronusAI/HaluBench\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wFIqlc00wgw"
      },
      "outputs": [],
      "source": [
        "dataset[\"test\"][:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0OWbde4qmmT"
      },
      "outputs": [],
      "source": [
        "rows = []\n",
        "for i in dataset[\"test\"]:\n",
        "    if isinstance(i[\"answer\"], list):\n",
        "        correct_answers = \"; \".join(i[\"answer\"])\n",
        "    else:\n",
        "        correct_answers = str(i[\"answer\"])\n",
        "\n",
        "    rows.append(\n",
        "        {\n",
        "            \"question\": i[\"question\"],\n",
        "            \"context\": i['passage'],\n",
        "            \"correct_answers\": correct_answers,\n",
        "            \"label\": i[\"label\"]\n",
        "        }\n",
        "    )\n",
        "\n",
        "df = pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p54iEZs11Ho"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5c0MU_c4rNa"
      },
      "outputs": [],
      "source": [
        "print(df.isnull().value_counts())\n",
        "fail_count = df['label'].value_counts().get('FAIL', 0)\n",
        "\n",
        "print(f\"Number of 'FAIL' occurrences: {fail_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmyOn2kj41uL"
      },
      "outputs": [],
      "source": [
        "def format_example(row: dict):\n",
        "    prompt = dedent(\n",
        "        f\"\"\"\n",
        "        ### Instruction:\n",
        "        {row[\"context\"]}\n",
        "\n",
        "        ### Input:\n",
        "        {row[\"question\"]}\n",
        "\n",
        "        ### Response:\n",
        "        {row[\"correct_answers\"]}\n",
        "\n",
        "        ### Evaluation:\n",
        "        \"\"\"\n",
        "    )\n",
        "    messages = (\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Read the passage and evaluate if the provided answer is correct. Respond with 'PASS' if the answer is correct and 'FAIL' if the answer is incorrect.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        },\n",
        "    )\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-7jrZDC5fvl"
      },
      "outputs": [],
      "source": [
        "df[\"text\"] = df.apply(format_example, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDIfeBjhq1Hn"
      },
      "outputs": [],
      "source": [
        "def count_tokens(row: dict)->int:\n",
        "    return len(\n",
        "        tokenizer(\n",
        "            row[\"text\"],\n",
        "            add_special_tokens=True,\n",
        "            return_attention_mask=False,\n",
        "            )[\"input_ids\"]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JV5ik7Kg5-3L"
      },
      "outputs": [],
      "source": [
        "df[\"token_count\"] = df.apply(count_tokens, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lfd6heO36C72"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xz19AsG6e04"
      },
      "outputs": [],
      "source": [
        "print(df.text.iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRTVjG7M6sTa"
      },
      "outputs": [],
      "source": [
        "plt.hist(df.token_count, weights=np.ones(len(df.token_count)) / len(df.token_count))\n",
        "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
        "plt.xlabel(\"Token count\")\n",
        "plt.ylabel(\"Percentage\")\n",
        "plt.title(\"Token count distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6NPheGTYLNY"
      },
      "outputs": [],
      "source": [
        "upper_bound = 1000\n",
        "lower_bound = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0csLP977L1u"
      },
      "outputs": [],
      "source": [
        "len(df[(df.token_count < upper_bound) & (df.token_count > lower_bound)]), len(df), len(df[(df.token_count < upper_bound)  & (df.token_count > lower_bound)]) / len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqbdCkXZaQha"
      },
      "outputs": [],
      "source": [
        "total_num = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmQC-n8f8PIM"
      },
      "outputs": [],
      "source": [
        "df = df[(df.token_count < upper_bound) & (df.token_count > lower_bound)]\n",
        "df = df.sample(total_num)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs7qHNL89sqx"
      },
      "outputs": [],
      "source": [
        "# train, temp = train_test_split(df, test_size=0.2, random_state=SEED)\n",
        "# val, test = train_test_split(temp, test_size=0.2, random_state=SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCwxBHiB94MU"
      },
      "outputs": [],
      "source": [
        "# len(train) / len(df), len(val) / len(df), len(test) / len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpaBB8f4-BM7"
      },
      "outputs": [],
      "source": [
        "# len(train), len(val), len(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVrW4qH_aTuZ"
      },
      "outputs": [],
      "source": [
        "# train_num = 1500\n",
        "# val_num = 450\n",
        "# test_num = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LC59jZaS-EF5"
      },
      "outputs": [],
      "source": [
        "# train.sample(n=train_num).to_json(\"train.json\", orient=\"records\", lines=True)\n",
        "# val.sample(n=val_num).to_json(\"val.json\", orient=\"records\", lines=True)\n",
        "# test.sample(n=test_num).to_json(\"test.json\", orient=\"records\", lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9C2vU9fh-WnF"
      },
      "outputs": [],
      "source": [
        "# dataset = load_dataset(\n",
        "#     \"json\",\n",
        "#     data_files={\n",
        "#         \"train\": \"train.json\",\n",
        "#         \"validation\": \"val.json\",\n",
        "#         \"test\": \"test.json\"\n",
        "#         }\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHYo2XOn8oIY"
      },
      "outputs": [],
      "source": [
        "test = df.sample(n=total_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a1TAvFE8apo"
      },
      "outputs": [],
      "source": [
        "test.to_json(\"test.json\", orient=\"records\", lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfZV4F7f5X-4"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files={\n",
        "        \"test\": \"test.json\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVSuNXSN-iQV"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3C3-ON-a-pXE"
      },
      "outputs": [],
      "source": [
        "print(dataset[\"test\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-nJ1IRS-0W-"
      },
      "source": [
        "### **Test Base Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCv04xk7-tnG"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens = 128,\n",
        "    return_full_text = False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "go0HPeRx_opz"
      },
      "outputs": [],
      "source": [
        "def create_test_prompt(data_row):\n",
        "    prompt = dedent(\n",
        "        f\"\"\"\n",
        "        ### Instruction:\n",
        "        {data_row[\"context\"]}\n",
        "\n",
        "        ### Input:\n",
        "        {data_row[\"question\"]}\n",
        "\n",
        "        ### Provided answer:\n",
        "        {data_row[\"correct_answers\"]}\n",
        "\n",
        "        ### Response:\n",
        "        \"\"\"\n",
        "    )\n",
        "    messages = (\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Read the context and evaluate if the provided answer is correct. Respond with 'PASS' if the answer is correct and 'FAIL' if the answer is incorrect.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        },\n",
        "    )\n",
        "    return tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVxYseaxA_8T"
      },
      "outputs": [],
      "source": [
        "row = dataset[\"test\"][0]\n",
        "prompt = create_test_prompt(row)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nM-sfzWBFMZ"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "output = pipe(prompt)\n",
        "response = f\"\"\"\n",
        "answer: {row[\"label\"]}\n",
        "prediction: {output[0][\"generated_text\"]}\n",
        "\"\"\"\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZF_bg1sBnUf"
      },
      "outputs": [],
      "source": [
        "row = dataset[\"test\"][1]\n",
        "prompt = create_test_prompt(row)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RLDCQbrBuMB"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "output = pipe(prompt)\n",
        "response = f\"\"\"\n",
        "answer: {row[\"label\"]}\n",
        "prediction: {output[0][\"generated_text\"]}\n",
        "\"\"\"\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hIT7QhCCLWH"
      },
      "outputs": [],
      "source": [
        "row = dataset[\"test\"][2]\n",
        "prompt = create_test_prompt(row)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMu-_EULCM9p"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "output = pipe(prompt)\n",
        "response = f\"\"\"\n",
        "answer: {row[\"label\"]}\n",
        "prediction: {output[0][\"generated_text\"]}\n",
        "\"\"\"\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7LcTFraVKbF"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(prediction, reference):\n",
        "    pred_words = set(word_tokenize(prediction))\n",
        "    ref_words = set(word_tokenize(reference))\n",
        "\n",
        "    common_words = pred_words.intersection(ref_words)\n",
        "\n",
        "    accuracy_pred = len(common_words) / len(pred_words) if pred_words else 0\n",
        "    accuracy_ref = len(common_words) / len(ref_words) if ref_words else 0\n",
        "\n",
        "    return accuracy_pred, accuracy_ref"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kles8gQx_8-u"
      },
      "outputs": [],
      "source": [
        "from rouge import Rouge\n",
        "import nltk\n",
        "rows = []\n",
        "rouge = Rouge()\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "for row in tqdm(dataset[\"test\"]):\n",
        "    prompt = create_test_prompt(row)\n",
        "    output = pipe(prompt)\n",
        "    prediction = output[0][\"generated_text\"]\n",
        "    reference = row[\"label\"]\n",
        "\n",
        "    meteor = meteor_score([word_tokenize(reference)], word_tokenize(prediction), alpha=0.9, beta=3, gamma=0.5)\n",
        "\n",
        "    bleu = sentence_bleu([word_tokenize(reference)], word_tokenize(prediction))\n",
        "\n",
        "    rouge_scores = rouge.get_scores(prediction, reference, avg=True)\n",
        "\n",
        "    accuracy_pred, accuracy_ref = calculate_accuracy(prediction, reference)\n",
        "\n",
        "    rows.append(\n",
        "        {\n",
        "            \"question\": row[\"question\"],\n",
        "            \"context\": row[\"context\"],\n",
        "            \"answer\": reference,\n",
        "            \"prediction\": prediction,\n",
        "            \"meteor_score\": meteor,\n",
        "            \"bleu_score\": bleu,\n",
        "            \"rouge_1\": rouge_scores['rouge-1']['f'],\n",
        "            #\"rouge_2\": rouge_scores['rouge-2']['f'],\n",
        "            \"rouge_l\": rouge_scores['rouge-l']['f'],\n",
        "            \"Accuracy_pred\": accuracy_pred,\n",
        "            \"Accuracy_ref\": accuracy_ref,\n",
        "            \"Token_count\": row[\"token_count\"]\n",
        "        }\n",
        "    )\n",
        "\n",
        "predictions_df = pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0Oo2i2ZZ_6l"
      },
      "outputs": [],
      "source": [
        "predictions_df.to_csv('base_eval.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
