
  0%|                                                                                                                                                                   | 0/500 [00:00<?, ?it/s]

  0%|▎                                                                                                                                                        | 1/500 [00:17<2:29:06, 17.93s/it]

  0%|▌                                                                                                                                                        | 2/500 [00:35<2:26:15, 17.62s/it]

  1%|▉                                                                                                                                                        | 3/500 [00:53<2:26:38, 17.70s/it]

  1%|█▏                                                                                                                                                       | 4/500 [01:10<2:25:00, 17.54s/it]


  1%|█▊                                                                                                                                                       | 6/500 [01:45<2:24:58, 17.61s/it]
{'loss': 2.3392, 'grad_norm': 5.586999893188477, 'learning_rate': 4e-05, 'epoch': 0.0}

  1%|██▏                                                                                                                                                      | 7/500 [02:03<2:24:06, 17.54s/it]

  2%|██▍                                                                                                                                                      | 8/500 [02:20<2:22:24, 17.37s/it]

  2%|██▊                                                                                                                                                      | 9/500 [02:37<2:22:04, 17.36s/it]

  2%|███                                                                                                                                                     | 10/500 [02:55<2:22:27, 17.44s/it]

  2%|███▎                                                                                                                                                    | 11/500 [03:12<2:22:20, 17.47s/it]


  3%|███▉                                                                                                                                                    | 13/500 [03:47<2:22:24, 17.55s/it]

  3%|████▎                                                                                                                                                   | 14/500 [04:05<2:23:23, 17.70s/it]
{'loss': 1.3234, 'grad_norm': 5.698432445526123, 'learning_rate': 0.00010400000000000001, 'epoch': 0.01}


  3%|████▊                                                                                                                                                   | 16/500 [04:39<2:19:49, 17.33s/it]
{'loss': 1.1639, 'grad_norm': 3.0398921966552734, 'learning_rate': 0.00012, 'epoch': 0.01}

  3%|█████▏                                                                                                                                                  | 17/500 [04:57<2:20:02, 17.40s/it]

  4%|█████▍                                                                                                                                                  | 18/500 [05:15<2:20:30, 17.49s/it]

  4%|█████▊                                                                                                                                                  | 19/500 [05:32<2:19:02, 17.34s/it]

  4%|██████                                                                                                                                                  | 20/500 [05:49<2:17:51, 17.23s/it]

  4%|██████▍                                                                                                                                                 | 21/500 [06:07<2:19:54, 17.53s/it]

  4%|██████▋                                                                                                                                                 | 22/500 [06:24<2:18:08, 17.34s/it]

  5%|██████▉                                                                                                                                                 | 23/500 [06:41<2:17:10, 17.26s/it]

  5%|███████▎                                                                                                                                                | 24/500 [06:59<2:19:09, 17.54s/it]

  5%|███████▌                                                                                                                                                | 25/500 [07:16<2:18:27, 17.49s/it]

  5%|███████▉                                                                                                                                                | 26/500 [07:34<2:17:50, 17.45s/it]


  6%|████████▌                                                                                                                                               | 28/500 [08:07<2:14:49, 17.14s/it]
{'loss': 1.0166, 'grad_norm': 1.8228421211242676, 'learning_rate': 0.00019986005377693825, 'epoch': 0.01}

  6%|████████▊                                                                                                                                               | 29/500 [08:24<2:13:47, 17.04s/it]


  6%|█████████▍                                                                                                                                              | 31/500 [09:02<2:20:52, 18.02s/it]
{'loss': 0.9762, 'grad_norm': 1.6704727411270142, 'learning_rate': 0.00019912640693269752, 'epoch': 0.01}

  6%|█████████▋                                                                                                                                              | 32/500 [09:28<2:40:59, 20.64s/it]

  7%|██████████                                                                                                                                              | 33/500 [09:59<3:04:42, 23.73s/it]


  7%|██████████▋                                                                                                                                             | 35/500 [10:58<3:25:50, 26.56s/it]
{'loss': 1.1143, 'grad_norm': 1.596184253692627, 'learning_rate': 0.00019717879379723012, 'epoch': 0.01}

  7%|██████████▉                                                                                                                                             | 36/500 [11:35<3:49:52, 29.73s/it]

  7%|███████████▏                                                                                                                                            | 37/500 [12:51<5:36:40, 43.63s/it]

  8%|███████████▍                                                                                                                                          | 38/500 [31:33<47:06:50, 367.12s/it]Traceback (most recent call last):
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 253, in <module>
    main()
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 249, in main
    trainer.train()
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\trl\trainer\sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 1948, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\utils\operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\utils\operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\amp\autocast_mode.py", line 43, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\peft\peft_model.py", line 1577, in forward
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\peft\tuners\tuners_utils.py", line 188, in forward
    return self.model.forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 1189, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 1001, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 734, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 617, in forward
    query_states = self.q_proj(hidden_states)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\peft\tuners\lora\bnb.py", line 489, in forward
    output = lora_B(lora_A(dropout(x))) * scaling
                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\linear.py", line 117, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt