
  0%|                                                                                                                                               | 0/110 [00:00<?, ?it/s]

  1%|█▏                                                                                                                                   | 1/110 [01:16<2:18:40, 76.33s/it]

  2%|██▍                                                                                                                                  | 2/110 [02:18<2:02:18, 67.95s/it]

  3%|███▋                                                                                                                                 | 3/110 [03:03<1:42:21, 57.39s/it]

  4%|████▊                                                                                                                                | 4/110 [03:46<1:31:47, 51.96s/it]

  5%|██████                                                                                                                               | 5/110 [04:30<1:25:43, 48.99s/it]

  5%|███████▎                                                                                                                             | 6/110 [05:17<1:23:38, 48.26s/it]

  6%|████████▍                                                                                                                            | 7/110 [06:02<1:20:53, 47.12s/it]Traceback (most recent call last):
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_LoRA.py", line 211, in <module>
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_LoRA.py", line 207, in main
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\trl\trainer\sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 1948, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3359, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\accelerator.py", line 2155, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\autograd\__init__.py", line 289, in backward
    _engine_run_backward(
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\autograd\graph.py", line 768, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt