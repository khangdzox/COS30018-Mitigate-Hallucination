

  0%|▎                                                                                                                                          | 1/500 [00:04<37:51,  4.55s/it]

  0%|▌                                                                                                                                          | 2/500 [00:08<36:10,  4.36s/it]

  1%|▊                                                                                                                                          | 3/500 [00:12<35:22,  4.27s/it]

  1%|█                                                                                                                                          | 4/500 [00:17<35:31,  4.30s/it]

  1%|█▍                                                                                                                                         | 5/500 [00:21<35:07,  4.26s/it]

  1%|█▋                                                                                                                                         | 6/500 [00:25<34:33,  4.20s/it]

  1%|█▉                                                                                                                                         | 7/500 [00:29<34:34,  4.21s/it]
{'loss': 2.0585, 'grad_norm': 1.2719693183898926, 'learning_rate': 4.93e-05, 'epoch': 0.0}


  2%|██▌                                                                                                                                        | 9/500 [00:38<34:26,  4.21s/it]

  2%|██▊                                                                                                                                       | 10/500 [00:42<34:24,  4.21s/it]

  2%|███                                                                                                                                       | 11/500 [00:48<38:26,  4.72s/it]

  2%|███▎                                                                                                                                      | 12/500 [00:52<37:14,  4.58s/it]

  3%|███▌                                                                                                                                      | 13/500 [00:56<36:01,  4.44s/it]

  3%|███▊                                                                                                                                      | 14/500 [01:00<35:25,  4.37s/it]

  3%|████▏                                                                                                                                     | 15/500 [01:05<34:50,  4.31s/it]

  3%|████▍                                                                                                                                     | 16/500 [01:09<34:31,  4.28s/it]

  3%|████▋                                                                                                                                     | 17/500 [01:13<34:16,  4.26s/it]

  4%|████▉                                                                                                                                     | 18/500 [01:17<33:48,  4.21s/it]

  4%|█████▏                                                                                                                                    | 19/500 [01:21<33:29,  4.18s/it]

  4%|█████▌                                                                                                                                    | 20/500 [01:25<33:28,  4.18s/it]

  4%|█████▊                                                                                                                                    | 21/500 [01:30<35:03,  4.39s/it]

  4%|██████                                                                                                                                    | 22/500 [01:34<34:35,  4.34s/it]

  5%|██████▎                                                                                                                                   | 23/500 [01:39<34:11,  4.30s/it]

  5%|██████▌                                                                                                                                   | 24/500 [01:43<33:56,  4.28s/it]

  5%|██████▉                                                                                                                                   | 25/500 [01:47<34:00,  4.30s/it]

  5%|███████▏                                                                                                                                  | 26/500 [01:51<33:35,  4.25s/it]
{'loss': 1.3879, 'grad_norm': 3.0204832553863525, 'learning_rate': 4.74e-05, 'epoch': 0.0}


  6%|███████▋                                                                                                                                  | 28/500 [02:00<32:55,  4.19s/it]

  6%|████████                                                                                                                                  | 29/500 [02:04<32:54,  4.19s/it]

  6%|████████▎                                                                                                                                 | 30/500 [02:08<32:39,  4.17s/it]

  6%|████████▌                                                                                                                                 | 31/500 [02:13<34:01,  4.35s/it]

  6%|████████▊                                                                                                                                 | 32/500 [02:17<33:25,  4.29s/it]

  7%|█████████                                                                                                                                 | 33/500 [02:21<33:18,  4.28s/it]

  7%|█████████▍                                                                                                                                | 34/500 [02:25<32:55,  4.24s/it]

  7%|█████████▋                                                                                                                                | 35/500 [02:29<32:49,  4.24s/it]
{'loss': 1.3347, 'grad_norm': 1.3253307342529297, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.0}

  7%|█████████▉                                                                                                                                | 36/500 [02:34<32:26,  4.19s/it]


  8%|██████████▍                                                                                                                               | 38/500 [02:42<32:06,  4.17s/it]

  8%|██████████▊                                                                                                                               | 39/500 [02:46<31:50,  4.15s/it]

  8%|███████████                                                                                                                               | 40/500 [02:50<31:43,  4.14s/it]

  8%|███████████▎                                                                                                                              | 41/500 [02:55<33:58,  4.44s/it]

  8%|███████████▌                                                                                                                              | 42/500 [02:59<33:09,  4.34s/it]

  9%|███████████▊                                                                                                                              | 43/500 [03:03<32:23,  4.25s/it]
{'loss': 1.028, 'grad_norm': 1.3253475427627563, 'learning_rate': 4.5700000000000006e-05, 'epoch': 0.0}

  9%|████████████▏                                                                                                                             | 44/500 [03:08<32:11,  4.24s/it]


  9%|████████████▋                                                                                                                             | 46/500 [03:16<31:56,  4.22s/it]

  9%|████████████▉                                                                                                                             | 47/500 [03:20<31:29,  4.17s/it]

 10%|█████████████▏                                                                                                                            | 48/500 [03:24<31:16,  4.15s/it]

 10%|█████████████▌                                                                                                                            | 49/500 [03:29<31:57,  4.25s/it]

 10%|█████████████▊                                                                                                                            | 50/500 [03:33<31:40,  4.22s/it]
{'loss': 0.9697, 'grad_norm': 0.9678144454956055, 'learning_rate': 4.5e-05, 'epoch': 0.0}


 10%|██████████████▎                                                                                                                           | 52/500 [03:42<32:25,  4.34s/it]

 11%|██████████████▋                                                                                                                           | 53/500 [03:46<32:28,  4.36s/it]

 11%|██████████████▉                                                                                                                           | 54/500 [03:50<32:03,  4.31s/it]

 11%|███████████████▏                                                                                                                          | 55/500 [03:55<32:29,  4.38s/it]

 11%|███████████████▍                                                                                                                          | 56/500 [03:59<31:37,  4.27s/it]

 11%|███████████████▋                                                                                                                          | 57/500 [04:03<31:35,  4.28s/it]

 12%|████████████████                                                                                                                          | 58/500 [04:08<31:31,  4.28s/it]
 12%|████████████████                                                                                                                          | 58/500 [04:08<31:31,  4.28s/it]Traceback (most recent call last):
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 241, in <module>
    main()
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 237, in main
    trainer.train()
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\trl\trainer\sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 1948, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\utils\operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\utils\operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\amp\autocast_mode.py", line 43, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\peft\peft_model.py", line 1577, in forward
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\peft\tuners\tuners_utils.py", line 188, in forward
    return self.model.forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 1189, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 1001, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 734, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 618, in forward
    key_states = self.k_proj(hidden_states)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\bitsandbytes\nn\modules.py", line 477, in forward
    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\bitsandbytes\autograd\_functions.py", line 579, in matmul_4bit
    return MatMul4Bit.apply(A, B, out, bias, quant_state)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\autograd\function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\bitsandbytes\autograd\_functions.py", line 509, in forward
    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\bitsandbytes\functional.py", line 1347, in dequantize_4bit
    absmax = dequantize_blockwise(quant_state.absmax, quant_state.state2)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\bitsandbytes\functional.py", line 980, in dequantize_blockwise
    get_ptr(absmax),
    ^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\bitsandbytes\functional.py", line 458, in get_ptr
    return ct.c_void_p(A.data.data_ptr())
                       ^^^^^^^^^^^^^^^^^
KeyboardInterrupt