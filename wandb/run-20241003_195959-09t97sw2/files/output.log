

  1%|█▏                                                                                                                                     | 1/110 [01:43<3:07:53, 103.43s/it]
{'loss': 2.0825, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 0.0}


  3%|███▋                                                                                                                                    | 3/110 [04:13<2:22:54, 80.14s/it]
{'loss': 2.1604, 'grad_norm': 3.485483407974243, 'learning_rate': 2e-05, 'epoch': 0.0}


  5%|██████▏                                                                                                                                 | 5/110 [06:03<1:53:06, 64.64s/it]
{'loss': 2.259, 'grad_norm': 3.693016290664673, 'learning_rate': 6e-05, 'epoch': 0.0}

  5%|███████▍                                                                                                                                | 6/110 [06:52<1:42:42, 59.26s/it]


  7%|█████████▉                                                                                                                              | 8/110 [08:35<1:33:41, 55.11s/it]

  8%|███████████▏                                                                                                                            | 9/110 [09:23<1:28:56, 52.83s/it]
{'loss': 1.9704, 'grad_norm': 4.331701278686523, 'learning_rate': 9.997762161417517e-05, 'epoch': 0.0}

  9%|████████████▎                                                                                                                          | 10/110 [10:11<1:25:15, 51.16s/it]


 11%|██████████████▋                                                                                                                        | 12/110 [11:34<1:14:36, 45.68s/it]
{'loss': 2.0498, 'grad_norm': 4.444845199584961, 'learning_rate': 9.964234631709187e-05, 'epoch': 0.0}


 13%|█████████████████▏                                                                                                                     | 14/110 [12:35<1:00:46, 37.99s/it]
{'loss': 2.1425, 'grad_norm': 4.86154317855835, 'learning_rate': 9.919647942993148e-05, 'epoch': 0.0}

 14%|██████████████████▋                                                                                                                      | 15/110 [13:06<56:44, 35.84s/it]

 15%|███████████████████▉                                                                                                                     | 16/110 [13:37<53:43, 34.29s/it]

 15%|█████████████████████▏                                                                                                                   | 17/110 [14:22<58:15, 37.59s/it]

 16%|██████████████████████▍                                                                                                                  | 18/110 [14:50<53:23, 34.82s/it]

 17%|███████████████████████▋                                                                                                                 | 19/110 [15:19<49:45, 32.80s/it]


 19%|██████████████████████████▏                                                                                                              | 21/110 [16:18<46:11, 31.14s/it]
{'loss': 2.2915, 'grad_norm': 8.723076820373535, 'learning_rate': 9.731636918995821e-05, 'epoch': 0.0}

 20%|███████████████████████████▍                                                                                                             | 22/110 [16:50<46:20, 31.60s/it]Traceback (most recent call last):
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_LoRA.py", line 218, in <module>
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_LoRA.py", line 214, in main
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\trl\trainer\sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 1948, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3359, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\accelerator.py", line 2155, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\autograd\__init__.py", line 289, in backward
    _engine_run_backward(
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\autograd\graph.py", line 768, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt