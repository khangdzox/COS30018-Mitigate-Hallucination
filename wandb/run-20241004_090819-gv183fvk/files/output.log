

  0%|▎                                                                                                                                       | 1/500 [00:15<2:05:38, 15.11s/it]

  0%|▌                                                                                                                                       | 2/500 [00:34<2:28:09, 17.85s/it]

  1%|▊                                                                                                                                       | 3/500 [00:53<2:31:20, 18.27s/it]

  1%|█                                                                                                                                       | 4/500 [01:13<2:37:30, 19.05s/it]

  1%|█▎                                                                                                                                      | 5/500 [01:33<2:37:26, 19.08s/it]

  1%|█▋                                                                                                                                      | 6/500 [01:52<2:39:27, 19.37s/it]

  1%|█▉                                                                                                                                      | 7/500 [02:13<2:41:28, 19.65s/it]

  2%|██▏                                                                                                                                     | 8/500 [02:35<2:46:56, 20.36s/it]
{'loss': 2.3599, 'grad_norm': 4.697566509246826, 'learning_rate': 0.0002, 'epoch': 0.0}

  2%|██▍                                                                                                                                     | 9/500 [02:56<2:49:47, 20.75s/it]


  2%|██▉                                                                                                                                    | 11/500 [03:50<3:11:58, 23.56s/it]
{'loss': 1.8719, 'grad_norm': nan, 'learning_rate': 0.00019999194411028594, 'epoch': 0.0}

  2%|███▏                                                                                                                                   | 12/500 [04:31<3:54:45, 28.86s/it]

  3%|███▌                                                                                                                                   | 13/500 [04:42<3:12:18, 23.69s/it]


  3%|████                                                                                                                                   | 15/500 [05:06<2:21:32, 17.51s/it]

  3%|████▎                                                                                                                                  | 16/500 [05:17<2:07:11, 15.77s/it]

  3%|████▌                                                                                                                                  | 17/500 [05:29<1:56:52, 14.52s/it]
{'loss': 1.8256, 'grad_norm': 62.501285552978516, 'learning_rate': 0.00019996777773909093, 'epoch': 0.0}


  4%|█████▏                                                                                                                                 | 19/500 [05:53<1:47:42, 13.44s/it]

  4%|█████▍                                                                                                                                 | 20/500 [06:05<1:43:40, 12.96s/it]
{'loss': 1.7174, 'grad_norm': 9.029497146606445, 'learning_rate': 0.0001999013302564544, 'epoch': 0.0}

  4%|█████▋                                                                                                                                 | 21/500 [06:19<1:44:10, 13.05s/it]

  4%|█████▉                                                                                                                                 | 22/500 [06:32<1:44:52, 13.16s/it]

  5%|██████▏                                                                                                                                | 23/500 [06:44<1:42:43, 12.92s/it]


  5%|██████▊                                                                                                                                | 25/500 [07:29<2:17:28, 17.36s/it]

  5%|███████                                                                                                                                | 26/500 [07:44<2:11:07, 16.60s/it]
{'loss': 1.1801, 'grad_norm': 8.047276496887207, 'learning_rate': 0.00019965982712740808, 'epoch': 0.0}

  5%|███████▎                                                                                                                               | 27/500 [08:58<4:27:30, 33.93s/it]

  6%|███████▌                                                                                                                               | 28/500 [09:13<3:41:13, 28.12s/it]


  6%|████████                                                                                                                               | 30/500 [10:12<3:33:15, 27.23s/it]
{'loss': 1.2988, 'grad_norm': 7.95374059677124, 'learning_rate': 0.00019941851856316548, 'epoch': 0.0}


  6%|████████▋                                                                                                                              | 32/500 [10:47<2:57:53, 22.81s/it]

  7%|████████▉                                                                                                                              | 33/500 [11:01<2:36:39, 20.13s/it]
{'loss': 1.1689, 'grad_norm': 7.198323726654053, 'learning_rate': 0.00019919548128307954, 'epoch': 0.0}


  7%|█████████▍                                                                                                                             | 35/500 [11:24<2:01:10, 15.64s/it]
{'loss': 0.9923, 'grad_norm': 6.892137050628662, 'learning_rate': 0.00019902680687415705, 'epoch': 0.0}

  7%|█████████▋                                                                                                                             | 36/500 [11:34<1:49:07, 14.11s/it]

  7%|█████████▉                                                                                                                             | 37/500 [11:44<1:39:07, 12.85s/it]


  8%|██████████▌                                                                                                                            | 39/500 [12:34<2:36:50, 20.41s/it]

  8%|██████████▊                                                                                                                            | 40/500 [14:33<6:24:47, 50.19s/it]

  8%|███████████                                                                                                                            | 41/500 [16:27<8:50:35, 69.36s/it]
{'loss': 0.9503, 'grad_norm': 8.421199798583984, 'learning_rate': 0.00019842517531735838, 'epoch': 0.0}


  9%|███████████▍                                                                                                                         | 43/500 [21:01<13:14:59, 104.38s/it]
  9%|███████████▍                                                                                                                         | 43/500 [21:01<13:14:59, 104.38s/it]Traceback (most recent call last):
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 235, in <module>
    main()
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 231, in main
    trainer.train()
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\trl\trainer\sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 1948, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3359, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\accelerator.py", line 2155, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\autograd\__init__.py", line 289, in backward
    _engine_run_backward(
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\autograd\graph.py", line 768, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt