

  0%|▎                                                                                                                                | 1/500 [00:03<26:48,  3.22s/it]
{'loss': 1.7031, 'grad_norm': 1.8000617027282715, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}


  1%|▊                                                                                                                                | 3/500 [00:09<25:30,  3.08s/it]
{'loss': 2.419, 'grad_norm': 2.4497716426849365, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}


  1%|█▎                                                                                                                               | 5/500 [00:15<24:45,  3.00s/it]
{'loss': 2.3355, 'grad_norm': 2.5227978229522705, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}


  1%|█▊                                                                                                                               | 7/500 [00:21<24:56,  3.04s/it]

  2%|██                                                                                                                               | 8/500 [00:24<26:27,  3.23s/it]
{'loss': 2.4524, 'grad_norm': 2.571469306945801, 'learning_rate': 9.95959595959596e-06, 'epoch': 0.0}


  2%|██▌                                                                                                                             | 10/500 [00:34<34:53,  4.27s/it]
{'loss': 2.1638, 'grad_norm': 3.0374882221221924, 'learning_rate': 9.91919191919192e-06, 'epoch': 0.0}


  2%|███                                                                                                                             | 12/500 [00:50<46:46,  5.75s/it]
{'loss': 2.4587, 'grad_norm': 3.333221673965454, 'learning_rate': 9.87878787878788e-06, 'epoch': 0.0}


  3%|███▌                                                                                                                            | 14/500 [00:56<35:13,  4.35s/it]
{'loss': 2.5168, 'grad_norm': 2.790709972381592, 'learning_rate': 9.85858585858586e-06, 'epoch': 0.0}


  3%|████                                                                                                                            | 16/500 [01:03<31:28,  3.90s/it]

  3%|████▎                                                                                                                           | 17/500 [01:07<30:10,  3.75s/it]
{'loss': 2.157, 'grad_norm': 3.3105978965759277, 'learning_rate': 9.797979797979798e-06, 'epoch': 0.0}


  4%|████▊                                                                                                                           | 19/500 [01:13<27:00,  3.37s/it]
{'loss': 2.1762, 'grad_norm': 2.715923547744751, 'learning_rate': 9.757575757575758e-06, 'epoch': 0.0}

  4%|█████                                                                                                                           | 20/500 [01:16<25:52,  3.24s/it]


  4%|█████▋                                                                                                                          | 22/500 [01:23<27:41,  3.48s/it]
{'loss': 2.1537, 'grad_norm': 3.0645618438720703, 'learning_rate': 9.696969696969698e-06, 'epoch': 0.0}


  5%|██████▏                                                                                                                         | 24/500 [01:29<25:38,  3.23s/it]
{'loss': 2.3204, 'grad_norm': 3.2449679374694824, 'learning_rate': 9.656565656565657e-06, 'epoch': 0.0}


  5%|██████▋                                                                                                                         | 26/500 [01:35<24:33,  3.11s/it]
{'loss': 2.1218, 'grad_norm': 3.119340658187866, 'learning_rate': 9.616161616161616e-06, 'epoch': 0.0}

  5%|██████▉                                                                                                                         | 27/500 [01:40<27:37,  3.51s/it]

  6%|███████▏                                                                                                                        | 28/500 [01:44<28:28,  3.62s/it]


  6%|███████▋                                                                                                                        | 30/500 [01:51<28:55,  3.69s/it]

  6%|███████▉                                                                                                                        | 31/500 [01:55<28:31,  3.65s/it]
{'loss': 2.1725, 'grad_norm': 3.6914775371551514, 'learning_rate': 9.515151515151516e-06, 'epoch': 0.0}


  7%|████████▍                                                                                                                       | 33/500 [02:01<25:46,  3.31s/it]
{'loss': 2.3161, 'grad_norm': 3.575195789337158, 'learning_rate': 9.474747474747475e-06, 'epoch': 0.0}


  7%|████████▉                                                                                                                       | 35/500 [02:07<24:11,  3.12s/it]

  7%|█████████▏                                                                                                                      | 36/500 [02:09<23:37,  3.06s/it]

  7%|█████████▍                                                                                                                      | 37/500 [02:12<23:29,  3.04s/it]

  8%|█████████▋                                                                                                                      | 38/500 [02:15<23:07,  3.00s/it]
{'loss': 2.1578, 'grad_norm': 3.9031150341033936, 'learning_rate': 9.373737373737375e-06, 'epoch': 0.0}


  8%|██████████▏                                                                                                                     | 40/500 [02:21<22:36,  2.95s/it]

  8%|██████████▍                                                                                                                     | 41/500 [02:25<23:54,  3.12s/it]
{'loss': 2.2038, 'grad_norm': 4.2921319007873535, 'learning_rate': 9.313131313131313e-06, 'epoch': 0.0}


  9%|███████████                                                                                                                     | 43/500 [02:31<23:09,  3.04s/it]

  9%|███████████▎                                                                                                                    | 44/500 [02:33<22:48,  3.00s/it]
{'loss': 2.1396, 'grad_norm': 4.055013179779053, 'learning_rate': 9.252525252525253e-06, 'epoch': 0.0}

  9%|███████████▌                                                                                                                    | 45/500 [02:36<22:32,  2.97s/it]

  9%|███████████▊                                                                                                                    | 46/500 [02:50<46:07,  6.10s/it]


 10%|████████████▎                                                                                                                   | 48/500 [02:57<36:11,  4.80s/it]
{'loss': 2.1188, 'grad_norm': 4.382883071899414, 'learning_rate': 9.171717171717172e-06, 'epoch': 0.0}


 10%|████████████▊                                                                                                                   | 50/500 [03:03<28:53,  3.85s/it]

 10%|█████████████                                                                                                                   | 51/500 [03:07<28:44,  3.84s/it]

 10%|█████████████▎                                                                                                                  | 52/500 [03:11<28:12,  3.78s/it]
{'loss': 1.8589, 'grad_norm': 2.3319668769836426, 'learning_rate': 9.090909090909091e-06, 'epoch': 0.0}

 11%|█████████████▌                                                                                                                  | 53/500 [03:14<27:11,  3.65s/it]

 11%|█████████████▊                                                                                                                  | 54/500 [03:18<28:04,  3.78s/it]

 11%|██████████████                                                                                                                  | 55/500 [03:22<29:33,  3.98s/it]

 11%|██████████████▎                                                                                                                 | 56/500 [03:26<29:20,  3.97s/it]


 12%|██████████████▊                                                                                                                 | 58/500 [03:33<26:25,  3.59s/it]
{'loss': 2.1022, 'grad_norm': 2.805593967437744, 'learning_rate': 8.969696969696971e-06, 'epoch': 0.0}


 12%|███████████████▎                                                                                                                | 60/500 [03:39<23:59,  3.27s/it]
{'loss': 2.1827, 'grad_norm': 2.925051689147949, 'learning_rate': 8.92929292929293e-06, 'epoch': 0.0}


 12%|███████████████▊                                                                                                                | 62/500 [03:45<23:40,  3.24s/it]
{'loss': 1.87, 'grad_norm': 2.751314163208008, 'learning_rate': 8.888888888888888e-06, 'epoch': 0.0}

 13%|████████████████▏                                                                                                               | 63/500 [03:48<23:02,  3.16s/it]

 13%|████████████████▍                                                                                                               | 64/500 [04:00<42:02,  5.79s/it]


 13%|████████████████▉                                                                                                               | 66/500 [04:07<33:22,  4.61s/it]
{'loss': 2.1502, 'grad_norm': 3.7703440189361572, 'learning_rate': 8.808080808080809e-06, 'epoch': 0.0}


 14%|█████████████████▍                                                                                                              | 68/500 [04:13<27:18,  3.79s/it]
{'loss': 1.9306, 'grad_norm': 3.0283613204956055, 'learning_rate': 8.767676767676768e-06, 'epoch': 0.0}


 14%|█████████████████▉                                                                                                              | 70/500 [04:19<24:13,  3.38s/it]

 14%|██████████████████▏                                                                                                             | 71/500 [04:23<24:40,  3.45s/it]
{'loss': 1.9582, 'grad_norm': 3.0331175327301025, 'learning_rate': 8.707070707070707e-06, 'epoch': 0.0}


 15%|██████████████████▋                                                                                                             | 73/500 [04:29<22:50,  3.21s/it]
{'loss': 2.1685, 'grad_norm': 3.402872323989868, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.0}


 15%|███████████████████▏                                                                                                            | 75/500 [04:35<21:46,  3.07s/it]
{'loss': 1.8792, 'grad_norm': 3.3197414875030518, 'learning_rate': 8.626262626262627e-06, 'epoch': 0.0}


 15%|███████████████████▋                                                                                                            | 77/500 [04:41<21:10,  3.00s/it]

 16%|███████████████████▉                                                                                                            | 78/500 [04:44<20:59,  2.98s/it]

 16%|████████████████████▏                                                                                                           | 79/500 [04:47<20:56,  2.98s/it]

 16%|████████████████████▍                                                                                                           | 80/500 [04:49<20:43,  2.96s/it]

 16%|████████████████████▋                                                                                                           | 81/500 [04:53<22:44,  3.26s/it]
{'loss': 1.6369, 'grad_norm': 4.125376224517822, 'learning_rate': 8.505050505050506e-06, 'epoch': 0.0}


 17%|█████████████████████▏                                                                                                          | 83/500 [04:59<21:33,  3.10s/it]
{'loss': 2.0983, 'grad_norm': 4.750070571899414, 'learning_rate': 8.464646464646465e-06, 'epoch': 0.0}


 17%|█████████████████████▊                                                                                                          | 85/500 [05:05<20:55,  3.02s/it]
{'loss': 1.8861, 'grad_norm': 5.010998725891113, 'learning_rate': 8.424242424242425e-06, 'epoch': 0.0}


 17%|██████████████████████▎                                                                                                         | 87/500 [05:11<20:36,  3.00s/it]
{'loss': 1.748, 'grad_norm': 5.114875793457031, 'learning_rate': 8.383838383838384e-06, 'epoch': 0.0}


 18%|██████████████████████▊                                                                                                         | 89/500 [05:17<20:15,  2.96s/it]
{'loss': 1.6576, 'grad_norm': 4.78546142578125, 'learning_rate': 8.343434343434345e-06, 'epoch': 0.0}


 18%|███████████████████████▎                                                                                                        | 91/500 [05:23<21:14,  3.12s/it]
{'loss': 1.5389, 'grad_norm': 4.930922031402588, 'learning_rate': 8.303030303030305e-06, 'epoch': 0.0}


 19%|███████████████████████▊                                                                                                        | 93/500 [05:29<20:34,  3.03s/it]
{'loss': 1.4487, 'grad_norm': 5.398181438446045, 'learning_rate': 8.262626262626264e-06, 'epoch': 0.0}


 19%|████████████████████████▎                                                                                                       | 95/500 [05:35<20:27,  3.03s/it]
{'loss': 1.5325, 'grad_norm': 5.1185407638549805, 'learning_rate': 8.222222222222222e-06, 'epoch': 0.0}


 19%|████████████████████████▊                                                                                                       | 97/500 [05:41<20:01,  2.98s/it]
{'loss': 1.4813, 'grad_norm': 5.390684127807617, 'learning_rate': 8.181818181818183e-06, 'epoch': 0.0}


 20%|█████████████████████████▎                                                                                                      | 99/500 [05:47<19:39,  2.94s/it]
{'loss': 1.5704, 'grad_norm': 6.925620079040527, 'learning_rate': 8.141414141414142e-06, 'epoch': 0.0}


 20%|█████████████████████████▋                                                                                                     | 101/500 [05:54<21:10,  3.18s/it]

 20%|█████████████████████████▉                                                                                                     | 102/500 [05:57<20:50,  3.14s/it]
{'loss': 1.6302, 'grad_norm': 3.2697508335113525, 'learning_rate': 8.08080808080808e-06, 'epoch': 0.0}


 21%|██████████████████████████▍                                                                                                    | 104/500 [06:04<21:51,  3.31s/it]

 21%|██████████████████████████▋                                                                                                    | 105/500 [06:07<21:39,  3.29s/it]

 21%|██████████████████████████▉                                                                                                    | 106/500 [06:11<23:21,  3.56s/it]

 21%|███████████████████████████▏                                                                                                   | 107/500 [06:15<23:23,  3.57s/it]
{'loss': 1.573, 'grad_norm': 3.3796966075897217, 'learning_rate': 7.97979797979798e-06, 'epoch': 0.0}


 22%|███████████████████████████▋                                                                                                   | 109/500 [06:21<22:17,  3.42s/it]

 22%|███████████████████████████▉                                                                                                   | 110/500 [06:25<22:59,  3.54s/it]

 22%|████████████████████████████▏                                                                                                  | 111/500 [06:29<24:27,  3.77s/it]

 22%|████████████████████████████▍                                                                                                  | 112/500 [06:33<23:49,  3.68s/it]
{'loss': 1.8003, 'grad_norm': 3.868321180343628, 'learning_rate': 7.87878787878788e-06, 'epoch': 0.0}


 23%|████████████████████████████▉                                                                                                  | 114/500 [06:40<22:45,  3.54s/it]

 23%|█████████████████████████████▏                                                                                                 | 115/500 [06:43<21:55,  3.42s/it]
{'loss': 1.5141, 'grad_norm': 3.51059627532959, 'learning_rate': 7.81818181818182e-06, 'epoch': 0.0}


 23%|█████████████████████████████▋                                                                                                 | 117/500 [06:50<21:48,  3.42s/it]

 24%|█████████████████████████████▉                                                                                                 | 118/500 [06:53<21:32,  3.38s/it]
{'loss': 1.5716, 'grad_norm': 3.5976402759552, 'learning_rate': 7.757575757575758e-06, 'epoch': 0.0}

 24%|██████████████████████████████▏                                                                                                | 119/500 [06:56<21:27,  3.38s/it]

 24%|██████████████████████████████▍                                                                                                | 120/500 [07:00<21:41,  3.43s/it]


 24%|██████████████████████████████▉                                                                                                | 122/500 [07:07<22:29,  3.57s/it]

 25%|███████████████████████████████▏                                                                                               | 123/500 [07:13<26:52,  4.28s/it]

 25%|███████████████████████████████▍                                                                                               | 124/500 [07:24<38:32,  6.15s/it]

 25%|███████████████████████████████▊                                                                                               | 125/500 [07:27<32:50,  5.25s/it]
{'loss': 1.7036, 'grad_norm': 4.233964920043945, 'learning_rate': 7.616161616161617e-06, 'epoch': 0.01}


 25%|████████████████████████████████▎                                                                                              | 127/500 [07:33<25:20,  4.08s/it]

 26%|████████████████████████████████▌                                                                                              | 128/500 [07:36<23:10,  3.74s/it]

 26%|████████████████████████████████▊                                                                                              | 129/500 [07:39<21:39,  3.50s/it]

 26%|█████████████████████████████████                                                                                              | 130/500 [07:42<20:34,  3.34s/it]

 26%|█████████████████████████████████▎                                                                                             | 131/500 [07:45<20:57,  3.41s/it]
{'loss': 1.5017, 'grad_norm': 4.825773239135742, 'learning_rate': 7.494949494949496e-06, 'epoch': 0.01}


 27%|█████████████████████████████████▊                                                                                             | 133/500 [07:51<19:18,  3.16s/it]
{'loss': 1.3664, 'grad_norm': 5.730544090270996, 'learning_rate': 7.454545454545456e-06, 'epoch': 0.01}


 27%|██████████████████████████████████▎                                                                                            | 135/500 [07:57<18:36,  3.06s/it]
{'loss': 1.214, 'grad_norm': 4.612298488616943, 'learning_rate': 7.414141414141415e-06, 'epoch': 0.01}


 27%|██████████████████████████████████▊                                                                                            | 137/500 [08:03<18:02,  2.98s/it]

 28%|███████████████████████████████████                                                                                            | 138/500 [08:06<17:56,  2.97s/it]
{'loss': 1.0741, 'grad_norm': 5.863149166107178, 'learning_rate': 7.353535353535353e-06, 'epoch': 0.01}


 28%|███████████████████████████████████▌                                                                                           | 140/500 [08:12<17:38,  2.94s/it]

 28%|███████████████████████████████████▊                                                                                           | 141/500 [08:16<19:21,  3.23s/it]
{'loss': 1.1749, 'grad_norm': 6.554968357086182, 'learning_rate': 7.2929292929292934e-06, 'epoch': 0.01}


 29%|████████████████████████████████████▎                                                                                          | 143/500 [08:21<18:13,  3.06s/it]
{'loss': 1.2063, 'grad_norm': 6.346780776977539, 'learning_rate': 7.252525252525253e-06, 'epoch': 0.01}


 29%|████████████████████████████████████▊                                                                                          | 145/500 [08:27<17:40,  2.99s/it]
{'loss': 0.8462, 'grad_norm': 6.178715229034424, 'learning_rate': 7.212121212121212e-06, 'epoch': 0.01}


 29%|█████████████████████████████████████▎                                                                                         | 147/500 [08:33<17:22,  2.95s/it]

 30%|█████████████████████████████████████▌                                                                                         | 148/500 [08:36<17:11,  2.93s/it]

 30%|█████████████████████████████████████▊                                                                                         | 149/500 [08:39<17:12,  2.94s/it]

 30%|██████████████████████████████████████                                                                                         | 150/500 [08:42<17:05,  2.93s/it]

 30%|██████████████████████████████████████▎                                                                                        | 151/500 [08:46<18:41,  3.21s/it]
{'loss': 1.6963, 'grad_norm': 3.3159310817718506, 'learning_rate': 7.111111111111112e-06, 'epoch': 0.01}


 31%|██████████████████████████████████████▊                                                                                        | 153/500 [08:52<17:54,  3.10s/it]
{'loss': 1.4082, 'grad_norm': 3.5118038654327393, 'learning_rate': 7.070707070707071e-06, 'epoch': 0.01}


 31%|███████████████████████████████████████▎                                                                                       | 155/500 [08:58<17:32,  3.05s/it]
{'loss': 1.7123, 'grad_norm': 3.8448424339294434, 'learning_rate': 7.030303030303031e-06, 'epoch': 0.01}


 31%|███████████████████████████████████████▉                                                                                       | 157/500 [09:04<17:25,  3.05s/it]
{'loss': 1.5125, 'grad_norm': 4.1983819007873535, 'learning_rate': 6.98989898989899e-06, 'epoch': 0.01}


 32%|████████████████████████████████████████▍                                                                                      | 159/500 [09:10<17:01,  3.00s/it]
{'loss': 1.4062, 'grad_norm': 3.6043195724487305, 'learning_rate': 6.9494949494949505e-06, 'epoch': 0.01}

 32%|████████████████████████████████████████▋                                                                                      | 160/500 [09:13<16:59,  3.00s/it]


 32%|█████████████████████████████████████████▏                                                                                     | 162/500 [09:19<17:49,  3.16s/it]
{'loss': 1.4817, 'grad_norm': 3.7782156467437744, 'learning_rate': 6.88888888888889e-06, 'epoch': 0.01}


 33%|█████████████████████████████████████████▋                                                                                     | 164/500 [09:25<17:02,  3.04s/it]
{'loss': 1.4981, 'grad_norm': 3.615703582763672, 'learning_rate': 6.848484848484849e-06, 'epoch': 0.01}


 33%|██████████████████████████████████████████▏                                                                                    | 166/500 [09:32<17:30,  3.15s/it]

 33%|██████████████████████████████████████████▍                                                                                    | 167/500 [09:35<17:43,  3.19s/it]
{'loss': 1.2975, 'grad_norm': 4.0917887687683105, 'learning_rate': 6.787878787878789e-06, 'epoch': 0.01}


 34%|██████████████████████████████████████████▉                                                                                    | 169/500 [09:41<17:42,  3.21s/it]
{'loss': 1.4722, 'grad_norm': 4.7141594886779785, 'learning_rate': 6.747474747474749e-06, 'epoch': 0.01}

 34%|███████████████████████████████████████████▏                                                                                   | 170/500 [09:45<17:30,  3.18s/it]


 34%|███████████████████████████████████████████▋                                                                                   | 172/500 [09:52<18:23,  3.37s/it]

 35%|███████████████████████████████████████████▉                                                                                   | 173/500 [09:55<18:09,  3.33s/it]
{'loss': 1.2423, 'grad_norm': 5.094705104827881, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.01}


 35%|████████████████████████████████████████████▍                                                                                  | 175/500 [10:01<17:31,  3.23s/it]
{'loss': 1.0528, 'grad_norm': 4.592033863067627, 'learning_rate': 6.626262626262627e-06, 'epoch': 0.01}


 35%|████████████████████████████████████████████▉                                                                                  | 177/500 [10:08<17:12,  3.20s/it]
{'loss': 1.2757, 'grad_norm': 4.47966194152832, 'learning_rate': 6.585858585858586e-06, 'epoch': 0.01}


 36%|█████████████████████████████████████████████▍                                                                                 | 179/500 [10:14<17:04,  3.19s/it]

 36%|█████████████████████████████████████████████▋                                                                                 | 180/500 [10:17<17:24,  3.26s/it]

 36%|█████████████████████████████████████████████▉                                                                                 | 181/500 [10:21<17:57,  3.38s/it]

 36%|██████████████████████████████████████████████▏                                                                                | 182/500 [10:24<17:17,  3.26s/it]

 37%|██████████████████████████████████████████████▍                                                                                | 183/500 [10:27<16:44,  3.17s/it]
{'loss': 1.3975, 'grad_norm': 4.354394912719727, 'learning_rate': 6.464646464646466e-06, 'epoch': 0.01}

 37%|██████████████████████████████████████████████▋                                                                                | 184/500 [10:30<16:21,  3.11s/it]


 37%|███████████████████████████████████████████████▏                                                                               | 186/500 [10:36<15:42,  3.00s/it]

 37%|███████████████████████████████████████████████▍                                                                               | 187/500 [10:39<15:57,  3.06s/it]

 38%|███████████████████████████████████████████████▊                                                                               | 188/500 [10:42<15:53,  3.06s/it]

 38%|████████████████████████████████████████████████                                                                               | 189/500 [10:45<16:04,  3.10s/it]
{'loss': 1.2119, 'grad_norm': 5.409146785736084, 'learning_rate': 6.343434343434344e-06, 'epoch': 0.01}

 38%|████████████████████████████████████████████████▎                                                                              | 190/500 [10:48<16:02,  3.11s/it]

 38%|████████████████████████████████████████████████▌                                                                              | 191/500 [10:58<26:44,  5.19s/it]


 39%|█████████████████████████████████████████████████                                                                              | 193/500 [11:07<23:33,  4.60s/it]
{'loss': 1.1066, 'grad_norm': 4.251420974731445, 'learning_rate': 6.262626262626264e-06, 'epoch': 0.01}


 39%|█████████████████████████████████████████████████▌                                                                             | 195/500 [11:13<19:50,  3.90s/it]

 39%|█████████████████████████████████████████████████▊                                                                             | 196/500 [11:17<20:05,  3.97s/it]

 39%|██████████████████████████████████████████████████                                                                             | 197/500 [11:22<20:22,  4.04s/it]

 40%|██████████████████████████████████████████████████▎                                                                            | 198/500 [11:25<19:20,  3.84s/it]
{'loss': 0.9219, 'grad_norm': 6.135031223297119, 'learning_rate': 6.1616161616161615e-06, 'epoch': 0.01}


 40%|██████████████████████████████████████████████████▊                                                                            | 200/500 [11:31<17:34,  3.51s/it]
{'loss': 0.8272, 'grad_norm': 5.964542388916016, 'learning_rate': 6.121212121212121e-06, 'epoch': 0.01}


 40%|███████████████████████████████████████████████████▎                                                                           | 202/500 [11:45<25:21,  5.11s/it]
{'loss': 1.5091, 'grad_norm': 3.457245349884033, 'learning_rate': 6.080808080808081e-06, 'epoch': 0.01}


 41%|███████████████████████████████████████████████████▊                                                                           | 204/500 [11:55<23:47,  4.82s/it]
{'loss': 1.4937, 'grad_norm': 3.1782937049865723, 'learning_rate': 6.040404040404041e-06, 'epoch': 0.01}

 41%|████████████████████████████████████████████████████                                                                           | 205/500 [11:58<22:09,  4.51s/it]


 41%|████████████████████████████████████████████████████▌                                                                          | 207/500 [12:05<19:06,  3.91s/it]
{'loss': 1.3797, 'grad_norm': 3.3224472999572754, 'learning_rate': 5.97979797979798e-06, 'epoch': 0.01}


 42%|█████████████████████████████████████████████████████                                                                          | 209/500 [12:12<17:15,  3.56s/it]

 42%|█████████████████████████████████████████████████████▎                                                                         | 210/500 [12:15<16:39,  3.45s/it]

 42%|█████████████████████████████████████████████████████▌                                                                         | 211/500 [12:19<17:25,  3.62s/it]
{'loss': 1.293, 'grad_norm': 4.194050312042236, 'learning_rate': 5.8989898989899e-06, 'epoch': 0.01}


 43%|██████████████████████████████████████████████████████                                                                         | 213/500 [12:25<16:23,  3.43s/it]
{'loss': 1.348, 'grad_norm': 3.4856836795806885, 'learning_rate': 5.858585858585859e-06, 'epoch': 0.01}

 43%|██████████████████████████████████████████████████████▎                                                                        | 214/500 [12:29<16:09,  3.39s/it]


 43%|██████████████████████████████████████████████████████▊                                                                        | 216/500 [12:35<15:43,  3.32s/it]
{'loss': 1.2756, 'grad_norm': 3.689061403274536, 'learning_rate': 5.797979797979798e-06, 'epoch': 0.01}


 44%|███████████████████████████████████████████████████████▎                                                                       | 218/500 [12:41<15:12,  3.23s/it]

 44%|███████████████████████████████████████████████████████▋                                                                       | 219/500 [12:45<15:14,  3.25s/it]
 44%|███████████████████████████████████████████████████████▋                                                                       | 219/500 [12:45<15:14,  3.25s/it]Traceback (most recent call last):
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 244, in <module>
    main()
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 240, in main
    trainer.train()
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\trl\trainer\sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 1948, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\utils\operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\utils\operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\amp\autocast_mode.py", line 43, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\peft\peft_model.py", line 1577, in forward
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\peft\tuners\tuners_utils.py", line 188, in forward
    return self.model.forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 1189, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 1001, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 750, in forward
    hidden_states = self.mlp(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 309, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\bitsandbytes\nn\modules.py", line 477, in forward
    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\bitsandbytes\autograd\_functions.py", line 579, in matmul_4bit
    return MatMul4Bit.apply(A, B, out, bias, quant_state)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\autograd\function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\bitsandbytes\autograd\_functions.py", line 509, in forward
    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt