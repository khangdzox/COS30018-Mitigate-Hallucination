
  0%|                                                                                                                                                                           | 0/500 [00:00<?, ?it/s]


  0%|▋                                                                                                                                                                | 2/500 [00:15<1:04:07,  7.73s/it]
{'loss': 2.3663, 'grad_norm': 4.638459205627441, 'learning_rate': 0.0008, 'epoch': 0.0}

  1%|▉                                                                                                                                                                | 3/500 [00:23<1:03:52,  7.71s/it]

  1%|█▎                                                                                                                                                               | 4/500 [00:30<1:03:33,  7.69s/it]

  1%|█▌                                                                                                                                                               | 5/500 [00:38<1:03:14,  7.66s/it]


  1%|██▎                                                                                                                                                              | 7/500 [00:53<1:02:41,  7.63s/it]
{'loss': 2.2737, 'grad_norm': 4.240165710449219, 'learning_rate': 0.0024, 'epoch': 0.0}

  2%|██▌                                                                                                                                                              | 8/500 [01:01<1:02:30,  7.62s/it]

  2%|██▉                                                                                                                                                              | 9/500 [01:08<1:02:16,  7.61s/it]

  2%|███▏                                                                                                                                                            | 10/500 [01:16<1:02:15,  7.62s/it]

  2%|███▌                                                                                                                                                            | 11/500 [01:24<1:03:51,  7.84s/it]

  2%|███▊                                                                                                                                                            | 12/500 [01:32<1:02:55,  7.74s/it]


  3%|████▍                                                                                                                                                           | 14/500 [01:47<1:02:21,  7.70s/it]
{'loss': 2.3859, 'grad_norm': nan, 'learning_rate': 0.0044, 'epoch': 0.0}

  3%|████▊                                                                                                                                                           | 15/500 [01:55<1:02:00,  7.67s/it]

  3%|█████                                                                                                                                                           | 16/500 [02:02<1:01:49,  7.66s/it]

  3%|█████▍                                                                                                                                                          | 17/500 [02:10<1:01:46,  7.67s/it]


  4%|██████                                                                                                                                                          | 19/500 [02:25<1:01:34,  7.68s/it]

  4%|██████▍                                                                                                                                                         | 20/500 [02:33<1:01:28,  7.68s/it]

  4%|██████▋                                                                                                                                                         | 21/500 [02:41<1:02:40,  7.85s/it]
{'loss': 2.2141, 'grad_norm': 4.182592391967773, 'learning_rate': 0.0064, 'epoch': 0.0}

  4%|███████                                                                                                                                                         | 22/500 [02:49<1:01:52,  7.77s/it]

  5%|███████▎                                                                                                                                                        | 23/500 [02:57<1:01:19,  7.71s/it]

  5%|███████▋                                                                                                                                                        | 24/500 [03:04<1:01:00,  7.69s/it]


  5%|████████▎                                                                                                                                                       | 26/500 [03:19<1:00:30,  7.66s/it]
{'loss': 2.1554, 'grad_norm': 3.5522758960723877, 'learning_rate': 0.0084, 'epoch': 0.01}

  5%|████████▋                                                                                                                                                       | 27/500 [03:27<1:00:20,  7.65s/it]

  6%|████████▉                                                                                                                                                       | 28/500 [03:35<1:00:07,  7.64s/it]

  6%|█████████▍                                                                                                                                                        | 29/500 [03:42<59:52,  7.63s/it]

  6%|█████████▋                                                                                                                                                        | 30/500 [03:50<59:27,  7.59s/it]

  6%|█████████▉                                                                                                                                                      | 31/500 [03:58<1:01:53,  7.92s/it]

  6%|██████████▏                                                                                                                                                     | 32/500 [04:06<1:00:59,  7.82s/it]


  7%|███████████                                                                                                                                                       | 34/500 [04:21<59:48,  7.70s/it]
{'loss': 1.9644, 'grad_norm': 8.974360466003418, 'learning_rate': 0.009998250366089848, 'epoch': 0.01}

  7%|███████████▎                                                                                                                                                      | 35/500 [04:29<59:11,  7.64s/it]

  7%|███████████▋                                                                                                                                                      | 36/500 [04:36<59:12,  7.66s/it]

  7%|███████████▉                                                                                                                                                      | 37/500 [04:44<58:58,  7.64s/it]Traceback (most recent call last):
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 243, in <module>
    main()
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 239, in main
    trainer.train()
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\trl\trainer\sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 1948, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3359, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\accelerator.py", line 2155, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\autograd\__init__.py", line 289, in backward
    _engine_run_backward(
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\autograd\graph.py", line 768, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt