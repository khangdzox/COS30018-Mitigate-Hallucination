
  0%|▌                                                                                                                                | 2/500 [00:01<07:16,  1.14it/s]
{'loss': 1.0583, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 0.0}

  1%|█                                                                                                                                | 4/500 [00:03<06:48,  1.21it/s]
{'loss': 1.6878, 'grad_norm': 3.4385628700256348, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
{'loss': 2.1694, 'grad_norm': 2.8672728538513184, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}

  1%|█▊                                                                                                                               | 7/500 [00:05<06:33,  1.25it/s]
{'loss': 1.6798, 'grad_norm': 3.137204647064209, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}

  2%|██▎                                                                                                                              | 9/500 [00:07<06:28,  1.26it/s]
{'loss': 2.0364, 'grad_norm': 3.7381179332733154, 'learning_rate': 9.999899300364534e-06, 'epoch': 0.0}
{'loss': 1.7096, 'grad_norm': 4.768440246582031, 'learning_rate': 9.999597205514298e-06, 'epoch': 0.0}

  2%|██▌                                                                                                                             | 10/500 [00:08<06:28,  1.26it/s]

  3%|███▎                                                                                                                            | 13/500 [00:11<07:33,  1.07it/s]
{'loss': 2.0742, 'grad_norm': 3.830134391784668, 'learning_rate': 9.997482711915926e-06, 'epoch': 0.0}
{'loss': 2.7606, 'grad_norm': 3.7945284843444824, 'learning_rate': 9.99637523900237e-06, 'epoch': 0.0}

  3%|████                                                                                                                            | 16/500 [00:13<06:48,  1.18it/s]
{'loss': 2.4943, 'grad_norm': 3.4181931018829346, 'learning_rate': 9.99506651282272e-06, 'epoch': 0.0}

  4%|████▌                                                                                                                           | 18/500 [00:15<06:25,  1.25it/s]
{'loss': 2.4077, 'grad_norm': 3.847649097442627, 'learning_rate': 9.991845519630679e-06, 'epoch': 0.0}
{'loss': 1.8812, 'grad_norm': 4.198042869567871, 'learning_rate': 9.989933382359423e-06, 'epoch': 0.0}

  4%|█████                                                                                                                           | 20/500 [00:16<06:16,  1.27it/s]
{'loss': 2.2801, 'grad_norm': 4.102980613708496, 'learning_rate': 9.985506211566388e-06, 'epoch': 0.0}

  5%|█████▉                                                                                                                          | 23/500 [00:19<06:51,  1.16it/s]
{'loss': 2.0649, 'grad_norm': 3.3413259983062744, 'learning_rate': 9.98027578700917e-06, 'epoch': 0.0}

  5%|██████▍                                                                                                                         | 25/500 [00:21<06:27,  1.23it/s]
{'loss': 2.6495, 'grad_norm': 4.9576568603515625, 'learning_rate': 9.974242951402236e-06, 'epoch': 0.0}
{'loss': 2.28, 'grad_norm': 4.550416469573975, 'learning_rate': 9.970925928158275e-06, 'epoch': 0.0}

  6%|███████▏                                                                                                                        | 28/500 [00:23<06:10,  1.27it/s]
{'loss': 2.4854, 'grad_norm': 5.180719375610352, 'learning_rate': 9.963691338830045e-06, 'epoch': 0.0}
{'loss': 2.2122, 'grad_norm': 4.7154622077941895, 'learning_rate': 9.959774064153977e-06, 'epoch': 0.0}

  6%|███████▋                                                                                                                        | 30/500 [00:25<06:11,  1.27it/s]

  6%|████████▏                                                                                                                       | 32/500 [00:27<07:07,  1.10it/s]
{'loss': 1.8897, 'grad_norm': 3.8450253009796143, 'learning_rate': 9.946824237646823e-06, 'epoch': 0.0}
{'loss': 2.3187, 'grad_norm': 5.5923638343811035, 'learning_rate': 9.942108874226812e-06, 'epoch': 0.0}

  7%|████████▉                                                                                                                       | 35/500 [00:29<06:15,  1.24it/s]
{'loss': 2.386, 'grad_norm': 4.9845404624938965, 'learning_rate': 9.93208114306486e-06, 'epoch': 0.0}

  7%|█████████▍                                                                                                                      | 37/500 [00:31<06:10,  1.25it/s]
{'loss': 2.556, 'grad_norm': 5.884649753570557, 'learning_rate': 9.921258765867919e-06, 'epoch': 0.0}
{'loss': 2.4262, 'grad_norm': 5.392366409301758, 'learning_rate': 9.915550124911866e-06, 'epoch': 0.0}

  8%|██████████▏                                                                                                                     | 40/500 [00:33<05:54,  1.30it/s]
{'loss': 2.1673, 'grad_norm': 5.2380852699279785, 'learning_rate': 9.903539087991462e-06, 'epoch': 0.0}

  8%|██████████▍                                                                                                                     | 41/500 [00:35<08:14,  1.08s/it]
{'loss': 3.0546, 'grad_norm': 8.322303771972656, 'learning_rate': 9.890738003669029e-06, 'epoch': 0.0}

  9%|███████████▎                                                                                                                    | 44/500 [00:37<06:41,  1.14it/s]
{'loss': 2.6276, 'grad_norm': 6.573944568634033, 'learning_rate': 9.877148934427037e-06, 'epoch': 0.0}
{'loss': 2.2345, 'grad_norm': 7.599636554718018, 'learning_rate': 9.870059584711668e-06, 'epoch': 0.0}

  9%|███████████▊                                                                                                                    | 46/500 [00:39<06:10,  1.22it/s]
{'loss': 1.9592, 'grad_norm': 6.144455432891846, 'learning_rate': 9.855292682870552e-06, 'epoch': 0.0}

 10%|████████████▌                                                                                                                   | 49/500 [00:41<06:00,  1.25it/s]
{'loss': 2.4537, 'grad_norm': nan, 'learning_rate': 9.847615725553457e-06, 'epoch': 0.0}
{'loss': 2.483, 'grad_norm': 6.3541178703308105, 'learning_rate': 9.839743506981783e-06, 'epoch': 0.0}

 10%|█████████████                                                                                                                   | 51/500 [00:43<07:36,  1.02s/it]

 11%|█████████████▌                                                                                                                  | 53/500 [00:45<06:40,  1.12it/s]
{'loss': 1.3361, 'grad_norm': 2.8425283432006836, 'learning_rate': 9.814958493905962e-06, 'epoch': 0.0}
{'loss': 2.1815, 'grad_norm': 4.7071943283081055, 'learning_rate': 9.806308479691595e-06, 'epoch': 0.0}

 11%|██████████████▎                                                                                                                 | 56/500 [00:47<06:07,  1.21it/s]
{'loss': 1.9772, 'grad_norm': 3.7428247928619385, 'learning_rate': 9.788428015268027e-06, 'epoch': 0.0}
{'loss': 2.5453, 'grad_norm': 5.264117240905762, 'learning_rate': 9.779198285281326e-06, 'epoch': 0.0}

 12%|██████████████▊                                                                                                                 | 58/500 [00:49<05:53,  1.25it/s]
{'loss': 2.5799, 'grad_norm': 5.665273666381836, 'learning_rate': 9.760161688604008e-06, 'epoch': 0.0}

 12%|███████████████▎                                                                                                                | 60/500 [00:50<05:48,  1.26it/s]
{'loss': 2.1741, 'grad_norm': 3.861506700515747, 'learning_rate': 9.740358145174999e-06, 'epoch': 0.0}

 13%|████████████████▏                                                                                                               | 63/500 [00:53<06:20,  1.15it/s]
{'loss': 1.9872, 'grad_norm': 3.720238447189331, 'learning_rate': 9.719790845697534e-06, 'epoch': 0.0}

 13%|████████████████▋                                                                                                               | 65/500 [00:55<06:00,  1.21it/s]
{'loss': 2.134, 'grad_norm': 3.5170047283172607, 'learning_rate': 9.698463103929542e-06, 'epoch': 0.0}
 13%|████████████████▋                                                                                                               | 65/500 [00:55<06:00,  1.21it/s]Traceback (most recent call last):
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 244, in <module>
    main()
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 240, in main
    trainer.train()
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\trl\trainer\sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 1948, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3359, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\accelerator.py", line 2155, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\autograd\__init__.py", line 289, in backward
    _engine_run_backward(
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\autograd\graph.py", line 768, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt