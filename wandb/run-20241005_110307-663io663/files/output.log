

  0%|▎                                                                                                                                        | 1/500 [00:08<1:09:30,  8.36s/it]

  0%|▌                                                                                                                                        | 2/500 [00:16<1:07:19,  8.11s/it]

  1%|▊                                                                                                                                        | 3/500 [00:23<1:05:26,  7.90s/it]

  1%|█                                                                                                                                        | 4/500 [00:31<1:05:11,  7.89s/it]

  1%|█▎                                                                                                                                       | 5/500 [00:39<1:04:56,  7.87s/it]

  1%|█▋                                                                                                                                       | 6/500 [00:47<1:04:49,  7.87s/it]

  1%|█▉                                                                                                                                       | 7/500 [00:55<1:04:16,  7.82s/it]

  2%|██▏                                                                                                                                      | 8/500 [01:03<1:04:10,  7.83s/it]

  2%|██▍                                                                                                                                      | 9/500 [01:10<1:03:51,  7.80s/it]

  2%|██▋                                                                                                                                     | 10/500 [01:18<1:03:39,  7.79s/it]

  2%|██▉                                                                                                                                     | 11/500 [01:27<1:07:07,  8.24s/it]

  2%|███▎                                                                                                                                    | 12/500 [01:35<1:06:03,  8.12s/it]

  3%|███▌                                                                                                                                    | 13/500 [01:44<1:06:40,  8.22s/it]

  3%|███▊                                                                                                                                    | 14/500 [01:52<1:07:08,  8.29s/it]

  3%|████                                                                                                                                    | 15/500 [02:01<1:07:49,  8.39s/it]

  3%|████▎                                                                                                                                   | 16/500 [02:10<1:08:39,  8.51s/it]

  3%|████▌                                                                                                                                   | 17/500 [02:19<1:11:05,  8.83s/it]

  4%|████▉                                                                                                                                   | 18/500 [02:28<1:10:18,  8.75s/it]

  4%|█████▏                                                                                                                                  | 19/500 [02:36<1:09:34,  8.68s/it]
{'loss': 1.3817, 'grad_norm': nan, 'learning_rate': 9.985506211566388e-05, 'epoch': 0.0}

  4%|█████▍                                                                                                                                  | 20/500 [02:45<1:08:58,  8.62s/it]

  4%|█████▋                                                                                                                                  | 21/500 [02:54<1:11:03,  8.90s/it]


  5%|██████▎                                                                                                                                 | 23/500 [03:12<1:10:50,  8.91s/it]
{'loss': 1.2858, 'grad_norm': 3.454882860183716, 'learning_rate': 9.974242951402235e-05, 'epoch': 0.0}

  5%|██████▌                                                                                                                                 | 24/500 [03:20<1:09:18,  8.74s/it]

  5%|██████▊                                                                                                                                 | 25/500 [03:29<1:08:37,  8.67s/it]

  5%|███████                                                                                                                                 | 26/500 [03:37<1:08:00,  8.61s/it]


  6%|███████▌                                                                                                                                | 28/500 [03:54<1:07:32,  8.59s/it]
{'loss': 1.2099, 'grad_norm': 3.2937939167022705, 'learning_rate': 9.955657010501806e-05, 'epoch': 0.0}


  6%|████████▏                                                                                                                               | 30/500 [04:12<1:07:05,  8.56s/it]

  6%|████████▍                                                                                                                               | 31/500 [04:21<1:08:03,  8.71s/it]

  6%|████████▋                                                                                                                               | 32/500 [04:29<1:07:56,  8.71s/it]

  7%|████████▉                                                                                                                               | 33/500 [04:38<1:07:50,  8.72s/it]

  7%|█████████▏                                                                                                                              | 34/500 [04:49<1:12:20,  9.31s/it]
{'loss': 1.109, 'grad_norm': 2.7515289783477783, 'learning_rate': 9.926769179238466e-05, 'epoch': 0.0}

  7%|█████████▌                                                                                                                              | 35/500 [04:59<1:15:28,  9.74s/it]

  7%|█████████▊                                                                                                                              | 36/500 [05:10<1:17:17, 10.00s/it]

  7%|██████████                                                                                                                              | 37/500 [05:22<1:22:47, 10.73s/it]


  8%|██████████▌                                                                                                                             | 39/500 [05:45<1:23:53, 10.92s/it]
{'loss': 1.1045, 'grad_norm': 2.774125814437866, 'learning_rate': 9.897237175829926e-05, 'epoch': 0.01}

  8%|██████████▉                                                                                                                             | 40/500 [05:55<1:23:10, 10.85s/it]

  8%|███████████▏                                                                                                                            | 41/500 [06:06<1:23:43, 10.94s/it]

  8%|███████████▍                                                                                                                            | 42/500 [06:21<1:31:06, 11.94s/it]Traceback (most recent call last):
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 243, in <module>
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 239, in main
    # Start training
    ^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\trl\trainer\sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 1948, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\utils\operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\utils\operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\amp\autocast_mode.py", line 43, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\peft\peft_model.py", line 1577, in forward
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\peft\tuners\tuners_utils.py", line 188, in forward
    return self.model.forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 1189, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 1001, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 750, in forward
    hidden_states = self.mlp(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 309, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
                                                                ^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\bitsandbytes\nn\modules.py", line 477, in forward
    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\bitsandbytes\autograd\_functions.py", line 579, in matmul_4bit
    return MatMul4Bit.apply(A, B, out, bias, quant_state)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\autograd\function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\bitsandbytes\autograd\_functions.py", line 509, in forward
    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\bitsandbytes\functional.py", line 1353, in dequantize_4bit
    out = torch.empty(quant_state.shape, dtype=quant_state.dtype, device=A.device)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt