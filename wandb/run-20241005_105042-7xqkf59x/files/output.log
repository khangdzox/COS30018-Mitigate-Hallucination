
  0%|▎                                                                                                                                          | 1/500 [00:01<13:45,  1.66s/it]
{'loss': 2.2091, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 0.0}

  1%|▊                                                                                                                                          | 3/500 [00:03<09:29,  1.15s/it]
{'loss': 2.2944, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 0.0}

  1%|█▍                                                                                                                                         | 5/500 [00:05<08:48,  1.07s/it]
{'loss': 2.0814, 'grad_norm': 3.7850139141082764, 'learning_rate': 4e-05, 'epoch': 0.0}

  1%|█▉                                                                                                                                         | 7/500 [00:07<08:26,  1.03s/it]
{'loss': 2.5242, 'grad_norm': 3.7404069900512695, 'learning_rate': 8e-05, 'epoch': 0.0}

  2%|██▌                                                                                                                                        | 9/500 [00:09<08:01,  1.02it/s]
{'loss': 2.2933, 'grad_norm': nan, 'learning_rate': 0.0001, 'epoch': 0.0}

  2%|██▊                                                                                                                                       | 10/500 [00:10<08:05,  1.01it/s]
{'loss': 2.1945, 'grad_norm': 5.238886833190918, 'learning_rate': 9.974242951402235e-05, 'epoch': 0.0}

  2%|███▎                                                                                                                                      | 12/500 [00:13<09:02,  1.11s/it]

  3%|███▊                                                                                                                                      | 14/500 [00:15<08:37,  1.07s/it]
{'loss': 1.709, 'grad_norm': 4.147189140319824, 'learning_rate': 9.839743506981782e-05, 'epoch': 0.0}

  3%|████▍                                                                                                                                     | 16/500 [00:17<08:22,  1.04s/it]
{'loss': 1.9838, 'grad_norm': 4.302422523498535, 'learning_rate': 9.687515136252731e-05, 'epoch': 0.0}
{'loss': 2.1479, 'grad_norm': 5.017236709594727, 'learning_rate': 9.593172782532268e-05, 'epoch': 0.0}

  4%|████▉                                                                                                                                     | 18/500 [00:19<08:04,  1.00s/it]
{'loss': 1.7649, 'grad_norm': 5.02488899230957, 'learning_rate': 9.369246885348926e-05, 'epoch': 0.0}

  4%|█████▌                                                                                                                                    | 20/500 [00:21<07:52,  1.02it/s]

  4%|██████                                                                                                                                    | 22/500 [00:23<08:44,  1.10s/it]
{'loss': 1.9405, 'grad_norm': 11.82225227355957, 'learning_rate': 9.100305426420956e-05, 'epoch': 0.0}

  5%|██████▌                                                                                                                                   | 24/500 [00:25<08:06,  1.02s/it]
{'loss': 1.4836, 'grad_norm': 8.844101905822754, 'learning_rate': 8.789119261039385e-05, 'epoch': 0.0}

  5%|███████▏                                                                                                                                  | 26/500 [00:27<07:58,  1.01s/it]
{'loss': 1.6256, 'grad_norm': 5.835538864135742, 'learning_rate': 8.438894484078086e-05, 'epoch': 0.0}

  6%|███████▋                                                                                                                                  | 28/500 [00:29<07:50,  1.00it/s]

  7%|█████████                                                                                                                                 | 33/500 [00:35<08:24,  1.08s/it]
{'loss': 1.5857, 'grad_norm': 6.464504241943359, 'learning_rate': 7.848353992861195e-05, 'epoch': 0.0}
{'loss': 1.1843, 'grad_norm': 6.893014907836914, 'learning_rate': 7.636127338052512e-05, 'epoch': 0.0}
{'loss': 1.7704, 'grad_norm': 7.242428779602051, 'learning_rate': 7.417106419422819e-05, 'epoch': 0.0}
{'loss': 1.0749, 'grad_norm': 7.132585048675537, 'learning_rate': 7.191855733945387e-05, 'epoch': 0.0}
{'loss': 1.5996, 'grad_norm': 7.171245098114014, 'learning_rate': 6.960955834980028e-05, 'epoch': 0.0}
{'loss': 1.412, 'grad_norm': 6.969776153564453, 'learning_rate': 6.725001835974853e-05, 'epoch': 0.0}

  7%|█████████▋                                                                                                                                | 35/500 [00:37<08:04,  1.04s/it]

  7%|██████████▏                                                                                                                               | 37/500 [00:39<08:12,  1.06s/it]
{'loss': 1.1123, 'grad_norm': 5.825486183166504, 'learning_rate': 5.992952333228728e-05, 'epoch': 0.0}
{'loss': 0.6954, 'grad_norm': 8.800251960754395, 'learning_rate': 5.7429699097518585e-05, 'epoch': 0.0}

  8%|██████████▊                                                                                                                               | 39/500 [00:41<07:50,  1.02s/it]

  8%|███████████▎                                                                                                                              | 41/500 [00:43<09:00,  1.18s/it]
{'loss': 1.1744, 'grad_norm': 7.081913471221924, 'learning_rate': 4.984133397065889e-05, 'epoch': 0.0}

  9%|███████████▊                                                                                                                              | 43/500 [00:45<08:15,  1.08s/it]
{'loss': 1.3937, 'grad_norm': 5.895700454711914, 'learning_rate': 4.477357683661734e-05, 'epoch': 0.0}

  9%|████████████▍                                                                                                                             | 45/500 [00:47<07:48,  1.03s/it]
{'loss': 1.6007, 'grad_norm': 6.081400394439697, 'learning_rate': 3.9759666596740476e-05, 'epoch': 0.0}

  9%|████████████▉                                                                                                                             | 47/500 [00:49<07:42,  1.02s/it]
{'loss': 1.6865, 'grad_norm': 4.709901809692383, 'learning_rate': 3.485126066291364e-05, 'epoch': 0.0}

 10%|█████████████▌                                                                                                                            | 49/500 [00:51<07:23,  1.02it/s]
{'loss': 0.9208, 'grad_norm': 8.806902885437012, 'learning_rate': 3.0098929455206904e-05, 'epoch': 0.0}

 10%|█████████████▊                                                                                                                            | 50/500 [00:52<07:19,  1.02it/s]

 10%|██████████████▎                                                                                                                           | 52/500 [00:55<08:51,  1.19s/it]
{'loss': 1.2612, 'grad_norm': 6.277507781982422, 'learning_rate': 2.336961377126001e-05, 'epoch': 0.0}

 11%|██████████████▉                                                                                                                           | 54/500 [00:57<07:57,  1.07s/it]
{'loss': 1.1002, 'grad_norm': 7.2385640144348145, 'learning_rate': 1.9216926233717085e-05, 'epoch': 0.0}

 11%|███████████████▍                                                                                                                          | 56/500 [00:59<07:37,  1.03s/it]
{'loss': 1.3943, 'grad_norm': 4.452342510223389, 'learning_rate': 1.5381391146968866e-05, 'epoch': 0.0}

 12%|████████████████                                                                                                                          | 58/500 [01:01<07:23,  1.00s/it]
{'loss': 1.1605, 'grad_norm': 4.673879623413086, 'learning_rate': 1.1902525336466464e-05, 'epoch': 0.0}

 12%|████████████████▌                                                                                                                         | 60/500 [01:03<07:15,  1.01it/s]
{'loss': 1.2216, 'grad_norm': 7.266345977783203, 'learning_rate': 8.816170928508365e-06, 'epoch': 0.0}

 12%|█████████████████                                                                                                                         | 62/500 [01:05<08:10,  1.12s/it]
{'loss': 1.3183, 'grad_norm': 6.289580345153809, 'learning_rate': 6.154126075284855e-06, 'epoch': 0.0}

 13%|█████████████████▋                                                                                                                        | 64/500 [01:07<07:36,  1.05s/it]
{'loss': 1.3596, 'grad_norm': 5.643648624420166, 'learning_rate': 3.9438173442575e-06, 'epoch': 0.0}

 13%|██████████████████▏                                                                                                                       | 66/500 [01:09<07:25,  1.03s/it]
{'loss': 1.1829, 'grad_norm': 5.860846996307373, 'learning_rate': 2.208017147186736e-06, 'epoch': 0.0}

 14%|██████████████████▊                                                                                                                       | 68/500 [01:11<07:14,  1.01s/it]
{'loss': 1.3287, 'grad_norm': 5.246278285980225, 'learning_rate': 9.646091200853802e-07, 'epoch': 0.0}

 14%|███████████████████▎                                                                                                                      | 70/500 [01:13<06:59,  1.02it/s]

 14%|███████████████████▌                                                                                                                      | 71/500 [01:15<08:13,  1.15s/it]
{'loss': 1.1884, 'grad_norm': 6.607775688171387, 'learning_rate': 4.933487177280482e-08, 'epoch': 0.0}
{'loss': 1.4036, 'grad_norm': 5.635900020599365, 'learning_rate': 1.0069963546743832e-09, 'epoch': 0.0}

 15%|████████████████████▏                                                                                                                     | 73/500 [01:17<07:28,  1.05s/it]
{'loss': 0.8184, 'grad_norm': 9.598010063171387, 'learning_rate': 2.9074071841726503e-07, 'epoch': 0.0}

 15%|████████████████████▋                                                                                                                     | 75/500 [01:19<07:07,  1.00s/it]
{'loss': 1.3468, 'grad_norm': 6.711480140686035, 'learning_rate': 1.0926199633097157e-06, 'epoch': 0.0}

 16%|█████████████████████▌                                                                                                                    | 78/500 [01:22<06:50,  1.03it/s]
{'loss': 1.196, 'grad_norm': 8.350147247314453, 'learning_rate': 2.3983831139599287e-06, 'epoch': 0.0}

 16%|█████████████████████▊                                                                                                                    | 79/500 [01:23<06:56,  1.01it/s]

 16%|██████████████████████▎                                                                                                                   | 81/500 [01:25<08:21,  1.20s/it]
{'loss': 1.545, 'grad_norm': 6.030534744262695, 'learning_rate': 5.27099513780232e-06, 'epoch': 0.0}

 17%|██████████████████████▉                                                                                                                   | 83/500 [01:27<07:30,  1.08s/it]
{'loss': 1.3218, 'grad_norm': 7.974303245544434, 'learning_rate': 7.766608697888089e-06, 'epoch': 0.0}

 17%|███████████████████████▍                                                                                                                  | 85/500 [01:29<07:08,  1.03s/it]
{'loss': 1.051, 'grad_norm': 4.90064001083374, 'learning_rate': 1.069734526286063e-05, 'epoch': 0.0}

 17%|████████████████████████                                                                                                                  | 87/500 [01:31<07:12,  1.05s/it]
{'loss': 1.2052, 'grad_norm': 5.719120979309082, 'learning_rate': 1.4033009983067447e-05, 'epoch': 0.0}

 18%|████████████████████████▌                                                                                                                 | 89/500 [01:33<07:02,  1.03s/it]
{'loss': 1.0919, 'grad_norm': 6.866873741149902, 'learning_rate': 1.7739236107186852e-05, 'epoch': 0.0}

 18%|████████████████████████▊                                                                                                                 | 90/500 [01:34<07:06,  1.04s/it]

 18%|█████████████████████████▍                                                                                                                | 92/500 [01:37<07:58,  1.17s/it]
{'loss': 1.3582, 'grad_norm': 6.137578964233398, 'learning_rate': 2.3908901295937703e-05, 'epoch': 0.0}

 19%|█████████████████████████▉                                                                                                                | 94/500 [01:39<07:31,  1.11s/it]
{'loss': 1.4152, 'grad_norm': 6.338191032409668, 'learning_rate': 2.8367098827674588e-05, 'epoch': 0.0}

 19%|██████████████████████████▍                                                                                                               | 96/500 [01:41<07:04,  1.05s/it]
{'loss': 0.8159, 'grad_norm': 6.15369987487793, 'learning_rate': 3.304817623413397e-05, 'epoch': 0.0}

 20%|███████████████████████████                                                                                                               | 98/500 [01:43<06:55,  1.03s/it]

 20%|███████████████████████████▎                                                                                                              | 99/500 [01:45<08:00,  1.20s/it]
{'loss': 1.2062, 'grad_norm': 5.619089603424072, 'learning_rate': 4.0381686698436956e-05, 'epoch': 0.0}

 20%|███████████████████████████▍                                                                                                             | 100/500 [01:46<08:21,  1.25s/it]

 20%|███████████████████████████▉                                                                                                             | 102/500 [01:49<09:11,  1.39s/it]
{'loss': 0.7882, 'grad_norm': 5.5313262939453125, 'learning_rate': 4.793792315676663e-05, 'epoch': 0.0}

 21%|████████████████████████████▏                                                                                                            | 103/500 [01:51<09:03,  1.37s/it]

 21%|████████████████████████████▊                                                                                                            | 105/500 [01:53<08:47,  1.33s/it]
{'loss': 1.2291, 'grad_norm': 4.285276412963867, 'learning_rate': 5.554190999505055e-05, 'epoch': 0.0}


 21%|█████████████████████████████▎                                                                                                           | 107/500 [01:57<11:05,  1.69s/it]

 22%|█████████████████████████████▌                                                                                                           | 108/500 [01:59<11:45,  1.80s/it]
{'loss': 0.7429, 'grad_norm': 4.733377456665039, 'learning_rate': 6.301756586991559e-05, 'epoch': 0.0}

 22%|██████████████████████████████▏                                                                                                          | 110/500 [02:01<09:20,  1.44s/it]
{'loss': 0.9682, 'grad_norm': 6.763462066650391, 'learning_rate': 6.784431107959359e-05, 'epoch': 0.0}

 22%|██████████████████████████████▋                                                                                                          | 112/500 [02:05<10:57,  1.69s/it]

 23%|██████████████████████████████▉                                                                                                          | 113/500 [02:07<10:53,  1.69s/it]
{'loss': 1.6827, 'grad_norm': 6.599861145019531, 'learning_rate': 7.472468026127384e-05, 'epoch': 0.0}

 23%|███████████████████████████████▏                                                                                                         | 114/500 [02:09<10:42,  1.66s/it]

 23%|███████████████████████████████▊                                                                                                         | 116/500 [02:11<09:30,  1.49s/it]
{'loss': 1.2777, 'grad_norm': 5.256059169769287, 'learning_rate': 8.103251361307119e-05, 'epoch': 0.0}

 23%|████████████████████████████████                                                                                                         | 117/500 [02:13<09:45,  1.53s/it]

 24%|████████████████████████████████▌                                                                                                        | 119/500 [02:15<08:28,  1.33s/it]
{'loss': 1.1103, 'grad_norm': 6.805161952972412, 'learning_rate': 8.662174410541555e-05, 'epoch': 0.0}

 24%|████████████████████████████████▉                                                                                                        | 120/500 [02:16<08:21,  1.32s/it]

 24%|█████████████████████████████████▍                                                                                                       | 122/500 [02:19<08:24,  1.34s/it]
 24%|█████████████████████████████████▍                                                                                                       | 122/500 [02:19<08:24,  1.34s/it]Traceback (most recent call last):
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 243, in <module>
    main()
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 239, in main
    trainer.train()
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\trl\trainer\sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 1948, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\utils\operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\utils\operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\amp\autocast_mode.py", line 43, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\peft\peft_model.py", line 1577, in forward
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\peft\tuners\tuners_utils.py", line 188, in forward
    return self.model.forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 1189, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 1001, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 731, in forward
    hidden_states = self.input_layernorm(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 125, in forward
    return self.weight * hidden_states.to(input_dtype)
           ^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1716, in __getattr__
    def __getattr__(self, name: str) -> Any:
KeyboardInterrupt