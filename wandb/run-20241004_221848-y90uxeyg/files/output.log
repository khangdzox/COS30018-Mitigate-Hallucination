

  0%|▎                                                                                                                                | 1/500 [00:05<44:06,  5.30s/it]
{'loss': 1.9595, 'grad_norm': 1.864973545074463, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.0}


  1%|▊                                                                                                                                | 3/500 [00:14<40:38,  4.91s/it]

  1%|█                                                                                                                                | 4/500 [00:19<40:02,  4.84s/it]
{'loss': 2.1384, 'grad_norm': 2.3749091625213623, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.0}


  1%|█▌                                                                                                                               | 6/500 [00:29<39:18,  4.78s/it]

  1%|█▊                                                                                                                               | 7/500 [00:33<39:01,  4.75s/it]
{'loss': 2.289, 'grad_norm': 2.4372804164886475, 'learning_rate': 8.99990937032808e-06, 'epoch': 0.0}


  2%|██▎                                                                                                                              | 9/500 [00:43<38:57,  4.76s/it]
{'loss': 2.4579, 'grad_norm': 2.7335386276245117, 'learning_rate': 8.999184354855867e-06, 'epoch': 0.0}

  2%|██▌                                                                                                                             | 10/500 [00:48<39:39,  4.86s/it]

  2%|██▊                                                                                                                             | 11/500 [00:54<41:58,  5.15s/it]


  3%|███▎                                                                                                                            | 13/500 [01:03<39:26,  4.86s/it]
{'loss': 2.4276, 'grad_norm': 2.6903767585754395, 'learning_rate': 8.995559861540447e-06, 'epoch': 0.0}

  3%|███▌                                                                                                                            | 14/500 [01:07<38:39,  4.77s/it]


  3%|████                                                                                                                            | 16/500 [01:17<37:46,  4.68s/it]

  3%|████▎                                                                                                                           | 17/500 [01:21<37:24,  4.65s/it]
{'loss': 2.3318, 'grad_norm': 2.930285692214966, 'learning_rate': 8.989038226169209e-06, 'epoch': 0.0}

  4%|████▌                                                                                                                           | 18/500 [01:26<37:08,  4.62s/it]


  4%|█████                                                                                                                           | 20/500 [01:35<36:48,  4.60s/it]
  4%|█████                                                                                                                           | 20/500 [01:35<36:48,  4.60s/it]Traceback (most recent call last):
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 244, in <module>
    main()
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 240, in main
    trainer.train()
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\trl\trainer\sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 1948, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2366, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2817, in _maybe_log_save_evaluate
    self._save_checkpoint(model, trial, metrics=metrics)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2896, in _save_checkpoint
    self.save_model(output_dir, _internal_call=True)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3464, in save_model
    self._save(output_dir)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3540, in _save
    self.tokenizer.save_pretrained(output_dir)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\tokenization_utils_base.py", line 2697, in save_pretrained
    save_files = self._save_pretrained(
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\tokenization_utils_fast.py", line 708, in _save_pretrained
    self.backend_tokenizer.save(tokenizer_file)
KeyboardInterrupt