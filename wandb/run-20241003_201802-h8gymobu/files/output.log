
  0%|                                                                                                                                                  | 0/110 [00:00<?, ?it/s]


  2%|██▍                                                                                                                                     | 2/110 [02:29<2:13:57, 74.42s/it]
{'loss': 2.0823, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 0.0}


  4%|████▉                                                                                                                                   | 4/110 [04:18<1:47:01, 60.58s/it]
{'loss': 2.1626, 'grad_norm': 3.0688271522521973, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}

  5%|██████▏                                                                                                                                 | 5/110 [05:05<1:37:34, 55.76s/it]

  5%|███████▍                                                                                                                                | 6/110 [05:53<1:32:11, 53.19s/it]


  7%|█████████▉                                                                                                                              | 8/110 [07:34<1:27:33, 51.51s/it]
{'loss': 2.0732, 'grad_norm': 4.393585205078125, 'learning_rate': 5e-06, 'epoch': 0.0}

  8%|███████████▏                                                                                                                            | 9/110 [08:20<1:24:14, 50.04s/it]

  9%|████████████▎                                                                                                                          | 10/110 [09:07<1:21:24, 48.85s/it]

 10%|█████████████▌                                                                                                                         | 11/110 [09:53<1:19:33, 48.21s/it]

 11%|██████████████▋                                                                                                                        | 12/110 [10:28<1:12:16, 44.25s/it]

 12%|███████████████▉                                                                                                                       | 13/110 [10:59<1:04:58, 40.19s/it]

 13%|█████████████████▍                                                                                                                       | 14/110 [11:30<59:50, 37.40s/it]

 14%|██████████████████▋                                                                                                                      | 15/110 [12:04<57:30, 36.32s/it]

 15%|███████████████████▉                                                                                                                     | 16/110 [12:38<55:59, 35.74s/it]

 15%|████████████████████▊                                                                                                                  | 17/110 [13:31<1:03:08, 40.74s/it]

 16%|██████████████████████                                                                                                                 | 18/110 [14:06<1:00:01, 39.15s/it]

 17%|███████████████████████▎                                                                                                               | 19/110 [14:47<1:00:01, 39.58s/it]Traceback (most recent call last):
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_LoRA.py", line 213, in <module>
    main()
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_LoRA.py", line 209, in main
    trainer.train()
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\trl\trainer\sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 1948, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3359, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\accelerator.py", line 2155, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\autograd\__init__.py", line 289, in backward
    _engine_run_backward(
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\autograd\graph.py", line 768, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt