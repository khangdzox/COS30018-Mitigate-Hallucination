
  0%|                                                                                                                                                   | 0/500 [00:00<?, ?it/s]

  0%|▎                                                                                                                                        | 1/500 [00:08<1:08:13,  8.20s/it]

  0%|▌                                                                                                                                        | 2/500 [00:16<1:06:54,  8.06s/it]

  1%|▊                                                                                                                                        | 3/500 [00:24<1:06:13,  7.99s/it]

  1%|█                                                                                                                                        | 4/500 [00:31<1:05:46,  7.96s/it]

  1%|█▎                                                                                                                                       | 5/500 [00:39<1:05:30,  7.94s/it]


  1%|█▉                                                                                                                                       | 7/500 [00:55<1:05:16,  7.94s/it]
{'loss': 1.652, 'grad_norm': nan, 'learning_rate': 0.0016, 'epoch': 0.0}

  2%|██▏                                                                                                                                      | 8/500 [01:03<1:04:53,  7.91s/it]

  2%|██▍                                                                                                                                      | 9/500 [01:11<1:04:46,  7.91s/it]

  2%|██▋                                                                                                                                     | 10/500 [01:19<1:04:33,  7.90s/it]

  2%|██▉                                                                                                                                     | 11/500 [01:27<1:05:43,  8.06s/it]

  2%|███▎                                                                                                                                    | 12/500 [01:35<1:05:13,  8.02s/it]

  3%|███▌                                                                                                                                    | 13/500 [01:44<1:05:38,  8.09s/it]


  3%|████                                                                                                                                    | 15/500 [01:59<1:04:42,  8.00s/it]
{'loss': 1.844, 'grad_norm': 74.15067291259766, 'learning_rate': 0.0044, 'epoch': 0.0}

  3%|████▎                                                                                                                                   | 16/500 [02:07<1:04:20,  7.98s/it]

  3%|████▌                                                                                                                                   | 17/500 [02:15<1:04:05,  7.96s/it]

  4%|████▉                                                                                                                                   | 18/500 [02:23<1:03:22,  7.89s/it]

  4%|█████▏                                                                                                                                  | 19/500 [02:31<1:03:26,  7.91s/it]Traceback (most recent call last):
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 243, in <module>
    main()
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 239, in main
    trainer.train()
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\trl\trainer\sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 1948, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3359, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\accelerator.py", line 2155, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\autograd\__init__.py", line 289, in backward
    _engine_run_backward(
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\autograd\graph.py", line 768, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt