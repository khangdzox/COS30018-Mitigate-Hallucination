

  0%|▎                                                                                                                                       | 1/500 [00:21<3:01:13, 21.79s/it]

  0%|▌                                                                                                                                       | 2/500 [00:31<2:00:10, 14.48s/it]
{'loss': 2.1197, 'grad_norm': 2.610870599746704, 'learning_rate': 4e-05, 'epoch': 0.0}

  1%|▊                                                                                                                                       | 3/500 [00:40<1:39:33, 12.02s/it]

  1%|█                                                                                                                                       | 4/500 [00:48<1:27:15, 10.56s/it]


  1%|█▋                                                                                                                                      | 6/500 [01:03<1:12:49,  8.84s/it]

  1%|█▉                                                                                                                                      | 7/500 [01:11<1:09:34,  8.47s/it]

  2%|██▏                                                                                                                                     | 8/500 [01:19<1:08:51,  8.40s/it]
{'loss': 2.3268, 'grad_norm': 3.9526755809783936, 'learning_rate': 0.0002, 'epoch': 0.0}

  2%|██▍                                                                                                                                     | 9/500 [01:27<1:08:26,  8.36s/it]


  2%|██▉                                                                                                                                    | 11/500 [01:45<1:10:13,  8.62s/it]
{'loss': 1.9085, 'grad_norm': 3.74198579788208, 'learning_rate': 0.0001999818745523526, 'epoch': 0.0}

  2%|███▏                                                                                                                                   | 12/500 [01:54<1:10:17,  8.64s/it]


  3%|███▊                                                                                                                                   | 14/500 [02:09<1:05:00,  8.03s/it]
{'loss': 1.8486, 'grad_norm': 5.5250244140625, 'learning_rate': 0.00019992750478004738, 'epoch': 0.0}

  3%|████                                                                                                                                   | 15/500 [02:16<1:01:40,  7.63s/it]

  3%|████▍                                                                                                                                    | 16/500 [02:22<58:54,  7.30s/it]

  3%|████▋                                                                                                                                    | 17/500 [02:29<57:25,  7.13s/it]

  4%|████▉                                                                                                                                    | 18/500 [02:36<56:24,  7.02s/it]

  4%|█████▏                                                                                                                                   | 19/500 [02:43<55:54,  6.97s/it]
  4%|█████▍                                                                                                                                   | 20/500 [02:50<55:57,  6.99s/it]Traceback (most recent call last):
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 246, in <module>
    main()
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 242, in main
    trainer.train()
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\trl\trainer\sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 1948, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2366, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2817, in _maybe_log_save_evaluate
    self._save_checkpoint(model, trial, metrics=metrics)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2896, in _save_checkpoint
    self.save_model(output_dir, _internal_call=True)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3464, in save_model
    self._save(output_dir)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3540, in _save
    self.tokenizer.save_pretrained(output_dir)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\tokenization_utils_base.py", line 2697, in save_pretrained
    save_files = self._save_pretrained(
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\tokenization_utils_fast.py", line 708, in _save_pretrained
    self.backend_tokenizer.save(tokenizer_file)
Exception: The requested operation cannot be performed on a file with a user-mapped section open. (os error 1224)