
  0%|                                                                                                                                                   | 0/500 [00:00<?, ?it/s]


  0%|▌                                                                                                                                        | 2/500 [00:15<1:04:55,  7.82s/it]
{'loss': 2.2935, 'grad_norm': 4.083846569061279, 'learning_rate': 4e-05, 'epoch': 0.0}

  1%|▊                                                                                                                                        | 3/500 [00:23<1:04:10,  7.75s/it]

  1%|█                                                                                                                                        | 4/500 [00:30<1:03:39,  7.70s/it]

  1%|█▎                                                                                                                                       | 5/500 [00:38<1:03:37,  7.71s/it]

  1%|█▋                                                                                                                                       | 6/500 [00:46<1:03:34,  7.72s/it]


  2%|██▏                                                                                                                                      | 8/500 [01:01<1:03:06,  7.70s/it]

  2%|██▍                                                                                                                                      | 9/500 [01:09<1:02:57,  7.69s/it]
{'loss': 1.9843, 'grad_norm': 4.099848747253418, 'learning_rate': 0.00032, 'epoch': 0.0}

  2%|██▋                                                                                                                                     | 10/500 [01:17<1:02:46,  7.69s/it]

  2%|██▉                                                                                                                                     | 11/500 [01:26<1:06:15,  8.13s/it]


  3%|███▌                                                                                                                                    | 13/500 [01:41<1:04:19,  7.93s/it]
{'loss': 1.5839, 'grad_norm': 4.995751857757568, 'learning_rate': 0.0004, 'epoch': 0.0}

  3%|███▊                                                                                                                                    | 14/500 [01:49<1:03:25,  7.83s/it]

  3%|████                                                                                                                                    | 15/500 [01:57<1:02:54,  7.78s/it]

  3%|████▎                                                                                                                                   | 16/500 [02:04<1:02:21,  7.73s/it]


  4%|████▉                                                                                                                                   | 18/500 [02:19<1:01:33,  7.66s/it]

  4%|█████▏                                                                                                                                  | 19/500 [02:27<1:01:38,  7.69s/it]
{'loss': 1.3611, 'grad_norm': nan, 'learning_rate': 0.0006, 'epoch': 0.0}


  4%|█████▋                                                                                                                                  | 21/500 [02:43<1:03:20,  7.93s/it]
{'loss': 1.3361, 'grad_norm': 53.532188415527344, 'learning_rate': 0.00064, 'epoch': 0.0}

  4%|█████▉                                                                                                                                  | 22/500 [02:51<1:02:21,  7.83s/it]

  5%|██████▎                                                                                                                                 | 23/500 [02:58<1:01:42,  7.76s/it]

  5%|██████▌                                                                                                                                 | 24/500 [03:06<1:01:14,  7.72s/it]


  5%|███████                                                                                                                                 | 26/500 [03:21<1:00:25,  7.65s/it]
{'loss': 1.1054, 'grad_norm': 1.7926496267318726, 'learning_rate': 0.00084, 'epoch': 0.0}

  5%|███████▎                                                                                                                                | 27/500 [03:29<1:00:01,  7.61s/it]

  6%|███████▋                                                                                                                                  | 28/500 [03:36<59:48,  7.60s/it]

  6%|████████                                                                                                                                  | 29/500 [03:44<59:41,  7.60s/it]

  6%|████████▎                                                                                                                                 | 30/500 [03:52<59:38,  7.61s/it]


  6%|████████▋                                                                                                                               | 32/500 [04:07<1:00:24,  7.74s/it]
{'loss': 1.0444, 'grad_norm': 1.4853276014328003, 'learning_rate': 0.000999956257238817, 'epoch': 0.01}

  7%|█████████                                                                                                                                 | 33/500 [04:15<59:54,  7.70s/it]

  7%|█████████▍                                                                                                                                | 34/500 [04:23<59:48,  7.70s/it]

  7%|█████████▋                                                                                                                                | 35/500 [04:30<59:37,  7.69s/it]

  7%|█████████▉                                                                                                                                | 36/500 [04:38<59:23,  7.68s/it]


  8%|██████████▍                                                                                                                               | 38/500 [04:53<58:47,  7.63s/it]
{'loss': 1.0318, 'grad_norm': 2.6709399223327637, 'learning_rate': 0.0009993002688846913, 'epoch': 0.01}

  8%|██████████▊                                                                                                                               | 39/500 [05:01<58:43,  7.64s/it]


  8%|███████████▏                                                                                                                            | 41/500 [05:17<1:00:57,  7.97s/it]
{'loss': 1.095, 'grad_norm': 1.7357611656188965, 'learning_rate': 0.000998677345729831, 'epoch': 0.01}

  8%|███████████▍                                                                                                                            | 42/500 [05:25<1:00:09,  7.88s/it]

  9%|███████████▊                                                                                                                              | 43/500 [05:33<59:16,  7.78s/it]

  9%|████████████▏                                                                                                                             | 44/500 [05:40<58:50,  7.74s/it]

  9%|████████████▍                                                                                                                             | 45/500 [05:48<58:43,  7.74s/it]


  9%|████████████▉                                                                                                                             | 47/500 [06:03<58:26,  7.74s/it]
{'loss': 1.0905, 'grad_norm': 1.4927111864089966, 'learning_rate': 0.0009968428675226715, 'epoch': 0.01}

 10%|█████████████▏                                                                                                                            | 48/500 [06:11<58:07,  7.71s/it]

 10%|█████████████▌                                                                                                                            | 49/500 [06:19<57:48,  7.69s/it]

 10%|█████████████▊                                                                                                                            | 50/500 [06:26<57:29,  7.67s/it]

 10%|██████████████                                                                                                                            | 51/500 [06:35<58:46,  7.85s/it]

 10%|██████████████▎                                                                                                                           | 52/500 [06:42<58:12,  7.79s/it]


 11%|██████████████▉                                                                                                                           | 54/500 [06:58<57:20,  7.71s/it]
{'loss': 0.9291, 'grad_norm': 1.5150513648986816, 'learning_rate': 0.0009937141654477529, 'epoch': 0.01}

 11%|███████████████▏                                                                                                                          | 55/500 [07:05<56:58,  7.68s/it]

 11%|███████████████▍                                                                                                                          | 56/500 [07:13<56:33,  7.64s/it]

 11%|███████████████▋                                                                                                                          | 57/500 [07:20<56:19,  7.63s/it]


 12%|████████████████▎                                                                                                                         | 59/500 [07:35<55:46,  7.59s/it]
{'loss': 0.9678, 'grad_norm': 1.4314366579055786, 'learning_rate': 0.0009908311110469882, 'epoch': 0.01}

 12%|████████████████▌                                                                                                                         | 60/500 [07:43<55:52,  7.62s/it]Traceback (most recent call last):
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 243, in <module>
    main()
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 239, in main
    trainer.train()
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\trl\trainer\sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 1948, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\utils\operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\utils\operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\amp\autocast_mode.py", line 43, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\peft\peft_model.py", line 1577, in forward
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\peft\tuners\tuners_utils.py", line 188, in forward
    return self.model.forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 1189, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 1001, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 734, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 619, in forward
    value_states = self.v_proj(hidden_states)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\peft\tuners\lora\bnb.py", line 467, in forward
    result = self.base_layer(x, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\bitsandbytes\nn\modules.py", line 477, in forward
    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\bitsandbytes\autograd\_functions.py", line 579, in matmul_4bit
    return MatMul4Bit.apply(A, B, out, bias, quant_state)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\autograd\function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\bitsandbytes\autograd\_functions.py", line 509, in forward
    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\bitsandbytes\functional.py", line 1353, in dequantize_4bit
    out = torch.empty(quant_state.shape, dtype=quant_state.dtype, device=A.device)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt