

  0%|▎                                                                                                                                        | 1/500 [00:08<1:11:02,  8.54s/it]

  0%|▌                                                                                                                                        | 2/500 [00:16<1:08:14,  8.22s/it]

  1%|▊                                                                                                                                        | 3/500 [00:24<1:05:41,  7.93s/it]

  1%|█                                                                                                                                        | 4/500 [00:32<1:05:46,  7.96s/it]
{'loss': 2.2637, 'grad_norm': 3.7795403003692627, 'learning_rate': 6e-05, 'epoch': 0.0}

  1%|█▎                                                                                                                                       | 5/500 [00:40<1:05:24,  7.93s/it]


  1%|█▉                                                                                                                                       | 7/500 [00:55<1:04:35,  7.86s/it]

  2%|██▏                                                                                                                                      | 8/500 [01:03<1:04:19,  7.84s/it]

  2%|██▍                                                                                                                                      | 9/500 [01:11<1:04:00,  7.82s/it]

  2%|██▋                                                                                                                                     | 10/500 [01:19<1:03:47,  7.81s/it]

  2%|██▉                                                                                                                                     | 11/500 [01:27<1:05:32,  8.04s/it]

  2%|███▎                                                                                                                                    | 12/500 [01:35<1:04:57,  7.99s/it]
{'loss': 1.9294, 'grad_norm': 3.6217100620269775, 'learning_rate': 9.769776049884563e-05, 'epoch': 0.0}


  3%|███▊                                                                                                                                    | 14/500 [01:53<1:09:01,  8.52s/it]
{'loss': 1.8375, 'grad_norm': 3.9768521785736084, 'learning_rate': 9.593172782532268e-05, 'epoch': 0.0}


  3%|████▎                                                                                                                                   | 16/500 [02:10<1:08:52,  8.54s/it]

  3%|████▌                                                                                                                                   | 17/500 [02:19<1:08:53,  8.56s/it]

  4%|████▉                                                                                                                                   | 18/500 [02:27<1:08:39,  8.55s/it]

  4%|█████▏                                                                                                                                  | 19/500 [02:36<1:09:08,  8.62s/it]

  4%|█████▍                                                                                                                                  | 20/500 [02:44<1:08:24,  8.55s/it]

  4%|█████▋                                                                                                                                  | 21/500 [02:53<1:09:36,  8.72s/it]
{'loss': 1.3852, 'grad_norm': 4.715266704559326, 'learning_rate': 8.618670190525352e-05, 'epoch': 0.0}


  5%|██████▎                                                                                                                                 | 23/500 [03:11<1:10:44,  8.90s/it]

  5%|██████▌                                                                                                                                 | 24/500 [03:21<1:11:55,  9.07s/it]

  5%|██████▊                                                                                                                                 | 25/500 [03:46<1:49:53, 13.88s/it]

  5%|███████                                                                                                                                 | 26/500 [04:01<1:52:35, 14.25s/it]

  5%|███████▎                                                                                                                                | 27/500 [04:29<2:25:03, 18.40s/it]

  6%|███████▌                                                                                                                                | 28/500 [04:50<2:31:09, 19.22s/it]

  6%|███████▉                                                                                                                                | 29/500 [05:13<2:39:37, 20.33s/it]

  6%|████████▏                                                                                                                               | 30/500 [05:25<2:19:22, 17.79s/it]

  6%|████████▍                                                                                                                               | 31/500 [05:48<2:32:30, 19.51s/it]

  6%|████████▋                                                                                                                               | 32/500 [06:15<2:48:33, 21.61s/it]

  7%|████████▉                                                                                                                               | 33/500 [06:36<2:47:10, 21.48s/it]

  7%|█████████▏                                                                                                                              | 34/500 [06:52<2:33:00, 19.70s/it]
{'loss': 1.1585, 'grad_norm': nan, 'learning_rate': 5.992952333228728e-05, 'epoch': 0.0}


  7%|█████████▊                                                                                                                              | 36/500 [07:39<2:44:55, 21.33s/it]

  7%|██████████                                                                                                                              | 37/500 [07:58<2:39:29, 20.67s/it]

  8%|██████████▎                                                                                                                             | 38/500 [08:19<2:38:49, 20.63s/it]

  8%|██████████▌                                                                                                                             | 39/500 [08:42<2:43:49, 21.32s/it]

  8%|██████████▉                                                                                                                             | 40/500 [09:11<3:02:01, 23.74s/it]
  8%|██████████▉                                                                                                                             | 40/500 [09:11<3:02:01, 23.74s/it]Traceback (most recent call last):
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 243, in <module>
    main()
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 239, in main
    trainer.train()
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\trl\trainer\sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 1948, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3359, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\accelerator.py", line 2155, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\autograd\__init__.py", line 289, in backward
    _engine_run_backward(
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\autograd\graph.py", line 768, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt