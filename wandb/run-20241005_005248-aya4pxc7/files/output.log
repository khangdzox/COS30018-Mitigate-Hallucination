

  0%|▎                                                                                                                             | 1/500 [00:04<36:32,  4.39s/it]

  0%|▌                                                                                                                             | 2/500 [00:08<34:28,  4.15s/it]

  1%|▊                                                                                                                             | 3/500 [00:12<33:36,  4.06s/it]

  1%|█                                                                                                                             | 4/500 [00:16<33:06,  4.01s/it]

  1%|█▎                                                                                                                            | 5/500 [00:20<32:52,  3.99s/it]

  1%|█▌                                                                                                                            | 6/500 [00:24<32:41,  3.97s/it]

  1%|█▊                                                                                                                            | 7/500 [00:28<32:32,  3.96s/it]

  2%|██                                                                                                                            | 8/500 [00:32<32:23,  3.95s/it]

  2%|██▎                                                                                                                           | 9/500 [00:35<32:15,  3.94s/it]
{'loss': 1.4175, 'grad_norm': 2.6003971099853516, 'learning_rate': 0.0003809654104932039, 'epoch': 0.0}


  2%|██▊                                                                                                                          | 11/500 [00:44<33:19,  4.09s/it]

  2%|███                                                                                                                          | 12/500 [00:48<32:35,  4.01s/it]
{'loss': 1.3761, 'grad_norm': 1.9245786666870117, 'learning_rate': 0.0003618033988749895, 'epoch': 0.0}


  3%|███▌                                                                                                                         | 14/500 [00:55<31:39,  3.91s/it]

  3%|███▊                                                                                                                         | 15/500 [00:59<31:20,  3.88s/it]

  3%|████                                                                                                                         | 16/500 [01:03<31:06,  3.86s/it]

  3%|████▎                                                                                                                        | 17/500 [01:07<30:55,  3.84s/it]

  4%|████▌                                                                                                                        | 18/500 [01:10<30:46,  3.83s/it]

  4%|████▊                                                                                                                        | 19/500 [01:14<30:39,  3.82s/it]

  4%|█████                                                                                                                        | 20/500 [01:19<32:37,  4.08s/it]

  4%|█████▎                                                                                                                       | 21/500 [01:24<34:14,  4.29s/it]

  4%|█████▌                                                                                                                       | 22/500 [01:28<33:11,  4.17s/it]

  5%|█████▊                                                                                                                       | 23/500 [01:32<32:33,  4.10s/it]
{'loss': 1.3026, 'grad_norm': 0.8957276344299316, 'learning_rate': 0.0002497379774329709, 'epoch': 0.0}


  5%|██████▎                                                                                                                      | 25/500 [01:39<31:42,  4.01s/it]

  5%|██████▌                                                                                                                      | 26/500 [01:43<31:25,  3.98s/it]

  5%|██████▊                                                                                                                      | 27/500 [01:47<31:04,  3.94s/it]

  6%|███████                                                                                                                      | 28/500 [01:51<30:47,  3.91s/it]

  6%|███████▎                                                                                                                     | 29/500 [01:55<30:36,  3.90s/it]

  6%|███████▌                                                                                                                     | 30/500 [01:59<30:30,  3.89s/it]
{'loss': 0.9285, 'grad_norm': 0.9185276627540588, 'learning_rate': 0.00016252373708285504, 'epoch': 0.0}


  6%|████████                                                                                                                     | 32/500 [02:07<31:36,  4.05s/it]

  7%|████████▎                                                                                                                    | 33/500 [02:11<31:09,  4.00s/it]

  7%|████████▌                                                                                                                    | 34/500 [02:15<30:51,  3.97s/it]

  7%|████████▊                                                                                                                    | 35/500 [02:19<31:25,  4.06s/it]

  7%|█████████                                                                                                                    | 36/500 [02:24<31:44,  4.10s/it]

  7%|█████████▎                                                                                                                   | 37/500 [02:28<31:53,  4.13s/it]

  8%|█████████▌                                                                                                                   | 38/500 [02:32<31:39,  4.11s/it]

  8%|█████████▊                                                                                                                   | 39/500 [02:36<30:57,  4.03s/it]
{'loss': 0.8627, 'grad_norm': 1.123545527458191, 'learning_rate': 6.309057881426226e-05, 'epoch': 0.0}


  8%|██████████▎                                                                                                                  | 41/500 [02:44<31:31,  4.12s/it]

  8%|██████████▌                                                                                                                  | 42/500 [02:48<30:37,  4.01s/it]

  9%|██████████▊                                                                                                                  | 43/500 [02:51<29:59,  3.94s/it]

  9%|███████████                                                                                                                  | 44/500 [02:55<29:53,  3.93s/it]

  9%|███████████▎                                                                                                                 | 45/500 [03:01<33:06,  4.37s/it]

  9%|███████████▌                                                                                                                 | 46/500 [03:05<32:46,  4.33s/it]

  9%|███████████▊                                                                                                                 | 47/500 [03:09<31:26,  4.16s/it]

 10%|████████████                                                                                                                 | 48/500 [03:13<30:30,  4.05s/it]

 10%|████████████▎                                                                                                                | 49/500 [03:16<29:47,  3.96s/it]

 10%|████████████▌                                                                                                                | 50/500 [03:20<29:14,  3.90s/it]

 10%|████████████▊                                                                                                                | 51/500 [03:25<31:24,  4.20s/it]

 10%|█████████████                                                                                                                | 52/500 [03:29<31:11,  4.18s/it]

 11%|█████████████▎                                                                                                               | 53/500 [03:33<31:16,  4.20s/it]
{'loss': 1.2328, 'grad_norm': 0.9327777028083801, 'learning_rate': 0.00039960534568565434, 'epoch': 0.0}


 11%|█████████████▊                                                                                                               | 55/500 [03:42<31:12,  4.21s/it]

 11%|██████████████                                                                                                               | 56/500 [03:46<31:09,  4.21s/it]

 11%|██████████████▎                                                                                                              | 57/500 [03:50<31:00,  4.20s/it]

 12%|██████████████▌                                                                                                              | 58/500 [03:54<30:48,  4.18s/it]

 12%|██████████████▊                                                                                                              | 59/500 [03:58<30:30,  4.15s/it]

 12%|███████████████                                                                                                              | 60/500 [04:03<30:27,  4.15s/it]

 12%|███████████████▎                                                                                                             | 61/500 [04:07<31:37,  4.32s/it]

 12%|███████████████▌                                                                                                             | 62/500 [04:13<35:02,  4.80s/it]

 13%|███████████████▊                                                                                                             | 63/500 [04:18<35:09,  4.83s/it]

 13%|████████████████                                                                                                             | 64/500 [04:23<35:40,  4.91s/it]

 13%|████████████████▎                                                                                                            | 65/500 [04:27<33:49,  4.67s/it]

 13%|████████████████▌                                                                                                            | 66/500 [04:31<32:17,  4.46s/it]

 13%|████████████████▊                                                                                                            | 67/500 [04:35<31:08,  4.32s/it]

 14%|█████████████████                                                                                                            | 68/500 [04:39<30:24,  4.22s/it]

 14%|█████████████████▎                                                                                                           | 69/500 [04:43<29:42,  4.14s/it]

 14%|█████████████████▌                                                                                                           | 70/500 [04:47<29:12,  4.08s/it]

 14%|█████████████████▊                                                                                                           | 71/500 [04:52<30:17,  4.24s/it]

 14%|██████████████████                                                                                                           | 72/500 [04:56<29:48,  4.18s/it]

 15%|██████████████████▎                                                                                                          | 73/500 [05:00<29:21,  4.12s/it]

 15%|██████████████████▌                                                                                                          | 74/500 [05:04<29:03,  4.09s/it]

 15%|██████████████████▊                                                                                                          | 75/500 [05:08<28:59,  4.09s/it]

 15%|███████████████████                                                                                                          | 76/500 [05:12<28:41,  4.06s/it]

 15%|███████████████████▎                                                                                                         | 77/500 [05:16<28:21,  4.02s/it]

 16%|███████████████████▌                                                                                                         | 78/500 [05:20<28:33,  4.06s/it]

 16%|███████████████████▊                                                                                                         | 79/500 [05:24<28:48,  4.11s/it]

 16%|████████████████████                                                                                                         | 80/500 [05:29<30:05,  4.30s/it]
 16%|████████████████████                                                                                                         | 80/500 [05:29<30:05,  4.30s/it]Traceback (most recent call last):
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 244, in <module>
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 240, in main
    if __name__ == "__main__":
        ^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\trl\trainer\sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 1948, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\utils\operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\utils\operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\amp\autocast_mode.py", line 43, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\peft\peft_model.py", line 1577, in forward
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\peft\tuners\tuners_utils.py", line 188, in forward
    return self.model.forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 1189, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 1001, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 734, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 672, in forward
    attn_output = self.o_proj(attn_output)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\bitsandbytes\nn\modules.py", line 477, in forward
    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\bitsandbytes\autograd\_functions.py", line 579, in matmul_4bit
    return MatMul4Bit.apply(A, B, out, bias, quant_state)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\autograd\function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt