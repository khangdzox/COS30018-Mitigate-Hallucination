

  0%|▎                                                                                                                                    | 1/500 [00:04<36:31,  4.39s/it]

  0%|▌                                                                                                                                    | 2/500 [00:08<34:06,  4.11s/it]

  1%|▊                                                                                                                                    | 3/500 [00:12<33:26,  4.04s/it]

  1%|█                                                                                                                                    | 4/500 [00:16<33:03,  4.00s/it]

  1%|█▎                                                                                                                                   | 5/500 [00:20<32:47,  3.97s/it]

  1%|█▌                                                                                                                                   | 6/500 [00:24<32:29,  3.95s/it]
{'loss': 2.1863, 'grad_norm': 1.7556465864181519, 'learning_rate': 6e-06, 'epoch': 0.0}


  2%|██▏                                                                                                                                  | 8/500 [00:31<32:08,  3.92s/it]

  2%|██▍                                                                                                                                  | 9/500 [00:35<32:05,  3.92s/it]

  2%|██▋                                                                                                                                 | 10/500 [00:39<32:10,  3.94s/it]

  2%|██▉                                                                                                                                 | 11/500 [00:44<35:16,  4.33s/it]

  2%|███▏                                                                                                                                | 12/500 [00:49<34:36,  4.25s/it]

  3%|███▍                                                                                                                                | 13/500 [00:53<34:02,  4.19s/it]

  3%|███▋                                                                                                                                | 14/500 [00:57<33:22,  4.12s/it]

  3%|███▉                                                                                                                                | 15/500 [01:00<32:45,  4.05s/it]

  3%|████▏                                                                                                                               | 16/500 [01:04<32:18,  4.01s/it]
  3%|████▏                                                                                                                               | 16/500 [01:04<32:18,  4.01s/it]Traceback (most recent call last):
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 242, in <module>
    main()
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 238, in main
    trainer.train()
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\trl\trainer\sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 1948, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3359, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\accelerator.py", line 2155, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\autograd\__init__.py", line 289, in backward
    _engine_run_backward(
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\autograd\graph.py", line 768, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt