

  0%|▎                                                                                                                                        | 1/500 [00:10<1:26:38, 10.42s/it]

  0%|▌                                                                                                                                        | 2/500 [00:20<1:24:48, 10.22s/it]

  1%|▊                                                                                                                                        | 3/500 [00:30<1:23:43, 10.11s/it]

  1%|█                                                                                                                                        | 4/500 [00:40<1:23:46, 10.13s/it]

  1%|█▎                                                                                                                                       | 5/500 [00:50<1:23:48, 10.16s/it]

  1%|█▋                                                                                                                                       | 6/500 [01:00<1:23:26, 10.14s/it]

  1%|█▉                                                                                                                                       | 7/500 [01:10<1:22:56, 10.09s/it]

  2%|██▏                                                                                                                                      | 8/500 [01:21<1:22:51, 10.11s/it]

  2%|██▍                                                                                                                                      | 9/500 [01:30<1:22:08, 10.04s/it]

  2%|██▋                                                                                                                                     | 10/500 [01:41<1:22:10, 10.06s/it]

  2%|██▉                                                                                                                                     | 11/500 [01:52<1:24:40, 10.39s/it]

  2%|███▎                                                                                                                                    | 12/500 [02:02<1:24:19, 10.37s/it]

  3%|███▌                                                                                                                                    | 13/500 [02:12<1:24:09, 10.37s/it]

  3%|███▊                                                                                                                                    | 14/500 [02:23<1:24:30, 10.43s/it]

  3%|████                                                                                                                                    | 15/500 [02:33<1:22:58, 10.27s/it]

  3%|████▎                                                                                                                                   | 16/500 [02:43<1:21:45, 10.13s/it]

  3%|████▌                                                                                                                                   | 17/500 [02:53<1:21:30, 10.13s/it]

  4%|████▉                                                                                                                                   | 18/500 [03:03<1:20:35, 10.03s/it]

  4%|█████▏                                                                                                                                  | 19/500 [03:12<1:19:53,  9.97s/it]

  4%|█████▍                                                                                                                                  | 20/500 [03:22<1:19:39,  9.96s/it]

  4%|█████▋                                                                                                                                  | 21/500 [03:33<1:22:04, 10.28s/it]

  4%|█████▉                                                                                                                                  | 22/500 [03:43<1:21:18, 10.21s/it]

  5%|██████▎                                                                                                                                 | 23/500 [03:53<1:20:41, 10.15s/it]

  5%|██████▌                                                                                                                                 | 24/500 [04:03<1:20:13, 10.11s/it]

  5%|██████▊                                                                                                                                 | 25/500 [04:13<1:19:45, 10.07s/it]

  5%|███████                                                                                                                                 | 26/500 [04:23<1:19:27, 10.06s/it]

  5%|███████▎                                                                                                                                | 27/500 [04:33<1:18:55, 10.01s/it]

  6%|███████▌                                                                                                                                | 28/500 [04:43<1:18:28,  9.98s/it]

  6%|███████▉                                                                                                                                | 29/500 [04:53<1:18:11,  9.96s/it]

  6%|████████▏                                                                                                                               | 30/500 [05:03<1:18:06,  9.97s/it]
{'loss': 1.7023, 'grad_norm': 6.106549263000488, 'learning_rate': 0.0001, 'epoch': 0.01}


  6%|████████▋                                                                                                                               | 32/500 [05:23<1:18:05, 10.01s/it]

  7%|████████▉                                                                                                                               | 33/500 [05:33<1:17:20,  9.94s/it]

  7%|█████████▏                                                                                                                              | 34/500 [05:43<1:16:33,  9.86s/it]

  7%|█████████▌                                                                                                                              | 35/500 [05:53<1:16:31,  9.87s/it]

  7%|█████████▊                                                                                                                              | 36/500 [06:03<1:16:54,  9.94s/it]

  7%|██████████                                                                                                                              | 37/500 [06:13<1:17:24, 10.03s/it]

  8%|██████████▎                                                                                                                             | 38/500 [06:23<1:17:32, 10.07s/it]

  8%|██████████▌                                                                                                                             | 39/500 [06:33<1:17:25, 10.08s/it]

  8%|██████████▉                                                                                                                             | 40/500 [06:43<1:17:14, 10.07s/it]

  8%|███████████▏                                                                                                                            | 41/500 [06:54<1:19:02, 10.33s/it]

  8%|███████████▍                                                                                                                            | 42/500 [07:04<1:18:08, 10.24s/it]

  9%|███████████▋                                                                                                                            | 43/500 [07:15<1:17:42, 10.20s/it]

  9%|███████████▉                                                                                                                            | 44/500 [07:24<1:16:47, 10.10s/it]

  9%|████████████▏                                                                                                                           | 45/500 [07:34<1:16:30, 10.09s/it]

  9%|████████████▌                                                                                                                           | 46/500 [07:45<1:16:17, 10.08s/it]

  9%|████████████▊                                                                                                                           | 47/500 [07:55<1:16:04, 10.08s/it]

 10%|█████████████                                                                                                                           | 48/500 [08:05<1:16:17, 10.13s/it]

 10%|█████████████▎                                                                                                                          | 49/500 [08:15<1:15:50, 10.09s/it]

 10%|█████████████▌                                                                                                                          | 50/500 [08:25<1:15:44, 10.10s/it]

 10%|█████████████▊                                                                                                                          | 51/500 [08:36<1:17:16, 10.33s/it]

 10%|██████████████▏                                                                                                                         | 52/500 [08:46<1:16:35, 10.26s/it]
{'loss': 1.0316, 'grad_norm': 2.0453460216522217, 'learning_rate': 9.94716380576598e-05, 'epoch': 0.01}

 11%|██████████████▍                                                                                                                         | 53/500 [08:56<1:16:03, 10.21s/it]


 11%|██████████████▉                                                                                                                         | 55/500 [09:16<1:14:31, 10.05s/it]

 11%|███████████████▏                                                                                                                        | 56/500 [09:26<1:14:05, 10.01s/it]
 11%|███████████████▏                                                                                                                        | 56/500 [09:26<1:14:05, 10.01s/it]Traceback (most recent call last):
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 243, in <module>
    main()
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 239, in main
    trainer.train()
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\trl\trainer\sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 1948, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2289, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3328, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3373, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\utils\operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\utils\operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\amp\autocast_mode.py", line 43, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\peft\peft_model.py", line 1577, in forward
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\peft\tuners\tuners_utils.py", line 188, in forward
    return self.model.forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 1189, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 1001, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 734, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\models\llama\modeling_llama.py", line 619, in forward
    value_states = self.v_proj(hidden_states)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\peft\tuners\lora\bnb.py", line 467, in forward
    result = self.base_layer(x, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\accelerate\hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\bitsandbytes\nn\modules.py", line 477, in forward
    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\bitsandbytes\autograd\_functions.py", line 579, in matmul_4bit
    return MatMul4Bit.apply(A, B, out, bias, quant_state)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\torch\autograd\function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\bitsandbytes\autograd\_functions.py", line 509, in forward
    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt