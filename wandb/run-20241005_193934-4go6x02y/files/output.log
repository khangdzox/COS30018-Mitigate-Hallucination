
  0%|                                                                                                                                                                           | 0/500 [00:00<?, ?it/s]

  0%|▎                                                                                                                                                                | 1/500 [00:08<1:08:13,  8.20s/it]

  0%|▋                                                                                                                                                                | 2/500 [00:16<1:06:24,  8.00s/it]

  1%|▉                                                                                                                                                                | 3/500 [00:24<1:06:09,  7.99s/it]


  1%|█▌                                                                                                                                                               | 5/500 [00:39<1:05:30,  7.94s/it]
{'loss': 2.3395, 'grad_norm': 4.791674613952637, 'learning_rate': 0.0016, 'epoch': 0.0}

  1%|█▉                                                                                                                                                               | 6/500 [00:47<1:05:02,  7.90s/it]

  1%|██▎                                                                                                                                                              | 7/500 [00:55<1:04:55,  7.90s/it]

  2%|██▌                                                                                                                                                              | 8/500 [01:03<1:04:43,  7.89s/it]

  2%|██▉                                                                                                                                                              | 9/500 [01:11<1:04:30,  7.88s/it]


  2%|███▌                                                                                                                                                            | 11/500 [01:27<1:06:00,  8.10s/it]
{'loss': 2.2273, 'grad_norm': 3.818269729614258, 'learning_rate': 0.004, 'epoch': 0.0}

  2%|███▊                                                                                                                                                            | 12/500 [01:35<1:05:06,  8.01s/it]

  3%|████▏                                                                                                                                                           | 13/500 [01:43<1:04:46,  7.98s/it]

  3%|████▍                                                                                                                                                           | 14/500 [01:51<1:04:35,  7.97s/it]

  3%|████▊                                                                                                                                                           | 15/500 [01:59<1:04:13,  7.94s/it]

  3%|█████                                                                                                                                                           | 16/500 [02:07<1:04:02,  7.94s/it]

  3%|█████▍                                                                                                                                                          | 17/500 [02:15<1:03:59,  7.95s/it]

  4%|█████▊                                                                                                                                                          | 18/500 [02:23<1:03:32,  7.91s/it]

  4%|██████                                                                                                                                                          | 19/500 [02:31<1:03:47,  7.96s/it]

  4%|██████▍                                                                                                                                                         | 20/500 [02:39<1:03:41,  7.96s/it]


  4%|███████                                                                                                                                                         | 22/500 [02:55<1:04:51,  8.14s/it]
{'loss': 2.1726, 'grad_norm': nan, 'learning_rate': 0.0072, 'epoch': 0.0}

  5%|███████▎                                                                                                                                                        | 23/500 [03:03<1:04:03,  8.06s/it]

  5%|███████▋                                                                                                                                                        | 24/500 [03:11<1:03:34,  8.01s/it]

  5%|████████                                                                                                                                                        | 25/500 [03:19<1:03:10,  7.98s/it]

  5%|████████▎                                                                                                                                                       | 26/500 [03:27<1:02:50,  7.96s/it]

  5%|████████▋                                                                                                                                                       | 27/500 [03:35<1:02:39,  7.95s/it]

  6%|████████▉                                                                                                                                                       | 28/500 [03:43<1:02:25,  7.93s/it]

  6%|█████████▎                                                                                                                                                      | 29/500 [03:51<1:02:07,  7.91s/it]
  6%|█████████▌                                                                                                                                                      | 30/500 [03:58<1:01:40,  7.87s/it]Traceback (most recent call last):
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 243, in <module>
    main()
  File "c:\Users\pink\Documents\Study\Intelligent Systems\COS30018-Mitigate-Hallucination\Finetuning\LLAMA3_QLoRA.py", line 239, in main
    trainer.train()
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\trl\trainer\sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 1948, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2366, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2817, in _maybe_log_save_evaluate
    self._save_checkpoint(model, trial, metrics=metrics)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 2896, in _save_checkpoint
    self.save_model(output_dir, _internal_call=True)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3464, in save_model
    self._save(output_dir)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\trainer.py", line 3540, in _save
    self.tokenizer.save_pretrained(output_dir)
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\tokenization_utils_base.py", line 2697, in save_pretrained
    save_files = self._save_pretrained(
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pink\anaconda3\envs\LLama3-hallucinate\Lib\site-packages\transformers\tokenization_utils_fast.py", line 708, in _save_pretrained
    self.backend_tokenizer.save(tokenizer_file)
Exception: The requested operation cannot be performed on a file with a user-mapped section open. (os error 1224)