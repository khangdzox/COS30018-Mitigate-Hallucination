{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.7573457050470969,
  "eval_steps": 500,
  "global_step": 12500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00014058765640376775,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.298,
      "step": 1
    },
    {
      "epoch": 0.0002811753128075355,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.226,
      "step": 2
    },
    {
      "epoch": 0.00042176296921130323,
      "grad_norm": 7.245819091796875,
      "learning_rate": 2.8089887640449437e-07,
      "loss": 2.2579,
      "step": 3
    },
    {
      "epoch": 0.000562350625615071,
      "grad_norm": NaN,
      "learning_rate": 2.8089887640449437e-07,
      "loss": 2.3189,
      "step": 4
    },
    {
      "epoch": 0.0007029382820188388,
      "grad_norm": 8.10453987121582,
      "learning_rate": 5.617977528089887e-07,
      "loss": 2.2984,
      "step": 5
    },
    {
      "epoch": 0.0008435259384226065,
      "grad_norm": 8.996807098388672,
      "learning_rate": 8.426966292134832e-07,
      "loss": 2.2909,
      "step": 6
    },
    {
      "epoch": 0.0009841135948263741,
      "grad_norm": 8.052919387817383,
      "learning_rate": 1.1235955056179775e-06,
      "loss": 2.2472,
      "step": 7
    },
    {
      "epoch": 0.001124701251230142,
      "grad_norm": 8.764071464538574,
      "learning_rate": 1.404494382022472e-06,
      "loss": 2.2049,
      "step": 8
    },
    {
      "epoch": 0.0012652889076339097,
      "grad_norm": 7.241598129272461,
      "learning_rate": 1.6853932584269663e-06,
      "loss": 2.3267,
      "step": 9
    },
    {
      "epoch": 0.0014058765640376776,
      "grad_norm": 9.075315475463867,
      "learning_rate": 1.966292134831461e-06,
      "loss": 2.3615,
      "step": 10
    },
    {
      "epoch": 0.0015464642204414453,
      "grad_norm": 7.958353519439697,
      "learning_rate": 2.247191011235955e-06,
      "loss": 2.2994,
      "step": 11
    },
    {
      "epoch": 0.001687051876845213,
      "grad_norm": 8.55456256866455,
      "learning_rate": 2.5280898876404495e-06,
      "loss": 2.192,
      "step": 12
    },
    {
      "epoch": 0.0018276395332489808,
      "grad_norm": 9.233574867248535,
      "learning_rate": 2.808988764044944e-06,
      "loss": 2.0955,
      "step": 13
    },
    {
      "epoch": 0.0019682271896527483,
      "grad_norm": 8.310784339904785,
      "learning_rate": 3.089887640449438e-06,
      "loss": 2.2296,
      "step": 14
    },
    {
      "epoch": 0.002108814846056516,
      "grad_norm": 8.425408363342285,
      "learning_rate": 3.3707865168539327e-06,
      "loss": 2.1196,
      "step": 15
    },
    {
      "epoch": 0.002249402502460284,
      "grad_norm": 6.740445613861084,
      "learning_rate": 3.651685393258427e-06,
      "loss": 2.1978,
      "step": 16
    },
    {
      "epoch": 0.002389990158864052,
      "grad_norm": 7.740170001983643,
      "learning_rate": 3.932584269662922e-06,
      "loss": 2.1673,
      "step": 17
    },
    {
      "epoch": 0.0025305778152678194,
      "grad_norm": 8.986848831176758,
      "learning_rate": 4.213483146067416e-06,
      "loss": 2.3169,
      "step": 18
    },
    {
      "epoch": 0.0026711654716715873,
      "grad_norm": 7.2965898513793945,
      "learning_rate": 4.49438202247191e-06,
      "loss": 2.2532,
      "step": 19
    },
    {
      "epoch": 0.002811753128075355,
      "grad_norm": 7.668450355529785,
      "learning_rate": 4.7752808988764044e-06,
      "loss": 2.3154,
      "step": 20
    },
    {
      "epoch": 0.0029523407844791226,
      "grad_norm": 5.917820930480957,
      "learning_rate": 5.056179775280899e-06,
      "loss": 2.1,
      "step": 21
    },
    {
      "epoch": 0.0030929284408828905,
      "grad_norm": 5.082604885101318,
      "learning_rate": 5.3370786516853935e-06,
      "loss": 2.2146,
      "step": 22
    },
    {
      "epoch": 0.0032335160972866584,
      "grad_norm": 8.554009437561035,
      "learning_rate": 5.617977528089888e-06,
      "loss": 2.0414,
      "step": 23
    },
    {
      "epoch": 0.003374103753690426,
      "grad_norm": NaN,
      "learning_rate": 5.617977528089888e-06,
      "loss": 2.2316,
      "step": 24
    },
    {
      "epoch": 0.0035146914100941938,
      "grad_norm": 17.223711013793945,
      "learning_rate": 5.8988764044943826e-06,
      "loss": 2.1168,
      "step": 25
    },
    {
      "epoch": 0.0036552790664979616,
      "grad_norm": NaN,
      "learning_rate": 5.8988764044943826e-06,
      "loss": 2.1397,
      "step": 26
    },
    {
      "epoch": 0.003795866722901729,
      "grad_norm": 35.933746337890625,
      "learning_rate": 6.179775280898876e-06,
      "loss": 1.9748,
      "step": 27
    },
    {
      "epoch": 0.0039364543793054966,
      "grad_norm": 30.63168716430664,
      "learning_rate": 6.460674157303372e-06,
      "loss": 2.1746,
      "step": 28
    },
    {
      "epoch": 0.004077042035709265,
      "grad_norm": 29.3762264251709,
      "learning_rate": 6.741573033707865e-06,
      "loss": 2.0077,
      "step": 29
    },
    {
      "epoch": 0.004217629692113032,
      "grad_norm": 27.41930389404297,
      "learning_rate": 7.022471910112361e-06,
      "loss": 2.055,
      "step": 30
    },
    {
      "epoch": 0.004358217348516801,
      "grad_norm": 26.99652862548828,
      "learning_rate": 7.303370786516854e-06,
      "loss": 2.0234,
      "step": 31
    },
    {
      "epoch": 0.004498805004920568,
      "grad_norm": 19.323719024658203,
      "learning_rate": 7.584269662921349e-06,
      "loss": 2.1001,
      "step": 32
    },
    {
      "epoch": 0.004639392661324336,
      "grad_norm": 7.347014427185059,
      "learning_rate": 7.865168539325843e-06,
      "loss": 1.8841,
      "step": 33
    },
    {
      "epoch": 0.004779980317728104,
      "grad_norm": 4.512160778045654,
      "learning_rate": 8.146067415730338e-06,
      "loss": 2.0841,
      "step": 34
    },
    {
      "epoch": 0.004920567974131871,
      "grad_norm": 8.604472160339355,
      "learning_rate": 8.426966292134832e-06,
      "loss": 1.8288,
      "step": 35
    },
    {
      "epoch": 0.005061155630535639,
      "grad_norm": 10.339221000671387,
      "learning_rate": 8.707865168539327e-06,
      "loss": 1.8591,
      "step": 36
    },
    {
      "epoch": 0.005201743286939407,
      "grad_norm": 10.072115898132324,
      "learning_rate": 8.98876404494382e-06,
      "loss": 1.8997,
      "step": 37
    },
    {
      "epoch": 0.005342330943343175,
      "grad_norm": 14.209969520568848,
      "learning_rate": 9.269662921348316e-06,
      "loss": 1.985,
      "step": 38
    },
    {
      "epoch": 0.005482918599746942,
      "grad_norm": 11.842117309570312,
      "learning_rate": 9.550561797752809e-06,
      "loss": 1.8838,
      "step": 39
    },
    {
      "epoch": 0.00562350625615071,
      "grad_norm": 11.571284294128418,
      "learning_rate": 9.831460674157303e-06,
      "loss": 1.8013,
      "step": 40
    },
    {
      "epoch": 0.005764093912554478,
      "grad_norm": 9.730956077575684,
      "learning_rate": 1.0112359550561798e-05,
      "loss": 1.8069,
      "step": 41
    },
    {
      "epoch": 0.005904681568958245,
      "grad_norm": 3.7914469242095947,
      "learning_rate": 1.0393258426966292e-05,
      "loss": 1.7939,
      "step": 42
    },
    {
      "epoch": 0.006045269225362014,
      "grad_norm": 8.825640678405762,
      "learning_rate": 1.0674157303370787e-05,
      "loss": 1.7602,
      "step": 43
    },
    {
      "epoch": 0.006185856881765781,
      "grad_norm": 16.071815490722656,
      "learning_rate": 1.0955056179775282e-05,
      "loss": 1.7593,
      "step": 44
    },
    {
      "epoch": 0.0063264445381695485,
      "grad_norm": 10.431527137756348,
      "learning_rate": 1.1235955056179776e-05,
      "loss": 1.7729,
      "step": 45
    },
    {
      "epoch": 0.006467032194573317,
      "grad_norm": 8.959321022033691,
      "learning_rate": 1.151685393258427e-05,
      "loss": 1.6891,
      "step": 46
    },
    {
      "epoch": 0.006607619850977084,
      "grad_norm": 9.0274076461792,
      "learning_rate": 1.1797752808988765e-05,
      "loss": 1.6288,
      "step": 47
    },
    {
      "epoch": 0.006748207507380852,
      "grad_norm": 6.833653450012207,
      "learning_rate": 1.207865168539326e-05,
      "loss": 1.6786,
      "step": 48
    },
    {
      "epoch": 0.00688879516378462,
      "grad_norm": 11.65913200378418,
      "learning_rate": 1.2359550561797752e-05,
      "loss": 1.5992,
      "step": 49
    },
    {
      "epoch": 0.0070293828201883875,
      "grad_norm": 6.385910987854004,
      "learning_rate": 1.2640449438202249e-05,
      "loss": 1.5568,
      "step": 50
    },
    {
      "epoch": 0.007169970476592155,
      "grad_norm": 8.652697563171387,
      "learning_rate": 1.2921348314606743e-05,
      "loss": 1.7493,
      "step": 51
    },
    {
      "epoch": 0.007310558132995923,
      "grad_norm": 7.682476043701172,
      "learning_rate": 1.3202247191011236e-05,
      "loss": 1.6931,
      "step": 52
    },
    {
      "epoch": 0.007451145789399691,
      "grad_norm": 9.820348739624023,
      "learning_rate": 1.348314606741573e-05,
      "loss": 1.567,
      "step": 53
    },
    {
      "epoch": 0.007591733445803458,
      "grad_norm": 6.381404876708984,
      "learning_rate": 1.3764044943820225e-05,
      "loss": 1.5004,
      "step": 54
    },
    {
      "epoch": 0.0077323211022072265,
      "grad_norm": 8.900049209594727,
      "learning_rate": 1.4044943820224721e-05,
      "loss": 1.4591,
      "step": 55
    },
    {
      "epoch": 0.007872908758610993,
      "grad_norm": 10.145153999328613,
      "learning_rate": 1.4325842696629212e-05,
      "loss": 1.3139,
      "step": 56
    },
    {
      "epoch": 0.008013496415014762,
      "grad_norm": 5.016868591308594,
      "learning_rate": 1.4606741573033709e-05,
      "loss": 1.3611,
      "step": 57
    },
    {
      "epoch": 0.00815408407141853,
      "grad_norm": 7.837327003479004,
      "learning_rate": 1.4887640449438203e-05,
      "loss": 1.3761,
      "step": 58
    },
    {
      "epoch": 0.008294671727822297,
      "grad_norm": 6.839343547821045,
      "learning_rate": 1.5168539325842698e-05,
      "loss": 1.3073,
      "step": 59
    },
    {
      "epoch": 0.008435259384226065,
      "grad_norm": 4.361875057220459,
      "learning_rate": 1.544943820224719e-05,
      "loss": 1.2112,
      "step": 60
    },
    {
      "epoch": 0.008575847040629832,
      "grad_norm": 3.535879611968994,
      "learning_rate": 1.5730337078651687e-05,
      "loss": 1.3656,
      "step": 61
    },
    {
      "epoch": 0.008716434697033601,
      "grad_norm": 13.417065620422363,
      "learning_rate": 1.601123595505618e-05,
      "loss": 1.299,
      "step": 62
    },
    {
      "epoch": 0.008857022353437369,
      "grad_norm": 10.369181632995605,
      "learning_rate": 1.6292134831460676e-05,
      "loss": 1.3113,
      "step": 63
    },
    {
      "epoch": 0.008997610009841136,
      "grad_norm": 9.918169975280762,
      "learning_rate": 1.657303370786517e-05,
      "loss": 1.1965,
      "step": 64
    },
    {
      "epoch": 0.009138197666244904,
      "grad_norm": 4.8357319831848145,
      "learning_rate": 1.6853932584269665e-05,
      "loss": 1.1972,
      "step": 65
    },
    {
      "epoch": 0.009278785322648671,
      "grad_norm": 6.956521511077881,
      "learning_rate": 1.7134831460674158e-05,
      "loss": 1.2793,
      "step": 66
    },
    {
      "epoch": 0.009419372979052439,
      "grad_norm": 5.534328937530518,
      "learning_rate": 1.7415730337078654e-05,
      "loss": 1.3016,
      "step": 67
    },
    {
      "epoch": 0.009559960635456208,
      "grad_norm": 2.918738842010498,
      "learning_rate": 1.7696629213483147e-05,
      "loss": 1.1916,
      "step": 68
    },
    {
      "epoch": 0.009700548291859975,
      "grad_norm": 3.8146347999572754,
      "learning_rate": 1.797752808988764e-05,
      "loss": 1.3315,
      "step": 69
    },
    {
      "epoch": 0.009841135948263743,
      "grad_norm": 2.690953493118286,
      "learning_rate": 1.8258426966292136e-05,
      "loss": 1.3645,
      "step": 70
    },
    {
      "epoch": 0.00998172360466751,
      "grad_norm": 2.82820987701416,
      "learning_rate": 1.8539325842696632e-05,
      "loss": 1.2824,
      "step": 71
    },
    {
      "epoch": 0.010122311261071278,
      "grad_norm": 2.4890432357788086,
      "learning_rate": 1.8820224719101125e-05,
      "loss": 1.1715,
      "step": 72
    },
    {
      "epoch": 0.010262898917475045,
      "grad_norm": 2.3097381591796875,
      "learning_rate": 1.9101123595505618e-05,
      "loss": 1.2078,
      "step": 73
    },
    {
      "epoch": 0.010403486573878814,
      "grad_norm": 2.7446506023406982,
      "learning_rate": 1.9382022471910114e-05,
      "loss": 1.0115,
      "step": 74
    },
    {
      "epoch": 0.010544074230282582,
      "grad_norm": 2.422837734222412,
      "learning_rate": 1.9662921348314607e-05,
      "loss": 1.4363,
      "step": 75
    },
    {
      "epoch": 0.01068466188668635,
      "grad_norm": 3.793195962905884,
      "learning_rate": 1.99438202247191e-05,
      "loss": 1.0729,
      "step": 76
    },
    {
      "epoch": 0.010825249543090117,
      "grad_norm": 2.3601086139678955,
      "learning_rate": 2.0224719101123596e-05,
      "loss": 1.2179,
      "step": 77
    },
    {
      "epoch": 0.010965837199493884,
      "grad_norm": 2.886718511581421,
      "learning_rate": 2.0505617977528092e-05,
      "loss": 1.1277,
      "step": 78
    },
    {
      "epoch": 0.011106424855897652,
      "grad_norm": 3.333782911300659,
      "learning_rate": 2.0786516853932585e-05,
      "loss": 1.1409,
      "step": 79
    },
    {
      "epoch": 0.01124701251230142,
      "grad_norm": 2.8088269233703613,
      "learning_rate": 2.1067415730337078e-05,
      "loss": 1.0883,
      "step": 80
    },
    {
      "epoch": 0.011387600168705188,
      "grad_norm": 2.827481746673584,
      "learning_rate": 2.1348314606741574e-05,
      "loss": 1.1035,
      "step": 81
    },
    {
      "epoch": 0.011528187825108956,
      "grad_norm": 2.746737003326416,
      "learning_rate": 2.1629213483146067e-05,
      "loss": 1.2402,
      "step": 82
    },
    {
      "epoch": 0.011668775481512723,
      "grad_norm": 2.808919906616211,
      "learning_rate": 2.1910112359550563e-05,
      "loss": 1.2405,
      "step": 83
    },
    {
      "epoch": 0.01180936313791649,
      "grad_norm": 2.9713563919067383,
      "learning_rate": 2.2191011235955056e-05,
      "loss": 1.4111,
      "step": 84
    },
    {
      "epoch": 0.011949950794320258,
      "grad_norm": 2.4026224613189697,
      "learning_rate": 2.2471910112359552e-05,
      "loss": 1.197,
      "step": 85
    },
    {
      "epoch": 0.012090538450724027,
      "grad_norm": 2.6723475456237793,
      "learning_rate": 2.2752808988764045e-05,
      "loss": 1.1703,
      "step": 86
    },
    {
      "epoch": 0.012231126107127795,
      "grad_norm": 2.7952113151550293,
      "learning_rate": 2.303370786516854e-05,
      "loss": 1.1918,
      "step": 87
    },
    {
      "epoch": 0.012371713763531562,
      "grad_norm": 2.122035264968872,
      "learning_rate": 2.3314606741573034e-05,
      "loss": 1.351,
      "step": 88
    },
    {
      "epoch": 0.01251230141993533,
      "grad_norm": 2.3143932819366455,
      "learning_rate": 2.359550561797753e-05,
      "loss": 1.1774,
      "step": 89
    },
    {
      "epoch": 0.012652889076339097,
      "grad_norm": 2.3971498012542725,
      "learning_rate": 2.3876404494382023e-05,
      "loss": 1.1994,
      "step": 90
    },
    {
      "epoch": 0.012793476732742864,
      "grad_norm": 2.1432976722717285,
      "learning_rate": 2.415730337078652e-05,
      "loss": 1.2284,
      "step": 91
    },
    {
      "epoch": 0.012934064389146634,
      "grad_norm": 2.5893337726593018,
      "learning_rate": 2.4438202247191012e-05,
      "loss": 1.1681,
      "step": 92
    },
    {
      "epoch": 0.013074652045550401,
      "grad_norm": 3.9374842643737793,
      "learning_rate": 2.4719101123595505e-05,
      "loss": 1.0558,
      "step": 93
    },
    {
      "epoch": 0.013215239701954169,
      "grad_norm": 2.2501206398010254,
      "learning_rate": 2.5e-05,
      "loss": 1.0544,
      "step": 94
    },
    {
      "epoch": 0.013355827358357936,
      "grad_norm": 2.6196744441986084,
      "learning_rate": 2.5280898876404497e-05,
      "loss": 1.0449,
      "step": 95
    },
    {
      "epoch": 0.013496415014761703,
      "grad_norm": 2.6912648677825928,
      "learning_rate": 2.556179775280899e-05,
      "loss": 1.3097,
      "step": 96
    },
    {
      "epoch": 0.013637002671165471,
      "grad_norm": 2.8213303089141846,
      "learning_rate": 2.5842696629213486e-05,
      "loss": 1.078,
      "step": 97
    },
    {
      "epoch": 0.01377759032756924,
      "grad_norm": 2.288163185119629,
      "learning_rate": 2.6123595505617983e-05,
      "loss": 1.2944,
      "step": 98
    },
    {
      "epoch": 0.013918177983973008,
      "grad_norm": 2.4742157459259033,
      "learning_rate": 2.6404494382022472e-05,
      "loss": 1.112,
      "step": 99
    },
    {
      "epoch": 0.014058765640376775,
      "grad_norm": 2.6970505714416504,
      "learning_rate": 2.6685393258426965e-05,
      "loss": 1.2189,
      "step": 100
    },
    {
      "epoch": 0.014199353296780542,
      "grad_norm": 2.327686071395874,
      "learning_rate": 2.696629213483146e-05,
      "loss": 1.3227,
      "step": 101
    },
    {
      "epoch": 0.01433994095318431,
      "grad_norm": 2.6703150272369385,
      "learning_rate": 2.7247191011235957e-05,
      "loss": 1.0781,
      "step": 102
    },
    {
      "epoch": 0.014480528609588077,
      "grad_norm": 2.652482032775879,
      "learning_rate": 2.752808988764045e-05,
      "loss": 1.1364,
      "step": 103
    },
    {
      "epoch": 0.014621116265991847,
      "grad_norm": 2.9150097370147705,
      "learning_rate": 2.7808988764044946e-05,
      "loss": 1.0665,
      "step": 104
    },
    {
      "epoch": 0.014761703922395614,
      "grad_norm": 2.5454089641571045,
      "learning_rate": 2.8089887640449443e-05,
      "loss": 1.1735,
      "step": 105
    },
    {
      "epoch": 0.014902291578799381,
      "grad_norm": 2.6714460849761963,
      "learning_rate": 2.8370786516853936e-05,
      "loss": 1.1242,
      "step": 106
    },
    {
      "epoch": 0.015042879235203149,
      "grad_norm": 2.2454006671905518,
      "learning_rate": 2.8651685393258425e-05,
      "loss": 1.3172,
      "step": 107
    },
    {
      "epoch": 0.015183466891606916,
      "grad_norm": 2.5474889278411865,
      "learning_rate": 2.893258426966292e-05,
      "loss": 1.1229,
      "step": 108
    },
    {
      "epoch": 0.015324054548010684,
      "grad_norm": 2.181563377380371,
      "learning_rate": 2.9213483146067417e-05,
      "loss": 1.2548,
      "step": 109
    },
    {
      "epoch": 0.015464642204414453,
      "grad_norm": 2.527151107788086,
      "learning_rate": 2.949438202247191e-05,
      "loss": 1.1055,
      "step": 110
    },
    {
      "epoch": 0.01560522986081822,
      "grad_norm": 2.00559139251709,
      "learning_rate": 2.9775280898876406e-05,
      "loss": 1.0931,
      "step": 111
    },
    {
      "epoch": 0.015745817517221986,
      "grad_norm": 2.2237608432769775,
      "learning_rate": 3.0056179775280903e-05,
      "loss": 1.1805,
      "step": 112
    },
    {
      "epoch": 0.015886405173625755,
      "grad_norm": 1.9891678094863892,
      "learning_rate": 3.0337078651685396e-05,
      "loss": 1.1655,
      "step": 113
    },
    {
      "epoch": 0.016026992830029525,
      "grad_norm": 2.9956021308898926,
      "learning_rate": 3.061797752808989e-05,
      "loss": 1.0302,
      "step": 114
    },
    {
      "epoch": 0.01616758048643329,
      "grad_norm": 2.1211533546447754,
      "learning_rate": 3.089887640449438e-05,
      "loss": 0.9253,
      "step": 115
    },
    {
      "epoch": 0.01630816814283706,
      "grad_norm": 2.7525973320007324,
      "learning_rate": 3.1179775280898874e-05,
      "loss": 1.038,
      "step": 116
    },
    {
      "epoch": 0.016448755799240825,
      "grad_norm": 2.4122238159179688,
      "learning_rate": 3.1460674157303374e-05,
      "loss": 1.1103,
      "step": 117
    },
    {
      "epoch": 0.016589343455644594,
      "grad_norm": 2.704362392425537,
      "learning_rate": 3.1741573033707866e-05,
      "loss": 1.0881,
      "step": 118
    },
    {
      "epoch": 0.016729931112048364,
      "grad_norm": 2.2417240142822266,
      "learning_rate": 3.202247191011236e-05,
      "loss": 1.2084,
      "step": 119
    },
    {
      "epoch": 0.01687051876845213,
      "grad_norm": 2.310286521911621,
      "learning_rate": 3.230337078651686e-05,
      "loss": 1.2365,
      "step": 120
    },
    {
      "epoch": 0.0170111064248559,
      "grad_norm": 2.479375123977661,
      "learning_rate": 3.258426966292135e-05,
      "loss": 1.0883,
      "step": 121
    },
    {
      "epoch": 0.017151694081259664,
      "grad_norm": 3.379467248916626,
      "learning_rate": 3.2865168539325845e-05,
      "loss": 1.1684,
      "step": 122
    },
    {
      "epoch": 0.017292281737663433,
      "grad_norm": 3.8731865882873535,
      "learning_rate": 3.314606741573034e-05,
      "loss": 1.1307,
      "step": 123
    },
    {
      "epoch": 0.017432869394067203,
      "grad_norm": 2.757347583770752,
      "learning_rate": 3.342696629213483e-05,
      "loss": 1.2052,
      "step": 124
    },
    {
      "epoch": 0.01757345705047097,
      "grad_norm": 3.3242719173431396,
      "learning_rate": 3.370786516853933e-05,
      "loss": 0.9349,
      "step": 125
    },
    {
      "epoch": 0.017714044706874738,
      "grad_norm": 2.385014295578003,
      "learning_rate": 3.398876404494382e-05,
      "loss": 1.407,
      "step": 126
    },
    {
      "epoch": 0.017854632363278503,
      "grad_norm": 2.5655360221862793,
      "learning_rate": 3.4269662921348316e-05,
      "loss": 1.0885,
      "step": 127
    },
    {
      "epoch": 0.017995220019682272,
      "grad_norm": 2.4665701389312744,
      "learning_rate": 3.455056179775281e-05,
      "loss": 1.0578,
      "step": 128
    },
    {
      "epoch": 0.018135807676086038,
      "grad_norm": 3.0541493892669678,
      "learning_rate": 3.483146067415731e-05,
      "loss": 1.1622,
      "step": 129
    },
    {
      "epoch": 0.018276395332489807,
      "grad_norm": 2.253767967224121,
      "learning_rate": 3.51123595505618e-05,
      "loss": 1.328,
      "step": 130
    },
    {
      "epoch": 0.018416982988893577,
      "grad_norm": 2.155627727508545,
      "learning_rate": 3.5393258426966294e-05,
      "loss": 1.1824,
      "step": 131
    },
    {
      "epoch": 0.018557570645297342,
      "grad_norm": 2.2615280151367188,
      "learning_rate": 3.5674157303370787e-05,
      "loss": 1.1132,
      "step": 132
    },
    {
      "epoch": 0.01869815830170111,
      "grad_norm": 2.0365400314331055,
      "learning_rate": 3.595505617977528e-05,
      "loss": 1.2145,
      "step": 133
    },
    {
      "epoch": 0.018838745958104877,
      "grad_norm": 2.1860344409942627,
      "learning_rate": 3.623595505617978e-05,
      "loss": 1.0693,
      "step": 134
    },
    {
      "epoch": 0.018979333614508646,
      "grad_norm": 2.77993106842041,
      "learning_rate": 3.651685393258427e-05,
      "loss": 1.132,
      "step": 135
    },
    {
      "epoch": 0.019119921270912416,
      "grad_norm": 2.168142318725586,
      "learning_rate": 3.6797752808988765e-05,
      "loss": 1.2314,
      "step": 136
    },
    {
      "epoch": 0.01926050892731618,
      "grad_norm": 2.6242620944976807,
      "learning_rate": 3.7078651685393264e-05,
      "loss": 1.2417,
      "step": 137
    },
    {
      "epoch": 0.01940109658371995,
      "grad_norm": 2.464482069015503,
      "learning_rate": 3.735955056179776e-05,
      "loss": 1.1672,
      "step": 138
    },
    {
      "epoch": 0.019541684240123716,
      "grad_norm": 2.4668045043945312,
      "learning_rate": 3.764044943820225e-05,
      "loss": 1.1354,
      "step": 139
    },
    {
      "epoch": 0.019682271896527485,
      "grad_norm": 2.401266098022461,
      "learning_rate": 3.792134831460674e-05,
      "loss": 1.1899,
      "step": 140
    },
    {
      "epoch": 0.01982285955293125,
      "grad_norm": 2.495306968688965,
      "learning_rate": 3.8202247191011236e-05,
      "loss": 0.974,
      "step": 141
    },
    {
      "epoch": 0.01996344720933502,
      "grad_norm": 2.9178707599639893,
      "learning_rate": 3.8483146067415735e-05,
      "loss": 1.1458,
      "step": 142
    },
    {
      "epoch": 0.02010403486573879,
      "grad_norm": 2.1856331825256348,
      "learning_rate": 3.876404494382023e-05,
      "loss": 1.2444,
      "step": 143
    },
    {
      "epoch": 0.020244622522142555,
      "grad_norm": 2.6706271171569824,
      "learning_rate": 3.904494382022472e-05,
      "loss": 1.0776,
      "step": 144
    },
    {
      "epoch": 0.020385210178546324,
      "grad_norm": 2.398362398147583,
      "learning_rate": 3.9325842696629214e-05,
      "loss": 0.9715,
      "step": 145
    },
    {
      "epoch": 0.02052579783495009,
      "grad_norm": 2.7096288204193115,
      "learning_rate": 3.960674157303371e-05,
      "loss": 1.1082,
      "step": 146
    },
    {
      "epoch": 0.02066638549135386,
      "grad_norm": 2.482909679412842,
      "learning_rate": 3.98876404494382e-05,
      "loss": 1.1334,
      "step": 147
    },
    {
      "epoch": 0.02080697314775763,
      "grad_norm": 2.3365519046783447,
      "learning_rate": 4.01685393258427e-05,
      "loss": 1.2429,
      "step": 148
    },
    {
      "epoch": 0.020947560804161394,
      "grad_norm": 2.356829881668091,
      "learning_rate": 4.044943820224719e-05,
      "loss": 1.1163,
      "step": 149
    },
    {
      "epoch": 0.021088148460565163,
      "grad_norm": 2.738759994506836,
      "learning_rate": 4.0730337078651685e-05,
      "loss": 0.9433,
      "step": 150
    },
    {
      "epoch": 0.02122873611696893,
      "grad_norm": 2.3433585166931152,
      "learning_rate": 4.1011235955056184e-05,
      "loss": 1.0974,
      "step": 151
    },
    {
      "epoch": 0.0213693237733727,
      "grad_norm": 2.164194107055664,
      "learning_rate": 4.129213483146068e-05,
      "loss": 1.1723,
      "step": 152
    },
    {
      "epoch": 0.021509911429776464,
      "grad_norm": 2.3013522624969482,
      "learning_rate": 4.157303370786517e-05,
      "loss": 1.2237,
      "step": 153
    },
    {
      "epoch": 0.021650499086180233,
      "grad_norm": 2.5231783390045166,
      "learning_rate": 4.185393258426967e-05,
      "loss": 1.0693,
      "step": 154
    },
    {
      "epoch": 0.021791086742584002,
      "grad_norm": 3.1130521297454834,
      "learning_rate": 4.2134831460674156e-05,
      "loss": 1.0361,
      "step": 155
    },
    {
      "epoch": 0.021931674398987768,
      "grad_norm": 2.4152162075042725,
      "learning_rate": 4.2415730337078655e-05,
      "loss": 1.1798,
      "step": 156
    },
    {
      "epoch": 0.022072262055391537,
      "grad_norm": 2.383089542388916,
      "learning_rate": 4.269662921348315e-05,
      "loss": 1.325,
      "step": 157
    },
    {
      "epoch": 0.022212849711795303,
      "grad_norm": 2.531989336013794,
      "learning_rate": 4.297752808988764e-05,
      "loss": 1.2037,
      "step": 158
    },
    {
      "epoch": 0.022353437368199072,
      "grad_norm": 2.649341106414795,
      "learning_rate": 4.3258426966292134e-05,
      "loss": 1.1645,
      "step": 159
    },
    {
      "epoch": 0.02249402502460284,
      "grad_norm": 2.839721918106079,
      "learning_rate": 4.353932584269663e-05,
      "loss": 1.1336,
      "step": 160
    },
    {
      "epoch": 0.022634612681006607,
      "grad_norm": 2.6971545219421387,
      "learning_rate": 4.3820224719101126e-05,
      "loss": 1.0353,
      "step": 161
    },
    {
      "epoch": 0.022775200337410376,
      "grad_norm": 2.523728847503662,
      "learning_rate": 4.410112359550562e-05,
      "loss": 1.1275,
      "step": 162
    },
    {
      "epoch": 0.022915787993814142,
      "grad_norm": 2.7905707359313965,
      "learning_rate": 4.438202247191011e-05,
      "loss": 0.899,
      "step": 163
    },
    {
      "epoch": 0.02305637565021791,
      "grad_norm": 2.790402412414551,
      "learning_rate": 4.4662921348314605e-05,
      "loss": 1.0566,
      "step": 164
    },
    {
      "epoch": 0.023196963306621677,
      "grad_norm": 2.46990966796875,
      "learning_rate": 4.4943820224719104e-05,
      "loss": 1.3215,
      "step": 165
    },
    {
      "epoch": 0.023337550963025446,
      "grad_norm": 2.8140146732330322,
      "learning_rate": 4.52247191011236e-05,
      "loss": 1.1687,
      "step": 166
    },
    {
      "epoch": 0.023478138619429215,
      "grad_norm": 2.4004974365234375,
      "learning_rate": 4.550561797752809e-05,
      "loss": 1.072,
      "step": 167
    },
    {
      "epoch": 0.02361872627583298,
      "grad_norm": 2.1957356929779053,
      "learning_rate": 4.578651685393259e-05,
      "loss": 1.1038,
      "step": 168
    },
    {
      "epoch": 0.02375931393223675,
      "grad_norm": 2.382897138595581,
      "learning_rate": 4.606741573033708e-05,
      "loss": 1.1327,
      "step": 169
    },
    {
      "epoch": 0.023899901588640516,
      "grad_norm": 2.21454119682312,
      "learning_rate": 4.6348314606741575e-05,
      "loss": 1.188,
      "step": 170
    },
    {
      "epoch": 0.024040489245044285,
      "grad_norm": 2.0289082527160645,
      "learning_rate": 4.662921348314607e-05,
      "loss": 1.186,
      "step": 171
    },
    {
      "epoch": 0.024181076901448054,
      "grad_norm": 2.2515382766723633,
      "learning_rate": 4.691011235955056e-05,
      "loss": 1.0877,
      "step": 172
    },
    {
      "epoch": 0.02432166455785182,
      "grad_norm": 3.099454402923584,
      "learning_rate": 4.719101123595506e-05,
      "loss": 1.1245,
      "step": 173
    },
    {
      "epoch": 0.02446225221425559,
      "grad_norm": 2.608018636703491,
      "learning_rate": 4.747191011235955e-05,
      "loss": 1.0546,
      "step": 174
    },
    {
      "epoch": 0.024602839870659355,
      "grad_norm": 2.5215113162994385,
      "learning_rate": 4.7752808988764046e-05,
      "loss": 0.9787,
      "step": 175
    },
    {
      "epoch": 0.024743427527063124,
      "grad_norm": 2.3730759620666504,
      "learning_rate": 4.803370786516854e-05,
      "loss": 1.1932,
      "step": 176
    },
    {
      "epoch": 0.02488401518346689,
      "grad_norm": 3.0721728801727295,
      "learning_rate": 4.831460674157304e-05,
      "loss": 1.08,
      "step": 177
    },
    {
      "epoch": 0.02502460283987066,
      "grad_norm": 2.733149766921997,
      "learning_rate": 4.859550561797753e-05,
      "loss": 1.1693,
      "step": 178
    },
    {
      "epoch": 0.02516519049627443,
      "grad_norm": 2.5139222145080566,
      "learning_rate": 4.8876404494382024e-05,
      "loss": 1.0516,
      "step": 179
    },
    {
      "epoch": 0.025305778152678194,
      "grad_norm": 2.671976327896118,
      "learning_rate": 4.915730337078652e-05,
      "loss": 1.1726,
      "step": 180
    },
    {
      "epoch": 0.025446365809081963,
      "grad_norm": 2.4634792804718018,
      "learning_rate": 4.943820224719101e-05,
      "loss": 0.8458,
      "step": 181
    },
    {
      "epoch": 0.02558695346548573,
      "grad_norm": 2.570363998413086,
      "learning_rate": 4.971910112359551e-05,
      "loss": 1.2142,
      "step": 182
    },
    {
      "epoch": 0.025727541121889498,
      "grad_norm": 2.3932912349700928,
      "learning_rate": 5e-05,
      "loss": 1.1424,
      "step": 183
    },
    {
      "epoch": 0.025868128778293267,
      "grad_norm": 2.595952272415161,
      "learning_rate": 5.0280898876404495e-05,
      "loss": 1.1049,
      "step": 184
    },
    {
      "epoch": 0.026008716434697033,
      "grad_norm": 2.281751871109009,
      "learning_rate": 5.0561797752808995e-05,
      "loss": 1.2497,
      "step": 185
    },
    {
      "epoch": 0.026149304091100802,
      "grad_norm": 2.7008118629455566,
      "learning_rate": 5.084269662921348e-05,
      "loss": 1.1453,
      "step": 186
    },
    {
      "epoch": 0.026289891747504568,
      "grad_norm": 2.398432970046997,
      "learning_rate": 5.112359550561798e-05,
      "loss": 1.1492,
      "step": 187
    },
    {
      "epoch": 0.026430479403908337,
      "grad_norm": 2.0010013580322266,
      "learning_rate": 5.140449438202247e-05,
      "loss": 1.2415,
      "step": 188
    },
    {
      "epoch": 0.026571067060312106,
      "grad_norm": 2.4201300144195557,
      "learning_rate": 5.168539325842697e-05,
      "loss": 0.8927,
      "step": 189
    },
    {
      "epoch": 0.026711654716715872,
      "grad_norm": 2.545823812484741,
      "learning_rate": 5.1966292134831466e-05,
      "loss": 1.225,
      "step": 190
    },
    {
      "epoch": 0.02685224237311964,
      "grad_norm": 2.206869602203369,
      "learning_rate": 5.2247191011235965e-05,
      "loss": 1.2117,
      "step": 191
    },
    {
      "epoch": 0.026992830029523407,
      "grad_norm": 2.336686134338379,
      "learning_rate": 5.252808988764045e-05,
      "loss": 1.1213,
      "step": 192
    },
    {
      "epoch": 0.027133417685927176,
      "grad_norm": 2.0921790599823,
      "learning_rate": 5.2808988764044944e-05,
      "loss": 1.0929,
      "step": 193
    },
    {
      "epoch": 0.027274005342330942,
      "grad_norm": 2.2613587379455566,
      "learning_rate": 5.3089887640449444e-05,
      "loss": 1.1412,
      "step": 194
    },
    {
      "epoch": 0.02741459299873471,
      "grad_norm": 2.6331818103790283,
      "learning_rate": 5.337078651685393e-05,
      "loss": 1.0595,
      "step": 195
    },
    {
      "epoch": 0.02755518065513848,
      "grad_norm": 2.4552652835845947,
      "learning_rate": 5.365168539325843e-05,
      "loss": 0.9995,
      "step": 196
    },
    {
      "epoch": 0.027695768311542246,
      "grad_norm": 2.0642356872558594,
      "learning_rate": 5.393258426966292e-05,
      "loss": 1.221,
      "step": 197
    },
    {
      "epoch": 0.027836355967946015,
      "grad_norm": 2.213430881500244,
      "learning_rate": 5.421348314606742e-05,
      "loss": 1.3011,
      "step": 198
    },
    {
      "epoch": 0.02797694362434978,
      "grad_norm": 2.8076305389404297,
      "learning_rate": 5.4494382022471915e-05,
      "loss": 1.099,
      "step": 199
    },
    {
      "epoch": 0.02811753128075355,
      "grad_norm": 2.231855630874634,
      "learning_rate": 5.47752808988764e-05,
      "loss": 1.0115,
      "step": 200
    },
    {
      "epoch": 0.02825811893715732,
      "grad_norm": 2.500011682510376,
      "learning_rate": 5.50561797752809e-05,
      "loss": 1.1292,
      "step": 201
    },
    {
      "epoch": 0.028398706593561085,
      "grad_norm": 2.4029178619384766,
      "learning_rate": 5.533707865168539e-05,
      "loss": 1.22,
      "step": 202
    },
    {
      "epoch": 0.028539294249964854,
      "grad_norm": 1.6618330478668213,
      "learning_rate": 5.561797752808989e-05,
      "loss": 1.1847,
      "step": 203
    },
    {
      "epoch": 0.02867988190636862,
      "grad_norm": 2.378413438796997,
      "learning_rate": 5.5898876404494386e-05,
      "loss": 1.1032,
      "step": 204
    },
    {
      "epoch": 0.02882046956277239,
      "grad_norm": 2.0340278148651123,
      "learning_rate": 5.6179775280898885e-05,
      "loss": 1.1781,
      "step": 205
    },
    {
      "epoch": 0.028961057219176155,
      "grad_norm": 2.2804934978485107,
      "learning_rate": 5.646067415730337e-05,
      "loss": 1.1772,
      "step": 206
    },
    {
      "epoch": 0.029101644875579924,
      "grad_norm": 2.3874778747558594,
      "learning_rate": 5.674157303370787e-05,
      "loss": 1.0767,
      "step": 207
    },
    {
      "epoch": 0.029242232531983693,
      "grad_norm": 2.644440174102783,
      "learning_rate": 5.7022471910112364e-05,
      "loss": 1.1308,
      "step": 208
    },
    {
      "epoch": 0.02938282018838746,
      "grad_norm": 2.157053232192993,
      "learning_rate": 5.730337078651685e-05,
      "loss": 1.0378,
      "step": 209
    },
    {
      "epoch": 0.029523407844791228,
      "grad_norm": 3.0746865272521973,
      "learning_rate": 5.758426966292135e-05,
      "loss": 1.2433,
      "step": 210
    },
    {
      "epoch": 0.029663995501194994,
      "grad_norm": 2.237730026245117,
      "learning_rate": 5.786516853932584e-05,
      "loss": 1.0372,
      "step": 211
    },
    {
      "epoch": 0.029804583157598763,
      "grad_norm": 2.3634049892425537,
      "learning_rate": 5.814606741573034e-05,
      "loss": 1.0036,
      "step": 212
    },
    {
      "epoch": 0.029945170814002532,
      "grad_norm": 2.357161521911621,
      "learning_rate": 5.8426966292134835e-05,
      "loss": 1.2991,
      "step": 213
    },
    {
      "epoch": 0.030085758470406298,
      "grad_norm": 2.7408816814422607,
      "learning_rate": 5.8707865168539334e-05,
      "loss": 1.079,
      "step": 214
    },
    {
      "epoch": 0.030226346126810067,
      "grad_norm": 2.2548444271087646,
      "learning_rate": 5.898876404494382e-05,
      "loss": 1.2007,
      "step": 215
    },
    {
      "epoch": 0.030366933783213833,
      "grad_norm": 2.246663808822632,
      "learning_rate": 5.926966292134831e-05,
      "loss": 1.1618,
      "step": 216
    },
    {
      "epoch": 0.030507521439617602,
      "grad_norm": 2.304464340209961,
      "learning_rate": 5.955056179775281e-05,
      "loss": 1.2105,
      "step": 217
    },
    {
      "epoch": 0.030648109096021368,
      "grad_norm": 2.4901199340820312,
      "learning_rate": 5.9831460674157306e-05,
      "loss": 1.0181,
      "step": 218
    },
    {
      "epoch": 0.030788696752425137,
      "grad_norm": 1.9396816492080688,
      "learning_rate": 6.0112359550561805e-05,
      "loss": 1.1744,
      "step": 219
    },
    {
      "epoch": 0.030929284408828906,
      "grad_norm": 2.299004077911377,
      "learning_rate": 6.039325842696629e-05,
      "loss": 1.1852,
      "step": 220
    },
    {
      "epoch": 0.031069872065232672,
      "grad_norm": 2.522064208984375,
      "learning_rate": 6.067415730337079e-05,
      "loss": 1.0475,
      "step": 221
    },
    {
      "epoch": 0.03121045972163644,
      "grad_norm": 2.192898988723755,
      "learning_rate": 6.0955056179775284e-05,
      "loss": 1.0947,
      "step": 222
    },
    {
      "epoch": 0.03135104737804021,
      "grad_norm": 2.0952770709991455,
      "learning_rate": 6.123595505617978e-05,
      "loss": 1.0183,
      "step": 223
    },
    {
      "epoch": 0.03149163503444397,
      "grad_norm": 2.294480085372925,
      "learning_rate": 6.151685393258427e-05,
      "loss": 1.0435,
      "step": 224
    },
    {
      "epoch": 0.03163222269084774,
      "grad_norm": 2.631321668624878,
      "learning_rate": 6.179775280898876e-05,
      "loss": 1.0553,
      "step": 225
    },
    {
      "epoch": 0.03177281034725151,
      "grad_norm": 2.6891255378723145,
      "learning_rate": 6.207865168539327e-05,
      "loss": 1.1566,
      "step": 226
    },
    {
      "epoch": 0.03191339800365528,
      "grad_norm": 2.035349130630493,
      "learning_rate": 6.235955056179775e-05,
      "loss": 1.0791,
      "step": 227
    },
    {
      "epoch": 0.03205398566005905,
      "grad_norm": 2.2789969444274902,
      "learning_rate": 6.264044943820225e-05,
      "loss": 1.0835,
      "step": 228
    },
    {
      "epoch": 0.03219457331646281,
      "grad_norm": 2.1781015396118164,
      "learning_rate": 6.292134831460675e-05,
      "loss": 1.0812,
      "step": 229
    },
    {
      "epoch": 0.03233516097286658,
      "grad_norm": 2.4605274200439453,
      "learning_rate": 6.320224719101124e-05,
      "loss": 0.9776,
      "step": 230
    },
    {
      "epoch": 0.03247574862927035,
      "grad_norm": 2.287975788116455,
      "learning_rate": 6.348314606741573e-05,
      "loss": 1.183,
      "step": 231
    },
    {
      "epoch": 0.03261633628567412,
      "grad_norm": 2.30143141746521,
      "learning_rate": 6.376404494382023e-05,
      "loss": 1.1468,
      "step": 232
    },
    {
      "epoch": 0.03275692394207789,
      "grad_norm": 2.6938745975494385,
      "learning_rate": 6.404494382022472e-05,
      "loss": 1.1487,
      "step": 233
    },
    {
      "epoch": 0.03289751159848165,
      "grad_norm": 2.258444309234619,
      "learning_rate": 6.432584269662921e-05,
      "loss": 1.1181,
      "step": 234
    },
    {
      "epoch": 0.03303809925488542,
      "grad_norm": 2.083493947982788,
      "learning_rate": 6.460674157303372e-05,
      "loss": 1.1556,
      "step": 235
    },
    {
      "epoch": 0.03317868691128919,
      "grad_norm": 3.012044906616211,
      "learning_rate": 6.48876404494382e-05,
      "loss": 1.215,
      "step": 236
    },
    {
      "epoch": 0.03331927456769296,
      "grad_norm": 2.19004487991333,
      "learning_rate": 6.51685393258427e-05,
      "loss": 1.0915,
      "step": 237
    },
    {
      "epoch": 0.03345986222409673,
      "grad_norm": 2.1062936782836914,
      "learning_rate": 6.54494382022472e-05,
      "loss": 1.0245,
      "step": 238
    },
    {
      "epoch": 0.03360044988050049,
      "grad_norm": 2.043398380279541,
      "learning_rate": 6.573033707865169e-05,
      "loss": 1.1205,
      "step": 239
    },
    {
      "epoch": 0.03374103753690426,
      "grad_norm": 2.2317874431610107,
      "learning_rate": 6.601123595505618e-05,
      "loss": 1.0545,
      "step": 240
    },
    {
      "epoch": 0.03388162519330803,
      "grad_norm": 1.9159241914749146,
      "learning_rate": 6.629213483146067e-05,
      "loss": 1.1124,
      "step": 241
    },
    {
      "epoch": 0.0340222128497118,
      "grad_norm": 2.085078239440918,
      "learning_rate": 6.657303370786517e-05,
      "loss": 1.2404,
      "step": 242
    },
    {
      "epoch": 0.034162800506115566,
      "grad_norm": 2.2065136432647705,
      "learning_rate": 6.685393258426966e-05,
      "loss": 1.1184,
      "step": 243
    },
    {
      "epoch": 0.03430338816251933,
      "grad_norm": 2.4882514476776123,
      "learning_rate": 6.713483146067417e-05,
      "loss": 1.0858,
      "step": 244
    },
    {
      "epoch": 0.0344439758189231,
      "grad_norm": 2.479884386062622,
      "learning_rate": 6.741573033707866e-05,
      "loss": 1.1801,
      "step": 245
    },
    {
      "epoch": 0.03458456347532687,
      "grad_norm": 2.1303014755249023,
      "learning_rate": 6.769662921348315e-05,
      "loss": 1.016,
      "step": 246
    },
    {
      "epoch": 0.034725151131730636,
      "grad_norm": 2.3555989265441895,
      "learning_rate": 6.797752808988765e-05,
      "loss": 1.0965,
      "step": 247
    },
    {
      "epoch": 0.034865738788134405,
      "grad_norm": 2.18965220451355,
      "learning_rate": 6.825842696629214e-05,
      "loss": 1.1348,
      "step": 248
    },
    {
      "epoch": 0.03500632644453817,
      "grad_norm": 2.250523090362549,
      "learning_rate": 6.853932584269663e-05,
      "loss": 1.0747,
      "step": 249
    },
    {
      "epoch": 0.03514691410094194,
      "grad_norm": 2.521613597869873,
      "learning_rate": 6.882022471910112e-05,
      "loss": 1.1777,
      "step": 250
    },
    {
      "epoch": 0.035287501757345706,
      "grad_norm": 2.0158755779266357,
      "learning_rate": 6.910112359550562e-05,
      "loss": 1.0722,
      "step": 251
    },
    {
      "epoch": 0.035428089413749475,
      "grad_norm": 2.098524808883667,
      "learning_rate": 6.938202247191011e-05,
      "loss": 1.1995,
      "step": 252
    },
    {
      "epoch": 0.03556867707015324,
      "grad_norm": 2.0240628719329834,
      "learning_rate": 6.966292134831462e-05,
      "loss": 1.0318,
      "step": 253
    },
    {
      "epoch": 0.035709264726557007,
      "grad_norm": 2.0322062969207764,
      "learning_rate": 6.994382022471911e-05,
      "loss": 1.0994,
      "step": 254
    },
    {
      "epoch": 0.035849852382960776,
      "grad_norm": 2.2563600540161133,
      "learning_rate": 7.02247191011236e-05,
      "loss": 0.9107,
      "step": 255
    },
    {
      "epoch": 0.035990440039364545,
      "grad_norm": 2.184483766555786,
      "learning_rate": 7.05056179775281e-05,
      "loss": 1.118,
      "step": 256
    },
    {
      "epoch": 0.036131027695768314,
      "grad_norm": 2.503980875015259,
      "learning_rate": 7.078651685393259e-05,
      "loss": 1.0554,
      "step": 257
    },
    {
      "epoch": 0.036271615352172076,
      "grad_norm": 2.0762598514556885,
      "learning_rate": 7.106741573033708e-05,
      "loss": 1.2578,
      "step": 258
    },
    {
      "epoch": 0.036412203008575846,
      "grad_norm": 2.0117056369781494,
      "learning_rate": 7.134831460674157e-05,
      "loss": 1.172,
      "step": 259
    },
    {
      "epoch": 0.036552790664979615,
      "grad_norm": 2.1322569847106934,
      "learning_rate": 7.162921348314608e-05,
      "loss": 1.197,
      "step": 260
    },
    {
      "epoch": 0.036693378321383384,
      "grad_norm": 2.1132771968841553,
      "learning_rate": 7.191011235955056e-05,
      "loss": 1.3144,
      "step": 261
    },
    {
      "epoch": 0.03683396597778715,
      "grad_norm": 2.0350265502929688,
      "learning_rate": 7.219101123595507e-05,
      "loss": 1.1994,
      "step": 262
    },
    {
      "epoch": 0.036974553634190915,
      "grad_norm": 2.016650915145874,
      "learning_rate": 7.247191011235956e-05,
      "loss": 1.116,
      "step": 263
    },
    {
      "epoch": 0.037115141290594685,
      "grad_norm": 1.9997117519378662,
      "learning_rate": 7.275280898876404e-05,
      "loss": 1.2052,
      "step": 264
    },
    {
      "epoch": 0.037255728946998454,
      "grad_norm": 2.043107509613037,
      "learning_rate": 7.303370786516854e-05,
      "loss": 0.9719,
      "step": 265
    },
    {
      "epoch": 0.03739631660340222,
      "grad_norm": 1.9556926488876343,
      "learning_rate": 7.331460674157304e-05,
      "loss": 1.098,
      "step": 266
    },
    {
      "epoch": 0.03753690425980599,
      "grad_norm": 2.0700576305389404,
      "learning_rate": 7.359550561797753e-05,
      "loss": 1.0785,
      "step": 267
    },
    {
      "epoch": 0.037677491916209754,
      "grad_norm": 2.1003987789154053,
      "learning_rate": 7.387640449438202e-05,
      "loss": 1.1215,
      "step": 268
    },
    {
      "epoch": 0.037818079572613524,
      "grad_norm": 2.437917709350586,
      "learning_rate": 7.415730337078653e-05,
      "loss": 1.0728,
      "step": 269
    },
    {
      "epoch": 0.03795866722901729,
      "grad_norm": 2.0074656009674072,
      "learning_rate": 7.443820224719101e-05,
      "loss": 1.1405,
      "step": 270
    },
    {
      "epoch": 0.03809925488542106,
      "grad_norm": 2.084153652191162,
      "learning_rate": 7.471910112359551e-05,
      "loss": 1.1492,
      "step": 271
    },
    {
      "epoch": 0.03823984254182483,
      "grad_norm": 2.122835159301758,
      "learning_rate": 7.500000000000001e-05,
      "loss": 0.9276,
      "step": 272
    },
    {
      "epoch": 0.03838043019822859,
      "grad_norm": 2.180314064025879,
      "learning_rate": 7.52808988764045e-05,
      "loss": 1.1885,
      "step": 273
    },
    {
      "epoch": 0.03852101785463236,
      "grad_norm": 2.369109630584717,
      "learning_rate": 7.556179775280899e-05,
      "loss": 1.2765,
      "step": 274
    },
    {
      "epoch": 0.03866160551103613,
      "grad_norm": 2.5607521533966064,
      "learning_rate": 7.584269662921349e-05,
      "loss": 1.1109,
      "step": 275
    },
    {
      "epoch": 0.0388021931674399,
      "grad_norm": 1.9126816987991333,
      "learning_rate": 7.612359550561798e-05,
      "loss": 1.2163,
      "step": 276
    },
    {
      "epoch": 0.03894278082384366,
      "grad_norm": 2.0749309062957764,
      "learning_rate": 7.640449438202247e-05,
      "loss": 1.1678,
      "step": 277
    },
    {
      "epoch": 0.03908336848024743,
      "grad_norm": 1.990980863571167,
      "learning_rate": 7.668539325842698e-05,
      "loss": 1.1352,
      "step": 278
    },
    {
      "epoch": 0.0392239561366512,
      "grad_norm": 1.94240140914917,
      "learning_rate": 7.696629213483147e-05,
      "loss": 0.9731,
      "step": 279
    },
    {
      "epoch": 0.03936454379305497,
      "grad_norm": 1.923457384109497,
      "learning_rate": 7.724719101123596e-05,
      "loss": 1.1118,
      "step": 280
    },
    {
      "epoch": 0.03950513144945874,
      "grad_norm": 2.148200035095215,
      "learning_rate": 7.752808988764046e-05,
      "loss": 1.0145,
      "step": 281
    },
    {
      "epoch": 0.0396457191058625,
      "grad_norm": 2.017226219177246,
      "learning_rate": 7.780898876404495e-05,
      "loss": 1.059,
      "step": 282
    },
    {
      "epoch": 0.03978630676226627,
      "grad_norm": 1.9769501686096191,
      "learning_rate": 7.808988764044944e-05,
      "loss": 1.103,
      "step": 283
    },
    {
      "epoch": 0.03992689441867004,
      "grad_norm": 1.8001704216003418,
      "learning_rate": 7.837078651685393e-05,
      "loss": 1.0417,
      "step": 284
    },
    {
      "epoch": 0.04006748207507381,
      "grad_norm": 1.9419679641723633,
      "learning_rate": 7.865168539325843e-05,
      "loss": 1.198,
      "step": 285
    },
    {
      "epoch": 0.04020806973147758,
      "grad_norm": 1.985488772392273,
      "learning_rate": 7.893258426966292e-05,
      "loss": 1.2187,
      "step": 286
    },
    {
      "epoch": 0.04034865738788134,
      "grad_norm": 2.0303990840911865,
      "learning_rate": 7.921348314606743e-05,
      "loss": 1.2952,
      "step": 287
    },
    {
      "epoch": 0.04048924504428511,
      "grad_norm": 2.178532123565674,
      "learning_rate": 7.949438202247192e-05,
      "loss": 1.1285,
      "step": 288
    },
    {
      "epoch": 0.04062983270068888,
      "grad_norm": 1.9010953903198242,
      "learning_rate": 7.97752808988764e-05,
      "loss": 1.3672,
      "step": 289
    },
    {
      "epoch": 0.04077042035709265,
      "grad_norm": 2.2792856693267822,
      "learning_rate": 8.00561797752809e-05,
      "loss": 1.1916,
      "step": 290
    },
    {
      "epoch": 0.04091100801349642,
      "grad_norm": 2.3124701976776123,
      "learning_rate": 8.03370786516854e-05,
      "loss": 1.071,
      "step": 291
    },
    {
      "epoch": 0.04105159566990018,
      "grad_norm": 2.770250082015991,
      "learning_rate": 8.061797752808989e-05,
      "loss": 1.112,
      "step": 292
    },
    {
      "epoch": 0.04119218332630395,
      "grad_norm": 2.3447165489196777,
      "learning_rate": 8.089887640449438e-05,
      "loss": 1.2814,
      "step": 293
    },
    {
      "epoch": 0.04133277098270772,
      "grad_norm": 2.8198485374450684,
      "learning_rate": 8.117977528089889e-05,
      "loss": 1.0931,
      "step": 294
    },
    {
      "epoch": 0.04147335863911149,
      "grad_norm": 2.048471689224243,
      "learning_rate": 8.146067415730337e-05,
      "loss": 1.2159,
      "step": 295
    },
    {
      "epoch": 0.04161394629551526,
      "grad_norm": 1.9437874555587769,
      "learning_rate": 8.174157303370788e-05,
      "loss": 1.2323,
      "step": 296
    },
    {
      "epoch": 0.04175453395191902,
      "grad_norm": 2.513530731201172,
      "learning_rate": 8.202247191011237e-05,
      "loss": 1.2376,
      "step": 297
    },
    {
      "epoch": 0.04189512160832279,
      "grad_norm": 1.920276403427124,
      "learning_rate": 8.230337078651685e-05,
      "loss": 1.3009,
      "step": 298
    },
    {
      "epoch": 0.04203570926472656,
      "grad_norm": 2.0422301292419434,
      "learning_rate": 8.258426966292135e-05,
      "loss": 1.0507,
      "step": 299
    },
    {
      "epoch": 0.04217629692113033,
      "grad_norm": 1.961079716682434,
      "learning_rate": 8.286516853932585e-05,
      "loss": 1.1055,
      "step": 300
    },
    {
      "epoch": 0.04231688457753409,
      "grad_norm": 2.009704351425171,
      "learning_rate": 8.314606741573034e-05,
      "loss": 1.0891,
      "step": 301
    },
    {
      "epoch": 0.04245747223393786,
      "grad_norm": 1.8657206296920776,
      "learning_rate": 8.342696629213483e-05,
      "loss": 1.0174,
      "step": 302
    },
    {
      "epoch": 0.04259805989034163,
      "grad_norm": 1.7488481998443604,
      "learning_rate": 8.370786516853934e-05,
      "loss": 1.2348,
      "step": 303
    },
    {
      "epoch": 0.0427386475467454,
      "grad_norm": 2.3507354259490967,
      "learning_rate": 8.398876404494382e-05,
      "loss": 1.0175,
      "step": 304
    },
    {
      "epoch": 0.042879235203149166,
      "grad_norm": 2.4048309326171875,
      "learning_rate": 8.426966292134831e-05,
      "loss": 1.1215,
      "step": 305
    },
    {
      "epoch": 0.04301982285955293,
      "grad_norm": 2.105654239654541,
      "learning_rate": 8.455056179775282e-05,
      "loss": 1.168,
      "step": 306
    },
    {
      "epoch": 0.0431604105159567,
      "grad_norm": 2.3572912216186523,
      "learning_rate": 8.483146067415731e-05,
      "loss": 1.0993,
      "step": 307
    },
    {
      "epoch": 0.043300998172360466,
      "grad_norm": 1.7615408897399902,
      "learning_rate": 8.51123595505618e-05,
      "loss": 1.1127,
      "step": 308
    },
    {
      "epoch": 0.043441585828764236,
      "grad_norm": 2.054922580718994,
      "learning_rate": 8.53932584269663e-05,
      "loss": 1.1241,
      "step": 309
    },
    {
      "epoch": 0.043582173485168005,
      "grad_norm": 2.2332963943481445,
      "learning_rate": 8.567415730337079e-05,
      "loss": 1.1968,
      "step": 310
    },
    {
      "epoch": 0.04372276114157177,
      "grad_norm": 2.0861120223999023,
      "learning_rate": 8.595505617977528e-05,
      "loss": 1.093,
      "step": 311
    },
    {
      "epoch": 0.043863348797975536,
      "grad_norm": 1.9801466464996338,
      "learning_rate": 8.623595505617979e-05,
      "loss": 1.0117,
      "step": 312
    },
    {
      "epoch": 0.044003936454379305,
      "grad_norm": 2.4185404777526855,
      "learning_rate": 8.651685393258427e-05,
      "loss": 1.0047,
      "step": 313
    },
    {
      "epoch": 0.044144524110783075,
      "grad_norm": 2.201678991317749,
      "learning_rate": 8.679775280898876e-05,
      "loss": 1.0933,
      "step": 314
    },
    {
      "epoch": 0.044285111767186844,
      "grad_norm": 2.145491361618042,
      "learning_rate": 8.707865168539327e-05,
      "loss": 1.2162,
      "step": 315
    },
    {
      "epoch": 0.044425699423590606,
      "grad_norm": 2.2059593200683594,
      "learning_rate": 8.735955056179776e-05,
      "loss": 1.1908,
      "step": 316
    },
    {
      "epoch": 0.044566287079994375,
      "grad_norm": 2.0032339096069336,
      "learning_rate": 8.764044943820225e-05,
      "loss": 1.1617,
      "step": 317
    },
    {
      "epoch": 0.044706874736398144,
      "grad_norm": 2.3210153579711914,
      "learning_rate": 8.792134831460675e-05,
      "loss": 1.0734,
      "step": 318
    },
    {
      "epoch": 0.044847462392801914,
      "grad_norm": 2.0029022693634033,
      "learning_rate": 8.820224719101124e-05,
      "loss": 1.1205,
      "step": 319
    },
    {
      "epoch": 0.04498805004920568,
      "grad_norm": 1.9865777492523193,
      "learning_rate": 8.848314606741573e-05,
      "loss": 1.1574,
      "step": 320
    },
    {
      "epoch": 0.045128637705609445,
      "grad_norm": 2.024789810180664,
      "learning_rate": 8.876404494382022e-05,
      "loss": 1.1571,
      "step": 321
    },
    {
      "epoch": 0.045269225362013214,
      "grad_norm": 2.3479764461517334,
      "learning_rate": 8.904494382022473e-05,
      "loss": 1.2468,
      "step": 322
    },
    {
      "epoch": 0.045409813018416983,
      "grad_norm": 2.052243709564209,
      "learning_rate": 8.932584269662921e-05,
      "loss": 1.1533,
      "step": 323
    },
    {
      "epoch": 0.04555040067482075,
      "grad_norm": 1.8498810529708862,
      "learning_rate": 8.960674157303372e-05,
      "loss": 1.1715,
      "step": 324
    },
    {
      "epoch": 0.04569098833122452,
      "grad_norm": 2.0218467712402344,
      "learning_rate": 8.988764044943821e-05,
      "loss": 1.1039,
      "step": 325
    },
    {
      "epoch": 0.045831575987628284,
      "grad_norm": 2.101044178009033,
      "learning_rate": 9.01685393258427e-05,
      "loss": 1.1501,
      "step": 326
    },
    {
      "epoch": 0.04597216364403205,
      "grad_norm": 2.143216609954834,
      "learning_rate": 9.04494382022472e-05,
      "loss": 1.1552,
      "step": 327
    },
    {
      "epoch": 0.04611275130043582,
      "grad_norm": 1.831234335899353,
      "learning_rate": 9.07303370786517e-05,
      "loss": 1.099,
      "step": 328
    },
    {
      "epoch": 0.04625333895683959,
      "grad_norm": 1.945765495300293,
      "learning_rate": 9.101123595505618e-05,
      "loss": 1.0431,
      "step": 329
    },
    {
      "epoch": 0.046393926613243354,
      "grad_norm": 2.026486873626709,
      "learning_rate": 9.129213483146067e-05,
      "loss": 1.1586,
      "step": 330
    },
    {
      "epoch": 0.04653451426964712,
      "grad_norm": 1.7436408996582031,
      "learning_rate": 9.157303370786518e-05,
      "loss": 1.1859,
      "step": 331
    },
    {
      "epoch": 0.04667510192605089,
      "grad_norm": 2.026146411895752,
      "learning_rate": 9.185393258426966e-05,
      "loss": 1.0987,
      "step": 332
    },
    {
      "epoch": 0.04681568958245466,
      "grad_norm": 1.8193981647491455,
      "learning_rate": 9.213483146067416e-05,
      "loss": 0.8461,
      "step": 333
    },
    {
      "epoch": 0.04695627723885843,
      "grad_norm": 1.8346997499465942,
      "learning_rate": 9.241573033707866e-05,
      "loss": 1.1395,
      "step": 334
    },
    {
      "epoch": 0.04709686489526219,
      "grad_norm": 2.025394916534424,
      "learning_rate": 9.269662921348315e-05,
      "loss": 1.1004,
      "step": 335
    },
    {
      "epoch": 0.04723745255166596,
      "grad_norm": 1.9196642637252808,
      "learning_rate": 9.297752808988764e-05,
      "loss": 1.2217,
      "step": 336
    },
    {
      "epoch": 0.04737804020806973,
      "grad_norm": 2.174111843109131,
      "learning_rate": 9.325842696629214e-05,
      "loss": 0.9076,
      "step": 337
    },
    {
      "epoch": 0.0475186278644735,
      "grad_norm": 1.9009904861450195,
      "learning_rate": 9.353932584269663e-05,
      "loss": 1.0436,
      "step": 338
    },
    {
      "epoch": 0.04765921552087727,
      "grad_norm": 1.8688868284225464,
      "learning_rate": 9.382022471910112e-05,
      "loss": 0.9755,
      "step": 339
    },
    {
      "epoch": 0.04779980317728103,
      "grad_norm": 1.7450469732284546,
      "learning_rate": 9.410112359550563e-05,
      "loss": 1.2946,
      "step": 340
    },
    {
      "epoch": 0.0479403908336848,
      "grad_norm": 1.7886625528335571,
      "learning_rate": 9.438202247191012e-05,
      "loss": 1.0457,
      "step": 341
    },
    {
      "epoch": 0.04808097849008857,
      "grad_norm": 1.8535023927688599,
      "learning_rate": 9.466292134831461e-05,
      "loss": 1.1785,
      "step": 342
    },
    {
      "epoch": 0.04822156614649234,
      "grad_norm": 1.8058137893676758,
      "learning_rate": 9.49438202247191e-05,
      "loss": 1.0187,
      "step": 343
    },
    {
      "epoch": 0.04836215380289611,
      "grad_norm": 2.078225612640381,
      "learning_rate": 9.52247191011236e-05,
      "loss": 1.1926,
      "step": 344
    },
    {
      "epoch": 0.04850274145929987,
      "grad_norm": 1.9953665733337402,
      "learning_rate": 9.550561797752809e-05,
      "loss": 0.9939,
      "step": 345
    },
    {
      "epoch": 0.04864332911570364,
      "grad_norm": 1.8920668363571167,
      "learning_rate": 9.578651685393259e-05,
      "loss": 1.1864,
      "step": 346
    },
    {
      "epoch": 0.04878391677210741,
      "grad_norm": 2.0446574687957764,
      "learning_rate": 9.606741573033708e-05,
      "loss": 1.1335,
      "step": 347
    },
    {
      "epoch": 0.04892450442851118,
      "grad_norm": 2.398526906967163,
      "learning_rate": 9.634831460674157e-05,
      "loss": 1.0252,
      "step": 348
    },
    {
      "epoch": 0.04906509208491495,
      "grad_norm": 1.8374841213226318,
      "learning_rate": 9.662921348314608e-05,
      "loss": 1.0459,
      "step": 349
    },
    {
      "epoch": 0.04920567974131871,
      "grad_norm": 1.9044111967086792,
      "learning_rate": 9.691011235955057e-05,
      "loss": 1.0905,
      "step": 350
    },
    {
      "epoch": 0.04934626739772248,
      "grad_norm": 1.8970874547958374,
      "learning_rate": 9.719101123595506e-05,
      "loss": 1.2355,
      "step": 351
    },
    {
      "epoch": 0.04948685505412625,
      "grad_norm": 1.911007046699524,
      "learning_rate": 9.747191011235956e-05,
      "loss": 1.2255,
      "step": 352
    },
    {
      "epoch": 0.04962744271053002,
      "grad_norm": 1.7863138914108276,
      "learning_rate": 9.775280898876405e-05,
      "loss": 1.0012,
      "step": 353
    },
    {
      "epoch": 0.04976803036693378,
      "grad_norm": 1.8198041915893555,
      "learning_rate": 9.803370786516854e-05,
      "loss": 1.1793,
      "step": 354
    },
    {
      "epoch": 0.04990861802333755,
      "grad_norm": 1.8485674858093262,
      "learning_rate": 9.831460674157303e-05,
      "loss": 1.1655,
      "step": 355
    },
    {
      "epoch": 0.05004920567974132,
      "grad_norm": 1.8849211931228638,
      "learning_rate": 9.859550561797754e-05,
      "loss": 1.1498,
      "step": 356
    },
    {
      "epoch": 0.05018979333614509,
      "grad_norm": 1.7031837701797485,
      "learning_rate": 9.887640449438202e-05,
      "loss": 1.2748,
      "step": 357
    },
    {
      "epoch": 0.05033038099254886,
      "grad_norm": 1.8861690759658813,
      "learning_rate": 9.915730337078653e-05,
      "loss": 1.2024,
      "step": 358
    },
    {
      "epoch": 0.05047096864895262,
      "grad_norm": 1.842575192451477,
      "learning_rate": 9.943820224719102e-05,
      "loss": 1.1389,
      "step": 359
    },
    {
      "epoch": 0.05061155630535639,
      "grad_norm": 1.7442415952682495,
      "learning_rate": 9.971910112359551e-05,
      "loss": 1.0326,
      "step": 360
    },
    {
      "epoch": 0.05075214396176016,
      "grad_norm": 1.705456256866455,
      "learning_rate": 0.0001,
      "loss": 1.1371,
      "step": 361
    },
    {
      "epoch": 0.050892731618163926,
      "grad_norm": 1.7730551958084106,
      "learning_rate": 0.00010028089887640451,
      "loss": 1.0203,
      "step": 362
    },
    {
      "epoch": 0.051033319274567696,
      "grad_norm": 1.8496006727218628,
      "learning_rate": 0.00010056179775280899,
      "loss": 0.9351,
      "step": 363
    },
    {
      "epoch": 0.05117390693097146,
      "grad_norm": 1.6997305154800415,
      "learning_rate": 0.00010084269662921348,
      "loss": 1.2247,
      "step": 364
    },
    {
      "epoch": 0.05131449458737523,
      "grad_norm": 1.8198490142822266,
      "learning_rate": 0.00010112359550561799,
      "loss": 1.1204,
      "step": 365
    },
    {
      "epoch": 0.051455082243778996,
      "grad_norm": 1.684792399406433,
      "learning_rate": 0.00010140449438202248,
      "loss": 1.1637,
      "step": 366
    },
    {
      "epoch": 0.051595669900182765,
      "grad_norm": 2.0518200397491455,
      "learning_rate": 0.00010168539325842696,
      "loss": 1.1182,
      "step": 367
    },
    {
      "epoch": 0.051736257556586535,
      "grad_norm": 1.8118394613265991,
      "learning_rate": 0.00010196629213483147,
      "loss": 1.0646,
      "step": 368
    },
    {
      "epoch": 0.0518768452129903,
      "grad_norm": 1.8747377395629883,
      "learning_rate": 0.00010224719101123596,
      "loss": 1.0507,
      "step": 369
    },
    {
      "epoch": 0.052017432869394066,
      "grad_norm": 1.9094622135162354,
      "learning_rate": 0.00010252808988764044,
      "loss": 0.9639,
      "step": 370
    },
    {
      "epoch": 0.052158020525797835,
      "grad_norm": 1.9474825859069824,
      "learning_rate": 0.00010280898876404495,
      "loss": 1.2047,
      "step": 371
    },
    {
      "epoch": 0.052298608182201604,
      "grad_norm": 2.3158607482910156,
      "learning_rate": 0.00010308988764044944,
      "loss": 1.191,
      "step": 372
    },
    {
      "epoch": 0.052439195838605374,
      "grad_norm": 2.24672269821167,
      "learning_rate": 0.00010337078651685395,
      "loss": 1.1924,
      "step": 373
    },
    {
      "epoch": 0.052579783495009136,
      "grad_norm": 1.8184025287628174,
      "learning_rate": 0.00010365168539325843,
      "loss": 1.0638,
      "step": 374
    },
    {
      "epoch": 0.052720371151412905,
      "grad_norm": 1.8283076286315918,
      "learning_rate": 0.00010393258426966293,
      "loss": 1.0986,
      "step": 375
    },
    {
      "epoch": 0.052860958807816674,
      "grad_norm": 1.9615730047225952,
      "learning_rate": 0.00010421348314606742,
      "loss": 1.082,
      "step": 376
    },
    {
      "epoch": 0.05300154646422044,
      "grad_norm": 2.1095666885375977,
      "learning_rate": 0.00010449438202247193,
      "loss": 1.0846,
      "step": 377
    },
    {
      "epoch": 0.05314213412062421,
      "grad_norm": 1.998531699180603,
      "learning_rate": 0.00010477528089887641,
      "loss": 1.0905,
      "step": 378
    },
    {
      "epoch": 0.053282721777027975,
      "grad_norm": 1.856663465499878,
      "learning_rate": 0.0001050561797752809,
      "loss": 1.0076,
      "step": 379
    },
    {
      "epoch": 0.053423309433431744,
      "grad_norm": 2.3263988494873047,
      "learning_rate": 0.00010533707865168541,
      "loss": 1.2179,
      "step": 380
    },
    {
      "epoch": 0.05356389708983551,
      "grad_norm": 2.1896450519561768,
      "learning_rate": 0.00010561797752808989,
      "loss": 0.9622,
      "step": 381
    },
    {
      "epoch": 0.05370448474623928,
      "grad_norm": 2.013543128967285,
      "learning_rate": 0.00010589887640449438,
      "loss": 1.0705,
      "step": 382
    },
    {
      "epoch": 0.053845072402643045,
      "grad_norm": 1.7349814176559448,
      "learning_rate": 0.00010617977528089889,
      "loss": 1.2628,
      "step": 383
    },
    {
      "epoch": 0.053985660059046814,
      "grad_norm": 2.307243585586548,
      "learning_rate": 0.00010646067415730338,
      "loss": 1.1435,
      "step": 384
    },
    {
      "epoch": 0.05412624771545058,
      "grad_norm": 1.7858071327209473,
      "learning_rate": 0.00010674157303370786,
      "loss": 0.9407,
      "step": 385
    },
    {
      "epoch": 0.05426683537185435,
      "grad_norm": 1.7288439273834229,
      "learning_rate": 0.00010702247191011237,
      "loss": 1.0302,
      "step": 386
    },
    {
      "epoch": 0.05440742302825812,
      "grad_norm": 2.2780795097351074,
      "learning_rate": 0.00010730337078651686,
      "loss": 0.9335,
      "step": 387
    },
    {
      "epoch": 0.054548010684661884,
      "grad_norm": 1.8261387348175049,
      "learning_rate": 0.00010758426966292134,
      "loss": 1.0925,
      "step": 388
    },
    {
      "epoch": 0.05468859834106565,
      "grad_norm": 1.704158067703247,
      "learning_rate": 0.00010786516853932584,
      "loss": 1.1602,
      "step": 389
    },
    {
      "epoch": 0.05482918599746942,
      "grad_norm": 2.0686705112457275,
      "learning_rate": 0.00010814606741573035,
      "loss": 1.0942,
      "step": 390
    },
    {
      "epoch": 0.05496977365387319,
      "grad_norm": 1.8318482637405396,
      "learning_rate": 0.00010842696629213484,
      "loss": 1.1306,
      "step": 391
    },
    {
      "epoch": 0.05511036131027696,
      "grad_norm": 2.0179178714752197,
      "learning_rate": 0.00010870786516853932,
      "loss": 0.9653,
      "step": 392
    },
    {
      "epoch": 0.05525094896668072,
      "grad_norm": 1.7563968896865845,
      "learning_rate": 0.00010898876404494383,
      "loss": 1.2019,
      "step": 393
    },
    {
      "epoch": 0.05539153662308449,
      "grad_norm": 2.065092086791992,
      "learning_rate": 0.00010926966292134832,
      "loss": 1.1277,
      "step": 394
    },
    {
      "epoch": 0.05553212427948826,
      "grad_norm": 1.8390979766845703,
      "learning_rate": 0.0001095505617977528,
      "loss": 1.237,
      "step": 395
    },
    {
      "epoch": 0.05567271193589203,
      "grad_norm": 1.858350396156311,
      "learning_rate": 0.00010983146067415731,
      "loss": 1.161,
      "step": 396
    },
    {
      "epoch": 0.0558132995922958,
      "grad_norm": 1.7543202638626099,
      "learning_rate": 0.0001101123595505618,
      "loss": 0.9903,
      "step": 397
    },
    {
      "epoch": 0.05595388724869956,
      "grad_norm": 1.5905228853225708,
      "learning_rate": 0.00011039325842696631,
      "loss": 1.1832,
      "step": 398
    },
    {
      "epoch": 0.05609447490510333,
      "grad_norm": 1.792826533317566,
      "learning_rate": 0.00011067415730337079,
      "loss": 1.0221,
      "step": 399
    },
    {
      "epoch": 0.0562350625615071,
      "grad_norm": 1.9710779190063477,
      "learning_rate": 0.00011095505617977528,
      "loss": 1.0721,
      "step": 400
    },
    {
      "epoch": 0.05637565021791087,
      "grad_norm": 1.7682522535324097,
      "learning_rate": 0.00011123595505617979,
      "loss": 1.0015,
      "step": 401
    },
    {
      "epoch": 0.05651623787431464,
      "grad_norm": 1.6694178581237793,
      "learning_rate": 0.00011151685393258427,
      "loss": 1.223,
      "step": 402
    },
    {
      "epoch": 0.0566568255307184,
      "grad_norm": 1.5102684497833252,
      "learning_rate": 0.00011179775280898877,
      "loss": 1.2068,
      "step": 403
    },
    {
      "epoch": 0.05679741318712217,
      "grad_norm": 2.240840435028076,
      "learning_rate": 0.00011207865168539326,
      "loss": 1.0236,
      "step": 404
    },
    {
      "epoch": 0.05693800084352594,
      "grad_norm": 1.8824405670166016,
      "learning_rate": 0.00011235955056179777,
      "loss": 1.2227,
      "step": 405
    },
    {
      "epoch": 0.05707858849992971,
      "grad_norm": 1.743129014968872,
      "learning_rate": 0.00011264044943820225,
      "loss": 1.1667,
      "step": 406
    },
    {
      "epoch": 0.05721917615633347,
      "grad_norm": 1.730711817741394,
      "learning_rate": 0.00011292134831460674,
      "loss": 1.1722,
      "step": 407
    },
    {
      "epoch": 0.05735976381273724,
      "grad_norm": 1.920936107635498,
      "learning_rate": 0.00011320224719101125,
      "loss": 1.3821,
      "step": 408
    },
    {
      "epoch": 0.05750035146914101,
      "grad_norm": 2.180739641189575,
      "learning_rate": 0.00011348314606741574,
      "loss": 1.0573,
      "step": 409
    },
    {
      "epoch": 0.05764093912554478,
      "grad_norm": 2.103883981704712,
      "learning_rate": 0.00011376404494382022,
      "loss": 0.9449,
      "step": 410
    },
    {
      "epoch": 0.05778152678194855,
      "grad_norm": 1.7258987426757812,
      "learning_rate": 0.00011404494382022473,
      "loss": 1.2481,
      "step": 411
    },
    {
      "epoch": 0.05792211443835231,
      "grad_norm": 1.70970618724823,
      "learning_rate": 0.00011432584269662922,
      "loss": 1.0839,
      "step": 412
    },
    {
      "epoch": 0.05806270209475608,
      "grad_norm": 1.6732537746429443,
      "learning_rate": 0.0001146067415730337,
      "loss": 1.0663,
      "step": 413
    },
    {
      "epoch": 0.05820328975115985,
      "grad_norm": 2.24345326423645,
      "learning_rate": 0.0001148876404494382,
      "loss": 1.1719,
      "step": 414
    },
    {
      "epoch": 0.05834387740756362,
      "grad_norm": 2.0123746395111084,
      "learning_rate": 0.0001151685393258427,
      "loss": 1.0762,
      "step": 415
    },
    {
      "epoch": 0.058484465063967386,
      "grad_norm": 1.678879737854004,
      "learning_rate": 0.0001154494382022472,
      "loss": 1.1174,
      "step": 416
    },
    {
      "epoch": 0.05862505272037115,
      "grad_norm": 1.9418655633926392,
      "learning_rate": 0.00011573033707865168,
      "loss": 1.0919,
      "step": 417
    },
    {
      "epoch": 0.05876564037677492,
      "grad_norm": 1.7571818828582764,
      "learning_rate": 0.00011601123595505619,
      "loss": 1.0777,
      "step": 418
    },
    {
      "epoch": 0.05890622803317869,
      "grad_norm": 1.7855383157730103,
      "learning_rate": 0.00011629213483146068,
      "loss": 0.943,
      "step": 419
    },
    {
      "epoch": 0.059046815689582456,
      "grad_norm": 1.8255548477172852,
      "learning_rate": 0.00011657303370786516,
      "loss": 1.0377,
      "step": 420
    },
    {
      "epoch": 0.059187403345986225,
      "grad_norm": 1.7663806676864624,
      "learning_rate": 0.00011685393258426967,
      "loss": 0.9912,
      "step": 421
    },
    {
      "epoch": 0.05932799100238999,
      "grad_norm": 1.754003882408142,
      "learning_rate": 0.00011713483146067416,
      "loss": 1.1618,
      "step": 422
    },
    {
      "epoch": 0.05946857865879376,
      "grad_norm": 1.8772457838058472,
      "learning_rate": 0.00011741573033707867,
      "loss": 0.9797,
      "step": 423
    },
    {
      "epoch": 0.059609166315197526,
      "grad_norm": 2.236052989959717,
      "learning_rate": 0.00011769662921348315,
      "loss": 0.9297,
      "step": 424
    },
    {
      "epoch": 0.059749753971601295,
      "grad_norm": 1.421188473701477,
      "learning_rate": 0.00011797752808988764,
      "loss": 1.1765,
      "step": 425
    },
    {
      "epoch": 0.059890341628005064,
      "grad_norm": 2.3465042114257812,
      "learning_rate": 0.00011825842696629215,
      "loss": 1.0487,
      "step": 426
    },
    {
      "epoch": 0.06003092928440883,
      "grad_norm": 1.721390962600708,
      "learning_rate": 0.00011853932584269663,
      "loss": 1.0607,
      "step": 427
    },
    {
      "epoch": 0.060171516940812596,
      "grad_norm": 1.7980175018310547,
      "learning_rate": 0.00011882022471910112,
      "loss": 1.1593,
      "step": 428
    },
    {
      "epoch": 0.060312104597216365,
      "grad_norm": 2.1639087200164795,
      "learning_rate": 0.00011910112359550563,
      "loss": 0.9912,
      "step": 429
    },
    {
      "epoch": 0.060452692253620134,
      "grad_norm": 1.771360158920288,
      "learning_rate": 0.00011938202247191013,
      "loss": 1.1965,
      "step": 430
    },
    {
      "epoch": 0.0605932799100239,
      "grad_norm": 1.9211609363555908,
      "learning_rate": 0.00011966292134831461,
      "loss": 1.0698,
      "step": 431
    },
    {
      "epoch": 0.060733867566427666,
      "grad_norm": 2.00108003616333,
      "learning_rate": 0.0001199438202247191,
      "loss": 0.9576,
      "step": 432
    },
    {
      "epoch": 0.060874455222831435,
      "grad_norm": 1.5641167163848877,
      "learning_rate": 0.00012022471910112361,
      "loss": 1.0633,
      "step": 433
    },
    {
      "epoch": 0.061015042879235204,
      "grad_norm": 1.8282947540283203,
      "learning_rate": 0.00012050561797752809,
      "loss": 1.0306,
      "step": 434
    },
    {
      "epoch": 0.06115563053563897,
      "grad_norm": 1.892506718635559,
      "learning_rate": 0.00012078651685393258,
      "loss": 1.1866,
      "step": 435
    },
    {
      "epoch": 0.061296218192042735,
      "grad_norm": 1.9772627353668213,
      "learning_rate": 0.00012106741573033709,
      "loss": 1.0723,
      "step": 436
    },
    {
      "epoch": 0.061436805848446505,
      "grad_norm": 1.923082947731018,
      "learning_rate": 0.00012134831460674158,
      "loss": 0.9867,
      "step": 437
    },
    {
      "epoch": 0.061577393504850274,
      "grad_norm": 1.7222837209701538,
      "learning_rate": 0.00012162921348314606,
      "loss": 1.1901,
      "step": 438
    },
    {
      "epoch": 0.06171798116125404,
      "grad_norm": 1.622645378112793,
      "learning_rate": 0.00012191011235955057,
      "loss": 1.1384,
      "step": 439
    },
    {
      "epoch": 0.06185856881765781,
      "grad_norm": 1.6764731407165527,
      "learning_rate": 0.00012219101123595507,
      "loss": 1.087,
      "step": 440
    },
    {
      "epoch": 0.061999156474061574,
      "grad_norm": 1.8648253679275513,
      "learning_rate": 0.00012247191011235955,
      "loss": 0.9271,
      "step": 441
    },
    {
      "epoch": 0.062139744130465344,
      "grad_norm": 2.3056352138519287,
      "learning_rate": 0.00012275280898876403,
      "loss": 1.1476,
      "step": 442
    },
    {
      "epoch": 0.06228033178686911,
      "grad_norm": 1.8037205934524536,
      "learning_rate": 0.00012303370786516854,
      "loss": 1.2106,
      "step": 443
    },
    {
      "epoch": 0.06242091944327288,
      "grad_norm": 1.7559583187103271,
      "learning_rate": 0.00012331460674157305,
      "loss": 1.1545,
      "step": 444
    },
    {
      "epoch": 0.06256150709967664,
      "grad_norm": 2.0375609397888184,
      "learning_rate": 0.00012359550561797752,
      "loss": 1.2179,
      "step": 445
    },
    {
      "epoch": 0.06270209475608042,
      "grad_norm": 1.6765224933624268,
      "learning_rate": 0.00012387640449438203,
      "loss": 1.0761,
      "step": 446
    },
    {
      "epoch": 0.06284268241248418,
      "grad_norm": 1.9799795150756836,
      "learning_rate": 0.00012415730337078654,
      "loss": 1.1826,
      "step": 447
    },
    {
      "epoch": 0.06298327006888794,
      "grad_norm": 1.898092269897461,
      "learning_rate": 0.00012443820224719102,
      "loss": 1.0829,
      "step": 448
    },
    {
      "epoch": 0.06312385772529172,
      "grad_norm": 1.8634573221206665,
      "learning_rate": 0.0001247191011235955,
      "loss": 1.0304,
      "step": 449
    },
    {
      "epoch": 0.06326444538169548,
      "grad_norm": 1.8832370042800903,
      "learning_rate": 0.000125,
      "loss": 1.1452,
      "step": 450
    },
    {
      "epoch": 0.06340503303809926,
      "grad_norm": 1.8605411052703857,
      "learning_rate": 0.0001252808988764045,
      "loss": 0.9622,
      "step": 451
    },
    {
      "epoch": 0.06354562069450302,
      "grad_norm": 1.773528814315796,
      "learning_rate": 0.000125561797752809,
      "loss": 1.1905,
      "step": 452
    },
    {
      "epoch": 0.06368620835090678,
      "grad_norm": 1.866137146949768,
      "learning_rate": 0.0001258426966292135,
      "loss": 1.1053,
      "step": 453
    },
    {
      "epoch": 0.06382679600731056,
      "grad_norm": 1.7090567350387573,
      "learning_rate": 0.00012612359550561797,
      "loss": 1.1265,
      "step": 454
    },
    {
      "epoch": 0.06396738366371432,
      "grad_norm": 1.7179371118545532,
      "learning_rate": 0.00012640449438202248,
      "loss": 0.9163,
      "step": 455
    },
    {
      "epoch": 0.0641079713201181,
      "grad_norm": 1.9382511377334595,
      "learning_rate": 0.00012668539325842696,
      "loss": 1.1028,
      "step": 456
    },
    {
      "epoch": 0.06424855897652186,
      "grad_norm": 1.6923611164093018,
      "learning_rate": 0.00012696629213483147,
      "loss": 0.9442,
      "step": 457
    },
    {
      "epoch": 0.06438914663292562,
      "grad_norm": 1.7376362085342407,
      "learning_rate": 0.00012724719101123597,
      "loss": 0.914,
      "step": 458
    },
    {
      "epoch": 0.0645297342893294,
      "grad_norm": 1.9974411725997925,
      "learning_rate": 0.00012752808988764045,
      "loss": 0.9906,
      "step": 459
    },
    {
      "epoch": 0.06467032194573316,
      "grad_norm": 1.7774581909179688,
      "learning_rate": 0.00012780898876404496,
      "loss": 1.1434,
      "step": 460
    },
    {
      "epoch": 0.06481090960213694,
      "grad_norm": 1.5525000095367432,
      "learning_rate": 0.00012808988764044944,
      "loss": 1.1474,
      "step": 461
    },
    {
      "epoch": 0.0649514972585407,
      "grad_norm": 2.11816668510437,
      "learning_rate": 0.00012837078651685394,
      "loss": 1.0836,
      "step": 462
    },
    {
      "epoch": 0.06509208491494446,
      "grad_norm": 1.9258075952529907,
      "learning_rate": 0.00012865168539325842,
      "loss": 1.1757,
      "step": 463
    },
    {
      "epoch": 0.06523267257134824,
      "grad_norm": 1.952625036239624,
      "learning_rate": 0.00012893258426966293,
      "loss": 1.0129,
      "step": 464
    },
    {
      "epoch": 0.065373260227752,
      "grad_norm": 1.868016242980957,
      "learning_rate": 0.00012921348314606744,
      "loss": 1.2818,
      "step": 465
    },
    {
      "epoch": 0.06551384788415578,
      "grad_norm": 1.664654016494751,
      "learning_rate": 0.00012949438202247192,
      "loss": 1.0918,
      "step": 466
    },
    {
      "epoch": 0.06565443554055954,
      "grad_norm": 1.638277292251587,
      "learning_rate": 0.0001297752808988764,
      "loss": 1.1189,
      "step": 467
    },
    {
      "epoch": 0.0657950231969633,
      "grad_norm": 1.5944491624832153,
      "learning_rate": 0.0001300561797752809,
      "loss": 1.1991,
      "step": 468
    },
    {
      "epoch": 0.06593561085336708,
      "grad_norm": 1.966022253036499,
      "learning_rate": 0.0001303370786516854,
      "loss": 0.9867,
      "step": 469
    },
    {
      "epoch": 0.06607619850977084,
      "grad_norm": 1.5669136047363281,
      "learning_rate": 0.00013061797752808989,
      "loss": 1.2034,
      "step": 470
    },
    {
      "epoch": 0.06621678616617462,
      "grad_norm": 2.050879716873169,
      "learning_rate": 0.0001308988764044944,
      "loss": 1.3139,
      "step": 471
    },
    {
      "epoch": 0.06635737382257838,
      "grad_norm": 1.7758675813674927,
      "learning_rate": 0.0001311797752808989,
      "loss": 1.0843,
      "step": 472
    },
    {
      "epoch": 0.06649796147898214,
      "grad_norm": 1.9246795177459717,
      "learning_rate": 0.00013146067415730338,
      "loss": 1.1369,
      "step": 473
    },
    {
      "epoch": 0.06663854913538592,
      "grad_norm": 1.8221930265426636,
      "learning_rate": 0.00013174157303370786,
      "loss": 1.1094,
      "step": 474
    },
    {
      "epoch": 0.06677913679178968,
      "grad_norm": 1.9627691507339478,
      "learning_rate": 0.00013202247191011236,
      "loss": 1.1141,
      "step": 475
    },
    {
      "epoch": 0.06691972444819345,
      "grad_norm": 1.95071280002594,
      "learning_rate": 0.00013230337078651687,
      "loss": 1.0104,
      "step": 476
    },
    {
      "epoch": 0.06706031210459722,
      "grad_norm": 2.104509115219116,
      "learning_rate": 0.00013258426966292135,
      "loss": 1.1799,
      "step": 477
    },
    {
      "epoch": 0.06720089976100098,
      "grad_norm": 1.7447333335876465,
      "learning_rate": 0.00013286516853932586,
      "loss": 1.1466,
      "step": 478
    },
    {
      "epoch": 0.06734148741740476,
      "grad_norm": 1.6265498399734497,
      "learning_rate": 0.00013314606741573034,
      "loss": 1.2452,
      "step": 479
    },
    {
      "epoch": 0.06748207507380852,
      "grad_norm": 1.9182617664337158,
      "learning_rate": 0.00013342696629213484,
      "loss": 0.9609,
      "step": 480
    },
    {
      "epoch": 0.0676226627302123,
      "grad_norm": 1.8662413358688354,
      "learning_rate": 0.00013370786516853932,
      "loss": 0.9989,
      "step": 481
    },
    {
      "epoch": 0.06776325038661606,
      "grad_norm": 1.6455252170562744,
      "learning_rate": 0.00013398876404494383,
      "loss": 1.1521,
      "step": 482
    },
    {
      "epoch": 0.06790383804301982,
      "grad_norm": 1.578442931175232,
      "learning_rate": 0.00013426966292134833,
      "loss": 1.1742,
      "step": 483
    },
    {
      "epoch": 0.0680444256994236,
      "grad_norm": 1.5100979804992676,
      "learning_rate": 0.0001345505617977528,
      "loss": 1.1147,
      "step": 484
    },
    {
      "epoch": 0.06818501335582736,
      "grad_norm": 1.8408451080322266,
      "learning_rate": 0.00013483146067415732,
      "loss": 1.2507,
      "step": 485
    },
    {
      "epoch": 0.06832560101223113,
      "grad_norm": 1.7546958923339844,
      "learning_rate": 0.0001351123595505618,
      "loss": 1.1536,
      "step": 486
    },
    {
      "epoch": 0.0684661886686349,
      "grad_norm": 1.6906131505966187,
      "learning_rate": 0.0001353932584269663,
      "loss": 1.078,
      "step": 487
    },
    {
      "epoch": 0.06860677632503866,
      "grad_norm": 1.734832525253296,
      "learning_rate": 0.00013567415730337078,
      "loss": 1.0737,
      "step": 488
    },
    {
      "epoch": 0.06874736398144243,
      "grad_norm": 1.392675757408142,
      "learning_rate": 0.0001359550561797753,
      "loss": 1.207,
      "step": 489
    },
    {
      "epoch": 0.0688879516378462,
      "grad_norm": 1.7717937231063843,
      "learning_rate": 0.0001362359550561798,
      "loss": 1.189,
      "step": 490
    },
    {
      "epoch": 0.06902853929424997,
      "grad_norm": 1.5874918699264526,
      "learning_rate": 0.00013651685393258428,
      "loss": 1.2526,
      "step": 491
    },
    {
      "epoch": 0.06916912695065373,
      "grad_norm": 1.9996018409729004,
      "learning_rate": 0.00013679775280898876,
      "loss": 1.2061,
      "step": 492
    },
    {
      "epoch": 0.0693097146070575,
      "grad_norm": 1.8376109600067139,
      "learning_rate": 0.00013707865168539326,
      "loss": 1.0134,
      "step": 493
    },
    {
      "epoch": 0.06945030226346127,
      "grad_norm": 1.7855921983718872,
      "learning_rate": 0.00013735955056179777,
      "loss": 0.9627,
      "step": 494
    },
    {
      "epoch": 0.06959088991986503,
      "grad_norm": 1.604891300201416,
      "learning_rate": 0.00013764044943820225,
      "loss": 1.0695,
      "step": 495
    },
    {
      "epoch": 0.06973147757626881,
      "grad_norm": 1.878560185432434,
      "learning_rate": 0.00013792134831460675,
      "loss": 0.9567,
      "step": 496
    },
    {
      "epoch": 0.06987206523267257,
      "grad_norm": 1.6329827308654785,
      "learning_rate": 0.00013820224719101123,
      "loss": 1.1128,
      "step": 497
    },
    {
      "epoch": 0.07001265288907633,
      "grad_norm": 1.6349804401397705,
      "learning_rate": 0.00013848314606741574,
      "loss": 1.1287,
      "step": 498
    },
    {
      "epoch": 0.07015324054548011,
      "grad_norm": 2.2976999282836914,
      "learning_rate": 0.00013876404494382022,
      "loss": 1.2004,
      "step": 499
    },
    {
      "epoch": 0.07029382820188387,
      "grad_norm": 1.8524725437164307,
      "learning_rate": 0.00013904494382022473,
      "loss": 1.1196,
      "step": 500
    },
    {
      "epoch": 0.07029382820188387,
      "eval_loss": 1.1718494892120361,
      "eval_runtime": 769.9388,
      "eval_samples_per_second": 16.425,
      "eval_steps_per_second": 8.212,
      "step": 500
    },
    {
      "epoch": 0.07043441585828764,
      "grad_norm": 1.7514917850494385,
      "learning_rate": 0.00013932584269662923,
      "loss": 1.2251,
      "step": 501
    },
    {
      "epoch": 0.07057500351469141,
      "grad_norm": 2.02458119392395,
      "learning_rate": 0.0001396067415730337,
      "loss": 1.1656,
      "step": 502
    },
    {
      "epoch": 0.07071559117109517,
      "grad_norm": 1.8836143016815186,
      "learning_rate": 0.00013988764044943822,
      "loss": 1.0837,
      "step": 503
    },
    {
      "epoch": 0.07085617882749895,
      "grad_norm": 2.006038188934326,
      "learning_rate": 0.0001401685393258427,
      "loss": 1.0856,
      "step": 504
    },
    {
      "epoch": 0.07099676648390271,
      "grad_norm": 1.9374271631240845,
      "learning_rate": 0.0001404494382022472,
      "loss": 1.0558,
      "step": 505
    },
    {
      "epoch": 0.07113735414030647,
      "grad_norm": 1.9645938873291016,
      "learning_rate": 0.00014073033707865168,
      "loss": 1.1528,
      "step": 506
    },
    {
      "epoch": 0.07127794179671025,
      "grad_norm": 1.6080759763717651,
      "learning_rate": 0.0001410112359550562,
      "loss": 1.1595,
      "step": 507
    },
    {
      "epoch": 0.07141852945311401,
      "grad_norm": 2.364879608154297,
      "learning_rate": 0.0001412921348314607,
      "loss": 1.0813,
      "step": 508
    },
    {
      "epoch": 0.07155911710951779,
      "grad_norm": 2.1399950981140137,
      "learning_rate": 0.00014157303370786517,
      "loss": 1.1126,
      "step": 509
    },
    {
      "epoch": 0.07169970476592155,
      "grad_norm": 1.5613672733306885,
      "learning_rate": 0.00014185393258426965,
      "loss": 1.1565,
      "step": 510
    },
    {
      "epoch": 0.07184029242232531,
      "grad_norm": 1.8278011083602905,
      "learning_rate": 0.00014213483146067416,
      "loss": 1.1397,
      "step": 511
    },
    {
      "epoch": 0.07198088007872909,
      "grad_norm": 1.7835471630096436,
      "learning_rate": 0.00014241573033707867,
      "loss": 1.0844,
      "step": 512
    },
    {
      "epoch": 0.07212146773513285,
      "grad_norm": 1.7860360145568848,
      "learning_rate": 0.00014269662921348315,
      "loss": 1.1197,
      "step": 513
    },
    {
      "epoch": 0.07226205539153663,
      "grad_norm": 1.5854675769805908,
      "learning_rate": 0.00014297752808988765,
      "loss": 1.169,
      "step": 514
    },
    {
      "epoch": 0.07240264304794039,
      "grad_norm": 1.7821388244628906,
      "learning_rate": 0.00014325842696629216,
      "loss": 1.1698,
      "step": 515
    },
    {
      "epoch": 0.07254323070434415,
      "grad_norm": 1.5901366472244263,
      "learning_rate": 0.00014353932584269664,
      "loss": 1.287,
      "step": 516
    },
    {
      "epoch": 0.07268381836074793,
      "grad_norm": 1.6734418869018555,
      "learning_rate": 0.00014382022471910112,
      "loss": 0.9163,
      "step": 517
    },
    {
      "epoch": 0.07282440601715169,
      "grad_norm": 1.5354588031768799,
      "learning_rate": 0.00014410112359550562,
      "loss": 1.1316,
      "step": 518
    },
    {
      "epoch": 0.07296499367355547,
      "grad_norm": 1.9380322694778442,
      "learning_rate": 0.00014438202247191013,
      "loss": 1.0502,
      "step": 519
    },
    {
      "epoch": 0.07310558132995923,
      "grad_norm": 1.779509425163269,
      "learning_rate": 0.0001446629213483146,
      "loss": 0.9424,
      "step": 520
    },
    {
      "epoch": 0.07324616898636299,
      "grad_norm": 1.7071030139923096,
      "learning_rate": 0.00014494382022471912,
      "loss": 1.2362,
      "step": 521
    },
    {
      "epoch": 0.07338675664276677,
      "grad_norm": 1.8892433643341064,
      "learning_rate": 0.0001452247191011236,
      "loss": 0.9979,
      "step": 522
    },
    {
      "epoch": 0.07352734429917053,
      "grad_norm": 2.017568588256836,
      "learning_rate": 0.00014550561797752807,
      "loss": 1.1366,
      "step": 523
    },
    {
      "epoch": 0.0736679319555743,
      "grad_norm": 1.9562225341796875,
      "learning_rate": 0.00014578651685393258,
      "loss": 1.2052,
      "step": 524
    },
    {
      "epoch": 0.07380851961197807,
      "grad_norm": 1.6277035474777222,
      "learning_rate": 0.0001460674157303371,
      "loss": 1.066,
      "step": 525
    },
    {
      "epoch": 0.07394910726838183,
      "grad_norm": 1.742151141166687,
      "learning_rate": 0.0001463483146067416,
      "loss": 1.2969,
      "step": 526
    },
    {
      "epoch": 0.0740896949247856,
      "grad_norm": 1.7842857837677002,
      "learning_rate": 0.00014662921348314607,
      "loss": 1.1289,
      "step": 527
    },
    {
      "epoch": 0.07423028258118937,
      "grad_norm": 1.598276972770691,
      "learning_rate": 0.00014691011235955058,
      "loss": 1.0112,
      "step": 528
    },
    {
      "epoch": 0.07437087023759315,
      "grad_norm": 1.7384521961212158,
      "learning_rate": 0.00014719101123595506,
      "loss": 1.2159,
      "step": 529
    },
    {
      "epoch": 0.07451145789399691,
      "grad_norm": 1.5853840112686157,
      "learning_rate": 0.00014747191011235956,
      "loss": 1.1601,
      "step": 530
    },
    {
      "epoch": 0.07465204555040067,
      "grad_norm": 1.6281217336654663,
      "learning_rate": 0.00014775280898876404,
      "loss": 1.0389,
      "step": 531
    },
    {
      "epoch": 0.07479263320680445,
      "grad_norm": 1.8669724464416504,
      "learning_rate": 0.00014803370786516855,
      "loss": 1.1562,
      "step": 532
    },
    {
      "epoch": 0.07493322086320821,
      "grad_norm": 1.8381009101867676,
      "learning_rate": 0.00014831460674157306,
      "loss": 1.0725,
      "step": 533
    },
    {
      "epoch": 0.07507380851961198,
      "grad_norm": 2.249843120574951,
      "learning_rate": 0.00014859550561797754,
      "loss": 1.0988,
      "step": 534
    },
    {
      "epoch": 0.07521439617601575,
      "grad_norm": 1.9389251470565796,
      "learning_rate": 0.00014887640449438202,
      "loss": 1.1899,
      "step": 535
    },
    {
      "epoch": 0.07535498383241951,
      "grad_norm": 1.8265089988708496,
      "learning_rate": 0.00014915730337078652,
      "loss": 0.9782,
      "step": 536
    },
    {
      "epoch": 0.07549557148882328,
      "grad_norm": 1.7461419105529785,
      "learning_rate": 0.00014943820224719103,
      "loss": 1.0795,
      "step": 537
    },
    {
      "epoch": 0.07563615914522705,
      "grad_norm": 1.792741298675537,
      "learning_rate": 0.0001497191011235955,
      "loss": 1.2463,
      "step": 538
    },
    {
      "epoch": 0.07577674680163082,
      "grad_norm": 1.8163418769836426,
      "learning_rate": 0.00015000000000000001,
      "loss": 1.1509,
      "step": 539
    },
    {
      "epoch": 0.07591733445803459,
      "grad_norm": 1.9132137298583984,
      "learning_rate": 0.00015028089887640452,
      "loss": 1.122,
      "step": 540
    },
    {
      "epoch": 0.07605792211443835,
      "grad_norm": 1.6287851333618164,
      "learning_rate": 0.000150561797752809,
      "loss": 1.0958,
      "step": 541
    },
    {
      "epoch": 0.07619850977084212,
      "grad_norm": 1.8988511562347412,
      "learning_rate": 0.00015084269662921348,
      "loss": 1.0721,
      "step": 542
    },
    {
      "epoch": 0.07633909742724589,
      "grad_norm": 1.8509657382965088,
      "learning_rate": 0.00015112359550561799,
      "loss": 1.1495,
      "step": 543
    },
    {
      "epoch": 0.07647968508364966,
      "grad_norm": 1.6124266386032104,
      "learning_rate": 0.0001514044943820225,
      "loss": 1.0116,
      "step": 544
    },
    {
      "epoch": 0.07662027274005342,
      "grad_norm": 2.014547109603882,
      "learning_rate": 0.00015168539325842697,
      "loss": 1.0104,
      "step": 545
    },
    {
      "epoch": 0.07676086039645719,
      "grad_norm": 2.216648817062378,
      "learning_rate": 0.00015196629213483148,
      "loss": 1.1623,
      "step": 546
    },
    {
      "epoch": 0.07690144805286096,
      "grad_norm": 1.8466733694076538,
      "learning_rate": 0.00015224719101123596,
      "loss": 1.1612,
      "step": 547
    },
    {
      "epoch": 0.07704203570926473,
      "grad_norm": 1.629024624824524,
      "learning_rate": 0.00015252808988764044,
      "loss": 1.0822,
      "step": 548
    },
    {
      "epoch": 0.0771826233656685,
      "grad_norm": 1.821184515953064,
      "learning_rate": 0.00015280898876404494,
      "loss": 1.0843,
      "step": 549
    },
    {
      "epoch": 0.07732321102207226,
      "grad_norm": 1.9131529331207275,
      "learning_rate": 0.00015308988764044945,
      "loss": 1.1521,
      "step": 550
    },
    {
      "epoch": 0.07746379867847603,
      "grad_norm": 1.9130793809890747,
      "learning_rate": 0.00015337078651685396,
      "loss": 1.1443,
      "step": 551
    },
    {
      "epoch": 0.0776043863348798,
      "grad_norm": 2.003965139389038,
      "learning_rate": 0.00015365168539325843,
      "loss": 1.0441,
      "step": 552
    },
    {
      "epoch": 0.07774497399128356,
      "grad_norm": 2.0951876640319824,
      "learning_rate": 0.00015393258426966294,
      "loss": 0.9906,
      "step": 553
    },
    {
      "epoch": 0.07788556164768733,
      "grad_norm": 1.7366385459899902,
      "learning_rate": 0.00015421348314606742,
      "loss": 1.1766,
      "step": 554
    },
    {
      "epoch": 0.0780261493040911,
      "grad_norm": 1.8172181844711304,
      "learning_rate": 0.00015449438202247193,
      "loss": 1.1716,
      "step": 555
    },
    {
      "epoch": 0.07816673696049486,
      "grad_norm": 1.5847293138504028,
      "learning_rate": 0.0001547752808988764,
      "loss": 1.2206,
      "step": 556
    },
    {
      "epoch": 0.07830732461689864,
      "grad_norm": 1.6581417322158813,
      "learning_rate": 0.0001550561797752809,
      "loss": 0.8883,
      "step": 557
    },
    {
      "epoch": 0.0784479122733024,
      "grad_norm": 1.7546782493591309,
      "learning_rate": 0.00015533707865168542,
      "loss": 1.2298,
      "step": 558
    },
    {
      "epoch": 0.07858849992970617,
      "grad_norm": 1.741114854812622,
      "learning_rate": 0.0001556179775280899,
      "loss": 0.9923,
      "step": 559
    },
    {
      "epoch": 0.07872908758610994,
      "grad_norm": 1.5951638221740723,
      "learning_rate": 0.00015589887640449438,
      "loss": 1.3051,
      "step": 560
    },
    {
      "epoch": 0.0788696752425137,
      "grad_norm": 2.006472587585449,
      "learning_rate": 0.00015617977528089888,
      "loss": 1.0406,
      "step": 561
    },
    {
      "epoch": 0.07901026289891748,
      "grad_norm": 1.8330334424972534,
      "learning_rate": 0.0001564606741573034,
      "loss": 1.0594,
      "step": 562
    },
    {
      "epoch": 0.07915085055532124,
      "grad_norm": 1.9694157838821411,
      "learning_rate": 0.00015674157303370787,
      "loss": 1.2719,
      "step": 563
    },
    {
      "epoch": 0.079291438211725,
      "grad_norm": 1.6476695537567139,
      "learning_rate": 0.00015702247191011238,
      "loss": 1.1895,
      "step": 564
    },
    {
      "epoch": 0.07943202586812878,
      "grad_norm": 1.9823668003082275,
      "learning_rate": 0.00015730337078651685,
      "loss": 1.0257,
      "step": 565
    },
    {
      "epoch": 0.07957261352453254,
      "grad_norm": 1.9492820501327515,
      "learning_rate": 0.00015758426966292133,
      "loss": 1.1818,
      "step": 566
    },
    {
      "epoch": 0.07971320118093632,
      "grad_norm": 1.8337290287017822,
      "learning_rate": 0.00015786516853932584,
      "loss": 0.9912,
      "step": 567
    },
    {
      "epoch": 0.07985378883734008,
      "grad_norm": 1.722230076789856,
      "learning_rate": 0.00015814606741573035,
      "loss": 1.1455,
      "step": 568
    },
    {
      "epoch": 0.07999437649374384,
      "grad_norm": 1.7482761144638062,
      "learning_rate": 0.00015842696629213485,
      "loss": 0.9863,
      "step": 569
    },
    {
      "epoch": 0.08013496415014762,
      "grad_norm": 1.862717628479004,
      "learning_rate": 0.00015870786516853933,
      "loss": 1.1273,
      "step": 570
    },
    {
      "epoch": 0.08027555180655138,
      "grad_norm": 1.6712069511413574,
      "learning_rate": 0.00015898876404494384,
      "loss": 1.063,
      "step": 571
    },
    {
      "epoch": 0.08041613946295516,
      "grad_norm": 1.6191927194595337,
      "learning_rate": 0.00015926966292134832,
      "loss": 1.1544,
      "step": 572
    },
    {
      "epoch": 0.08055672711935892,
      "grad_norm": 2.0997235774993896,
      "learning_rate": 0.0001595505617977528,
      "loss": 1.1348,
      "step": 573
    },
    {
      "epoch": 0.08069731477576268,
      "grad_norm": 1.7952507734298706,
      "learning_rate": 0.0001598314606741573,
      "loss": 1.1795,
      "step": 574
    },
    {
      "epoch": 0.08083790243216646,
      "grad_norm": 1.8866488933563232,
      "learning_rate": 0.0001601123595505618,
      "loss": 1.2695,
      "step": 575
    },
    {
      "epoch": 0.08097849008857022,
      "grad_norm": 1.900767207145691,
      "learning_rate": 0.00016039325842696632,
      "loss": 1.0304,
      "step": 576
    },
    {
      "epoch": 0.081119077744974,
      "grad_norm": 1.5800843238830566,
      "learning_rate": 0.0001606741573033708,
      "loss": 1.2319,
      "step": 577
    },
    {
      "epoch": 0.08125966540137776,
      "grad_norm": 1.7135732173919678,
      "learning_rate": 0.00016095505617977528,
      "loss": 1.1433,
      "step": 578
    },
    {
      "epoch": 0.08140025305778152,
      "grad_norm": 1.6325292587280273,
      "learning_rate": 0.00016123595505617978,
      "loss": 1.2152,
      "step": 579
    },
    {
      "epoch": 0.0815408407141853,
      "grad_norm": 1.806302547454834,
      "learning_rate": 0.00016151685393258426,
      "loss": 0.9553,
      "step": 580
    },
    {
      "epoch": 0.08168142837058906,
      "grad_norm": 1.9692156314849854,
      "learning_rate": 0.00016179775280898877,
      "loss": 0.9634,
      "step": 581
    },
    {
      "epoch": 0.08182201602699284,
      "grad_norm": 1.9859001636505127,
      "learning_rate": 0.00016207865168539327,
      "loss": 1.2463,
      "step": 582
    },
    {
      "epoch": 0.0819626036833966,
      "grad_norm": 1.7884678840637207,
      "learning_rate": 0.00016235955056179778,
      "loss": 1.0531,
      "step": 583
    },
    {
      "epoch": 0.08210319133980036,
      "grad_norm": 1.6303131580352783,
      "learning_rate": 0.00016264044943820226,
      "loss": 1.1544,
      "step": 584
    },
    {
      "epoch": 0.08224377899620414,
      "grad_norm": 1.8227020502090454,
      "learning_rate": 0.00016292134831460674,
      "loss": 1.0264,
      "step": 585
    },
    {
      "epoch": 0.0823843666526079,
      "grad_norm": 1.8657948970794678,
      "learning_rate": 0.00016320224719101124,
      "loss": 1.0811,
      "step": 586
    },
    {
      "epoch": 0.08252495430901167,
      "grad_norm": 1.9294456243515015,
      "learning_rate": 0.00016348314606741575,
      "loss": 1.1492,
      "step": 587
    },
    {
      "epoch": 0.08266554196541544,
      "grad_norm": 1.7479487657546997,
      "learning_rate": 0.00016376404494382023,
      "loss": 1.2064,
      "step": 588
    },
    {
      "epoch": 0.0828061296218192,
      "grad_norm": 1.9480308294296265,
      "learning_rate": 0.00016404494382022474,
      "loss": 1.1464,
      "step": 589
    },
    {
      "epoch": 0.08294671727822298,
      "grad_norm": 1.9083218574523926,
      "learning_rate": 0.00016432584269662922,
      "loss": 1.2696,
      "step": 590
    },
    {
      "epoch": 0.08308730493462674,
      "grad_norm": 1.9076848030090332,
      "learning_rate": 0.0001646067415730337,
      "loss": 1.189,
      "step": 591
    },
    {
      "epoch": 0.08322789259103051,
      "grad_norm": 1.7921706438064575,
      "learning_rate": 0.0001648876404494382,
      "loss": 1.1898,
      "step": 592
    },
    {
      "epoch": 0.08336848024743428,
      "grad_norm": 1.7707024812698364,
      "learning_rate": 0.0001651685393258427,
      "loss": 1.0333,
      "step": 593
    },
    {
      "epoch": 0.08350906790383804,
      "grad_norm": 1.7503043413162231,
      "learning_rate": 0.00016544943820224721,
      "loss": 1.0849,
      "step": 594
    },
    {
      "epoch": 0.08364965556024181,
      "grad_norm": 1.9450523853302002,
      "learning_rate": 0.0001657303370786517,
      "loss": 1.2105,
      "step": 595
    },
    {
      "epoch": 0.08379024321664558,
      "grad_norm": 1.636885404586792,
      "learning_rate": 0.0001660112359550562,
      "loss": 1.2523,
      "step": 596
    },
    {
      "epoch": 0.08393083087304935,
      "grad_norm": 1.8565523624420166,
      "learning_rate": 0.00016629213483146068,
      "loss": 1.0361,
      "step": 597
    },
    {
      "epoch": 0.08407141852945312,
      "grad_norm": 1.6406056880950928,
      "learning_rate": 0.00016657303370786516,
      "loss": 1.1161,
      "step": 598
    },
    {
      "epoch": 0.08421200618585688,
      "grad_norm": 1.6104568243026733,
      "learning_rate": 0.00016685393258426967,
      "loss": 1.0603,
      "step": 599
    },
    {
      "epoch": 0.08435259384226065,
      "grad_norm": 1.8450829982757568,
      "learning_rate": 0.00016713483146067417,
      "loss": 1.0511,
      "step": 600
    },
    {
      "epoch": 0.08449318149866442,
      "grad_norm": 1.745871663093567,
      "learning_rate": 0.00016741573033707868,
      "loss": 1.0386,
      "step": 601
    },
    {
      "epoch": 0.08463376915506818,
      "grad_norm": 1.6379019021987915,
      "learning_rate": 0.00016769662921348316,
      "loss": 1.2001,
      "step": 602
    },
    {
      "epoch": 0.08477435681147195,
      "grad_norm": 1.8524993658065796,
      "learning_rate": 0.00016797752808988764,
      "loss": 1.2422,
      "step": 603
    },
    {
      "epoch": 0.08491494446787572,
      "grad_norm": 1.74254310131073,
      "learning_rate": 0.00016825842696629214,
      "loss": 1.1174,
      "step": 604
    },
    {
      "epoch": 0.08505553212427949,
      "grad_norm": 1.9355604648590088,
      "learning_rate": 0.00016853932584269662,
      "loss": 0.9207,
      "step": 605
    },
    {
      "epoch": 0.08519611978068325,
      "grad_norm": 2.0613884925842285,
      "learning_rate": 0.00016882022471910113,
      "loss": 1.0576,
      "step": 606
    },
    {
      "epoch": 0.08533670743708702,
      "grad_norm": 2.111945390701294,
      "learning_rate": 0.00016910112359550564,
      "loss": 1.2108,
      "step": 607
    },
    {
      "epoch": 0.0854772950934908,
      "grad_norm": 1.6916882991790771,
      "learning_rate": 0.00016938202247191014,
      "loss": 1.1034,
      "step": 608
    },
    {
      "epoch": 0.08561788274989456,
      "grad_norm": 1.4921767711639404,
      "learning_rate": 0.00016966292134831462,
      "loss": 1.1564,
      "step": 609
    },
    {
      "epoch": 0.08575847040629833,
      "grad_norm": 1.8833372592926025,
      "learning_rate": 0.0001699438202247191,
      "loss": 1.0603,
      "step": 610
    },
    {
      "epoch": 0.0858990580627021,
      "grad_norm": 1.9325768947601318,
      "learning_rate": 0.0001702247191011236,
      "loss": 1.2213,
      "step": 611
    },
    {
      "epoch": 0.08603964571910586,
      "grad_norm": 1.5197745561599731,
      "learning_rate": 0.00017050561797752809,
      "loss": 1.0998,
      "step": 612
    },
    {
      "epoch": 0.08618023337550963,
      "grad_norm": 1.6265339851379395,
      "learning_rate": 0.0001707865168539326,
      "loss": 1.1814,
      "step": 613
    },
    {
      "epoch": 0.0863208210319134,
      "grad_norm": 1.7477728128433228,
      "learning_rate": 0.0001710674157303371,
      "loss": 1.144,
      "step": 614
    },
    {
      "epoch": 0.08646140868831717,
      "grad_norm": 1.9814451932907104,
      "learning_rate": 0.00017134831460674158,
      "loss": 1.0513,
      "step": 615
    },
    {
      "epoch": 0.08660199634472093,
      "grad_norm": 1.913831353187561,
      "learning_rate": 0.00017162921348314606,
      "loss": 1.0525,
      "step": 616
    },
    {
      "epoch": 0.0867425840011247,
      "grad_norm": 1.803867220878601,
      "learning_rate": 0.00017191011235955056,
      "loss": 1.1482,
      "step": 617
    },
    {
      "epoch": 0.08688317165752847,
      "grad_norm": 1.9065260887145996,
      "learning_rate": 0.00017219101123595507,
      "loss": 1.0226,
      "step": 618
    },
    {
      "epoch": 0.08702375931393223,
      "grad_norm": 2.0222091674804688,
      "learning_rate": 0.00017247191011235958,
      "loss": 1.0375,
      "step": 619
    },
    {
      "epoch": 0.08716434697033601,
      "grad_norm": 1.7485917806625366,
      "learning_rate": 0.00017275280898876406,
      "loss": 0.9999,
      "step": 620
    },
    {
      "epoch": 0.08730493462673977,
      "grad_norm": 1.9900448322296143,
      "learning_rate": 0.00017303370786516853,
      "loss": 1.0697,
      "step": 621
    },
    {
      "epoch": 0.08744552228314353,
      "grad_norm": 1.8691809177398682,
      "learning_rate": 0.00017331460674157304,
      "loss": 1.1992,
      "step": 622
    },
    {
      "epoch": 0.08758610993954731,
      "grad_norm": 1.6825190782546997,
      "learning_rate": 0.00017359550561797752,
      "loss": 1.2592,
      "step": 623
    },
    {
      "epoch": 0.08772669759595107,
      "grad_norm": 1.7848769426345825,
      "learning_rate": 0.00017387640449438203,
      "loss": 1.0815,
      "step": 624
    },
    {
      "epoch": 0.08786728525235485,
      "grad_norm": 1.9021390676498413,
      "learning_rate": 0.00017415730337078653,
      "loss": 1.0105,
      "step": 625
    },
    {
      "epoch": 0.08800787290875861,
      "grad_norm": 1.6025691032409668,
      "learning_rate": 0.00017443820224719104,
      "loss": 1.1498,
      "step": 626
    },
    {
      "epoch": 0.08814846056516237,
      "grad_norm": 1.560238003730774,
      "learning_rate": 0.00017471910112359552,
      "loss": 1.1215,
      "step": 627
    },
    {
      "epoch": 0.08828904822156615,
      "grad_norm": 1.6160931587219238,
      "learning_rate": 0.000175,
      "loss": 1.0533,
      "step": 628
    },
    {
      "epoch": 0.08842963587796991,
      "grad_norm": 1.7445076704025269,
      "learning_rate": 0.0001752808988764045,
      "loss": 1.001,
      "step": 629
    },
    {
      "epoch": 0.08857022353437369,
      "grad_norm": 1.9323867559432983,
      "learning_rate": 0.00017556179775280898,
      "loss": 1.3074,
      "step": 630
    },
    {
      "epoch": 0.08871081119077745,
      "grad_norm": 1.6116667985916138,
      "learning_rate": 0.0001758426966292135,
      "loss": 1.1119,
      "step": 631
    },
    {
      "epoch": 0.08885139884718121,
      "grad_norm": 1.7237968444824219,
      "learning_rate": 0.000176123595505618,
      "loss": 1.135,
      "step": 632
    },
    {
      "epoch": 0.08899198650358499,
      "grad_norm": 1.9896175861358643,
      "learning_rate": 0.00017640449438202248,
      "loss": 1.0288,
      "step": 633
    },
    {
      "epoch": 0.08913257415998875,
      "grad_norm": 1.9382209777832031,
      "learning_rate": 0.00017668539325842696,
      "loss": 1.0016,
      "step": 634
    },
    {
      "epoch": 0.08927316181639253,
      "grad_norm": 1.6360071897506714,
      "learning_rate": 0.00017696629213483146,
      "loss": 1.3079,
      "step": 635
    },
    {
      "epoch": 0.08941374947279629,
      "grad_norm": 1.778294563293457,
      "learning_rate": 0.00017724719101123597,
      "loss": 0.9889,
      "step": 636
    },
    {
      "epoch": 0.08955433712920005,
      "grad_norm": 1.5348527431488037,
      "learning_rate": 0.00017752808988764045,
      "loss": 1.1335,
      "step": 637
    },
    {
      "epoch": 0.08969492478560383,
      "grad_norm": 1.6764742136001587,
      "learning_rate": 0.00017780898876404495,
      "loss": 1.1802,
      "step": 638
    },
    {
      "epoch": 0.08983551244200759,
      "grad_norm": 1.8254311084747314,
      "learning_rate": 0.00017808988764044946,
      "loss": 1.055,
      "step": 639
    },
    {
      "epoch": 0.08997610009841137,
      "grad_norm": 1.6666100025177002,
      "learning_rate": 0.00017837078651685394,
      "loss": 1.1079,
      "step": 640
    },
    {
      "epoch": 0.09011668775481513,
      "grad_norm": 1.8659517765045166,
      "learning_rate": 0.00017865168539325842,
      "loss": 1.118,
      "step": 641
    },
    {
      "epoch": 0.09025727541121889,
      "grad_norm": 1.7432852983474731,
      "learning_rate": 0.00017893258426966293,
      "loss": 1.0121,
      "step": 642
    },
    {
      "epoch": 0.09039786306762267,
      "grad_norm": 1.7917046546936035,
      "learning_rate": 0.00017921348314606743,
      "loss": 1.0073,
      "step": 643
    },
    {
      "epoch": 0.09053845072402643,
      "grad_norm": 1.8301162719726562,
      "learning_rate": 0.00017949438202247194,
      "loss": 1.0701,
      "step": 644
    },
    {
      "epoch": 0.0906790383804302,
      "grad_norm": 2.3536174297332764,
      "learning_rate": 0.00017977528089887642,
      "loss": 0.9693,
      "step": 645
    },
    {
      "epoch": 0.09081962603683397,
      "grad_norm": 1.8948757648468018,
      "learning_rate": 0.0001800561797752809,
      "loss": 1.1838,
      "step": 646
    },
    {
      "epoch": 0.09096021369323773,
      "grad_norm": 1.6695860624313354,
      "learning_rate": 0.0001803370786516854,
      "loss": 0.9769,
      "step": 647
    },
    {
      "epoch": 0.0911008013496415,
      "grad_norm": 1.7207157611846924,
      "learning_rate": 0.00018061797752808988,
      "loss": 1.066,
      "step": 648
    },
    {
      "epoch": 0.09124138900604527,
      "grad_norm": 1.9401487112045288,
      "learning_rate": 0.0001808988764044944,
      "loss": 1.2778,
      "step": 649
    },
    {
      "epoch": 0.09138197666244904,
      "grad_norm": 1.84334397315979,
      "learning_rate": 0.0001811797752808989,
      "loss": 1.0494,
      "step": 650
    },
    {
      "epoch": 0.0915225643188528,
      "grad_norm": 1.870233178138733,
      "learning_rate": 0.0001814606741573034,
      "loss": 1.0135,
      "step": 651
    },
    {
      "epoch": 0.09166315197525657,
      "grad_norm": 1.5925264358520508,
      "learning_rate": 0.00018174157303370788,
      "loss": 1.1819,
      "step": 652
    },
    {
      "epoch": 0.09180373963166034,
      "grad_norm": 1.8965181112289429,
      "learning_rate": 0.00018202247191011236,
      "loss": 1.0563,
      "step": 653
    },
    {
      "epoch": 0.0919443272880641,
      "grad_norm": 1.8356221914291382,
      "learning_rate": 0.00018230337078651687,
      "loss": 1.1288,
      "step": 654
    },
    {
      "epoch": 0.09208491494446787,
      "grad_norm": 1.7094335556030273,
      "learning_rate": 0.00018258426966292135,
      "loss": 1.1203,
      "step": 655
    },
    {
      "epoch": 0.09222550260087164,
      "grad_norm": 1.9737379550933838,
      "learning_rate": 0.00018286516853932585,
      "loss": 1.1346,
      "step": 656
    },
    {
      "epoch": 0.09236609025727541,
      "grad_norm": 1.8414993286132812,
      "learning_rate": 0.00018314606741573036,
      "loss": 1.0781,
      "step": 657
    },
    {
      "epoch": 0.09250667791367918,
      "grad_norm": 1.623682975769043,
      "learning_rate": 0.00018342696629213484,
      "loss": 1.2254,
      "step": 658
    },
    {
      "epoch": 0.09264726557008295,
      "grad_norm": 1.7520537376403809,
      "learning_rate": 0.00018370786516853932,
      "loss": 1.0468,
      "step": 659
    },
    {
      "epoch": 0.09278785322648671,
      "grad_norm": 1.6468555927276611,
      "learning_rate": 0.00018398876404494382,
      "loss": 1.1156,
      "step": 660
    },
    {
      "epoch": 0.09292844088289048,
      "grad_norm": 2.0489284992218018,
      "learning_rate": 0.00018426966292134833,
      "loss": 0.9821,
      "step": 661
    },
    {
      "epoch": 0.09306902853929425,
      "grad_norm": 2.0867116451263428,
      "learning_rate": 0.0001845505617977528,
      "loss": 1.0497,
      "step": 662
    },
    {
      "epoch": 0.09320961619569802,
      "grad_norm": 1.656579852104187,
      "learning_rate": 0.00018483146067415732,
      "loss": 1.206,
      "step": 663
    },
    {
      "epoch": 0.09335020385210178,
      "grad_norm": 1.8074613809585571,
      "learning_rate": 0.00018511235955056182,
      "loss": 1.0693,
      "step": 664
    },
    {
      "epoch": 0.09349079150850555,
      "grad_norm": 1.6910744905471802,
      "learning_rate": 0.0001853932584269663,
      "loss": 1.2117,
      "step": 665
    },
    {
      "epoch": 0.09363137916490932,
      "grad_norm": 1.582107663154602,
      "learning_rate": 0.00018567415730337078,
      "loss": 1.0747,
      "step": 666
    },
    {
      "epoch": 0.09377196682131309,
      "grad_norm": 1.6360985040664673,
      "learning_rate": 0.0001859550561797753,
      "loss": 1.2478,
      "step": 667
    },
    {
      "epoch": 0.09391255447771686,
      "grad_norm": 1.818196177482605,
      "learning_rate": 0.0001862359550561798,
      "loss": 1.1352,
      "step": 668
    },
    {
      "epoch": 0.09405314213412062,
      "grad_norm": 1.8028786182403564,
      "learning_rate": 0.00018651685393258427,
      "loss": 1.1622,
      "step": 669
    },
    {
      "epoch": 0.09419372979052439,
      "grad_norm": 1.7853981256484985,
      "learning_rate": 0.00018679775280898878,
      "loss": 1.1282,
      "step": 670
    },
    {
      "epoch": 0.09433431744692816,
      "grad_norm": 1.794421911239624,
      "learning_rate": 0.00018707865168539326,
      "loss": 1.1044,
      "step": 671
    },
    {
      "epoch": 0.09447490510333192,
      "grad_norm": 1.6608794927597046,
      "learning_rate": 0.00018735955056179776,
      "loss": 1.1318,
      "step": 672
    },
    {
      "epoch": 0.0946154927597357,
      "grad_norm": 1.6244912147521973,
      "learning_rate": 0.00018764044943820224,
      "loss": 1.1027,
      "step": 673
    },
    {
      "epoch": 0.09475608041613946,
      "grad_norm": 1.7887078523635864,
      "learning_rate": 0.00018792134831460675,
      "loss": 1.3462,
      "step": 674
    },
    {
      "epoch": 0.09489666807254322,
      "grad_norm": 1.7533149719238281,
      "learning_rate": 0.00018820224719101126,
      "loss": 1.1196,
      "step": 675
    },
    {
      "epoch": 0.095037255728947,
      "grad_norm": 1.4965059757232666,
      "learning_rate": 0.00018848314606741576,
      "loss": 1.1716,
      "step": 676
    },
    {
      "epoch": 0.09517784338535076,
      "grad_norm": 1.6005154848098755,
      "learning_rate": 0.00018876404494382024,
      "loss": 1.1042,
      "step": 677
    },
    {
      "epoch": 0.09531843104175454,
      "grad_norm": 1.5777097940444946,
      "learning_rate": 0.00018904494382022472,
      "loss": 1.1525,
      "step": 678
    },
    {
      "epoch": 0.0954590186981583,
      "grad_norm": 1.9517756700515747,
      "learning_rate": 0.00018932584269662923,
      "loss": 1.0807,
      "step": 679
    },
    {
      "epoch": 0.09559960635456206,
      "grad_norm": 2.1595444679260254,
      "learning_rate": 0.0001896067415730337,
      "loss": 1.0131,
      "step": 680
    },
    {
      "epoch": 0.09574019401096584,
      "grad_norm": 1.7587543725967407,
      "learning_rate": 0.0001898876404494382,
      "loss": 1.0882,
      "step": 681
    },
    {
      "epoch": 0.0958807816673696,
      "grad_norm": 1.9410675764083862,
      "learning_rate": 0.00019016853932584272,
      "loss": 1.1872,
      "step": 682
    },
    {
      "epoch": 0.09602136932377338,
      "grad_norm": 1.6068451404571533,
      "learning_rate": 0.0001904494382022472,
      "loss": 1.2421,
      "step": 683
    },
    {
      "epoch": 0.09616195698017714,
      "grad_norm": 1.8151118755340576,
      "learning_rate": 0.00019073033707865168,
      "loss": 1.251,
      "step": 684
    },
    {
      "epoch": 0.0963025446365809,
      "grad_norm": 1.8112213611602783,
      "learning_rate": 0.00019101123595505618,
      "loss": 1.136,
      "step": 685
    },
    {
      "epoch": 0.09644313229298468,
      "grad_norm": 1.7125979661941528,
      "learning_rate": 0.0001912921348314607,
      "loss": 1.0098,
      "step": 686
    },
    {
      "epoch": 0.09658371994938844,
      "grad_norm": 1.7419513463974,
      "learning_rate": 0.00019157303370786517,
      "loss": 1.0809,
      "step": 687
    },
    {
      "epoch": 0.09672430760579222,
      "grad_norm": 2.0882179737091064,
      "learning_rate": 0.00019185393258426968,
      "loss": 1.0208,
      "step": 688
    },
    {
      "epoch": 0.09686489526219598,
      "grad_norm": 1.7340087890625,
      "learning_rate": 0.00019213483146067416,
      "loss": 1.0653,
      "step": 689
    },
    {
      "epoch": 0.09700548291859974,
      "grad_norm": 1.830165147781372,
      "learning_rate": 0.00019241573033707866,
      "loss": 1.2455,
      "step": 690
    },
    {
      "epoch": 0.09714607057500352,
      "grad_norm": 1.7749359607696533,
      "learning_rate": 0.00019269662921348314,
      "loss": 1.0964,
      "step": 691
    },
    {
      "epoch": 0.09728665823140728,
      "grad_norm": 1.8344537019729614,
      "learning_rate": 0.00019297752808988765,
      "loss": 1.2556,
      "step": 692
    },
    {
      "epoch": 0.09742724588781106,
      "grad_norm": 1.7142237424850464,
      "learning_rate": 0.00019325842696629215,
      "loss": 1.1526,
      "step": 693
    },
    {
      "epoch": 0.09756783354421482,
      "grad_norm": 1.8167173862457275,
      "learning_rate": 0.00019353932584269663,
      "loss": 1.1217,
      "step": 694
    },
    {
      "epoch": 0.09770842120061858,
      "grad_norm": 1.626859426498413,
      "learning_rate": 0.00019382022471910114,
      "loss": 1.015,
      "step": 695
    },
    {
      "epoch": 0.09784900885702236,
      "grad_norm": 1.9020107984542847,
      "learning_rate": 0.00019410112359550562,
      "loss": 1.1219,
      "step": 696
    },
    {
      "epoch": 0.09798959651342612,
      "grad_norm": 1.4585378170013428,
      "learning_rate": 0.00019438202247191013,
      "loss": 1.1194,
      "step": 697
    },
    {
      "epoch": 0.0981301841698299,
      "grad_norm": 1.6336524486541748,
      "learning_rate": 0.0001946629213483146,
      "loss": 1.2226,
      "step": 698
    },
    {
      "epoch": 0.09827077182623366,
      "grad_norm": 1.5979409217834473,
      "learning_rate": 0.0001949438202247191,
      "loss": 0.9857,
      "step": 699
    },
    {
      "epoch": 0.09841135948263742,
      "grad_norm": 1.7562235593795776,
      "learning_rate": 0.00019522471910112362,
      "loss": 1.2133,
      "step": 700
    },
    {
      "epoch": 0.0985519471390412,
      "grad_norm": 1.6409391164779663,
      "learning_rate": 0.0001955056179775281,
      "loss": 1.1855,
      "step": 701
    },
    {
      "epoch": 0.09869253479544496,
      "grad_norm": 1.8380610942840576,
      "learning_rate": 0.00019578651685393258,
      "loss": 1.1689,
      "step": 702
    },
    {
      "epoch": 0.09883312245184873,
      "grad_norm": 1.833868145942688,
      "learning_rate": 0.00019606741573033708,
      "loss": 1.0304,
      "step": 703
    },
    {
      "epoch": 0.0989737101082525,
      "grad_norm": 1.953900694847107,
      "learning_rate": 0.0001963483146067416,
      "loss": 1.1952,
      "step": 704
    },
    {
      "epoch": 0.09911429776465626,
      "grad_norm": 1.9749133586883545,
      "learning_rate": 0.00019662921348314607,
      "loss": 1.0658,
      "step": 705
    },
    {
      "epoch": 0.09925488542106004,
      "grad_norm": 1.7559146881103516,
      "learning_rate": 0.00019691011235955057,
      "loss": 1.0223,
      "step": 706
    },
    {
      "epoch": 0.0993954730774638,
      "grad_norm": 1.5474345684051514,
      "learning_rate": 0.00019719101123595508,
      "loss": 1.1272,
      "step": 707
    },
    {
      "epoch": 0.09953606073386756,
      "grad_norm": 1.5787681341171265,
      "learning_rate": 0.00019747191011235956,
      "loss": 1.3885,
      "step": 708
    },
    {
      "epoch": 0.09967664839027134,
      "grad_norm": 2.1223723888397217,
      "learning_rate": 0.00019775280898876404,
      "loss": 1.0917,
      "step": 709
    },
    {
      "epoch": 0.0998172360466751,
      "grad_norm": 1.616790771484375,
      "learning_rate": 0.00019803370786516855,
      "loss": 1.0732,
      "step": 710
    },
    {
      "epoch": 0.09995782370307887,
      "grad_norm": 1.7142314910888672,
      "learning_rate": 0.00019831460674157305,
      "loss": 1.0919,
      "step": 711
    },
    {
      "epoch": 0.10009841135948264,
      "grad_norm": 1.6671388149261475,
      "learning_rate": 0.00019859550561797753,
      "loss": 1.1458,
      "step": 712
    },
    {
      "epoch": 0.1002389990158864,
      "grad_norm": 1.910531997680664,
      "learning_rate": 0.00019887640449438204,
      "loss": 1.0609,
      "step": 713
    },
    {
      "epoch": 0.10037958667229017,
      "grad_norm": 1.9436869621276855,
      "learning_rate": 0.00019915730337078652,
      "loss": 1.0601,
      "step": 714
    },
    {
      "epoch": 0.10052017432869394,
      "grad_norm": 1.6710931062698364,
      "learning_rate": 0.00019943820224719102,
      "loss": 1.1476,
      "step": 715
    },
    {
      "epoch": 0.10066076198509771,
      "grad_norm": 1.756036400794983,
      "learning_rate": 0.0001997191011235955,
      "loss": 0.8512,
      "step": 716
    },
    {
      "epoch": 0.10080134964150148,
      "grad_norm": 1.8097018003463745,
      "learning_rate": 0.0002,
      "loss": 1.1795,
      "step": 717
    },
    {
      "epoch": 0.10094193729790524,
      "grad_norm": 2.1565442085266113,
      "learning_rate": 0.00019999972978980868,
      "loss": 1.1092,
      "step": 718
    },
    {
      "epoch": 0.10108252495430901,
      "grad_norm": 1.9004820585250854,
      "learning_rate": 0.000199998919160695,
      "loss": 1.0295,
      "step": 719
    },
    {
      "epoch": 0.10122311261071278,
      "grad_norm": 1.6766606569290161,
      "learning_rate": 0.00019999756811703976,
      "loss": 1.1088,
      "step": 720
    },
    {
      "epoch": 0.10136370026711655,
      "grad_norm": 1.7533552646636963,
      "learning_rate": 0.00019999567666614424,
      "loss": 1.2282,
      "step": 721
    },
    {
      "epoch": 0.10150428792352031,
      "grad_norm": 1.7137117385864258,
      "learning_rate": 0.00019999324481823028,
      "loss": 1.1871,
      "step": 722
    },
    {
      "epoch": 0.10164487557992408,
      "grad_norm": 1.9025242328643799,
      "learning_rate": 0.0001999902725864401,
      "loss": 1.2295,
      "step": 723
    },
    {
      "epoch": 0.10178546323632785,
      "grad_norm": 2.310025930404663,
      "learning_rate": 0.00019998675998683614,
      "loss": 1.0922,
      "step": 724
    },
    {
      "epoch": 0.10192605089273162,
      "grad_norm": 1.6799606084823608,
      "learning_rate": 0.00019998270703840127,
      "loss": 1.0283,
      "step": 725
    },
    {
      "epoch": 0.10206663854913539,
      "grad_norm": 1.6531925201416016,
      "learning_rate": 0.0001999781137630385,
      "loss": 1.119,
      "step": 726
    },
    {
      "epoch": 0.10220722620553915,
      "grad_norm": 1.9506559371948242,
      "learning_rate": 0.00019997298018557073,
      "loss": 0.9707,
      "step": 727
    },
    {
      "epoch": 0.10234781386194292,
      "grad_norm": 1.9363943338394165,
      "learning_rate": 0.00019996730633374093,
      "loss": 1.1395,
      "step": 728
    },
    {
      "epoch": 0.10248840151834669,
      "grad_norm": 1.620685338973999,
      "learning_rate": 0.00019996109223821172,
      "loss": 1.012,
      "step": 729
    },
    {
      "epoch": 0.10262898917475045,
      "grad_norm": 1.6238377094268799,
      "learning_rate": 0.00019995433793256536,
      "loss": 1.1666,
      "step": 730
    },
    {
      "epoch": 0.10276957683115423,
      "grad_norm": 1.7145230770111084,
      "learning_rate": 0.00019994704345330347,
      "loss": 1.119,
      "step": 731
    },
    {
      "epoch": 0.10291016448755799,
      "grad_norm": 1.7807226181030273,
      "learning_rate": 0.0001999392088398469,
      "loss": 0.912,
      "step": 732
    },
    {
      "epoch": 0.10305075214396175,
      "grad_norm": 1.8043700456619263,
      "learning_rate": 0.00019993083413453552,
      "loss": 1.2289,
      "step": 733
    },
    {
      "epoch": 0.10319133980036553,
      "grad_norm": 1.7714390754699707,
      "learning_rate": 0.00019992191938262793,
      "loss": 1.1216,
      "step": 734
    },
    {
      "epoch": 0.10333192745676929,
      "grad_norm": 1.6482774019241333,
      "learning_rate": 0.00019991246463230125,
      "loss": 1.1996,
      "step": 735
    },
    {
      "epoch": 0.10347251511317307,
      "grad_norm": 1.609484076499939,
      "learning_rate": 0.00019990246993465096,
      "loss": 1.1219,
      "step": 736
    },
    {
      "epoch": 0.10361310276957683,
      "grad_norm": 1.9731221199035645,
      "learning_rate": 0.00019989193534369033,
      "loss": 1.0847,
      "step": 737
    },
    {
      "epoch": 0.1037536904259806,
      "grad_norm": 1.664136290550232,
      "learning_rate": 0.00019988086091635052,
      "loss": 1.0822,
      "step": 738
    },
    {
      "epoch": 0.10389427808238437,
      "grad_norm": 1.7071564197540283,
      "learning_rate": 0.00019986924671247996,
      "loss": 1.221,
      "step": 739
    },
    {
      "epoch": 0.10403486573878813,
      "grad_norm": 1.7084354162216187,
      "learning_rate": 0.00019985709279484416,
      "loss": 1.0769,
      "step": 740
    },
    {
      "epoch": 0.10417545339519191,
      "grad_norm": 1.6223692893981934,
      "learning_rate": 0.0001998443992291254,
      "loss": 0.9717,
      "step": 741
    },
    {
      "epoch": 0.10431604105159567,
      "grad_norm": 1.9365100860595703,
      "learning_rate": 0.00019983116608392228,
      "loss": 1.0572,
      "step": 742
    },
    {
      "epoch": 0.10445662870799943,
      "grad_norm": 1.8046034574508667,
      "learning_rate": 0.0001998173934307494,
      "loss": 1.0397,
      "step": 743
    },
    {
      "epoch": 0.10459721636440321,
      "grad_norm": 1.9192757606506348,
      "learning_rate": 0.00019980308134403703,
      "loss": 1.0233,
      "step": 744
    },
    {
      "epoch": 0.10473780402080697,
      "grad_norm": 1.8963937759399414,
      "learning_rate": 0.00019978822990113053,
      "loss": 1.1353,
      "step": 745
    },
    {
      "epoch": 0.10487839167721075,
      "grad_norm": 1.8415473699569702,
      "learning_rate": 0.00019977283918229022,
      "loss": 1.1226,
      "step": 746
    },
    {
      "epoch": 0.10501897933361451,
      "grad_norm": 1.7553741931915283,
      "learning_rate": 0.0001997569092706906,
      "loss": 1.0111,
      "step": 747
    },
    {
      "epoch": 0.10515956699001827,
      "grad_norm": 1.7204335927963257,
      "learning_rate": 0.0001997404402524202,
      "loss": 1.0447,
      "step": 748
    },
    {
      "epoch": 0.10530015464642205,
      "grad_norm": 1.7673938274383545,
      "learning_rate": 0.00019972343221648093,
      "loss": 0.9877,
      "step": 749
    },
    {
      "epoch": 0.10544074230282581,
      "grad_norm": 1.609743356704712,
      "learning_rate": 0.0001997058852547877,
      "loss": 1.2175,
      "step": 750
    },
    {
      "epoch": 0.10558132995922959,
      "grad_norm": 2.0031604766845703,
      "learning_rate": 0.00019968779946216784,
      "loss": 1.0592,
      "step": 751
    },
    {
      "epoch": 0.10572191761563335,
      "grad_norm": 2.078827142715454,
      "learning_rate": 0.00019966917493636073,
      "loss": 1.1613,
      "step": 752
    },
    {
      "epoch": 0.10586250527203711,
      "grad_norm": 1.7516379356384277,
      "learning_rate": 0.00019965001177801704,
      "loss": 1.2842,
      "step": 753
    },
    {
      "epoch": 0.10600309292844089,
      "grad_norm": 1.6744953393936157,
      "learning_rate": 0.00019963031009069838,
      "loss": 1.1167,
      "step": 754
    },
    {
      "epoch": 0.10614368058484465,
      "grad_norm": 1.8029953241348267,
      "learning_rate": 0.00019961006998087672,
      "loss": 1.1007,
      "step": 755
    },
    {
      "epoch": 0.10628426824124843,
      "grad_norm": 1.764657735824585,
      "learning_rate": 0.0001995892915579337,
      "loss": 1.1764,
      "step": 756
    },
    {
      "epoch": 0.10642485589765219,
      "grad_norm": 1.6398683786392212,
      "learning_rate": 0.0001995679749341602,
      "loss": 1.1326,
      "step": 757
    },
    {
      "epoch": 0.10656544355405595,
      "grad_norm": 1.682534098625183,
      "learning_rate": 0.00019954612022475557,
      "loss": 1.3239,
      "step": 758
    },
    {
      "epoch": 0.10670603121045973,
      "grad_norm": 1.9262290000915527,
      "learning_rate": 0.00019952372754782709,
      "loss": 1.2834,
      "step": 759
    },
    {
      "epoch": 0.10684661886686349,
      "grad_norm": 1.614549994468689,
      "learning_rate": 0.0001995007970243894,
      "loss": 1.303,
      "step": 760
    },
    {
      "epoch": 0.10698720652326725,
      "grad_norm": 1.81157648563385,
      "learning_rate": 0.0001994773287783637,
      "loss": 1.2104,
      "step": 761
    },
    {
      "epoch": 0.10712779417967103,
      "grad_norm": 1.6479265689849854,
      "learning_rate": 0.00019945332293657718,
      "loss": 0.9948,
      "step": 762
    },
    {
      "epoch": 0.10726838183607479,
      "grad_norm": 1.6148130893707275,
      "learning_rate": 0.00019942877962876232,
      "loss": 0.9941,
      "step": 763
    },
    {
      "epoch": 0.10740896949247856,
      "grad_norm": 1.8652920722961426,
      "learning_rate": 0.0001994036989875561,
      "loss": 1.0309,
      "step": 764
    },
    {
      "epoch": 0.10754955714888233,
      "grad_norm": 1.566698431968689,
      "learning_rate": 0.00019937808114849946,
      "loss": 1.118,
      "step": 765
    },
    {
      "epoch": 0.10769014480528609,
      "grad_norm": 1.644484043121338,
      "learning_rate": 0.00019935192625003642,
      "loss": 1.2316,
      "step": 766
    },
    {
      "epoch": 0.10783073246168987,
      "grad_norm": 1.6326709985733032,
      "learning_rate": 0.0001993252344335134,
      "loss": 1.0213,
      "step": 767
    },
    {
      "epoch": 0.10797132011809363,
      "grad_norm": 1.6306262016296387,
      "learning_rate": 0.0001992980058431784,
      "loss": 1.2588,
      "step": 768
    },
    {
      "epoch": 0.1081119077744974,
      "grad_norm": 1.6410911083221436,
      "learning_rate": 0.00019927024062618026,
      "loss": 1.0411,
      "step": 769
    },
    {
      "epoch": 0.10825249543090117,
      "grad_norm": 1.528041958808899,
      "learning_rate": 0.00019924193893256783,
      "loss": 1.2674,
      "step": 770
    },
    {
      "epoch": 0.10839308308730493,
      "grad_norm": 1.6769347190856934,
      "learning_rate": 0.00019921310091528934,
      "loss": 1.0128,
      "step": 771
    },
    {
      "epoch": 0.1085336707437087,
      "grad_norm": 1.6287634372711182,
      "learning_rate": 0.00019918372673019126,
      "loss": 1.124,
      "step": 772
    },
    {
      "epoch": 0.10867425840011247,
      "grad_norm": 1.501391053199768,
      "learning_rate": 0.0001991538165360176,
      "loss": 1.1239,
      "step": 773
    },
    {
      "epoch": 0.10881484605651624,
      "grad_norm": 1.6663458347320557,
      "learning_rate": 0.00019912337049440927,
      "loss": 1.1463,
      "step": 774
    },
    {
      "epoch": 0.10895543371292,
      "grad_norm": 1.6017011404037476,
      "learning_rate": 0.00019909238876990285,
      "loss": 1.2326,
      "step": 775
    },
    {
      "epoch": 0.10909602136932377,
      "grad_norm": 1.6592146158218384,
      "learning_rate": 0.00019906087152992986,
      "loss": 1.0684,
      "step": 776
    },
    {
      "epoch": 0.10923660902572754,
      "grad_norm": 1.8617225885391235,
      "learning_rate": 0.00019902881894481587,
      "loss": 1.1333,
      "step": 777
    },
    {
      "epoch": 0.1093771966821313,
      "grad_norm": 1.6567895412445068,
      "learning_rate": 0.00019899623118777962,
      "loss": 1.2425,
      "step": 778
    },
    {
      "epoch": 0.10951778433853508,
      "grad_norm": 1.6943678855895996,
      "learning_rate": 0.00019896310843493199,
      "loss": 1.1357,
      "step": 779
    },
    {
      "epoch": 0.10965837199493884,
      "grad_norm": 1.730482578277588,
      "learning_rate": 0.00019892945086527508,
      "loss": 1.0053,
      "step": 780
    },
    {
      "epoch": 0.1097989596513426,
      "grad_norm": 1.4923803806304932,
      "learning_rate": 0.0001988952586607013,
      "loss": 1.1075,
      "step": 781
    },
    {
      "epoch": 0.10993954730774638,
      "grad_norm": 1.544702410697937,
      "learning_rate": 0.0001988605320059922,
      "loss": 1.2568,
      "step": 782
    },
    {
      "epoch": 0.11008013496415014,
      "grad_norm": 1.6441439390182495,
      "learning_rate": 0.00019882527108881778,
      "loss": 1.1359,
      "step": 783
    },
    {
      "epoch": 0.11022072262055392,
      "grad_norm": 1.9902136325836182,
      "learning_rate": 0.0001987894760997352,
      "loss": 0.9854,
      "step": 784
    },
    {
      "epoch": 0.11036131027695768,
      "grad_norm": 1.69729483127594,
      "learning_rate": 0.00019875314723218787,
      "loss": 1.3543,
      "step": 785
    },
    {
      "epoch": 0.11050189793336145,
      "grad_norm": 1.7221208810806274,
      "learning_rate": 0.00019871628468250442,
      "loss": 1.2423,
      "step": 786
    },
    {
      "epoch": 0.11064248558976522,
      "grad_norm": 1.855310320854187,
      "learning_rate": 0.00019867888864989752,
      "loss": 1.1866,
      "step": 787
    },
    {
      "epoch": 0.11078307324616898,
      "grad_norm": 1.6051859855651855,
      "learning_rate": 0.000198640959336463,
      "loss": 1.1526,
      "step": 788
    },
    {
      "epoch": 0.11092366090257276,
      "grad_norm": 1.9342533349990845,
      "learning_rate": 0.00019860249694717862,
      "loss": 1.1782,
      "step": 789
    },
    {
      "epoch": 0.11106424855897652,
      "grad_norm": 1.8113036155700684,
      "learning_rate": 0.00019856350168990293,
      "loss": 1.2724,
      "step": 790
    },
    {
      "epoch": 0.11120483621538028,
      "grad_norm": 1.6168088912963867,
      "learning_rate": 0.00019852397377537428,
      "loss": 1.1832,
      "step": 791
    },
    {
      "epoch": 0.11134542387178406,
      "grad_norm": 1.5755318403244019,
      "learning_rate": 0.00019848391341720952,
      "loss": 1.1026,
      "step": 792
    },
    {
      "epoch": 0.11148601152818782,
      "grad_norm": 1.5602980852127075,
      "learning_rate": 0.0001984433208319031,
      "loss": 1.3621,
      "step": 793
    },
    {
      "epoch": 0.1116265991845916,
      "grad_norm": 1.7920963764190674,
      "learning_rate": 0.0001984021962388255,
      "loss": 1.0684,
      "step": 794
    },
    {
      "epoch": 0.11176718684099536,
      "grad_norm": 1.6979635953903198,
      "learning_rate": 0.0001983605398602225,
      "loss": 0.986,
      "step": 795
    },
    {
      "epoch": 0.11190777449739912,
      "grad_norm": 1.6279672384262085,
      "learning_rate": 0.0001983183519212136,
      "loss": 0.9931,
      "step": 796
    },
    {
      "epoch": 0.1120483621538029,
      "grad_norm": 1.5883936882019043,
      "learning_rate": 0.00019827563264979104,
      "loss": 1.1241,
      "step": 797
    },
    {
      "epoch": 0.11218894981020666,
      "grad_norm": 1.6425966024398804,
      "learning_rate": 0.00019823238227681847,
      "loss": 1.0232,
      "step": 798
    },
    {
      "epoch": 0.11232953746661044,
      "grad_norm": 1.7365361452102661,
      "learning_rate": 0.00019818860103602975,
      "loss": 1.2351,
      "step": 799
    },
    {
      "epoch": 0.1124701251230142,
      "grad_norm": 1.7571967840194702,
      "learning_rate": 0.00019814428916402755,
      "loss": 1.0893,
      "step": 800
    },
    {
      "epoch": 0.11261071277941796,
      "grad_norm": 1.8802690505981445,
      "learning_rate": 0.00019809944690028238,
      "loss": 1.1532,
      "step": 801
    },
    {
      "epoch": 0.11275130043582174,
      "grad_norm": 1.868325114250183,
      "learning_rate": 0.00019805407448713084,
      "loss": 1.2235,
      "step": 802
    },
    {
      "epoch": 0.1128918880922255,
      "grad_norm": 1.8131580352783203,
      "learning_rate": 0.0001980081721697748,
      "loss": 1.2256,
      "step": 803
    },
    {
      "epoch": 0.11303247574862928,
      "grad_norm": 1.8067150115966797,
      "learning_rate": 0.0001979617401962797,
      "loss": 1.1348,
      "step": 804
    },
    {
      "epoch": 0.11317306340503304,
      "grad_norm": 1.958418846130371,
      "learning_rate": 0.00019791477881757338,
      "loss": 1.0459,
      "step": 805
    },
    {
      "epoch": 0.1133136510614368,
      "grad_norm": 1.7763800621032715,
      "learning_rate": 0.0001978672882874447,
      "loss": 1.2021,
      "step": 806
    },
    {
      "epoch": 0.11345423871784058,
      "grad_norm": 1.674647331237793,
      "learning_rate": 0.0001978192688625422,
      "loss": 1.1365,
      "step": 807
    },
    {
      "epoch": 0.11359482637424434,
      "grad_norm": 1.7318278551101685,
      "learning_rate": 0.00019777072080237262,
      "loss": 1.022,
      "step": 808
    },
    {
      "epoch": 0.11373541403064812,
      "grad_norm": 1.497750997543335,
      "learning_rate": 0.00019772164436929958,
      "loss": 1.2131,
      "step": 809
    },
    {
      "epoch": 0.11387600168705188,
      "grad_norm": 1.7818105220794678,
      "learning_rate": 0.0001976720398285421,
      "loss": 1.0894,
      "step": 810
    },
    {
      "epoch": 0.11401658934345564,
      "grad_norm": 1.5615174770355225,
      "learning_rate": 0.00019762190744817323,
      "loss": 1.1634,
      "step": 811
    },
    {
      "epoch": 0.11415717699985942,
      "grad_norm": 1.9910293817520142,
      "learning_rate": 0.00019757124749911862,
      "loss": 1.1923,
      "step": 812
    },
    {
      "epoch": 0.11429776465626318,
      "grad_norm": 1.4571202993392944,
      "learning_rate": 0.00019752006025515492,
      "loss": 1.2356,
      "step": 813
    },
    {
      "epoch": 0.11443835231266694,
      "grad_norm": 1.4373924732208252,
      "learning_rate": 0.0001974683459929084,
      "loss": 1.2263,
      "step": 814
    },
    {
      "epoch": 0.11457893996907072,
      "grad_norm": 1.8476452827453613,
      "learning_rate": 0.00019741610499185357,
      "loss": 1.0835,
      "step": 815
    },
    {
      "epoch": 0.11471952762547448,
      "grad_norm": 1.623260498046875,
      "learning_rate": 0.0001973633375343114,
      "loss": 0.9936,
      "step": 816
    },
    {
      "epoch": 0.11486011528187826,
      "grad_norm": 1.5206754207611084,
      "learning_rate": 0.00019731004390544793,
      "loss": 1.1707,
      "step": 817
    },
    {
      "epoch": 0.11500070293828202,
      "grad_norm": 1.871897578239441,
      "learning_rate": 0.00019725622439327283,
      "loss": 1.1204,
      "step": 818
    },
    {
      "epoch": 0.11514129059468578,
      "grad_norm": 1.4997847080230713,
      "learning_rate": 0.00019720187928863776,
      "loss": 1.125,
      "step": 819
    },
    {
      "epoch": 0.11528187825108956,
      "grad_norm": 1.746768832206726,
      "learning_rate": 0.0001971470088852347,
      "loss": 1.0402,
      "step": 820
    },
    {
      "epoch": 0.11542246590749332,
      "grad_norm": 1.8480232954025269,
      "learning_rate": 0.00019709161347959448,
      "loss": 1.209,
      "step": 821
    },
    {
      "epoch": 0.1155630535638971,
      "grad_norm": 1.9023429155349731,
      "learning_rate": 0.0001970356933710852,
      "loss": 0.8656,
      "step": 822
    },
    {
      "epoch": 0.11570364122030086,
      "grad_norm": 2.008901357650757,
      "learning_rate": 0.0001969792488619105,
      "loss": 0.9642,
      "step": 823
    },
    {
      "epoch": 0.11584422887670462,
      "grad_norm": 1.8698229789733887,
      "learning_rate": 0.000196922280257108,
      "loss": 1.2692,
      "step": 824
    },
    {
      "epoch": 0.1159848165331084,
      "grad_norm": 1.8681187629699707,
      "learning_rate": 0.0001968647878645477,
      "loss": 1.0934,
      "step": 825
    },
    {
      "epoch": 0.11612540418951216,
      "grad_norm": 1.7884559631347656,
      "learning_rate": 0.0001968067719949302,
      "loss": 1.0187,
      "step": 826
    },
    {
      "epoch": 0.11626599184591593,
      "grad_norm": 1.6061149835586548,
      "learning_rate": 0.00019674823296178504,
      "loss": 1.157,
      "step": 827
    },
    {
      "epoch": 0.1164065795023197,
      "grad_norm": 2.1427001953125,
      "learning_rate": 0.0001966891710814691,
      "loss": 1.0998,
      "step": 828
    },
    {
      "epoch": 0.11654716715872346,
      "grad_norm": 1.782666563987732,
      "learning_rate": 0.00019662958667316483,
      "loss": 1.1009,
      "step": 829
    },
    {
      "epoch": 0.11668775481512723,
      "grad_norm": 1.4928667545318604,
      "learning_rate": 0.0001965694800588785,
      "loss": 1.0806,
      "step": 830
    },
    {
      "epoch": 0.116828342471531,
      "grad_norm": 1.6990327835083008,
      "learning_rate": 0.00019650885156343858,
      "loss": 1.0635,
      "step": 831
    },
    {
      "epoch": 0.11696893012793477,
      "grad_norm": 1.7601444721221924,
      "learning_rate": 0.00019644770151449372,
      "loss": 1.0377,
      "step": 832
    },
    {
      "epoch": 0.11710951778433853,
      "grad_norm": 1.626050353050232,
      "learning_rate": 0.0001963860302425113,
      "loss": 1.2453,
      "step": 833
    },
    {
      "epoch": 0.1172501054407423,
      "grad_norm": 1.6260331869125366,
      "learning_rate": 0.00019632383808077544,
      "loss": 1.0988,
      "step": 834
    },
    {
      "epoch": 0.11739069309714607,
      "grad_norm": 1.7578555345535278,
      "learning_rate": 0.00019626112536538524,
      "loss": 1.2342,
      "step": 835
    },
    {
      "epoch": 0.11753128075354984,
      "grad_norm": 1.7177125215530396,
      "learning_rate": 0.00019619789243525298,
      "loss": 1.1355,
      "step": 836
    },
    {
      "epoch": 0.11767186840995361,
      "grad_norm": 1.7187654972076416,
      "learning_rate": 0.00019613413963210237,
      "loss": 1.312,
      "step": 837
    },
    {
      "epoch": 0.11781245606635737,
      "grad_norm": 1.6125352382659912,
      "learning_rate": 0.0001960698673004665,
      "loss": 1.2712,
      "step": 838
    },
    {
      "epoch": 0.11795304372276114,
      "grad_norm": 1.8964959383010864,
      "learning_rate": 0.00019600507578768618,
      "loss": 1.0047,
      "step": 839
    },
    {
      "epoch": 0.11809363137916491,
      "grad_norm": 1.6479511260986328,
      "learning_rate": 0.00019593976544390792,
      "loss": 1.1585,
      "step": 840
    },
    {
      "epoch": 0.11823421903556867,
      "grad_norm": 1.8979135751724243,
      "learning_rate": 0.00019587393662208214,
      "loss": 1.1298,
      "step": 841
    },
    {
      "epoch": 0.11837480669197245,
      "grad_norm": 1.5763911008834839,
      "learning_rate": 0.0001958075896779612,
      "loss": 1.2022,
      "step": 842
    },
    {
      "epoch": 0.11851539434837621,
      "grad_norm": 1.7359899282455444,
      "learning_rate": 0.00019574072497009754,
      "loss": 1.1492,
      "step": 843
    },
    {
      "epoch": 0.11865598200477998,
      "grad_norm": 1.7291985750198364,
      "learning_rate": 0.00019567334285984167,
      "loss": 1.2491,
      "step": 844
    },
    {
      "epoch": 0.11879656966118375,
      "grad_norm": 1.6721079349517822,
      "learning_rate": 0.00019560544371134022,
      "loss": 1.1245,
      "step": 845
    },
    {
      "epoch": 0.11893715731758751,
      "grad_norm": 1.6582481861114502,
      "learning_rate": 0.00019553702789153405,
      "loss": 1.1848,
      "step": 846
    },
    {
      "epoch": 0.11907774497399129,
      "grad_norm": 2.071378231048584,
      "learning_rate": 0.0001954680957701562,
      "loss": 1.0362,
      "step": 847
    },
    {
      "epoch": 0.11921833263039505,
      "grad_norm": 1.802564263343811,
      "learning_rate": 0.00019539864771972985,
      "loss": 1.1561,
      "step": 848
    },
    {
      "epoch": 0.11935892028679881,
      "grad_norm": 1.877838134765625,
      "learning_rate": 0.00019532868411556645,
      "loss": 1.0868,
      "step": 849
    },
    {
      "epoch": 0.11949950794320259,
      "grad_norm": 1.7935469150543213,
      "learning_rate": 0.00019525820533576363,
      "loss": 1.1655,
      "step": 850
    },
    {
      "epoch": 0.11964009559960635,
      "grad_norm": 1.7193894386291504,
      "learning_rate": 0.00019518721176120301,
      "loss": 1.0828,
      "step": 851
    },
    {
      "epoch": 0.11978068325601013,
      "grad_norm": 1.633362889289856,
      "learning_rate": 0.0001951157037755484,
      "loss": 1.1482,
      "step": 852
    },
    {
      "epoch": 0.11992127091241389,
      "grad_norm": 1.6131726503372192,
      "learning_rate": 0.00019504368176524349,
      "loss": 1.1502,
      "step": 853
    },
    {
      "epoch": 0.12006185856881765,
      "grad_norm": 1.5067028999328613,
      "learning_rate": 0.00019497114611950988,
      "loss": 1.2548,
      "step": 854
    },
    {
      "epoch": 0.12020244622522143,
      "grad_norm": 1.469435691833496,
      "learning_rate": 0.00019489809723034505,
      "loss": 1.1741,
      "step": 855
    },
    {
      "epoch": 0.12034303388162519,
      "grad_norm": 1.5044822692871094,
      "learning_rate": 0.00019482453549252003,
      "loss": 1.031,
      "step": 856
    },
    {
      "epoch": 0.12048362153802897,
      "grad_norm": 1.7028956413269043,
      "learning_rate": 0.00019475046130357747,
      "loss": 1.1577,
      "step": 857
    },
    {
      "epoch": 0.12062420919443273,
      "grad_norm": 1.6984260082244873,
      "learning_rate": 0.00019467587506382937,
      "loss": 1.0575,
      "step": 858
    },
    {
      "epoch": 0.12076479685083649,
      "grad_norm": 1.8840218782424927,
      "learning_rate": 0.000194600777176355,
      "loss": 0.9721,
      "step": 859
    },
    {
      "epoch": 0.12090538450724027,
      "grad_norm": 1.6320379972457886,
      "learning_rate": 0.0001945251680469986,
      "loss": 1.1231,
      "step": 860
    },
    {
      "epoch": 0.12104597216364403,
      "grad_norm": 1.59637451171875,
      "learning_rate": 0.00019444904808436737,
      "loss": 1.1682,
      "step": 861
    },
    {
      "epoch": 0.1211865598200478,
      "grad_norm": 2.23915958404541,
      "learning_rate": 0.00019437241769982907,
      "loss": 0.9961,
      "step": 862
    },
    {
      "epoch": 0.12132714747645157,
      "grad_norm": 1.8534209728240967,
      "learning_rate": 0.00019429527730750997,
      "loss": 0.9723,
      "step": 863
    },
    {
      "epoch": 0.12146773513285533,
      "grad_norm": 2.257800817489624,
      "learning_rate": 0.0001942176273242924,
      "loss": 1.1227,
      "step": 864
    },
    {
      "epoch": 0.12160832278925911,
      "grad_norm": 1.6702897548675537,
      "learning_rate": 0.00019413946816981275,
      "loss": 1.1769,
      "step": 865
    },
    {
      "epoch": 0.12174891044566287,
      "grad_norm": 1.6733086109161377,
      "learning_rate": 0.000194060800266459,
      "loss": 1.154,
      "step": 866
    },
    {
      "epoch": 0.12188949810206663,
      "grad_norm": 1.6607953310012817,
      "learning_rate": 0.0001939816240393685,
      "loss": 1.1568,
      "step": 867
    },
    {
      "epoch": 0.12203008575847041,
      "grad_norm": 1.886461615562439,
      "learning_rate": 0.0001939019399164258,
      "loss": 1.1869,
      "step": 868
    },
    {
      "epoch": 0.12217067341487417,
      "grad_norm": 1.61186945438385,
      "learning_rate": 0.0001938217483282601,
      "loss": 1.2545,
      "step": 869
    },
    {
      "epoch": 0.12231126107127795,
      "grad_norm": 1.6861602067947388,
      "learning_rate": 0.00019374104970824302,
      "loss": 1.1052,
      "step": 870
    },
    {
      "epoch": 0.12245184872768171,
      "grad_norm": 1.6776971817016602,
      "learning_rate": 0.0001936598444924865,
      "loss": 1.1183,
      "step": 871
    },
    {
      "epoch": 0.12259243638408547,
      "grad_norm": 1.751807689666748,
      "learning_rate": 0.00019357813311983994,
      "loss": 1.0871,
      "step": 872
    },
    {
      "epoch": 0.12273302404048925,
      "grad_norm": 1.6255249977111816,
      "learning_rate": 0.00019349591603188833,
      "loss": 1.2106,
      "step": 873
    },
    {
      "epoch": 0.12287361169689301,
      "grad_norm": 1.804284930229187,
      "learning_rate": 0.00019341319367294953,
      "loss": 1.0673,
      "step": 874
    },
    {
      "epoch": 0.12301419935329679,
      "grad_norm": 1.7132014036178589,
      "learning_rate": 0.00019332996649007206,
      "loss": 1.194,
      "step": 875
    },
    {
      "epoch": 0.12315478700970055,
      "grad_norm": 1.8012069463729858,
      "learning_rate": 0.00019324623493303258,
      "loss": 1.1137,
      "step": 876
    },
    {
      "epoch": 0.12329537466610431,
      "grad_norm": 1.5478216409683228,
      "learning_rate": 0.00019316199945433344,
      "loss": 1.1363,
      "step": 877
    },
    {
      "epoch": 0.12343596232250809,
      "grad_norm": 1.6551463603973389,
      "learning_rate": 0.00019307726050920042,
      "loss": 1.2638,
      "step": 878
    },
    {
      "epoch": 0.12357654997891185,
      "grad_norm": 1.486127495765686,
      "learning_rate": 0.00019299201855558,
      "loss": 1.165,
      "step": 879
    },
    {
      "epoch": 0.12371713763531562,
      "grad_norm": 1.5192784070968628,
      "learning_rate": 0.00019290627405413704,
      "loss": 0.9008,
      "step": 880
    },
    {
      "epoch": 0.12385772529171939,
      "grad_norm": 1.466822624206543,
      "learning_rate": 0.00019282002746825235,
      "loss": 1.1012,
      "step": 881
    },
    {
      "epoch": 0.12399831294812315,
      "grad_norm": 1.637532353401184,
      "learning_rate": 0.00019273327926402008,
      "loss": 0.9136,
      "step": 882
    },
    {
      "epoch": 0.12413890060452692,
      "grad_norm": 1.503531575202942,
      "learning_rate": 0.00019264602991024514,
      "loss": 1.3157,
      "step": 883
    },
    {
      "epoch": 0.12427948826093069,
      "grad_norm": 1.6211224794387817,
      "learning_rate": 0.00019255827987844088,
      "loss": 1.0263,
      "step": 884
    },
    {
      "epoch": 0.12442007591733446,
      "grad_norm": 1.617193341255188,
      "learning_rate": 0.00019247002964282634,
      "loss": 1.2933,
      "step": 885
    },
    {
      "epoch": 0.12456066357373823,
      "grad_norm": 1.527893304824829,
      "learning_rate": 0.00019238127968032378,
      "loss": 1.1301,
      "step": 886
    },
    {
      "epoch": 0.12470125123014199,
      "grad_norm": 1.9666965007781982,
      "learning_rate": 0.00019229203047055609,
      "loss": 1.0104,
      "step": 887
    },
    {
      "epoch": 0.12484183888654576,
      "grad_norm": 1.6190413236618042,
      "learning_rate": 0.00019220228249584414,
      "loss": 1.3377,
      "step": 888
    },
    {
      "epoch": 0.12498242654294953,
      "grad_norm": 1.5455448627471924,
      "learning_rate": 0.00019211203624120438,
      "loss": 1.0578,
      "step": 889
    },
    {
      "epoch": 0.1251230141993533,
      "grad_norm": 1.5654734373092651,
      "learning_rate": 0.00019202129219434588,
      "loss": 1.0732,
      "step": 890
    },
    {
      "epoch": 0.12526360185575705,
      "grad_norm": 1.7902004718780518,
      "learning_rate": 0.00019193005084566797,
      "loss": 1.1163,
      "step": 891
    },
    {
      "epoch": 0.12540418951216084,
      "grad_norm": 1.504434585571289,
      "learning_rate": 0.00019183831268825756,
      "loss": 1.2872,
      "step": 892
    },
    {
      "epoch": 0.1255447771685646,
      "grad_norm": 1.8008671998977661,
      "learning_rate": 0.0001917460782178863,
      "loss": 1.1406,
      "step": 893
    },
    {
      "epoch": 0.12568536482496837,
      "grad_norm": 1.9844818115234375,
      "learning_rate": 0.00019165334793300808,
      "loss": 1.2337,
      "step": 894
    },
    {
      "epoch": 0.12582595248137213,
      "grad_norm": 1.9781428575515747,
      "learning_rate": 0.00019156012233475625,
      "loss": 1.2473,
      "step": 895
    },
    {
      "epoch": 0.1259665401377759,
      "grad_norm": 1.814671277999878,
      "learning_rate": 0.00019146640192694095,
      "loss": 1.105,
      "step": 896
    },
    {
      "epoch": 0.12610712779417968,
      "grad_norm": 1.8396697044372559,
      "learning_rate": 0.00019137218721604636,
      "loss": 1.2127,
      "step": 897
    },
    {
      "epoch": 0.12624771545058344,
      "grad_norm": 1.6790088415145874,
      "learning_rate": 0.000191277478711228,
      "loss": 1.0319,
      "step": 898
    },
    {
      "epoch": 0.1263883031069872,
      "grad_norm": 1.5947331190109253,
      "learning_rate": 0.00019118227692430995,
      "loss": 1.094,
      "step": 899
    },
    {
      "epoch": 0.12652889076339097,
      "grad_norm": 1.7621901035308838,
      "learning_rate": 0.00019108658236978204,
      "loss": 1.1414,
      "step": 900
    },
    {
      "epoch": 0.12666947841979473,
      "grad_norm": 1.664345622062683,
      "learning_rate": 0.00019099039556479713,
      "loss": 1.2783,
      "step": 901
    },
    {
      "epoch": 0.12681006607619852,
      "grad_norm": 1.945640206336975,
      "learning_rate": 0.00019089371702916836,
      "loss": 0.9035,
      "step": 902
    },
    {
      "epoch": 0.12695065373260228,
      "grad_norm": 2.161634683609009,
      "learning_rate": 0.00019079654728536622,
      "loss": 1.1437,
      "step": 903
    },
    {
      "epoch": 0.12709124138900604,
      "grad_norm": 1.7287232875823975,
      "learning_rate": 0.00019069888685851582,
      "loss": 1.0271,
      "step": 904
    },
    {
      "epoch": 0.1272318290454098,
      "grad_norm": 1.3658607006072998,
      "learning_rate": 0.00019060073627639402,
      "loss": 1.1608,
      "step": 905
    },
    {
      "epoch": 0.12737241670181357,
      "grad_norm": 1.637449026107788,
      "learning_rate": 0.00019050209606942653,
      "loss": 1.0364,
      "step": 906
    },
    {
      "epoch": 0.12751300435821736,
      "grad_norm": 1.5898149013519287,
      "learning_rate": 0.00019040296677068518,
      "loss": 1.0067,
      "step": 907
    },
    {
      "epoch": 0.12765359201462112,
      "grad_norm": 1.7914780378341675,
      "learning_rate": 0.0001903033489158849,
      "loss": 1.1886,
      "step": 908
    },
    {
      "epoch": 0.12779417967102488,
      "grad_norm": 1.9881715774536133,
      "learning_rate": 0.00019020324304338087,
      "loss": 1.0153,
      "step": 909
    },
    {
      "epoch": 0.12793476732742864,
      "grad_norm": 1.625450611114502,
      "learning_rate": 0.00019010264969416562,
      "loss": 1.1228,
      "step": 910
    },
    {
      "epoch": 0.1280753549838324,
      "grad_norm": 1.493229866027832,
      "learning_rate": 0.00019000156941186613,
      "loss": 1.01,
      "step": 911
    },
    {
      "epoch": 0.1282159426402362,
      "grad_norm": 1.6298959255218506,
      "learning_rate": 0.00018990000274274086,
      "loss": 1.2154,
      "step": 912
    },
    {
      "epoch": 0.12835653029663996,
      "grad_norm": 1.6940168142318726,
      "learning_rate": 0.00018979795023567675,
      "loss": 0.9648,
      "step": 913
    },
    {
      "epoch": 0.12849711795304372,
      "grad_norm": 1.5592472553253174,
      "learning_rate": 0.0001896954124421864,
      "loss": 1.0738,
      "step": 914
    },
    {
      "epoch": 0.12863770560944748,
      "grad_norm": 1.7209265232086182,
      "learning_rate": 0.0001895923899164049,
      "loss": 1.2261,
      "step": 915
    },
    {
      "epoch": 0.12877829326585125,
      "grad_norm": 1.6379361152648926,
      "learning_rate": 0.000189488883215087,
      "loss": 1.1198,
      "step": 916
    },
    {
      "epoch": 0.12891888092225504,
      "grad_norm": 1.7611297369003296,
      "learning_rate": 0.000189384892897604,
      "loss": 1.3228,
      "step": 917
    },
    {
      "epoch": 0.1290594685786588,
      "grad_norm": 1.6604681015014648,
      "learning_rate": 0.00018928041952594076,
      "loss": 1.1154,
      "step": 918
    },
    {
      "epoch": 0.12920005623506256,
      "grad_norm": 1.5681349039077759,
      "learning_rate": 0.00018917546366469274,
      "loss": 1.0363,
      "step": 919
    },
    {
      "epoch": 0.12934064389146632,
      "grad_norm": 1.6858689785003662,
      "learning_rate": 0.00018907002588106276,
      "loss": 1.1757,
      "step": 920
    },
    {
      "epoch": 0.12948123154787008,
      "grad_norm": 1.7433886528015137,
      "learning_rate": 0.00018896410674485809,
      "loss": 1.17,
      "step": 921
    },
    {
      "epoch": 0.12962181920427387,
      "grad_norm": 1.6968393325805664,
      "learning_rate": 0.00018885770682848733,
      "loss": 1.2699,
      "step": 922
    },
    {
      "epoch": 0.12976240686067764,
      "grad_norm": 1.9381352663040161,
      "learning_rate": 0.00018875082670695732,
      "loss": 1.1441,
      "step": 923
    },
    {
      "epoch": 0.1299029945170814,
      "grad_norm": 2.0201773643493652,
      "learning_rate": 0.00018864346695787005,
      "loss": 0.9518,
      "step": 924
    },
    {
      "epoch": 0.13004358217348516,
      "grad_norm": 1.5604139566421509,
      "learning_rate": 0.00018853562816141943,
      "loss": 1.2644,
      "step": 925
    },
    {
      "epoch": 0.13018416982988892,
      "grad_norm": 2.0192368030548096,
      "learning_rate": 0.00018842731090038835,
      "loss": 1.1635,
      "step": 926
    },
    {
      "epoch": 0.1303247574862927,
      "grad_norm": 1.6466028690338135,
      "learning_rate": 0.00018831851576014535,
      "loss": 1.1925,
      "step": 927
    },
    {
      "epoch": 0.13046534514269648,
      "grad_norm": 2.528287410736084,
      "learning_rate": 0.0001882092433286415,
      "loss": 1.0843,
      "step": 928
    },
    {
      "epoch": 0.13060593279910024,
      "grad_norm": 1.4437416791915894,
      "learning_rate": 0.00018809949419640735,
      "loss": 1.1253,
      "step": 929
    },
    {
      "epoch": 0.130746520455504,
      "grad_norm": 1.61188542842865,
      "learning_rate": 0.00018798926895654958,
      "loss": 1.0088,
      "step": 930
    },
    {
      "epoch": 0.13088710811190776,
      "grad_norm": 1.7455470561981201,
      "learning_rate": 0.00018787856820474779,
      "loss": 1.1054,
      "step": 931
    },
    {
      "epoch": 0.13102769576831155,
      "grad_norm": 1.8056749105453491,
      "learning_rate": 0.0001877673925392515,
      "loss": 1.1488,
      "step": 932
    },
    {
      "epoch": 0.13116828342471532,
      "grad_norm": 1.7331604957580566,
      "learning_rate": 0.00018765574256087656,
      "loss": 1.271,
      "step": 933
    },
    {
      "epoch": 0.13130887108111908,
      "grad_norm": 1.66908860206604,
      "learning_rate": 0.0001875436188730023,
      "loss": 1.1986,
      "step": 934
    },
    {
      "epoch": 0.13144945873752284,
      "grad_norm": 1.5953601598739624,
      "learning_rate": 0.00018743102208156794,
      "loss": 1.0438,
      "step": 935
    },
    {
      "epoch": 0.1315900463939266,
      "grad_norm": 1.675052523612976,
      "learning_rate": 0.00018731795279506954,
      "loss": 0.9654,
      "step": 936
    },
    {
      "epoch": 0.1317306340503304,
      "grad_norm": 1.8025383949279785,
      "learning_rate": 0.00018720441162455652,
      "loss": 1.0812,
      "step": 937
    },
    {
      "epoch": 0.13187122170673415,
      "grad_norm": 1.9278911352157593,
      "learning_rate": 0.0001870903991836285,
      "loss": 1.1794,
      "step": 938
    },
    {
      "epoch": 0.13201180936313792,
      "grad_norm": 1.733852505683899,
      "learning_rate": 0.00018697591608843195,
      "loss": 1.2703,
      "step": 939
    },
    {
      "epoch": 0.13215239701954168,
      "grad_norm": 1.5474668741226196,
      "learning_rate": 0.0001868609629576569,
      "loss": 1.1347,
      "step": 940
    },
    {
      "epoch": 0.13229298467594544,
      "grad_norm": 1.7299009561538696,
      "learning_rate": 0.00018674554041253345,
      "loss": 1.2138,
      "step": 941
    },
    {
      "epoch": 0.13243357233234923,
      "grad_norm": 1.7482271194458008,
      "learning_rate": 0.00018662964907682859,
      "loss": 1.2078,
      "step": 942
    },
    {
      "epoch": 0.132574159988753,
      "grad_norm": 1.604552984237671,
      "learning_rate": 0.0001865132895768427,
      "loss": 1.3709,
      "step": 943
    },
    {
      "epoch": 0.13271474764515676,
      "grad_norm": 1.692020058631897,
      "learning_rate": 0.00018639646254140627,
      "loss": 1.2802,
      "step": 944
    },
    {
      "epoch": 0.13285533530156052,
      "grad_norm": 1.678795576095581,
      "learning_rate": 0.00018627916860187638,
      "loss": 0.997,
      "step": 945
    },
    {
      "epoch": 0.13299592295796428,
      "grad_norm": 1.5958257913589478,
      "learning_rate": 0.00018616140839213338,
      "loss": 1.18,
      "step": 946
    },
    {
      "epoch": 0.13313651061436807,
      "grad_norm": 1.6167479753494263,
      "learning_rate": 0.00018604318254857746,
      "loss": 1.1346,
      "step": 947
    },
    {
      "epoch": 0.13327709827077183,
      "grad_norm": 1.5460007190704346,
      "learning_rate": 0.0001859244917101252,
      "loss": 1.0497,
      "step": 948
    },
    {
      "epoch": 0.1334176859271756,
      "grad_norm": 1.7104015350341797,
      "learning_rate": 0.00018580533651820603,
      "loss": 1.285,
      "step": 949
    },
    {
      "epoch": 0.13355827358357936,
      "grad_norm": 2.072925329208374,
      "learning_rate": 0.00018568571761675893,
      "loss": 1.0927,
      "step": 950
    },
    {
      "epoch": 0.13369886123998312,
      "grad_norm": 1.6212879419326782,
      "learning_rate": 0.00018556563565222885,
      "loss": 1.1707,
      "step": 951
    },
    {
      "epoch": 0.1338394488963869,
      "grad_norm": 1.8161336183547974,
      "learning_rate": 0.0001854450912735631,
      "loss": 1.1267,
      "step": 952
    },
    {
      "epoch": 0.13398003655279067,
      "grad_norm": 1.6966172456741333,
      "learning_rate": 0.0001853240851322082,
      "loss": 1.057,
      "step": 953
    },
    {
      "epoch": 0.13412062420919443,
      "grad_norm": 1.9814555644989014,
      "learning_rate": 0.00018520261788210595,
      "loss": 1.0283,
      "step": 954
    },
    {
      "epoch": 0.1342612118655982,
      "grad_norm": 1.8035095930099487,
      "learning_rate": 0.00018508069017969008,
      "loss": 1.0,
      "step": 955
    },
    {
      "epoch": 0.13440179952200196,
      "grad_norm": 1.4374579191207886,
      "learning_rate": 0.0001849583026838828,
      "loss": 1.0989,
      "step": 956
    },
    {
      "epoch": 0.13454238717840575,
      "grad_norm": 1.8490337133407593,
      "learning_rate": 0.0001848354560560911,
      "loss": 0.9351,
      "step": 957
    },
    {
      "epoch": 0.1346829748348095,
      "grad_norm": 1.6642589569091797,
      "learning_rate": 0.00018471215096020315,
      "loss": 1.0083,
      "step": 958
    },
    {
      "epoch": 0.13482356249121327,
      "grad_norm": 1.6418250799179077,
      "learning_rate": 0.00018458838806258485,
      "loss": 1.0447,
      "step": 959
    },
    {
      "epoch": 0.13496415014761703,
      "grad_norm": 1.6546956300735474,
      "learning_rate": 0.0001844641680320761,
      "loss": 1.123,
      "step": 960
    },
    {
      "epoch": 0.1351047378040208,
      "grad_norm": 1.6175512075424194,
      "learning_rate": 0.00018433949153998729,
      "loss": 1.0804,
      "step": 961
    },
    {
      "epoch": 0.1352453254604246,
      "grad_norm": 1.7042362689971924,
      "learning_rate": 0.00018421435926009556,
      "loss": 1.0515,
      "step": 962
    },
    {
      "epoch": 0.13538591311682835,
      "grad_norm": 1.3553853034973145,
      "learning_rate": 0.0001840887718686413,
      "loss": 1.1896,
      "step": 963
    },
    {
      "epoch": 0.1355265007732321,
      "grad_norm": 1.5464295148849487,
      "learning_rate": 0.00018396273004432433,
      "loss": 1.1303,
      "step": 964
    },
    {
      "epoch": 0.13566708842963587,
      "grad_norm": 1.8611130714416504,
      "learning_rate": 0.0001838362344683004,
      "loss": 1.1515,
      "step": 965
    },
    {
      "epoch": 0.13580767608603964,
      "grad_norm": 1.7415069341659546,
      "learning_rate": 0.00018370928582417734,
      "loss": 1.126,
      "step": 966
    },
    {
      "epoch": 0.13594826374244343,
      "grad_norm": 1.5930153131484985,
      "learning_rate": 0.00018358188479801157,
      "loss": 1.3169,
      "step": 967
    },
    {
      "epoch": 0.1360888513988472,
      "grad_norm": 1.8581985235214233,
      "learning_rate": 0.00018345403207830412,
      "loss": 1.0928,
      "step": 968
    },
    {
      "epoch": 0.13622943905525095,
      "grad_norm": 1.7309702634811401,
      "learning_rate": 0.0001833257283559972,
      "loss": 1.1546,
      "step": 969
    },
    {
      "epoch": 0.1363700267116547,
      "grad_norm": 1.7902735471725464,
      "learning_rate": 0.00018319697432447025,
      "loss": 1.0919,
      "step": 970
    },
    {
      "epoch": 0.13651061436805847,
      "grad_norm": 1.5937029123306274,
      "learning_rate": 0.00018306777067953633,
      "loss": 1.2703,
      "step": 971
    },
    {
      "epoch": 0.13665120202446226,
      "grad_norm": 1.6938567161560059,
      "learning_rate": 0.00018293811811943825,
      "loss": 1.1316,
      "step": 972
    },
    {
      "epoch": 0.13679178968086603,
      "grad_norm": 1.8981819152832031,
      "learning_rate": 0.00018280801734484484,
      "loss": 1.1425,
      "step": 973
    },
    {
      "epoch": 0.1369323773372698,
      "grad_norm": 1.5287666320800781,
      "learning_rate": 0.00018267746905884725,
      "loss": 1.1709,
      "step": 974
    },
    {
      "epoch": 0.13707296499367355,
      "grad_norm": 1.5672818422317505,
      "learning_rate": 0.00018254647396695505,
      "loss": 1.2134,
      "step": 975
    },
    {
      "epoch": 0.13721355265007731,
      "grad_norm": 1.5814554691314697,
      "learning_rate": 0.00018241503277709238,
      "loss": 1.0446,
      "step": 976
    },
    {
      "epoch": 0.1373541403064811,
      "grad_norm": 1.6790499687194824,
      "learning_rate": 0.0001822831461995942,
      "loss": 1.0445,
      "step": 977
    },
    {
      "epoch": 0.13749472796288487,
      "grad_norm": 1.7274872064590454,
      "learning_rate": 0.00018215081494720248,
      "loss": 1.16,
      "step": 978
    },
    {
      "epoch": 0.13763531561928863,
      "grad_norm": 1.8139612674713135,
      "learning_rate": 0.0001820180397350623,
      "loss": 1.0569,
      "step": 979
    },
    {
      "epoch": 0.1377759032756924,
      "grad_norm": 1.64694082736969,
      "learning_rate": 0.0001818848212807179,
      "loss": 1.0438,
      "step": 980
    },
    {
      "epoch": 0.13791649093209615,
      "grad_norm": 1.706474781036377,
      "learning_rate": 0.00018175116030410906,
      "loss": 1.2435,
      "step": 981
    },
    {
      "epoch": 0.13805707858849994,
      "grad_norm": 1.8787643909454346,
      "learning_rate": 0.0001816170575275669,
      "loss": 1.0368,
      "step": 982
    },
    {
      "epoch": 0.1381976662449037,
      "grad_norm": 1.771226167678833,
      "learning_rate": 0.00018148251367581012,
      "loss": 1.1704,
      "step": 983
    },
    {
      "epoch": 0.13833825390130747,
      "grad_norm": 1.623578429222107,
      "learning_rate": 0.00018134752947594117,
      "loss": 1.0215,
      "step": 984
    },
    {
      "epoch": 0.13847884155771123,
      "grad_norm": 2.0968096256256104,
      "learning_rate": 0.00018121210565744215,
      "loss": 0.9641,
      "step": 985
    },
    {
      "epoch": 0.138619429214115,
      "grad_norm": 1.71736478805542,
      "learning_rate": 0.00018107624295217098,
      "loss": 1.0783,
      "step": 986
    },
    {
      "epoch": 0.13876001687051878,
      "grad_norm": 1.7923246622085571,
      "learning_rate": 0.00018093994209435744,
      "loss": 1.1529,
      "step": 987
    },
    {
      "epoch": 0.13890060452692254,
      "grad_norm": 1.4781681299209595,
      "learning_rate": 0.00018080320382059907,
      "loss": 1.1552,
      "step": 988
    },
    {
      "epoch": 0.1390411921833263,
      "grad_norm": 1.6792172193527222,
      "learning_rate": 0.0001806660288698575,
      "loss": 1.1742,
      "step": 989
    },
    {
      "epoch": 0.13918177983973007,
      "grad_norm": 1.6657360792160034,
      "learning_rate": 0.000180528417983454,
      "loss": 1.1085,
      "step": 990
    },
    {
      "epoch": 0.13932236749613383,
      "grad_norm": 1.8550443649291992,
      "learning_rate": 0.0001803903719050659,
      "loss": 1.1547,
      "step": 991
    },
    {
      "epoch": 0.13946295515253762,
      "grad_norm": 1.6780256032943726,
      "learning_rate": 0.0001802518913807224,
      "loss": 1.2343,
      "step": 992
    },
    {
      "epoch": 0.13960354280894138,
      "grad_norm": 1.613356590270996,
      "learning_rate": 0.00018011297715880037,
      "loss": 1.1944,
      "step": 993
    },
    {
      "epoch": 0.13974413046534515,
      "grad_norm": 1.6818861961364746,
      "learning_rate": 0.00017997362999002067,
      "loss": 1.2028,
      "step": 994
    },
    {
      "epoch": 0.1398847181217489,
      "grad_norm": 1.651341438293457,
      "learning_rate": 0.00017983385062744374,
      "loss": 1.1549,
      "step": 995
    },
    {
      "epoch": 0.14002530577815267,
      "grad_norm": 1.5224683284759521,
      "learning_rate": 0.0001796936398264658,
      "loss": 1.2329,
      "step": 996
    },
    {
      "epoch": 0.14016589343455643,
      "grad_norm": 1.7024986743927002,
      "learning_rate": 0.00017955299834481454,
      "loss": 1.0476,
      "step": 997
    },
    {
      "epoch": 0.14030648109096022,
      "grad_norm": 1.5648019313812256,
      "learning_rate": 0.00017941192694254522,
      "loss": 1.0987,
      "step": 998
    },
    {
      "epoch": 0.14044706874736398,
      "grad_norm": 1.974932312965393,
      "learning_rate": 0.00017927042638203649,
      "loss": 1.1048,
      "step": 999
    },
    {
      "epoch": 0.14058765640376775,
      "grad_norm": 1.4856538772583008,
      "learning_rate": 0.00017912849742798617,
      "loss": 1.0872,
      "step": 1000
    },
    {
      "epoch": 0.14058765640376775,
      "eval_loss": 1.1844638586044312,
      "eval_runtime": 772.6922,
      "eval_samples_per_second": 16.366,
      "eval_steps_per_second": 8.183,
      "step": 1000
    },
    {
      "epoch": 0.1407282440601715,
      "grad_norm": 1.4653431177139282,
      "learning_rate": 0.00017898614084740726,
      "loss": 1.2353,
      "step": 1001
    },
    {
      "epoch": 0.14086883171657527,
      "grad_norm": 1.6938291788101196,
      "learning_rate": 0.00017884335740962377,
      "loss": 1.1338,
      "step": 1002
    },
    {
      "epoch": 0.14100941937297906,
      "grad_norm": 1.9971638917922974,
      "learning_rate": 0.00017870014788626647,
      "loss": 0.9759,
      "step": 1003
    },
    {
      "epoch": 0.14115000702938282,
      "grad_norm": 1.690354585647583,
      "learning_rate": 0.00017855651305126883,
      "loss": 1.1758,
      "step": 1004
    },
    {
      "epoch": 0.14129059468578659,
      "grad_norm": 1.8836556673049927,
      "learning_rate": 0.00017841245368086276,
      "loss": 1.2231,
      "step": 1005
    },
    {
      "epoch": 0.14143118234219035,
      "grad_norm": 1.8976904153823853,
      "learning_rate": 0.00017826797055357449,
      "loss": 1.1497,
      "step": 1006
    },
    {
      "epoch": 0.1415717699985941,
      "grad_norm": 1.6494817733764648,
      "learning_rate": 0.00017812306445022025,
      "loss": 1.0998,
      "step": 1007
    },
    {
      "epoch": 0.1417123576549979,
      "grad_norm": 1.685690999031067,
      "learning_rate": 0.00017797773615390222,
      "loss": 1.0268,
      "step": 1008
    },
    {
      "epoch": 0.14185294531140166,
      "grad_norm": 1.9979948997497559,
      "learning_rate": 0.00017783198645000402,
      "loss": 1.0126,
      "step": 1009
    },
    {
      "epoch": 0.14199353296780542,
      "grad_norm": 1.6119532585144043,
      "learning_rate": 0.0001776858161261869,
      "loss": 1.006,
      "step": 1010
    },
    {
      "epoch": 0.1421341206242092,
      "grad_norm": 1.629258394241333,
      "learning_rate": 0.00017753922597238498,
      "loss": 1.13,
      "step": 1011
    },
    {
      "epoch": 0.14227470828061295,
      "grad_norm": 1.7351263761520386,
      "learning_rate": 0.00017739221678080136,
      "loss": 1.0525,
      "step": 1012
    },
    {
      "epoch": 0.14241529593701674,
      "grad_norm": 1.448646068572998,
      "learning_rate": 0.00017724478934590373,
      "loss": 1.0499,
      "step": 1013
    },
    {
      "epoch": 0.1425558835934205,
      "grad_norm": 1.949147343635559,
      "learning_rate": 0.00017709694446441988,
      "loss": 1.0475,
      "step": 1014
    },
    {
      "epoch": 0.14269647124982426,
      "grad_norm": 1.5671868324279785,
      "learning_rate": 0.0001769486829353338,
      "loss": 1.0901,
      "step": 1015
    },
    {
      "epoch": 0.14283705890622803,
      "grad_norm": 1.5275238752365112,
      "learning_rate": 0.00017680000555988094,
      "loss": 1.2857,
      "step": 1016
    },
    {
      "epoch": 0.1429776465626318,
      "grad_norm": 1.558746099472046,
      "learning_rate": 0.00017665091314154413,
      "loss": 1.1042,
      "step": 1017
    },
    {
      "epoch": 0.14311823421903558,
      "grad_norm": 1.721778392791748,
      "learning_rate": 0.0001765014064860493,
      "loss": 1.0334,
      "step": 1018
    },
    {
      "epoch": 0.14325882187543934,
      "grad_norm": 1.903743028640747,
      "learning_rate": 0.00017635148640136078,
      "loss": 1.0948,
      "step": 1019
    },
    {
      "epoch": 0.1433994095318431,
      "grad_norm": 1.9835774898529053,
      "learning_rate": 0.0001762011536976773,
      "loss": 1.116,
      "step": 1020
    },
    {
      "epoch": 0.14353999718824687,
      "grad_norm": 1.7087197303771973,
      "learning_rate": 0.0001760504091874274,
      "loss": 1.2273,
      "step": 1021
    },
    {
      "epoch": 0.14368058484465063,
      "grad_norm": 1.5870301723480225,
      "learning_rate": 0.00017589925368526524,
      "loss": 1.1477,
      "step": 1022
    },
    {
      "epoch": 0.14382117250105442,
      "grad_norm": 1.7587378025054932,
      "learning_rate": 0.00017574768800806583,
      "loss": 1.138,
      "step": 1023
    },
    {
      "epoch": 0.14396176015745818,
      "grad_norm": 1.647207260131836,
      "learning_rate": 0.0001755957129749211,
      "loss": 1.1336,
      "step": 1024
    },
    {
      "epoch": 0.14410234781386194,
      "grad_norm": 1.5192877054214478,
      "learning_rate": 0.00017544332940713499,
      "loss": 1.2482,
      "step": 1025
    },
    {
      "epoch": 0.1442429354702657,
      "grad_norm": 1.7180424928665161,
      "learning_rate": 0.00017529053812821946,
      "loss": 0.9861,
      "step": 1026
    },
    {
      "epoch": 0.14438352312666947,
      "grad_norm": 1.821111798286438,
      "learning_rate": 0.00017513733996388968,
      "loss": 1.3363,
      "step": 1027
    },
    {
      "epoch": 0.14452411078307326,
      "grad_norm": 1.5975958108901978,
      "learning_rate": 0.00017498373574205975,
      "loss": 0.9352,
      "step": 1028
    },
    {
      "epoch": 0.14466469843947702,
      "grad_norm": 1.9245553016662598,
      "learning_rate": 0.00017482972629283822,
      "loss": 1.3045,
      "step": 1029
    },
    {
      "epoch": 0.14480528609588078,
      "grad_norm": 1.3543527126312256,
      "learning_rate": 0.00017467531244852352,
      "loss": 1.1293,
      "step": 1030
    },
    {
      "epoch": 0.14494587375228454,
      "grad_norm": 1.6222951412200928,
      "learning_rate": 0.00017452049504359954,
      "loss": 1.3402,
      "step": 1031
    },
    {
      "epoch": 0.1450864614086883,
      "grad_norm": 1.3777356147766113,
      "learning_rate": 0.00017436527491473115,
      "loss": 1.2337,
      "step": 1032
    },
    {
      "epoch": 0.1452270490650921,
      "grad_norm": 1.3549246788024902,
      "learning_rate": 0.00017420965290075948,
      "loss": 1.1743,
      "step": 1033
    },
    {
      "epoch": 0.14536763672149586,
      "grad_norm": 1.5738165378570557,
      "learning_rate": 0.00017405362984269763,
      "loss": 1.0522,
      "step": 1034
    },
    {
      "epoch": 0.14550822437789962,
      "grad_norm": 1.5752593278884888,
      "learning_rate": 0.00017389720658372607,
      "loss": 1.0736,
      "step": 1035
    },
    {
      "epoch": 0.14564881203430338,
      "grad_norm": 1.717543125152588,
      "learning_rate": 0.00017374038396918788,
      "loss": 1.1973,
      "step": 1036
    },
    {
      "epoch": 0.14578939969070714,
      "grad_norm": 1.512918472290039,
      "learning_rate": 0.00017358316284658452,
      "loss": 1.159,
      "step": 1037
    },
    {
      "epoch": 0.14592998734711093,
      "grad_norm": 1.5849283933639526,
      "learning_rate": 0.00017342554406557094,
      "loss": 1.0506,
      "step": 1038
    },
    {
      "epoch": 0.1460705750035147,
      "grad_norm": 1.6444880962371826,
      "learning_rate": 0.00017326752847795116,
      "loss": 1.1642,
      "step": 1039
    },
    {
      "epoch": 0.14621116265991846,
      "grad_norm": 1.7328587770462036,
      "learning_rate": 0.00017310911693767361,
      "loss": 1.0294,
      "step": 1040
    },
    {
      "epoch": 0.14635175031632222,
      "grad_norm": 1.7611771821975708,
      "learning_rate": 0.00017295031030082657,
      "loss": 1.1775,
      "step": 1041
    },
    {
      "epoch": 0.14649233797272598,
      "grad_norm": 1.7040449380874634,
      "learning_rate": 0.0001727911094256335,
      "loss": 1.2337,
      "step": 1042
    },
    {
      "epoch": 0.14663292562912977,
      "grad_norm": 1.5207765102386475,
      "learning_rate": 0.00017263151517244832,
      "loss": 1.0865,
      "step": 1043
    },
    {
      "epoch": 0.14677351328553354,
      "grad_norm": 1.690390944480896,
      "learning_rate": 0.00017247152840375093,
      "loss": 1.1811,
      "step": 1044
    },
    {
      "epoch": 0.1469141009419373,
      "grad_norm": 1.6098686456680298,
      "learning_rate": 0.00017231114998414243,
      "loss": 1.2069,
      "step": 1045
    },
    {
      "epoch": 0.14705468859834106,
      "grad_norm": 1.6763060092926025,
      "learning_rate": 0.00017215038078034048,
      "loss": 1.1971,
      "step": 1046
    },
    {
      "epoch": 0.14719527625474482,
      "grad_norm": 1.9448530673980713,
      "learning_rate": 0.00017198922166117468,
      "loss": 0.9447,
      "step": 1047
    },
    {
      "epoch": 0.1473358639111486,
      "grad_norm": 1.594589114189148,
      "learning_rate": 0.0001718276734975817,
      "loss": 1.2447,
      "step": 1048
    },
    {
      "epoch": 0.14747645156755237,
      "grad_norm": 1.520838737487793,
      "learning_rate": 0.00017166573716260075,
      "loss": 1.1248,
      "step": 1049
    },
    {
      "epoch": 0.14761703922395614,
      "grad_norm": 1.426365613937378,
      "learning_rate": 0.00017150341353136883,
      "loss": 1.2571,
      "step": 1050
    },
    {
      "epoch": 0.1477576268803599,
      "grad_norm": 1.5624771118164062,
      "learning_rate": 0.00017134070348111589,
      "loss": 1.0025,
      "step": 1051
    },
    {
      "epoch": 0.14789821453676366,
      "grad_norm": 1.7122548818588257,
      "learning_rate": 0.0001711776078911602,
      "loss": 1.0879,
      "step": 1052
    },
    {
      "epoch": 0.14803880219316745,
      "grad_norm": 1.6780216693878174,
      "learning_rate": 0.00017101412764290364,
      "loss": 1.1705,
      "step": 1053
    },
    {
      "epoch": 0.1481793898495712,
      "grad_norm": 1.6072584390640259,
      "learning_rate": 0.0001708502636198267,
      "loss": 1.0866,
      "step": 1054
    },
    {
      "epoch": 0.14831997750597498,
      "grad_norm": 1.4053975343704224,
      "learning_rate": 0.000170686016707484,
      "loss": 1.3563,
      "step": 1055
    },
    {
      "epoch": 0.14846056516237874,
      "grad_norm": 2.2360880374908447,
      "learning_rate": 0.00017052138779349938,
      "loss": 0.9693,
      "step": 1056
    },
    {
      "epoch": 0.1486011528187825,
      "grad_norm": 1.5913816690444946,
      "learning_rate": 0.00017035637776756097,
      "loss": 1.1838,
      "step": 1057
    },
    {
      "epoch": 0.1487417404751863,
      "grad_norm": 1.5517102479934692,
      "learning_rate": 0.00017019098752141663,
      "loss": 1.2439,
      "step": 1058
    },
    {
      "epoch": 0.14888232813159005,
      "grad_norm": 1.688496708869934,
      "learning_rate": 0.00017002521794886897,
      "loss": 1.2124,
      "step": 1059
    },
    {
      "epoch": 0.14902291578799381,
      "grad_norm": 1.5988236665725708,
      "learning_rate": 0.0001698590699457705,
      "loss": 1.0438,
      "step": 1060
    },
    {
      "epoch": 0.14916350344439758,
      "grad_norm": 1.9951744079589844,
      "learning_rate": 0.0001696925444100189,
      "loss": 1.1374,
      "step": 1061
    },
    {
      "epoch": 0.14930409110080134,
      "grad_norm": 1.7398035526275635,
      "learning_rate": 0.00016952564224155214,
      "loss": 0.9458,
      "step": 1062
    },
    {
      "epoch": 0.14944467875720513,
      "grad_norm": 1.6933164596557617,
      "learning_rate": 0.00016935836434234358,
      "loss": 1.1297,
      "step": 1063
    },
    {
      "epoch": 0.1495852664136089,
      "grad_norm": 1.6245601177215576,
      "learning_rate": 0.00016919071161639702,
      "loss": 1.0529,
      "step": 1064
    },
    {
      "epoch": 0.14972585407001265,
      "grad_norm": 1.6169276237487793,
      "learning_rate": 0.00016902268496974201,
      "loss": 0.9082,
      "step": 1065
    },
    {
      "epoch": 0.14986644172641642,
      "grad_norm": 1.6225723028182983,
      "learning_rate": 0.00016885428531042876,
      "loss": 1.2522,
      "step": 1066
    },
    {
      "epoch": 0.15000702938282018,
      "grad_norm": 1.8111348152160645,
      "learning_rate": 0.00016868551354852338,
      "loss": 1.0121,
      "step": 1067
    },
    {
      "epoch": 0.15014761703922397,
      "grad_norm": 1.5175821781158447,
      "learning_rate": 0.00016851637059610286,
      "loss": 1.1695,
      "step": 1068
    },
    {
      "epoch": 0.15028820469562773,
      "grad_norm": 1.5991849899291992,
      "learning_rate": 0.0001683468573672502,
      "loss": 1.2418,
      "step": 1069
    },
    {
      "epoch": 0.1504287923520315,
      "grad_norm": 1.9400492906570435,
      "learning_rate": 0.0001681769747780494,
      "loss": 1.0747,
      "step": 1070
    },
    {
      "epoch": 0.15056938000843526,
      "grad_norm": 2.0559115409851074,
      "learning_rate": 0.00016800672374658068,
      "loss": 1.1583,
      "step": 1071
    },
    {
      "epoch": 0.15070996766483902,
      "grad_norm": 1.8014408349990845,
      "learning_rate": 0.00016783610519291523,
      "loss": 1.1659,
      "step": 1072
    },
    {
      "epoch": 0.1508505553212428,
      "grad_norm": 1.8288286924362183,
      "learning_rate": 0.00016766512003911057,
      "loss": 1.1827,
      "step": 1073
    },
    {
      "epoch": 0.15099114297764657,
      "grad_norm": 1.6986114978790283,
      "learning_rate": 0.00016749376920920526,
      "loss": 1.1024,
      "step": 1074
    },
    {
      "epoch": 0.15113173063405033,
      "grad_norm": 1.709017038345337,
      "learning_rate": 0.0001673220536292141,
      "loss": 1.3148,
      "step": 1075
    },
    {
      "epoch": 0.1512723182904541,
      "grad_norm": 1.8994615077972412,
      "learning_rate": 0.00016714997422712316,
      "loss": 0.9449,
      "step": 1076
    },
    {
      "epoch": 0.15141290594685786,
      "grad_norm": 1.7733842134475708,
      "learning_rate": 0.00016697753193288455,
      "loss": 1.1377,
      "step": 1077
    },
    {
      "epoch": 0.15155349360326165,
      "grad_norm": 1.4710599184036255,
      "learning_rate": 0.00016680472767841158,
      "loss": 1.1781,
      "step": 1078
    },
    {
      "epoch": 0.1516940812596654,
      "grad_norm": 1.5988775491714478,
      "learning_rate": 0.00016663156239757367,
      "loss": 1.1423,
      "step": 1079
    },
    {
      "epoch": 0.15183466891606917,
      "grad_norm": 1.608449935913086,
      "learning_rate": 0.00016645803702619127,
      "loss": 1.2623,
      "step": 1080
    },
    {
      "epoch": 0.15197525657247293,
      "grad_norm": 1.667382836341858,
      "learning_rate": 0.00016628415250203094,
      "loss": 1.0726,
      "step": 1081
    },
    {
      "epoch": 0.1521158442288767,
      "grad_norm": 1.7795186042785645,
      "learning_rate": 0.00016610990976479996,
      "loss": 1.1201,
      "step": 1082
    },
    {
      "epoch": 0.15225643188528049,
      "grad_norm": 1.4416404962539673,
      "learning_rate": 0.00016593530975614172,
      "loss": 1.1491,
      "step": 1083
    },
    {
      "epoch": 0.15239701954168425,
      "grad_norm": 1.4938645362854004,
      "learning_rate": 0.0001657603534196302,
      "loss": 1.1161,
      "step": 1084
    },
    {
      "epoch": 0.152537607198088,
      "grad_norm": 1.6270085573196411,
      "learning_rate": 0.00016558504170076504,
      "loss": 1.1529,
      "step": 1085
    },
    {
      "epoch": 0.15267819485449177,
      "grad_norm": 1.4684978723526,
      "learning_rate": 0.0001654093755469666,
      "loss": 1.1583,
      "step": 1086
    },
    {
      "epoch": 0.15281878251089553,
      "grad_norm": 1.8414126634597778,
      "learning_rate": 0.0001652333559075706,
      "loss": 1.211,
      "step": 1087
    },
    {
      "epoch": 0.15295937016729932,
      "grad_norm": 1.5331807136535645,
      "learning_rate": 0.00016505698373382296,
      "loss": 1.1336,
      "step": 1088
    },
    {
      "epoch": 0.1530999578237031,
      "grad_norm": 1.5899224281311035,
      "learning_rate": 0.00016488025997887483,
      "loss": 1.2014,
      "step": 1089
    },
    {
      "epoch": 0.15324054548010685,
      "grad_norm": 1.4688620567321777,
      "learning_rate": 0.00016470318559777756,
      "loss": 1.1044,
      "step": 1090
    },
    {
      "epoch": 0.1533811331365106,
      "grad_norm": 1.551069974899292,
      "learning_rate": 0.00016452576154747702,
      "loss": 0.9358,
      "step": 1091
    },
    {
      "epoch": 0.15352172079291437,
      "grad_norm": 1.5020296573638916,
      "learning_rate": 0.0001643479887868091,
      "loss": 1.1172,
      "step": 1092
    },
    {
      "epoch": 0.15366230844931816,
      "grad_norm": 1.8346796035766602,
      "learning_rate": 0.00016416986827649392,
      "loss": 1.3032,
      "step": 1093
    },
    {
      "epoch": 0.15380289610572193,
      "grad_norm": 1.7420330047607422,
      "learning_rate": 0.00016399140097913105,
      "loss": 1.2877,
      "step": 1094
    },
    {
      "epoch": 0.1539434837621257,
      "grad_norm": 1.5457777976989746,
      "learning_rate": 0.00016381258785919415,
      "loss": 1.1206,
      "step": 1095
    },
    {
      "epoch": 0.15408407141852945,
      "grad_norm": 1.6148498058319092,
      "learning_rate": 0.0001636334298830258,
      "loss": 1.2383,
      "step": 1096
    },
    {
      "epoch": 0.1542246590749332,
      "grad_norm": 1.5052673816680908,
      "learning_rate": 0.00016345392801883217,
      "loss": 1.1735,
      "step": 1097
    },
    {
      "epoch": 0.154365246731337,
      "grad_norm": 1.8841081857681274,
      "learning_rate": 0.00016327408323667795,
      "loss": 1.0366,
      "step": 1098
    },
    {
      "epoch": 0.15450583438774076,
      "grad_norm": 1.7392793893814087,
      "learning_rate": 0.00016309389650848098,
      "loss": 1.2043,
      "step": 1099
    },
    {
      "epoch": 0.15464642204414453,
      "grad_norm": 1.6795672178268433,
      "learning_rate": 0.00016291336880800707,
      "loss": 0.9804,
      "step": 1100
    },
    {
      "epoch": 0.1547870097005483,
      "grad_norm": 1.6420036554336548,
      "learning_rate": 0.00016273250111086473,
      "loss": 0.9697,
      "step": 1101
    },
    {
      "epoch": 0.15492759735695205,
      "grad_norm": 1.6834732294082642,
      "learning_rate": 0.00016255129439449986,
      "loss": 1.169,
      "step": 1102
    },
    {
      "epoch": 0.1550681850133558,
      "grad_norm": 1.7680649757385254,
      "learning_rate": 0.00016236974963819046,
      "loss": 1.0393,
      "step": 1103
    },
    {
      "epoch": 0.1552087726697596,
      "grad_norm": 1.8938243389129639,
      "learning_rate": 0.00016218786782304143,
      "loss": 1.0337,
      "step": 1104
    },
    {
      "epoch": 0.15534936032616337,
      "grad_norm": 1.7788703441619873,
      "learning_rate": 0.00016200564993197914,
      "loss": 1.1102,
      "step": 1105
    },
    {
      "epoch": 0.15548994798256713,
      "grad_norm": 1.7918554544448853,
      "learning_rate": 0.00016182309694974623,
      "loss": 1.0357,
      "step": 1106
    },
    {
      "epoch": 0.1556305356389709,
      "grad_norm": 1.7530405521392822,
      "learning_rate": 0.00016164020986289624,
      "loss": 1.2663,
      "step": 1107
    },
    {
      "epoch": 0.15577112329537465,
      "grad_norm": 1.4205873012542725,
      "learning_rate": 0.0001614569896597882,
      "loss": 1.0402,
      "step": 1108
    },
    {
      "epoch": 0.15591171095177844,
      "grad_norm": 1.599515676498413,
      "learning_rate": 0.00016127343733058152,
      "loss": 0.9739,
      "step": 1109
    },
    {
      "epoch": 0.1560522986081822,
      "grad_norm": 1.8438628911972046,
      "learning_rate": 0.00016108955386723037,
      "loss": 1.0795,
      "step": 1110
    },
    {
      "epoch": 0.15619288626458597,
      "grad_norm": 1.4634827375411987,
      "learning_rate": 0.00016090534026347845,
      "loss": 1.1873,
      "step": 1111
    },
    {
      "epoch": 0.15633347392098973,
      "grad_norm": 1.7450485229492188,
      "learning_rate": 0.00016072079751485366,
      "loss": 1.0869,
      "step": 1112
    },
    {
      "epoch": 0.1564740615773935,
      "grad_norm": 1.5935403108596802,
      "learning_rate": 0.00016053592661866253,
      "loss": 1.1678,
      "step": 1113
    },
    {
      "epoch": 0.15661464923379728,
      "grad_norm": 2.031660318374634,
      "learning_rate": 0.00016035072857398515,
      "loss": 1.0324,
      "step": 1114
    },
    {
      "epoch": 0.15675523689020104,
      "grad_norm": 1.6534945964813232,
      "learning_rate": 0.00016016520438166953,
      "loss": 1.1367,
      "step": 1115
    },
    {
      "epoch": 0.1568958245466048,
      "grad_norm": 1.7433925867080688,
      "learning_rate": 0.00015997935504432614,
      "loss": 1.1403,
      "step": 1116
    },
    {
      "epoch": 0.15703641220300857,
      "grad_norm": 1.7363579273223877,
      "learning_rate": 0.00015979318156632272,
      "loss": 1.0351,
      "step": 1117
    },
    {
      "epoch": 0.15717699985941233,
      "grad_norm": 1.6750723123550415,
      "learning_rate": 0.0001596066849537787,
      "loss": 1.1363,
      "step": 1118
    },
    {
      "epoch": 0.15731758751581612,
      "grad_norm": 1.7257009744644165,
      "learning_rate": 0.00015941986621455974,
      "loss": 1.0569,
      "step": 1119
    },
    {
      "epoch": 0.15745817517221988,
      "grad_norm": 2.103485107421875,
      "learning_rate": 0.00015923272635827248,
      "loss": 0.9484,
      "step": 1120
    },
    {
      "epoch": 0.15759876282862365,
      "grad_norm": 1.6324942111968994,
      "learning_rate": 0.00015904526639625872,
      "loss": 1.1169,
      "step": 1121
    },
    {
      "epoch": 0.1577393504850274,
      "grad_norm": 1.9000846147537231,
      "learning_rate": 0.0001588574873415904,
      "loss": 1.1394,
      "step": 1122
    },
    {
      "epoch": 0.15787993814143117,
      "grad_norm": 1.6071754693984985,
      "learning_rate": 0.0001586693902090638,
      "loss": 1.0975,
      "step": 1123
    },
    {
      "epoch": 0.15802052579783496,
      "grad_norm": 1.4703553915023804,
      "learning_rate": 0.00015848097601519408,
      "loss": 1.0902,
      "step": 1124
    },
    {
      "epoch": 0.15816111345423872,
      "grad_norm": 1.3413752317428589,
      "learning_rate": 0.00015829224577821008,
      "loss": 1.1801,
      "step": 1125
    },
    {
      "epoch": 0.15830170111064248,
      "grad_norm": 1.5939141511917114,
      "learning_rate": 0.00015810320051804836,
      "loss": 1.0596,
      "step": 1126
    },
    {
      "epoch": 0.15844228876704625,
      "grad_norm": 1.4229445457458496,
      "learning_rate": 0.00015791384125634806,
      "loss": 1.1374,
      "step": 1127
    },
    {
      "epoch": 0.15858287642345,
      "grad_norm": 1.6670811176300049,
      "learning_rate": 0.00015772416901644527,
      "loss": 1.1288,
      "step": 1128
    },
    {
      "epoch": 0.1587234640798538,
      "grad_norm": 1.4844356775283813,
      "learning_rate": 0.00015753418482336739,
      "loss": 1.2545,
      "step": 1129
    },
    {
      "epoch": 0.15886405173625756,
      "grad_norm": 1.6271159648895264,
      "learning_rate": 0.00015734388970382773,
      "loss": 1.0123,
      "step": 1130
    },
    {
      "epoch": 0.15900463939266132,
      "grad_norm": 1.6552551984786987,
      "learning_rate": 0.00015715328468621996,
      "loss": 1.3156,
      "step": 1131
    },
    {
      "epoch": 0.15914522704906509,
      "grad_norm": 1.664570927619934,
      "learning_rate": 0.00015696237080061235,
      "loss": 1.2914,
      "step": 1132
    },
    {
      "epoch": 0.15928581470546885,
      "grad_norm": 1.8602396249771118,
      "learning_rate": 0.0001567711490787425,
      "loss": 1.1545,
      "step": 1133
    },
    {
      "epoch": 0.15942640236187264,
      "grad_norm": 1.6165016889572144,
      "learning_rate": 0.00015657962055401158,
      "loss": 1.1555,
      "step": 1134
    },
    {
      "epoch": 0.1595669900182764,
      "grad_norm": 1.7556298971176147,
      "learning_rate": 0.00015638778626147877,
      "loss": 1.125,
      "step": 1135
    },
    {
      "epoch": 0.15970757767468016,
      "grad_norm": 1.4579581022262573,
      "learning_rate": 0.00015619564723785568,
      "loss": 1.2311,
      "step": 1136
    },
    {
      "epoch": 0.15984816533108392,
      "grad_norm": 1.8416825532913208,
      "learning_rate": 0.00015600320452150075,
      "loss": 1.23,
      "step": 1137
    },
    {
      "epoch": 0.1599887529874877,
      "grad_norm": 1.5021090507507324,
      "learning_rate": 0.00015581045915241367,
      "loss": 1.2702,
      "step": 1138
    },
    {
      "epoch": 0.16012934064389148,
      "grad_norm": 1.762222170829773,
      "learning_rate": 0.00015561741217222968,
      "loss": 1.1255,
      "step": 1139
    },
    {
      "epoch": 0.16026992830029524,
      "grad_norm": 1.8725061416625977,
      "learning_rate": 0.00015542406462421403,
      "loss": 1.264,
      "step": 1140
    },
    {
      "epoch": 0.160410515956699,
      "grad_norm": 1.6377803087234497,
      "learning_rate": 0.00015523041755325623,
      "loss": 1.266,
      "step": 1141
    },
    {
      "epoch": 0.16055110361310276,
      "grad_norm": 1.620621681213379,
      "learning_rate": 0.00015503647200586457,
      "loss": 1.1323,
      "step": 1142
    },
    {
      "epoch": 0.16069169126950653,
      "grad_norm": 1.8762290477752686,
      "learning_rate": 0.00015484222903016032,
      "loss": 1.1598,
      "step": 1143
    },
    {
      "epoch": 0.16083227892591032,
      "grad_norm": 1.5318349599838257,
      "learning_rate": 0.00015464768967587207,
      "loss": 1.1294,
      "step": 1144
    },
    {
      "epoch": 0.16097286658231408,
      "grad_norm": 1.5075684785842896,
      "learning_rate": 0.0001544528549943302,
      "loss": 1.0009,
      "step": 1145
    },
    {
      "epoch": 0.16111345423871784,
      "grad_norm": 1.4730637073516846,
      "learning_rate": 0.000154257726038461,
      "loss": 1.1174,
      "step": 1146
    },
    {
      "epoch": 0.1612540418951216,
      "grad_norm": 1.4208699464797974,
      "learning_rate": 0.00015406230386278113,
      "loss": 1.1688,
      "step": 1147
    },
    {
      "epoch": 0.16139462955152536,
      "grad_norm": 1.427156686782837,
      "learning_rate": 0.00015386658952339186,
      "loss": 1.1978,
      "step": 1148
    },
    {
      "epoch": 0.16153521720792915,
      "grad_norm": 1.5315887928009033,
      "learning_rate": 0.0001536705840779734,
      "loss": 1.0988,
      "step": 1149
    },
    {
      "epoch": 0.16167580486433292,
      "grad_norm": 1.7283567190170288,
      "learning_rate": 0.00015347428858577904,
      "loss": 1.192,
      "step": 1150
    },
    {
      "epoch": 0.16181639252073668,
      "grad_norm": 1.4939123392105103,
      "learning_rate": 0.0001532777041076297,
      "loss": 1.2926,
      "step": 1151
    },
    {
      "epoch": 0.16195698017714044,
      "grad_norm": 1.7236601114273071,
      "learning_rate": 0.000153080831705908,
      "loss": 1.1002,
      "step": 1152
    },
    {
      "epoch": 0.1620975678335442,
      "grad_norm": 1.5009121894836426,
      "learning_rate": 0.00015288367244455247,
      "loss": 1.1093,
      "step": 1153
    },
    {
      "epoch": 0.162238155489948,
      "grad_norm": 1.6926287412643433,
      "learning_rate": 0.00015268622738905197,
      "loss": 1.2539,
      "step": 1154
    },
    {
      "epoch": 0.16237874314635176,
      "grad_norm": 1.7517296075820923,
      "learning_rate": 0.00015248849760643983,
      "loss": 1.0578,
      "step": 1155
    },
    {
      "epoch": 0.16251933080275552,
      "grad_norm": 1.8113586902618408,
      "learning_rate": 0.00015229048416528804,
      "loss": 0.9115,
      "step": 1156
    },
    {
      "epoch": 0.16265991845915928,
      "grad_norm": 1.5501978397369385,
      "learning_rate": 0.00015209218813570167,
      "loss": 1.1284,
      "step": 1157
    },
    {
      "epoch": 0.16280050611556304,
      "grad_norm": 1.5686930418014526,
      "learning_rate": 0.00015189361058931284,
      "loss": 0.9794,
      "step": 1158
    },
    {
      "epoch": 0.16294109377196683,
      "grad_norm": 1.6491615772247314,
      "learning_rate": 0.00015169475259927512,
      "loss": 0.8952,
      "step": 1159
    },
    {
      "epoch": 0.1630816814283706,
      "grad_norm": 1.5278033018112183,
      "learning_rate": 0.00015149561524025756,
      "loss": 1.2329,
      "step": 1160
    },
    {
      "epoch": 0.16322226908477436,
      "grad_norm": 1.7710986137390137,
      "learning_rate": 0.00015129619958843907,
      "loss": 1.2735,
      "step": 1161
    },
    {
      "epoch": 0.16336285674117812,
      "grad_norm": 1.3923383951187134,
      "learning_rate": 0.00015109650672150252,
      "loss": 1.2091,
      "step": 1162
    },
    {
      "epoch": 0.16350344439758188,
      "grad_norm": 1.5282748937606812,
      "learning_rate": 0.00015089653771862882,
      "loss": 1.1982,
      "step": 1163
    },
    {
      "epoch": 0.16364403205398567,
      "grad_norm": 1.7950266599655151,
      "learning_rate": 0.00015069629366049124,
      "loss": 1.0834,
      "step": 1164
    },
    {
      "epoch": 0.16378461971038943,
      "grad_norm": 1.6748510599136353,
      "learning_rate": 0.00015049577562924947,
      "loss": 1.0728,
      "step": 1165
    },
    {
      "epoch": 0.1639252073667932,
      "grad_norm": 1.6119117736816406,
      "learning_rate": 0.00015029498470854383,
      "loss": 0.9656,
      "step": 1166
    },
    {
      "epoch": 0.16406579502319696,
      "grad_norm": 1.493622064590454,
      "learning_rate": 0.00015009392198348938,
      "loss": 1.4991,
      "step": 1167
    },
    {
      "epoch": 0.16420638267960072,
      "grad_norm": 1.5013182163238525,
      "learning_rate": 0.00014989258854067005,
      "loss": 1.1298,
      "step": 1168
    },
    {
      "epoch": 0.1643469703360045,
      "grad_norm": 1.812945008277893,
      "learning_rate": 0.00014969098546813284,
      "loss": 1.3134,
      "step": 1169
    },
    {
      "epoch": 0.16448755799240827,
      "grad_norm": 1.696524977684021,
      "learning_rate": 0.0001494891138553818,
      "loss": 0.9993,
      "step": 1170
    },
    {
      "epoch": 0.16462814564881204,
      "grad_norm": 1.6282621622085571,
      "learning_rate": 0.0001492869747933723,
      "loss": 1.2707,
      "step": 1171
    },
    {
      "epoch": 0.1647687333052158,
      "grad_norm": 1.515710711479187,
      "learning_rate": 0.00014908456937450505,
      "loss": 1.0481,
      "step": 1172
    },
    {
      "epoch": 0.16490932096161956,
      "grad_norm": 1.555892825126648,
      "learning_rate": 0.00014888189869262017,
      "loss": 1.0063,
      "step": 1173
    },
    {
      "epoch": 0.16504990861802335,
      "grad_norm": 1.7203866243362427,
      "learning_rate": 0.00014867896384299133,
      "loss": 1.0275,
      "step": 1174
    },
    {
      "epoch": 0.1651904962744271,
      "grad_norm": 1.4863207340240479,
      "learning_rate": 0.0001484757659223198,
      "loss": 1.1541,
      "step": 1175
    },
    {
      "epoch": 0.16533108393083087,
      "grad_norm": 1.4583079814910889,
      "learning_rate": 0.00014827230602872864,
      "loss": 1.1395,
      "step": 1176
    },
    {
      "epoch": 0.16547167158723464,
      "grad_norm": 1.6193957328796387,
      "learning_rate": 0.0001480685852617565,
      "loss": 1.2919,
      "step": 1177
    },
    {
      "epoch": 0.1656122592436384,
      "grad_norm": 1.676221489906311,
      "learning_rate": 0.00014786460472235197,
      "loss": 1.2986,
      "step": 1178
    },
    {
      "epoch": 0.1657528469000422,
      "grad_norm": 1.7942696809768677,
      "learning_rate": 0.00014766036551286743,
      "loss": 1.257,
      "step": 1179
    },
    {
      "epoch": 0.16589343455644595,
      "grad_norm": 1.618816614151001,
      "learning_rate": 0.00014745586873705322,
      "loss": 1.1083,
      "step": 1180
    },
    {
      "epoch": 0.1660340222128497,
      "grad_norm": 1.5589632987976074,
      "learning_rate": 0.0001472511155000516,
      "loss": 1.1353,
      "step": 1181
    },
    {
      "epoch": 0.16617460986925348,
      "grad_norm": 1.721117377281189,
      "learning_rate": 0.0001470461069083908,
      "loss": 1.2002,
      "step": 1182
    },
    {
      "epoch": 0.16631519752565724,
      "grad_norm": 1.5311018228530884,
      "learning_rate": 0.00014684084406997903,
      "loss": 1.1617,
      "step": 1183
    },
    {
      "epoch": 0.16645578518206103,
      "grad_norm": 1.685840129852295,
      "learning_rate": 0.0001466353280940985,
      "loss": 1.0656,
      "step": 1184
    },
    {
      "epoch": 0.1665963728384648,
      "grad_norm": 1.718794345855713,
      "learning_rate": 0.00014642956009139942,
      "loss": 1.069,
      "step": 1185
    },
    {
      "epoch": 0.16673696049486855,
      "grad_norm": 1.3564704656600952,
      "learning_rate": 0.00014622354117389407,
      "loss": 1.1476,
      "step": 1186
    },
    {
      "epoch": 0.16687754815127231,
      "grad_norm": 1.583802342414856,
      "learning_rate": 0.00014601727245495063,
      "loss": 1.1093,
      "step": 1187
    },
    {
      "epoch": 0.16701813580767608,
      "grad_norm": 1.5046725273132324,
      "learning_rate": 0.00014581075504928732,
      "loss": 1.0348,
      "step": 1188
    },
    {
      "epoch": 0.16715872346407987,
      "grad_norm": 1.6267790794372559,
      "learning_rate": 0.00014560399007296625,
      "loss": 1.0416,
      "step": 1189
    },
    {
      "epoch": 0.16729931112048363,
      "grad_norm": 1.5688893795013428,
      "learning_rate": 0.00014539697864338752,
      "loss": 1.2062,
      "step": 1190
    },
    {
      "epoch": 0.1674398987768874,
      "grad_norm": 1.4886304140090942,
      "learning_rate": 0.00014518972187928312,
      "loss": 1.176,
      "step": 1191
    },
    {
      "epoch": 0.16758048643329115,
      "grad_norm": 1.617308497428894,
      "learning_rate": 0.00014498222090071085,
      "loss": 0.9904,
      "step": 1192
    },
    {
      "epoch": 0.16772107408969492,
      "grad_norm": 1.545033574104309,
      "learning_rate": 0.00014477447682904824,
      "loss": 1.0335,
      "step": 1193
    },
    {
      "epoch": 0.1678616617460987,
      "grad_norm": 1.639428734779358,
      "learning_rate": 0.00014456649078698662,
      "loss": 0.9852,
      "step": 1194
    },
    {
      "epoch": 0.16800224940250247,
      "grad_norm": 1.5077521800994873,
      "learning_rate": 0.00014435826389852497,
      "loss": 1.0831,
      "step": 1195
    },
    {
      "epoch": 0.16814283705890623,
      "grad_norm": 1.5754035711288452,
      "learning_rate": 0.00014414979728896385,
      "loss": 1.1572,
      "step": 1196
    },
    {
      "epoch": 0.16828342471531,
      "grad_norm": 1.573744773864746,
      "learning_rate": 0.00014394109208489927,
      "loss": 1.2173,
      "step": 1197
    },
    {
      "epoch": 0.16842401237171375,
      "grad_norm": 1.4838769435882568,
      "learning_rate": 0.00014373214941421672,
      "loss": 1.137,
      "step": 1198
    },
    {
      "epoch": 0.16856460002811754,
      "grad_norm": 1.4547994136810303,
      "learning_rate": 0.00014352297040608493,
      "loss": 1.0167,
      "step": 1199
    },
    {
      "epoch": 0.1687051876845213,
      "grad_norm": 1.5064082145690918,
      "learning_rate": 0.00014331355619094996,
      "loss": 1.1612,
      "step": 1200
    },
    {
      "epoch": 0.16884577534092507,
      "grad_norm": 1.6504435539245605,
      "learning_rate": 0.00014310390790052887,
      "loss": 1.1476,
      "step": 1201
    },
    {
      "epoch": 0.16898636299732883,
      "grad_norm": 1.603424072265625,
      "learning_rate": 0.00014289402666780378,
      "loss": 1.1337,
      "step": 1202
    },
    {
      "epoch": 0.1691269506537326,
      "grad_norm": 1.5893702507019043,
      "learning_rate": 0.00014268391362701568,
      "loss": 1.2875,
      "step": 1203
    },
    {
      "epoch": 0.16926753831013636,
      "grad_norm": 1.380562663078308,
      "learning_rate": 0.00014247356991365818,
      "loss": 1.1297,
      "step": 1204
    },
    {
      "epoch": 0.16940812596654015,
      "grad_norm": 1.8207881450653076,
      "learning_rate": 0.00014226299666447167,
      "loss": 0.9739,
      "step": 1205
    },
    {
      "epoch": 0.1695487136229439,
      "grad_norm": 1.4543616771697998,
      "learning_rate": 0.00014205219501743684,
      "loss": 1.1145,
      "step": 1206
    },
    {
      "epoch": 0.16968930127934767,
      "grad_norm": 1.4963948726654053,
      "learning_rate": 0.00014184116611176884,
      "loss": 1.246,
      "step": 1207
    },
    {
      "epoch": 0.16982988893575143,
      "grad_norm": 1.6215029954910278,
      "learning_rate": 0.00014162991108791079,
      "loss": 1.1795,
      "step": 1208
    },
    {
      "epoch": 0.1699704765921552,
      "grad_norm": 1.6002600193023682,
      "learning_rate": 0.00014141843108752796,
      "loss": 1.2587,
      "step": 1209
    },
    {
      "epoch": 0.17011106424855899,
      "grad_norm": 1.685610294342041,
      "learning_rate": 0.00014120672725350135,
      "loss": 1.0208,
      "step": 1210
    },
    {
      "epoch": 0.17025165190496275,
      "grad_norm": 1.4858587980270386,
      "learning_rate": 0.00014099480072992166,
      "loss": 1.1563,
      "step": 1211
    },
    {
      "epoch": 0.1703922395613665,
      "grad_norm": 1.5352383852005005,
      "learning_rate": 0.00014078265266208298,
      "loss": 1.242,
      "step": 1212
    },
    {
      "epoch": 0.17053282721777027,
      "grad_norm": 2.1248230934143066,
      "learning_rate": 0.00014057028419647678,
      "loss": 1.1326,
      "step": 1213
    },
    {
      "epoch": 0.17067341487417403,
      "grad_norm": 1.6415833234786987,
      "learning_rate": 0.00014035769648078545,
      "loss": 1.0923,
      "step": 1214
    },
    {
      "epoch": 0.17081400253057782,
      "grad_norm": 1.8061373233795166,
      "learning_rate": 0.0001401448906638764,
      "loss": 1.0783,
      "step": 1215
    },
    {
      "epoch": 0.1709545901869816,
      "grad_norm": 1.5099493265151978,
      "learning_rate": 0.00013993186789579558,
      "loss": 1.0227,
      "step": 1216
    },
    {
      "epoch": 0.17109517784338535,
      "grad_norm": 1.5410233736038208,
      "learning_rate": 0.00013971862932776153,
      "loss": 0.9975,
      "step": 1217
    },
    {
      "epoch": 0.1712357654997891,
      "grad_norm": 1.7846852540969849,
      "learning_rate": 0.00013950517611215882,
      "loss": 1.0035,
      "step": 1218
    },
    {
      "epoch": 0.17137635315619287,
      "grad_norm": 1.6204864978790283,
      "learning_rate": 0.00013929150940253225,
      "loss": 1.1605,
      "step": 1219
    },
    {
      "epoch": 0.17151694081259666,
      "grad_norm": 1.7248669862747192,
      "learning_rate": 0.00013907763035358022,
      "loss": 1.0879,
      "step": 1220
    },
    {
      "epoch": 0.17165752846900043,
      "grad_norm": 2.0563011169433594,
      "learning_rate": 0.00013886354012114868,
      "loss": 1.0881,
      "step": 1221
    },
    {
      "epoch": 0.1717981161254042,
      "grad_norm": 1.5618643760681152,
      "learning_rate": 0.00013864923986222494,
      "loss": 1.0681,
      "step": 1222
    },
    {
      "epoch": 0.17193870378180795,
      "grad_norm": 1.4689831733703613,
      "learning_rate": 0.00013843473073493122,
      "loss": 0.9956,
      "step": 1223
    },
    {
      "epoch": 0.1720792914382117,
      "grad_norm": 1.5191527605056763,
      "learning_rate": 0.0001382200138985186,
      "loss": 1.103,
      "step": 1224
    },
    {
      "epoch": 0.1722198790946155,
      "grad_norm": 1.5009140968322754,
      "learning_rate": 0.00013800509051336066,
      "loss": 1.0746,
      "step": 1225
    },
    {
      "epoch": 0.17236046675101926,
      "grad_norm": 1.7424370050430298,
      "learning_rate": 0.00013778996174094714,
      "loss": 1.0526,
      "step": 1226
    },
    {
      "epoch": 0.17250105440742303,
      "grad_norm": 1.7765791416168213,
      "learning_rate": 0.00013757462874387777,
      "loss": 1.3221,
      "step": 1227
    },
    {
      "epoch": 0.1726416420638268,
      "grad_norm": 1.5429083108901978,
      "learning_rate": 0.00013735909268585596,
      "loss": 1.1067,
      "step": 1228
    },
    {
      "epoch": 0.17278222972023055,
      "grad_norm": 1.735177993774414,
      "learning_rate": 0.0001371433547316825,
      "loss": 1.137,
      "step": 1229
    },
    {
      "epoch": 0.17292281737663434,
      "grad_norm": 1.5234744548797607,
      "learning_rate": 0.00013692741604724926,
      "loss": 1.1555,
      "step": 1230
    },
    {
      "epoch": 0.1730634050330381,
      "grad_norm": 1.7986416816711426,
      "learning_rate": 0.00013671127779953294,
      "loss": 1.1391,
      "step": 1231
    },
    {
      "epoch": 0.17320399268944187,
      "grad_norm": 1.5526134967803955,
      "learning_rate": 0.00013649494115658867,
      "loss": 1.0346,
      "step": 1232
    },
    {
      "epoch": 0.17334458034584563,
      "grad_norm": 1.5493760108947754,
      "learning_rate": 0.00013627840728754376,
      "loss": 1.2556,
      "step": 1233
    },
    {
      "epoch": 0.1734851680022494,
      "grad_norm": 1.7535312175750732,
      "learning_rate": 0.00013606167736259137,
      "loss": 1.0871,
      "step": 1234
    },
    {
      "epoch": 0.17362575565865318,
      "grad_norm": 1.5342096090316772,
      "learning_rate": 0.00013584475255298419,
      "loss": 1.0869,
      "step": 1235
    },
    {
      "epoch": 0.17376634331505694,
      "grad_norm": 1.6589802503585815,
      "learning_rate": 0.0001356276340310281,
      "loss": 1.3657,
      "step": 1236
    },
    {
      "epoch": 0.1739069309714607,
      "grad_norm": 1.9831687211990356,
      "learning_rate": 0.00013541032297007585,
      "loss": 1.1042,
      "step": 1237
    },
    {
      "epoch": 0.17404751862786447,
      "grad_norm": 2.1309776306152344,
      "learning_rate": 0.0001351928205445207,
      "loss": 1.2484,
      "step": 1238
    },
    {
      "epoch": 0.17418810628426823,
      "grad_norm": 1.6874473094940186,
      "learning_rate": 0.00013497512792979007,
      "loss": 1.0093,
      "step": 1239
    },
    {
      "epoch": 0.17432869394067202,
      "grad_norm": 1.5356870889663696,
      "learning_rate": 0.00013475724630233933,
      "loss": 1.0726,
      "step": 1240
    },
    {
      "epoch": 0.17446928159707578,
      "grad_norm": 1.7879536151885986,
      "learning_rate": 0.0001345391768396451,
      "loss": 1.136,
      "step": 1241
    },
    {
      "epoch": 0.17460986925347954,
      "grad_norm": 1.7186193466186523,
      "learning_rate": 0.00013432092072019925,
      "loss": 1.2223,
      "step": 1242
    },
    {
      "epoch": 0.1747504569098833,
      "grad_norm": 1.8174771070480347,
      "learning_rate": 0.00013410247912350228,
      "loss": 1.1058,
      "step": 1243
    },
    {
      "epoch": 0.17489104456628707,
      "grad_norm": 1.5616546869277954,
      "learning_rate": 0.00013388385323005717,
      "loss": 1.0496,
      "step": 1244
    },
    {
      "epoch": 0.17503163222269086,
      "grad_norm": 1.7404396533966064,
      "learning_rate": 0.00013366504422136278,
      "loss": 1.1168,
      "step": 1245
    },
    {
      "epoch": 0.17517221987909462,
      "grad_norm": 1.4800913333892822,
      "learning_rate": 0.00013344605327990757,
      "loss": 1.1511,
      "step": 1246
    },
    {
      "epoch": 0.17531280753549838,
      "grad_norm": 1.711107611656189,
      "learning_rate": 0.00013322688158916326,
      "loss": 1.1268,
      "step": 1247
    },
    {
      "epoch": 0.17545339519190215,
      "grad_norm": 2.1455037593841553,
      "learning_rate": 0.0001330075303335783,
      "loss": 1.0663,
      "step": 1248
    },
    {
      "epoch": 0.1755939828483059,
      "grad_norm": 1.6677528619766235,
      "learning_rate": 0.00013278800069857166,
      "loss": 1.0846,
      "step": 1249
    },
    {
      "epoch": 0.1757345705047097,
      "grad_norm": 1.597369909286499,
      "learning_rate": 0.00013256829387052615,
      "loss": 0.9002,
      "step": 1250
    },
    {
      "epoch": 0.17587515816111346,
      "grad_norm": 1.6138945817947388,
      "learning_rate": 0.00013234841103678232,
      "loss": 1.1126,
      "step": 1251
    },
    {
      "epoch": 0.17601574581751722,
      "grad_norm": 1.4623186588287354,
      "learning_rate": 0.00013212835338563176,
      "loss": 1.1943,
      "step": 1252
    },
    {
      "epoch": 0.17615633347392098,
      "grad_norm": 1.5830717086791992,
      "learning_rate": 0.00013190812210631093,
      "loss": 1.1907,
      "step": 1253
    },
    {
      "epoch": 0.17629692113032475,
      "grad_norm": 1.4829117059707642,
      "learning_rate": 0.00013168771838899447,
      "loss": 1.1644,
      "step": 1254
    },
    {
      "epoch": 0.17643750878672854,
      "grad_norm": 1.547098159790039,
      "learning_rate": 0.00013146714342478907,
      "loss": 1.0827,
      "step": 1255
    },
    {
      "epoch": 0.1765780964431323,
      "grad_norm": 1.6194298267364502,
      "learning_rate": 0.0001312463984057268,
      "loss": 1.1379,
      "step": 1256
    },
    {
      "epoch": 0.17671868409953606,
      "grad_norm": 1.568115234375,
      "learning_rate": 0.00013102548452475864,
      "loss": 1.1859,
      "step": 1257
    },
    {
      "epoch": 0.17685927175593982,
      "grad_norm": 1.833924412727356,
      "learning_rate": 0.00013080440297574833,
      "loss": 1.082,
      "step": 1258
    },
    {
      "epoch": 0.17699985941234359,
      "grad_norm": 1.5304882526397705,
      "learning_rate": 0.0001305831549534656,
      "loss": 1.1773,
      "step": 1259
    },
    {
      "epoch": 0.17714044706874738,
      "grad_norm": 1.4984384775161743,
      "learning_rate": 0.00013036174165357978,
      "loss": 1.2305,
      "step": 1260
    },
    {
      "epoch": 0.17728103472515114,
      "grad_norm": 1.3323620557785034,
      "learning_rate": 0.00013014016427265364,
      "loss": 1.2288,
      "step": 1261
    },
    {
      "epoch": 0.1774216223815549,
      "grad_norm": 1.5911346673965454,
      "learning_rate": 0.00012991842400813635,
      "loss": 1.2362,
      "step": 1262
    },
    {
      "epoch": 0.17756221003795866,
      "grad_norm": 1.8944346904754639,
      "learning_rate": 0.00012969652205835756,
      "loss": 1.1027,
      "step": 1263
    },
    {
      "epoch": 0.17770279769436242,
      "grad_norm": 1.513683557510376,
      "learning_rate": 0.00012947445962252065,
      "loss": 1.062,
      "step": 1264
    },
    {
      "epoch": 0.17784338535076621,
      "grad_norm": 1.4962800741195679,
      "learning_rate": 0.00012925223790069624,
      "loss": 1.2281,
      "step": 1265
    },
    {
      "epoch": 0.17798397300716998,
      "grad_norm": 1.5849968194961548,
      "learning_rate": 0.00012902985809381588,
      "loss": 1.0707,
      "step": 1266
    },
    {
      "epoch": 0.17812456066357374,
      "grad_norm": 1.5764967203140259,
      "learning_rate": 0.00012880732140366528,
      "loss": 1.1237,
      "step": 1267
    },
    {
      "epoch": 0.1782651483199775,
      "grad_norm": 1.4192633628845215,
      "learning_rate": 0.00012858462903287814,
      "loss": 1.1249,
      "step": 1268
    },
    {
      "epoch": 0.17840573597638126,
      "grad_norm": 3.358327865600586,
      "learning_rate": 0.00012836178218492942,
      "loss": 1.2096,
      "step": 1269
    },
    {
      "epoch": 0.17854632363278505,
      "grad_norm": 1.4648364782333374,
      "learning_rate": 0.00012813878206412887,
      "loss": 1.137,
      "step": 1270
    },
    {
      "epoch": 0.17868691128918882,
      "grad_norm": 1.8063987493515015,
      "learning_rate": 0.00012791562987561462,
      "loss": 1.1124,
      "step": 1271
    },
    {
      "epoch": 0.17882749894559258,
      "grad_norm": 1.4417414665222168,
      "learning_rate": 0.0001276923268253466,
      "loss": 1.0889,
      "step": 1272
    },
    {
      "epoch": 0.17896808660199634,
      "grad_norm": 1.4105467796325684,
      "learning_rate": 0.00012746887412009997,
      "loss": 1.0915,
      "step": 1273
    },
    {
      "epoch": 0.1791086742584001,
      "grad_norm": 1.7349767684936523,
      "learning_rate": 0.00012724527296745873,
      "loss": 1.0046,
      "step": 1274
    },
    {
      "epoch": 0.1792492619148039,
      "grad_norm": 1.6212068796157837,
      "learning_rate": 0.00012702152457580906,
      "loss": 1.0946,
      "step": 1275
    },
    {
      "epoch": 0.17938984957120765,
      "grad_norm": 1.8262444734573364,
      "learning_rate": 0.0001267976301543329,
      "loss": 1.073,
      "step": 1276
    },
    {
      "epoch": 0.17953043722761142,
      "grad_norm": 1.565361738204956,
      "learning_rate": 0.00012657359091300125,
      "loss": 1.1366,
      "step": 1277
    },
    {
      "epoch": 0.17967102488401518,
      "grad_norm": 1.9158523082733154,
      "learning_rate": 0.00012634940806256797,
      "loss": 1.0816,
      "step": 1278
    },
    {
      "epoch": 0.17981161254041894,
      "grad_norm": 1.5099929571151733,
      "learning_rate": 0.00012612508281456281,
      "loss": 1.1825,
      "step": 1279
    },
    {
      "epoch": 0.17995220019682273,
      "grad_norm": 1.53780198097229,
      "learning_rate": 0.00012590061638128512,
      "loss": 1.1161,
      "step": 1280
    },
    {
      "epoch": 0.1800927878532265,
      "grad_norm": 1.6130229234695435,
      "learning_rate": 0.00012567600997579728,
      "loss": 1.0057,
      "step": 1281
    },
    {
      "epoch": 0.18023337550963026,
      "grad_norm": 1.6890398263931274,
      "learning_rate": 0.0001254512648119181,
      "loss": 1.2831,
      "step": 1282
    },
    {
      "epoch": 0.18037396316603402,
      "grad_norm": 1.6563471555709839,
      "learning_rate": 0.00012522638210421625,
      "loss": 0.8955,
      "step": 1283
    },
    {
      "epoch": 0.18051455082243778,
      "grad_norm": 2.2905988693237305,
      "learning_rate": 0.00012500136306800369,
      "loss": 1.0198,
      "step": 1284
    },
    {
      "epoch": 0.18065513847884157,
      "grad_norm": 1.3179066181182861,
      "learning_rate": 0.00012477620891932918,
      "loss": 1.3198,
      "step": 1285
    },
    {
      "epoch": 0.18079572613524533,
      "grad_norm": 1.7086939811706543,
      "learning_rate": 0.0001245509208749716,
      "loss": 0.8963,
      "step": 1286
    },
    {
      "epoch": 0.1809363137916491,
      "grad_norm": 1.8725422620773315,
      "learning_rate": 0.0001243255001524335,
      "loss": 1.1303,
      "step": 1287
    },
    {
      "epoch": 0.18107690144805286,
      "grad_norm": 1.6194939613342285,
      "learning_rate": 0.0001240999479699344,
      "loss": 1.1249,
      "step": 1288
    },
    {
      "epoch": 0.18121748910445662,
      "grad_norm": 1.5600429773330688,
      "learning_rate": 0.00012387426554640426,
      "loss": 1.0468,
      "step": 1289
    },
    {
      "epoch": 0.1813580767608604,
      "grad_norm": 1.4525588750839233,
      "learning_rate": 0.0001236484541014769,
      "loss": 0.9897,
      "step": 1290
    },
    {
      "epoch": 0.18149866441726417,
      "grad_norm": 1.6308281421661377,
      "learning_rate": 0.0001234225148554834,
      "loss": 1.1793,
      "step": 1291
    },
    {
      "epoch": 0.18163925207366793,
      "grad_norm": 1.4816070795059204,
      "learning_rate": 0.0001231964490294455,
      "loss": 1.2532,
      "step": 1292
    },
    {
      "epoch": 0.1817798397300717,
      "grad_norm": 1.430775761604309,
      "learning_rate": 0.00012297025784506897,
      "loss": 1.3044,
      "step": 1293
    },
    {
      "epoch": 0.18192042738647546,
      "grad_norm": 1.6915491819381714,
      "learning_rate": 0.00012274394252473713,
      "loss": 1.0323,
      "step": 1294
    },
    {
      "epoch": 0.18206101504287925,
      "grad_norm": 1.6300327777862549,
      "learning_rate": 0.00012251750429150406,
      "loss": 0.9453,
      "step": 1295
    },
    {
      "epoch": 0.182201602699283,
      "grad_norm": 1.6402223110198975,
      "learning_rate": 0.00012229094436908813,
      "loss": 1.0659,
      "step": 1296
    },
    {
      "epoch": 0.18234219035568677,
      "grad_norm": 1.405741572380066,
      "learning_rate": 0.00012206426398186534,
      "loss": 1.231,
      "step": 1297
    },
    {
      "epoch": 0.18248277801209054,
      "grad_norm": 1.4749175310134888,
      "learning_rate": 0.00012183746435486274,
      "loss": 1.0851,
      "step": 1298
    },
    {
      "epoch": 0.1826233656684943,
      "grad_norm": 1.671188235282898,
      "learning_rate": 0.0001216105467137517,
      "loss": 0.9777,
      "step": 1299
    },
    {
      "epoch": 0.1827639533248981,
      "grad_norm": 1.5334643125534058,
      "learning_rate": 0.00012138351228484142,
      "loss": 1.1589,
      "step": 1300
    },
    {
      "epoch": 0.18290454098130185,
      "grad_norm": 1.3781603574752808,
      "learning_rate": 0.00012115636229507223,
      "loss": 0.9816,
      "step": 1301
    },
    {
      "epoch": 0.1830451286377056,
      "grad_norm": 1.6746119260787964,
      "learning_rate": 0.00012092909797200897,
      "loss": 1.1228,
      "step": 1302
    },
    {
      "epoch": 0.18318571629410937,
      "grad_norm": 1.8171316385269165,
      "learning_rate": 0.00012070172054383438,
      "loss": 1.0219,
      "step": 1303
    },
    {
      "epoch": 0.18332630395051314,
      "grad_norm": 1.5420523881912231,
      "learning_rate": 0.00012047423123934241,
      "loss": 1.0635,
      "step": 1304
    },
    {
      "epoch": 0.18346689160691693,
      "grad_norm": 1.3950395584106445,
      "learning_rate": 0.00012024663128793164,
      "loss": 1.1086,
      "step": 1305
    },
    {
      "epoch": 0.1836074792633207,
      "grad_norm": 1.4450749158859253,
      "learning_rate": 0.00012001892191959857,
      "loss": 1.249,
      "step": 1306
    },
    {
      "epoch": 0.18374806691972445,
      "grad_norm": 1.4706854820251465,
      "learning_rate": 0.00011979110436493104,
      "loss": 0.964,
      "step": 1307
    },
    {
      "epoch": 0.1838886545761282,
      "grad_norm": 1.8329100608825684,
      "learning_rate": 0.00011956317985510162,
      "loss": 1.0893,
      "step": 1308
    },
    {
      "epoch": 0.18402924223253198,
      "grad_norm": 1.520957589149475,
      "learning_rate": 0.00011933514962186074,
      "loss": 1.1175,
      "step": 1309
    },
    {
      "epoch": 0.18416982988893574,
      "grad_norm": 1.702605128288269,
      "learning_rate": 0.00011910701489753031,
      "loss": 0.9617,
      "step": 1310
    },
    {
      "epoch": 0.18431041754533953,
      "grad_norm": 1.5094081163406372,
      "learning_rate": 0.00011887877691499686,
      "loss": 1.0702,
      "step": 1311
    },
    {
      "epoch": 0.1844510052017433,
      "grad_norm": 1.400079369544983,
      "learning_rate": 0.00011865043690770496,
      "loss": 1.0654,
      "step": 1312
    },
    {
      "epoch": 0.18459159285814705,
      "grad_norm": 1.9245373010635376,
      "learning_rate": 0.00011842199610965057,
      "loss": 1.1123,
      "step": 1313
    },
    {
      "epoch": 0.18473218051455081,
      "grad_norm": 1.4354326725006104,
      "learning_rate": 0.0001181934557553743,
      "loss": 1.1156,
      "step": 1314
    },
    {
      "epoch": 0.18487276817095458,
      "grad_norm": 1.7361444234848022,
      "learning_rate": 0.00011796481707995484,
      "loss": 1.1486,
      "step": 1315
    },
    {
      "epoch": 0.18501335582735837,
      "grad_norm": 1.7279713153839111,
      "learning_rate": 0.00011773608131900211,
      "loss": 1.1953,
      "step": 1316
    },
    {
      "epoch": 0.18515394348376213,
      "grad_norm": 1.4192051887512207,
      "learning_rate": 0.0001175072497086509,
      "loss": 1.1249,
      "step": 1317
    },
    {
      "epoch": 0.1852945311401659,
      "grad_norm": 1.815427303314209,
      "learning_rate": 0.00011727832348555383,
      "loss": 0.9903,
      "step": 1318
    },
    {
      "epoch": 0.18543511879656965,
      "grad_norm": 1.6502643823623657,
      "learning_rate": 0.00011704930388687488,
      "loss": 1.141,
      "step": 1319
    },
    {
      "epoch": 0.18557570645297342,
      "grad_norm": 1.464217185974121,
      "learning_rate": 0.00011682019215028262,
      "loss": 1.1862,
      "step": 1320
    },
    {
      "epoch": 0.1857162941093772,
      "grad_norm": 1.5675817728042603,
      "learning_rate": 0.00011659098951394356,
      "loss": 1.1354,
      "step": 1321
    },
    {
      "epoch": 0.18585688176578097,
      "grad_norm": 1.4344136714935303,
      "learning_rate": 0.00011636169721651548,
      "loss": 1.1758,
      "step": 1322
    },
    {
      "epoch": 0.18599746942218473,
      "grad_norm": 1.5571012496948242,
      "learning_rate": 0.00011613231649714073,
      "loss": 1.0417,
      "step": 1323
    },
    {
      "epoch": 0.1861380570785885,
      "grad_norm": 1.7190297842025757,
      "learning_rate": 0.00011590284859543943,
      "loss": 1.1797,
      "step": 1324
    },
    {
      "epoch": 0.18627864473499225,
      "grad_norm": 1.6015808582305908,
      "learning_rate": 0.00011567329475150288,
      "loss": 1.1435,
      "step": 1325
    },
    {
      "epoch": 0.18641923239139604,
      "grad_norm": 1.7364178895950317,
      "learning_rate": 0.00011544365620588687,
      "loss": 1.0509,
      "step": 1326
    },
    {
      "epoch": 0.1865598200477998,
      "grad_norm": 1.5401281118392944,
      "learning_rate": 0.00011521393419960489,
      "loss": 1.0546,
      "step": 1327
    },
    {
      "epoch": 0.18670040770420357,
      "grad_norm": 1.7021801471710205,
      "learning_rate": 0.00011498412997412151,
      "loss": 1.1009,
      "step": 1328
    },
    {
      "epoch": 0.18684099536060733,
      "grad_norm": 1.7119665145874023,
      "learning_rate": 0.00011475424477134557,
      "loss": 0.8585,
      "step": 1329
    },
    {
      "epoch": 0.1869815830170111,
      "grad_norm": 1.5866222381591797,
      "learning_rate": 0.00011452427983362362,
      "loss": 1.0964,
      "step": 1330
    },
    {
      "epoch": 0.18712217067341488,
      "grad_norm": 1.646464228630066,
      "learning_rate": 0.00011429423640373297,
      "loss": 1.1932,
      "step": 1331
    },
    {
      "epoch": 0.18726275832981865,
      "grad_norm": 1.5609792470932007,
      "learning_rate": 0.0001140641157248753,
      "loss": 1.0946,
      "step": 1332
    },
    {
      "epoch": 0.1874033459862224,
      "grad_norm": 1.5953153371810913,
      "learning_rate": 0.00011383391904066958,
      "loss": 1.0399,
      "step": 1333
    },
    {
      "epoch": 0.18754393364262617,
      "grad_norm": 1.5688011646270752,
      "learning_rate": 0.00011360364759514566,
      "loss": 1.1094,
      "step": 1334
    },
    {
      "epoch": 0.18768452129902993,
      "grad_norm": 1.4564937353134155,
      "learning_rate": 0.00011337330263273735,
      "loss": 1.1111,
      "step": 1335
    },
    {
      "epoch": 0.18782510895543372,
      "grad_norm": 1.6002782583236694,
      "learning_rate": 0.00011314288539827576,
      "loss": 0.9529,
      "step": 1336
    },
    {
      "epoch": 0.18796569661183749,
      "grad_norm": 1.491550087928772,
      "learning_rate": 0.00011291239713698263,
      "loss": 1.0693,
      "step": 1337
    },
    {
      "epoch": 0.18810628426824125,
      "grad_norm": 1.4589107036590576,
      "learning_rate": 0.00011268183909446348,
      "loss": 1.1449,
      "step": 1338
    },
    {
      "epoch": 0.188246871924645,
      "grad_norm": 1.3745561838150024,
      "learning_rate": 0.00011245121251670099,
      "loss": 1.224,
      "step": 1339
    },
    {
      "epoch": 0.18838745958104877,
      "grad_norm": 1.3769426345825195,
      "learning_rate": 0.00011222051865004814,
      "loss": 1.2929,
      "step": 1340
    },
    {
      "epoch": 0.18852804723745256,
      "grad_norm": 1.698346734046936,
      "learning_rate": 0.00011198975874122167,
      "loss": 1.1087,
      "step": 1341
    },
    {
      "epoch": 0.18866863489385632,
      "grad_norm": 1.5066137313842773,
      "learning_rate": 0.00011175893403729511,
      "loss": 1.2172,
      "step": 1342
    },
    {
      "epoch": 0.1888092225502601,
      "grad_norm": 1.5342847108840942,
      "learning_rate": 0.00011152804578569225,
      "loss": 1.1171,
      "step": 1343
    },
    {
      "epoch": 0.18894981020666385,
      "grad_norm": 1.5742909908294678,
      "learning_rate": 0.00011129709523418024,
      "loss": 1.0647,
      "step": 1344
    },
    {
      "epoch": 0.1890903978630676,
      "grad_norm": 1.591515302658081,
      "learning_rate": 0.00011106608363086293,
      "loss": 1.2038,
      "step": 1345
    },
    {
      "epoch": 0.1892309855194714,
      "grad_norm": 1.5007787942886353,
      "learning_rate": 0.00011083501222417411,
      "loss": 1.2787,
      "step": 1346
    },
    {
      "epoch": 0.18937157317587516,
      "grad_norm": 1.6680328845977783,
      "learning_rate": 0.00011060388226287079,
      "loss": 1.0636,
      "step": 1347
    },
    {
      "epoch": 0.18951216083227893,
      "grad_norm": 1.3917148113250732,
      "learning_rate": 0.00011037269499602637,
      "loss": 1.1025,
      "step": 1348
    },
    {
      "epoch": 0.1896527484886827,
      "grad_norm": 1.5578041076660156,
      "learning_rate": 0.00011014145167302396,
      "loss": 1.3563,
      "step": 1349
    },
    {
      "epoch": 0.18979333614508645,
      "grad_norm": 1.5441862344741821,
      "learning_rate": 0.00010991015354354962,
      "loss": 1.0881,
      "step": 1350
    },
    {
      "epoch": 0.18993392380149024,
      "grad_norm": 1.6318796873092651,
      "learning_rate": 0.00010967880185758557,
      "loss": 1.076,
      "step": 1351
    },
    {
      "epoch": 0.190074511457894,
      "grad_norm": 1.6025941371917725,
      "learning_rate": 0.00010944739786540348,
      "loss": 1.1541,
      "step": 1352
    },
    {
      "epoch": 0.19021509911429776,
      "grad_norm": 1.658018946647644,
      "learning_rate": 0.0001092159428175577,
      "loss": 1.0439,
      "step": 1353
    },
    {
      "epoch": 0.19035568677070153,
      "grad_norm": 1.637986183166504,
      "learning_rate": 0.0001089844379648785,
      "loss": 0.9705,
      "step": 1354
    },
    {
      "epoch": 0.1904962744271053,
      "grad_norm": 2.000197410583496,
      "learning_rate": 0.00010875288455846519,
      "loss": 1.0934,
      "step": 1355
    },
    {
      "epoch": 0.19063686208350908,
      "grad_norm": 1.6505094766616821,
      "learning_rate": 0.00010852128384967975,
      "loss": 1.3775,
      "step": 1356
    },
    {
      "epoch": 0.19077744973991284,
      "grad_norm": 1.5769799947738647,
      "learning_rate": 0.00010828963709013948,
      "loss": 1.135,
      "step": 1357
    },
    {
      "epoch": 0.1909180373963166,
      "grad_norm": 1.4312957525253296,
      "learning_rate": 0.00010805794553171076,
      "loss": 1.2252,
      "step": 1358
    },
    {
      "epoch": 0.19105862505272037,
      "grad_norm": 1.4554307460784912,
      "learning_rate": 0.00010782621042650194,
      "loss": 1.2334,
      "step": 1359
    },
    {
      "epoch": 0.19119921270912413,
      "grad_norm": 1.6789618730545044,
      "learning_rate": 0.00010759443302685679,
      "loss": 1.0231,
      "step": 1360
    },
    {
      "epoch": 0.19133980036552792,
      "grad_norm": 1.475894808769226,
      "learning_rate": 0.00010736261458534764,
      "loss": 1.1066,
      "step": 1361
    },
    {
      "epoch": 0.19148038802193168,
      "grad_norm": 1.6469441652297974,
      "learning_rate": 0.00010713075635476854,
      "loss": 1.1275,
      "step": 1362
    },
    {
      "epoch": 0.19162097567833544,
      "grad_norm": 1.6057630777359009,
      "learning_rate": 0.00010689885958812871,
      "loss": 1.0155,
      "step": 1363
    },
    {
      "epoch": 0.1917615633347392,
      "grad_norm": 1.4159632921218872,
      "learning_rate": 0.00010666692553864545,
      "loss": 1.0388,
      "step": 1364
    },
    {
      "epoch": 0.19190215099114297,
      "grad_norm": 1.377211332321167,
      "learning_rate": 0.0001064349554597377,
      "loss": 1.1202,
      "step": 1365
    },
    {
      "epoch": 0.19204273864754676,
      "grad_norm": 1.4382482767105103,
      "learning_rate": 0.00010620295060501901,
      "loss": 0.9757,
      "step": 1366
    },
    {
      "epoch": 0.19218332630395052,
      "grad_norm": 1.9213197231292725,
      "learning_rate": 0.00010597091222829097,
      "loss": 1.1281,
      "step": 1367
    },
    {
      "epoch": 0.19232391396035428,
      "grad_norm": 1.5118399858474731,
      "learning_rate": 0.0001057388415835362,
      "loss": 1.1234,
      "step": 1368
    },
    {
      "epoch": 0.19246450161675804,
      "grad_norm": 1.5118408203125,
      "learning_rate": 0.00010550673992491178,
      "loss": 1.0254,
      "step": 1369
    },
    {
      "epoch": 0.1926050892731618,
      "grad_norm": 1.9071398973464966,
      "learning_rate": 0.00010527460850674237,
      "loss": 1.0211,
      "step": 1370
    },
    {
      "epoch": 0.1927456769295656,
      "grad_norm": 1.5908268690109253,
      "learning_rate": 0.00010504244858351353,
      "loss": 1.1597,
      "step": 1371
    },
    {
      "epoch": 0.19288626458596936,
      "grad_norm": 1.800240159034729,
      "learning_rate": 0.00010481026140986474,
      "loss": 0.8931,
      "step": 1372
    },
    {
      "epoch": 0.19302685224237312,
      "grad_norm": 1.702511191368103,
      "learning_rate": 0.00010457804824058282,
      "loss": 1.0382,
      "step": 1373
    },
    {
      "epoch": 0.19316743989877688,
      "grad_norm": 1.5173929929733276,
      "learning_rate": 0.0001043458103305951,
      "loss": 1.2864,
      "step": 1374
    },
    {
      "epoch": 0.19330802755518064,
      "grad_norm": 1.554733395576477,
      "learning_rate": 0.00010411354893496252,
      "loss": 1.1777,
      "step": 1375
    },
    {
      "epoch": 0.19344861521158443,
      "grad_norm": 1.7179187536239624,
      "learning_rate": 0.00010388126530887307,
      "loss": 1.1325,
      "step": 1376
    },
    {
      "epoch": 0.1935892028679882,
      "grad_norm": 1.6883199214935303,
      "learning_rate": 0.00010364896070763482,
      "loss": 1.1589,
      "step": 1377
    },
    {
      "epoch": 0.19372979052439196,
      "grad_norm": 1.8239678144454956,
      "learning_rate": 0.00010341663638666912,
      "loss": 1.0661,
      "step": 1378
    },
    {
      "epoch": 0.19387037818079572,
      "grad_norm": 1.7536067962646484,
      "learning_rate": 0.000103184293601504,
      "loss": 1.1754,
      "step": 1379
    },
    {
      "epoch": 0.19401096583719948,
      "grad_norm": 1.6542768478393555,
      "learning_rate": 0.00010295193360776721,
      "loss": 1.2543,
      "step": 1380
    },
    {
      "epoch": 0.19415155349360327,
      "grad_norm": 1.7918944358825684,
      "learning_rate": 0.00010271955766117951,
      "loss": 1.2192,
      "step": 1381
    },
    {
      "epoch": 0.19429214115000704,
      "grad_norm": 1.6903663873672485,
      "learning_rate": 0.0001024871670175479,
      "loss": 1.1681,
      "step": 1382
    },
    {
      "epoch": 0.1944327288064108,
      "grad_norm": 1.529369592666626,
      "learning_rate": 0.00010225476293275877,
      "loss": 1.1418,
      "step": 1383
    },
    {
      "epoch": 0.19457331646281456,
      "grad_norm": 1.425983190536499,
      "learning_rate": 0.00010202234666277115,
      "loss": 1.0477,
      "step": 1384
    },
    {
      "epoch": 0.19471390411921832,
      "grad_norm": 1.654103398323059,
      "learning_rate": 0.00010178991946360998,
      "loss": 1.164,
      "step": 1385
    },
    {
      "epoch": 0.1948544917756221,
      "grad_norm": 1.5258557796478271,
      "learning_rate": 0.0001015574825913592,
      "loss": 1.0823,
      "step": 1386
    },
    {
      "epoch": 0.19499507943202588,
      "grad_norm": 1.6584981679916382,
      "learning_rate": 0.00010132503730215505,
      "loss": 1.1922,
      "step": 1387
    },
    {
      "epoch": 0.19513566708842964,
      "grad_norm": 1.5751091241836548,
      "learning_rate": 0.00010109258485217923,
      "loss": 1.0277,
      "step": 1388
    },
    {
      "epoch": 0.1952762547448334,
      "grad_norm": 1.5051928758621216,
      "learning_rate": 0.00010086012649765218,
      "loss": 1.2561,
      "step": 1389
    },
    {
      "epoch": 0.19541684240123716,
      "grad_norm": 1.5144636631011963,
      "learning_rate": 0.00010062766349482622,
      "loss": 1.1918,
      "step": 1390
    },
    {
      "epoch": 0.19555743005764095,
      "grad_norm": 1.8928519487380981,
      "learning_rate": 0.00010039519709997877,
      "loss": 1.0145,
      "step": 1391
    },
    {
      "epoch": 0.19569801771404471,
      "grad_norm": 1.6500763893127441,
      "learning_rate": 0.00010016272856940567,
      "loss": 1.1139,
      "step": 1392
    },
    {
      "epoch": 0.19583860537044848,
      "grad_norm": 1.5982919931411743,
      "learning_rate": 9.993025915941418e-05,
      "loss": 1.1678,
      "step": 1393
    },
    {
      "epoch": 0.19597919302685224,
      "grad_norm": 1.514268398284912,
      "learning_rate": 9.969779012631643e-05,
      "loss": 1.162,
      "step": 1394
    },
    {
      "epoch": 0.196119780683256,
      "grad_norm": 1.5842721462249756,
      "learning_rate": 9.946532272642243e-05,
      "loss": 0.9992,
      "step": 1395
    },
    {
      "epoch": 0.1962603683396598,
      "grad_norm": 1.8290197849273682,
      "learning_rate": 9.923285821603341e-05,
      "loss": 1.1016,
      "step": 1396
    },
    {
      "epoch": 0.19640095599606355,
      "grad_norm": 1.3618017435073853,
      "learning_rate": 9.900039785143495e-05,
      "loss": 0.9754,
      "step": 1397
    },
    {
      "epoch": 0.19654154365246732,
      "grad_norm": 1.7173397541046143,
      "learning_rate": 9.876794288889024e-05,
      "loss": 1.0833,
      "step": 1398
    },
    {
      "epoch": 0.19668213130887108,
      "grad_norm": 1.6006220579147339,
      "learning_rate": 9.853549458463329e-05,
      "loss": 1.1778,
      "step": 1399
    },
    {
      "epoch": 0.19682271896527484,
      "grad_norm": 1.4291013479232788,
      "learning_rate": 9.830305419486212e-05,
      "loss": 1.0909,
      "step": 1400
    },
    {
      "epoch": 0.19696330662167863,
      "grad_norm": 1.3172802925109863,
      "learning_rate": 9.807062297573196e-05,
      "loss": 1.2324,
      "step": 1401
    },
    {
      "epoch": 0.1971038942780824,
      "grad_norm": 1.4713881015777588,
      "learning_rate": 9.783820218334848e-05,
      "loss": 1.1855,
      "step": 1402
    },
    {
      "epoch": 0.19724448193448615,
      "grad_norm": 1.5639885663986206,
      "learning_rate": 9.760579307376104e-05,
      "loss": 1.074,
      "step": 1403
    },
    {
      "epoch": 0.19738506959088992,
      "grad_norm": 1.7781959772109985,
      "learning_rate": 9.737339690295583e-05,
      "loss": 0.9695,
      "step": 1404
    },
    {
      "epoch": 0.19752565724729368,
      "grad_norm": 1.857828974723816,
      "learning_rate": 9.714101492684913e-05,
      "loss": 1.0935,
      "step": 1405
    },
    {
      "epoch": 0.19766624490369747,
      "grad_norm": 1.5970592498779297,
      "learning_rate": 9.69086484012805e-05,
      "loss": 1.1547,
      "step": 1406
    },
    {
      "epoch": 0.19780683256010123,
      "grad_norm": 1.5884848833084106,
      "learning_rate": 9.667629858200602e-05,
      "loss": 1.2728,
      "step": 1407
    },
    {
      "epoch": 0.197947420216505,
      "grad_norm": 1.6224325895309448,
      "learning_rate": 9.644396672469145e-05,
      "loss": 1.1774,
      "step": 1408
    },
    {
      "epoch": 0.19808800787290876,
      "grad_norm": 1.6667064428329468,
      "learning_rate": 9.621165408490552e-05,
      "loss": 1.2981,
      "step": 1409
    },
    {
      "epoch": 0.19822859552931252,
      "grad_norm": 1.56403386592865,
      "learning_rate": 9.597936191811307e-05,
      "loss": 1.0784,
      "step": 1410
    },
    {
      "epoch": 0.1983691831857163,
      "grad_norm": 1.404476523399353,
      "learning_rate": 9.574709147966834e-05,
      "loss": 1.0627,
      "step": 1411
    },
    {
      "epoch": 0.19850977084212007,
      "grad_norm": 1.4382264614105225,
      "learning_rate": 9.55148440248081e-05,
      "loss": 1.1818,
      "step": 1412
    },
    {
      "epoch": 0.19865035849852383,
      "grad_norm": 1.2532752752304077,
      "learning_rate": 9.528262080864496e-05,
      "loss": 1.2242,
      "step": 1413
    },
    {
      "epoch": 0.1987909461549276,
      "grad_norm": 1.4197824001312256,
      "learning_rate": 9.505042308616046e-05,
      "loss": 1.1909,
      "step": 1414
    },
    {
      "epoch": 0.19893153381133136,
      "grad_norm": 1.6274960041046143,
      "learning_rate": 9.481825211219847e-05,
      "loss": 1.1785,
      "step": 1415
    },
    {
      "epoch": 0.19907212146773512,
      "grad_norm": 1.521952748298645,
      "learning_rate": 9.458610914145826e-05,
      "loss": 1.0457,
      "step": 1416
    },
    {
      "epoch": 0.1992127091241389,
      "grad_norm": 1.5183908939361572,
      "learning_rate": 9.435399542848773e-05,
      "loss": 1.141,
      "step": 1417
    },
    {
      "epoch": 0.19935329678054267,
      "grad_norm": 1.4806602001190186,
      "learning_rate": 9.412191222767673e-05,
      "loss": 1.0853,
      "step": 1418
    },
    {
      "epoch": 0.19949388443694643,
      "grad_norm": 1.4998570680618286,
      "learning_rate": 9.388986079325012e-05,
      "loss": 1.0522,
      "step": 1419
    },
    {
      "epoch": 0.1996344720933502,
      "grad_norm": 1.5656377077102661,
      "learning_rate": 9.365784237926122e-05,
      "loss": 1.2075,
      "step": 1420
    },
    {
      "epoch": 0.19977505974975396,
      "grad_norm": 1.370351791381836,
      "learning_rate": 9.342585823958481e-05,
      "loss": 1.2778,
      "step": 1421
    },
    {
      "epoch": 0.19991564740615775,
      "grad_norm": 1.576250672340393,
      "learning_rate": 9.319390962791044e-05,
      "loss": 1.1349,
      "step": 1422
    },
    {
      "epoch": 0.2000562350625615,
      "grad_norm": 1.7526202201843262,
      "learning_rate": 9.296199779773569e-05,
      "loss": 0.9806,
      "step": 1423
    },
    {
      "epoch": 0.20019682271896527,
      "grad_norm": 1.5378177165985107,
      "learning_rate": 9.273012400235938e-05,
      "loss": 1.3376,
      "step": 1424
    },
    {
      "epoch": 0.20033741037536903,
      "grad_norm": 1.6682908535003662,
      "learning_rate": 9.249828949487478e-05,
      "loss": 1.0819,
      "step": 1425
    },
    {
      "epoch": 0.2004779980317728,
      "grad_norm": 1.3697816133499146,
      "learning_rate": 9.226649552816277e-05,
      "loss": 1.2011,
      "step": 1426
    },
    {
      "epoch": 0.2006185856881766,
      "grad_norm": 1.4844970703125,
      "learning_rate": 9.203474335488524e-05,
      "loss": 1.1425,
      "step": 1427
    },
    {
      "epoch": 0.20075917334458035,
      "grad_norm": 1.4531599283218384,
      "learning_rate": 9.180303422747814e-05,
      "loss": 1.0955,
      "step": 1428
    },
    {
      "epoch": 0.2008997610009841,
      "grad_norm": 2.0588364601135254,
      "learning_rate": 9.157136939814481e-05,
      "loss": 1.1304,
      "step": 1429
    },
    {
      "epoch": 0.20104034865738787,
      "grad_norm": 1.7277637720108032,
      "learning_rate": 9.133975011884925e-05,
      "loss": 1.1107,
      "step": 1430
    },
    {
      "epoch": 0.20118093631379164,
      "grad_norm": 1.5379058122634888,
      "learning_rate": 9.110817764130925e-05,
      "loss": 1.0152,
      "step": 1431
    },
    {
      "epoch": 0.20132152397019543,
      "grad_norm": 1.8112249374389648,
      "learning_rate": 9.087665321698962e-05,
      "loss": 1.1344,
      "step": 1432
    },
    {
      "epoch": 0.2014621116265992,
      "grad_norm": 1.565848708152771,
      "learning_rate": 9.06451780970956e-05,
      "loss": 1.0578,
      "step": 1433
    },
    {
      "epoch": 0.20160269928300295,
      "grad_norm": 1.643454909324646,
      "learning_rate": 9.041375353256591e-05,
      "loss": 1.1706,
      "step": 1434
    },
    {
      "epoch": 0.2017432869394067,
      "grad_norm": 1.6742842197418213,
      "learning_rate": 9.018238077406607e-05,
      "loss": 1.2154,
      "step": 1435
    },
    {
      "epoch": 0.20188387459581048,
      "grad_norm": 1.474034070968628,
      "learning_rate": 8.995106107198161e-05,
      "loss": 1.1003,
      "step": 1436
    },
    {
      "epoch": 0.20202446225221427,
      "grad_norm": 1.6321419477462769,
      "learning_rate": 8.971979567641135e-05,
      "loss": 1.1107,
      "step": 1437
    },
    {
      "epoch": 0.20216504990861803,
      "grad_norm": 1.3588474988937378,
      "learning_rate": 8.948858583716065e-05,
      "loss": 1.2893,
      "step": 1438
    },
    {
      "epoch": 0.2023056375650218,
      "grad_norm": 1.5399224758148193,
      "learning_rate": 8.925743280373459e-05,
      "loss": 1.1653,
      "step": 1439
    },
    {
      "epoch": 0.20244622522142555,
      "grad_norm": 1.5717051029205322,
      "learning_rate": 8.902633782533126e-05,
      "loss": 1.192,
      "step": 1440
    },
    {
      "epoch": 0.20258681287782931,
      "grad_norm": 1.6661121845245361,
      "learning_rate": 8.879530215083505e-05,
      "loss": 1.1465,
      "step": 1441
    },
    {
      "epoch": 0.2027274005342331,
      "grad_norm": 1.6912463903427124,
      "learning_rate": 8.856432702880984e-05,
      "loss": 1.1298,
      "step": 1442
    },
    {
      "epoch": 0.20286798819063687,
      "grad_norm": 1.5537707805633545,
      "learning_rate": 8.833341370749222e-05,
      "loss": 1.0928,
      "step": 1443
    },
    {
      "epoch": 0.20300857584704063,
      "grad_norm": 1.369154453277588,
      "learning_rate": 8.810256343478492e-05,
      "loss": 1.1032,
      "step": 1444
    },
    {
      "epoch": 0.2031491635034444,
      "grad_norm": 1.3402235507965088,
      "learning_rate": 8.787177745824982e-05,
      "loss": 1.1507,
      "step": 1445
    },
    {
      "epoch": 0.20328975115984815,
      "grad_norm": 1.307456135749817,
      "learning_rate": 8.76410570251014e-05,
      "loss": 1.1453,
      "step": 1446
    },
    {
      "epoch": 0.20343033881625194,
      "grad_norm": 1.370740532875061,
      "learning_rate": 8.741040338219988e-05,
      "loss": 1.1765,
      "step": 1447
    },
    {
      "epoch": 0.2035709264726557,
      "grad_norm": 1.6243501901626587,
      "learning_rate": 8.717981777604458e-05,
      "loss": 1.1154,
      "step": 1448
    },
    {
      "epoch": 0.20371151412905947,
      "grad_norm": 1.480530023574829,
      "learning_rate": 8.694930145276712e-05,
      "loss": 1.1484,
      "step": 1449
    },
    {
      "epoch": 0.20385210178546323,
      "grad_norm": 1.628445029258728,
      "learning_rate": 8.671885565812467e-05,
      "loss": 1.1773,
      "step": 1450
    },
    {
      "epoch": 0.203992689441867,
      "grad_norm": 1.4507479667663574,
      "learning_rate": 8.64884816374933e-05,
      "loss": 1.0243,
      "step": 1451
    },
    {
      "epoch": 0.20413327709827078,
      "grad_norm": 1.5000957250595093,
      "learning_rate": 8.625818063586117e-05,
      "loss": 1.1115,
      "step": 1452
    },
    {
      "epoch": 0.20427386475467454,
      "grad_norm": 1.6769925355911255,
      "learning_rate": 8.602795389782178e-05,
      "loss": 1.136,
      "step": 1453
    },
    {
      "epoch": 0.2044144524110783,
      "grad_norm": 1.603663444519043,
      "learning_rate": 8.579780266756745e-05,
      "loss": 1.1332,
      "step": 1454
    },
    {
      "epoch": 0.20455504006748207,
      "grad_norm": 1.7009077072143555,
      "learning_rate": 8.556772818888228e-05,
      "loss": 1.1123,
      "step": 1455
    },
    {
      "epoch": 0.20469562772388583,
      "grad_norm": 1.4416491985321045,
      "learning_rate": 8.533773170513565e-05,
      "loss": 1.1668,
      "step": 1456
    },
    {
      "epoch": 0.20483621538028962,
      "grad_norm": 1.7653838396072388,
      "learning_rate": 8.510781445927545e-05,
      "loss": 1.032,
      "step": 1457
    },
    {
      "epoch": 0.20497680303669338,
      "grad_norm": 1.6930487155914307,
      "learning_rate": 8.487797769382134e-05,
      "loss": 1.1271,
      "step": 1458
    },
    {
      "epoch": 0.20511739069309715,
      "grad_norm": 1.3983714580535889,
      "learning_rate": 8.464822265085802e-05,
      "loss": 1.0839,
      "step": 1459
    },
    {
      "epoch": 0.2052579783495009,
      "grad_norm": 1.7862848043441772,
      "learning_rate": 8.441855057202859e-05,
      "loss": 1.0208,
      "step": 1460
    },
    {
      "epoch": 0.20539856600590467,
      "grad_norm": 1.6046888828277588,
      "learning_rate": 8.418896269852779e-05,
      "loss": 1.2313,
      "step": 1461
    },
    {
      "epoch": 0.20553915366230846,
      "grad_norm": 1.8656378984451294,
      "learning_rate": 8.395946027109524e-05,
      "loss": 1.2311,
      "step": 1462
    },
    {
      "epoch": 0.20567974131871222,
      "grad_norm": 1.6971181631088257,
      "learning_rate": 8.373004453000893e-05,
      "loss": 1.2033,
      "step": 1463
    },
    {
      "epoch": 0.20582032897511598,
      "grad_norm": 2.239182472229004,
      "learning_rate": 8.350071671507819e-05,
      "loss": 1.0428,
      "step": 1464
    },
    {
      "epoch": 0.20596091663151975,
      "grad_norm": 1.635581374168396,
      "learning_rate": 8.327147806563733e-05,
      "loss": 0.923,
      "step": 1465
    },
    {
      "epoch": 0.2061015042879235,
      "grad_norm": 1.7282065153121948,
      "learning_rate": 8.30423298205387e-05,
      "loss": 1.3557,
      "step": 1466
    },
    {
      "epoch": 0.2062420919443273,
      "grad_norm": 1.5362571477890015,
      "learning_rate": 8.281327321814615e-05,
      "loss": 1.0538,
      "step": 1467
    },
    {
      "epoch": 0.20638267960073106,
      "grad_norm": 1.5600779056549072,
      "learning_rate": 8.258430949632823e-05,
      "loss": 1.1058,
      "step": 1468
    },
    {
      "epoch": 0.20652326725713482,
      "grad_norm": 1.4205633401870728,
      "learning_rate": 8.235543989245159e-05,
      "loss": 1.1746,
      "step": 1469
    },
    {
      "epoch": 0.20666385491353859,
      "grad_norm": 1.5221099853515625,
      "learning_rate": 8.212666564337415e-05,
      "loss": 1.0727,
      "step": 1470
    },
    {
      "epoch": 0.20680444256994235,
      "grad_norm": 1.4003214836120605,
      "learning_rate": 8.189798798543868e-05,
      "loss": 1.2705,
      "step": 1471
    },
    {
      "epoch": 0.20694503022634614,
      "grad_norm": 1.9143023490905762,
      "learning_rate": 8.166940815446576e-05,
      "loss": 1.0249,
      "step": 1472
    },
    {
      "epoch": 0.2070856178827499,
      "grad_norm": 1.6624321937561035,
      "learning_rate": 8.144092738574745e-05,
      "loss": 1.0697,
      "step": 1473
    },
    {
      "epoch": 0.20722620553915366,
      "grad_norm": 1.740128517150879,
      "learning_rate": 8.12125469140404e-05,
      "loss": 1.1407,
      "step": 1474
    },
    {
      "epoch": 0.20736679319555743,
      "grad_norm": 1.4182095527648926,
      "learning_rate": 8.098426797355919e-05,
      "loss": 1.2012,
      "step": 1475
    },
    {
      "epoch": 0.2075073808519612,
      "grad_norm": 1.513156533241272,
      "learning_rate": 8.075609179796976e-05,
      "loss": 1.0681,
      "step": 1476
    },
    {
      "epoch": 0.20764796850836498,
      "grad_norm": 1.4541505575180054,
      "learning_rate": 8.052801962038268e-05,
      "loss": 1.0625,
      "step": 1477
    },
    {
      "epoch": 0.20778855616476874,
      "grad_norm": 1.6394994258880615,
      "learning_rate": 8.030005267334647e-05,
      "loss": 1.0094,
      "step": 1478
    },
    {
      "epoch": 0.2079291438211725,
      "grad_norm": 1.4724233150482178,
      "learning_rate": 8.007219218884096e-05,
      "loss": 1.1438,
      "step": 1479
    },
    {
      "epoch": 0.20806973147757626,
      "grad_norm": 1.6223320960998535,
      "learning_rate": 7.984443939827072e-05,
      "loss": 1.14,
      "step": 1480
    },
    {
      "epoch": 0.20821031913398003,
      "grad_norm": 1.6267187595367432,
      "learning_rate": 7.961679553245816e-05,
      "loss": 1.0042,
      "step": 1481
    },
    {
      "epoch": 0.20835090679038382,
      "grad_norm": 1.7666763067245483,
      "learning_rate": 7.938926182163719e-05,
      "loss": 0.9712,
      "step": 1482
    },
    {
      "epoch": 0.20849149444678758,
      "grad_norm": 1.5993558168411255,
      "learning_rate": 7.916183949544635e-05,
      "loss": 1.1248,
      "step": 1483
    },
    {
      "epoch": 0.20863208210319134,
      "grad_norm": 1.487294316291809,
      "learning_rate": 7.893452978292226e-05,
      "loss": 1.1371,
      "step": 1484
    },
    {
      "epoch": 0.2087726697595951,
      "grad_norm": 1.38978910446167,
      "learning_rate": 7.870733391249292e-05,
      "loss": 1.1734,
      "step": 1485
    },
    {
      "epoch": 0.20891325741599887,
      "grad_norm": 1.5067280530929565,
      "learning_rate": 7.84802531119711e-05,
      "loss": 1.1935,
      "step": 1486
    },
    {
      "epoch": 0.20905384507240266,
      "grad_norm": 1.6772255897521973,
      "learning_rate": 7.825328860854778e-05,
      "loss": 1.0135,
      "step": 1487
    },
    {
      "epoch": 0.20919443272880642,
      "grad_norm": 1.6285632848739624,
      "learning_rate": 7.802644162878537e-05,
      "loss": 1.059,
      "step": 1488
    },
    {
      "epoch": 0.20933502038521018,
      "grad_norm": 1.5710265636444092,
      "learning_rate": 7.779971339861119e-05,
      "loss": 0.9314,
      "step": 1489
    },
    {
      "epoch": 0.20947560804161394,
      "grad_norm": 1.6923407316207886,
      "learning_rate": 7.75731051433108e-05,
      "loss": 1.0963,
      "step": 1490
    },
    {
      "epoch": 0.2096161956980177,
      "grad_norm": 1.6393663883209229,
      "learning_rate": 7.73466180875214e-05,
      "loss": 1.2781,
      "step": 1491
    },
    {
      "epoch": 0.2097567833544215,
      "grad_norm": 1.5296030044555664,
      "learning_rate": 7.712025345522521e-05,
      "loss": 1.1399,
      "step": 1492
    },
    {
      "epoch": 0.20989737101082526,
      "grad_norm": 1.5098450183868408,
      "learning_rate": 7.689401246974286e-05,
      "loss": 1.1963,
      "step": 1493
    },
    {
      "epoch": 0.21003795866722902,
      "grad_norm": 1.5548814535140991,
      "learning_rate": 7.666789635372673e-05,
      "loss": 1.1042,
      "step": 1494
    },
    {
      "epoch": 0.21017854632363278,
      "grad_norm": 1.5408406257629395,
      "learning_rate": 7.64419063291544e-05,
      "loss": 1.0141,
      "step": 1495
    },
    {
      "epoch": 0.21031913398003654,
      "grad_norm": 1.4169336557388306,
      "learning_rate": 7.621604361732204e-05,
      "loss": 0.9347,
      "step": 1496
    },
    {
      "epoch": 0.21045972163644033,
      "grad_norm": 1.4732944965362549,
      "learning_rate": 7.599030943883776e-05,
      "loss": 1.1267,
      "step": 1497
    },
    {
      "epoch": 0.2106003092928441,
      "grad_norm": 1.4074366092681885,
      "learning_rate": 7.576470501361509e-05,
      "loss": 1.123,
      "step": 1498
    },
    {
      "epoch": 0.21074089694924786,
      "grad_norm": 1.6032438278198242,
      "learning_rate": 7.55392315608663e-05,
      "loss": 1.0817,
      "step": 1499
    },
    {
      "epoch": 0.21088148460565162,
      "grad_norm": 1.828813910484314,
      "learning_rate": 7.531389029909594e-05,
      "loss": 1.1348,
      "step": 1500
    },
    {
      "epoch": 0.21088148460565162,
      "eval_loss": 1.160723328590393,
      "eval_runtime": 771.7664,
      "eval_samples_per_second": 16.386,
      "eval_steps_per_second": 8.193,
      "step": 1500
    },
    {
      "epoch": 0.21102207226205538,
      "grad_norm": 1.3949626684188843,
      "learning_rate": 7.508868244609404e-05,
      "loss": 1.1978,
      "step": 1501
    },
    {
      "epoch": 0.21116265991845917,
      "grad_norm": 1.5324956178665161,
      "learning_rate": 7.486360921892981e-05,
      "loss": 1.3277,
      "step": 1502
    },
    {
      "epoch": 0.21130324757486293,
      "grad_norm": 1.564859390258789,
      "learning_rate": 7.463867183394481e-05,
      "loss": 1.2096,
      "step": 1503
    },
    {
      "epoch": 0.2114438352312667,
      "grad_norm": 1.4992648363113403,
      "learning_rate": 7.441387150674655e-05,
      "loss": 1.0759,
      "step": 1504
    },
    {
      "epoch": 0.21158442288767046,
      "grad_norm": 1.35817289352417,
      "learning_rate": 7.418920945220178e-05,
      "loss": 1.0182,
      "step": 1505
    },
    {
      "epoch": 0.21172501054407422,
      "grad_norm": 1.4540961980819702,
      "learning_rate": 7.396468688443006e-05,
      "loss": 1.1367,
      "step": 1506
    },
    {
      "epoch": 0.211865598200478,
      "grad_norm": 1.890587329864502,
      "learning_rate": 7.37403050167971e-05,
      "loss": 1.2135,
      "step": 1507
    },
    {
      "epoch": 0.21200618585688177,
      "grad_norm": 1.4686682224273682,
      "learning_rate": 7.351606506190823e-05,
      "loss": 1.0165,
      "step": 1508
    },
    {
      "epoch": 0.21214677351328554,
      "grad_norm": 1.4976205825805664,
      "learning_rate": 7.32919682316019e-05,
      "loss": 1.0227,
      "step": 1509
    },
    {
      "epoch": 0.2122873611696893,
      "grad_norm": 1.6460814476013184,
      "learning_rate": 7.306801573694304e-05,
      "loss": 1.1969,
      "step": 1510
    },
    {
      "epoch": 0.21242794882609306,
      "grad_norm": 1.4040446281433105,
      "learning_rate": 7.284420878821657e-05,
      "loss": 1.143,
      "step": 1511
    },
    {
      "epoch": 0.21256853648249685,
      "grad_norm": 1.6532279253005981,
      "learning_rate": 7.262054859492091e-05,
      "loss": 1.058,
      "step": 1512
    },
    {
      "epoch": 0.2127091241389006,
      "grad_norm": 1.4413868188858032,
      "learning_rate": 7.239703636576129e-05,
      "loss": 1.0859,
      "step": 1513
    },
    {
      "epoch": 0.21284971179530437,
      "grad_norm": 1.8252434730529785,
      "learning_rate": 7.217367330864337e-05,
      "loss": 1.0662,
      "step": 1514
    },
    {
      "epoch": 0.21299029945170814,
      "grad_norm": 1.4108558893203735,
      "learning_rate": 7.195046063066661e-05,
      "loss": 1.1807,
      "step": 1515
    },
    {
      "epoch": 0.2131308871081119,
      "grad_norm": 1.4603064060211182,
      "learning_rate": 7.172739953811787e-05,
      "loss": 1.1337,
      "step": 1516
    },
    {
      "epoch": 0.21327147476451566,
      "grad_norm": 1.3226951360702515,
      "learning_rate": 7.15044912364647e-05,
      "loss": 1.1019,
      "step": 1517
    },
    {
      "epoch": 0.21341206242091945,
      "grad_norm": 1.5486570596694946,
      "learning_rate": 7.128173693034904e-05,
      "loss": 1.0859,
      "step": 1518
    },
    {
      "epoch": 0.21355265007732321,
      "grad_norm": 1.597848892211914,
      "learning_rate": 7.105913782358055e-05,
      "loss": 1.2847,
      "step": 1519
    },
    {
      "epoch": 0.21369323773372698,
      "grad_norm": 1.462699294090271,
      "learning_rate": 7.083669511913015e-05,
      "loss": 1.0937,
      "step": 1520
    },
    {
      "epoch": 0.21383382539013074,
      "grad_norm": 1.7216817140579224,
      "learning_rate": 7.061441001912359e-05,
      "loss": 1.0491,
      "step": 1521
    },
    {
      "epoch": 0.2139744130465345,
      "grad_norm": 1.6553010940551758,
      "learning_rate": 7.039228372483485e-05,
      "loss": 1.0338,
      "step": 1522
    },
    {
      "epoch": 0.2141150007029383,
      "grad_norm": 1.47225821018219,
      "learning_rate": 7.017031743667969e-05,
      "loss": 1.1387,
      "step": 1523
    },
    {
      "epoch": 0.21425558835934205,
      "grad_norm": 1.6392923593521118,
      "learning_rate": 6.994851235420918e-05,
      "loss": 1.1056,
      "step": 1524
    },
    {
      "epoch": 0.21439617601574582,
      "grad_norm": 1.4510540962219238,
      "learning_rate": 6.97268696761032e-05,
      "loss": 1.0782,
      "step": 1525
    },
    {
      "epoch": 0.21453676367214958,
      "grad_norm": 1.5914250612258911,
      "learning_rate": 6.950539060016395e-05,
      "loss": 1.1665,
      "step": 1526
    },
    {
      "epoch": 0.21467735132855334,
      "grad_norm": 1.6850095987319946,
      "learning_rate": 6.928407632330951e-05,
      "loss": 1.0402,
      "step": 1527
    },
    {
      "epoch": 0.21481793898495713,
      "grad_norm": 1.605427861213684,
      "learning_rate": 6.906292804156734e-05,
      "loss": 1.0249,
      "step": 1528
    },
    {
      "epoch": 0.2149585266413609,
      "grad_norm": 1.5581754446029663,
      "learning_rate": 6.88419469500678e-05,
      "loss": 1.1331,
      "step": 1529
    },
    {
      "epoch": 0.21509911429776465,
      "grad_norm": 1.5524507761001587,
      "learning_rate": 6.862113424303778e-05,
      "loss": 1.1539,
      "step": 1530
    },
    {
      "epoch": 0.21523970195416842,
      "grad_norm": 1.4787278175354004,
      "learning_rate": 6.840049111379416e-05,
      "loss": 1.1762,
      "step": 1531
    },
    {
      "epoch": 0.21538028961057218,
      "grad_norm": 1.5650027990341187,
      "learning_rate": 6.818001875473737e-05,
      "loss": 1.1503,
      "step": 1532
    },
    {
      "epoch": 0.21552087726697597,
      "grad_norm": 1.7275055646896362,
      "learning_rate": 6.7959718357345e-05,
      "loss": 1.1753,
      "step": 1533
    },
    {
      "epoch": 0.21566146492337973,
      "grad_norm": 1.7634620666503906,
      "learning_rate": 6.773959111216526e-05,
      "loss": 1.1478,
      "step": 1534
    },
    {
      "epoch": 0.2158020525797835,
      "grad_norm": 1.4024220705032349,
      "learning_rate": 6.751963820881068e-05,
      "loss": 1.1634,
      "step": 1535
    },
    {
      "epoch": 0.21594264023618726,
      "grad_norm": 1.4842162132263184,
      "learning_rate": 6.729986083595159e-05,
      "loss": 1.0393,
      "step": 1536
    },
    {
      "epoch": 0.21608322789259102,
      "grad_norm": 1.3746347427368164,
      "learning_rate": 6.708026018130969e-05,
      "loss": 1.2163,
      "step": 1537
    },
    {
      "epoch": 0.2162238155489948,
      "grad_norm": 1.506125807762146,
      "learning_rate": 6.686083743165166e-05,
      "loss": 1.0322,
      "step": 1538
    },
    {
      "epoch": 0.21636440320539857,
      "grad_norm": 1.4059820175170898,
      "learning_rate": 6.66415937727828e-05,
      "loss": 1.0576,
      "step": 1539
    },
    {
      "epoch": 0.21650499086180233,
      "grad_norm": 1.605995535850525,
      "learning_rate": 6.64225303895405e-05,
      "loss": 1.0361,
      "step": 1540
    },
    {
      "epoch": 0.2166455785182061,
      "grad_norm": 1.6578928232192993,
      "learning_rate": 6.620364846578796e-05,
      "loss": 0.9763,
      "step": 1541
    },
    {
      "epoch": 0.21678616617460986,
      "grad_norm": 1.353137493133545,
      "learning_rate": 6.598494918440769e-05,
      "loss": 1.0852,
      "step": 1542
    },
    {
      "epoch": 0.21692675383101365,
      "grad_norm": 1.4518400430679321,
      "learning_rate": 6.576643372729522e-05,
      "loss": 1.0375,
      "step": 1543
    },
    {
      "epoch": 0.2170673414874174,
      "grad_norm": 1.3996210098266602,
      "learning_rate": 6.554810327535257e-05,
      "loss": 1.1523,
      "step": 1544
    },
    {
      "epoch": 0.21720792914382117,
      "grad_norm": 1.4472051858901978,
      "learning_rate": 6.532995900848204e-05,
      "loss": 1.0209,
      "step": 1545
    },
    {
      "epoch": 0.21734851680022493,
      "grad_norm": 1.3614650964736938,
      "learning_rate": 6.511200210557969e-05,
      "loss": 1.1438,
      "step": 1546
    },
    {
      "epoch": 0.2174891044566287,
      "grad_norm": 1.7506767511367798,
      "learning_rate": 6.489423374452907e-05,
      "loss": 1.1916,
      "step": 1547
    },
    {
      "epoch": 0.21762969211303249,
      "grad_norm": 1.5632866621017456,
      "learning_rate": 6.467665510219474e-05,
      "loss": 1.1437,
      "step": 1548
    },
    {
      "epoch": 0.21777027976943625,
      "grad_norm": 1.4109556674957275,
      "learning_rate": 6.44592673544161e-05,
      "loss": 1.0679,
      "step": 1549
    },
    {
      "epoch": 0.21791086742584,
      "grad_norm": 1.3414338827133179,
      "learning_rate": 6.424207167600079e-05,
      "loss": 1.2132,
      "step": 1550
    },
    {
      "epoch": 0.21805145508224377,
      "grad_norm": 1.6753263473510742,
      "learning_rate": 6.402506924071856e-05,
      "loss": 1.2247,
      "step": 1551
    },
    {
      "epoch": 0.21819204273864753,
      "grad_norm": 1.5974037647247314,
      "learning_rate": 6.380826122129481e-05,
      "loss": 1.1316,
      "step": 1552
    },
    {
      "epoch": 0.21833263039505132,
      "grad_norm": 1.4769797325134277,
      "learning_rate": 6.359164878940425e-05,
      "loss": 1.2662,
      "step": 1553
    },
    {
      "epoch": 0.2184732180514551,
      "grad_norm": 1.483426570892334,
      "learning_rate": 6.33752331156646e-05,
      "loss": 1.0962,
      "step": 1554
    },
    {
      "epoch": 0.21861380570785885,
      "grad_norm": 1.4033695459365845,
      "learning_rate": 6.31590153696303e-05,
      "loss": 1.2437,
      "step": 1555
    },
    {
      "epoch": 0.2187543933642626,
      "grad_norm": 1.568790078163147,
      "learning_rate": 6.29429967197861e-05,
      "loss": 1.1156,
      "step": 1556
    },
    {
      "epoch": 0.21889498102066637,
      "grad_norm": 1.326011061668396,
      "learning_rate": 6.272717833354083e-05,
      "loss": 1.2154,
      "step": 1557
    },
    {
      "epoch": 0.21903556867707016,
      "grad_norm": 1.6700502634048462,
      "learning_rate": 6.251156137722103e-05,
      "loss": 1.1141,
      "step": 1558
    },
    {
      "epoch": 0.21917615633347393,
      "grad_norm": 1.4517412185668945,
      "learning_rate": 6.229614701606469e-05,
      "loss": 1.1952,
      "step": 1559
    },
    {
      "epoch": 0.2193167439898777,
      "grad_norm": 1.4636574983596802,
      "learning_rate": 6.208093641421489e-05,
      "loss": 1.0615,
      "step": 1560
    },
    {
      "epoch": 0.21945733164628145,
      "grad_norm": 1.4950652122497559,
      "learning_rate": 6.186593073471362e-05,
      "loss": 1.0599,
      "step": 1561
    },
    {
      "epoch": 0.2195979193026852,
      "grad_norm": 1.3167519569396973,
      "learning_rate": 6.16511311394954e-05,
      "loss": 1.1273,
      "step": 1562
    },
    {
      "epoch": 0.219738506959089,
      "grad_norm": 1.6112315654754639,
      "learning_rate": 6.1436538789381e-05,
      "loss": 1.0132,
      "step": 1563
    },
    {
      "epoch": 0.21987909461549277,
      "grad_norm": 1.4798012971878052,
      "learning_rate": 6.122215484407127e-05,
      "loss": 1.1377,
      "step": 1564
    },
    {
      "epoch": 0.22001968227189653,
      "grad_norm": 1.5212615728378296,
      "learning_rate": 6.100798046214067e-05,
      "loss": 1.1284,
      "step": 1565
    },
    {
      "epoch": 0.2201602699283003,
      "grad_norm": 1.316117525100708,
      "learning_rate": 6.07940168010313e-05,
      "loss": 1.0603,
      "step": 1566
    },
    {
      "epoch": 0.22030085758470405,
      "grad_norm": 1.8961280584335327,
      "learning_rate": 6.05802650170463e-05,
      "loss": 1.152,
      "step": 1567
    },
    {
      "epoch": 0.22044144524110784,
      "grad_norm": 1.4871350526809692,
      "learning_rate": 6.036672626534393e-05,
      "loss": 1.1659,
      "step": 1568
    },
    {
      "epoch": 0.2205820328975116,
      "grad_norm": 1.439705491065979,
      "learning_rate": 6.0153401699931156e-05,
      "loss": 1.0741,
      "step": 1569
    },
    {
      "epoch": 0.22072262055391537,
      "grad_norm": 1.422853708267212,
      "learning_rate": 5.994029247365736e-05,
      "loss": 1.1448,
      "step": 1570
    },
    {
      "epoch": 0.22086320821031913,
      "grad_norm": 1.5027722120285034,
      "learning_rate": 5.97273997382083e-05,
      "loss": 1.0057,
      "step": 1571
    },
    {
      "epoch": 0.2210037958667229,
      "grad_norm": 1.7878069877624512,
      "learning_rate": 5.951472464409964e-05,
      "loss": 1.1572,
      "step": 1572
    },
    {
      "epoch": 0.22114438352312668,
      "grad_norm": 1.7572718858718872,
      "learning_rate": 5.930226834067101e-05,
      "loss": 1.1072,
      "step": 1573
    },
    {
      "epoch": 0.22128497117953044,
      "grad_norm": 1.642195224761963,
      "learning_rate": 5.90900319760795e-05,
      "loss": 1.1669,
      "step": 1574
    },
    {
      "epoch": 0.2214255588359342,
      "grad_norm": 1.551194667816162,
      "learning_rate": 5.8878016697293756e-05,
      "loss": 1.0534,
      "step": 1575
    },
    {
      "epoch": 0.22156614649233797,
      "grad_norm": 1.3627210855484009,
      "learning_rate": 5.8666223650087494e-05,
      "loss": 1.2546,
      "step": 1576
    },
    {
      "epoch": 0.22170673414874173,
      "grad_norm": 1.4836134910583496,
      "learning_rate": 5.845465397903357e-05,
      "loss": 1.0173,
      "step": 1577
    },
    {
      "epoch": 0.22184732180514552,
      "grad_norm": 1.7789275646209717,
      "learning_rate": 5.8243308827497556e-05,
      "loss": 1.0423,
      "step": 1578
    },
    {
      "epoch": 0.22198790946154928,
      "grad_norm": 1.5005711317062378,
      "learning_rate": 5.8032189337631784e-05,
      "loss": 1.1835,
      "step": 1579
    },
    {
      "epoch": 0.22212849711795304,
      "grad_norm": 1.6039910316467285,
      "learning_rate": 5.782129665036893e-05,
      "loss": 1.2141,
      "step": 1580
    },
    {
      "epoch": 0.2222690847743568,
      "grad_norm": 1.565221905708313,
      "learning_rate": 5.761063190541617e-05,
      "loss": 1.086,
      "step": 1581
    },
    {
      "epoch": 0.22240967243076057,
      "grad_norm": 1.5367350578308105,
      "learning_rate": 5.740019624124862e-05,
      "loss": 1.0657,
      "step": 1582
    },
    {
      "epoch": 0.22255026008716436,
      "grad_norm": 1.509369969367981,
      "learning_rate": 5.718999079510358e-05,
      "loss": 0.9666,
      "step": 1583
    },
    {
      "epoch": 0.22269084774356812,
      "grad_norm": 1.755771279335022,
      "learning_rate": 5.698001670297401e-05,
      "loss": 1.1918,
      "step": 1584
    },
    {
      "epoch": 0.22283143539997188,
      "grad_norm": 1.4309196472167969,
      "learning_rate": 5.6770275099602874e-05,
      "loss": 1.1538,
      "step": 1585
    },
    {
      "epoch": 0.22297202305637565,
      "grad_norm": 1.5069326162338257,
      "learning_rate": 5.656076711847642e-05,
      "loss": 1.0827,
      "step": 1586
    },
    {
      "epoch": 0.2231126107127794,
      "grad_norm": 1.6586819887161255,
      "learning_rate": 5.635149389181855e-05,
      "loss": 1.0173,
      "step": 1587
    },
    {
      "epoch": 0.2232531983691832,
      "grad_norm": 1.5179388523101807,
      "learning_rate": 5.614245655058438e-05,
      "loss": 0.8962,
      "step": 1588
    },
    {
      "epoch": 0.22339378602558696,
      "grad_norm": 1.4269832372665405,
      "learning_rate": 5.593365622445437e-05,
      "loss": 1.0767,
      "step": 1589
    },
    {
      "epoch": 0.22353437368199072,
      "grad_norm": 1.4739983081817627,
      "learning_rate": 5.572509404182796e-05,
      "loss": 1.0932,
      "step": 1590
    },
    {
      "epoch": 0.22367496133839448,
      "grad_norm": 1.6911303997039795,
      "learning_rate": 5.551677112981779e-05,
      "loss": 1.038,
      "step": 1591
    },
    {
      "epoch": 0.22381554899479825,
      "grad_norm": 1.571319818496704,
      "learning_rate": 5.530868861424326e-05,
      "loss": 1.0677,
      "step": 1592
    },
    {
      "epoch": 0.22395613665120204,
      "grad_norm": 1.640388011932373,
      "learning_rate": 5.510084761962475e-05,
      "loss": 1.0034,
      "step": 1593
    },
    {
      "epoch": 0.2240967243076058,
      "grad_norm": 1.507590651512146,
      "learning_rate": 5.489324926917729e-05,
      "loss": 1.0588,
      "step": 1594
    },
    {
      "epoch": 0.22423731196400956,
      "grad_norm": 1.4195894002914429,
      "learning_rate": 5.468589468480478e-05,
      "loss": 1.0516,
      "step": 1595
    },
    {
      "epoch": 0.22437789962041332,
      "grad_norm": 1.4931979179382324,
      "learning_rate": 5.4478784987093566e-05,
      "loss": 1.0081,
      "step": 1596
    },
    {
      "epoch": 0.22451848727681709,
      "grad_norm": 1.6873202323913574,
      "learning_rate": 5.4271921295306736e-05,
      "loss": 1.0578,
      "step": 1597
    },
    {
      "epoch": 0.22465907493322088,
      "grad_norm": 1.4505760669708252,
      "learning_rate": 5.406530472737778e-05,
      "loss": 1.0172,
      "step": 1598
    },
    {
      "epoch": 0.22479966258962464,
      "grad_norm": 1.6177663803100586,
      "learning_rate": 5.385893639990483e-05,
      "loss": 1.0905,
      "step": 1599
    },
    {
      "epoch": 0.2249402502460284,
      "grad_norm": 1.373227834701538,
      "learning_rate": 5.36528174281443e-05,
      "loss": 1.0999,
      "step": 1600
    },
    {
      "epoch": 0.22508083790243216,
      "grad_norm": 1.530234932899475,
      "learning_rate": 5.34469489260052e-05,
      "loss": 1.136,
      "step": 1601
    },
    {
      "epoch": 0.22522142555883592,
      "grad_norm": 1.4088066816329956,
      "learning_rate": 5.3241332006042796e-05,
      "loss": 1.327,
      "step": 1602
    },
    {
      "epoch": 0.22536201321523971,
      "grad_norm": 1.4699517488479614,
      "learning_rate": 5.30359677794529e-05,
      "loss": 1.1842,
      "step": 1603
    },
    {
      "epoch": 0.22550260087164348,
      "grad_norm": 1.540465235710144,
      "learning_rate": 5.283085735606563e-05,
      "loss": 1.1753,
      "step": 1604
    },
    {
      "epoch": 0.22564318852804724,
      "grad_norm": 1.719286561012268,
      "learning_rate": 5.262600184433957e-05,
      "loss": 1.016,
      "step": 1605
    },
    {
      "epoch": 0.225783776184451,
      "grad_norm": 1.485685110092163,
      "learning_rate": 5.242140235135555e-05,
      "loss": 1.047,
      "step": 1606
    },
    {
      "epoch": 0.22592436384085476,
      "grad_norm": 1.4514281749725342,
      "learning_rate": 5.2217059982811014e-05,
      "loss": 1.1226,
      "step": 1607
    },
    {
      "epoch": 0.22606495149725855,
      "grad_norm": 1.4790730476379395,
      "learning_rate": 5.2012975843013786e-05,
      "loss": 1.223,
      "step": 1608
    },
    {
      "epoch": 0.22620553915366232,
      "grad_norm": 1.6410856246948242,
      "learning_rate": 5.1809151034876105e-05,
      "loss": 1.1419,
      "step": 1609
    },
    {
      "epoch": 0.22634612681006608,
      "grad_norm": 1.734657883644104,
      "learning_rate": 5.160558665990882e-05,
      "loss": 0.8445,
      "step": 1610
    },
    {
      "epoch": 0.22648671446646984,
      "grad_norm": 1.5317045450210571,
      "learning_rate": 5.140228381821526e-05,
      "loss": 1.1435,
      "step": 1611
    },
    {
      "epoch": 0.2266273021228736,
      "grad_norm": 1.4909271001815796,
      "learning_rate": 5.119924360848547e-05,
      "loss": 1.1352,
      "step": 1612
    },
    {
      "epoch": 0.2267678897792774,
      "grad_norm": 1.5143691301345825,
      "learning_rate": 5.0996467127990055e-05,
      "loss": 1.0999,
      "step": 1613
    },
    {
      "epoch": 0.22690847743568116,
      "grad_norm": 1.3751873970031738,
      "learning_rate": 5.079395547257455e-05,
      "loss": 1.1684,
      "step": 1614
    },
    {
      "epoch": 0.22704906509208492,
      "grad_norm": 1.5503463745117188,
      "learning_rate": 5.0591709736653106e-05,
      "loss": 1.1137,
      "step": 1615
    },
    {
      "epoch": 0.22718965274848868,
      "grad_norm": 1.7047022581100464,
      "learning_rate": 5.038973101320301e-05,
      "loss": 1.0292,
      "step": 1616
    },
    {
      "epoch": 0.22733024040489244,
      "grad_norm": 1.6617045402526855,
      "learning_rate": 5.018802039375836e-05,
      "loss": 1.0268,
      "step": 1617
    },
    {
      "epoch": 0.22747082806129623,
      "grad_norm": 1.70285165309906,
      "learning_rate": 4.998657896840454e-05,
      "loss": 1.095,
      "step": 1618
    },
    {
      "epoch": 0.2276114157177,
      "grad_norm": 1.449121117591858,
      "learning_rate": 4.978540782577199e-05,
      "loss": 1.0944,
      "step": 1619
    },
    {
      "epoch": 0.22775200337410376,
      "grad_norm": 1.6305643320083618,
      "learning_rate": 4.958450805303064e-05,
      "loss": 0.9266,
      "step": 1620
    },
    {
      "epoch": 0.22789259103050752,
      "grad_norm": 1.5039994716644287,
      "learning_rate": 4.9383880735883744e-05,
      "loss": 1.181,
      "step": 1621
    },
    {
      "epoch": 0.22803317868691128,
      "grad_norm": 1.6399520635604858,
      "learning_rate": 4.9183526958562296e-05,
      "loss": 1.0995,
      "step": 1622
    },
    {
      "epoch": 0.22817376634331504,
      "grad_norm": 1.3426909446716309,
      "learning_rate": 4.898344780381883e-05,
      "loss": 1.0154,
      "step": 1623
    },
    {
      "epoch": 0.22831435399971883,
      "grad_norm": 1.6068493127822876,
      "learning_rate": 4.878364435292202e-05,
      "loss": 1.0091,
      "step": 1624
    },
    {
      "epoch": 0.2284549416561226,
      "grad_norm": 1.601866364479065,
      "learning_rate": 4.858411768565033e-05,
      "loss": 1.0066,
      "step": 1625
    },
    {
      "epoch": 0.22859552931252636,
      "grad_norm": 2.0379581451416016,
      "learning_rate": 4.8384868880286625e-05,
      "loss": 1.1716,
      "step": 1626
    },
    {
      "epoch": 0.22873611696893012,
      "grad_norm": 1.6558034420013428,
      "learning_rate": 4.818589901361198e-05,
      "loss": 1.0903,
      "step": 1627
    },
    {
      "epoch": 0.22887670462533388,
      "grad_norm": 1.5233924388885498,
      "learning_rate": 4.798720916090018e-05,
      "loss": 0.8826,
      "step": 1628
    },
    {
      "epoch": 0.22901729228173767,
      "grad_norm": 2.1154236793518066,
      "learning_rate": 4.778880039591163e-05,
      "loss": 1.1461,
      "step": 1629
    },
    {
      "epoch": 0.22915787993814143,
      "grad_norm": 1.6643195152282715,
      "learning_rate": 4.7590673790887786e-05,
      "loss": 0.993,
      "step": 1630
    },
    {
      "epoch": 0.2292984675945452,
      "grad_norm": 1.433677315711975,
      "learning_rate": 4.739283041654514e-05,
      "loss": 1.0339,
      "step": 1631
    },
    {
      "epoch": 0.22943905525094896,
      "grad_norm": 1.4378267526626587,
      "learning_rate": 4.7195271342069695e-05,
      "loss": 1.0463,
      "step": 1632
    },
    {
      "epoch": 0.22957964290735272,
      "grad_norm": 1.4305810928344727,
      "learning_rate": 4.699799763511088e-05,
      "loss": 1.0145,
      "step": 1633
    },
    {
      "epoch": 0.2297202305637565,
      "grad_norm": 1.5089523792266846,
      "learning_rate": 4.680101036177609e-05,
      "loss": 1.002,
      "step": 1634
    },
    {
      "epoch": 0.22986081822016027,
      "grad_norm": 1.5013151168823242,
      "learning_rate": 4.660431058662461e-05,
      "loss": 1.2455,
      "step": 1635
    },
    {
      "epoch": 0.23000140587656404,
      "grad_norm": 1.4314576387405396,
      "learning_rate": 4.6407899372662225e-05,
      "loss": 0.9868,
      "step": 1636
    },
    {
      "epoch": 0.2301419935329678,
      "grad_norm": 1.9747835397720337,
      "learning_rate": 4.6211777781335074e-05,
      "loss": 0.9634,
      "step": 1637
    },
    {
      "epoch": 0.23028258118937156,
      "grad_norm": 1.4394334554672241,
      "learning_rate": 4.601594687252428e-05,
      "loss": 1.185,
      "step": 1638
    },
    {
      "epoch": 0.23042316884577535,
      "grad_norm": 1.6547013521194458,
      "learning_rate": 4.582040770453995e-05,
      "loss": 1.0897,
      "step": 1639
    },
    {
      "epoch": 0.2305637565021791,
      "grad_norm": 1.6926923990249634,
      "learning_rate": 4.562516133411563e-05,
      "loss": 0.8919,
      "step": 1640
    },
    {
      "epoch": 0.23070434415858287,
      "grad_norm": 1.547868013381958,
      "learning_rate": 4.543020881640245e-05,
      "loss": 1.0504,
      "step": 1641
    },
    {
      "epoch": 0.23084493181498664,
      "grad_norm": 1.439857840538025,
      "learning_rate": 4.52355512049636e-05,
      "loss": 1.1421,
      "step": 1642
    },
    {
      "epoch": 0.2309855194713904,
      "grad_norm": 1.5634474754333496,
      "learning_rate": 4.504118955176848e-05,
      "loss": 1.0783,
      "step": 1643
    },
    {
      "epoch": 0.2311261071277942,
      "grad_norm": 1.3428460359573364,
      "learning_rate": 4.484712490718708e-05,
      "loss": 1.1371,
      "step": 1644
    },
    {
      "epoch": 0.23126669478419795,
      "grad_norm": 1.8187869787216187,
      "learning_rate": 4.4653358319984314e-05,
      "loss": 1.3255,
      "step": 1645
    },
    {
      "epoch": 0.2314072824406017,
      "grad_norm": 2.1802961826324463,
      "learning_rate": 4.4459890837314264e-05,
      "loss": 1.0381,
      "step": 1646
    },
    {
      "epoch": 0.23154787009700548,
      "grad_norm": 1.4268701076507568,
      "learning_rate": 4.4266723504714716e-05,
      "loss": 1.0784,
      "step": 1647
    },
    {
      "epoch": 0.23168845775340924,
      "grad_norm": 1.51885187625885,
      "learning_rate": 4.407385736610121e-05,
      "loss": 1.2295,
      "step": 1648
    },
    {
      "epoch": 0.23182904540981303,
      "grad_norm": 1.4749325513839722,
      "learning_rate": 4.388129346376178e-05,
      "loss": 1.0939,
      "step": 1649
    },
    {
      "epoch": 0.2319696330662168,
      "grad_norm": 1.3162802457809448,
      "learning_rate": 4.368903283835088e-05,
      "loss": 1.1072,
      "step": 1650
    },
    {
      "epoch": 0.23211022072262055,
      "grad_norm": 1.315631628036499,
      "learning_rate": 4.3497076528884237e-05,
      "loss": 1.0922,
      "step": 1651
    },
    {
      "epoch": 0.23225080837902431,
      "grad_norm": 1.4175939559936523,
      "learning_rate": 4.330542557273278e-05,
      "loss": 1.1588,
      "step": 1652
    },
    {
      "epoch": 0.23239139603542808,
      "grad_norm": 1.511784315109253,
      "learning_rate": 4.311408100561741e-05,
      "loss": 1.0346,
      "step": 1653
    },
    {
      "epoch": 0.23253198369183187,
      "grad_norm": 1.5814801454544067,
      "learning_rate": 4.2923043861603106e-05,
      "loss": 1.07,
      "step": 1654
    },
    {
      "epoch": 0.23267257134823563,
      "grad_norm": 1.4917038679122925,
      "learning_rate": 4.273231517309361e-05,
      "loss": 1.1968,
      "step": 1655
    },
    {
      "epoch": 0.2328131590046394,
      "grad_norm": 1.6237690448760986,
      "learning_rate": 4.254189597082553e-05,
      "loss": 1.1568,
      "step": 1656
    },
    {
      "epoch": 0.23295374666104315,
      "grad_norm": 1.5046626329421997,
      "learning_rate": 4.235178728386315e-05,
      "loss": 1.1921,
      "step": 1657
    },
    {
      "epoch": 0.23309433431744692,
      "grad_norm": 1.9366413354873657,
      "learning_rate": 4.216199013959248e-05,
      "loss": 1.0157,
      "step": 1658
    },
    {
      "epoch": 0.2332349219738507,
      "grad_norm": 1.4819986820220947,
      "learning_rate": 4.197250556371604e-05,
      "loss": 0.9773,
      "step": 1659
    },
    {
      "epoch": 0.23337550963025447,
      "grad_norm": 1.5076452493667603,
      "learning_rate": 4.178333458024702e-05,
      "loss": 0.9949,
      "step": 1660
    },
    {
      "epoch": 0.23351609728665823,
      "grad_norm": 1.6471083164215088,
      "learning_rate": 4.159447821150408e-05,
      "loss": 1.1295,
      "step": 1661
    },
    {
      "epoch": 0.233656684943062,
      "grad_norm": 1.622885823249817,
      "learning_rate": 4.140593747810539e-05,
      "loss": 1.2203,
      "step": 1662
    },
    {
      "epoch": 0.23379727259946576,
      "grad_norm": 1.5501737594604492,
      "learning_rate": 4.121771339896365e-05,
      "loss": 1.05,
      "step": 1663
    },
    {
      "epoch": 0.23393786025586955,
      "grad_norm": 1.490450382232666,
      "learning_rate": 4.1029806991280064e-05,
      "loss": 1.2918,
      "step": 1664
    },
    {
      "epoch": 0.2340784479122733,
      "grad_norm": 1.5874296426773071,
      "learning_rate": 4.0842219270539205e-05,
      "loss": 1.0051,
      "step": 1665
    },
    {
      "epoch": 0.23421903556867707,
      "grad_norm": 1.597718596458435,
      "learning_rate": 4.0654951250503295e-05,
      "loss": 1.0586,
      "step": 1666
    },
    {
      "epoch": 0.23435962322508083,
      "grad_norm": 1.509056806564331,
      "learning_rate": 4.0468003943206946e-05,
      "loss": 1.1778,
      "step": 1667
    },
    {
      "epoch": 0.2345002108814846,
      "grad_norm": 1.3332123756408691,
      "learning_rate": 4.028137835895144e-05,
      "loss": 1.2028,
      "step": 1668
    },
    {
      "epoch": 0.23464079853788838,
      "grad_norm": 1.6622511148452759,
      "learning_rate": 4.009507550629955e-05,
      "loss": 1.1938,
      "step": 1669
    },
    {
      "epoch": 0.23478138619429215,
      "grad_norm": 1.5839459896087646,
      "learning_rate": 3.99090963920698e-05,
      "loss": 0.8822,
      "step": 1670
    },
    {
      "epoch": 0.2349219738506959,
      "grad_norm": 1.4915165901184082,
      "learning_rate": 3.972344202133129e-05,
      "loss": 1.0498,
      "step": 1671
    },
    {
      "epoch": 0.23506256150709967,
      "grad_norm": 1.4802489280700684,
      "learning_rate": 3.953811339739801e-05,
      "loss": 0.8337,
      "step": 1672
    },
    {
      "epoch": 0.23520314916350343,
      "grad_norm": 1.3509665727615356,
      "learning_rate": 3.9353111521823705e-05,
      "loss": 1.1655,
      "step": 1673
    },
    {
      "epoch": 0.23534373681990722,
      "grad_norm": 1.6669831275939941,
      "learning_rate": 3.916843739439614e-05,
      "loss": 0.8992,
      "step": 1674
    },
    {
      "epoch": 0.23548432447631099,
      "grad_norm": 1.2977465391159058,
      "learning_rate": 3.8984092013132e-05,
      "loss": 1.1515,
      "step": 1675
    },
    {
      "epoch": 0.23562491213271475,
      "grad_norm": 1.9068199396133423,
      "learning_rate": 3.8800076374271234e-05,
      "loss": 1.1516,
      "step": 1676
    },
    {
      "epoch": 0.2357654997891185,
      "grad_norm": 1.5420235395431519,
      "learning_rate": 3.861639147227195e-05,
      "loss": 1.0674,
      "step": 1677
    },
    {
      "epoch": 0.23590608744552227,
      "grad_norm": 1.5696589946746826,
      "learning_rate": 3.84330382998047e-05,
      "loss": 0.9411,
      "step": 1678
    },
    {
      "epoch": 0.23604667510192606,
      "grad_norm": 1.5182961225509644,
      "learning_rate": 3.82500178477475e-05,
      "loss": 1.1216,
      "step": 1679
    },
    {
      "epoch": 0.23618726275832982,
      "grad_norm": 1.3932242393493652,
      "learning_rate": 3.8067331105180085e-05,
      "loss": 1.0398,
      "step": 1680
    },
    {
      "epoch": 0.2363278504147336,
      "grad_norm": 1.4212230443954468,
      "learning_rate": 3.788497905937889e-05,
      "loss": 1.1285,
      "step": 1681
    },
    {
      "epoch": 0.23646843807113735,
      "grad_norm": 1.4621944427490234,
      "learning_rate": 3.770296269581153e-05,
      "loss": 1.0404,
      "step": 1682
    },
    {
      "epoch": 0.2366090257275411,
      "grad_norm": 1.5057135820388794,
      "learning_rate": 3.752128299813156e-05,
      "loss": 1.1097,
      "step": 1683
    },
    {
      "epoch": 0.2367496133839449,
      "grad_norm": 1.4208428859710693,
      "learning_rate": 3.73399409481731e-05,
      "loss": 1.0467,
      "step": 1684
    },
    {
      "epoch": 0.23689020104034866,
      "grad_norm": 1.283410906791687,
      "learning_rate": 3.71589375259455e-05,
      "loss": 1.265,
      "step": 1685
    },
    {
      "epoch": 0.23703078869675243,
      "grad_norm": 1.6303077936172485,
      "learning_rate": 3.697827370962821e-05,
      "loss": 0.9224,
      "step": 1686
    },
    {
      "epoch": 0.2371713763531562,
      "grad_norm": 1.4535539150238037,
      "learning_rate": 3.679795047556526e-05,
      "loss": 0.9732,
      "step": 1687
    },
    {
      "epoch": 0.23731196400955995,
      "grad_norm": 1.6086899042129517,
      "learning_rate": 3.661796879826021e-05,
      "loss": 0.9764,
      "step": 1688
    },
    {
      "epoch": 0.23745255166596374,
      "grad_norm": 1.3350915908813477,
      "learning_rate": 3.643832965037067e-05,
      "loss": 1.1078,
      "step": 1689
    },
    {
      "epoch": 0.2375931393223675,
      "grad_norm": 1.5258105993270874,
      "learning_rate": 3.625903400270329e-05,
      "loss": 1.2247,
      "step": 1690
    },
    {
      "epoch": 0.23773372697877126,
      "grad_norm": 1.5381146669387817,
      "learning_rate": 3.6080082824208206e-05,
      "loss": 0.9427,
      "step": 1691
    },
    {
      "epoch": 0.23787431463517503,
      "grad_norm": 1.3370639085769653,
      "learning_rate": 3.5901477081974147e-05,
      "loss": 1.1372,
      "step": 1692
    },
    {
      "epoch": 0.2380149022915788,
      "grad_norm": 1.5081748962402344,
      "learning_rate": 3.572321774122287e-05,
      "loss": 0.8799,
      "step": 1693
    },
    {
      "epoch": 0.23815548994798258,
      "grad_norm": 1.4730405807495117,
      "learning_rate": 3.554530576530425e-05,
      "loss": 1.3006,
      "step": 1694
    },
    {
      "epoch": 0.23829607760438634,
      "grad_norm": 1.3877323865890503,
      "learning_rate": 3.5367742115690814e-05,
      "loss": 1.074,
      "step": 1695
    },
    {
      "epoch": 0.2384366652607901,
      "grad_norm": 1.5649393796920776,
      "learning_rate": 3.519052775197279e-05,
      "loss": 1.1418,
      "step": 1696
    },
    {
      "epoch": 0.23857725291719387,
      "grad_norm": 1.601166844367981,
      "learning_rate": 3.5013663631852635e-05,
      "loss": 1.1208,
      "step": 1697
    },
    {
      "epoch": 0.23871784057359763,
      "grad_norm": 1.695304036140442,
      "learning_rate": 3.4837150711140174e-05,
      "loss": 0.9538,
      "step": 1698
    },
    {
      "epoch": 0.23885842823000142,
      "grad_norm": 1.8457608222961426,
      "learning_rate": 3.4660989943747144e-05,
      "loss": 1.0511,
      "step": 1699
    },
    {
      "epoch": 0.23899901588640518,
      "grad_norm": 1.421188473701477,
      "learning_rate": 3.4485182281682315e-05,
      "loss": 1.0203,
      "step": 1700
    },
    {
      "epoch": 0.23913960354280894,
      "grad_norm": 1.5967388153076172,
      "learning_rate": 3.4309728675046015e-05,
      "loss": 1.0787,
      "step": 1701
    },
    {
      "epoch": 0.2392801911992127,
      "grad_norm": 1.4043141603469849,
      "learning_rate": 3.413463007202543e-05,
      "loss": 1.1153,
      "step": 1702
    },
    {
      "epoch": 0.23942077885561647,
      "grad_norm": 1.6206440925598145,
      "learning_rate": 3.3959887418889e-05,
      "loss": 1.0456,
      "step": 1703
    },
    {
      "epoch": 0.23956136651202026,
      "grad_norm": 1.4887861013412476,
      "learning_rate": 3.378550165998172e-05,
      "loss": 1.1909,
      "step": 1704
    },
    {
      "epoch": 0.23970195416842402,
      "grad_norm": 1.7711763381958008,
      "learning_rate": 3.361147373771969e-05,
      "loss": 1.2139,
      "step": 1705
    },
    {
      "epoch": 0.23984254182482778,
      "grad_norm": 1.3990225791931152,
      "learning_rate": 3.343780459258535e-05,
      "loss": 1.0434,
      "step": 1706
    },
    {
      "epoch": 0.23998312948123154,
      "grad_norm": 1.4249845743179321,
      "learning_rate": 3.32644951631221e-05,
      "loss": 0.9736,
      "step": 1707
    },
    {
      "epoch": 0.2401237171376353,
      "grad_norm": 1.719207525253296,
      "learning_rate": 3.3091546385929474e-05,
      "loss": 1.1863,
      "step": 1708
    },
    {
      "epoch": 0.2402643047940391,
      "grad_norm": 1.4775670766830444,
      "learning_rate": 3.291895919565785e-05,
      "loss": 1.0718,
      "step": 1709
    },
    {
      "epoch": 0.24040489245044286,
      "grad_norm": 1.4064357280731201,
      "learning_rate": 3.2746734525003654e-05,
      "loss": 1.1656,
      "step": 1710
    },
    {
      "epoch": 0.24054548010684662,
      "grad_norm": 1.3772027492523193,
      "learning_rate": 3.2574873304704034e-05,
      "loss": 1.0772,
      "step": 1711
    },
    {
      "epoch": 0.24068606776325038,
      "grad_norm": 1.2557820081710815,
      "learning_rate": 3.240337646353214e-05,
      "loss": 1.13,
      "step": 1712
    },
    {
      "epoch": 0.24082665541965415,
      "grad_norm": 1.4893845319747925,
      "learning_rate": 3.223224492829178e-05,
      "loss": 1.1034,
      "step": 1713
    },
    {
      "epoch": 0.24096724307605794,
      "grad_norm": 1.4625838994979858,
      "learning_rate": 3.2061479623812715e-05,
      "loss": 1.05,
      "step": 1714
    },
    {
      "epoch": 0.2411078307324617,
      "grad_norm": 1.6931140422821045,
      "learning_rate": 3.189108147294539e-05,
      "loss": 1.0828,
      "step": 1715
    },
    {
      "epoch": 0.24124841838886546,
      "grad_norm": 1.558081865310669,
      "learning_rate": 3.1721051396556224e-05,
      "loss": 1.1279,
      "step": 1716
    },
    {
      "epoch": 0.24138900604526922,
      "grad_norm": 1.7570971250534058,
      "learning_rate": 3.1551390313522324e-05,
      "loss": 1.0684,
      "step": 1717
    },
    {
      "epoch": 0.24152959370167298,
      "grad_norm": 1.368325114250183,
      "learning_rate": 3.1382099140726836e-05,
      "loss": 1.2035,
      "step": 1718
    },
    {
      "epoch": 0.24167018135807677,
      "grad_norm": 1.5745594501495361,
      "learning_rate": 3.12131787930537e-05,
      "loss": 1.0438,
      "step": 1719
    },
    {
      "epoch": 0.24181076901448054,
      "grad_norm": 1.3207236528396606,
      "learning_rate": 3.104463018338293e-05,
      "loss": 0.9884,
      "step": 1720
    },
    {
      "epoch": 0.2419513566708843,
      "grad_norm": 1.6032792329788208,
      "learning_rate": 3.0876454222585584e-05,
      "loss": 1.1565,
      "step": 1721
    },
    {
      "epoch": 0.24209194432728806,
      "grad_norm": 1.7653945684432983,
      "learning_rate": 3.070865181951881e-05,
      "loss": 1.0585,
      "step": 1722
    },
    {
      "epoch": 0.24223253198369182,
      "grad_norm": 1.467819094657898,
      "learning_rate": 3.0541223881021053e-05,
      "loss": 1.1306,
      "step": 1723
    },
    {
      "epoch": 0.2423731196400956,
      "grad_norm": 1.4245917797088623,
      "learning_rate": 3.037417131190693e-05,
      "loss": 1.1954,
      "step": 1724
    },
    {
      "epoch": 0.24251370729649938,
      "grad_norm": 1.530781865119934,
      "learning_rate": 3.0207495014962662e-05,
      "loss": 1.2622,
      "step": 1725
    },
    {
      "epoch": 0.24265429495290314,
      "grad_norm": 1.5095685720443726,
      "learning_rate": 3.0041195890940854e-05,
      "loss": 1.1242,
      "step": 1726
    },
    {
      "epoch": 0.2427948826093069,
      "grad_norm": 1.336655855178833,
      "learning_rate": 2.9875274838555934e-05,
      "loss": 1.1351,
      "step": 1727
    },
    {
      "epoch": 0.24293547026571066,
      "grad_norm": 1.5856486558914185,
      "learning_rate": 2.9709732754479014e-05,
      "loss": 1.125,
      "step": 1728
    },
    {
      "epoch": 0.24307605792211442,
      "grad_norm": 1.5276812314987183,
      "learning_rate": 2.954457053333335e-05,
      "loss": 1.0831,
      "step": 1729
    },
    {
      "epoch": 0.24321664557851821,
      "grad_norm": 1.633736491203308,
      "learning_rate": 2.9379789067689157e-05,
      "loss": 1.0253,
      "step": 1730
    },
    {
      "epoch": 0.24335723323492198,
      "grad_norm": 1.6453917026519775,
      "learning_rate": 2.9215389248059133e-05,
      "loss": 1.1121,
      "step": 1731
    },
    {
      "epoch": 0.24349782089132574,
      "grad_norm": 1.317400336265564,
      "learning_rate": 2.9051371962893358e-05,
      "loss": 1.1974,
      "step": 1732
    },
    {
      "epoch": 0.2436384085477295,
      "grad_norm": 1.5056180953979492,
      "learning_rate": 2.8887738098574735e-05,
      "loss": 1.1148,
      "step": 1733
    },
    {
      "epoch": 0.24377899620413326,
      "grad_norm": 1.5619537830352783,
      "learning_rate": 2.8724488539413952e-05,
      "loss": 1.1276,
      "step": 1734
    },
    {
      "epoch": 0.24391958386053705,
      "grad_norm": 1.4676203727722168,
      "learning_rate": 2.856162416764496e-05,
      "loss": 1.1697,
      "step": 1735
    },
    {
      "epoch": 0.24406017151694082,
      "grad_norm": 1.4346168041229248,
      "learning_rate": 2.8399145863419962e-05,
      "loss": 1.1298,
      "step": 1736
    },
    {
      "epoch": 0.24420075917334458,
      "grad_norm": 1.5107377767562866,
      "learning_rate": 2.8237054504804893e-05,
      "loss": 1.1385,
      "step": 1737
    },
    {
      "epoch": 0.24434134682974834,
      "grad_norm": 1.5214862823486328,
      "learning_rate": 2.8075350967774426e-05,
      "loss": 0.963,
      "step": 1738
    },
    {
      "epoch": 0.2444819344861521,
      "grad_norm": 1.5271496772766113,
      "learning_rate": 2.791403612620751e-05,
      "loss": 1.0683,
      "step": 1739
    },
    {
      "epoch": 0.2446225221425559,
      "grad_norm": 1.5044151544570923,
      "learning_rate": 2.7753110851882324e-05,
      "loss": 1.2006,
      "step": 1740
    },
    {
      "epoch": 0.24476310979895965,
      "grad_norm": 2.0753235816955566,
      "learning_rate": 2.7592576014471983e-05,
      "loss": 1.1234,
      "step": 1741
    },
    {
      "epoch": 0.24490369745536342,
      "grad_norm": 2.1195249557495117,
      "learning_rate": 2.743243248153937e-05,
      "loss": 1.0563,
      "step": 1742
    },
    {
      "epoch": 0.24504428511176718,
      "grad_norm": 1.644473671913147,
      "learning_rate": 2.727268111853285e-05,
      "loss": 1.1172,
      "step": 1743
    },
    {
      "epoch": 0.24518487276817094,
      "grad_norm": 1.4608649015426636,
      "learning_rate": 2.711332278878127e-05,
      "loss": 1.0679,
      "step": 1744
    },
    {
      "epoch": 0.24532546042457473,
      "grad_norm": 1.3658154010772705,
      "learning_rate": 2.6954358353489606e-05,
      "loss": 1.1593,
      "step": 1745
    },
    {
      "epoch": 0.2454660480809785,
      "grad_norm": 1.4200356006622314,
      "learning_rate": 2.6795788671734003e-05,
      "loss": 1.057,
      "step": 1746
    },
    {
      "epoch": 0.24560663573738226,
      "grad_norm": 1.4890217781066895,
      "learning_rate": 2.6637614600457393e-05,
      "loss": 1.3531,
      "step": 1747
    },
    {
      "epoch": 0.24574722339378602,
      "grad_norm": 1.4032765626907349,
      "learning_rate": 2.6479836994464634e-05,
      "loss": 0.9356,
      "step": 1748
    },
    {
      "epoch": 0.24588781105018978,
      "grad_norm": 1.3872250318527222,
      "learning_rate": 2.6322456706418153e-05,
      "loss": 1.2236,
      "step": 1749
    },
    {
      "epoch": 0.24602839870659357,
      "grad_norm": 2.0748956203460693,
      "learning_rate": 2.6165474586833016e-05,
      "loss": 1.0308,
      "step": 1750
    },
    {
      "epoch": 0.24616898636299733,
      "grad_norm": 1.4895635843276978,
      "learning_rate": 2.600889148407267e-05,
      "loss": 0.9538,
      "step": 1751
    },
    {
      "epoch": 0.2463095740194011,
      "grad_norm": 1.4005964994430542,
      "learning_rate": 2.585270824434407e-05,
      "loss": 1.1905,
      "step": 1752
    },
    {
      "epoch": 0.24645016167580486,
      "grad_norm": 1.4222372770309448,
      "learning_rate": 2.5696925711693308e-05,
      "loss": 1.188,
      "step": 1753
    },
    {
      "epoch": 0.24659074933220862,
      "grad_norm": 1.3975499868392944,
      "learning_rate": 2.5541544728000898e-05,
      "loss": 1.0438,
      "step": 1754
    },
    {
      "epoch": 0.2467313369886124,
      "grad_norm": 1.5491129159927368,
      "learning_rate": 2.538656613297741e-05,
      "loss": 1.1743,
      "step": 1755
    },
    {
      "epoch": 0.24687192464501617,
      "grad_norm": 1.3603718280792236,
      "learning_rate": 2.5231990764158696e-05,
      "loss": 1.0597,
      "step": 1756
    },
    {
      "epoch": 0.24701251230141993,
      "grad_norm": 1.5148791074752808,
      "learning_rate": 2.5077819456901618e-05,
      "loss": 1.124,
      "step": 1757
    },
    {
      "epoch": 0.2471530999578237,
      "grad_norm": 1.760805606842041,
      "learning_rate": 2.492405304437928e-05,
      "loss": 1.1008,
      "step": 1758
    },
    {
      "epoch": 0.24729368761422746,
      "grad_norm": 1.5086336135864258,
      "learning_rate": 2.4770692357576742e-05,
      "loss": 1.0834,
      "step": 1759
    },
    {
      "epoch": 0.24743427527063125,
      "grad_norm": 1.519292950630188,
      "learning_rate": 2.4617738225286434e-05,
      "loss": 1.1452,
      "step": 1760
    },
    {
      "epoch": 0.247574862927035,
      "grad_norm": 1.468177080154419,
      "learning_rate": 2.4465191474103656e-05,
      "loss": 1.0417,
      "step": 1761
    },
    {
      "epoch": 0.24771545058343877,
      "grad_norm": 1.6986628770828247,
      "learning_rate": 2.4313052928422152e-05,
      "loss": 1.1877,
      "step": 1762
    },
    {
      "epoch": 0.24785603823984254,
      "grad_norm": 1.4548770189285278,
      "learning_rate": 2.4161323410429603e-05,
      "loss": 1.1647,
      "step": 1763
    },
    {
      "epoch": 0.2479966258962463,
      "grad_norm": 1.7443151473999023,
      "learning_rate": 2.401000374010329e-05,
      "loss": 1.0471,
      "step": 1764
    },
    {
      "epoch": 0.2481372135526501,
      "grad_norm": 1.7998054027557373,
      "learning_rate": 2.3859094735205513e-05,
      "loss": 1.027,
      "step": 1765
    },
    {
      "epoch": 0.24827780120905385,
      "grad_norm": 1.4809361696243286,
      "learning_rate": 2.370859721127934e-05,
      "loss": 1.2947,
      "step": 1766
    },
    {
      "epoch": 0.2484183888654576,
      "grad_norm": 1.2960363626480103,
      "learning_rate": 2.3558511981644004e-05,
      "loss": 1.1231,
      "step": 1767
    },
    {
      "epoch": 0.24855897652186137,
      "grad_norm": 1.558902382850647,
      "learning_rate": 2.340883985739074e-05,
      "loss": 1.1365,
      "step": 1768
    },
    {
      "epoch": 0.24869956417826514,
      "grad_norm": 1.4088468551635742,
      "learning_rate": 2.325958164737817e-05,
      "loss": 1.0559,
      "step": 1769
    },
    {
      "epoch": 0.24884015183466893,
      "grad_norm": 1.3945866823196411,
      "learning_rate": 2.311073815822812e-05,
      "loss": 1.0298,
      "step": 1770
    },
    {
      "epoch": 0.2489807394910727,
      "grad_norm": 1.4041543006896973,
      "learning_rate": 2.296231019432109e-05,
      "loss": 1.0752,
      "step": 1771
    },
    {
      "epoch": 0.24912132714747645,
      "grad_norm": 1.4116077423095703,
      "learning_rate": 2.2814298557792125e-05,
      "loss": 0.9228,
      "step": 1772
    },
    {
      "epoch": 0.2492619148038802,
      "grad_norm": 1.4872779846191406,
      "learning_rate": 2.26667040485262e-05,
      "loss": 1.088,
      "step": 1773
    },
    {
      "epoch": 0.24940250246028398,
      "grad_norm": 1.5760232210159302,
      "learning_rate": 2.25195274641542e-05,
      "loss": 1.1378,
      "step": 1774
    },
    {
      "epoch": 0.24954309011668777,
      "grad_norm": 1.5277187824249268,
      "learning_rate": 2.2372769600048317e-05,
      "loss": 1.1439,
      "step": 1775
    },
    {
      "epoch": 0.24968367777309153,
      "grad_norm": 1.4748845100402832,
      "learning_rate": 2.2226431249318015e-05,
      "loss": 1.2607,
      "step": 1776
    },
    {
      "epoch": 0.2498242654294953,
      "grad_norm": 1.2931876182556152,
      "learning_rate": 2.208051320280553e-05,
      "loss": 0.9953,
      "step": 1777
    },
    {
      "epoch": 0.24996485308589905,
      "grad_norm": 1.4739898443222046,
      "learning_rate": 2.1935016249081764e-05,
      "loss": 0.955,
      "step": 1778
    },
    {
      "epoch": 0.25010544074230284,
      "grad_norm": 1.5585743188858032,
      "learning_rate": 2.1789941174441843e-05,
      "loss": 1.1286,
      "step": 1779
    },
    {
      "epoch": 0.2502460283987066,
      "grad_norm": 1.384255051612854,
      "learning_rate": 2.164528876290114e-05,
      "loss": 1.1764,
      "step": 1780
    },
    {
      "epoch": 0.25038661605511037,
      "grad_norm": 1.360705018043518,
      "learning_rate": 2.1501059796190704e-05,
      "loss": 1.1269,
      "step": 1781
    },
    {
      "epoch": 0.2505272037115141,
      "grad_norm": 1.2704311609268188,
      "learning_rate": 2.13572550537533e-05,
      "loss": 1.1102,
      "step": 1782
    },
    {
      "epoch": 0.2506677913679179,
      "grad_norm": 1.397024393081665,
      "learning_rate": 2.121387531273903e-05,
      "loss": 1.2945,
      "step": 1783
    },
    {
      "epoch": 0.2508083790243217,
      "grad_norm": 1.5766592025756836,
      "learning_rate": 2.1070921348001293e-05,
      "loss": 1.0811,
      "step": 1784
    },
    {
      "epoch": 0.2509489666807254,
      "grad_norm": 1.3137331008911133,
      "learning_rate": 2.0928393932092394e-05,
      "loss": 1.0985,
      "step": 1785
    },
    {
      "epoch": 0.2510895543371292,
      "grad_norm": 1.583182692527771,
      "learning_rate": 2.0786293835259584e-05,
      "loss": 1.1992,
      "step": 1786
    },
    {
      "epoch": 0.25123014199353294,
      "grad_norm": 1.4840478897094727,
      "learning_rate": 2.064462182544071e-05,
      "loss": 1.1726,
      "step": 1787
    },
    {
      "epoch": 0.25137072964993673,
      "grad_norm": 1.4505600929260254,
      "learning_rate": 2.050337866826024e-05,
      "loss": 1.1221,
      "step": 1788
    },
    {
      "epoch": 0.2515113173063405,
      "grad_norm": 1.6483738422393799,
      "learning_rate": 2.0362565127024935e-05,
      "loss": 1.1328,
      "step": 1789
    },
    {
      "epoch": 0.25165190496274426,
      "grad_norm": 1.4563419818878174,
      "learning_rate": 2.022218196271992e-05,
      "loss": 1.1336,
      "step": 1790
    },
    {
      "epoch": 0.25179249261914805,
      "grad_norm": 1.3592698574066162,
      "learning_rate": 2.008222993400437e-05,
      "loss": 1.1693,
      "step": 1791
    },
    {
      "epoch": 0.2519330802755518,
      "grad_norm": 1.490612506866455,
      "learning_rate": 1.9942709797207636e-05,
      "loss": 1.0968,
      "step": 1792
    },
    {
      "epoch": 0.25207366793195557,
      "grad_norm": 1.3177897930145264,
      "learning_rate": 1.980362230632492e-05,
      "loss": 0.9714,
      "step": 1793
    },
    {
      "epoch": 0.25221425558835936,
      "grad_norm": 1.616123914718628,
      "learning_rate": 1.9664968213013436e-05,
      "loss": 1.2541,
      "step": 1794
    },
    {
      "epoch": 0.2523548432447631,
      "grad_norm": 1.3685609102249146,
      "learning_rate": 1.952674826658809e-05,
      "loss": 1.1202,
      "step": 1795
    },
    {
      "epoch": 0.2524954309011669,
      "grad_norm": 1.4508850574493408,
      "learning_rate": 1.938896321401772e-05,
      "loss": 1.1193,
      "step": 1796
    },
    {
      "epoch": 0.2526360185575706,
      "grad_norm": 1.4589567184448242,
      "learning_rate": 1.9251613799920764e-05,
      "loss": 1.0898,
      "step": 1797
    },
    {
      "epoch": 0.2527766062139744,
      "grad_norm": 1.6872867345809937,
      "learning_rate": 1.911470076656149e-05,
      "loss": 1.0175,
      "step": 1798
    },
    {
      "epoch": 0.2529171938703782,
      "grad_norm": 1.343597173690796,
      "learning_rate": 1.8978224853845826e-05,
      "loss": 1.1286,
      "step": 1799
    },
    {
      "epoch": 0.25305778152678193,
      "grad_norm": 1.3983393907546997,
      "learning_rate": 1.884218679931744e-05,
      "loss": 1.1296,
      "step": 1800
    },
    {
      "epoch": 0.2531983691831857,
      "grad_norm": 1.3995368480682373,
      "learning_rate": 1.8706587338153712e-05,
      "loss": 1.0079,
      "step": 1801
    },
    {
      "epoch": 0.25333895683958946,
      "grad_norm": 1.55381178855896,
      "learning_rate": 1.857142720316173e-05,
      "loss": 1.1234,
      "step": 1802
    },
    {
      "epoch": 0.25347954449599325,
      "grad_norm": 1.4618490934371948,
      "learning_rate": 1.8436707124774453e-05,
      "loss": 1.0462,
      "step": 1803
    },
    {
      "epoch": 0.25362013215239704,
      "grad_norm": 1.4780384302139282,
      "learning_rate": 1.830242783104661e-05,
      "loss": 0.9683,
      "step": 1804
    },
    {
      "epoch": 0.25376071980880077,
      "grad_norm": 1.5080485343933105,
      "learning_rate": 1.8168590047650913e-05,
      "loss": 1.145,
      "step": 1805
    },
    {
      "epoch": 0.25390130746520456,
      "grad_norm": 1.68174409866333,
      "learning_rate": 1.8035194497873966e-05,
      "loss": 1.1645,
      "step": 1806
    },
    {
      "epoch": 0.2540418951216083,
      "grad_norm": 1.3909717798233032,
      "learning_rate": 1.790224190261258e-05,
      "loss": 1.2145,
      "step": 1807
    },
    {
      "epoch": 0.2541824827780121,
      "grad_norm": 1.36217200756073,
      "learning_rate": 1.7769732980369612e-05,
      "loss": 1.1244,
      "step": 1808
    },
    {
      "epoch": 0.2543230704344159,
      "grad_norm": 1.435378074645996,
      "learning_rate": 1.7637668447250345e-05,
      "loss": 1.1516,
      "step": 1809
    },
    {
      "epoch": 0.2544636580908196,
      "grad_norm": 1.3950716257095337,
      "learning_rate": 1.750604901695837e-05,
      "loss": 1.1152,
      "step": 1810
    },
    {
      "epoch": 0.2546042457472234,
      "grad_norm": 1.3276385068893433,
      "learning_rate": 1.737487540079199e-05,
      "loss": 1.1566,
      "step": 1811
    },
    {
      "epoch": 0.25474483340362714,
      "grad_norm": 1.4438841342926025,
      "learning_rate": 1.7244148307640094e-05,
      "loss": 0.9031,
      "step": 1812
    },
    {
      "epoch": 0.2548854210600309,
      "grad_norm": 1.5685797929763794,
      "learning_rate": 1.71138684439786e-05,
      "loss": 0.9709,
      "step": 1813
    },
    {
      "epoch": 0.2550260087164347,
      "grad_norm": 1.3810696601867676,
      "learning_rate": 1.6984036513866385e-05,
      "loss": 1.1376,
      "step": 1814
    },
    {
      "epoch": 0.25516659637283845,
      "grad_norm": 1.5165419578552246,
      "learning_rate": 1.6854653218941718e-05,
      "loss": 1.25,
      "step": 1815
    },
    {
      "epoch": 0.25530718402924224,
      "grad_norm": 1.638188123703003,
      "learning_rate": 1.672571925841826e-05,
      "loss": 1.0126,
      "step": 1816
    },
    {
      "epoch": 0.255447771685646,
      "grad_norm": 1.4189975261688232,
      "learning_rate": 1.659723532908144e-05,
      "loss": 1.1698,
      "step": 1817
    },
    {
      "epoch": 0.25558835934204976,
      "grad_norm": 1.365018367767334,
      "learning_rate": 1.6469202125284532e-05,
      "loss": 1.2404,
      "step": 1818
    },
    {
      "epoch": 0.25572894699845355,
      "grad_norm": 1.5084635019302368,
      "learning_rate": 1.6341620338945185e-05,
      "loss": 1.2495,
      "step": 1819
    },
    {
      "epoch": 0.2558695346548573,
      "grad_norm": 1.2946443557739258,
      "learning_rate": 1.621449065954128e-05,
      "loss": 1.1152,
      "step": 1820
    },
    {
      "epoch": 0.2560101223112611,
      "grad_norm": 1.3421155214309692,
      "learning_rate": 1.6087813774107575e-05,
      "loss": 1.085,
      "step": 1821
    },
    {
      "epoch": 0.2561507099676648,
      "grad_norm": 1.7274938821792603,
      "learning_rate": 1.5961590367231726e-05,
      "loss": 0.9594,
      "step": 1822
    },
    {
      "epoch": 0.2562912976240686,
      "grad_norm": 1.5032212734222412,
      "learning_rate": 1.58358211210508e-05,
      "loss": 0.9701,
      "step": 1823
    },
    {
      "epoch": 0.2564318852804724,
      "grad_norm": 1.7710243463516235,
      "learning_rate": 1.57105067152474e-05,
      "loss": 1.0183,
      "step": 1824
    },
    {
      "epoch": 0.25657247293687613,
      "grad_norm": 1.4716575145721436,
      "learning_rate": 1.558564782704616e-05,
      "loss": 1.0826,
      "step": 1825
    },
    {
      "epoch": 0.2567130605932799,
      "grad_norm": 1.5251933336257935,
      "learning_rate": 1.54612451312099e-05,
      "loss": 1.0551,
      "step": 1826
    },
    {
      "epoch": 0.25685364824968365,
      "grad_norm": 1.5954726934432983,
      "learning_rate": 1.533729930003621e-05,
      "loss": 1.1242,
      "step": 1827
    },
    {
      "epoch": 0.25699423590608744,
      "grad_norm": 1.2775871753692627,
      "learning_rate": 1.5213811003353573e-05,
      "loss": 1.1193,
      "step": 1828
    },
    {
      "epoch": 0.25713482356249123,
      "grad_norm": 1.6025640964508057,
      "learning_rate": 1.5090780908517966e-05,
      "loss": 1.1527,
      "step": 1829
    },
    {
      "epoch": 0.25727541121889497,
      "grad_norm": 1.5332729816436768,
      "learning_rate": 1.496820968040904e-05,
      "loss": 1.1635,
      "step": 1830
    },
    {
      "epoch": 0.25741599887529876,
      "grad_norm": 1.5487300157546997,
      "learning_rate": 1.4846097981426744e-05,
      "loss": 1.0024,
      "step": 1831
    },
    {
      "epoch": 0.2575565865317025,
      "grad_norm": 1.6980130672454834,
      "learning_rate": 1.4724446471487551e-05,
      "loss": 1.2066,
      "step": 1832
    },
    {
      "epoch": 0.2576971741881063,
      "grad_norm": 1.6114153861999512,
      "learning_rate": 1.4603255808021054e-05,
      "loss": 1.2287,
      "step": 1833
    },
    {
      "epoch": 0.25783776184451007,
      "grad_norm": 1.859785795211792,
      "learning_rate": 1.4482526645966265e-05,
      "loss": 1.1445,
      "step": 1834
    },
    {
      "epoch": 0.2579783495009138,
      "grad_norm": 1.695548415184021,
      "learning_rate": 1.4362259637768215e-05,
      "loss": 1.456,
      "step": 1835
    },
    {
      "epoch": 0.2581189371573176,
      "grad_norm": 1.4714604616165161,
      "learning_rate": 1.4242455433374302e-05,
      "loss": 1.0026,
      "step": 1836
    },
    {
      "epoch": 0.25825952481372133,
      "grad_norm": 2.027747631072998,
      "learning_rate": 1.4123114680230854e-05,
      "loss": 0.9555,
      "step": 1837
    },
    {
      "epoch": 0.2584001124701251,
      "grad_norm": 1.5596187114715576,
      "learning_rate": 1.4004238023279682e-05,
      "loss": 1.134,
      "step": 1838
    },
    {
      "epoch": 0.2585407001265289,
      "grad_norm": 1.5754565000534058,
      "learning_rate": 1.3885826104954424e-05,
      "loss": 1.0461,
      "step": 1839
    },
    {
      "epoch": 0.25868128778293265,
      "grad_norm": 1.3882852792739868,
      "learning_rate": 1.3767879565177266e-05,
      "loss": 1.1517,
      "step": 1840
    },
    {
      "epoch": 0.25882187543933644,
      "grad_norm": 1.6804015636444092,
      "learning_rate": 1.3650399041355289e-05,
      "loss": 1.0366,
      "step": 1841
    },
    {
      "epoch": 0.25896246309574017,
      "grad_norm": 1.4737428426742554,
      "learning_rate": 1.3533385168377243e-05,
      "loss": 1.0292,
      "step": 1842
    },
    {
      "epoch": 0.25910305075214396,
      "grad_norm": 1.4385682344436646,
      "learning_rate": 1.3416838578609903e-05,
      "loss": 1.0684,
      "step": 1843
    },
    {
      "epoch": 0.25924363840854775,
      "grad_norm": 1.521018147468567,
      "learning_rate": 1.3300759901894844e-05,
      "loss": 1.1303,
      "step": 1844
    },
    {
      "epoch": 0.2593842260649515,
      "grad_norm": 1.4479395151138306,
      "learning_rate": 1.3185149765544847e-05,
      "loss": 1.0798,
      "step": 1845
    },
    {
      "epoch": 0.2595248137213553,
      "grad_norm": 1.4463353157043457,
      "learning_rate": 1.3070008794340693e-05,
      "loss": 1.0382,
      "step": 1846
    },
    {
      "epoch": 0.259665401377759,
      "grad_norm": 1.446382999420166,
      "learning_rate": 1.2955337610527619e-05,
      "loss": 1.0901,
      "step": 1847
    },
    {
      "epoch": 0.2598059890341628,
      "grad_norm": 1.5259506702423096,
      "learning_rate": 1.2841136833812118e-05,
      "loss": 1.1609,
      "step": 1848
    },
    {
      "epoch": 0.2599465766905666,
      "grad_norm": 1.5755846500396729,
      "learning_rate": 1.2727407081358433e-05,
      "loss": 1.0517,
      "step": 1849
    },
    {
      "epoch": 0.2600871643469703,
      "grad_norm": 1.6455343961715698,
      "learning_rate": 1.2614148967785344e-05,
      "loss": 0.9904,
      "step": 1850
    },
    {
      "epoch": 0.2602277520033741,
      "grad_norm": 1.4804887771606445,
      "learning_rate": 1.2501363105162766e-05,
      "loss": 0.8976,
      "step": 1851
    },
    {
      "epoch": 0.26036833965977785,
      "grad_norm": 1.3886507749557495,
      "learning_rate": 1.2389050103008515e-05,
      "loss": 1.0536,
      "step": 1852
    },
    {
      "epoch": 0.26050892731618164,
      "grad_norm": 1.5821760892868042,
      "learning_rate": 1.2277210568284913e-05,
      "loss": 1.0274,
      "step": 1853
    },
    {
      "epoch": 0.2606495149725854,
      "grad_norm": 1.6401944160461426,
      "learning_rate": 1.2165845105395645e-05,
      "loss": 1.0151,
      "step": 1854
    },
    {
      "epoch": 0.26079010262898916,
      "grad_norm": 1.2929768562316895,
      "learning_rate": 1.205495431618232e-05,
      "loss": 1.234,
      "step": 1855
    },
    {
      "epoch": 0.26093069028539295,
      "grad_norm": 1.2931005954742432,
      "learning_rate": 1.1944538799921411e-05,
      "loss": 1.1195,
      "step": 1856
    },
    {
      "epoch": 0.2610712779417967,
      "grad_norm": 1.5733193159103394,
      "learning_rate": 1.1834599153320825e-05,
      "loss": 1.0708,
      "step": 1857
    },
    {
      "epoch": 0.2612118655982005,
      "grad_norm": 1.6031277179718018,
      "learning_rate": 1.1725135970516876e-05,
      "loss": 1.0238,
      "step": 1858
    },
    {
      "epoch": 0.26135245325460427,
      "grad_norm": 1.386975884437561,
      "learning_rate": 1.1616149843070879e-05,
      "loss": 1.1222,
      "step": 1859
    },
    {
      "epoch": 0.261493040911008,
      "grad_norm": 1.4303991794586182,
      "learning_rate": 1.1507641359966114e-05,
      "loss": 1.0557,
      "step": 1860
    },
    {
      "epoch": 0.2616336285674118,
      "grad_norm": 1.4830299615859985,
      "learning_rate": 1.139961110760449e-05,
      "loss": 1.0217,
      "step": 1861
    },
    {
      "epoch": 0.2617742162238155,
      "grad_norm": 1.4652960300445557,
      "learning_rate": 1.1292059669803567e-05,
      "loss": 1.1043,
      "step": 1862
    },
    {
      "epoch": 0.2619148038802193,
      "grad_norm": 1.5590900182724,
      "learning_rate": 1.1184987627793175e-05,
      "loss": 1.1403,
      "step": 1863
    },
    {
      "epoch": 0.2620553915366231,
      "grad_norm": 1.7432669401168823,
      "learning_rate": 1.1078395560212518e-05,
      "loss": 0.969,
      "step": 1864
    },
    {
      "epoch": 0.26219597919302684,
      "grad_norm": 1.6110299825668335,
      "learning_rate": 1.0972284043106795e-05,
      "loss": 1.1986,
      "step": 1865
    },
    {
      "epoch": 0.26233656684943063,
      "grad_norm": 1.4141126871109009,
      "learning_rate": 1.086665364992433e-05,
      "loss": 1.0626,
      "step": 1866
    },
    {
      "epoch": 0.26247715450583436,
      "grad_norm": 1.4098074436187744,
      "learning_rate": 1.0761504951513246e-05,
      "loss": 1.1479,
      "step": 1867
    },
    {
      "epoch": 0.26261774216223815,
      "grad_norm": 1.4020349979400635,
      "learning_rate": 1.0656838516118584e-05,
      "loss": 1.2057,
      "step": 1868
    },
    {
      "epoch": 0.26275832981864194,
      "grad_norm": 1.659718632698059,
      "learning_rate": 1.0552654909379055e-05,
      "loss": 1.1311,
      "step": 1869
    },
    {
      "epoch": 0.2628989174750457,
      "grad_norm": 1.5277718305587769,
      "learning_rate": 1.0448954694324142e-05,
      "loss": 1.0182,
      "step": 1870
    },
    {
      "epoch": 0.26303950513144947,
      "grad_norm": 1.3547879457473755,
      "learning_rate": 1.0345738431370921e-05,
      "loss": 1.272,
      "step": 1871
    },
    {
      "epoch": 0.2631800927878532,
      "grad_norm": 1.383103847503662,
      "learning_rate": 1.0243006678321131e-05,
      "loss": 0.9634,
      "step": 1872
    },
    {
      "epoch": 0.263320680444257,
      "grad_norm": 1.5093876123428345,
      "learning_rate": 1.0140759990358073e-05,
      "loss": 0.8988,
      "step": 1873
    },
    {
      "epoch": 0.2634612681006608,
      "grad_norm": 1.3800232410430908,
      "learning_rate": 1.0038998920043741e-05,
      "loss": 1.1945,
      "step": 1874
    },
    {
      "epoch": 0.2636018557570645,
      "grad_norm": 1.4043328762054443,
      "learning_rate": 9.93772401731564e-06,
      "loss": 1.1256,
      "step": 1875
    },
    {
      "epoch": 0.2637424434134683,
      "grad_norm": 1.33708918094635,
      "learning_rate": 9.836935829484017e-06,
      "loss": 1.2925,
      "step": 1876
    },
    {
      "epoch": 0.26388303106987204,
      "grad_norm": 1.3147861957550049,
      "learning_rate": 9.736634901228814e-06,
      "loss": 1.0152,
      "step": 1877
    },
    {
      "epoch": 0.26402361872627583,
      "grad_norm": 1.538306713104248,
      "learning_rate": 9.636821774596638e-06,
      "loss": 1.0696,
      "step": 1878
    },
    {
      "epoch": 0.2641642063826796,
      "grad_norm": 1.5832607746124268,
      "learning_rate": 9.537496988998008e-06,
      "loss": 1.1029,
      "step": 1879
    },
    {
      "epoch": 0.26430479403908336,
      "grad_norm": 1.6709685325622559,
      "learning_rate": 9.438661081204281e-06,
      "loss": 1.0965,
      "step": 1880
    },
    {
      "epoch": 0.26444538169548715,
      "grad_norm": 1.5122888088226318,
      "learning_rate": 9.340314585344889e-06,
      "loss": 0.9282,
      "step": 1881
    },
    {
      "epoch": 0.2645859693518909,
      "grad_norm": 1.4539381265640259,
      "learning_rate": 9.24245803290429e-06,
      "loss": 1.0949,
      "step": 1882
    },
    {
      "epoch": 0.26472655700829467,
      "grad_norm": 1.4905940294265747,
      "learning_rate": 9.145091952719276e-06,
      "loss": 1.1354,
      "step": 1883
    },
    {
      "epoch": 0.26486714466469846,
      "grad_norm": 1.6049697399139404,
      "learning_rate": 9.048216870975968e-06,
      "loss": 1.2563,
      "step": 1884
    },
    {
      "epoch": 0.2650077323211022,
      "grad_norm": 1.3222384452819824,
      "learning_rate": 8.95183331120708e-06,
      "loss": 1.0591,
      "step": 1885
    },
    {
      "epoch": 0.265148319977506,
      "grad_norm": 1.526872158050537,
      "learning_rate": 8.85594179428898e-06,
      "loss": 1.2579,
      "step": 1886
    },
    {
      "epoch": 0.2652889076339097,
      "grad_norm": 1.4312899112701416,
      "learning_rate": 8.760542838439012e-06,
      "loss": 1.0564,
      "step": 1887
    },
    {
      "epoch": 0.2654294952903135,
      "grad_norm": 1.3813385963439941,
      "learning_rate": 8.66563695921252e-06,
      "loss": 1.061,
      "step": 1888
    },
    {
      "epoch": 0.2655700829467173,
      "grad_norm": 1.5017585754394531,
      "learning_rate": 8.571224669500289e-06,
      "loss": 1.18,
      "step": 1889
    },
    {
      "epoch": 0.26571067060312104,
      "grad_norm": 1.4125667810440063,
      "learning_rate": 8.477306479525515e-06,
      "loss": 1.0516,
      "step": 1890
    },
    {
      "epoch": 0.2658512582595248,
      "grad_norm": 1.436672329902649,
      "learning_rate": 8.383882896841288e-06,
      "loss": 1.1281,
      "step": 1891
    },
    {
      "epoch": 0.26599184591592856,
      "grad_norm": 1.3451474905014038,
      "learning_rate": 8.290954426327645e-06,
      "loss": 1.1069,
      "step": 1892
    },
    {
      "epoch": 0.26613243357233235,
      "grad_norm": 1.5095102787017822,
      "learning_rate": 8.198521570189033e-06,
      "loss": 1.1108,
      "step": 1893
    },
    {
      "epoch": 0.26627302122873614,
      "grad_norm": 1.439680814743042,
      "learning_rate": 8.106584827951402e-06,
      "loss": 1.0806,
      "step": 1894
    },
    {
      "epoch": 0.2664136088851399,
      "grad_norm": 1.6336694955825806,
      "learning_rate": 8.015144696459676e-06,
      "loss": 1.2163,
      "step": 1895
    },
    {
      "epoch": 0.26655419654154366,
      "grad_norm": 1.6116710901260376,
      "learning_rate": 7.92420166987492e-06,
      "loss": 1.0395,
      "step": 1896
    },
    {
      "epoch": 0.2666947841979474,
      "grad_norm": 1.3883085250854492,
      "learning_rate": 7.833756239671853e-06,
      "loss": 1.1377,
      "step": 1897
    },
    {
      "epoch": 0.2668353718543512,
      "grad_norm": 1.6591278314590454,
      "learning_rate": 7.743808894635962e-06,
      "loss": 1.1655,
      "step": 1898
    },
    {
      "epoch": 0.266975959510755,
      "grad_norm": 1.5283209085464478,
      "learning_rate": 7.654360120861071e-06,
      "loss": 1.1255,
      "step": 1899
    },
    {
      "epoch": 0.2671165471671587,
      "grad_norm": 1.3381026983261108,
      "learning_rate": 7.565410401746542e-06,
      "loss": 1.1394,
      "step": 1900
    },
    {
      "epoch": 0.2672571348235625,
      "grad_norm": 1.508339762687683,
      "learning_rate": 7.476960217994833e-06,
      "loss": 1.2876,
      "step": 1901
    },
    {
      "epoch": 0.26739772247996624,
      "grad_norm": 1.3482521772384644,
      "learning_rate": 7.389010047608724e-06,
      "loss": 1.0031,
      "step": 1902
    },
    {
      "epoch": 0.26753831013637003,
      "grad_norm": 1.4809972047805786,
      "learning_rate": 7.301560365888904e-06,
      "loss": 1.2275,
      "step": 1903
    },
    {
      "epoch": 0.2676788977927738,
      "grad_norm": 1.3467729091644287,
      "learning_rate": 7.214611645431235e-06,
      "loss": 0.9912,
      "step": 1904
    },
    {
      "epoch": 0.26781948544917755,
      "grad_norm": 1.6964353322982788,
      "learning_rate": 7.128164356124367e-06,
      "loss": 1.004,
      "step": 1905
    },
    {
      "epoch": 0.26796007310558134,
      "grad_norm": 1.4413080215454102,
      "learning_rate": 7.042218965147029e-06,
      "loss": 1.0731,
      "step": 1906
    },
    {
      "epoch": 0.2681006607619851,
      "grad_norm": 1.5642316341400146,
      "learning_rate": 6.956775936965665e-06,
      "loss": 1.0709,
      "step": 1907
    },
    {
      "epoch": 0.26824124841838887,
      "grad_norm": 1.4315415620803833,
      "learning_rate": 6.871835733331789e-06,
      "loss": 0.9987,
      "step": 1908
    },
    {
      "epoch": 0.26838183607479266,
      "grad_norm": 1.2421461343765259,
      "learning_rate": 6.787398813279611e-06,
      "loss": 1.0807,
      "step": 1909
    },
    {
      "epoch": 0.2685224237311964,
      "grad_norm": 1.5406197309494019,
      "learning_rate": 6.703465633123407e-06,
      "loss": 1.0189,
      "step": 1910
    },
    {
      "epoch": 0.2686630113876002,
      "grad_norm": 1.517959713935852,
      "learning_rate": 6.620036646455241e-06,
      "loss": 1.2547,
      "step": 1911
    },
    {
      "epoch": 0.2688035990440039,
      "grad_norm": 1.586104154586792,
      "learning_rate": 6.537112304142323e-06,
      "loss": 0.9596,
      "step": 1912
    },
    {
      "epoch": 0.2689441867004077,
      "grad_norm": 1.5606715679168701,
      "learning_rate": 6.4546930543247344e-06,
      "loss": 1.139,
      "step": 1913
    },
    {
      "epoch": 0.2690847743568115,
      "grad_norm": 1.4823366403579712,
      "learning_rate": 6.37277934241286e-06,
      "loss": 1.0259,
      "step": 1914
    },
    {
      "epoch": 0.26922536201321523,
      "grad_norm": 1.3722431659698486,
      "learning_rate": 6.2913716110851065e-06,
      "loss": 1.1036,
      "step": 1915
    },
    {
      "epoch": 0.269365949669619,
      "grad_norm": 1.6043699979782104,
      "learning_rate": 6.210470300285498e-06,
      "loss": 1.1526,
      "step": 1916
    },
    {
      "epoch": 0.26950653732602275,
      "grad_norm": 1.5225815773010254,
      "learning_rate": 6.130075847221139e-06,
      "loss": 0.9833,
      "step": 1917
    },
    {
      "epoch": 0.26964712498242654,
      "grad_norm": 1.3248240947723389,
      "learning_rate": 6.050188686360092e-06,
      "loss": 0.9865,
      "step": 1918
    },
    {
      "epoch": 0.26978771263883033,
      "grad_norm": 1.4484517574310303,
      "learning_rate": 5.970809249428821e-06,
      "loss": 1.0434,
      "step": 1919
    },
    {
      "epoch": 0.26992830029523407,
      "grad_norm": 1.5160471200942993,
      "learning_rate": 5.89193796541001e-06,
      "loss": 1.1612,
      "step": 1920
    },
    {
      "epoch": 0.27006888795163786,
      "grad_norm": 1.5475759506225586,
      "learning_rate": 5.813575260540128e-06,
      "loss": 1.117,
      "step": 1921
    },
    {
      "epoch": 0.2702094756080416,
      "grad_norm": 1.5457673072814941,
      "learning_rate": 5.735721558307228e-06,
      "loss": 1.1594,
      "step": 1922
    },
    {
      "epoch": 0.2703500632644454,
      "grad_norm": 1.4323554039001465,
      "learning_rate": 5.658377279448568e-06,
      "loss": 1.1538,
      "step": 1923
    },
    {
      "epoch": 0.2704906509208492,
      "grad_norm": 1.6739590167999268,
      "learning_rate": 5.581542841948417e-06,
      "loss": 1.1329,
      "step": 1924
    },
    {
      "epoch": 0.2706312385772529,
      "grad_norm": 1.577501654624939,
      "learning_rate": 5.505218661035705e-06,
      "loss": 1.1615,
      "step": 1925
    },
    {
      "epoch": 0.2707718262336567,
      "grad_norm": 1.6715196371078491,
      "learning_rate": 5.429405149181888e-06,
      "loss": 1.0045,
      "step": 1926
    },
    {
      "epoch": 0.27091241389006043,
      "grad_norm": 1.5919413566589355,
      "learning_rate": 5.3541027160986194e-06,
      "loss": 1.011,
      "step": 1927
    },
    {
      "epoch": 0.2710530015464642,
      "grad_norm": 1.6479839086532593,
      "learning_rate": 5.2793117687356175e-06,
      "loss": 1.0924,
      "step": 1928
    },
    {
      "epoch": 0.271193589202868,
      "grad_norm": 1.5627145767211914,
      "learning_rate": 5.205032711278379e-06,
      "loss": 1.0556,
      "step": 1929
    },
    {
      "epoch": 0.27133417685927175,
      "grad_norm": 2.0898404121398926,
      "learning_rate": 5.131265945146102e-06,
      "loss": 0.9866,
      "step": 1930
    },
    {
      "epoch": 0.27147476451567554,
      "grad_norm": 1.4196271896362305,
      "learning_rate": 5.058011868989388e-06,
      "loss": 1.1149,
      "step": 1931
    },
    {
      "epoch": 0.27161535217207927,
      "grad_norm": 1.3628184795379639,
      "learning_rate": 4.985270878688242e-06,
      "loss": 0.9059,
      "step": 1932
    },
    {
      "epoch": 0.27175593982848306,
      "grad_norm": 1.4847116470336914,
      "learning_rate": 4.9130433673497565e-06,
      "loss": 1.0147,
      "step": 1933
    },
    {
      "epoch": 0.27189652748488685,
      "grad_norm": 1.4532300233840942,
      "learning_rate": 4.841329725306143e-06,
      "loss": 1.2336,
      "step": 1934
    },
    {
      "epoch": 0.2720371151412906,
      "grad_norm": 1.974456548690796,
      "learning_rate": 4.770130340112555e-06,
      "loss": 1.014,
      "step": 1935
    },
    {
      "epoch": 0.2721777027976944,
      "grad_norm": 1.6286461353302002,
      "learning_rate": 4.699445596544971e-06,
      "loss": 1.0719,
      "step": 1936
    },
    {
      "epoch": 0.2723182904540981,
      "grad_norm": 1.5130809545516968,
      "learning_rate": 4.629275876598149e-06,
      "loss": 1.1562,
      "step": 1937
    },
    {
      "epoch": 0.2724588781105019,
      "grad_norm": 1.4163974523544312,
      "learning_rate": 4.559621559483562e-06,
      "loss": 1.1298,
      "step": 1938
    },
    {
      "epoch": 0.2725994657669057,
      "grad_norm": 1.4110751152038574,
      "learning_rate": 4.490483021627334e-06,
      "loss": 1.1904,
      "step": 1939
    },
    {
      "epoch": 0.2727400534233094,
      "grad_norm": 1.3694005012512207,
      "learning_rate": 4.421860636668218e-06,
      "loss": 1.0109,
      "step": 1940
    },
    {
      "epoch": 0.2728806410797132,
      "grad_norm": 1.5687670707702637,
      "learning_rate": 4.353754775455565e-06,
      "loss": 1.2247,
      "step": 1941
    },
    {
      "epoch": 0.27302122873611695,
      "grad_norm": 1.475361943244934,
      "learning_rate": 4.286165806047349e-06,
      "loss": 1.0641,
      "step": 1942
    },
    {
      "epoch": 0.27316181639252074,
      "grad_norm": 1.4667123556137085,
      "learning_rate": 4.219094093708109e-06,
      "loss": 1.1343,
      "step": 1943
    },
    {
      "epoch": 0.27330240404892453,
      "grad_norm": 1.3814541101455688,
      "learning_rate": 4.152540000907068e-06,
      "loss": 0.8916,
      "step": 1944
    },
    {
      "epoch": 0.27344299170532826,
      "grad_norm": 1.487830638885498,
      "learning_rate": 4.086503887316107e-06,
      "loss": 1.1881,
      "step": 1945
    },
    {
      "epoch": 0.27358357936173205,
      "grad_norm": 1.6003402471542358,
      "learning_rate": 4.0209861098078446e-06,
      "loss": 1.05,
      "step": 1946
    },
    {
      "epoch": 0.2737241670181358,
      "grad_norm": 1.540339708328247,
      "learning_rate": 3.955987022453689e-06,
      "loss": 1.1566,
      "step": 1947
    },
    {
      "epoch": 0.2738647546745396,
      "grad_norm": 1.6754003763198853,
      "learning_rate": 3.8915069765219746e-06,
      "loss": 1.091,
      "step": 1948
    },
    {
      "epoch": 0.27400534233094337,
      "grad_norm": 1.572129487991333,
      "learning_rate": 3.827546320476005e-06,
      "loss": 1.2666,
      "step": 1949
    },
    {
      "epoch": 0.2741459299873471,
      "grad_norm": 1.4516735076904297,
      "learning_rate": 3.7641053999722065e-06,
      "loss": 1.134,
      "step": 1950
    },
    {
      "epoch": 0.2742865176437509,
      "grad_norm": 1.7531778812408447,
      "learning_rate": 3.7011845578582392e-06,
      "loss": 1.0692,
      "step": 1951
    },
    {
      "epoch": 0.27442710530015463,
      "grad_norm": 1.7572572231292725,
      "learning_rate": 3.6387841341711693e-06,
      "loss": 1.0884,
      "step": 1952
    },
    {
      "epoch": 0.2745676929565584,
      "grad_norm": 1.5778717994689941,
      "learning_rate": 3.5769044661355887e-06,
      "loss": 1.1148,
      "step": 1953
    },
    {
      "epoch": 0.2747082806129622,
      "grad_norm": 1.7985799312591553,
      "learning_rate": 3.515545888161831e-06,
      "loss": 1.0171,
      "step": 1954
    },
    {
      "epoch": 0.27484886826936594,
      "grad_norm": 1.1958235502243042,
      "learning_rate": 3.4547087318442027e-06,
      "loss": 1.2906,
      "step": 1955
    },
    {
      "epoch": 0.27498945592576973,
      "grad_norm": 1.551285743713379,
      "learning_rate": 3.3943933259590445e-06,
      "loss": 1.0306,
      "step": 1956
    },
    {
      "epoch": 0.27513004358217347,
      "grad_norm": 1.5118900537490845,
      "learning_rate": 3.334599996463139e-06,
      "loss": 1.1275,
      "step": 1957
    },
    {
      "epoch": 0.27527063123857726,
      "grad_norm": 1.6719685792922974,
      "learning_rate": 3.2753290664918055e-06,
      "loss": 0.9402,
      "step": 1958
    },
    {
      "epoch": 0.27541121889498105,
      "grad_norm": 1.5568578243255615,
      "learning_rate": 3.216580856357243e-06,
      "loss": 0.93,
      "step": 1959
    },
    {
      "epoch": 0.2755518065513848,
      "grad_norm": 1.6808805465698242,
      "learning_rate": 3.1583556835467432e-06,
      "loss": 1.1385,
      "step": 1960
    },
    {
      "epoch": 0.27569239420778857,
      "grad_norm": 1.4610626697540283,
      "learning_rate": 3.1006538627210147e-06,
      "loss": 1.0016,
      "step": 1961
    },
    {
      "epoch": 0.2758329818641923,
      "grad_norm": 1.5331602096557617,
      "learning_rate": 3.043475705712462e-06,
      "loss": 1.101,
      "step": 1962
    },
    {
      "epoch": 0.2759735695205961,
      "grad_norm": 1.438628911972046,
      "learning_rate": 2.9868215215234862e-06,
      "loss": 1.1284,
      "step": 1963
    },
    {
      "epoch": 0.2761141571769999,
      "grad_norm": 1.472866415977478,
      "learning_rate": 2.9306916163248543e-06,
      "loss": 1.0798,
      "step": 1964
    },
    {
      "epoch": 0.2762547448334036,
      "grad_norm": 1.4375911951065063,
      "learning_rate": 2.875086293454021e-06,
      "loss": 1.1194,
      "step": 1965
    },
    {
      "epoch": 0.2763953324898074,
      "grad_norm": 1.4760693311691284,
      "learning_rate": 2.8200058534134766e-06,
      "loss": 1.0127,
      "step": 1966
    },
    {
      "epoch": 0.27653592014621114,
      "grad_norm": 1.408478021621704,
      "learning_rate": 2.7654505938691565e-06,
      "loss": 1.1113,
      "step": 1967
    },
    {
      "epoch": 0.27667650780261493,
      "grad_norm": 1.4464017152786255,
      "learning_rate": 2.7114208096487794e-06,
      "loss": 0.9501,
      "step": 1968
    },
    {
      "epoch": 0.2768170954590187,
      "grad_norm": 1.5480992794036865,
      "learning_rate": 2.657916792740334e-06,
      "loss": 1.0867,
      "step": 1969
    },
    {
      "epoch": 0.27695768311542246,
      "grad_norm": 1.4368162155151367,
      "learning_rate": 2.6049388322904265e-06,
      "loss": 1.1848,
      "step": 1970
    },
    {
      "epoch": 0.27709827077182625,
      "grad_norm": 1.3112915754318237,
      "learning_rate": 2.552487214602761e-06,
      "loss": 1.2264,
      "step": 1971
    },
    {
      "epoch": 0.27723885842823,
      "grad_norm": 1.4848741292953491,
      "learning_rate": 2.500562223136549e-06,
      "loss": 0.9093,
      "step": 1972
    },
    {
      "epoch": 0.2773794460846338,
      "grad_norm": 1.5114210844039917,
      "learning_rate": 2.4491641385050446e-06,
      "loss": 1.0165,
      "step": 1973
    },
    {
      "epoch": 0.27752003374103756,
      "grad_norm": 1.5234625339508057,
      "learning_rate": 2.39829323847397e-06,
      "loss": 1.0196,
      "step": 1974
    },
    {
      "epoch": 0.2776606213974413,
      "grad_norm": 1.5341310501098633,
      "learning_rate": 2.347949797960047e-06,
      "loss": 1.1115,
      "step": 1975
    },
    {
      "epoch": 0.2778012090538451,
      "grad_norm": 1.3445364236831665,
      "learning_rate": 2.29813408902948e-06,
      "loss": 1.0818,
      "step": 1976
    },
    {
      "epoch": 0.2779417967102488,
      "grad_norm": 1.488478422164917,
      "learning_rate": 2.248846380896519e-06,
      "loss": 1.025,
      "step": 1977
    },
    {
      "epoch": 0.2780823843666526,
      "grad_norm": 1.491041898727417,
      "learning_rate": 2.2000869399219637e-06,
      "loss": 0.9147,
      "step": 1978
    },
    {
      "epoch": 0.2782229720230564,
      "grad_norm": 1.62614107131958,
      "learning_rate": 2.1518560296118095e-06,
      "loss": 1.0665,
      "step": 1979
    },
    {
      "epoch": 0.27836355967946014,
      "grad_norm": 1.306634783744812,
      "learning_rate": 2.1041539106156917e-06,
      "loss": 1.1068,
      "step": 1980
    },
    {
      "epoch": 0.2785041473358639,
      "grad_norm": 1.3617976903915405,
      "learning_rate": 2.0569808407256086e-06,
      "loss": 1.0436,
      "step": 1981
    },
    {
      "epoch": 0.27864473499226766,
      "grad_norm": 1.3583171367645264,
      "learning_rate": 2.0103370748744244e-06,
      "loss": 1.0511,
      "step": 1982
    },
    {
      "epoch": 0.27878532264867145,
      "grad_norm": 1.4760396480560303,
      "learning_rate": 1.9642228651345796e-06,
      "loss": 0.9011,
      "step": 1983
    },
    {
      "epoch": 0.27892591030507524,
      "grad_norm": 1.5890097618103027,
      "learning_rate": 1.9186384607166376e-06,
      "loss": 1.2381,
      "step": 1984
    },
    {
      "epoch": 0.279066497961479,
      "grad_norm": 1.8947502374649048,
      "learning_rate": 1.8735841079680295e-06,
      "loss": 1.0892,
      "step": 1985
    },
    {
      "epoch": 0.27920708561788277,
      "grad_norm": 1.5266427993774414,
      "learning_rate": 1.829060050371656e-06,
      "loss": 0.9831,
      "step": 1986
    },
    {
      "epoch": 0.2793476732742865,
      "grad_norm": 1.468382477760315,
      "learning_rate": 1.785066528544599e-06,
      "loss": 1.13,
      "step": 1987
    },
    {
      "epoch": 0.2794882609306903,
      "grad_norm": 1.2977696657180786,
      "learning_rate": 1.7416037802368113e-06,
      "loss": 1.1986,
      "step": 1988
    },
    {
      "epoch": 0.279628848587094,
      "grad_norm": 1.498037338256836,
      "learning_rate": 1.6986720403298627e-06,
      "loss": 0.9628,
      "step": 1989
    },
    {
      "epoch": 0.2797694362434978,
      "grad_norm": 2.107191562652588,
      "learning_rate": 1.6562715408356078e-06,
      "loss": 1.0499,
      "step": 1990
    },
    {
      "epoch": 0.2799100238999016,
      "grad_norm": 1.478473424911499,
      "learning_rate": 1.6144025108949968e-06,
      "loss": 1.0049,
      "step": 1991
    },
    {
      "epoch": 0.28005061155630534,
      "grad_norm": 1.5224151611328125,
      "learning_rate": 1.573065176776789e-06,
      "loss": 1.1197,
      "step": 1992
    },
    {
      "epoch": 0.28019119921270913,
      "grad_norm": 1.508825659751892,
      "learning_rate": 1.5322597618763646e-06,
      "loss": 1.0493,
      "step": 1993
    },
    {
      "epoch": 0.28033178686911286,
      "grad_norm": 1.7198872566223145,
      "learning_rate": 1.4919864867145362e-06,
      "loss": 1.0902,
      "step": 1994
    },
    {
      "epoch": 0.28047237452551665,
      "grad_norm": 1.8344477415084839,
      "learning_rate": 1.4522455689362503e-06,
      "loss": 1.1235,
      "step": 1995
    },
    {
      "epoch": 0.28061296218192044,
      "grad_norm": 1.4096039533615112,
      "learning_rate": 1.4130372233095546e-06,
      "loss": 1.0658,
      "step": 1996
    },
    {
      "epoch": 0.2807535498383242,
      "grad_norm": 1.5403516292572021,
      "learning_rate": 1.3743616617243217e-06,
      "loss": 1.0565,
      "step": 1997
    },
    {
      "epoch": 0.28089413749472797,
      "grad_norm": 1.2957475185394287,
      "learning_rate": 1.3362190931911822e-06,
      "loss": 1.17,
      "step": 1998
    },
    {
      "epoch": 0.2810347251511317,
      "grad_norm": 1.6968591213226318,
      "learning_rate": 1.2986097238403384e-06,
      "loss": 1.0771,
      "step": 1999
    },
    {
      "epoch": 0.2811753128075355,
      "grad_norm": 1.4175798892974854,
      "learning_rate": 1.2615337569205077e-06,
      "loss": 1.1591,
      "step": 2000
    },
    {
      "epoch": 0.2811753128075355,
      "eval_loss": 1.1476349830627441,
      "eval_runtime": 771.8518,
      "eval_samples_per_second": 16.384,
      "eval_steps_per_second": 8.192,
      "step": 2000
    },
    {
      "epoch": 0.2813159004639393,
      "grad_norm": 1.5031172037124634,
      "learning_rate": 1.2249913927977475e-06,
      "loss": 1.0557,
      "step": 2001
    },
    {
      "epoch": 0.281456488120343,
      "grad_norm": 1.3757354021072388,
      "learning_rate": 1.1889828289544659e-06,
      "loss": 1.2578,
      "step": 2002
    },
    {
      "epoch": 0.2815970757767468,
      "grad_norm": 1.5433942079544067,
      "learning_rate": 1.1535082599882675e-06,
      "loss": 0.9502,
      "step": 2003
    },
    {
      "epoch": 0.28173766343315054,
      "grad_norm": 1.3650704622268677,
      "learning_rate": 1.1185678776109543e-06,
      "loss": 1.1037,
      "step": 2004
    },
    {
      "epoch": 0.28187825108955433,
      "grad_norm": 1.4916571378707886,
      "learning_rate": 1.084161870647471e-06,
      "loss": 1.0845,
      "step": 2005
    },
    {
      "epoch": 0.2820188387459581,
      "grad_norm": 1.615993618965149,
      "learning_rate": 1.0502904250349054e-06,
      "loss": 1.0139,
      "step": 2006
    },
    {
      "epoch": 0.28215942640236186,
      "grad_norm": 1.560158133506775,
      "learning_rate": 1.0169537238214455e-06,
      "loss": 0.9798,
      "step": 2007
    },
    {
      "epoch": 0.28230001405876565,
      "grad_norm": 1.4725921154022217,
      "learning_rate": 9.841519471654125e-07,
      "loss": 0.9966,
      "step": 2008
    },
    {
      "epoch": 0.2824406017151694,
      "grad_norm": 1.2675812244415283,
      "learning_rate": 9.518852723343075e-07,
      "loss": 1.1809,
      "step": 2009
    },
    {
      "epoch": 0.28258118937157317,
      "grad_norm": 1.3845819234848022,
      "learning_rate": 9.201538737038106e-07,
      "loss": 1.145,
      "step": 2010
    },
    {
      "epoch": 0.28272177702797696,
      "grad_norm": 1.5576871633529663,
      "learning_rate": 8.889579227568612e-07,
      "loss": 1.1835,
      "step": 2011
    },
    {
      "epoch": 0.2828623646843807,
      "grad_norm": 1.5273463726043701,
      "learning_rate": 8.58297588082746e-07,
      "loss": 1.195,
      "step": 2012
    },
    {
      "epoch": 0.2830029523407845,
      "grad_norm": 1.570312738418579,
      "learning_rate": 8.281730353761563e-07,
      "loss": 1.1027,
      "step": 2013
    },
    {
      "epoch": 0.2831435399971882,
      "grad_norm": 1.4842768907546997,
      "learning_rate": 7.985844274363329e-07,
      "loss": 1.0519,
      "step": 2014
    },
    {
      "epoch": 0.283284127653592,
      "grad_norm": 1.5416396856307983,
      "learning_rate": 7.695319241661114e-07,
      "loss": 1.0558,
      "step": 2015
    },
    {
      "epoch": 0.2834247153099958,
      "grad_norm": 1.5355985164642334,
      "learning_rate": 7.410156825711778e-07,
      "loss": 1.1238,
      "step": 2016
    },
    {
      "epoch": 0.28356530296639954,
      "grad_norm": 1.567523717880249,
      "learning_rate": 7.130358567590922e-07,
      "loss": 0.9577,
      "step": 2017
    },
    {
      "epoch": 0.2837058906228033,
      "grad_norm": 1.4636176824569702,
      "learning_rate": 6.855925979385447e-07,
      "loss": 1.2166,
      "step": 2018
    },
    {
      "epoch": 0.28384647827920706,
      "grad_norm": 1.3728286027908325,
      "learning_rate": 6.586860544184781e-07,
      "loss": 1.1496,
      "step": 2019
    },
    {
      "epoch": 0.28398706593561085,
      "grad_norm": 1.2497819662094116,
      "learning_rate": 6.323163716073776e-07,
      "loss": 1.151,
      "step": 2020
    },
    {
      "epoch": 0.28412765359201464,
      "grad_norm": 1.436539649963379,
      "learning_rate": 6.064836920123496e-07,
      "loss": 1.0769,
      "step": 2021
    },
    {
      "epoch": 0.2842682412484184,
      "grad_norm": 1.4800208806991577,
      "learning_rate": 5.811881552384768e-07,
      "loss": 1.1106,
      "step": 2022
    },
    {
      "epoch": 0.28440882890482216,
      "grad_norm": 1.5180922746658325,
      "learning_rate": 5.564298979879868e-07,
      "loss": 1.2679,
      "step": 2023
    },
    {
      "epoch": 0.2845494165612259,
      "grad_norm": 1.3939441442489624,
      "learning_rate": 5.322090540595515e-07,
      "loss": 1.2431,
      "step": 2024
    },
    {
      "epoch": 0.2846900042176297,
      "grad_norm": 1.5369542837142944,
      "learning_rate": 5.085257543475552e-07,
      "loss": 1.1129,
      "step": 2025
    },
    {
      "epoch": 0.2848305918740335,
      "grad_norm": 1.4204654693603516,
      "learning_rate": 4.853801268413616e-07,
      "loss": 1.1646,
      "step": 2026
    },
    {
      "epoch": 0.2849711795304372,
      "grad_norm": 1.5011147260665894,
      "learning_rate": 4.627722966246806e-07,
      "loss": 1.0921,
      "step": 2027
    },
    {
      "epoch": 0.285111767186841,
      "grad_norm": 1.4494242668151855,
      "learning_rate": 4.407023858748138e-07,
      "loss": 1.1376,
      "step": 2028
    },
    {
      "epoch": 0.28525235484324474,
      "grad_norm": 1.4798598289489746,
      "learning_rate": 4.1917051386206606e-07,
      "loss": 1.0422,
      "step": 2029
    },
    {
      "epoch": 0.2853929424996485,
      "grad_norm": 1.4072519540786743,
      "learning_rate": 3.9817679694906797e-07,
      "loss": 1.0959,
      "step": 2030
    },
    {
      "epoch": 0.2855335301560523,
      "grad_norm": 1.5167269706726074,
      "learning_rate": 3.777213485901432e-07,
      "loss": 1.1847,
      "step": 2031
    },
    {
      "epoch": 0.28567411781245605,
      "grad_norm": 1.7162890434265137,
      "learning_rate": 3.5780427933068685e-07,
      "loss": 1.1903,
      "step": 2032
    },
    {
      "epoch": 0.28581470546885984,
      "grad_norm": 1.1971735954284668,
      "learning_rate": 3.3842569680662127e-07,
      "loss": 1.0789,
      "step": 2033
    },
    {
      "epoch": 0.2859552931252636,
      "grad_norm": 1.4415709972381592,
      "learning_rate": 3.1958570574375237e-07,
      "loss": 1.0285,
      "step": 2034
    },
    {
      "epoch": 0.28609588078166737,
      "grad_norm": 1.690149188041687,
      "learning_rate": 3.0128440795723633e-07,
      "loss": 1.1027,
      "step": 2035
    },
    {
      "epoch": 0.28623646843807116,
      "grad_norm": 1.4007121324539185,
      "learning_rate": 2.835219023509916e-07,
      "loss": 1.141,
      "step": 2036
    },
    {
      "epoch": 0.2863770560944749,
      "grad_norm": 1.3839969635009766,
      "learning_rate": 2.662982849172546e-07,
      "loss": 1.0412,
      "step": 2037
    },
    {
      "epoch": 0.2865176437508787,
      "grad_norm": 1.5808452367782593,
      "learning_rate": 2.4961364873593576e-07,
      "loss": 1.02,
      "step": 2038
    },
    {
      "epoch": 0.2866582314072824,
      "grad_norm": 1.3689910173416138,
      "learning_rate": 2.3346808397423093e-07,
      "loss": 0.9294,
      "step": 2039
    },
    {
      "epoch": 0.2867988190636862,
      "grad_norm": 1.5193403959274292,
      "learning_rate": 2.1786167788604428e-07,
      "loss": 1.0765,
      "step": 2040
    },
    {
      "epoch": 0.28693940672009,
      "grad_norm": 1.277463436126709,
      "learning_rate": 2.027945148115884e-07,
      "loss": 1.12,
      "step": 2041
    },
    {
      "epoch": 0.28707999437649373,
      "grad_norm": 1.7244422435760498,
      "learning_rate": 1.8826667617687365e-07,
      "loss": 1.0167,
      "step": 2042
    },
    {
      "epoch": 0.2872205820328975,
      "grad_norm": 1.7544076442718506,
      "learning_rate": 1.7427824049330854e-07,
      "loss": 1.1316,
      "step": 2043
    },
    {
      "epoch": 0.28736116968930125,
      "grad_norm": 1.3945910930633545,
      "learning_rate": 1.6082928335724444e-07,
      "loss": 1.1873,
      "step": 2044
    },
    {
      "epoch": 0.28750175734570504,
      "grad_norm": 1.6343894004821777,
      "learning_rate": 1.4791987744959823e-07,
      "loss": 1.1147,
      "step": 2045
    },
    {
      "epoch": 0.28764234500210883,
      "grad_norm": 1.3554530143737793,
      "learning_rate": 1.3555009253543026e-07,
      "loss": 1.1975,
      "step": 2046
    },
    {
      "epoch": 0.28778293265851257,
      "grad_norm": 1.4255458116531372,
      "learning_rate": 1.2371999546355594e-07,
      "loss": 1.1673,
      "step": 2047
    },
    {
      "epoch": 0.28792352031491636,
      "grad_norm": 1.6745336055755615,
      "learning_rate": 1.1242965016625695e-07,
      "loss": 1.451,
      "step": 2048
    },
    {
      "epoch": 0.2880641079713201,
      "grad_norm": 1.5666884183883667,
      "learning_rate": 1.016791176588594e-07,
      "loss": 1.0282,
      "step": 2049
    },
    {
      "epoch": 0.2882046956277239,
      "grad_norm": 1.633862853050232,
      "learning_rate": 9.14684560394119e-08,
      "loss": 1.1106,
      "step": 2050
    },
    {
      "epoch": 0.2883452832841277,
      "grad_norm": 1.5345650911331177,
      "learning_rate": 8.179772048843015e-08,
      "loss": 1.0642,
      "step": 2051
    },
    {
      "epoch": 0.2884858709405314,
      "grad_norm": 1.4833000898361206,
      "learning_rate": 7.266696326853062e-08,
      "loss": 1.1598,
      "step": 2052
    },
    {
      "epoch": 0.2886264585969352,
      "grad_norm": 1.4100396633148193,
      "learning_rate": 6.407623372419735e-08,
      "loss": 1.0768,
      "step": 2053
    },
    {
      "epoch": 0.28876704625333893,
      "grad_norm": 1.4840251207351685,
      "learning_rate": 5.6025578281471145e-08,
      "loss": 1.0988,
      "step": 2054
    },
    {
      "epoch": 0.2889076339097427,
      "grad_norm": 1.553389549255371,
      "learning_rate": 4.851504044773858e-08,
      "loss": 1.1208,
      "step": 2055
    },
    {
      "epoch": 0.2890482215661465,
      "grad_norm": 1.300073266029358,
      "learning_rate": 4.154466081147668e-08,
      "loss": 0.9492,
      "step": 2056
    },
    {
      "epoch": 0.28918880922255025,
      "grad_norm": 1.3478394746780396,
      "learning_rate": 3.511447704204196e-08,
      "loss": 1.19,
      "step": 2057
    },
    {
      "epoch": 0.28932939687895404,
      "grad_norm": 1.4302232265472412,
      "learning_rate": 2.922452388945951e-08,
      "loss": 1.1609,
      "step": 2058
    },
    {
      "epoch": 0.28946998453535777,
      "grad_norm": 1.4906209707260132,
      "learning_rate": 2.3874833184234226e-08,
      "loss": 1.2879,
      "step": 2059
    },
    {
      "epoch": 0.28961057219176156,
      "grad_norm": 1.836055040359497,
      "learning_rate": 1.9065433837173184e-08,
      "loss": 1.1922,
      "step": 2060
    },
    {
      "epoch": 0.28975115984816535,
      "grad_norm": 1.511201024055481,
      "learning_rate": 1.4796351839263534e-08,
      "loss": 1.1624,
      "step": 2061
    },
    {
      "epoch": 0.2898917475045691,
      "grad_norm": 1.8918993473052979,
      "learning_rate": 1.1067610261494832e-08,
      "loss": 1.0161,
      "step": 2062
    },
    {
      "epoch": 0.2900323351609729,
      "grad_norm": 1.2767348289489746,
      "learning_rate": 7.879229254748043e-09,
      "loss": 1.2256,
      "step": 2063
    },
    {
      "epoch": 0.2901729228173766,
      "grad_norm": 1.5776616334915161,
      "learning_rate": 5.23122604967341e-09,
      "loss": 1.0756,
      "step": 2064
    },
    {
      "epoch": 0.2903135104737804,
      "grad_norm": 1.5285292863845825,
      "learning_rate": 3.123614956634935e-09,
      "loss": 1.052,
      "step": 2065
    },
    {
      "epoch": 0.2904540981301842,
      "grad_norm": 1.3718680143356323,
      "learning_rate": 1.5564073655771615e-09,
      "loss": 1.1153,
      "step": 2066
    },
    {
      "epoch": 0.2905946857865879,
      "grad_norm": 1.5747724771499634,
      "learning_rate": 5.296117460140693e-10,
      "loss": 1.0307,
      "step": 2067
    },
    {
      "epoch": 0.2907352734429917,
      "grad_norm": 1.3631147146224976,
      "learning_rate": 4.323364696245946e-11,
      "loss": 1.1694,
      "step": 2068
    },
    {
      "epoch": 0.29087586109939545,
      "grad_norm": 1.3556069135665894,
      "learning_rate": 9.727569690864968e-11,
      "loss": 1.2536,
      "step": 2069
    },
    {
      "epoch": 0.29101644875579924,
      "grad_norm": 1.371313452720642,
      "learning_rate": 6.917376037973711e-10,
      "loss": 1.1537,
      "step": 2070
    },
    {
      "epoch": 0.29115703641220303,
      "grad_norm": 1.554242730140686,
      "learning_rate": 1.8266161550317685e-09,
      "loss": 1.0196,
      "step": 2071
    },
    {
      "epoch": 0.29129762406860676,
      "grad_norm": 1.6822043657302856,
      "learning_rate": 3.501905217506707e-09,
      "loss": 1.0525,
      "step": 2072
    },
    {
      "epoch": 0.29143821172501055,
      "grad_norm": 1.3837276697158813,
      "learning_rate": 5.717595737608772e-09,
      "loss": 1.2177,
      "step": 2073
    },
    {
      "epoch": 0.2915787993814143,
      "grad_norm": 1.7001011371612549,
      "learning_rate": 8.473675741293986e-09,
      "loss": 1.0091,
      "step": 2074
    },
    {
      "epoch": 0.2917193870378181,
      "grad_norm": 1.5086917877197266,
      "learning_rate": 1.1770130334154417e-08,
      "loss": 1.1586,
      "step": 2075
    },
    {
      "epoch": 0.29185997469422187,
      "grad_norm": 1.5363924503326416,
      "learning_rate": 1.5606941701462596e-08,
      "loss": 0.9835,
      "step": 2076
    },
    {
      "epoch": 0.2920005623506256,
      "grad_norm": 1.3545851707458496,
      "learning_rate": 1.998408910831584e-08,
      "loss": 1.1644,
      "step": 2077
    },
    {
      "epoch": 0.2921411500070294,
      "grad_norm": 1.4151930809020996,
      "learning_rate": 2.490154889972507e-08,
      "loss": 1.2109,
      "step": 2078
    },
    {
      "epoch": 0.2922817376634331,
      "grad_norm": 1.4708632230758667,
      "learning_rate": 3.035929450072583e-08,
      "loss": 1.1935,
      "step": 2079
    },
    {
      "epoch": 0.2924223253198369,
      "grad_norm": 1.5734426975250244,
      "learning_rate": 3.635729641654484e-08,
      "loss": 1.0489,
      "step": 2080
    },
    {
      "epoch": 0.2925629129762407,
      "grad_norm": 1.4322646856307983,
      "learning_rate": 4.2895522232766496e-08,
      "loss": 1.0513,
      "step": 2081
    },
    {
      "epoch": 0.29270350063264444,
      "grad_norm": 1.4104416370391846,
      "learning_rate": 4.9973936615488326e-08,
      "loss": 1.272,
      "step": 2082
    },
    {
      "epoch": 0.29284408828904823,
      "grad_norm": 1.3405824899673462,
      "learning_rate": 5.7592501311498624e-08,
      "loss": 0.9622,
      "step": 2083
    },
    {
      "epoch": 0.29298467594545197,
      "grad_norm": 1.534777283668518,
      "learning_rate": 6.57511751485429e-08,
      "loss": 1.0869,
      "step": 2084
    },
    {
      "epoch": 0.29312526360185576,
      "grad_norm": 1.4600536823272705,
      "learning_rate": 7.44499140354682e-08,
      "loss": 1.0722,
      "step": 2085
    },
    {
      "epoch": 0.29326585125825955,
      "grad_norm": 1.5318377017974854,
      "learning_rate": 8.368867096252287e-08,
      "loss": 1.2274,
      "step": 2086
    },
    {
      "epoch": 0.2934064389146633,
      "grad_norm": 1.5747817754745483,
      "learning_rate": 9.346739600158972e-08,
      "loss": 1.0011,
      "step": 2087
    },
    {
      "epoch": 0.29354702657106707,
      "grad_norm": 1.5902436971664429,
      "learning_rate": 1.0378603630643025e-07,
      "loss": 1.0914,
      "step": 2088
    },
    {
      "epoch": 0.2936876142274708,
      "grad_norm": 1.395960807800293,
      "learning_rate": 1.1464453611300663e-07,
      "loss": 1.0747,
      "step": 2089
    },
    {
      "epoch": 0.2938282018838746,
      "grad_norm": 1.407586932182312,
      "learning_rate": 1.2604283673979257e-07,
      "loss": 1.0879,
      "step": 2090
    },
    {
      "epoch": 0.2939687895402784,
      "grad_norm": 1.4049508571624756,
      "learning_rate": 1.379808765880397e-07,
      "loss": 1.1831,
      "step": 2091
    },
    {
      "epoch": 0.2941093771966821,
      "grad_norm": 1.748708724975586,
      "learning_rate": 1.504585911421441e-07,
      "loss": 1.1425,
      "step": 2092
    },
    {
      "epoch": 0.2942499648530859,
      "grad_norm": 1.7278168201446533,
      "learning_rate": 1.6347591296999031e-07,
      "loss": 0.8781,
      "step": 2093
    },
    {
      "epoch": 0.29439055250948964,
      "grad_norm": 1.3054282665252686,
      "learning_rate": 1.770327717233178e-07,
      "loss": 1.1222,
      "step": 2094
    },
    {
      "epoch": 0.29453114016589343,
      "grad_norm": 1.5526026487350464,
      "learning_rate": 1.9112909413810942e-07,
      "loss": 1.1301,
      "step": 2095
    },
    {
      "epoch": 0.2946717278222972,
      "grad_norm": 1.634819746017456,
      "learning_rate": 2.0576480403495802e-07,
      "loss": 1.0431,
      "step": 2096
    },
    {
      "epoch": 0.29481231547870096,
      "grad_norm": 1.3928965330123901,
      "learning_rate": 2.2093982231951026e-07,
      "loss": 1.123,
      "step": 2097
    },
    {
      "epoch": 0.29495290313510475,
      "grad_norm": 1.507536768913269,
      "learning_rate": 2.3665406698285542e-07,
      "loss": 1.1259,
      "step": 2098
    },
    {
      "epoch": 0.2950934907915085,
      "grad_norm": 1.4346659183502197,
      "learning_rate": 2.529074531020359e-07,
      "loss": 1.213,
      "step": 2099
    },
    {
      "epoch": 0.2952340784479123,
      "grad_norm": 1.5032918453216553,
      "learning_rate": 2.6969989284042484e-07,
      "loss": 1.1541,
      "step": 2100
    },
    {
      "epoch": 0.29537466610431606,
      "grad_norm": 1.2891212701797485,
      "learning_rate": 2.8703129544825903e-07,
      "loss": 1.0075,
      "step": 2101
    },
    {
      "epoch": 0.2955152537607198,
      "grad_norm": 1.4989335536956787,
      "learning_rate": 3.049015672631161e-07,
      "loss": 1.0168,
      "step": 2102
    },
    {
      "epoch": 0.2956558414171236,
      "grad_norm": 1.6083587408065796,
      "learning_rate": 3.2331061171039234e-07,
      "loss": 1.0247,
      "step": 2103
    },
    {
      "epoch": 0.2957964290735273,
      "grad_norm": 1.4017013311386108,
      "learning_rate": 3.4225832930385726e-07,
      "loss": 1.1819,
      "step": 2104
    },
    {
      "epoch": 0.2959370167299311,
      "grad_norm": 1.3218934535980225,
      "learning_rate": 3.61744617646198e-07,
      "loss": 1.0511,
      "step": 2105
    },
    {
      "epoch": 0.2960776043863349,
      "grad_norm": 1.6402549743652344,
      "learning_rate": 3.81769371429519e-07,
      "loss": 1.1921,
      "step": 2106
    },
    {
      "epoch": 0.29621819204273864,
      "grad_norm": 1.6788759231567383,
      "learning_rate": 4.023324824359964e-07,
      "loss": 1.2626,
      "step": 2107
    },
    {
      "epoch": 0.2963587796991424,
      "grad_norm": 1.287074089050293,
      "learning_rate": 4.2343383953836745e-07,
      "loss": 1.0942,
      "step": 2108
    },
    {
      "epoch": 0.29649936735554616,
      "grad_norm": 1.7501200437545776,
      "learning_rate": 4.4507332870059594e-07,
      "loss": 1.1764,
      "step": 2109
    },
    {
      "epoch": 0.29663995501194995,
      "grad_norm": 1.4297988414764404,
      "learning_rate": 4.6725083297848304e-07,
      "loss": 0.9255,
      "step": 2110
    },
    {
      "epoch": 0.29678054266835374,
      "grad_norm": 1.4229438304901123,
      "learning_rate": 4.89966232520267e-07,
      "loss": 1.0835,
      "step": 2111
    },
    {
      "epoch": 0.2969211303247575,
      "grad_norm": 1.6475692987442017,
      "learning_rate": 5.132194045673111e-07,
      "loss": 1.1707,
      "step": 2112
    },
    {
      "epoch": 0.29706171798116127,
      "grad_norm": 1.6095492839813232,
      "learning_rate": 5.370102234547147e-07,
      "loss": 1.0918,
      "step": 2113
    },
    {
      "epoch": 0.297202305637565,
      "grad_norm": 1.4704030752182007,
      "learning_rate": 5.613385606120569e-07,
      "loss": 1.028,
      "step": 2114
    },
    {
      "epoch": 0.2973428932939688,
      "grad_norm": 1.6137768030166626,
      "learning_rate": 5.862042845640403e-07,
      "loss": 0.9637,
      "step": 2115
    },
    {
      "epoch": 0.2974834809503726,
      "grad_norm": 1.6161149740219116,
      "learning_rate": 6.11607260931224e-07,
      "loss": 1.1781,
      "step": 2116
    },
    {
      "epoch": 0.2976240686067763,
      "grad_norm": 1.6209287643432617,
      "learning_rate": 6.375473524307451e-07,
      "loss": 1.0292,
      "step": 2117
    },
    {
      "epoch": 0.2977646562631801,
      "grad_norm": 1.401870608329773,
      "learning_rate": 6.640244188770739e-07,
      "loss": 1.1445,
      "step": 2118
    },
    {
      "epoch": 0.29790524391958384,
      "grad_norm": 1.4407286643981934,
      "learning_rate": 6.910383171827351e-07,
      "loss": 1.1499,
      "step": 2119
    },
    {
      "epoch": 0.29804583157598763,
      "grad_norm": 1.5600831508636475,
      "learning_rate": 7.185889013591074e-07,
      "loss": 0.9308,
      "step": 2120
    },
    {
      "epoch": 0.2981864192323914,
      "grad_norm": 1.452297568321228,
      "learning_rate": 7.466760225172342e-07,
      "loss": 0.9862,
      "step": 2121
    },
    {
      "epoch": 0.29832700688879515,
      "grad_norm": 1.6887307167053223,
      "learning_rate": 7.752995288685894e-07,
      "loss": 1.0035,
      "step": 2122
    },
    {
      "epoch": 0.29846759454519894,
      "grad_norm": 1.4682317972183228,
      "learning_rate": 8.044592657258987e-07,
      "loss": 1.2522,
      "step": 2123
    },
    {
      "epoch": 0.2986081822016027,
      "grad_norm": 1.4903210401535034,
      "learning_rate": 8.341550755040062e-07,
      "loss": 0.9441,
      "step": 2124
    },
    {
      "epoch": 0.29874876985800647,
      "grad_norm": 1.2723902463912964,
      "learning_rate": 8.643867977206954e-07,
      "loss": 1.0111,
      "step": 2125
    },
    {
      "epoch": 0.29888935751441026,
      "grad_norm": 1.5484318733215332,
      "learning_rate": 8.951542689976e-07,
      "loss": 1.0249,
      "step": 2126
    },
    {
      "epoch": 0.299029945170814,
      "grad_norm": 1.4897651672363281,
      "learning_rate": 9.264573230610029e-07,
      "loss": 1.0199,
      "step": 2127
    },
    {
      "epoch": 0.2991705328272178,
      "grad_norm": 1.6007806062698364,
      "learning_rate": 9.58295790742847e-07,
      "loss": 1.1984,
      "step": 2128
    },
    {
      "epoch": 0.2993111204836215,
      "grad_norm": 1.7752472162246704,
      "learning_rate": 9.906694999815446e-07,
      "loss": 1.0633,
      "step": 2129
    },
    {
      "epoch": 0.2994517081400253,
      "grad_norm": 1.6050792932510376,
      "learning_rate": 1.0235782758229894e-06,
      "loss": 1.0824,
      "step": 2130
    },
    {
      "epoch": 0.2995922957964291,
      "grad_norm": 1.3362979888916016,
      "learning_rate": 1.057021940421421e-06,
      "loss": 1.0395,
      "step": 2131
    },
    {
      "epoch": 0.29973288345283283,
      "grad_norm": 1.3762074708938599,
      "learning_rate": 1.0910003130404911e-06,
      "loss": 1.2769,
      "step": 2132
    },
    {
      "epoch": 0.2998734711092366,
      "grad_norm": 1.5603691339492798,
      "learning_rate": 1.12551321005413e-06,
      "loss": 1.2457,
      "step": 2133
    },
    {
      "epoch": 0.30001405876564036,
      "grad_norm": 1.6741743087768555,
      "learning_rate": 1.1605604449476116e-06,
      "loss": 1.1638,
      "step": 2134
    },
    {
      "epoch": 0.30015464642204415,
      "grad_norm": 1.4574658870697021,
      "learning_rate": 1.196141828318531e-06,
      "loss": 1.0279,
      "step": 2135
    },
    {
      "epoch": 0.30029523407844794,
      "grad_norm": 1.3236932754516602,
      "learning_rate": 1.2322571678778482e-06,
      "loss": 1.0507,
      "step": 2136
    },
    {
      "epoch": 0.30043582173485167,
      "grad_norm": 1.717746615409851,
      "learning_rate": 1.2689062684508978e-06,
      "loss": 1.0859,
      "step": 2137
    },
    {
      "epoch": 0.30057640939125546,
      "grad_norm": 1.4784644842147827,
      "learning_rate": 1.3060889319784885e-06,
      "loss": 1.2348,
      "step": 2138
    },
    {
      "epoch": 0.3007169970476592,
      "grad_norm": 1.3315507173538208,
      "learning_rate": 1.3438049575179025e-06,
      "loss": 1.0709,
      "step": 2139
    },
    {
      "epoch": 0.300857584704063,
      "grad_norm": 1.584871768951416,
      "learning_rate": 1.3820541412440713e-06,
      "loss": 0.9577,
      "step": 2140
    },
    {
      "epoch": 0.3009981723604668,
      "grad_norm": 1.3968477249145508,
      "learning_rate": 1.420836276450599e-06,
      "loss": 1.1058,
      "step": 2141
    },
    {
      "epoch": 0.3011387600168705,
      "grad_norm": 1.5823681354522705,
      "learning_rate": 1.460151153550926e-06,
      "loss": 1.0269,
      "step": 2142
    },
    {
      "epoch": 0.3012793476732743,
      "grad_norm": 1.30938720703125,
      "learning_rate": 1.4999985600794298e-06,
      "loss": 1.0224,
      "step": 2143
    },
    {
      "epoch": 0.30141993532967803,
      "grad_norm": 1.4543120861053467,
      "learning_rate": 1.5403782806926225e-06,
      "loss": 1.1904,
      "step": 2144
    },
    {
      "epoch": 0.3015605229860818,
      "grad_norm": 1.7214415073394775,
      "learning_rate": 1.5812900971702627e-06,
      "loss": 1.1023,
      "step": 2145
    },
    {
      "epoch": 0.3017011106424856,
      "grad_norm": 1.532670021057129,
      "learning_rate": 1.6227337884165417e-06,
      "loss": 1.031,
      "step": 2146
    },
    {
      "epoch": 0.30184169829888935,
      "grad_norm": 1.3816742897033691,
      "learning_rate": 1.6647091304613172e-06,
      "loss": 0.9485,
      "step": 2147
    },
    {
      "epoch": 0.30198228595529314,
      "grad_norm": 1.3201311826705933,
      "learning_rate": 1.7072158964612672e-06,
      "loss": 0.9842,
      "step": 2148
    },
    {
      "epoch": 0.3021228736116969,
      "grad_norm": 1.3898518085479736,
      "learning_rate": 1.7502538567012007e-06,
      "loss": 1.1951,
      "step": 2149
    },
    {
      "epoch": 0.30226346126810066,
      "grad_norm": 1.3466874361038208,
      "learning_rate": 1.7938227785951668e-06,
      "loss": 1.2461,
      "step": 2150
    },
    {
      "epoch": 0.30240404892450445,
      "grad_norm": 1.5167697668075562,
      "learning_rate": 1.8379224266878548e-06,
      "loss": 1.1909,
      "step": 2151
    },
    {
      "epoch": 0.3025446365809082,
      "grad_norm": 1.4345914125442505,
      "learning_rate": 1.8825525626557594e-06,
      "loss": 1.171,
      "step": 2152
    },
    {
      "epoch": 0.302685224237312,
      "grad_norm": 1.429538369178772,
      "learning_rate": 1.927712945308557e-06,
      "loss": 1.114,
      "step": 2153
    },
    {
      "epoch": 0.3028258118937157,
      "grad_norm": 1.6275800466537476,
      "learning_rate": 1.9734033305903066e-06,
      "loss": 0.98,
      "step": 2154
    },
    {
      "epoch": 0.3029663995501195,
      "grad_norm": 1.8187564611434937,
      "learning_rate": 2.019623471580867e-06,
      "loss": 1.2625,
      "step": 2155
    },
    {
      "epoch": 0.3031069872065233,
      "grad_norm": 1.4205243587493896,
      "learning_rate": 2.0663731184971778e-06,
      "loss": 1.196,
      "step": 2156
    },
    {
      "epoch": 0.303247574862927,
      "grad_norm": 1.6312562227249146,
      "learning_rate": 2.113652018694612e-06,
      "loss": 1.2388,
      "step": 2157
    },
    {
      "epoch": 0.3033881625193308,
      "grad_norm": 1.3889849185943604,
      "learning_rate": 2.161459916668351e-06,
      "loss": 1.1208,
      "step": 2158
    },
    {
      "epoch": 0.30352875017573455,
      "grad_norm": 1.4144657850265503,
      "learning_rate": 2.2097965540547992e-06,
      "loss": 0.9841,
      "step": 2159
    },
    {
      "epoch": 0.30366933783213834,
      "grad_norm": 1.436781644821167,
      "learning_rate": 2.2586616696328664e-06,
      "loss": 1.0658,
      "step": 2160
    },
    {
      "epoch": 0.30380992548854213,
      "grad_norm": 1.5081862211227417,
      "learning_rate": 2.3080549993255595e-06,
      "loss": 1.0379,
      "step": 2161
    },
    {
      "epoch": 0.30395051314494587,
      "grad_norm": 1.4369542598724365,
      "learning_rate": 2.3579762762012127e-06,
      "loss": 1.2564,
      "step": 2162
    },
    {
      "epoch": 0.30409110080134966,
      "grad_norm": 1.6051793098449707,
      "learning_rate": 2.4084252304751088e-06,
      "loss": 1.0226,
      "step": 2163
    },
    {
      "epoch": 0.3042316884577534,
      "grad_norm": 1.6071279048919678,
      "learning_rate": 2.4594015895107904e-06,
      "loss": 1.0587,
      "step": 2164
    },
    {
      "epoch": 0.3043722761141572,
      "grad_norm": 1.4366730451583862,
      "learning_rate": 2.5109050778216347e-06,
      "loss": 1.1372,
      "step": 2165
    },
    {
      "epoch": 0.30451286377056097,
      "grad_norm": 1.665662407875061,
      "learning_rate": 2.562935417072276e-06,
      "loss": 1.1471,
      "step": 2166
    },
    {
      "epoch": 0.3046534514269647,
      "grad_norm": 1.3750959634780884,
      "learning_rate": 2.6154923260801824e-06,
      "loss": 0.9287,
      "step": 2167
    },
    {
      "epoch": 0.3047940390833685,
      "grad_norm": 1.663771629333496,
      "learning_rate": 2.668575520817096e-06,
      "loss": 1.0453,
      "step": 2168
    },
    {
      "epoch": 0.30493462673977223,
      "grad_norm": 1.258080005645752,
      "learning_rate": 2.722184714410614e-06,
      "loss": 1.1675,
      "step": 2169
    },
    {
      "epoch": 0.305075214396176,
      "grad_norm": 1.754284143447876,
      "learning_rate": 2.776319617145695e-06,
      "loss": 0.9575,
      "step": 2170
    },
    {
      "epoch": 0.3052158020525798,
      "grad_norm": 1.496250867843628,
      "learning_rate": 2.830979936466338e-06,
      "loss": 1.0309,
      "step": 2171
    },
    {
      "epoch": 0.30535638970898354,
      "grad_norm": 1.5736641883850098,
      "learning_rate": 2.8861653769770014e-06,
      "loss": 1.2617,
      "step": 2172
    },
    {
      "epoch": 0.30549697736538733,
      "grad_norm": 1.7080246210098267,
      "learning_rate": 2.941875640444347e-06,
      "loss": 0.9908,
      "step": 2173
    },
    {
      "epoch": 0.30563756502179107,
      "grad_norm": 1.525061845779419,
      "learning_rate": 2.998110425798717e-06,
      "loss": 0.9873,
      "step": 2174
    },
    {
      "epoch": 0.30577815267819486,
      "grad_norm": 1.3414701223373413,
      "learning_rate": 3.05486942913592e-06,
      "loss": 1.1748,
      "step": 2175
    },
    {
      "epoch": 0.30591874033459865,
      "grad_norm": 1.5051696300506592,
      "learning_rate": 3.1121523437186774e-06,
      "loss": 1.0573,
      "step": 2176
    },
    {
      "epoch": 0.3060593279910024,
      "grad_norm": 1.4089961051940918,
      "learning_rate": 3.1699588599784837e-06,
      "loss": 1.095,
      "step": 2177
    },
    {
      "epoch": 0.3061999156474062,
      "grad_norm": 1.6059106588363647,
      "learning_rate": 3.2282886655171207e-06,
      "loss": 1.0347,
      "step": 2178
    },
    {
      "epoch": 0.3063405033038099,
      "grad_norm": 1.4730181694030762,
      "learning_rate": 3.2871414451084547e-06,
      "loss": 1.2297,
      "step": 2179
    },
    {
      "epoch": 0.3064810909602137,
      "grad_norm": 1.4801195859909058,
      "learning_rate": 3.3465168807000345e-06,
      "loss": 0.9763,
      "step": 2180
    },
    {
      "epoch": 0.3066216786166175,
      "grad_norm": 1.6411943435668945,
      "learning_rate": 3.406414651414935e-06,
      "loss": 0.9714,
      "step": 2181
    },
    {
      "epoch": 0.3067622662730212,
      "grad_norm": 1.5691460371017456,
      "learning_rate": 3.4668344335533678e-06,
      "loss": 1.1237,
      "step": 2182
    },
    {
      "epoch": 0.306902853929425,
      "grad_norm": 1.4515266418457031,
      "learning_rate": 3.527775900594543e-06,
      "loss": 1.2826,
      "step": 2183
    },
    {
      "epoch": 0.30704344158582875,
      "grad_norm": 1.3572288751602173,
      "learning_rate": 3.5892387231983292e-06,
      "loss": 1.1915,
      "step": 2184
    },
    {
      "epoch": 0.30718402924223254,
      "grad_norm": 1.6041160821914673,
      "learning_rate": 3.6512225692071244e-06,
      "loss": 1.077,
      "step": 2185
    },
    {
      "epoch": 0.3073246168986363,
      "grad_norm": 1.52364981174469,
      "learning_rate": 3.7137271036475684e-06,
      "loss": 1.2156,
      "step": 2186
    },
    {
      "epoch": 0.30746520455504006,
      "grad_norm": 1.6818699836730957,
      "learning_rate": 3.7767519887324297e-06,
      "loss": 1.1872,
      "step": 2187
    },
    {
      "epoch": 0.30760579221144385,
      "grad_norm": 1.4065322875976562,
      "learning_rate": 3.840296883862393e-06,
      "loss": 1.0961,
      "step": 2188
    },
    {
      "epoch": 0.3077463798678476,
      "grad_norm": 1.4919540882110596,
      "learning_rate": 3.904361445627869e-06,
      "loss": 1.0487,
      "step": 2189
    },
    {
      "epoch": 0.3078869675242514,
      "grad_norm": 1.3514857292175293,
      "learning_rate": 3.968945327810925e-06,
      "loss": 0.9679,
      "step": 2190
    },
    {
      "epoch": 0.30802755518065517,
      "grad_norm": 1.5102182626724243,
      "learning_rate": 4.034048181387085e-06,
      "loss": 1.2892,
      "step": 2191
    },
    {
      "epoch": 0.3081681428370589,
      "grad_norm": 1.368980050086975,
      "learning_rate": 4.099669654527282e-06,
      "loss": 1.167,
      "step": 2192
    },
    {
      "epoch": 0.3083087304934627,
      "grad_norm": 1.3581781387329102,
      "learning_rate": 4.165809392599662e-06,
      "loss": 1.0965,
      "step": 2193
    },
    {
      "epoch": 0.3084493181498664,
      "grad_norm": 1.467124342918396,
      "learning_rate": 4.23246703817165e-06,
      "loss": 1.1199,
      "step": 2194
    },
    {
      "epoch": 0.3085899058062702,
      "grad_norm": 1.6823662519454956,
      "learning_rate": 4.2996422310116915e-06,
      "loss": 1.1668,
      "step": 2195
    },
    {
      "epoch": 0.308730493462674,
      "grad_norm": 1.6194134950637817,
      "learning_rate": 4.367334608091389e-06,
      "loss": 1.0588,
      "step": 2196
    },
    {
      "epoch": 0.30887108111907774,
      "grad_norm": 1.4680997133255005,
      "learning_rate": 4.4355438035873164e-06,
      "loss": 0.9803,
      "step": 2197
    },
    {
      "epoch": 0.30901166877548153,
      "grad_norm": 1.46083402633667,
      "learning_rate": 4.5042694488830915e-06,
      "loss": 1.1874,
      "step": 2198
    },
    {
      "epoch": 0.30915225643188526,
      "grad_norm": 1.4046075344085693,
      "learning_rate": 4.57351117257131e-06,
      "loss": 1.0454,
      "step": 2199
    },
    {
      "epoch": 0.30929284408828905,
      "grad_norm": 1.4879835844039917,
      "learning_rate": 4.643268600455608e-06,
      "loss": 1.0913,
      "step": 2200
    },
    {
      "epoch": 0.3094334317446928,
      "grad_norm": 1.466220498085022,
      "learning_rate": 4.7135413555525866e-06,
      "loss": 1.095,
      "step": 2201
    },
    {
      "epoch": 0.3095740194010966,
      "grad_norm": 1.361093521118164,
      "learning_rate": 4.784329058093984e-06,
      "loss": 1.1388,
      "step": 2202
    },
    {
      "epoch": 0.30971460705750037,
      "grad_norm": 1.3678358793258667,
      "learning_rate": 4.855631325528609e-06,
      "loss": 1.1929,
      "step": 2203
    },
    {
      "epoch": 0.3098551947139041,
      "grad_norm": 1.4158788919448853,
      "learning_rate": 4.927447772524507e-06,
      "loss": 1.0103,
      "step": 2204
    },
    {
      "epoch": 0.3099957823703079,
      "grad_norm": 1.8388233184814453,
      "learning_rate": 4.999778010970913e-06,
      "loss": 1.1256,
      "step": 2205
    },
    {
      "epoch": 0.3101363700267116,
      "grad_norm": 1.5419753789901733,
      "learning_rate": 5.072621649980525e-06,
      "loss": 1.0468,
      "step": 2206
    },
    {
      "epoch": 0.3102769576831154,
      "grad_norm": 1.5483195781707764,
      "learning_rate": 5.145978295891429e-06,
      "loss": 0.8499,
      "step": 2207
    },
    {
      "epoch": 0.3104175453395192,
      "grad_norm": 1.4003221988677979,
      "learning_rate": 5.219847552269385e-06,
      "loss": 1.0648,
      "step": 2208
    },
    {
      "epoch": 0.31055813299592294,
      "grad_norm": 1.6232738494873047,
      "learning_rate": 5.294229019909858e-06,
      "loss": 1.2759,
      "step": 2209
    },
    {
      "epoch": 0.31069872065232673,
      "grad_norm": 1.5645356178283691,
      "learning_rate": 5.369122296840256e-06,
      "loss": 0.9874,
      "step": 2210
    },
    {
      "epoch": 0.31083930830873047,
      "grad_norm": 1.313323974609375,
      "learning_rate": 5.444526978322007e-06,
      "loss": 1.1395,
      "step": 2211
    },
    {
      "epoch": 0.31097989596513426,
      "grad_norm": 1.44762122631073,
      "learning_rate": 5.520442656852887e-06,
      "loss": 1.3427,
      "step": 2212
    },
    {
      "epoch": 0.31112048362153805,
      "grad_norm": 1.3975509405136108,
      "learning_rate": 5.596868922169074e-06,
      "loss": 1.1523,
      "step": 2213
    },
    {
      "epoch": 0.3112610712779418,
      "grad_norm": 1.3087579011917114,
      "learning_rate": 5.673805361247453e-06,
      "loss": 1.043,
      "step": 2214
    },
    {
      "epoch": 0.31140165893434557,
      "grad_norm": 1.5022962093353271,
      "learning_rate": 5.751251558307824e-06,
      "loss": 1.1068,
      "step": 2215
    },
    {
      "epoch": 0.3115422465907493,
      "grad_norm": 1.5275300741195679,
      "learning_rate": 5.82920709481517e-06,
      "loss": 1.0497,
      "step": 2216
    },
    {
      "epoch": 0.3116828342471531,
      "grad_norm": 1.4267473220825195,
      "learning_rate": 5.907671549481853e-06,
      "loss": 1.2288,
      "step": 2217
    },
    {
      "epoch": 0.3118234219035569,
      "grad_norm": 1.5731414556503296,
      "learning_rate": 5.98664449827e-06,
      "loss": 1.1973,
      "step": 2218
    },
    {
      "epoch": 0.3119640095599606,
      "grad_norm": 1.4305567741394043,
      "learning_rate": 6.066125514393661e-06,
      "loss": 1.0401,
      "step": 2219
    },
    {
      "epoch": 0.3121045972163644,
      "grad_norm": 1.6521694660186768,
      "learning_rate": 6.14611416832126e-06,
      "loss": 1.1512,
      "step": 2220
    },
    {
      "epoch": 0.31224518487276814,
      "grad_norm": 1.3683497905731201,
      "learning_rate": 6.226610027777779e-06,
      "loss": 1.0566,
      "step": 2221
    },
    {
      "epoch": 0.31238577252917193,
      "grad_norm": 1.334836721420288,
      "learning_rate": 6.3076126577472195e-06,
      "loss": 0.9241,
      "step": 2222
    },
    {
      "epoch": 0.3125263601855757,
      "grad_norm": 1.3984036445617676,
      "learning_rate": 6.389121620474836e-06,
      "loss": 0.9636,
      "step": 2223
    },
    {
      "epoch": 0.31266694784197946,
      "grad_norm": 1.69764244556427,
      "learning_rate": 6.471136475469575e-06,
      "loss": 1.158,
      "step": 2224
    },
    {
      "epoch": 0.31280753549838325,
      "grad_norm": 1.3825101852416992,
      "learning_rate": 6.553656779506456e-06,
      "loss": 1.0799,
      "step": 2225
    },
    {
      "epoch": 0.312948123154787,
      "grad_norm": 1.546836495399475,
      "learning_rate": 6.63668208662892e-06,
      "loss": 1.0916,
      "step": 2226
    },
    {
      "epoch": 0.3130887108111908,
      "grad_norm": 1.504961371421814,
      "learning_rate": 6.720211948151334e-06,
      "loss": 0.9813,
      "step": 2227
    },
    {
      "epoch": 0.31322929846759456,
      "grad_norm": 1.5054129362106323,
      "learning_rate": 6.804245912661222e-06,
      "loss": 1.23,
      "step": 2228
    },
    {
      "epoch": 0.3133698861239983,
      "grad_norm": 1.4661927223205566,
      "learning_rate": 6.88878352602198e-06,
      "loss": 1.0572,
      "step": 2229
    },
    {
      "epoch": 0.3135104737804021,
      "grad_norm": 1.5271692276000977,
      "learning_rate": 6.973824331375056e-06,
      "loss": 1.1045,
      "step": 2230
    },
    {
      "epoch": 0.3136510614368058,
      "grad_norm": 1.2110599279403687,
      "learning_rate": 7.059367869142663e-06,
      "loss": 1.2416,
      "step": 2231
    },
    {
      "epoch": 0.3137916490932096,
      "grad_norm": 1.4872448444366455,
      "learning_rate": 7.145413677030044e-06,
      "loss": 1.1224,
      "step": 2232
    },
    {
      "epoch": 0.3139322367496134,
      "grad_norm": 1.5572465658187866,
      "learning_rate": 7.231961290028133e-06,
      "loss": 1.1053,
      "step": 2233
    },
    {
      "epoch": 0.31407282440601714,
      "grad_norm": 1.5224570035934448,
      "learning_rate": 7.31901024041598e-06,
      "loss": 1.022,
      "step": 2234
    },
    {
      "epoch": 0.3142134120624209,
      "grad_norm": 1.6546130180358887,
      "learning_rate": 7.406560057763334e-06,
      "loss": 1.1547,
      "step": 2235
    },
    {
      "epoch": 0.31435399971882466,
      "grad_norm": 1.5701643228530884,
      "learning_rate": 7.494610268933111e-06,
      "loss": 1.1964,
      "step": 2236
    },
    {
      "epoch": 0.31449458737522845,
      "grad_norm": 1.497252345085144,
      "learning_rate": 7.583160398084044e-06,
      "loss": 1.1127,
      "step": 2237
    },
    {
      "epoch": 0.31463517503163224,
      "grad_norm": 1.5550978183746338,
      "learning_rate": 7.67220996667316e-06,
      "loss": 1.2186,
      "step": 2238
    },
    {
      "epoch": 0.314775762688036,
      "grad_norm": 1.9865341186523438,
      "learning_rate": 7.761758493458482e-06,
      "loss": 0.9927,
      "step": 2239
    },
    {
      "epoch": 0.31491635034443977,
      "grad_norm": 1.5144331455230713,
      "learning_rate": 7.851805494501474e-06,
      "loss": 1.1337,
      "step": 2240
    },
    {
      "epoch": 0.3150569380008435,
      "grad_norm": 1.4463096857070923,
      "learning_rate": 7.942350483169814e-06,
      "loss": 1.0404,
      "step": 2241
    },
    {
      "epoch": 0.3151975256572473,
      "grad_norm": 1.5359266996383667,
      "learning_rate": 8.03339297013992e-06,
      "loss": 1.1162,
      "step": 2242
    },
    {
      "epoch": 0.3153381133136511,
      "grad_norm": 1.7842427492141724,
      "learning_rate": 8.124932463399648e-06,
      "loss": 0.9842,
      "step": 2243
    },
    {
      "epoch": 0.3154787009700548,
      "grad_norm": 1.7317911386489868,
      "learning_rate": 8.216968468250886e-06,
      "loss": 1.0105,
      "step": 2244
    },
    {
      "epoch": 0.3156192886264586,
      "grad_norm": 1.679977297782898,
      "learning_rate": 8.30950048731235e-06,
      "loss": 1.2097,
      "step": 2245
    },
    {
      "epoch": 0.31575987628286234,
      "grad_norm": 1.634305715560913,
      "learning_rate": 8.402528020522127e-06,
      "loss": 1.1803,
      "step": 2246
    },
    {
      "epoch": 0.31590046393926613,
      "grad_norm": 1.8168134689331055,
      "learning_rate": 8.496050565140457e-06,
      "loss": 0.978,
      "step": 2247
    },
    {
      "epoch": 0.3160410515956699,
      "grad_norm": 1.5968728065490723,
      "learning_rate": 8.590067615752428e-06,
      "loss": 1.0797,
      "step": 2248
    },
    {
      "epoch": 0.31618163925207365,
      "grad_norm": 1.4383652210235596,
      "learning_rate": 8.684578664270793e-06,
      "loss": 1.0999,
      "step": 2249
    },
    {
      "epoch": 0.31632222690847744,
      "grad_norm": 1.65225350856781,
      "learning_rate": 8.779583199938524e-06,
      "loss": 1.0242,
      "step": 2250
    },
    {
      "epoch": 0.3164628145648812,
      "grad_norm": 1.4342926740646362,
      "learning_rate": 8.875080709331795e-06,
      "loss": 1.2168,
      "step": 2251
    },
    {
      "epoch": 0.31660340222128497,
      "grad_norm": 1.8665882349014282,
      "learning_rate": 8.971070676362558e-06,
      "loss": 1.0841,
      "step": 2252
    },
    {
      "epoch": 0.31674398987768876,
      "grad_norm": 1.506164312362671,
      "learning_rate": 9.067552582281491e-06,
      "loss": 1.1334,
      "step": 2253
    },
    {
      "epoch": 0.3168845775340925,
      "grad_norm": 1.5472074747085571,
      "learning_rate": 9.164525905680699e-06,
      "loss": 0.9976,
      "step": 2254
    },
    {
      "epoch": 0.3170251651904963,
      "grad_norm": 1.3759838342666626,
      "learning_rate": 9.261990122496588e-06,
      "loss": 1.1492,
      "step": 2255
    },
    {
      "epoch": 0.3171657528469,
      "grad_norm": 1.594873070716858,
      "learning_rate": 9.359944706012646e-06,
      "loss": 1.2596,
      "step": 2256
    },
    {
      "epoch": 0.3173063405033038,
      "grad_norm": 1.480833888053894,
      "learning_rate": 9.458389126862355e-06,
      "loss": 1.2391,
      "step": 2257
    },
    {
      "epoch": 0.3174469281597076,
      "grad_norm": 1.4435936212539673,
      "learning_rate": 9.557322853031991e-06,
      "loss": 1.1073,
      "step": 2258
    },
    {
      "epoch": 0.31758751581611133,
      "grad_norm": 1.5617893934249878,
      "learning_rate": 9.656745349863539e-06,
      "loss": 0.9653,
      "step": 2259
    },
    {
      "epoch": 0.3177281034725151,
      "grad_norm": 1.4536830186843872,
      "learning_rate": 9.756656080057547e-06,
      "loss": 1.1221,
      "step": 2260
    },
    {
      "epoch": 0.31786869112891886,
      "grad_norm": 1.5346980094909668,
      "learning_rate": 9.857054503676077e-06,
      "loss": 0.985,
      "step": 2261
    },
    {
      "epoch": 0.31800927878532265,
      "grad_norm": 1.4085376262664795,
      "learning_rate": 9.9579400781456e-06,
      "loss": 1.2497,
      "step": 2262
    },
    {
      "epoch": 0.31814986644172644,
      "grad_norm": 1.7065951824188232,
      "learning_rate": 1.0059312258259856e-05,
      "loss": 1.1867,
      "step": 2263
    },
    {
      "epoch": 0.31829045409813017,
      "grad_norm": 1.4536925554275513,
      "learning_rate": 1.0161170496182981e-05,
      "loss": 1.17,
      "step": 2264
    },
    {
      "epoch": 0.31843104175453396,
      "grad_norm": 1.506919264793396,
      "learning_rate": 1.0263514241452255e-05,
      "loss": 1.09,
      "step": 2265
    },
    {
      "epoch": 0.3185716294109377,
      "grad_norm": 1.260092854499817,
      "learning_rate": 1.0366342940981255e-05,
      "loss": 0.9686,
      "step": 2266
    },
    {
      "epoch": 0.3187122170673415,
      "grad_norm": 1.380090355873108,
      "learning_rate": 1.0469656039062681e-05,
      "loss": 1.1334,
      "step": 2267
    },
    {
      "epoch": 0.3188528047237453,
      "grad_norm": 1.5744314193725586,
      "learning_rate": 1.0573452977371535e-05,
      "loss": 1.159,
      "step": 2268
    },
    {
      "epoch": 0.318993392380149,
      "grad_norm": 1.8518835306167603,
      "learning_rate": 1.0677733194967977e-05,
      "loss": 1.0681,
      "step": 2269
    },
    {
      "epoch": 0.3191339800365528,
      "grad_norm": 1.2564624547958374,
      "learning_rate": 1.0782496128300478e-05,
      "loss": 1.0016,
      "step": 2270
    },
    {
      "epoch": 0.31927456769295653,
      "grad_norm": 1.5382548570632935,
      "learning_rate": 1.0887741211208768e-05,
      "loss": 1.2086,
      "step": 2271
    },
    {
      "epoch": 0.3194151553493603,
      "grad_norm": 1.546035647392273,
      "learning_rate": 1.099346787492701e-05,
      "loss": 1.2433,
      "step": 2272
    },
    {
      "epoch": 0.3195557430057641,
      "grad_norm": 1.4775725603103638,
      "learning_rate": 1.1099675548086707e-05,
      "loss": 1.2518,
      "step": 2273
    },
    {
      "epoch": 0.31969633066216785,
      "grad_norm": 1.5403450727462769,
      "learning_rate": 1.1206363656719998e-05,
      "loss": 0.9665,
      "step": 2274
    },
    {
      "epoch": 0.31983691831857164,
      "grad_norm": 1.2333321571350098,
      "learning_rate": 1.1313531624262553e-05,
      "loss": 1.146,
      "step": 2275
    },
    {
      "epoch": 0.3199775059749754,
      "grad_norm": 1.353036642074585,
      "learning_rate": 1.1421178871556882e-05,
      "loss": 1.1944,
      "step": 2276
    },
    {
      "epoch": 0.32011809363137916,
      "grad_norm": 1.3629494905471802,
      "learning_rate": 1.1529304816855258e-05,
      "loss": 1.1527,
      "step": 2277
    },
    {
      "epoch": 0.32025868128778295,
      "grad_norm": 1.4694194793701172,
      "learning_rate": 1.16379088758231e-05,
      "loss": 0.959,
      "step": 2278
    },
    {
      "epoch": 0.3203992689441867,
      "grad_norm": 1.4279195070266724,
      "learning_rate": 1.1746990461541874e-05,
      "loss": 1.1624,
      "step": 2279
    },
    {
      "epoch": 0.3205398566005905,
      "grad_norm": 1.370681643486023,
      "learning_rate": 1.1856548984512517e-05,
      "loss": 1.2225,
      "step": 2280
    },
    {
      "epoch": 0.3206804442569942,
      "grad_norm": 1.769364833831787,
      "learning_rate": 1.1966583852658374e-05,
      "loss": 1.0911,
      "step": 2281
    },
    {
      "epoch": 0.320821031913398,
      "grad_norm": 1.6444621086120605,
      "learning_rate": 1.2077094471328655e-05,
      "loss": 1.1902,
      "step": 2282
    },
    {
      "epoch": 0.3209616195698018,
      "grad_norm": 1.3369948863983154,
      "learning_rate": 1.2188080243301403e-05,
      "loss": 0.9263,
      "step": 2283
    },
    {
      "epoch": 0.3211022072262055,
      "grad_norm": 1.5498321056365967,
      "learning_rate": 1.2299540568786916e-05,
      "loss": 1.0339,
      "step": 2284
    },
    {
      "epoch": 0.3212427948826093,
      "grad_norm": 1.7421201467514038,
      "learning_rate": 1.2411474845430871e-05,
      "loss": 1.0401,
      "step": 2285
    },
    {
      "epoch": 0.32138338253901305,
      "grad_norm": 1.7879258394241333,
      "learning_rate": 1.2523882468317616e-05,
      "loss": 0.9899,
      "step": 2286
    },
    {
      "epoch": 0.32152397019541684,
      "grad_norm": 1.4350789785385132,
      "learning_rate": 1.2636762829973414e-05,
      "loss": 1.0442,
      "step": 2287
    },
    {
      "epoch": 0.32166455785182063,
      "grad_norm": 1.5984934568405151,
      "learning_rate": 1.2750115320369837e-05,
      "loss": 1.088,
      "step": 2288
    },
    {
      "epoch": 0.32180514550822437,
      "grad_norm": 1.5120837688446045,
      "learning_rate": 1.2863939326926876e-05,
      "loss": 1.1737,
      "step": 2289
    },
    {
      "epoch": 0.32194573316462816,
      "grad_norm": 1.7485439777374268,
      "learning_rate": 1.297823423451645e-05,
      "loss": 1.115,
      "step": 2290
    },
    {
      "epoch": 0.3220863208210319,
      "grad_norm": 1.398285984992981,
      "learning_rate": 1.3092999425465536e-05,
      "loss": 1.1581,
      "step": 2291
    },
    {
      "epoch": 0.3222269084774357,
      "grad_norm": 1.6197527647018433,
      "learning_rate": 1.3208234279559673e-05,
      "loss": 1.1371,
      "step": 2292
    },
    {
      "epoch": 0.32236749613383947,
      "grad_norm": 1.4913413524627686,
      "learning_rate": 1.3323938174046202e-05,
      "loss": 1.112,
      "step": 2293
    },
    {
      "epoch": 0.3225080837902432,
      "grad_norm": 1.3870015144348145,
      "learning_rate": 1.3440110483637735e-05,
      "loss": 1.1318,
      "step": 2294
    },
    {
      "epoch": 0.322648671446647,
      "grad_norm": 1.5744941234588623,
      "learning_rate": 1.3556750580515376e-05,
      "loss": 0.9505,
      "step": 2295
    },
    {
      "epoch": 0.32278925910305073,
      "grad_norm": 1.3954246044158936,
      "learning_rate": 1.3673857834332315e-05,
      "loss": 0.9199,
      "step": 2296
    },
    {
      "epoch": 0.3229298467594545,
      "grad_norm": 1.6637446880340576,
      "learning_rate": 1.3791431612217043e-05,
      "loss": 1.2027,
      "step": 2297
    },
    {
      "epoch": 0.3230704344158583,
      "grad_norm": 1.768285870552063,
      "learning_rate": 1.3909471278776953e-05,
      "loss": 1.1354,
      "step": 2298
    },
    {
      "epoch": 0.32321102207226204,
      "grad_norm": 1.7372947931289673,
      "learning_rate": 1.4027976196101556e-05,
      "loss": 0.9705,
      "step": 2299
    },
    {
      "epoch": 0.32335160972866583,
      "grad_norm": 1.5063644647598267,
      "learning_rate": 1.4146945723766202e-05,
      "loss": 0.9833,
      "step": 2300
    },
    {
      "epoch": 0.32349219738506957,
      "grad_norm": 1.5225610733032227,
      "learning_rate": 1.4266379218835269e-05,
      "loss": 0.984,
      "step": 2301
    },
    {
      "epoch": 0.32363278504147336,
      "grad_norm": 1.6111737489700317,
      "learning_rate": 1.4386276035865786e-05,
      "loss": 1.0411,
      "step": 2302
    },
    {
      "epoch": 0.32377337269787715,
      "grad_norm": 2.144036054611206,
      "learning_rate": 1.4506635526910962e-05,
      "loss": 1.0549,
      "step": 2303
    },
    {
      "epoch": 0.3239139603542809,
      "grad_norm": 1.3386446237564087,
      "learning_rate": 1.462745704152354e-05,
      "loss": 1.1851,
      "step": 2304
    },
    {
      "epoch": 0.3240545480106847,
      "grad_norm": 1.3200876712799072,
      "learning_rate": 1.4748739926759481e-05,
      "loss": 1.0901,
      "step": 2305
    },
    {
      "epoch": 0.3241951356670884,
      "grad_norm": 1.4012330770492554,
      "learning_rate": 1.4870483527181278e-05,
      "loss": 1.1412,
      "step": 2306
    },
    {
      "epoch": 0.3243357233234922,
      "grad_norm": 1.6272919178009033,
      "learning_rate": 1.4992687184861753e-05,
      "loss": 1.0915,
      "step": 2307
    },
    {
      "epoch": 0.324476310979896,
      "grad_norm": 1.491696834564209,
      "learning_rate": 1.5115350239387394e-05,
      "loss": 1.1212,
      "step": 2308
    },
    {
      "epoch": 0.3246168986362997,
      "grad_norm": 1.5090833902359009,
      "learning_rate": 1.5238472027862093e-05,
      "loss": 1.2478,
      "step": 2309
    },
    {
      "epoch": 0.3247574862927035,
      "grad_norm": 1.4606928825378418,
      "learning_rate": 1.536205188491058e-05,
      "loss": 0.9366,
      "step": 2310
    },
    {
      "epoch": 0.32489807394910725,
      "grad_norm": 1.5856558084487915,
      "learning_rate": 1.5486089142682135e-05,
      "loss": 1.0018,
      "step": 2311
    },
    {
      "epoch": 0.32503866160551104,
      "grad_norm": 1.6098039150238037,
      "learning_rate": 1.5610583130854118e-05,
      "loss": 1.0025,
      "step": 2312
    },
    {
      "epoch": 0.3251792492619148,
      "grad_norm": 1.4660744667053223,
      "learning_rate": 1.5735533176635666e-05,
      "loss": 0.9527,
      "step": 2313
    },
    {
      "epoch": 0.32531983691831856,
      "grad_norm": 1.7182506322860718,
      "learning_rate": 1.5860938604771234e-05,
      "loss": 0.8222,
      "step": 2314
    },
    {
      "epoch": 0.32546042457472235,
      "grad_norm": 1.6362828016281128,
      "learning_rate": 1.5986798737544363e-05,
      "loss": 1.0384,
      "step": 2315
    },
    {
      "epoch": 0.3256010122311261,
      "grad_norm": 1.2885829210281372,
      "learning_rate": 1.61131128947812e-05,
      "loss": 1.1797,
      "step": 2316
    },
    {
      "epoch": 0.3257415998875299,
      "grad_norm": 1.4275461435317993,
      "learning_rate": 1.6239880393854344e-05,
      "loss": 1.276,
      "step": 2317
    },
    {
      "epoch": 0.32588218754393367,
      "grad_norm": 1.4545388221740723,
      "learning_rate": 1.6367100549686332e-05,
      "loss": 1.1703,
      "step": 2318
    },
    {
      "epoch": 0.3260227752003374,
      "grad_norm": 1.463460087776184,
      "learning_rate": 1.6494772674753568e-05,
      "loss": 1.0728,
      "step": 2319
    },
    {
      "epoch": 0.3261633628567412,
      "grad_norm": 1.6955108642578125,
      "learning_rate": 1.6622896079089813e-05,
      "loss": 0.9828,
      "step": 2320
    },
    {
      "epoch": 0.3263039505131449,
      "grad_norm": 1.5239710807800293,
      "learning_rate": 1.6751470070290133e-05,
      "loss": 0.9955,
      "step": 2321
    },
    {
      "epoch": 0.3264445381695487,
      "grad_norm": 1.2959400415420532,
      "learning_rate": 1.688049395351441e-05,
      "loss": 1.3148,
      "step": 2322
    },
    {
      "epoch": 0.3265851258259525,
      "grad_norm": 1.4270635843276978,
      "learning_rate": 1.7009967031491338e-05,
      "loss": 1.1632,
      "step": 2323
    },
    {
      "epoch": 0.32672571348235624,
      "grad_norm": 1.5348272323608398,
      "learning_rate": 1.7139888604521982e-05,
      "loss": 1.0418,
      "step": 2324
    },
    {
      "epoch": 0.32686630113876003,
      "grad_norm": 1.352413535118103,
      "learning_rate": 1.7270257970483695e-05,
      "loss": 1.3059,
      "step": 2325
    },
    {
      "epoch": 0.32700688879516376,
      "grad_norm": 1.5301002264022827,
      "learning_rate": 1.7401074424833807e-05,
      "loss": 0.9793,
      "step": 2326
    },
    {
      "epoch": 0.32714747645156755,
      "grad_norm": 1.4340777397155762,
      "learning_rate": 1.7532337260613596e-05,
      "loss": 1.1446,
      "step": 2327
    },
    {
      "epoch": 0.32728806410797134,
      "grad_norm": 1.5998631715774536,
      "learning_rate": 1.7664045768451887e-05,
      "loss": 1.1759,
      "step": 2328
    },
    {
      "epoch": 0.3274286517643751,
      "grad_norm": 1.340337872505188,
      "learning_rate": 1.779619923656912e-05,
      "loss": 1.0637,
      "step": 2329
    },
    {
      "epoch": 0.32756923942077887,
      "grad_norm": 1.6466315984725952,
      "learning_rate": 1.7928796950780957e-05,
      "loss": 1.0186,
      "step": 2330
    },
    {
      "epoch": 0.3277098270771826,
      "grad_norm": 1.523271918296814,
      "learning_rate": 1.8061838194502367e-05,
      "loss": 1.0571,
      "step": 2331
    },
    {
      "epoch": 0.3278504147335864,
      "grad_norm": 1.4927911758422852,
      "learning_rate": 1.8195322248751312e-05,
      "loss": 0.983,
      "step": 2332
    },
    {
      "epoch": 0.3279910023899902,
      "grad_norm": 1.4217561483383179,
      "learning_rate": 1.8329248392152797e-05,
      "loss": 1.0647,
      "step": 2333
    },
    {
      "epoch": 0.3281315900463939,
      "grad_norm": 1.316707730293274,
      "learning_rate": 1.846361590094261e-05,
      "loss": 1.1459,
      "step": 2334
    },
    {
      "epoch": 0.3282721777027977,
      "grad_norm": 1.3970845937728882,
      "learning_rate": 1.8598424048971386e-05,
      "loss": 1.1635,
      "step": 2335
    },
    {
      "epoch": 0.32841276535920144,
      "grad_norm": 1.411522388458252,
      "learning_rate": 1.8733672107708354e-05,
      "loss": 1.0915,
      "step": 2336
    },
    {
      "epoch": 0.32855335301560523,
      "grad_norm": 1.575118899345398,
      "learning_rate": 1.8869359346245517e-05,
      "loss": 1.1043,
      "step": 2337
    },
    {
      "epoch": 0.328693940672009,
      "grad_norm": 1.6396105289459229,
      "learning_rate": 1.9005485031301308e-05,
      "loss": 1.0091,
      "step": 2338
    },
    {
      "epoch": 0.32883452832841276,
      "grad_norm": 1.5332680940628052,
      "learning_rate": 1.9142048427224823e-05,
      "loss": 1.0859,
      "step": 2339
    },
    {
      "epoch": 0.32897511598481655,
      "grad_norm": 1.64375638961792,
      "learning_rate": 1.9279048795999633e-05,
      "loss": 1.1704,
      "step": 2340
    },
    {
      "epoch": 0.3291157036412203,
      "grad_norm": 1.4292243719100952,
      "learning_rate": 1.9416485397247785e-05,
      "loss": 1.2622,
      "step": 2341
    },
    {
      "epoch": 0.32925629129762407,
      "grad_norm": 1.9127782583236694,
      "learning_rate": 1.955435748823392e-05,
      "loss": 0.9249,
      "step": 2342
    },
    {
      "epoch": 0.32939687895402786,
      "grad_norm": 1.4954016208648682,
      "learning_rate": 1.9692664323869136e-05,
      "loss": 0.8843,
      "step": 2343
    },
    {
      "epoch": 0.3295374666104316,
      "grad_norm": 1.6087110042572021,
      "learning_rate": 1.98314051567151e-05,
      "loss": 1.059,
      "step": 2344
    },
    {
      "epoch": 0.3296780542668354,
      "grad_norm": 1.4285305738449097,
      "learning_rate": 1.9970579236988052e-05,
      "loss": 1.0439,
      "step": 2345
    },
    {
      "epoch": 0.3298186419232391,
      "grad_norm": 1.5899680852890015,
      "learning_rate": 2.0110185812562933e-05,
      "loss": 1.114,
      "step": 2346
    },
    {
      "epoch": 0.3299592295796429,
      "grad_norm": 1.2223150730133057,
      "learning_rate": 2.0250224128977314e-05,
      "loss": 1.361,
      "step": 2347
    },
    {
      "epoch": 0.3300998172360467,
      "grad_norm": 1.495377540588379,
      "learning_rate": 2.0390693429435637e-05,
      "loss": 1.0271,
      "step": 2348
    },
    {
      "epoch": 0.33024040489245043,
      "grad_norm": 1.550046443939209,
      "learning_rate": 2.0531592954813116e-05,
      "loss": 0.938,
      "step": 2349
    },
    {
      "epoch": 0.3303809925488542,
      "grad_norm": 1.3953087329864502,
      "learning_rate": 2.067292194366005e-05,
      "loss": 1.2211,
      "step": 2350
    },
    {
      "epoch": 0.33052158020525796,
      "grad_norm": 1.355339765548706,
      "learning_rate": 2.0814679632205736e-05,
      "loss": 1.1583,
      "step": 2351
    },
    {
      "epoch": 0.33066216786166175,
      "grad_norm": 1.518764853477478,
      "learning_rate": 2.0956865254362768e-05,
      "loss": 1.1266,
      "step": 2352
    },
    {
      "epoch": 0.33080275551806554,
      "grad_norm": 1.4083670377731323,
      "learning_rate": 2.1099478041731023e-05,
      "loss": 1.2368,
      "step": 2353
    },
    {
      "epoch": 0.3309433431744693,
      "grad_norm": 1.5342488288879395,
      "learning_rate": 2.1242517223601965e-05,
      "loss": 1.05,
      "step": 2354
    },
    {
      "epoch": 0.33108393083087306,
      "grad_norm": 1.6187372207641602,
      "learning_rate": 2.138598202696267e-05,
      "loss": 1.0528,
      "step": 2355
    },
    {
      "epoch": 0.3312245184872768,
      "grad_norm": 1.4941320419311523,
      "learning_rate": 2.1529871676500135e-05,
      "loss": 1.1158,
      "step": 2356
    },
    {
      "epoch": 0.3313651061436806,
      "grad_norm": 1.9820646047592163,
      "learning_rate": 2.1674185394605307e-05,
      "loss": 1.1017,
      "step": 2357
    },
    {
      "epoch": 0.3315056938000844,
      "grad_norm": 1.4911271333694458,
      "learning_rate": 2.1818922401377518e-05,
      "loss": 1.2104,
      "step": 2358
    },
    {
      "epoch": 0.3316462814564881,
      "grad_norm": 1.4940606355667114,
      "learning_rate": 2.19640819146284e-05,
      "loss": 1.0405,
      "step": 2359
    },
    {
      "epoch": 0.3317868691128919,
      "grad_norm": 1.719232201576233,
      "learning_rate": 2.210966314988643e-05,
      "loss": 0.9645,
      "step": 2360
    },
    {
      "epoch": 0.33192745676929564,
      "grad_norm": 1.2886637449264526,
      "learning_rate": 2.2255665320400877e-05,
      "loss": 0.9683,
      "step": 2361
    },
    {
      "epoch": 0.3320680444256994,
      "grad_norm": 1.5149534940719604,
      "learning_rate": 2.2402087637146295e-05,
      "loss": 0.9823,
      "step": 2362
    },
    {
      "epoch": 0.3322086320821032,
      "grad_norm": 1.6270469427108765,
      "learning_rate": 2.254892930882664e-05,
      "loss": 1.103,
      "step": 2363
    },
    {
      "epoch": 0.33234921973850695,
      "grad_norm": 1.7192723751068115,
      "learning_rate": 2.2696189541879565e-05,
      "loss": 1.1886,
      "step": 2364
    },
    {
      "epoch": 0.33248980739491074,
      "grad_norm": 1.5245076417922974,
      "learning_rate": 2.2843867540480736e-05,
      "loss": 0.9797,
      "step": 2365
    },
    {
      "epoch": 0.3326303950513145,
      "grad_norm": 1.47898268699646,
      "learning_rate": 2.2991962506548205e-05,
      "loss": 1.1021,
      "step": 2366
    },
    {
      "epoch": 0.33277098270771827,
      "grad_norm": 1.6667265892028809,
      "learning_rate": 2.314047363974653e-05,
      "loss": 1.2312,
      "step": 2367
    },
    {
      "epoch": 0.33291157036412206,
      "grad_norm": 1.6212310791015625,
      "learning_rate": 2.3289400137491334e-05,
      "loss": 1.0239,
      "step": 2368
    },
    {
      "epoch": 0.3330521580205258,
      "grad_norm": 1.4724760055541992,
      "learning_rate": 2.3438741194953407e-05,
      "loss": 0.9975,
      "step": 2369
    },
    {
      "epoch": 0.3331927456769296,
      "grad_norm": 1.474480390548706,
      "learning_rate": 2.358849600506331e-05,
      "loss": 1.0734,
      "step": 2370
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 1.5894185304641724,
      "learning_rate": 2.3738663758515445e-05,
      "loss": 1.0154,
      "step": 2371
    },
    {
      "epoch": 0.3334739209897371,
      "grad_norm": 1.4743986129760742,
      "learning_rate": 2.388924364377273e-05,
      "loss": 1.1581,
      "step": 2372
    },
    {
      "epoch": 0.3336145086461409,
      "grad_norm": 1.7137017250061035,
      "learning_rate": 2.404023484707072e-05,
      "loss": 1.1426,
      "step": 2373
    },
    {
      "epoch": 0.33375509630254463,
      "grad_norm": 1.2868447303771973,
      "learning_rate": 2.4191636552422226e-05,
      "loss": 1.1152,
      "step": 2374
    },
    {
      "epoch": 0.3338956839589484,
      "grad_norm": 1.3844937086105347,
      "learning_rate": 2.4343447941621523e-05,
      "loss": 1.0863,
      "step": 2375
    },
    {
      "epoch": 0.33403627161535215,
      "grad_norm": 1.459953784942627,
      "learning_rate": 2.4495668194248967e-05,
      "loss": 1.1047,
      "step": 2376
    },
    {
      "epoch": 0.33417685927175594,
      "grad_norm": 1.5539829730987549,
      "learning_rate": 2.464829648767525e-05,
      "loss": 1.1633,
      "step": 2377
    },
    {
      "epoch": 0.33431744692815973,
      "grad_norm": 1.3612070083618164,
      "learning_rate": 2.480133199706599e-05,
      "loss": 1.1886,
      "step": 2378
    },
    {
      "epoch": 0.33445803458456347,
      "grad_norm": 1.5586650371551514,
      "learning_rate": 2.4954773895386107e-05,
      "loss": 1.0055,
      "step": 2379
    },
    {
      "epoch": 0.33459862224096726,
      "grad_norm": 1.541959285736084,
      "learning_rate": 2.510862135340426e-05,
      "loss": 1.0048,
      "step": 2380
    },
    {
      "epoch": 0.334739209897371,
      "grad_norm": 1.6173774003982544,
      "learning_rate": 2.5262873539697486e-05,
      "loss": 1.0387,
      "step": 2381
    },
    {
      "epoch": 0.3348797975537748,
      "grad_norm": 1.6522225141525269,
      "learning_rate": 2.54175296206555e-05,
      "loss": 1.1124,
      "step": 2382
    },
    {
      "epoch": 0.3350203852101786,
      "grad_norm": 1.5080829858779907,
      "learning_rate": 2.5572588760485316e-05,
      "loss": 1.0256,
      "step": 2383
    },
    {
      "epoch": 0.3351609728665823,
      "grad_norm": 1.3646751642227173,
      "learning_rate": 2.572805012121572e-05,
      "loss": 1.0433,
      "step": 2384
    },
    {
      "epoch": 0.3353015605229861,
      "grad_norm": 1.4882711172103882,
      "learning_rate": 2.5883912862701885e-05,
      "loss": 1.0007,
      "step": 2385
    },
    {
      "epoch": 0.33544214817938983,
      "grad_norm": 1.7795170545578003,
      "learning_rate": 2.6040176142629723e-05,
      "loss": 1.0095,
      "step": 2386
    },
    {
      "epoch": 0.3355827358357936,
      "grad_norm": 1.59416663646698,
      "learning_rate": 2.6196839116520678e-05,
      "loss": 1.1357,
      "step": 2387
    },
    {
      "epoch": 0.3357233234921974,
      "grad_norm": 1.5449644327163696,
      "learning_rate": 2.6353900937736043e-05,
      "loss": 1.1156,
      "step": 2388
    },
    {
      "epoch": 0.33586391114860115,
      "grad_norm": 1.4335663318634033,
      "learning_rate": 2.6511360757481797e-05,
      "loss": 1.1743,
      "step": 2389
    },
    {
      "epoch": 0.33600449880500494,
      "grad_norm": 1.673878788948059,
      "learning_rate": 2.6669217724812912e-05,
      "loss": 1.0483,
      "step": 2390
    },
    {
      "epoch": 0.33614508646140867,
      "grad_norm": 2.0321521759033203,
      "learning_rate": 2.6827470986638215e-05,
      "loss": 1.152,
      "step": 2391
    },
    {
      "epoch": 0.33628567411781246,
      "grad_norm": 1.4057587385177612,
      "learning_rate": 2.6986119687724777e-05,
      "loss": 1.0251,
      "step": 2392
    },
    {
      "epoch": 0.33642626177421625,
      "grad_norm": 1.562355637550354,
      "learning_rate": 2.714516297070273e-05,
      "loss": 1.0936,
      "step": 2393
    },
    {
      "epoch": 0.33656684943062,
      "grad_norm": 1.4584121704101562,
      "learning_rate": 2.73045999760697e-05,
      "loss": 1.0935,
      "step": 2394
    },
    {
      "epoch": 0.3367074370870238,
      "grad_norm": 2.0481815338134766,
      "learning_rate": 2.746442984219567e-05,
      "loss": 1.2407,
      "step": 2395
    },
    {
      "epoch": 0.3368480247434275,
      "grad_norm": 1.4508947134017944,
      "learning_rate": 2.7624651705327397e-05,
      "loss": 1.123,
      "step": 2396
    },
    {
      "epoch": 0.3369886123998313,
      "grad_norm": 1.3487021923065186,
      "learning_rate": 2.7785264699593362e-05,
      "loss": 1.0399,
      "step": 2397
    },
    {
      "epoch": 0.3371292000562351,
      "grad_norm": 1.5952852964401245,
      "learning_rate": 2.794626795700811e-05,
      "loss": 0.9817,
      "step": 2398
    },
    {
      "epoch": 0.3372697877126388,
      "grad_norm": 1.5393096208572388,
      "learning_rate": 2.8107660607477314e-05,
      "loss": 1.0895,
      "step": 2399
    },
    {
      "epoch": 0.3374103753690426,
      "grad_norm": 1.5500117540359497,
      "learning_rate": 2.826944177880212e-05,
      "loss": 1.2071,
      "step": 2400
    },
    {
      "epoch": 0.33755096302544635,
      "grad_norm": 1.564423680305481,
      "learning_rate": 2.8431610596684165e-05,
      "loss": 1.0656,
      "step": 2401
    },
    {
      "epoch": 0.33769155068185014,
      "grad_norm": 1.3665696382522583,
      "learning_rate": 2.8594166184730064e-05,
      "loss": 1.1883,
      "step": 2402
    },
    {
      "epoch": 0.33783213833825393,
      "grad_norm": 1.4029409885406494,
      "learning_rate": 2.8757107664456305e-05,
      "loss": 1.1318,
      "step": 2403
    },
    {
      "epoch": 0.33797272599465766,
      "grad_norm": 1.503305196762085,
      "learning_rate": 2.8920434155293873e-05,
      "loss": 1.0945,
      "step": 2404
    },
    {
      "epoch": 0.33811331365106145,
      "grad_norm": 1.417162299156189,
      "learning_rate": 2.908414477459319e-05,
      "loss": 1.3645,
      "step": 2405
    },
    {
      "epoch": 0.3382539013074652,
      "grad_norm": 1.540455937385559,
      "learning_rate": 2.9248238637628645e-05,
      "loss": 1.092,
      "step": 2406
    },
    {
      "epoch": 0.338394488963869,
      "grad_norm": 1.7909809350967407,
      "learning_rate": 2.941271485760356e-05,
      "loss": 1.2518,
      "step": 2407
    },
    {
      "epoch": 0.3385350766202727,
      "grad_norm": 1.4971238374710083,
      "learning_rate": 2.957757254565502e-05,
      "loss": 1.0308,
      "step": 2408
    },
    {
      "epoch": 0.3386756642766765,
      "grad_norm": 1.826697587966919,
      "learning_rate": 2.974281081085831e-05,
      "loss": 1.1983,
      "step": 2409
    },
    {
      "epoch": 0.3388162519330803,
      "grad_norm": 1.530930757522583,
      "learning_rate": 2.9908428760232267e-05,
      "loss": 1.073,
      "step": 2410
    },
    {
      "epoch": 0.338956839589484,
      "grad_norm": 1.4239702224731445,
      "learning_rate": 3.0074425498743695e-05,
      "loss": 1.14,
      "step": 2411
    },
    {
      "epoch": 0.3390974272458878,
      "grad_norm": 1.4549081325531006,
      "learning_rate": 3.0240800129312485e-05,
      "loss": 0.9462,
      "step": 2412
    },
    {
      "epoch": 0.33923801490229155,
      "grad_norm": 1.49067223072052,
      "learning_rate": 3.0407551752816076e-05,
      "loss": 1.0657,
      "step": 2413
    },
    {
      "epoch": 0.33937860255869534,
      "grad_norm": 1.874159574508667,
      "learning_rate": 3.057467946809477e-05,
      "loss": 0.9128,
      "step": 2414
    },
    {
      "epoch": 0.33951919021509913,
      "grad_norm": 1.6167099475860596,
      "learning_rate": 3.074218237195633e-05,
      "loss": 0.9067,
      "step": 2415
    },
    {
      "epoch": 0.33965977787150287,
      "grad_norm": 1.4912806749343872,
      "learning_rate": 3.091005955918099e-05,
      "loss": 1.2298,
      "step": 2416
    },
    {
      "epoch": 0.33980036552790666,
      "grad_norm": 1.5594364404678345,
      "learning_rate": 3.107831012252609e-05,
      "loss": 1.0199,
      "step": 2417
    },
    {
      "epoch": 0.3399409531843104,
      "grad_norm": 1.7069963216781616,
      "learning_rate": 3.124693315273133e-05,
      "loss": 1.0515,
      "step": 2418
    },
    {
      "epoch": 0.3400815408407142,
      "grad_norm": 1.4183577299118042,
      "learning_rate": 3.1415927738523574e-05,
      "loss": 1.05,
      "step": 2419
    },
    {
      "epoch": 0.34022212849711797,
      "grad_norm": 1.4474772214889526,
      "learning_rate": 3.1585292966621506e-05,
      "loss": 1.0321,
      "step": 2420
    },
    {
      "epoch": 0.3403627161535217,
      "grad_norm": 1.5113989114761353,
      "learning_rate": 3.1755027921741e-05,
      "loss": 1.0553,
      "step": 2421
    },
    {
      "epoch": 0.3405033038099255,
      "grad_norm": 1.7249720096588135,
      "learning_rate": 3.1925131686599684e-05,
      "loss": 1.1238,
      "step": 2422
    },
    {
      "epoch": 0.34064389146632923,
      "grad_norm": 1.8996272087097168,
      "learning_rate": 3.209560334192228e-05,
      "loss": 1.177,
      "step": 2423
    },
    {
      "epoch": 0.340784479122733,
      "grad_norm": 1.3720005750656128,
      "learning_rate": 3.2266441966445035e-05,
      "loss": 1.1131,
      "step": 2424
    },
    {
      "epoch": 0.3409250667791368,
      "grad_norm": 1.4634134769439697,
      "learning_rate": 3.2437646636921304e-05,
      "loss": 1.1161,
      "step": 2425
    },
    {
      "epoch": 0.34106565443554054,
      "grad_norm": 1.4345488548278809,
      "learning_rate": 3.260921642812613e-05,
      "loss": 0.9998,
      "step": 2426
    },
    {
      "epoch": 0.34120624209194433,
      "grad_norm": 1.3437786102294922,
      "learning_rate": 3.278115041286147e-05,
      "loss": 1.1513,
      "step": 2427
    },
    {
      "epoch": 0.34134682974834807,
      "grad_norm": 1.5653249025344849,
      "learning_rate": 3.295344766196088e-05,
      "loss": 1.0626,
      "step": 2428
    },
    {
      "epoch": 0.34148741740475186,
      "grad_norm": 1.4749289751052856,
      "learning_rate": 3.312610724429499e-05,
      "loss": 1.0383,
      "step": 2429
    },
    {
      "epoch": 0.34162800506115565,
      "grad_norm": 1.614280104637146,
      "learning_rate": 3.3299128226776223e-05,
      "loss": 1.0549,
      "step": 2430
    },
    {
      "epoch": 0.3417685927175594,
      "grad_norm": 1.4573554992675781,
      "learning_rate": 3.347250967436397e-05,
      "loss": 0.9804,
      "step": 2431
    },
    {
      "epoch": 0.3419091803739632,
      "grad_norm": 1.6599959135055542,
      "learning_rate": 3.3646250650069435e-05,
      "loss": 1.1836,
      "step": 2432
    },
    {
      "epoch": 0.3420497680303669,
      "grad_norm": 1.562712550163269,
      "learning_rate": 3.3820350214961026e-05,
      "loss": 1.0066,
      "step": 2433
    },
    {
      "epoch": 0.3421903556867707,
      "grad_norm": 1.5676110982894897,
      "learning_rate": 3.399480742816923e-05,
      "loss": 1.0804,
      "step": 2434
    },
    {
      "epoch": 0.3423309433431745,
      "grad_norm": 1.3571357727050781,
      "learning_rate": 3.416962134689174e-05,
      "loss": 1.1824,
      "step": 2435
    },
    {
      "epoch": 0.3424715309995782,
      "grad_norm": 1.715049386024475,
      "learning_rate": 3.4344791026398404e-05,
      "loss": 1.2131,
      "step": 2436
    },
    {
      "epoch": 0.342612118655982,
      "grad_norm": 1.4518100023269653,
      "learning_rate": 3.4520315520036627e-05,
      "loss": 1.2068,
      "step": 2437
    },
    {
      "epoch": 0.34275270631238575,
      "grad_norm": 1.6421300172805786,
      "learning_rate": 3.46961938792363e-05,
      "loss": 1.0397,
      "step": 2438
    },
    {
      "epoch": 0.34289329396878954,
      "grad_norm": 1.3055039644241333,
      "learning_rate": 3.4872425153514886e-05,
      "loss": 1.3051,
      "step": 2439
    },
    {
      "epoch": 0.3430338816251933,
      "grad_norm": 1.4621165990829468,
      "learning_rate": 3.504900839048266e-05,
      "loss": 1.131,
      "step": 2440
    },
    {
      "epoch": 0.34317446928159706,
      "grad_norm": 1.4566413164138794,
      "learning_rate": 3.5225942635847765e-05,
      "loss": 1.1083,
      "step": 2441
    },
    {
      "epoch": 0.34331505693800085,
      "grad_norm": 1.5678629875183105,
      "learning_rate": 3.5403226933421585e-05,
      "loss": 1.1331,
      "step": 2442
    },
    {
      "epoch": 0.3434556445944046,
      "grad_norm": 1.6727914810180664,
      "learning_rate": 3.5580860325123575e-05,
      "loss": 1.1419,
      "step": 2443
    },
    {
      "epoch": 0.3435962322508084,
      "grad_norm": 1.6576087474822998,
      "learning_rate": 3.57588418509867e-05,
      "loss": 0.9238,
      "step": 2444
    },
    {
      "epoch": 0.34373681990721217,
      "grad_norm": 1.5231664180755615,
      "learning_rate": 3.5937170549162444e-05,
      "loss": 1.0182,
      "step": 2445
    },
    {
      "epoch": 0.3438774075636159,
      "grad_norm": 1.315199375152588,
      "learning_rate": 3.611584545592629e-05,
      "loss": 1.203,
      "step": 2446
    },
    {
      "epoch": 0.3440179952200197,
      "grad_norm": 1.7076830863952637,
      "learning_rate": 3.629486560568258e-05,
      "loss": 0.8852,
      "step": 2447
    },
    {
      "epoch": 0.3441585828764234,
      "grad_norm": 1.4460480213165283,
      "learning_rate": 3.647423003096992e-05,
      "loss": 1.0391,
      "step": 2448
    },
    {
      "epoch": 0.3442991705328272,
      "grad_norm": 1.7489732503890991,
      "learning_rate": 3.665393776246634e-05,
      "loss": 0.9945,
      "step": 2449
    },
    {
      "epoch": 0.344439758189231,
      "grad_norm": 1.4256293773651123,
      "learning_rate": 3.683398782899474e-05,
      "loss": 0.9465,
      "step": 2450
    },
    {
      "epoch": 0.34458034584563474,
      "grad_norm": 1.4857397079467773,
      "learning_rate": 3.70143792575279e-05,
      "loss": 1.088,
      "step": 2451
    },
    {
      "epoch": 0.34472093350203853,
      "grad_norm": 1.4502534866333008,
      "learning_rate": 3.719511107319354e-05,
      "loss": 1.2101,
      "step": 2452
    },
    {
      "epoch": 0.34486152115844226,
      "grad_norm": 1.4294525384902954,
      "learning_rate": 3.73761822992803e-05,
      "loss": 1.0677,
      "step": 2453
    },
    {
      "epoch": 0.34500210881484605,
      "grad_norm": 1.2757153511047363,
      "learning_rate": 3.7557591957242244e-05,
      "loss": 1.0271,
      "step": 2454
    },
    {
      "epoch": 0.34514269647124984,
      "grad_norm": 1.4106216430664062,
      "learning_rate": 3.773933906670479e-05,
      "loss": 1.2325,
      "step": 2455
    },
    {
      "epoch": 0.3452832841276536,
      "grad_norm": 1.3381438255310059,
      "learning_rate": 3.792142264546926e-05,
      "loss": 1.1584,
      "step": 2456
    },
    {
      "epoch": 0.34542387178405737,
      "grad_norm": 1.6820634603500366,
      "learning_rate": 3.8103841709519064e-05,
      "loss": 1.1718,
      "step": 2457
    },
    {
      "epoch": 0.3455644594404611,
      "grad_norm": 1.962124228477478,
      "learning_rate": 3.828659527302434e-05,
      "loss": 1.0573,
      "step": 2458
    },
    {
      "epoch": 0.3457050470968649,
      "grad_norm": 1.5643898248672485,
      "learning_rate": 3.846968234834759e-05,
      "loss": 1.1439,
      "step": 2459
    },
    {
      "epoch": 0.3458456347532687,
      "grad_norm": 1.3456926345825195,
      "learning_rate": 3.8653101946048856e-05,
      "loss": 1.0125,
      "step": 2460
    },
    {
      "epoch": 0.3459862224096724,
      "grad_norm": 1.4497411251068115,
      "learning_rate": 3.88368530748914e-05,
      "loss": 1.0025,
      "step": 2461
    },
    {
      "epoch": 0.3461268100660762,
      "grad_norm": 1.6087408065795898,
      "learning_rate": 3.9020934741846584e-05,
      "loss": 0.9139,
      "step": 2462
    },
    {
      "epoch": 0.34626739772247994,
      "grad_norm": 1.6611628532409668,
      "learning_rate": 3.9205345952099556e-05,
      "loss": 0.9596,
      "step": 2463
    },
    {
      "epoch": 0.34640798537888373,
      "grad_norm": 1.97322416305542,
      "learning_rate": 3.9390085709054505e-05,
      "loss": 1.0712,
      "step": 2464
    },
    {
      "epoch": 0.3465485730352875,
      "grad_norm": 1.2985522747039795,
      "learning_rate": 3.957515301434025e-05,
      "loss": 1.0423,
      "step": 2465
    },
    {
      "epoch": 0.34668916069169126,
      "grad_norm": 1.4826892614364624,
      "learning_rate": 3.976054686781527e-05,
      "loss": 1.1902,
      "step": 2466
    },
    {
      "epoch": 0.34682974834809505,
      "grad_norm": 1.5473721027374268,
      "learning_rate": 3.9946266267573426e-05,
      "loss": 1.044,
      "step": 2467
    },
    {
      "epoch": 0.3469703360044988,
      "grad_norm": 1.716592788696289,
      "learning_rate": 4.013231020994913e-05,
      "loss": 1.2394,
      "step": 2468
    },
    {
      "epoch": 0.34711092366090257,
      "grad_norm": 1.406256914138794,
      "learning_rate": 4.031867768952314e-05,
      "loss": 1.0849,
      "step": 2469
    },
    {
      "epoch": 0.34725151131730636,
      "grad_norm": 1.6054375171661377,
      "learning_rate": 4.050536769912763e-05,
      "loss": 1.1029,
      "step": 2470
    },
    {
      "epoch": 0.3473920989737101,
      "grad_norm": 1.3714641332626343,
      "learning_rate": 4.06923792298515e-05,
      "loss": 1.1757,
      "step": 2471
    },
    {
      "epoch": 0.3475326866301139,
      "grad_norm": 1.6408666372299194,
      "learning_rate": 4.0879711271046536e-05,
      "loss": 1.0641,
      "step": 2472
    },
    {
      "epoch": 0.3476732742865176,
      "grad_norm": 1.429000735282898,
      "learning_rate": 4.106736281033207e-05,
      "loss": 1.1391,
      "step": 2473
    },
    {
      "epoch": 0.3478138619429214,
      "grad_norm": 1.4571688175201416,
      "learning_rate": 4.1255332833601145e-05,
      "loss": 1.0126,
      "step": 2474
    },
    {
      "epoch": 0.3479544495993252,
      "grad_norm": 1.569838047027588,
      "learning_rate": 4.144362032502517e-05,
      "loss": 1.1348,
      "step": 2475
    },
    {
      "epoch": 0.34809503725572893,
      "grad_norm": 1.365267038345337,
      "learning_rate": 4.163222426706035e-05,
      "loss": 1.1204,
      "step": 2476
    },
    {
      "epoch": 0.3482356249121327,
      "grad_norm": 1.639432668685913,
      "learning_rate": 4.182114364045242e-05,
      "loss": 1.2048,
      "step": 2477
    },
    {
      "epoch": 0.34837621256853646,
      "grad_norm": 1.3537776470184326,
      "learning_rate": 4.20103774242428e-05,
      "loss": 1.0773,
      "step": 2478
    },
    {
      "epoch": 0.34851680022494025,
      "grad_norm": 1.1863987445831299,
      "learning_rate": 4.2199924595773235e-05,
      "loss": 1.0435,
      "step": 2479
    },
    {
      "epoch": 0.34865738788134404,
      "grad_norm": 1.5406614542007446,
      "learning_rate": 4.2389784130692415e-05,
      "loss": 1.2784,
      "step": 2480
    },
    {
      "epoch": 0.3487979755377478,
      "grad_norm": 1.5271189212799072,
      "learning_rate": 4.257995500296055e-05,
      "loss": 1.0205,
      "step": 2481
    },
    {
      "epoch": 0.34893856319415156,
      "grad_norm": 1.5101876258850098,
      "learning_rate": 4.277043618485572e-05,
      "loss": 1.0384,
      "step": 2482
    },
    {
      "epoch": 0.3490791508505553,
      "grad_norm": 1.7611315250396729,
      "learning_rate": 4.2961226646978506e-05,
      "loss": 0.9717,
      "step": 2483
    },
    {
      "epoch": 0.3492197385069591,
      "grad_norm": 1.319441318511963,
      "learning_rate": 4.315232535825856e-05,
      "loss": 1.1234,
      "step": 2484
    },
    {
      "epoch": 0.3493603261633629,
      "grad_norm": 1.6227055788040161,
      "learning_rate": 4.3343731285959375e-05,
      "loss": 1.077,
      "step": 2485
    },
    {
      "epoch": 0.3495009138197666,
      "grad_norm": 1.5977728366851807,
      "learning_rate": 4.35354433956845e-05,
      "loss": 1.1639,
      "step": 2486
    },
    {
      "epoch": 0.3496415014761704,
      "grad_norm": 1.3975216150283813,
      "learning_rate": 4.3727460651382345e-05,
      "loss": 1.2332,
      "step": 2487
    },
    {
      "epoch": 0.34978208913257414,
      "grad_norm": 1.2953253984451294,
      "learning_rate": 4.3919782015352726e-05,
      "loss": 1.1566,
      "step": 2488
    },
    {
      "epoch": 0.3499226767889779,
      "grad_norm": 1.6825931072235107,
      "learning_rate": 4.411240644825172e-05,
      "loss": 1.1227,
      "step": 2489
    },
    {
      "epoch": 0.3500632644453817,
      "grad_norm": 1.8936030864715576,
      "learning_rate": 4.430533290909764e-05,
      "loss": 1.0739,
      "step": 2490
    },
    {
      "epoch": 0.35020385210178545,
      "grad_norm": 1.578672170639038,
      "learning_rate": 4.4498560355276564e-05,
      "loss": 1.0969,
      "step": 2491
    },
    {
      "epoch": 0.35034443975818924,
      "grad_norm": 1.4653433561325073,
      "learning_rate": 4.4692087742547906e-05,
      "loss": 1.257,
      "step": 2492
    },
    {
      "epoch": 0.350485027414593,
      "grad_norm": 1.4573891162872314,
      "learning_rate": 4.488591402505038e-05,
      "loss": 1.1871,
      "step": 2493
    },
    {
      "epoch": 0.35062561507099677,
      "grad_norm": 1.4794780015945435,
      "learning_rate": 4.508003815530717e-05,
      "loss": 1.0954,
      "step": 2494
    },
    {
      "epoch": 0.35076620272740056,
      "grad_norm": 1.570761799812317,
      "learning_rate": 4.52744590842319e-05,
      "loss": 1.107,
      "step": 2495
    },
    {
      "epoch": 0.3509067903838043,
      "grad_norm": 1.5772807598114014,
      "learning_rate": 4.546917576113418e-05,
      "loss": 1.1497,
      "step": 2496
    },
    {
      "epoch": 0.3510473780402081,
      "grad_norm": 1.4165607690811157,
      "learning_rate": 4.566418713372561e-05,
      "loss": 1.0598,
      "step": 2497
    },
    {
      "epoch": 0.3511879656966118,
      "grad_norm": 1.5616035461425781,
      "learning_rate": 4.5859492148124705e-05,
      "loss": 1.2319,
      "step": 2498
    },
    {
      "epoch": 0.3513285533530156,
      "grad_norm": 1.5287199020385742,
      "learning_rate": 4.605508974886356e-05,
      "loss": 1.1435,
      "step": 2499
    },
    {
      "epoch": 0.3514691410094194,
      "grad_norm": 1.4001505374908447,
      "learning_rate": 4.625097887889274e-05,
      "loss": 1.0983,
      "step": 2500
    },
    {
      "epoch": 0.3514691410094194,
      "eval_loss": 1.1457109451293945,
      "eval_runtime": 771.3387,
      "eval_samples_per_second": 16.395,
      "eval_steps_per_second": 8.197,
      "step": 2500
    },
    {
      "epoch": 0.35160972866582313,
      "grad_norm": 1.4507442712783813,
      "learning_rate": 4.644715847958767e-05,
      "loss": 1.2703,
      "step": 2501
    },
    {
      "epoch": 0.3517503163222269,
      "grad_norm": 1.6655983924865723,
      "learning_rate": 4.664362749075352e-05,
      "loss": 0.9379,
      "step": 2502
    },
    {
      "epoch": 0.35189090397863065,
      "grad_norm": 1.8190733194351196,
      "learning_rate": 4.684038485063191e-05,
      "loss": 1.0496,
      "step": 2503
    },
    {
      "epoch": 0.35203149163503444,
      "grad_norm": 1.385292649269104,
      "learning_rate": 4.7037429495905824e-05,
      "loss": 1.1269,
      "step": 2504
    },
    {
      "epoch": 0.35217207929143823,
      "grad_norm": 1.763066053390503,
      "learning_rate": 4.7234760361706075e-05,
      "loss": 1.0584,
      "step": 2505
    },
    {
      "epoch": 0.35231266694784197,
      "grad_norm": 1.4243578910827637,
      "learning_rate": 4.7432376381616116e-05,
      "loss": 1.1571,
      "step": 2506
    },
    {
      "epoch": 0.35245325460424576,
      "grad_norm": 1.418994426727295,
      "learning_rate": 4.7630276487678946e-05,
      "loss": 1.0781,
      "step": 2507
    },
    {
      "epoch": 0.3525938422606495,
      "grad_norm": 1.7240078449249268,
      "learning_rate": 4.7828459610401885e-05,
      "loss": 1.0707,
      "step": 2508
    },
    {
      "epoch": 0.3527344299170533,
      "grad_norm": 1.6017993688583374,
      "learning_rate": 4.802692467876318e-05,
      "loss": 1.136,
      "step": 2509
    },
    {
      "epoch": 0.3528750175734571,
      "grad_norm": 1.4929568767547607,
      "learning_rate": 4.822567062021694e-05,
      "loss": 1.0192,
      "step": 2510
    },
    {
      "epoch": 0.3530156052298608,
      "grad_norm": 1.700010061264038,
      "learning_rate": 4.842469636069954e-05,
      "loss": 1.0165,
      "step": 2511
    },
    {
      "epoch": 0.3531561928862646,
      "grad_norm": 1.5188117027282715,
      "learning_rate": 4.86240008246355e-05,
      "loss": 0.9929,
      "step": 2512
    },
    {
      "epoch": 0.35329678054266833,
      "grad_norm": 1.5344212055206299,
      "learning_rate": 4.882358293494279e-05,
      "loss": 1.0763,
      "step": 2513
    },
    {
      "epoch": 0.3534373681990721,
      "grad_norm": 1.3881734609603882,
      "learning_rate": 4.9023441613039e-05,
      "loss": 1.1745,
      "step": 2514
    },
    {
      "epoch": 0.3535779558554759,
      "grad_norm": 1.5886605978012085,
      "learning_rate": 4.922357577884703e-05,
      "loss": 1.118,
      "step": 2515
    },
    {
      "epoch": 0.35371854351187965,
      "grad_norm": 1.7893551588058472,
      "learning_rate": 4.94239843508012e-05,
      "loss": 1.2688,
      "step": 2516
    },
    {
      "epoch": 0.35385913116828344,
      "grad_norm": 1.5427148342132568,
      "learning_rate": 4.962466624585267e-05,
      "loss": 1.2487,
      "step": 2517
    },
    {
      "epoch": 0.35399971882468717,
      "grad_norm": 1.4825206995010376,
      "learning_rate": 4.982562037947556e-05,
      "loss": 1.222,
      "step": 2518
    },
    {
      "epoch": 0.35414030648109096,
      "grad_norm": 1.342725396156311,
      "learning_rate": 5.002684566567271e-05,
      "loss": 1.0453,
      "step": 2519
    },
    {
      "epoch": 0.35428089413749475,
      "grad_norm": 1.421018362045288,
      "learning_rate": 5.022834101698182e-05,
      "loss": 1.1635,
      "step": 2520
    },
    {
      "epoch": 0.3544214817938985,
      "grad_norm": 1.3579864501953125,
      "learning_rate": 5.043010534448083e-05,
      "loss": 1.017,
      "step": 2521
    },
    {
      "epoch": 0.3545620694503023,
      "grad_norm": 1.5154105424880981,
      "learning_rate": 5.063213755779422e-05,
      "loss": 1.0561,
      "step": 2522
    },
    {
      "epoch": 0.354702657106706,
      "grad_norm": 1.3992061614990234,
      "learning_rate": 5.0834436565098655e-05,
      "loss": 1.0064,
      "step": 2523
    },
    {
      "epoch": 0.3548432447631098,
      "grad_norm": 1.6290247440338135,
      "learning_rate": 5.1037001273129205e-05,
      "loss": 1.0353,
      "step": 2524
    },
    {
      "epoch": 0.3549838324195136,
      "grad_norm": 1.655885934829712,
      "learning_rate": 5.1239830587184814e-05,
      "loss": 0.8956,
      "step": 2525
    },
    {
      "epoch": 0.3551244200759173,
      "grad_norm": 1.6506013870239258,
      "learning_rate": 5.1442923411134544e-05,
      "loss": 1.0831,
      "step": 2526
    },
    {
      "epoch": 0.3552650077323211,
      "grad_norm": 1.4425996541976929,
      "learning_rate": 5.1646278647423305e-05,
      "loss": 1.1718,
      "step": 2527
    },
    {
      "epoch": 0.35540559538872485,
      "grad_norm": 1.447643518447876,
      "learning_rate": 5.184989519707809e-05,
      "loss": 1.1533,
      "step": 2528
    },
    {
      "epoch": 0.35554618304512864,
      "grad_norm": 1.5244598388671875,
      "learning_rate": 5.205377195971357e-05,
      "loss": 1.1362,
      "step": 2529
    },
    {
      "epoch": 0.35568677070153243,
      "grad_norm": 1.4219790697097778,
      "learning_rate": 5.225790783353794e-05,
      "loss": 1.0479,
      "step": 2530
    },
    {
      "epoch": 0.35582735835793616,
      "grad_norm": 1.4828978776931763,
      "learning_rate": 5.246230171535955e-05,
      "loss": 0.9388,
      "step": 2531
    },
    {
      "epoch": 0.35596794601433995,
      "grad_norm": 1.354168176651001,
      "learning_rate": 5.2666952500592126e-05,
      "loss": 1.1174,
      "step": 2532
    },
    {
      "epoch": 0.3561085336707437,
      "grad_norm": 1.295822024345398,
      "learning_rate": 5.287185908326111e-05,
      "loss": 1.0898,
      "step": 2533
    },
    {
      "epoch": 0.3562491213271475,
      "grad_norm": 1.3160591125488281,
      "learning_rate": 5.307702035600949e-05,
      "loss": 1.0749,
      "step": 2534
    },
    {
      "epoch": 0.35638970898355127,
      "grad_norm": 1.7009356021881104,
      "learning_rate": 5.328243521010409e-05,
      "loss": 1.0313,
      "step": 2535
    },
    {
      "epoch": 0.356530296639955,
      "grad_norm": 1.4157969951629639,
      "learning_rate": 5.3488102535441075e-05,
      "loss": 1.0985,
      "step": 2536
    },
    {
      "epoch": 0.3566708842963588,
      "grad_norm": 1.6956347227096558,
      "learning_rate": 5.3694021220552325e-05,
      "loss": 1.0255,
      "step": 2537
    },
    {
      "epoch": 0.3568114719527625,
      "grad_norm": 1.5358480215072632,
      "learning_rate": 5.390019015261121e-05,
      "loss": 1.1684,
      "step": 2538
    },
    {
      "epoch": 0.3569520596091663,
      "grad_norm": 1.3121274709701538,
      "learning_rate": 5.4106608217438924e-05,
      "loss": 1.0868,
      "step": 2539
    },
    {
      "epoch": 0.3570926472655701,
      "grad_norm": 1.280051589012146,
      "learning_rate": 5.431327429951012e-05,
      "loss": 1.1875,
      "step": 2540
    },
    {
      "epoch": 0.35723323492197384,
      "grad_norm": 1.2072852849960327,
      "learning_rate": 5.452018728195918e-05,
      "loss": 1.2558,
      "step": 2541
    },
    {
      "epoch": 0.35737382257837763,
      "grad_norm": 1.6805657148361206,
      "learning_rate": 5.472734604658606e-05,
      "loss": 1.0867,
      "step": 2542
    },
    {
      "epoch": 0.35751441023478137,
      "grad_norm": 1.4239544868469238,
      "learning_rate": 5.4934749473862745e-05,
      "loss": 1.0881,
      "step": 2543
    },
    {
      "epoch": 0.35765499789118516,
      "grad_norm": 1.235000729560852,
      "learning_rate": 5.514239644293877e-05,
      "loss": 1.168,
      "step": 2544
    },
    {
      "epoch": 0.35779558554758895,
      "grad_norm": 1.6497955322265625,
      "learning_rate": 5.535028583164757e-05,
      "loss": 1.0009,
      "step": 2545
    },
    {
      "epoch": 0.3579361732039927,
      "grad_norm": 1.6011333465576172,
      "learning_rate": 5.5558416516512455e-05,
      "loss": 1.0191,
      "step": 2546
    },
    {
      "epoch": 0.35807676086039647,
      "grad_norm": 1.6763039827346802,
      "learning_rate": 5.5766787372752936e-05,
      "loss": 1.1318,
      "step": 2547
    },
    {
      "epoch": 0.3582173485168002,
      "grad_norm": 1.4215991497039795,
      "learning_rate": 5.597539727429042e-05,
      "loss": 1.1381,
      "step": 2548
    },
    {
      "epoch": 0.358357936173204,
      "grad_norm": 1.4474365711212158,
      "learning_rate": 5.618424509375425e-05,
      "loss": 0.9034,
      "step": 2549
    },
    {
      "epoch": 0.3584985238296078,
      "grad_norm": 1.3405014276504517,
      "learning_rate": 5.6393329702488496e-05,
      "loss": 1.0997,
      "step": 2550
    },
    {
      "epoch": 0.3586391114860115,
      "grad_norm": 1.4513859748840332,
      "learning_rate": 5.660264997055712e-05,
      "loss": 0.9969,
      "step": 2551
    },
    {
      "epoch": 0.3587796991424153,
      "grad_norm": 1.6310783624649048,
      "learning_rate": 5.6812204766751e-05,
      "loss": 1.0109,
      "step": 2552
    },
    {
      "epoch": 0.35892028679881904,
      "grad_norm": 1.3873109817504883,
      "learning_rate": 5.702199295859296e-05,
      "loss": 1.2235,
      "step": 2553
    },
    {
      "epoch": 0.35906087445522283,
      "grad_norm": 1.6274387836456299,
      "learning_rate": 5.723201341234512e-05,
      "loss": 1.0997,
      "step": 2554
    },
    {
      "epoch": 0.3592014621116266,
      "grad_norm": 1.6434168815612793,
      "learning_rate": 5.744226499301396e-05,
      "loss": 0.9769,
      "step": 2555
    },
    {
      "epoch": 0.35934204976803036,
      "grad_norm": 1.416793704032898,
      "learning_rate": 5.765274656435733e-05,
      "loss": 1.1818,
      "step": 2556
    },
    {
      "epoch": 0.35948263742443415,
      "grad_norm": 1.506474494934082,
      "learning_rate": 5.78634569888896e-05,
      "loss": 1.0603,
      "step": 2557
    },
    {
      "epoch": 0.3596232250808379,
      "grad_norm": 1.1603097915649414,
      "learning_rate": 5.8074395127888904e-05,
      "loss": 1.0457,
      "step": 2558
    },
    {
      "epoch": 0.3597638127372417,
      "grad_norm": 1.4657173156738281,
      "learning_rate": 5.8285559841402405e-05,
      "loss": 1.2711,
      "step": 2559
    },
    {
      "epoch": 0.35990440039364546,
      "grad_norm": 1.6975390911102295,
      "learning_rate": 5.8496949988253194e-05,
      "loss": 1.0443,
      "step": 2560
    },
    {
      "epoch": 0.3600449880500492,
      "grad_norm": 1.5168673992156982,
      "learning_rate": 5.870856442604551e-05,
      "loss": 1.0382,
      "step": 2561
    },
    {
      "epoch": 0.360185575706453,
      "grad_norm": 1.4871639013290405,
      "learning_rate": 5.892040201117202e-05,
      "loss": 1.191,
      "step": 2562
    },
    {
      "epoch": 0.3603261633628567,
      "grad_norm": 1.7900187969207764,
      "learning_rate": 5.913246159881909e-05,
      "loss": 1.0239,
      "step": 2563
    },
    {
      "epoch": 0.3604667510192605,
      "grad_norm": 1.4008746147155762,
      "learning_rate": 5.9344742042973725e-05,
      "loss": 0.9696,
      "step": 2564
    },
    {
      "epoch": 0.3606073386756643,
      "grad_norm": 1.6451793909072876,
      "learning_rate": 5.955724219642882e-05,
      "loss": 1.153,
      "step": 2565
    },
    {
      "epoch": 0.36074792633206804,
      "grad_norm": 1.4492735862731934,
      "learning_rate": 5.9769960910790455e-05,
      "loss": 1.1517,
      "step": 2566
    },
    {
      "epoch": 0.3608885139884718,
      "grad_norm": 1.4754942655563354,
      "learning_rate": 5.998289703648328e-05,
      "loss": 0.9439,
      "step": 2567
    },
    {
      "epoch": 0.36102910164487556,
      "grad_norm": 1.3372230529785156,
      "learning_rate": 6.019604942275706e-05,
      "loss": 1.1324,
      "step": 2568
    },
    {
      "epoch": 0.36116968930127935,
      "grad_norm": 1.449591040611267,
      "learning_rate": 6.0409416917692864e-05,
      "loss": 1.1285,
      "step": 2569
    },
    {
      "epoch": 0.36131027695768314,
      "grad_norm": 1.5005736351013184,
      "learning_rate": 6.062299836820915e-05,
      "loss": 1.0655,
      "step": 2570
    },
    {
      "epoch": 0.3614508646140869,
      "grad_norm": 1.5008031129837036,
      "learning_rate": 6.0836792620068495e-05,
      "loss": 0.9737,
      "step": 2571
    },
    {
      "epoch": 0.36159145227049067,
      "grad_norm": 1.577317237854004,
      "learning_rate": 6.105079851788287e-05,
      "loss": 1.0613,
      "step": 2572
    },
    {
      "epoch": 0.3617320399268944,
      "grad_norm": 1.5837997198104858,
      "learning_rate": 6.1265014905121e-05,
      "loss": 0.9307,
      "step": 2573
    },
    {
      "epoch": 0.3618726275832982,
      "grad_norm": 1.474953293800354,
      "learning_rate": 6.14794406241137e-05,
      "loss": 1.2642,
      "step": 2574
    },
    {
      "epoch": 0.362013215239702,
      "grad_norm": 1.5723570585250854,
      "learning_rate": 6.169407451606096e-05,
      "loss": 0.9335,
      "step": 2575
    },
    {
      "epoch": 0.3621538028961057,
      "grad_norm": 1.4281582832336426,
      "learning_rate": 6.190891542103711e-05,
      "loss": 1.0571,
      "step": 2576
    },
    {
      "epoch": 0.3622943905525095,
      "grad_norm": 1.525734543800354,
      "learning_rate": 6.21239621779984e-05,
      "loss": 0.9591,
      "step": 2577
    },
    {
      "epoch": 0.36243497820891324,
      "grad_norm": 2.071716785430908,
      "learning_rate": 6.233921362478812e-05,
      "loss": 0.9591,
      "step": 2578
    },
    {
      "epoch": 0.36257556586531703,
      "grad_norm": 1.732648253440857,
      "learning_rate": 6.255466859814383e-05,
      "loss": 1.2994,
      "step": 2579
    },
    {
      "epoch": 0.3627161535217208,
      "grad_norm": 1.3992712497711182,
      "learning_rate": 6.277032593370263e-05,
      "loss": 1.1042,
      "step": 2580
    },
    {
      "epoch": 0.36285674117812455,
      "grad_norm": 1.5451161861419678,
      "learning_rate": 6.298618446600856e-05,
      "loss": 0.9664,
      "step": 2581
    },
    {
      "epoch": 0.36299732883452834,
      "grad_norm": 1.4634068012237549,
      "learning_rate": 6.320224302851793e-05,
      "loss": 1.0748,
      "step": 2582
    },
    {
      "epoch": 0.3631379164909321,
      "grad_norm": 1.5656347274780273,
      "learning_rate": 6.341850045360652e-05,
      "loss": 1.0959,
      "step": 2583
    },
    {
      "epoch": 0.36327850414733587,
      "grad_norm": 1.6228914260864258,
      "learning_rate": 6.363495557257474e-05,
      "loss": 1.0709,
      "step": 2584
    },
    {
      "epoch": 0.36341909180373966,
      "grad_norm": 1.5089365243911743,
      "learning_rate": 6.385160721565529e-05,
      "loss": 1.1139,
      "step": 2585
    },
    {
      "epoch": 0.3635596794601434,
      "grad_norm": 1.390669584274292,
      "learning_rate": 6.406845421201836e-05,
      "loss": 1.1497,
      "step": 2586
    },
    {
      "epoch": 0.3637002671165472,
      "grad_norm": 1.4136580228805542,
      "learning_rate": 6.428549538977885e-05,
      "loss": 1.1325,
      "step": 2587
    },
    {
      "epoch": 0.3638408547729509,
      "grad_norm": 1.3321926593780518,
      "learning_rate": 6.450272957600172e-05,
      "loss": 1.0825,
      "step": 2588
    },
    {
      "epoch": 0.3639814424293547,
      "grad_norm": 1.3990774154663086,
      "learning_rate": 6.472015559670916e-05,
      "loss": 1.1876,
      "step": 2589
    },
    {
      "epoch": 0.3641220300857585,
      "grad_norm": 1.4466227293014526,
      "learning_rate": 6.49377722768868e-05,
      "loss": 1.0425,
      "step": 2590
    },
    {
      "epoch": 0.36426261774216223,
      "grad_norm": 1.3683024644851685,
      "learning_rate": 6.515557844048965e-05,
      "loss": 1.1391,
      "step": 2591
    },
    {
      "epoch": 0.364403205398566,
      "grad_norm": 1.305620551109314,
      "learning_rate": 6.537357291044883e-05,
      "loss": 1.0415,
      "step": 2592
    },
    {
      "epoch": 0.36454379305496976,
      "grad_norm": 1.520009160041809,
      "learning_rate": 6.559175450867771e-05,
      "loss": 0.9182,
      "step": 2593
    },
    {
      "epoch": 0.36468438071137355,
      "grad_norm": 1.4348278045654297,
      "learning_rate": 6.58101220560786e-05,
      "loss": 1.0984,
      "step": 2594
    },
    {
      "epoch": 0.36482496836777734,
      "grad_norm": 1.627060055732727,
      "learning_rate": 6.602867437254871e-05,
      "loss": 1.0077,
      "step": 2595
    },
    {
      "epoch": 0.36496555602418107,
      "grad_norm": 1.508205533027649,
      "learning_rate": 6.624741027698676e-05,
      "loss": 1.0647,
      "step": 2596
    },
    {
      "epoch": 0.36510614368058486,
      "grad_norm": 1.674864411354065,
      "learning_rate": 6.646632858729925e-05,
      "loss": 1.27,
      "step": 2597
    },
    {
      "epoch": 0.3652467313369886,
      "grad_norm": 1.5784465074539185,
      "learning_rate": 6.66854281204072e-05,
      "loss": 1.1944,
      "step": 2598
    },
    {
      "epoch": 0.3653873189933924,
      "grad_norm": 1.4915945529937744,
      "learning_rate": 6.690470769225196e-05,
      "loss": 1.1218,
      "step": 2599
    },
    {
      "epoch": 0.3655279066497962,
      "grad_norm": 1.5710467100143433,
      "learning_rate": 6.712416611780205e-05,
      "loss": 1.1205,
      "step": 2600
    },
    {
      "epoch": 0.3656684943061999,
      "grad_norm": 1.226946234703064,
      "learning_rate": 6.734380221105934e-05,
      "loss": 1.2748,
      "step": 2601
    },
    {
      "epoch": 0.3658090819626037,
      "grad_norm": 1.5513732433319092,
      "learning_rate": 6.756361478506574e-05,
      "loss": 1.0398,
      "step": 2602
    },
    {
      "epoch": 0.36594966961900743,
      "grad_norm": 1.424202799797058,
      "learning_rate": 6.778360265190924e-05,
      "loss": 1.1393,
      "step": 2603
    },
    {
      "epoch": 0.3660902572754112,
      "grad_norm": 1.2974752187728882,
      "learning_rate": 6.800376462273055e-05,
      "loss": 1.1833,
      "step": 2604
    },
    {
      "epoch": 0.366230844931815,
      "grad_norm": 1.5628727674484253,
      "learning_rate": 6.822409950772944e-05,
      "loss": 1.0884,
      "step": 2605
    },
    {
      "epoch": 0.36637143258821875,
      "grad_norm": 1.383550763130188,
      "learning_rate": 6.844460611617144e-05,
      "loss": 1.2088,
      "step": 2606
    },
    {
      "epoch": 0.36651202024462254,
      "grad_norm": 1.475934624671936,
      "learning_rate": 6.866528325639389e-05,
      "loss": 1.1163,
      "step": 2607
    },
    {
      "epoch": 0.3666526079010263,
      "grad_norm": 1.3810166120529175,
      "learning_rate": 6.888612973581231e-05,
      "loss": 1.2121,
      "step": 2608
    },
    {
      "epoch": 0.36679319555743006,
      "grad_norm": 1.6095088720321655,
      "learning_rate": 6.910714436092754e-05,
      "loss": 1.157,
      "step": 2609
    },
    {
      "epoch": 0.36693378321383385,
      "grad_norm": 1.6501710414886475,
      "learning_rate": 6.932832593733143e-05,
      "loss": 1.1965,
      "step": 2610
    },
    {
      "epoch": 0.3670743708702376,
      "grad_norm": 1.4720014333724976,
      "learning_rate": 6.954967326971364e-05,
      "loss": 1.124,
      "step": 2611
    },
    {
      "epoch": 0.3672149585266414,
      "grad_norm": 1.2395762205123901,
      "learning_rate": 6.977118516186801e-05,
      "loss": 1.0864,
      "step": 2612
    },
    {
      "epoch": 0.3673555461830451,
      "grad_norm": 1.6225031614303589,
      "learning_rate": 6.999286041669926e-05,
      "loss": 1.0279,
      "step": 2613
    },
    {
      "epoch": 0.3674961338394489,
      "grad_norm": 1.502915620803833,
      "learning_rate": 7.02146978362291e-05,
      "loss": 1.1785,
      "step": 2614
    },
    {
      "epoch": 0.3676367214958527,
      "grad_norm": 1.8486300706863403,
      "learning_rate": 7.043669622160283e-05,
      "loss": 0.9501,
      "step": 2615
    },
    {
      "epoch": 0.3677773091522564,
      "grad_norm": 1.4935492277145386,
      "learning_rate": 7.065885437309588e-05,
      "loss": 0.9412,
      "step": 2616
    },
    {
      "epoch": 0.3679178968086602,
      "grad_norm": 1.434739351272583,
      "learning_rate": 7.088117109012049e-05,
      "loss": 1.0935,
      "step": 2617
    },
    {
      "epoch": 0.36805848446506395,
      "grad_norm": 1.3069562911987305,
      "learning_rate": 7.110364517123172e-05,
      "loss": 1.2642,
      "step": 2618
    },
    {
      "epoch": 0.36819907212146774,
      "grad_norm": 1.882298231124878,
      "learning_rate": 7.132627541413428e-05,
      "loss": 1.0376,
      "step": 2619
    },
    {
      "epoch": 0.3683396597778715,
      "grad_norm": 1.5285580158233643,
      "learning_rate": 7.154906061568888e-05,
      "loss": 1.0073,
      "step": 2620
    },
    {
      "epoch": 0.36848024743427527,
      "grad_norm": 1.6254830360412598,
      "learning_rate": 7.177199957191905e-05,
      "loss": 1.1373,
      "step": 2621
    },
    {
      "epoch": 0.36862083509067906,
      "grad_norm": 1.4782743453979492,
      "learning_rate": 7.199509107801714e-05,
      "loss": 1.2047,
      "step": 2622
    },
    {
      "epoch": 0.3687614227470828,
      "grad_norm": 1.575298547744751,
      "learning_rate": 7.221833392835118e-05,
      "loss": 0.9939,
      "step": 2623
    },
    {
      "epoch": 0.3689020104034866,
      "grad_norm": 1.4251362085342407,
      "learning_rate": 7.244172691647124e-05,
      "loss": 1.0561,
      "step": 2624
    },
    {
      "epoch": 0.3690425980598903,
      "grad_norm": 1.6391470432281494,
      "learning_rate": 7.266526883511622e-05,
      "loss": 1.1494,
      "step": 2625
    },
    {
      "epoch": 0.3691831857162941,
      "grad_norm": 1.6320112943649292,
      "learning_rate": 7.288895847622003e-05,
      "loss": 1.0801,
      "step": 2626
    },
    {
      "epoch": 0.3693237733726979,
      "grad_norm": 1.526487946510315,
      "learning_rate": 7.311279463091802e-05,
      "loss": 1.0234,
      "step": 2627
    },
    {
      "epoch": 0.36946436102910163,
      "grad_norm": 1.487818956375122,
      "learning_rate": 7.333677608955425e-05,
      "loss": 0.8973,
      "step": 2628
    },
    {
      "epoch": 0.3696049486855054,
      "grad_norm": 1.5781453847885132,
      "learning_rate": 7.356090164168705e-05,
      "loss": 1.0543,
      "step": 2629
    },
    {
      "epoch": 0.36974553634190915,
      "grad_norm": 1.676884412765503,
      "learning_rate": 7.378517007609656e-05,
      "loss": 1.0374,
      "step": 2630
    },
    {
      "epoch": 0.36988612399831294,
      "grad_norm": 1.5493888854980469,
      "learning_rate": 7.400958018079005e-05,
      "loss": 1.0512,
      "step": 2631
    },
    {
      "epoch": 0.37002671165471673,
      "grad_norm": 1.7915970087051392,
      "learning_rate": 7.423413074300989e-05,
      "loss": 1.0287,
      "step": 2632
    },
    {
      "epoch": 0.37016729931112047,
      "grad_norm": 1.9011952877044678,
      "learning_rate": 7.445882054923886e-05,
      "loss": 1.1368,
      "step": 2633
    },
    {
      "epoch": 0.37030788696752426,
      "grad_norm": 1.9742546081542969,
      "learning_rate": 7.468364838520771e-05,
      "loss": 1.1437,
      "step": 2634
    },
    {
      "epoch": 0.370448474623928,
      "grad_norm": 1.7076935768127441,
      "learning_rate": 7.490861303590067e-05,
      "loss": 1.2344,
      "step": 2635
    },
    {
      "epoch": 0.3705890622803318,
      "grad_norm": 1.4989367723464966,
      "learning_rate": 7.513371328556316e-05,
      "loss": 1.0794,
      "step": 2636
    },
    {
      "epoch": 0.3707296499367356,
      "grad_norm": 1.3882806301116943,
      "learning_rate": 7.535894791770737e-05,
      "loss": 1.058,
      "step": 2637
    },
    {
      "epoch": 0.3708702375931393,
      "grad_norm": 1.4206311702728271,
      "learning_rate": 7.55843157151197e-05,
      "loss": 1.2033,
      "step": 2638
    },
    {
      "epoch": 0.3710108252495431,
      "grad_norm": 1.5078250169754028,
      "learning_rate": 7.580981545986629e-05,
      "loss": 1.1026,
      "step": 2639
    },
    {
      "epoch": 0.37115141290594683,
      "grad_norm": 1.6225999593734741,
      "learning_rate": 7.603544593330082e-05,
      "loss": 1.0038,
      "step": 2640
    },
    {
      "epoch": 0.3712920005623506,
      "grad_norm": 1.5948246717453003,
      "learning_rate": 7.626120591607006e-05,
      "loss": 1.0925,
      "step": 2641
    },
    {
      "epoch": 0.3714325882187544,
      "grad_norm": 1.5151712894439697,
      "learning_rate": 7.648709418812129e-05,
      "loss": 0.9591,
      "step": 2642
    },
    {
      "epoch": 0.37157317587515815,
      "grad_norm": 1.1876274347305298,
      "learning_rate": 7.67131095287079e-05,
      "loss": 1.3516,
      "step": 2643
    },
    {
      "epoch": 0.37171376353156194,
      "grad_norm": 1.527751088142395,
      "learning_rate": 7.693925071639719e-05,
      "loss": 1.1546,
      "step": 2644
    },
    {
      "epoch": 0.37185435118796567,
      "grad_norm": 1.6868016719818115,
      "learning_rate": 7.716551652907599e-05,
      "loss": 1.1371,
      "step": 2645
    },
    {
      "epoch": 0.37199493884436946,
      "grad_norm": 1.6738375425338745,
      "learning_rate": 7.739190574395776e-05,
      "loss": 1.0368,
      "step": 2646
    },
    {
      "epoch": 0.37213552650077325,
      "grad_norm": 1.4637868404388428,
      "learning_rate": 7.7618417137589e-05,
      "loss": 1.0858,
      "step": 2647
    },
    {
      "epoch": 0.372276114157177,
      "grad_norm": 1.3813661336898804,
      "learning_rate": 7.78450494858559e-05,
      "loss": 1.0966,
      "step": 2648
    },
    {
      "epoch": 0.3724167018135808,
      "grad_norm": 1.4209660291671753,
      "learning_rate": 7.807180156399132e-05,
      "loss": 1.0702,
      "step": 2649
    },
    {
      "epoch": 0.3725572894699845,
      "grad_norm": 1.4125865697860718,
      "learning_rate": 7.829867214658046e-05,
      "loss": 1.1159,
      "step": 2650
    },
    {
      "epoch": 0.3726978771263883,
      "grad_norm": 2.247926950454712,
      "learning_rate": 7.85256600075687e-05,
      "loss": 1.2132,
      "step": 2651
    },
    {
      "epoch": 0.3728384647827921,
      "grad_norm": 2.015129327774048,
      "learning_rate": 7.875276392026722e-05,
      "loss": 1.0757,
      "step": 2652
    },
    {
      "epoch": 0.3729790524391958,
      "grad_norm": 1.4920425415039062,
      "learning_rate": 7.897998265736042e-05,
      "loss": 1.1351,
      "step": 2653
    },
    {
      "epoch": 0.3731196400955996,
      "grad_norm": 1.3900187015533447,
      "learning_rate": 7.920731499091163e-05,
      "loss": 1.0663,
      "step": 2654
    },
    {
      "epoch": 0.37326022775200335,
      "grad_norm": 1.562975525856018,
      "learning_rate": 7.943475969237079e-05,
      "loss": 1.1458,
      "step": 2655
    },
    {
      "epoch": 0.37340081540840714,
      "grad_norm": 1.3884087800979614,
      "learning_rate": 7.966231553258022e-05,
      "loss": 1.0547,
      "step": 2656
    },
    {
      "epoch": 0.37354140306481093,
      "grad_norm": 1.4637973308563232,
      "learning_rate": 7.988998128178206e-05,
      "loss": 1.2117,
      "step": 2657
    },
    {
      "epoch": 0.37368199072121466,
      "grad_norm": 1.2843408584594727,
      "learning_rate": 8.011775570962382e-05,
      "loss": 1.1835,
      "step": 2658
    },
    {
      "epoch": 0.37382257837761845,
      "grad_norm": 1.451019048690796,
      "learning_rate": 8.034563758516635e-05,
      "loss": 1.0205,
      "step": 2659
    },
    {
      "epoch": 0.3739631660340222,
      "grad_norm": 1.2583070993423462,
      "learning_rate": 8.057362567688936e-05,
      "loss": 1.1231,
      "step": 2660
    },
    {
      "epoch": 0.374103753690426,
      "grad_norm": 1.4738574028015137,
      "learning_rate": 8.080171875269897e-05,
      "loss": 1.1987,
      "step": 2661
    },
    {
      "epoch": 0.37424434134682977,
      "grad_norm": 1.3363100290298462,
      "learning_rate": 8.10299155799334e-05,
      "loss": 1.0375,
      "step": 2662
    },
    {
      "epoch": 0.3743849290032335,
      "grad_norm": 1.4111905097961426,
      "learning_rate": 8.125821492537074e-05,
      "loss": 1.0124,
      "step": 2663
    },
    {
      "epoch": 0.3745255166596373,
      "grad_norm": 1.8315365314483643,
      "learning_rate": 8.148661555523459e-05,
      "loss": 1.0559,
      "step": 2664
    },
    {
      "epoch": 0.374666104316041,
      "grad_norm": 1.328649640083313,
      "learning_rate": 8.171511623520167e-05,
      "loss": 1.2059,
      "step": 2665
    },
    {
      "epoch": 0.3748066919724448,
      "grad_norm": 1.4611704349517822,
      "learning_rate": 8.194371573040747e-05,
      "loss": 1.1782,
      "step": 2666
    },
    {
      "epoch": 0.3749472796288486,
      "grad_norm": 1.4868890047073364,
      "learning_rate": 8.217241280545373e-05,
      "loss": 1.1153,
      "step": 2667
    },
    {
      "epoch": 0.37508786728525234,
      "grad_norm": 1.5664066076278687,
      "learning_rate": 8.240120622441501e-05,
      "loss": 1.0843,
      "step": 2668
    },
    {
      "epoch": 0.37522845494165613,
      "grad_norm": 1.4771925210952759,
      "learning_rate": 8.263009475084496e-05,
      "loss": 1.161,
      "step": 2669
    },
    {
      "epoch": 0.37536904259805987,
      "grad_norm": 1.7180688381195068,
      "learning_rate": 8.28590771477833e-05,
      "loss": 1.0963,
      "step": 2670
    },
    {
      "epoch": 0.37550963025446366,
      "grad_norm": 1.533623218536377,
      "learning_rate": 8.308815217776242e-05,
      "loss": 1.2222,
      "step": 2671
    },
    {
      "epoch": 0.37565021791086745,
      "grad_norm": 1.5842229127883911,
      "learning_rate": 8.331731860281435e-05,
      "loss": 1.0997,
      "step": 2672
    },
    {
      "epoch": 0.3757908055672712,
      "grad_norm": 1.4248261451721191,
      "learning_rate": 8.354657518447693e-05,
      "loss": 1.2097,
      "step": 2673
    },
    {
      "epoch": 0.37593139322367497,
      "grad_norm": 1.7966675758361816,
      "learning_rate": 8.377592068380085e-05,
      "loss": 1.2809,
      "step": 2674
    },
    {
      "epoch": 0.3760719808800787,
      "grad_norm": 1.501310110092163,
      "learning_rate": 8.40053538613562e-05,
      "loss": 0.8947,
      "step": 2675
    },
    {
      "epoch": 0.3762125685364825,
      "grad_norm": 1.587172031402588,
      "learning_rate": 8.423487347723949e-05,
      "loss": 1.1126,
      "step": 2676
    },
    {
      "epoch": 0.3763531561928863,
      "grad_norm": 1.351994276046753,
      "learning_rate": 8.446447829107987e-05,
      "loss": 1.12,
      "step": 2677
    },
    {
      "epoch": 0.37649374384929,
      "grad_norm": 1.196803331375122,
      "learning_rate": 8.469416706204611e-05,
      "loss": 1.3552,
      "step": 2678
    },
    {
      "epoch": 0.3766343315056938,
      "grad_norm": 1.454355001449585,
      "learning_rate": 8.49239385488532e-05,
      "loss": 1.1174,
      "step": 2679
    },
    {
      "epoch": 0.37677491916209754,
      "grad_norm": 1.5828267335891724,
      "learning_rate": 8.515379150976935e-05,
      "loss": 1.0268,
      "step": 2680
    },
    {
      "epoch": 0.37691550681850133,
      "grad_norm": 1.5164055824279785,
      "learning_rate": 8.538372470262223e-05,
      "loss": 1.0891,
      "step": 2681
    },
    {
      "epoch": 0.3770560944749051,
      "grad_norm": 1.2222626209259033,
      "learning_rate": 8.5613736884806e-05,
      "loss": 1.1558,
      "step": 2682
    },
    {
      "epoch": 0.37719668213130886,
      "grad_norm": 1.506642460823059,
      "learning_rate": 8.584382681328786e-05,
      "loss": 0.8551,
      "step": 2683
    },
    {
      "epoch": 0.37733726978771265,
      "grad_norm": 1.3947241306304932,
      "learning_rate": 8.607399324461511e-05,
      "loss": 1.0572,
      "step": 2684
    },
    {
      "epoch": 0.3774778574441164,
      "grad_norm": 1.797595739364624,
      "learning_rate": 8.630423493492144e-05,
      "loss": 1.0902,
      "step": 2685
    },
    {
      "epoch": 0.3776184451005202,
      "grad_norm": 1.4838796854019165,
      "learning_rate": 8.653455063993357e-05,
      "loss": 0.9942,
      "step": 2686
    },
    {
      "epoch": 0.37775903275692396,
      "grad_norm": 1.5957480669021606,
      "learning_rate": 8.676493911497875e-05,
      "loss": 0.9835,
      "step": 2687
    },
    {
      "epoch": 0.3778996204133277,
      "grad_norm": 1.377171277999878,
      "learning_rate": 8.69953991149906e-05,
      "loss": 1.0389,
      "step": 2688
    },
    {
      "epoch": 0.3780402080697315,
      "grad_norm": 1.6809266805648804,
      "learning_rate": 8.722592939451631e-05,
      "loss": 0.9829,
      "step": 2689
    },
    {
      "epoch": 0.3781807957261352,
      "grad_norm": 1.4696694612503052,
      "learning_rate": 8.745652870772318e-05,
      "loss": 1.1058,
      "step": 2690
    },
    {
      "epoch": 0.378321383382539,
      "grad_norm": 1.4977242946624756,
      "learning_rate": 8.768719580840569e-05,
      "loss": 0.9151,
      "step": 2691
    },
    {
      "epoch": 0.3784619710389428,
      "grad_norm": 1.6743556261062622,
      "learning_rate": 8.791792944999172e-05,
      "loss": 1.164,
      "step": 2692
    },
    {
      "epoch": 0.37860255869534654,
      "grad_norm": 1.6559395790100098,
      "learning_rate": 8.814872838554965e-05,
      "loss": 0.9558,
      "step": 2693
    },
    {
      "epoch": 0.3787431463517503,
      "grad_norm": 1.50855553150177,
      "learning_rate": 8.83795913677949e-05,
      "loss": 1.1404,
      "step": 2694
    },
    {
      "epoch": 0.37888373400815406,
      "grad_norm": 1.320807933807373,
      "learning_rate": 8.861051714909704e-05,
      "loss": 1.2877,
      "step": 2695
    },
    {
      "epoch": 0.37902432166455785,
      "grad_norm": 1.4517478942871094,
      "learning_rate": 8.884150448148598e-05,
      "loss": 1.0979,
      "step": 2696
    },
    {
      "epoch": 0.37916490932096164,
      "grad_norm": 1.4228923320770264,
      "learning_rate": 8.907255211665909e-05,
      "loss": 1.0249,
      "step": 2697
    },
    {
      "epoch": 0.3793054969773654,
      "grad_norm": 1.3634127378463745,
      "learning_rate": 8.930365880598777e-05,
      "loss": 1.1662,
      "step": 2698
    },
    {
      "epoch": 0.37944608463376917,
      "grad_norm": 1.51992666721344,
      "learning_rate": 8.953482330052457e-05,
      "loss": 1.0875,
      "step": 2699
    },
    {
      "epoch": 0.3795866722901729,
      "grad_norm": 1.3075053691864014,
      "learning_rate": 8.976604435100932e-05,
      "loss": 0.9798,
      "step": 2700
    },
    {
      "epoch": 0.3797272599465767,
      "grad_norm": 1.3148120641708374,
      "learning_rate": 8.999732070787635e-05,
      "loss": 0.997,
      "step": 2701
    },
    {
      "epoch": 0.3798678476029805,
      "grad_norm": 1.5131416320800781,
      "learning_rate": 9.0228651121261e-05,
      "loss": 1.1875,
      "step": 2702
    },
    {
      "epoch": 0.3800084352593842,
      "grad_norm": 1.3038175106048584,
      "learning_rate": 9.046003434100672e-05,
      "loss": 0.9805,
      "step": 2703
    },
    {
      "epoch": 0.380149022915788,
      "grad_norm": 1.537192463874817,
      "learning_rate": 9.069146911667147e-05,
      "loss": 1.0776,
      "step": 2704
    },
    {
      "epoch": 0.38028961057219174,
      "grad_norm": 1.3715260028839111,
      "learning_rate": 9.092295419753425e-05,
      "loss": 1.1686,
      "step": 2705
    },
    {
      "epoch": 0.38043019822859553,
      "grad_norm": 1.635269284248352,
      "learning_rate": 9.115448833260278e-05,
      "loss": 1.0898,
      "step": 2706
    },
    {
      "epoch": 0.3805707858849993,
      "grad_norm": 1.558831810951233,
      "learning_rate": 9.138607027061922e-05,
      "loss": 1.2921,
      "step": 2707
    },
    {
      "epoch": 0.38071137354140305,
      "grad_norm": 1.5317187309265137,
      "learning_rate": 9.161769876006783e-05,
      "loss": 1.0991,
      "step": 2708
    },
    {
      "epoch": 0.38085196119780684,
      "grad_norm": 1.5453031063079834,
      "learning_rate": 9.184937254918071e-05,
      "loss": 1.1204,
      "step": 2709
    },
    {
      "epoch": 0.3809925488542106,
      "grad_norm": 1.50357985496521,
      "learning_rate": 9.208109038594573e-05,
      "loss": 1.2385,
      "step": 2710
    },
    {
      "epoch": 0.38113313651061437,
      "grad_norm": 1.5680991411209106,
      "learning_rate": 9.23128510181123e-05,
      "loss": 1.0524,
      "step": 2711
    },
    {
      "epoch": 0.38127372416701816,
      "grad_norm": 1.5807344913482666,
      "learning_rate": 9.254465319319899e-05,
      "loss": 1.0231,
      "step": 2712
    },
    {
      "epoch": 0.3814143118234219,
      "grad_norm": 1.4988694190979004,
      "learning_rate": 9.277649565849925e-05,
      "loss": 0.9874,
      "step": 2713
    },
    {
      "epoch": 0.3815548994798257,
      "grad_norm": 1.587576150894165,
      "learning_rate": 9.300837716108939e-05,
      "loss": 1.1526,
      "step": 2714
    },
    {
      "epoch": 0.3816954871362294,
      "grad_norm": 1.4451206922531128,
      "learning_rate": 9.324029644783435e-05,
      "loss": 1.0644,
      "step": 2715
    },
    {
      "epoch": 0.3818360747926332,
      "grad_norm": 1.4432424306869507,
      "learning_rate": 9.347225226539528e-05,
      "loss": 1.1492,
      "step": 2716
    },
    {
      "epoch": 0.381976662449037,
      "grad_norm": 1.6694376468658447,
      "learning_rate": 9.370424336023532e-05,
      "loss": 1.2004,
      "step": 2717
    },
    {
      "epoch": 0.38211725010544073,
      "grad_norm": 1.247651219367981,
      "learning_rate": 9.393626847862757e-05,
      "loss": 1.1492,
      "step": 2718
    },
    {
      "epoch": 0.3822578377618445,
      "grad_norm": 1.8106296062469482,
      "learning_rate": 9.416832636666083e-05,
      "loss": 1.0977,
      "step": 2719
    },
    {
      "epoch": 0.38239842541824826,
      "grad_norm": 1.403602123260498,
      "learning_rate": 9.440041577024723e-05,
      "loss": 1.2037,
      "step": 2720
    },
    {
      "epoch": 0.38253901307465205,
      "grad_norm": 1.4064615964889526,
      "learning_rate": 9.463253543512797e-05,
      "loss": 1.0844,
      "step": 2721
    },
    {
      "epoch": 0.38267960073105584,
      "grad_norm": 1.544590950012207,
      "learning_rate": 9.486468410688132e-05,
      "loss": 1.0185,
      "step": 2722
    },
    {
      "epoch": 0.38282018838745957,
      "grad_norm": 1.2627629041671753,
      "learning_rate": 9.50968605309285e-05,
      "loss": 1.3227,
      "step": 2723
    },
    {
      "epoch": 0.38296077604386336,
      "grad_norm": 1.394309639930725,
      "learning_rate": 9.532906345254074e-05,
      "loss": 1.1162,
      "step": 2724
    },
    {
      "epoch": 0.3831013637002671,
      "grad_norm": 1.5603388547897339,
      "learning_rate": 9.556129161684615e-05,
      "loss": 1.3021,
      "step": 2725
    },
    {
      "epoch": 0.3832419513566709,
      "grad_norm": 1.5538884401321411,
      "learning_rate": 9.57935437688363e-05,
      "loss": 0.9972,
      "step": 2726
    },
    {
      "epoch": 0.3833825390130747,
      "grad_norm": 1.4970897436141968,
      "learning_rate": 9.602581865337349e-05,
      "loss": 1.3111,
      "step": 2727
    },
    {
      "epoch": 0.3835231266694784,
      "grad_norm": 1.3637696504592896,
      "learning_rate": 9.625811501519654e-05,
      "loss": 1.0333,
      "step": 2728
    },
    {
      "epoch": 0.3836637143258822,
      "grad_norm": 1.4109879732131958,
      "learning_rate": 9.649043159892882e-05,
      "loss": 1.261,
      "step": 2729
    },
    {
      "epoch": 0.38380430198228593,
      "grad_norm": 1.8951594829559326,
      "learning_rate": 9.6722767149084e-05,
      "loss": 1.1764,
      "step": 2730
    },
    {
      "epoch": 0.3839448896386897,
      "grad_norm": 1.321424126625061,
      "learning_rate": 9.695512041007369e-05,
      "loss": 1.0012,
      "step": 2731
    },
    {
      "epoch": 0.3840854772950935,
      "grad_norm": 1.6736654043197632,
      "learning_rate": 9.718749012621311e-05,
      "loss": 1.112,
      "step": 2732
    },
    {
      "epoch": 0.38422606495149725,
      "grad_norm": 1.4725463390350342,
      "learning_rate": 9.741987504172924e-05,
      "loss": 1.1515,
      "step": 2733
    },
    {
      "epoch": 0.38436665260790104,
      "grad_norm": 1.4629011154174805,
      "learning_rate": 9.765227390076647e-05,
      "loss": 1.0395,
      "step": 2734
    },
    {
      "epoch": 0.3845072402643048,
      "grad_norm": 1.3974485397338867,
      "learning_rate": 9.788468544739422e-05,
      "loss": 1.1495,
      "step": 2735
    },
    {
      "epoch": 0.38464782792070856,
      "grad_norm": 1.3937411308288574,
      "learning_rate": 9.811710842561283e-05,
      "loss": 1.1027,
      "step": 2736
    },
    {
      "epoch": 0.38478841557711235,
      "grad_norm": 1.537476897239685,
      "learning_rate": 9.834954157936136e-05,
      "loss": 1.3229,
      "step": 2737
    },
    {
      "epoch": 0.3849290032335161,
      "grad_norm": 1.6366387605667114,
      "learning_rate": 9.85819836525235e-05,
      "loss": 1.1379,
      "step": 2738
    },
    {
      "epoch": 0.3850695908899199,
      "grad_norm": 1.5734015703201294,
      "learning_rate": 9.881443338893522e-05,
      "loss": 1.1432,
      "step": 2739
    },
    {
      "epoch": 0.3852101785463236,
      "grad_norm": 1.4866102933883667,
      "learning_rate": 9.904688953239035e-05,
      "loss": 1.1908,
      "step": 2740
    },
    {
      "epoch": 0.3853507662027274,
      "grad_norm": 1.5062594413757324,
      "learning_rate": 9.927935082664881e-05,
      "loss": 1.0613,
      "step": 2741
    },
    {
      "epoch": 0.3854913538591312,
      "grad_norm": 1.4154350757598877,
      "learning_rate": 9.95118160154423e-05,
      "loss": 1.2732,
      "step": 2742
    },
    {
      "epoch": 0.3856319415155349,
      "grad_norm": 1.6992545127868652,
      "learning_rate": 9.974428384248153e-05,
      "loss": 1.0493,
      "step": 2743
    },
    {
      "epoch": 0.3857725291719387,
      "grad_norm": 1.6510138511657715,
      "learning_rate": 9.997675305146302e-05,
      "loss": 0.9758,
      "step": 2744
    },
    {
      "epoch": 0.38591311682834245,
      "grad_norm": 1.379258632659912,
      "learning_rate": 0.00010020922238607563,
      "loss": 1.3721,
      "step": 2745
    },
    {
      "epoch": 0.38605370448474624,
      "grad_norm": 1.594556450843811,
      "learning_rate": 0.00010044169059000796,
      "loss": 1.1424,
      "step": 2746
    },
    {
      "epoch": 0.38619429214115003,
      "grad_norm": 1.5866600275039673,
      "learning_rate": 0.00010067415640695427,
      "loss": 1.0464,
      "step": 2747
    },
    {
      "epoch": 0.38633487979755377,
      "grad_norm": 1.398728609085083,
      "learning_rate": 0.00010090661858062199,
      "loss": 1.2069,
      "step": 2748
    },
    {
      "epoch": 0.38647546745395756,
      "grad_norm": 1.880760669708252,
      "learning_rate": 0.00010113907585473799,
      "loss": 0.9429,
      "step": 2749
    },
    {
      "epoch": 0.3866160551103613,
      "grad_norm": 1.5909810066223145,
      "learning_rate": 0.00010137152697305599,
      "loss": 1.2509,
      "step": 2750
    },
    {
      "epoch": 0.3867566427667651,
      "grad_norm": 1.5487297773361206,
      "learning_rate": 0.00010160397067936268,
      "loss": 1.0246,
      "step": 2751
    },
    {
      "epoch": 0.38689723042316887,
      "grad_norm": 1.7061704397201538,
      "learning_rate": 0.00010183640571748487,
      "loss": 1.1875,
      "step": 2752
    },
    {
      "epoch": 0.3870378180795726,
      "grad_norm": 1.6238174438476562,
      "learning_rate": 0.00010206883083129619,
      "loss": 1.0919,
      "step": 2753
    },
    {
      "epoch": 0.3871784057359764,
      "grad_norm": 1.4496607780456543,
      "learning_rate": 0.0001023012447647241,
      "loss": 1.1428,
      "step": 2754
    },
    {
      "epoch": 0.38731899339238013,
      "grad_norm": 1.5121047496795654,
      "learning_rate": 0.00010253364626175623,
      "loss": 1.0819,
      "step": 2755
    },
    {
      "epoch": 0.3874595810487839,
      "grad_norm": 1.8277976512908936,
      "learning_rate": 0.00010276603406644756,
      "loss": 1.2466,
      "step": 2756
    },
    {
      "epoch": 0.3876001687051877,
      "grad_norm": 1.4639886617660522,
      "learning_rate": 0.00010299840692292688,
      "loss": 1.2695,
      "step": 2757
    },
    {
      "epoch": 0.38774075636159144,
      "grad_norm": 1.416172981262207,
      "learning_rate": 0.00010323076357540416,
      "loss": 1.1507,
      "step": 2758
    },
    {
      "epoch": 0.38788134401799523,
      "grad_norm": 1.4498172998428345,
      "learning_rate": 0.00010346310276817655,
      "loss": 1.083,
      "step": 2759
    },
    {
      "epoch": 0.38802193167439897,
      "grad_norm": 1.6471742391586304,
      "learning_rate": 0.0001036954232456357,
      "loss": 1.0357,
      "step": 2760
    },
    {
      "epoch": 0.38816251933080276,
      "grad_norm": 1.3745568990707397,
      "learning_rate": 0.00010392772375227434,
      "loss": 1.1784,
      "step": 2761
    },
    {
      "epoch": 0.38830310698720655,
      "grad_norm": 1.585745096206665,
      "learning_rate": 0.00010416000303269331,
      "loss": 1.1654,
      "step": 2762
    },
    {
      "epoch": 0.3884436946436103,
      "grad_norm": 1.4576352834701538,
      "learning_rate": 0.00010439225983160808,
      "loss": 1.1116,
      "step": 2763
    },
    {
      "epoch": 0.38858428230001407,
      "grad_norm": 1.5870800018310547,
      "learning_rate": 0.00010462449289385528,
      "loss": 1.0153,
      "step": 2764
    },
    {
      "epoch": 0.3887248699564178,
      "grad_norm": 1.4250578880310059,
      "learning_rate": 0.00010485670096440036,
      "loss": 0.9867,
      "step": 2765
    },
    {
      "epoch": 0.3888654576128216,
      "grad_norm": 1.625108242034912,
      "learning_rate": 0.00010508888278834352,
      "loss": 1.1796,
      "step": 2766
    },
    {
      "epoch": 0.3890060452692254,
      "grad_norm": 1.385621190071106,
      "learning_rate": 0.00010532103711092684,
      "loss": 0.9232,
      "step": 2767
    },
    {
      "epoch": 0.3891466329256291,
      "grad_norm": 1.6536011695861816,
      "learning_rate": 0.00010555316267754096,
      "loss": 1.2118,
      "step": 2768
    },
    {
      "epoch": 0.3892872205820329,
      "grad_norm": 1.4167743921279907,
      "learning_rate": 0.00010578525823373217,
      "loss": 1.2118,
      "step": 2769
    },
    {
      "epoch": 0.38942780823843665,
      "grad_norm": 1.7858283519744873,
      "learning_rate": 0.00010601732252520873,
      "loss": 0.9912,
      "step": 2770
    },
    {
      "epoch": 0.38956839589484044,
      "grad_norm": 1.4761207103729248,
      "learning_rate": 0.00010624935429784783,
      "loss": 1.1684,
      "step": 2771
    },
    {
      "epoch": 0.3897089835512442,
      "grad_norm": 1.7772603034973145,
      "learning_rate": 0.00010648135229770246,
      "loss": 1.0862,
      "step": 2772
    },
    {
      "epoch": 0.38984957120764796,
      "grad_norm": 1.8215521574020386,
      "learning_rate": 0.00010671331527100827,
      "loss": 0.9245,
      "step": 2773
    },
    {
      "epoch": 0.38999015886405175,
      "grad_norm": 1.546992540359497,
      "learning_rate": 0.00010694524196419002,
      "loss": 1.2089,
      "step": 2774
    },
    {
      "epoch": 0.3901307465204555,
      "grad_norm": 1.312246561050415,
      "learning_rate": 0.00010717713112386858,
      "loss": 1.1017,
      "step": 2775
    },
    {
      "epoch": 0.3902713341768593,
      "grad_norm": 1.9738069772720337,
      "learning_rate": 0.00010740898149686756,
      "loss": 1.0586,
      "step": 2776
    },
    {
      "epoch": 0.39041192183326306,
      "grad_norm": 1.5985878705978394,
      "learning_rate": 0.00010764079183022047,
      "loss": 1.1733,
      "step": 2777
    },
    {
      "epoch": 0.3905525094896668,
      "grad_norm": 1.4716832637786865,
      "learning_rate": 0.00010787256087117692,
      "loss": 1.1007,
      "step": 2778
    },
    {
      "epoch": 0.3906930971460706,
      "grad_norm": 1.7397269010543823,
      "learning_rate": 0.00010810428736720981,
      "loss": 1.1342,
      "step": 2779
    },
    {
      "epoch": 0.3908336848024743,
      "grad_norm": 1.5310194492340088,
      "learning_rate": 0.00010833597006602181,
      "loss": 1.0981,
      "step": 2780
    },
    {
      "epoch": 0.3909742724588781,
      "grad_norm": 1.5887361764907837,
      "learning_rate": 0.00010856760771555257,
      "loss": 1.1661,
      "step": 2781
    },
    {
      "epoch": 0.3911148601152819,
      "grad_norm": 2.104175090789795,
      "learning_rate": 0.000108799199063985,
      "loss": 1.0713,
      "step": 2782
    },
    {
      "epoch": 0.39125544777168564,
      "grad_norm": 1.4610835313796997,
      "learning_rate": 0.00010903074285975202,
      "loss": 1.1687,
      "step": 2783
    },
    {
      "epoch": 0.39139603542808943,
      "grad_norm": 1.3477734327316284,
      "learning_rate": 0.00010926223785154401,
      "loss": 1.0126,
      "step": 2784
    },
    {
      "epoch": 0.39153662308449316,
      "grad_norm": 1.6047677993774414,
      "learning_rate": 0.00010949368278831466,
      "loss": 1.0636,
      "step": 2785
    },
    {
      "epoch": 0.39167721074089695,
      "grad_norm": 1.5342696905136108,
      "learning_rate": 0.00010972507641928865,
      "loss": 1.0178,
      "step": 2786
    },
    {
      "epoch": 0.39181779839730074,
      "grad_norm": 1.555497407913208,
      "learning_rate": 0.00010995641749396726,
      "loss": 1.242,
      "step": 2787
    },
    {
      "epoch": 0.3919583860537045,
      "grad_norm": 1.417351245880127,
      "learning_rate": 0.00011018770476213645,
      "loss": 1.0619,
      "step": 2788
    },
    {
      "epoch": 0.39209897371010827,
      "grad_norm": 1.3778603076934814,
      "learning_rate": 0.00011041893697387247,
      "loss": 1.1434,
      "step": 2789
    },
    {
      "epoch": 0.392239561366512,
      "grad_norm": 1.5204204320907593,
      "learning_rate": 0.00011065011287954963,
      "loss": 1.1391,
      "step": 2790
    },
    {
      "epoch": 0.3923801490229158,
      "grad_norm": 1.6635582447052002,
      "learning_rate": 0.00011088123122984582,
      "loss": 1.1301,
      "step": 2791
    },
    {
      "epoch": 0.3925207366793196,
      "grad_norm": 1.425424575805664,
      "learning_rate": 0.00011111229077575068,
      "loss": 1.0611,
      "step": 2792
    },
    {
      "epoch": 0.3926613243357233,
      "grad_norm": 1.5582612752914429,
      "learning_rate": 0.00011134329026857108,
      "loss": 1.2223,
      "step": 2793
    },
    {
      "epoch": 0.3928019119921271,
      "grad_norm": 1.5527281761169434,
      "learning_rate": 0.00011157422845993901,
      "loss": 1.0499,
      "step": 2794
    },
    {
      "epoch": 0.39294249964853084,
      "grad_norm": 1.488529920578003,
      "learning_rate": 0.00011180510410181701,
      "loss": 1.1541,
      "step": 2795
    },
    {
      "epoch": 0.39308308730493463,
      "grad_norm": 1.495755672454834,
      "learning_rate": 0.00011203591594650638,
      "loss": 0.9875,
      "step": 2796
    },
    {
      "epoch": 0.3932236749613384,
      "grad_norm": 1.6893550157546997,
      "learning_rate": 0.0001122666627466526,
      "loss": 1.153,
      "step": 2797
    },
    {
      "epoch": 0.39336426261774216,
      "grad_norm": 1.4782142639160156,
      "learning_rate": 0.0001124973432552533,
      "loss": 1.0843,
      "step": 2798
    },
    {
      "epoch": 0.39350485027414595,
      "grad_norm": 1.3603196144104004,
      "learning_rate": 0.00011272795622566354,
      "loss": 1.2425,
      "step": 2799
    },
    {
      "epoch": 0.3936454379305497,
      "grad_norm": 1.375249981880188,
      "learning_rate": 0.00011295850041160413,
      "loss": 1.0328,
      "step": 2800
    },
    {
      "epoch": 0.39378602558695347,
      "grad_norm": 1.6747864484786987,
      "learning_rate": 0.00011318897456716724,
      "loss": 1.1218,
      "step": 2801
    },
    {
      "epoch": 0.39392661324335726,
      "grad_norm": 1.708353877067566,
      "learning_rate": 0.00011341937744682352,
      "loss": 1.1157,
      "step": 2802
    },
    {
      "epoch": 0.394067200899761,
      "grad_norm": 1.5765055418014526,
      "learning_rate": 0.00011364970780542885,
      "loss": 1.1037,
      "step": 2803
    },
    {
      "epoch": 0.3942077885561648,
      "grad_norm": 1.6798044443130493,
      "learning_rate": 0.00011387996439823094,
      "loss": 1.0998,
      "step": 2804
    },
    {
      "epoch": 0.3943483762125685,
      "grad_norm": 1.4637730121612549,
      "learning_rate": 0.00011411014598087648,
      "loss": 1.1002,
      "step": 2805
    },
    {
      "epoch": 0.3944889638689723,
      "grad_norm": 1.451733112335205,
      "learning_rate": 0.00011434025130941697,
      "loss": 0.9097,
      "step": 2806
    },
    {
      "epoch": 0.3946295515253761,
      "grad_norm": 1.389222502708435,
      "learning_rate": 0.00011457027914031653,
      "loss": 1.1002,
      "step": 2807
    },
    {
      "epoch": 0.39477013918177983,
      "grad_norm": 1.5107685327529907,
      "learning_rate": 0.00011480022823045772,
      "loss": 1.1398,
      "step": 2808
    },
    {
      "epoch": 0.3949107268381836,
      "grad_norm": 1.5314667224884033,
      "learning_rate": 0.00011503009733714907,
      "loss": 0.9945,
      "step": 2809
    },
    {
      "epoch": 0.39505131449458736,
      "grad_norm": 1.6043691635131836,
      "learning_rate": 0.00011525988521813069,
      "loss": 1.1565,
      "step": 2810
    },
    {
      "epoch": 0.39519190215099115,
      "grad_norm": 1.3084238767623901,
      "learning_rate": 0.00011548959063158232,
      "loss": 1.2511,
      "step": 2811
    },
    {
      "epoch": 0.39533248980739494,
      "grad_norm": 1.4939817190170288,
      "learning_rate": 0.0001157192123361289,
      "loss": 1.0819,
      "step": 2812
    },
    {
      "epoch": 0.3954730774637987,
      "grad_norm": 1.4628314971923828,
      "learning_rate": 0.00011594874909084818,
      "loss": 1.1061,
      "step": 2813
    },
    {
      "epoch": 0.39561366512020246,
      "grad_norm": 1.4906792640686035,
      "learning_rate": 0.00011617819965527645,
      "loss": 1.1486,
      "step": 2814
    },
    {
      "epoch": 0.3957542527766062,
      "grad_norm": 1.513870120048523,
      "learning_rate": 0.00011640756278941632,
      "loss": 0.87,
      "step": 2815
    },
    {
      "epoch": 0.39589484043301,
      "grad_norm": 1.7726430892944336,
      "learning_rate": 0.00011663683725374248,
      "loss": 1.0281,
      "step": 2816
    },
    {
      "epoch": 0.3960354280894138,
      "grad_norm": 1.6841741800308228,
      "learning_rate": 0.00011686602180920927,
      "loss": 1.0919,
      "step": 2817
    },
    {
      "epoch": 0.3961760157458175,
      "grad_norm": 1.2849020957946777,
      "learning_rate": 0.00011709511521725631,
      "loss": 1.0851,
      "step": 2818
    },
    {
      "epoch": 0.3963166034022213,
      "grad_norm": 1.6976521015167236,
      "learning_rate": 0.00011732411623981635,
      "loss": 1.2005,
      "step": 2819
    },
    {
      "epoch": 0.39645719105862504,
      "grad_norm": 1.5611851215362549,
      "learning_rate": 0.00011755302363932111,
      "loss": 0.984,
      "step": 2820
    },
    {
      "epoch": 0.3965977787150288,
      "grad_norm": 1.326809287071228,
      "learning_rate": 0.00011778183617870838,
      "loss": 1.0749,
      "step": 2821
    },
    {
      "epoch": 0.3967383663714326,
      "grad_norm": 1.806330680847168,
      "learning_rate": 0.00011801055262142851,
      "loss": 1.1152,
      "step": 2822
    },
    {
      "epoch": 0.39687895402783635,
      "grad_norm": 1.7327831983566284,
      "learning_rate": 0.00011823917173145115,
      "loss": 1.1218,
      "step": 2823
    },
    {
      "epoch": 0.39701954168424014,
      "grad_norm": 1.6013824939727783,
      "learning_rate": 0.00011846769227327224,
      "loss": 1.0205,
      "step": 2824
    },
    {
      "epoch": 0.3971601293406439,
      "grad_norm": 1.423599362373352,
      "learning_rate": 0.00011869611301192007,
      "loss": 1.262,
      "step": 2825
    },
    {
      "epoch": 0.39730071699704766,
      "grad_norm": 1.236615777015686,
      "learning_rate": 0.00011892443271296243,
      "loss": 1.0432,
      "step": 2826
    },
    {
      "epoch": 0.3974413046534514,
      "grad_norm": 1.702130913734436,
      "learning_rate": 0.00011915265014251303,
      "loss": 1.147,
      "step": 2827
    },
    {
      "epoch": 0.3975818923098552,
      "grad_norm": 1.7817691564559937,
      "learning_rate": 0.00011938076406723853,
      "loss": 1.0417,
      "step": 2828
    },
    {
      "epoch": 0.397722479966259,
      "grad_norm": 1.3875641822814941,
      "learning_rate": 0.00011960877325436468,
      "loss": 0.9951,
      "step": 2829
    },
    {
      "epoch": 0.3978630676226627,
      "grad_norm": 1.4442225694656372,
      "learning_rate": 0.0001198366764716834,
      "loss": 1.012,
      "step": 2830
    },
    {
      "epoch": 0.3980036552790665,
      "grad_norm": 1.666203498840332,
      "learning_rate": 0.00012006447248755912,
      "loss": 1.0681,
      "step": 2831
    },
    {
      "epoch": 0.39814424293547024,
      "grad_norm": 1.6569268703460693,
      "learning_rate": 0.00012029216007093595,
      "loss": 1.1869,
      "step": 2832
    },
    {
      "epoch": 0.39828483059187403,
      "grad_norm": 1.4847407341003418,
      "learning_rate": 0.00012051973799134368,
      "loss": 1.072,
      "step": 2833
    },
    {
      "epoch": 0.3984254182482778,
      "grad_norm": 1.573314905166626,
      "learning_rate": 0.00012074720501890484,
      "loss": 1.0942,
      "step": 2834
    },
    {
      "epoch": 0.39856600590468155,
      "grad_norm": 1.7595103979110718,
      "learning_rate": 0.00012097455992434114,
      "loss": 1.0387,
      "step": 2835
    },
    {
      "epoch": 0.39870659356108534,
      "grad_norm": 1.5664292573928833,
      "learning_rate": 0.00012120180147898055,
      "loss": 1.1238,
      "step": 2836
    },
    {
      "epoch": 0.3988471812174891,
      "grad_norm": 1.731202483177185,
      "learning_rate": 0.00012142892845476328,
      "loss": 1.0974,
      "step": 2837
    },
    {
      "epoch": 0.39898776887389287,
      "grad_norm": 1.5997743606567383,
      "learning_rate": 0.00012165593962424886,
      "loss": 1.2444,
      "step": 2838
    },
    {
      "epoch": 0.39912835653029666,
      "grad_norm": 1.5531119108200073,
      "learning_rate": 0.00012188283376062258,
      "loss": 1.1331,
      "step": 2839
    },
    {
      "epoch": 0.3992689441867004,
      "grad_norm": 2.7208261489868164,
      "learning_rate": 0.00012210960963770243,
      "loss": 1.0357,
      "step": 2840
    },
    {
      "epoch": 0.3994095318431042,
      "grad_norm": 1.6400601863861084,
      "learning_rate": 0.00012233626602994535,
      "loss": 1.1569,
      "step": 2841
    },
    {
      "epoch": 0.3995501194995079,
      "grad_norm": 1.5690133571624756,
      "learning_rate": 0.00012256280171245377,
      "loss": 0.9446,
      "step": 2842
    },
    {
      "epoch": 0.3996907071559117,
      "grad_norm": 1.7000865936279297,
      "learning_rate": 0.00012278921546098292,
      "loss": 1.1804,
      "step": 2843
    },
    {
      "epoch": 0.3998312948123155,
      "grad_norm": 1.9064589738845825,
      "learning_rate": 0.00012301550605194666,
      "loss": 1.148,
      "step": 2844
    },
    {
      "epoch": 0.39997188246871923,
      "grad_norm": 2.013197660446167,
      "learning_rate": 0.00012324167226242456,
      "loss": 1.2414,
      "step": 2845
    },
    {
      "epoch": 0.400112470125123,
      "grad_norm": 1.5411672592163086,
      "learning_rate": 0.00012346771287016818,
      "loss": 1.102,
      "step": 2846
    },
    {
      "epoch": 0.40025305778152676,
      "grad_norm": 1.4576671123504639,
      "learning_rate": 0.0001236936266536082,
      "loss": 1.1733,
      "step": 2847
    },
    {
      "epoch": 0.40039364543793055,
      "grad_norm": 1.3971939086914062,
      "learning_rate": 0.0001239194123918604,
      "loss": 1.0915,
      "step": 2848
    },
    {
      "epoch": 0.40053423309433434,
      "grad_norm": 1.804437279701233,
      "learning_rate": 0.00012414506886473264,
      "loss": 1.1236,
      "step": 2849
    },
    {
      "epoch": 0.40067482075073807,
      "grad_norm": 1.6832995414733887,
      "learning_rate": 0.00012437059485273123,
      "loss": 1.1256,
      "step": 2850
    },
    {
      "epoch": 0.40081540840714186,
      "grad_norm": 1.412095308303833,
      "learning_rate": 0.00012459598913706796,
      "loss": 1.091,
      "step": 2851
    },
    {
      "epoch": 0.4009559960635456,
      "grad_norm": 1.6633363962173462,
      "learning_rate": 0.0001248212504996661,
      "loss": 1.0493,
      "step": 2852
    },
    {
      "epoch": 0.4010965837199494,
      "grad_norm": 1.4391154050827026,
      "learning_rate": 0.00012504637772316727,
      "loss": 1.0843,
      "step": 2853
    },
    {
      "epoch": 0.4012371713763532,
      "grad_norm": 1.5341330766677856,
      "learning_rate": 0.00012527136959093805,
      "loss": 1.0699,
      "step": 2854
    },
    {
      "epoch": 0.4013777590327569,
      "grad_norm": 1.4618231058120728,
      "learning_rate": 0.00012549622488707666,
      "loss": 1.0547,
      "step": 2855
    },
    {
      "epoch": 0.4015183466891607,
      "grad_norm": 1.6272594928741455,
      "learning_rate": 0.0001257209423964192,
      "loss": 1.0922,
      "step": 2856
    },
    {
      "epoch": 0.40165893434556443,
      "grad_norm": 1.420209527015686,
      "learning_rate": 0.0001259455209045464,
      "loss": 1.0182,
      "step": 2857
    },
    {
      "epoch": 0.4017995220019682,
      "grad_norm": 1.3456954956054688,
      "learning_rate": 0.0001261699591977902,
      "loss": 1.0612,
      "step": 2858
    },
    {
      "epoch": 0.401940109658372,
      "grad_norm": 1.637459635734558,
      "learning_rate": 0.00012639425606324043,
      "loss": 1.0711,
      "step": 2859
    },
    {
      "epoch": 0.40208069731477575,
      "grad_norm": 1.6655423641204834,
      "learning_rate": 0.0001266184102887511,
      "loss": 1.1509,
      "step": 2860
    },
    {
      "epoch": 0.40222128497117954,
      "grad_norm": 1.3559671640396118,
      "learning_rate": 0.00012684242066294692,
      "loss": 1.1669,
      "step": 2861
    },
    {
      "epoch": 0.4023618726275833,
      "grad_norm": 1.420451283454895,
      "learning_rate": 0.00012706628597523035,
      "loss": 1.0183,
      "step": 2862
    },
    {
      "epoch": 0.40250246028398706,
      "grad_norm": 1.342098593711853,
      "learning_rate": 0.00012729000501578744,
      "loss": 1.3323,
      "step": 2863
    },
    {
      "epoch": 0.40264304794039085,
      "grad_norm": 1.6255697011947632,
      "learning_rate": 0.00012751357657559518,
      "loss": 1.209,
      "step": 2864
    },
    {
      "epoch": 0.4027836355967946,
      "grad_norm": 1.3923523426055908,
      "learning_rate": 0.00012773699944642694,
      "loss": 0.9507,
      "step": 2865
    },
    {
      "epoch": 0.4029242232531984,
      "grad_norm": 1.8647217750549316,
      "learning_rate": 0.00012796027242086027,
      "loss": 1.0069,
      "step": 2866
    },
    {
      "epoch": 0.4030648109096021,
      "grad_norm": 1.5726319551467896,
      "learning_rate": 0.00012818339429228228,
      "loss": 1.0048,
      "step": 2867
    },
    {
      "epoch": 0.4032053985660059,
      "grad_norm": 1.6928491592407227,
      "learning_rate": 0.0001284063638548972,
      "loss": 1.0711,
      "step": 2868
    },
    {
      "epoch": 0.4033459862224097,
      "grad_norm": 1.3873882293701172,
      "learning_rate": 0.0001286291799037317,
      "loss": 1.0502,
      "step": 2869
    },
    {
      "epoch": 0.4034865738788134,
      "grad_norm": 1.609147310256958,
      "learning_rate": 0.0001288518412346427,
      "loss": 1.1799,
      "step": 2870
    },
    {
      "epoch": 0.4036271615352172,
      "grad_norm": 2.014526128768921,
      "learning_rate": 0.00012907434664432286,
      "loss": 1.1634,
      "step": 2871
    },
    {
      "epoch": 0.40376774919162095,
      "grad_norm": 1.588838815689087,
      "learning_rate": 0.00012929669493030776,
      "loss": 1.1438,
      "step": 2872
    },
    {
      "epoch": 0.40390833684802474,
      "grad_norm": 1.5564947128295898,
      "learning_rate": 0.00012951888489098169,
      "loss": 1.1072,
      "step": 2873
    },
    {
      "epoch": 0.40404892450442853,
      "grad_norm": 1.3501005172729492,
      "learning_rate": 0.00012974091532558495,
      "loss": 1.0458,
      "step": 2874
    },
    {
      "epoch": 0.40418951216083226,
      "grad_norm": 1.6562769412994385,
      "learning_rate": 0.0001299627850342198,
      "loss": 1.099,
      "step": 2875
    },
    {
      "epoch": 0.40433009981723605,
      "grad_norm": 1.8822752237319946,
      "learning_rate": 0.00013018449281785725,
      "loss": 0.9899,
      "step": 2876
    },
    {
      "epoch": 0.4044706874736398,
      "grad_norm": 1.6780750751495361,
      "learning_rate": 0.00013040603747834295,
      "loss": 1.0572,
      "step": 2877
    },
    {
      "epoch": 0.4046112751300436,
      "grad_norm": 1.299143671989441,
      "learning_rate": 0.00013062741781840465,
      "loss": 1.3015,
      "step": 2878
    },
    {
      "epoch": 0.40475186278644737,
      "grad_norm": 1.4591505527496338,
      "learning_rate": 0.0001308486326416578,
      "loss": 1.0704,
      "step": 2879
    },
    {
      "epoch": 0.4048924504428511,
      "grad_norm": 1.6327036619186401,
      "learning_rate": 0.00013106968075261236,
      "loss": 1.2162,
      "step": 2880
    },
    {
      "epoch": 0.4050330380992549,
      "grad_norm": 1.654765009880066,
      "learning_rate": 0.00013129056095667933,
      "loss": 1.0945,
      "step": 2881
    },
    {
      "epoch": 0.40517362575565863,
      "grad_norm": 1.3370532989501953,
      "learning_rate": 0.00013151127206017694,
      "loss": 1.1783,
      "step": 2882
    },
    {
      "epoch": 0.4053142134120624,
      "grad_norm": 1.607530951499939,
      "learning_rate": 0.00013173181287033772,
      "loss": 1.074,
      "step": 2883
    },
    {
      "epoch": 0.4054548010684662,
      "grad_norm": 1.4095925092697144,
      "learning_rate": 0.00013195218219531375,
      "loss": 1.2326,
      "step": 2884
    },
    {
      "epoch": 0.40559538872486994,
      "grad_norm": 1.7238799333572388,
      "learning_rate": 0.00013217237884418463,
      "loss": 1.0588,
      "step": 2885
    },
    {
      "epoch": 0.40573597638127373,
      "grad_norm": 1.558576226234436,
      "learning_rate": 0.00013239240162696254,
      "loss": 1.1278,
      "step": 2886
    },
    {
      "epoch": 0.40587656403767747,
      "grad_norm": 1.8359451293945312,
      "learning_rate": 0.00013261224935459982,
      "loss": 1.0794,
      "step": 2887
    },
    {
      "epoch": 0.40601715169408126,
      "grad_norm": 1.313831090927124,
      "learning_rate": 0.00013283192083899417,
      "loss": 1.2111,
      "step": 2888
    },
    {
      "epoch": 0.40615773935048505,
      "grad_norm": 1.7834773063659668,
      "learning_rate": 0.00013305141489299637,
      "loss": 0.9836,
      "step": 2889
    },
    {
      "epoch": 0.4062983270068888,
      "grad_norm": 1.4945993423461914,
      "learning_rate": 0.00013327073033041567,
      "loss": 1.1248,
      "step": 2890
    },
    {
      "epoch": 0.40643891466329257,
      "grad_norm": 1.5489299297332764,
      "learning_rate": 0.00013348986596602698,
      "loss": 1.1626,
      "step": 2891
    },
    {
      "epoch": 0.4065795023196963,
      "grad_norm": 1.428887963294983,
      "learning_rate": 0.0001337088206155763,
      "loss": 1.2761,
      "step": 2892
    },
    {
      "epoch": 0.4067200899761001,
      "grad_norm": 1.4626760482788086,
      "learning_rate": 0.00013392759309578838,
      "loss": 1.2295,
      "step": 2893
    },
    {
      "epoch": 0.4068606776325039,
      "grad_norm": 1.5018242597579956,
      "learning_rate": 0.00013414618222437193,
      "loss": 0.9411,
      "step": 2894
    },
    {
      "epoch": 0.4070012652889076,
      "grad_norm": 1.4080866575241089,
      "learning_rate": 0.000134364586820027,
      "loss": 1.0266,
      "step": 2895
    },
    {
      "epoch": 0.4071418529453114,
      "grad_norm": 1.7509691715240479,
      "learning_rate": 0.00013458280570245035,
      "loss": 0.9685,
      "step": 2896
    },
    {
      "epoch": 0.40728244060171515,
      "grad_norm": 1.6060086488723755,
      "learning_rate": 0.00013480083769234287,
      "loss": 1.0531,
      "step": 2897
    },
    {
      "epoch": 0.40742302825811894,
      "grad_norm": 1.5436334609985352,
      "learning_rate": 0.00013501868161141514,
      "loss": 1.0758,
      "step": 2898
    },
    {
      "epoch": 0.4075636159145227,
      "grad_norm": 1.502274513244629,
      "learning_rate": 0.0001352363362823943,
      "loss": 1.1289,
      "step": 2899
    },
    {
      "epoch": 0.40770420357092646,
      "grad_norm": 1.5212819576263428,
      "learning_rate": 0.00013545380052903006,
      "loss": 1.1647,
      "step": 2900
    },
    {
      "epoch": 0.40784479122733025,
      "grad_norm": 1.7440624237060547,
      "learning_rate": 0.00013567107317610127,
      "loss": 1.1144,
      "step": 2901
    },
    {
      "epoch": 0.407985378883734,
      "grad_norm": 1.4481451511383057,
      "learning_rate": 0.0001358881530494224,
      "loss": 1.1747,
      "step": 2902
    },
    {
      "epoch": 0.4081259665401378,
      "grad_norm": 1.5458532571792603,
      "learning_rate": 0.00013610503897584948,
      "loss": 1.1451,
      "step": 2903
    },
    {
      "epoch": 0.40826655419654156,
      "grad_norm": 1.6322726011276245,
      "learning_rate": 0.00013632172978328685,
      "loss": 1.0914,
      "step": 2904
    },
    {
      "epoch": 0.4084071418529453,
      "grad_norm": 1.4892418384552002,
      "learning_rate": 0.000136538224300693,
      "loss": 0.9638,
      "step": 2905
    },
    {
      "epoch": 0.4085477295093491,
      "grad_norm": 1.4691433906555176,
      "learning_rate": 0.00013675452135808767,
      "loss": 1.0652,
      "step": 2906
    },
    {
      "epoch": 0.4086883171657528,
      "grad_norm": 1.6278977394104004,
      "learning_rate": 0.00013697061978655737,
      "loss": 0.9694,
      "step": 2907
    },
    {
      "epoch": 0.4088289048221566,
      "grad_norm": 1.4346688985824585,
      "learning_rate": 0.00013718651841826215,
      "loss": 1.0892,
      "step": 2908
    },
    {
      "epoch": 0.4089694924785604,
      "grad_norm": 1.728352665901184,
      "learning_rate": 0.00013740221608644175,
      "loss": 1.065,
      "step": 2909
    },
    {
      "epoch": 0.40911008013496414,
      "grad_norm": 1.300907850265503,
      "learning_rate": 0.00013761771162542212,
      "loss": 1.1333,
      "step": 2910
    },
    {
      "epoch": 0.40925066779136793,
      "grad_norm": 1.6274948120117188,
      "learning_rate": 0.0001378330038706214,
      "loss": 1.0331,
      "step": 2911
    },
    {
      "epoch": 0.40939125544777166,
      "grad_norm": 1.4976552724838257,
      "learning_rate": 0.00013804809165855639,
      "loss": 1.2377,
      "step": 2912
    },
    {
      "epoch": 0.40953184310417545,
      "grad_norm": 1.7361762523651123,
      "learning_rate": 0.00013826297382684878,
      "loss": 1.1758,
      "step": 2913
    },
    {
      "epoch": 0.40967243076057924,
      "grad_norm": 1.5712218284606934,
      "learning_rate": 0.00013847764921423169,
      "loss": 1.0705,
      "step": 2914
    },
    {
      "epoch": 0.409813018416983,
      "grad_norm": 1.412294626235962,
      "learning_rate": 0.00013869211666055545,
      "loss": 1.0501,
      "step": 2915
    },
    {
      "epoch": 0.40995360607338677,
      "grad_norm": 1.6149150133132935,
      "learning_rate": 0.00013890637500679434,
      "loss": 1.1581,
      "step": 2916
    },
    {
      "epoch": 0.4100941937297905,
      "grad_norm": 1.4340790510177612,
      "learning_rate": 0.00013912042309505244,
      "loss": 1.011,
      "step": 2917
    },
    {
      "epoch": 0.4102347813861943,
      "grad_norm": 1.5137197971343994,
      "learning_rate": 0.00013933425976857052,
      "loss": 1.1836,
      "step": 2918
    },
    {
      "epoch": 0.4103753690425981,
      "grad_norm": 1.4428815841674805,
      "learning_rate": 0.00013954788387173138,
      "loss": 1.0942,
      "step": 2919
    },
    {
      "epoch": 0.4105159566990018,
      "grad_norm": 1.600741982460022,
      "learning_rate": 0.00013976129425006682,
      "loss": 1.0992,
      "step": 2920
    },
    {
      "epoch": 0.4106565443554056,
      "grad_norm": 1.9680043458938599,
      "learning_rate": 0.00013997448975026382,
      "loss": 1.1282,
      "step": 2921
    },
    {
      "epoch": 0.41079713201180934,
      "grad_norm": 1.424501657485962,
      "learning_rate": 0.00014018746922017035,
      "loss": 1.0927,
      "step": 2922
    },
    {
      "epoch": 0.41093771966821313,
      "grad_norm": 1.5407642126083374,
      "learning_rate": 0.00014040023150880193,
      "loss": 1.1138,
      "step": 2923
    },
    {
      "epoch": 0.4110783073246169,
      "grad_norm": 1.5971639156341553,
      "learning_rate": 0.0001406127754663477,
      "loss": 1.2358,
      "step": 2924
    },
    {
      "epoch": 0.41121889498102066,
      "grad_norm": 1.3560789823532104,
      "learning_rate": 0.00014082509994417703,
      "loss": 1.1417,
      "step": 2925
    },
    {
      "epoch": 0.41135948263742445,
      "grad_norm": 1.566992998123169,
      "learning_rate": 0.00014103720379484498,
      "loss": 1.1302,
      "step": 2926
    },
    {
      "epoch": 0.4115000702938282,
      "grad_norm": 1.4654635190963745,
      "learning_rate": 0.00014124908587209923,
      "loss": 0.9713,
      "step": 2927
    },
    {
      "epoch": 0.41164065795023197,
      "grad_norm": 1.574826717376709,
      "learning_rate": 0.0001414607450308856,
      "loss": 1.1673,
      "step": 2928
    },
    {
      "epoch": 0.41178124560663576,
      "grad_norm": 1.8358408212661743,
      "learning_rate": 0.0001416721801273552,
      "loss": 1.1911,
      "step": 2929
    },
    {
      "epoch": 0.4119218332630395,
      "grad_norm": 1.839680552482605,
      "learning_rate": 0.00014188339001886942,
      "loss": 1.079,
      "step": 2930
    },
    {
      "epoch": 0.4120624209194433,
      "grad_norm": 1.4203612804412842,
      "learning_rate": 0.0001420943735640071,
      "loss": 1.1397,
      "step": 2931
    },
    {
      "epoch": 0.412203008575847,
      "grad_norm": 1.441125750541687,
      "learning_rate": 0.00014230512962257,
      "loss": 0.9713,
      "step": 2932
    },
    {
      "epoch": 0.4123435962322508,
      "grad_norm": 1.3836162090301514,
      "learning_rate": 0.00014251565705558958,
      "loss": 1.1895,
      "step": 2933
    },
    {
      "epoch": 0.4124841838886546,
      "grad_norm": 1.5409700870513916,
      "learning_rate": 0.0001427259547253326,
      "loss": 0.8422,
      "step": 2934
    },
    {
      "epoch": 0.41262477154505833,
      "grad_norm": 1.5604654550552368,
      "learning_rate": 0.00014293602149530765,
      "loss": 0.9874,
      "step": 2935
    },
    {
      "epoch": 0.4127653592014621,
      "grad_norm": 1.4251446723937988,
      "learning_rate": 0.00014314585623027094,
      "loss": 1.1511,
      "step": 2936
    },
    {
      "epoch": 0.41290594685786586,
      "grad_norm": 1.5880742073059082,
      "learning_rate": 0.000143355457796233,
      "loss": 1.1298,
      "step": 2937
    },
    {
      "epoch": 0.41304653451426965,
      "grad_norm": 1.6004259586334229,
      "learning_rate": 0.0001435648250604642,
      "loss": 1.0671,
      "step": 2938
    },
    {
      "epoch": 0.41318712217067344,
      "grad_norm": 1.5575200319290161,
      "learning_rate": 0.00014377395689150094,
      "loss": 1.1502,
      "step": 2939
    },
    {
      "epoch": 0.41332770982707717,
      "grad_norm": 1.8404197692871094,
      "learning_rate": 0.00014398285215915245,
      "loss": 1.0864,
      "step": 2940
    },
    {
      "epoch": 0.41346829748348096,
      "grad_norm": 1.7299280166625977,
      "learning_rate": 0.00014419150973450596,
      "loss": 1.0222,
      "step": 2941
    },
    {
      "epoch": 0.4136088851398847,
      "grad_norm": 1.6255651712417603,
      "learning_rate": 0.00014439992848993362,
      "loss": 1.1334,
      "step": 2942
    },
    {
      "epoch": 0.4137494727962885,
      "grad_norm": 1.5738003253936768,
      "learning_rate": 0.00014460810729909768,
      "loss": 1.2508,
      "step": 2943
    },
    {
      "epoch": 0.4138900604526923,
      "grad_norm": 1.6456865072250366,
      "learning_rate": 0.00014481604503695765,
      "loss": 1.0012,
      "step": 2944
    },
    {
      "epoch": 0.414030648109096,
      "grad_norm": 1.5446182489395142,
      "learning_rate": 0.0001450237405797755,
      "loss": 1.0496,
      "step": 2945
    },
    {
      "epoch": 0.4141712357654998,
      "grad_norm": 1.515960931777954,
      "learning_rate": 0.00014523119280512235,
      "loss": 1.0741,
      "step": 2946
    },
    {
      "epoch": 0.41431182342190354,
      "grad_norm": 1.477173089981079,
      "learning_rate": 0.00014543840059188384,
      "loss": 1.0405,
      "step": 2947
    },
    {
      "epoch": 0.4144524110783073,
      "grad_norm": 1.5881997346878052,
      "learning_rate": 0.00014564536282026707,
      "loss": 1.1181,
      "step": 2948
    },
    {
      "epoch": 0.4145929987347111,
      "grad_norm": 1.6715365648269653,
      "learning_rate": 0.0001458520783718058,
      "loss": 1.0741,
      "step": 2949
    },
    {
      "epoch": 0.41473358639111485,
      "grad_norm": 1.6618218421936035,
      "learning_rate": 0.00014605854612936728,
      "loss": 1.0791,
      "step": 2950
    },
    {
      "epoch": 0.41487417404751864,
      "grad_norm": 1.33306884765625,
      "learning_rate": 0.0001462647649771574,
      "loss": 1.127,
      "step": 2951
    },
    {
      "epoch": 0.4150147617039224,
      "grad_norm": 1.7019565105438232,
      "learning_rate": 0.00014647073380072767,
      "loss": 1.0308,
      "step": 2952
    },
    {
      "epoch": 0.41515534936032616,
      "grad_norm": 1.4814045429229736,
      "learning_rate": 0.00014667645148698047,
      "loss": 1.1589,
      "step": 2953
    },
    {
      "epoch": 0.41529593701672995,
      "grad_norm": 1.4768359661102295,
      "learning_rate": 0.00014688191692417566,
      "loss": 1.0956,
      "step": 2954
    },
    {
      "epoch": 0.4154365246731337,
      "grad_norm": 1.5776194334030151,
      "learning_rate": 0.00014708712900193585,
      "loss": 1.0253,
      "step": 2955
    },
    {
      "epoch": 0.4155771123295375,
      "grad_norm": 1.6728999614715576,
      "learning_rate": 0.00014729208661125345,
      "loss": 1.1096,
      "step": 2956
    },
    {
      "epoch": 0.4157176999859412,
      "grad_norm": 1.5672919750213623,
      "learning_rate": 0.00014749678864449565,
      "loss": 1.1028,
      "step": 2957
    },
    {
      "epoch": 0.415858287642345,
      "grad_norm": 1.6628248691558838,
      "learning_rate": 0.00014770123399541078,
      "loss": 1.0709,
      "step": 2958
    },
    {
      "epoch": 0.4159988752987488,
      "grad_norm": 1.4324936866760254,
      "learning_rate": 0.00014790542155913473,
      "loss": 1.0271,
      "step": 2959
    },
    {
      "epoch": 0.41613946295515253,
      "grad_norm": 1.852441430091858,
      "learning_rate": 0.00014810935023219608,
      "loss": 1.0764,
      "step": 2960
    },
    {
      "epoch": 0.4162800506115563,
      "grad_norm": 1.4862960577011108,
      "learning_rate": 0.00014831301891252298,
      "loss": 1.0403,
      "step": 2961
    },
    {
      "epoch": 0.41642063826796005,
      "grad_norm": 1.65050208568573,
      "learning_rate": 0.00014851642649944805,
      "loss": 0.9827,
      "step": 2962
    },
    {
      "epoch": 0.41656122592436384,
      "grad_norm": 1.494929552078247,
      "learning_rate": 0.00014871957189371545,
      "loss": 1.2136,
      "step": 2963
    },
    {
      "epoch": 0.41670181358076763,
      "grad_norm": 1.961538314819336,
      "learning_rate": 0.00014892245399748596,
      "loss": 1.0617,
      "step": 2964
    },
    {
      "epoch": 0.41684240123717137,
      "grad_norm": 1.4828922748565674,
      "learning_rate": 0.00014912507171434354,
      "loss": 1.2917,
      "step": 2965
    },
    {
      "epoch": 0.41698298889357516,
      "grad_norm": 1.38355553150177,
      "learning_rate": 0.0001493274239493004,
      "loss": 1.239,
      "step": 2966
    },
    {
      "epoch": 0.4171235765499789,
      "grad_norm": 1.7497987747192383,
      "learning_rate": 0.0001495295096088041,
      "loss": 0.9614,
      "step": 2967
    },
    {
      "epoch": 0.4172641642063827,
      "grad_norm": 1.6070762872695923,
      "learning_rate": 0.00014973132760074238,
      "loss": 1.1154,
      "step": 2968
    },
    {
      "epoch": 0.41740475186278647,
      "grad_norm": 1.5242149829864502,
      "learning_rate": 0.0001499328768344499,
      "loss": 0.9579,
      "step": 2969
    },
    {
      "epoch": 0.4175453395191902,
      "grad_norm": 1.4175968170166016,
      "learning_rate": 0.00015013415622071321,
      "loss": 1.0251,
      "step": 2970
    },
    {
      "epoch": 0.417685927175594,
      "grad_norm": 1.5839974880218506,
      "learning_rate": 0.0001503351646717777,
      "loss": 1.0758,
      "step": 2971
    },
    {
      "epoch": 0.41782651483199773,
      "grad_norm": 1.546884536743164,
      "learning_rate": 0.00015053590110135261,
      "loss": 1.0589,
      "step": 2972
    },
    {
      "epoch": 0.4179671024884015,
      "grad_norm": 1.4209792613983154,
      "learning_rate": 0.00015073636442461757,
      "loss": 0.9951,
      "step": 2973
    },
    {
      "epoch": 0.4181076901448053,
      "grad_norm": 1.4988601207733154,
      "learning_rate": 0.0001509365535582276,
      "loss": 1.072,
      "step": 2974
    },
    {
      "epoch": 0.41824827780120905,
      "grad_norm": 1.4333910942077637,
      "learning_rate": 0.00015113646742032007,
      "loss": 1.258,
      "step": 2975
    },
    {
      "epoch": 0.41838886545761284,
      "grad_norm": 1.5245790481567383,
      "learning_rate": 0.00015133610493051967,
      "loss": 1.0192,
      "step": 2976
    },
    {
      "epoch": 0.41852945311401657,
      "grad_norm": 1.6075663566589355,
      "learning_rate": 0.00015153546500994456,
      "loss": 1.209,
      "step": 2977
    },
    {
      "epoch": 0.41867004077042036,
      "grad_norm": 1.5810633897781372,
      "learning_rate": 0.00015173454658121225,
      "loss": 1.0476,
      "step": 2978
    },
    {
      "epoch": 0.41881062842682415,
      "grad_norm": 1.565464973449707,
      "learning_rate": 0.00015193334856844522,
      "loss": 1.0102,
      "step": 2979
    },
    {
      "epoch": 0.4189512160832279,
      "grad_norm": 1.355522871017456,
      "learning_rate": 0.00015213186989727727,
      "loss": 1.0545,
      "step": 2980
    },
    {
      "epoch": 0.4190918037396317,
      "grad_norm": 1.5387210845947266,
      "learning_rate": 0.00015233010949485855,
      "loss": 1.1499,
      "step": 2981
    },
    {
      "epoch": 0.4192323913960354,
      "grad_norm": 1.5735410451889038,
      "learning_rate": 0.00015252806628986184,
      "loss": 1.0572,
      "step": 2982
    },
    {
      "epoch": 0.4193729790524392,
      "grad_norm": 1.4129856824874878,
      "learning_rate": 0.00015272573921248822,
      "loss": 1.0532,
      "step": 2983
    },
    {
      "epoch": 0.419513566708843,
      "grad_norm": 1.6190142631530762,
      "learning_rate": 0.00015292312719447307,
      "loss": 1.1817,
      "step": 2984
    },
    {
      "epoch": 0.4196541543652467,
      "grad_norm": 1.4294264316558838,
      "learning_rate": 0.00015312022916909144,
      "loss": 1.1697,
      "step": 2985
    },
    {
      "epoch": 0.4197947420216505,
      "grad_norm": 1.5456002950668335,
      "learning_rate": 0.00015331704407116407,
      "loss": 1.0864,
      "step": 2986
    },
    {
      "epoch": 0.41993532967805425,
      "grad_norm": 2.660444974899292,
      "learning_rate": 0.00015351357083706304,
      "loss": 1.1426,
      "step": 2987
    },
    {
      "epoch": 0.42007591733445804,
      "grad_norm": 1.4615892171859741,
      "learning_rate": 0.00015370980840471782,
      "loss": 1.0267,
      "step": 2988
    },
    {
      "epoch": 0.4202165049908618,
      "grad_norm": 1.3637720346450806,
      "learning_rate": 0.00015390575571362048,
      "loss": 1.117,
      "step": 2989
    },
    {
      "epoch": 0.42035709264726556,
      "grad_norm": 1.4376306533813477,
      "learning_rate": 0.00015410141170483187,
      "loss": 1.2871,
      "step": 2990
    },
    {
      "epoch": 0.42049768030366935,
      "grad_norm": 1.4531259536743164,
      "learning_rate": 0.00015429677532098702,
      "loss": 1.043,
      "step": 2991
    },
    {
      "epoch": 0.4206382679600731,
      "grad_norm": 1.671582579612732,
      "learning_rate": 0.0001544918455063013,
      "loss": 1.168,
      "step": 2992
    },
    {
      "epoch": 0.4207788556164769,
      "grad_norm": 1.9900532960891724,
      "learning_rate": 0.0001546866212065756,
      "loss": 1.0291,
      "step": 2993
    },
    {
      "epoch": 0.42091944327288067,
      "grad_norm": 1.4720362424850464,
      "learning_rate": 0.00015488110136920228,
      "loss": 1.144,
      "step": 2994
    },
    {
      "epoch": 0.4210600309292844,
      "grad_norm": 1.6281218528747559,
      "learning_rate": 0.0001550752849431709,
      "loss": 1.1344,
      "step": 2995
    },
    {
      "epoch": 0.4212006185856882,
      "grad_norm": 1.6025186777114868,
      "learning_rate": 0.00015526917087907405,
      "loss": 1.0855,
      "step": 2996
    },
    {
      "epoch": 0.4213412062420919,
      "grad_norm": 1.5312020778656006,
      "learning_rate": 0.00015546275812911237,
      "loss": 1.0624,
      "step": 2997
    },
    {
      "epoch": 0.4214817938984957,
      "grad_norm": 1.6187868118286133,
      "learning_rate": 0.0001556560456471009,
      "loss": 1.1644,
      "step": 2998
    },
    {
      "epoch": 0.4216223815548995,
      "grad_norm": 1.5114648342132568,
      "learning_rate": 0.00015584903238847467,
      "loss": 1.0112,
      "step": 2999
    },
    {
      "epoch": 0.42176296921130324,
      "grad_norm": 1.5982730388641357,
      "learning_rate": 0.0001560417173102939,
      "loss": 1.0994,
      "step": 3000
    },
    {
      "epoch": 0.42176296921130324,
      "eval_loss": 1.1572028398513794,
      "eval_runtime": 771.8801,
      "eval_samples_per_second": 16.383,
      "eval_steps_per_second": 8.192,
      "step": 3000
    },
    {
      "epoch": 0.42190355686770703,
      "grad_norm": 1.5199434757232666,
      "learning_rate": 0.00015623409937125,
      "loss": 1.1533,
      "step": 3001
    },
    {
      "epoch": 0.42204414452411076,
      "grad_norm": 1.4094321727752686,
      "learning_rate": 0.00015642617753167104,
      "loss": 1.0087,
      "step": 3002
    },
    {
      "epoch": 0.42218473218051455,
      "grad_norm": 1.625590443611145,
      "learning_rate": 0.00015661795075352762,
      "loss": 1.0713,
      "step": 3003
    },
    {
      "epoch": 0.42232531983691834,
      "grad_norm": 1.6807918548583984,
      "learning_rate": 0.0001568094180004381,
      "loss": 0.9579,
      "step": 3004
    },
    {
      "epoch": 0.4224659074933221,
      "grad_norm": 1.6047941446304321,
      "learning_rate": 0.00015700057823767445,
      "loss": 1.2268,
      "step": 3005
    },
    {
      "epoch": 0.42260649514972587,
      "grad_norm": 1.5363056659698486,
      "learning_rate": 0.00015719143043216768,
      "loss": 0.9944,
      "step": 3006
    },
    {
      "epoch": 0.4227470828061296,
      "grad_norm": 1.4162005186080933,
      "learning_rate": 0.00015738197355251386,
      "loss": 1.0533,
      "step": 3007
    },
    {
      "epoch": 0.4228876704625334,
      "grad_norm": 1.8770489692687988,
      "learning_rate": 0.00015757220656897896,
      "loss": 1.0291,
      "step": 3008
    },
    {
      "epoch": 0.4230282581189372,
      "grad_norm": 1.4308953285217285,
      "learning_rate": 0.00015776212845350503,
      "loss": 1.1729,
      "step": 3009
    },
    {
      "epoch": 0.4231688457753409,
      "grad_norm": 1.8837121725082397,
      "learning_rate": 0.0001579517381797154,
      "loss": 0.9711,
      "step": 3010
    },
    {
      "epoch": 0.4233094334317447,
      "grad_norm": 1.2511197328567505,
      "learning_rate": 0.00015814103472292068,
      "loss": 1.1842,
      "step": 3011
    },
    {
      "epoch": 0.42345002108814844,
      "grad_norm": 1.6152673959732056,
      "learning_rate": 0.00015833001706012362,
      "loss": 1.2408,
      "step": 3012
    },
    {
      "epoch": 0.42359060874455223,
      "grad_norm": 1.347214698791504,
      "learning_rate": 0.00015851868417002512,
      "loss": 1.2525,
      "step": 3013
    },
    {
      "epoch": 0.423731196400956,
      "grad_norm": 1.5896176099777222,
      "learning_rate": 0.0001587070350330297,
      "loss": 0.9658,
      "step": 3014
    },
    {
      "epoch": 0.42387178405735976,
      "grad_norm": 1.7637876272201538,
      "learning_rate": 0.00015889506863125095,
      "loss": 1.0555,
      "step": 3015
    },
    {
      "epoch": 0.42401237171376355,
      "grad_norm": 1.6514075994491577,
      "learning_rate": 0.00015908278394851706,
      "loss": 1.1692,
      "step": 3016
    },
    {
      "epoch": 0.4241529593701673,
      "grad_norm": 1.486755132675171,
      "learning_rate": 0.00015927017997037593,
      "loss": 1.0373,
      "step": 3017
    },
    {
      "epoch": 0.42429354702657107,
      "grad_norm": 1.7084097862243652,
      "learning_rate": 0.00015945725568410147,
      "loss": 1.0924,
      "step": 3018
    },
    {
      "epoch": 0.42443413468297486,
      "grad_norm": 1.3989841938018799,
      "learning_rate": 0.00015964401007869826,
      "loss": 1.2461,
      "step": 3019
    },
    {
      "epoch": 0.4245747223393786,
      "grad_norm": 1.4139690399169922,
      "learning_rate": 0.00015983044214490767,
      "loss": 1.1871,
      "step": 3020
    },
    {
      "epoch": 0.4247153099957824,
      "grad_norm": 1.5244067907333374,
      "learning_rate": 0.00016001655087521263,
      "loss": 1.0598,
      "step": 3021
    },
    {
      "epoch": 0.4248558976521861,
      "grad_norm": 1.736275553703308,
      "learning_rate": 0.00016020233526384373,
      "loss": 0.9674,
      "step": 3022
    },
    {
      "epoch": 0.4249964853085899,
      "grad_norm": 1.4941325187683105,
      "learning_rate": 0.0001603877943067842,
      "loss": 1.03,
      "step": 3023
    },
    {
      "epoch": 0.4251370729649937,
      "grad_norm": 1.5721123218536377,
      "learning_rate": 0.00016057292700177574,
      "loss": 0.958,
      "step": 3024
    },
    {
      "epoch": 0.42527766062139744,
      "grad_norm": 1.823833703994751,
      "learning_rate": 0.00016075773234832324,
      "loss": 1.2836,
      "step": 3025
    },
    {
      "epoch": 0.4254182482778012,
      "grad_norm": 1.4654356241226196,
      "learning_rate": 0.0001609422093477012,
      "loss": 1.0804,
      "step": 3026
    },
    {
      "epoch": 0.42555883593420496,
      "grad_norm": 1.8365566730499268,
      "learning_rate": 0.0001611263570029581,
      "loss": 1.0859,
      "step": 3027
    },
    {
      "epoch": 0.42569942359060875,
      "grad_norm": 1.6284703016281128,
      "learning_rate": 0.00016131017431892278,
      "loss": 1.1184,
      "step": 3028
    },
    {
      "epoch": 0.42584001124701254,
      "grad_norm": 1.611696720123291,
      "learning_rate": 0.00016149366030220866,
      "loss": 1.1583,
      "step": 3029
    },
    {
      "epoch": 0.4259805989034163,
      "grad_norm": 1.577688217163086,
      "learning_rate": 0.00016167681396122029,
      "loss": 1.2137,
      "step": 3030
    },
    {
      "epoch": 0.42612118655982006,
      "grad_norm": 1.4833835363388062,
      "learning_rate": 0.00016185963430615785,
      "loss": 1.0918,
      "step": 3031
    },
    {
      "epoch": 0.4262617742162238,
      "grad_norm": 1.4100712537765503,
      "learning_rate": 0.00016204212034902314,
      "loss": 1.1324,
      "step": 3032
    },
    {
      "epoch": 0.4264023618726276,
      "grad_norm": 1.4558285474777222,
      "learning_rate": 0.00016222427110362407,
      "loss": 1.1077,
      "step": 3033
    },
    {
      "epoch": 0.4265429495290313,
      "grad_norm": 1.5927988290786743,
      "learning_rate": 0.0001624060855855811,
      "loss": 1.2149,
      "step": 3034
    },
    {
      "epoch": 0.4266835371854351,
      "grad_norm": 1.3322768211364746,
      "learning_rate": 0.00016258756281233169,
      "loss": 1.2117,
      "step": 3035
    },
    {
      "epoch": 0.4268241248418389,
      "grad_norm": 1.5151448249816895,
      "learning_rate": 0.0001627687018031357,
      "loss": 1.1246,
      "step": 3036
    },
    {
      "epoch": 0.42696471249824264,
      "grad_norm": 1.7026020288467407,
      "learning_rate": 0.00016294950157908132,
      "loss": 1.063,
      "step": 3037
    },
    {
      "epoch": 0.42710530015464643,
      "grad_norm": 1.6033897399902344,
      "learning_rate": 0.00016312996116308955,
      "loss": 1.0555,
      "step": 3038
    },
    {
      "epoch": 0.42724588781105016,
      "grad_norm": 1.4728339910507202,
      "learning_rate": 0.00016331007957992027,
      "loss": 1.0439,
      "step": 3039
    },
    {
      "epoch": 0.42738647546745395,
      "grad_norm": 1.6386981010437012,
      "learning_rate": 0.00016348985585617652,
      "loss": 1.3243,
      "step": 3040
    },
    {
      "epoch": 0.42752706312385774,
      "grad_norm": 1.3627372980117798,
      "learning_rate": 0.00016366928902031088,
      "loss": 1.2796,
      "step": 3041
    },
    {
      "epoch": 0.4276676507802615,
      "grad_norm": 1.6082005500793457,
      "learning_rate": 0.00016384837810262982,
      "loss": 1.1428,
      "step": 3042
    },
    {
      "epoch": 0.42780823843666527,
      "grad_norm": 1.4834266901016235,
      "learning_rate": 0.00016402712213529967,
      "loss": 1.0866,
      "step": 3043
    },
    {
      "epoch": 0.427948826093069,
      "grad_norm": 1.4153090715408325,
      "learning_rate": 0.00016420552015235096,
      "loss": 1.1137,
      "step": 3044
    },
    {
      "epoch": 0.4280894137494728,
      "grad_norm": 1.8289520740509033,
      "learning_rate": 0.00016438357118968457,
      "loss": 1.2164,
      "step": 3045
    },
    {
      "epoch": 0.4282300014058766,
      "grad_norm": 1.584618091583252,
      "learning_rate": 0.0001645612742850764,
      "loss": 1.1602,
      "step": 3046
    },
    {
      "epoch": 0.4283705890622803,
      "grad_norm": 1.5300061702728271,
      "learning_rate": 0.0001647386284781828,
      "loss": 1.3412,
      "step": 3047
    },
    {
      "epoch": 0.4285111767186841,
      "grad_norm": 1.4399769306182861,
      "learning_rate": 0.00016491563281054533,
      "loss": 1.2017,
      "step": 3048
    },
    {
      "epoch": 0.42865176437508784,
      "grad_norm": 1.8793187141418457,
      "learning_rate": 0.0001650922863255967,
      "loss": 1.2046,
      "step": 3049
    },
    {
      "epoch": 0.42879235203149163,
      "grad_norm": 1.8270939588546753,
      "learning_rate": 0.00016526858806866518,
      "loss": 0.992,
      "step": 3050
    },
    {
      "epoch": 0.4289329396878954,
      "grad_norm": 1.207284927368164,
      "learning_rate": 0.0001654445370869804,
      "loss": 1.1456,
      "step": 3051
    },
    {
      "epoch": 0.42907352734429915,
      "grad_norm": 1.6767165660858154,
      "learning_rate": 0.00016562013242967777,
      "loss": 1.1756,
      "step": 3052
    },
    {
      "epoch": 0.42921411500070294,
      "grad_norm": 1.6849076747894287,
      "learning_rate": 0.0001657953731478044,
      "loss": 1.3141,
      "step": 3053
    },
    {
      "epoch": 0.4293547026571067,
      "grad_norm": 1.5969510078430176,
      "learning_rate": 0.00016597025829432377,
      "loss": 1.2258,
      "step": 3054
    },
    {
      "epoch": 0.42949529031351047,
      "grad_norm": 1.5226308107376099,
      "learning_rate": 0.0001661447869241208,
      "loss": 1.0344,
      "step": 3055
    },
    {
      "epoch": 0.42963587796991426,
      "grad_norm": 1.609743595123291,
      "learning_rate": 0.00016631895809400722,
      "loss": 1.0461,
      "step": 3056
    },
    {
      "epoch": 0.429776465626318,
      "grad_norm": 1.9165412187576294,
      "learning_rate": 0.00016649277086272648,
      "loss": 1.0449,
      "step": 3057
    },
    {
      "epoch": 0.4299170532827218,
      "grad_norm": 1.478758454322815,
      "learning_rate": 0.0001666662242909591,
      "loss": 1.0117,
      "step": 3058
    },
    {
      "epoch": 0.4300576409391255,
      "grad_norm": 1.7024550437927246,
      "learning_rate": 0.00016683931744132733,
      "loss": 1.08,
      "step": 3059
    },
    {
      "epoch": 0.4301982285955293,
      "grad_norm": 2.077120780944824,
      "learning_rate": 0.00016701204937840048,
      "loss": 1.2719,
      "step": 3060
    },
    {
      "epoch": 0.4303388162519331,
      "grad_norm": 1.4326013326644897,
      "learning_rate": 0.00016718441916869988,
      "loss": 1.1236,
      "step": 3061
    },
    {
      "epoch": 0.43047940390833683,
      "grad_norm": 1.5313353538513184,
      "learning_rate": 0.0001673564258807042,
      "loss": 0.9819,
      "step": 3062
    },
    {
      "epoch": 0.4306199915647406,
      "grad_norm": 1.468106985092163,
      "learning_rate": 0.00016752806858485406,
      "loss": 1.2153,
      "step": 3063
    },
    {
      "epoch": 0.43076057922114436,
      "grad_norm": 1.6533043384552002,
      "learning_rate": 0.00016769934635355727,
      "loss": 1.0984,
      "step": 3064
    },
    {
      "epoch": 0.43090116687754815,
      "grad_norm": 1.5463413000106812,
      "learning_rate": 0.00016787025826119382,
      "loss": 1.0949,
      "step": 3065
    },
    {
      "epoch": 0.43104175453395194,
      "grad_norm": 1.5521867275238037,
      "learning_rate": 0.00016804080338412103,
      "loss": 1.069,
      "step": 3066
    },
    {
      "epoch": 0.43118234219035567,
      "grad_norm": 1.5262343883514404,
      "learning_rate": 0.00016821098080067824,
      "loss": 1.1698,
      "step": 3067
    },
    {
      "epoch": 0.43132292984675946,
      "grad_norm": 1.6990517377853394,
      "learning_rate": 0.000168380789591192,
      "loss": 1.0846,
      "step": 3068
    },
    {
      "epoch": 0.4314635175031632,
      "grad_norm": 1.554581642150879,
      "learning_rate": 0.00016855022883798099,
      "loss": 1.0727,
      "step": 3069
    },
    {
      "epoch": 0.431604105159567,
      "grad_norm": 1.6806628704071045,
      "learning_rate": 0.000168719297625361,
      "loss": 1.1985,
      "step": 3070
    },
    {
      "epoch": 0.4317446928159708,
      "grad_norm": 1.4954893589019775,
      "learning_rate": 0.00016888799503964985,
      "loss": 1.0592,
      "step": 3071
    },
    {
      "epoch": 0.4318852804723745,
      "grad_norm": 1.845212459564209,
      "learning_rate": 0.0001690563201691723,
      "loss": 1.1563,
      "step": 3072
    },
    {
      "epoch": 0.4320258681287783,
      "grad_norm": 1.6395840644836426,
      "learning_rate": 0.000169224272104265,
      "loss": 1.0874,
      "step": 3073
    },
    {
      "epoch": 0.43216645578518204,
      "grad_norm": 1.4616835117340088,
      "learning_rate": 0.00016939184993728165,
      "loss": 1.1315,
      "step": 3074
    },
    {
      "epoch": 0.4323070434415858,
      "grad_norm": 1.9215165376663208,
      "learning_rate": 0.0001695590527625973,
      "loss": 1.0752,
      "step": 3075
    },
    {
      "epoch": 0.4324476310979896,
      "grad_norm": 1.4891427755355835,
      "learning_rate": 0.00016972587967661377,
      "loss": 1.2469,
      "step": 3076
    },
    {
      "epoch": 0.43258821875439335,
      "grad_norm": 1.7168625593185425,
      "learning_rate": 0.00016989232977776455,
      "loss": 1.1921,
      "step": 3077
    },
    {
      "epoch": 0.43272880641079714,
      "grad_norm": 1.513603687286377,
      "learning_rate": 0.0001700584021665193,
      "loss": 1.0417,
      "step": 3078
    },
    {
      "epoch": 0.4328693940672009,
      "grad_norm": 1.7891192436218262,
      "learning_rate": 0.00017022409594538905,
      "loss": 1.0575,
      "step": 3079
    },
    {
      "epoch": 0.43300998172360466,
      "grad_norm": 1.579747200012207,
      "learning_rate": 0.00017038941021893064,
      "loss": 1.1787,
      "step": 3080
    },
    {
      "epoch": 0.43315056938000845,
      "grad_norm": 1.6555976867675781,
      "learning_rate": 0.0001705543440937523,
      "loss": 1.2629,
      "step": 3081
    },
    {
      "epoch": 0.4332911570364122,
      "grad_norm": 1.6423802375793457,
      "learning_rate": 0.00017071889667851764,
      "loss": 1.0974,
      "step": 3082
    },
    {
      "epoch": 0.433431744692816,
      "grad_norm": 1.4550368785858154,
      "learning_rate": 0.00017088306708395093,
      "loss": 1.3165,
      "step": 3083
    },
    {
      "epoch": 0.4335723323492197,
      "grad_norm": 1.3182145357131958,
      "learning_rate": 0.0001710468544228418,
      "loss": 1.2327,
      "step": 3084
    },
    {
      "epoch": 0.4337129200056235,
      "grad_norm": 1.4700767993927002,
      "learning_rate": 0.00017121025781005023,
      "loss": 1.1725,
      "step": 3085
    },
    {
      "epoch": 0.4338535076620273,
      "grad_norm": 1.4697145223617554,
      "learning_rate": 0.0001713732763625109,
      "loss": 0.8295,
      "step": 3086
    },
    {
      "epoch": 0.43399409531843103,
      "grad_norm": 1.741134762763977,
      "learning_rate": 0.00017153590919923833,
      "loss": 1.156,
      "step": 3087
    },
    {
      "epoch": 0.4341346829748348,
      "grad_norm": 1.8644394874572754,
      "learning_rate": 0.00017169815544133154,
      "loss": 1.2204,
      "step": 3088
    },
    {
      "epoch": 0.43427527063123855,
      "grad_norm": 1.5213323831558228,
      "learning_rate": 0.0001718600142119788,
      "loss": 1.0984,
      "step": 3089
    },
    {
      "epoch": 0.43441585828764234,
      "grad_norm": 1.7008649110794067,
      "learning_rate": 0.0001720214846364623,
      "loss": 0.9691,
      "step": 3090
    },
    {
      "epoch": 0.43455644594404613,
      "grad_norm": 1.485600233078003,
      "learning_rate": 0.00017218256584216292,
      "loss": 1.2285,
      "step": 3091
    },
    {
      "epoch": 0.43469703360044987,
      "grad_norm": 1.4808274507522583,
      "learning_rate": 0.00017234325695856498,
      "loss": 1.2879,
      "step": 3092
    },
    {
      "epoch": 0.43483762125685366,
      "grad_norm": 1.5319359302520752,
      "learning_rate": 0.00017250355711726097,
      "loss": 1.261,
      "step": 3093
    },
    {
      "epoch": 0.4349782089132574,
      "grad_norm": 1.2687233686447144,
      "learning_rate": 0.00017266346545195628,
      "loss": 1.2744,
      "step": 3094
    },
    {
      "epoch": 0.4351187965696612,
      "grad_norm": 1.510211706161499,
      "learning_rate": 0.00017282298109847342,
      "loss": 0.9513,
      "step": 3095
    },
    {
      "epoch": 0.43525938422606497,
      "grad_norm": 1.6213937997817993,
      "learning_rate": 0.00017298210319475753,
      "loss": 1.0499,
      "step": 3096
    },
    {
      "epoch": 0.4353999718824687,
      "grad_norm": 1.5843737125396729,
      "learning_rate": 0.00017314083088088022,
      "loss": 1.0328,
      "step": 3097
    },
    {
      "epoch": 0.4355405595388725,
      "grad_norm": 1.7623302936553955,
      "learning_rate": 0.00017329916329904494,
      "loss": 1.0789,
      "step": 3098
    },
    {
      "epoch": 0.43568114719527623,
      "grad_norm": 1.5139172077178955,
      "learning_rate": 0.00017345709959359074,
      "loss": 1.1196,
      "step": 3099
    },
    {
      "epoch": 0.43582173485168,
      "grad_norm": 1.6108108758926392,
      "learning_rate": 0.00017361463891099791,
      "loss": 1.3057,
      "step": 3100
    },
    {
      "epoch": 0.4359623225080838,
      "grad_norm": 1.5227322578430176,
      "learning_rate": 0.00017377178039989176,
      "loss": 1.0845,
      "step": 3101
    },
    {
      "epoch": 0.43610291016448754,
      "grad_norm": 1.794675350189209,
      "learning_rate": 0.0001739285232110478,
      "loss": 1.1376,
      "step": 3102
    },
    {
      "epoch": 0.43624349782089133,
      "grad_norm": 1.4200152158737183,
      "learning_rate": 0.0001740848664973957,
      "loss": 1.1565,
      "step": 3103
    },
    {
      "epoch": 0.43638408547729507,
      "grad_norm": 1.6973844766616821,
      "learning_rate": 0.00017424080941402464,
      "loss": 1.0875,
      "step": 3104
    },
    {
      "epoch": 0.43652467313369886,
      "grad_norm": 1.5803894996643066,
      "learning_rate": 0.00017439635111818723,
      "loss": 1.1276,
      "step": 3105
    },
    {
      "epoch": 0.43666526079010265,
      "grad_norm": 1.4979039430618286,
      "learning_rate": 0.00017455149076930457,
      "loss": 1.1304,
      "step": 3106
    },
    {
      "epoch": 0.4368058484465064,
      "grad_norm": 1.7445011138916016,
      "learning_rate": 0.0001747062275289701,
      "loss": 1.1516,
      "step": 3107
    },
    {
      "epoch": 0.4369464361029102,
      "grad_norm": 1.5882351398468018,
      "learning_rate": 0.000174860560560955,
      "loss": 1.1913,
      "step": 3108
    },
    {
      "epoch": 0.4370870237593139,
      "grad_norm": 1.5771902799606323,
      "learning_rate": 0.00017501448903121204,
      "loss": 1.0647,
      "step": 3109
    },
    {
      "epoch": 0.4372276114157177,
      "grad_norm": 1.4274848699569702,
      "learning_rate": 0.00017516801210788052,
      "loss": 1.1781,
      "step": 3110
    },
    {
      "epoch": 0.4373681990721215,
      "grad_norm": 1.5162875652313232,
      "learning_rate": 0.00017532112896129025,
      "loss": 1.1423,
      "step": 3111
    },
    {
      "epoch": 0.4375087867285252,
      "grad_norm": 1.2680790424346924,
      "learning_rate": 0.0001754738387639667,
      "loss": 1.2876,
      "step": 3112
    },
    {
      "epoch": 0.437649374384929,
      "grad_norm": 1.5769133567810059,
      "learning_rate": 0.0001756261406906349,
      "loss": 1.1895,
      "step": 3113
    },
    {
      "epoch": 0.43778996204133275,
      "grad_norm": 1.437627911567688,
      "learning_rate": 0.00017577803391822416,
      "loss": 1.1296,
      "step": 3114
    },
    {
      "epoch": 0.43793054969773654,
      "grad_norm": 1.3613845109939575,
      "learning_rate": 0.0001759295176258726,
      "loss": 1.3032,
      "step": 3115
    },
    {
      "epoch": 0.4380711373541403,
      "grad_norm": 1.362497091293335,
      "learning_rate": 0.00017608059099493128,
      "loss": 1.1505,
      "step": 3116
    },
    {
      "epoch": 0.43821172501054406,
      "grad_norm": 1.558774709701538,
      "learning_rate": 0.00017623125320896912,
      "loss": 1.0451,
      "step": 3117
    },
    {
      "epoch": 0.43835231266694785,
      "grad_norm": 1.4125198125839233,
      "learning_rate": 0.00017638150345377658,
      "loss": 1.0924,
      "step": 3118
    },
    {
      "epoch": 0.4384929003233516,
      "grad_norm": 1.4377515316009521,
      "learning_rate": 0.0001765313409173708,
      "loss": 1.1572,
      "step": 3119
    },
    {
      "epoch": 0.4386334879797554,
      "grad_norm": 1.607118844985962,
      "learning_rate": 0.0001766807647899996,
      "loss": 1.0829,
      "step": 3120
    },
    {
      "epoch": 0.43877407563615917,
      "grad_norm": 1.4993189573287964,
      "learning_rate": 0.00017682977426414603,
      "loss": 1.0262,
      "step": 3121
    },
    {
      "epoch": 0.4389146632925629,
      "grad_norm": 1.4892578125,
      "learning_rate": 0.00017697836853453224,
      "loss": 1.1052,
      "step": 3122
    },
    {
      "epoch": 0.4390552509489667,
      "grad_norm": 1.6505454778671265,
      "learning_rate": 0.00017712654679812476,
      "loss": 1.1106,
      "step": 3123
    },
    {
      "epoch": 0.4391958386053704,
      "grad_norm": 1.6752421855926514,
      "learning_rate": 0.0001772743082541379,
      "loss": 1.193,
      "step": 3124
    },
    {
      "epoch": 0.4393364262617742,
      "grad_norm": 1.7045884132385254,
      "learning_rate": 0.00017742165210403877,
      "loss": 1.1279,
      "step": 3125
    },
    {
      "epoch": 0.439477013918178,
      "grad_norm": 1.400654673576355,
      "learning_rate": 0.000177568577551551,
      "loss": 1.1325,
      "step": 3126
    },
    {
      "epoch": 0.43961760157458174,
      "grad_norm": 1.6140096187591553,
      "learning_rate": 0.0001777150838026597,
      "loss": 1.1179,
      "step": 3127
    },
    {
      "epoch": 0.43975818923098553,
      "grad_norm": 1.3939998149871826,
      "learning_rate": 0.00017786117006561504,
      "loss": 1.2008,
      "step": 3128
    },
    {
      "epoch": 0.43989877688738926,
      "grad_norm": 1.8683339357376099,
      "learning_rate": 0.00017800683555093733,
      "loss": 0.9515,
      "step": 3129
    },
    {
      "epoch": 0.44003936454379305,
      "grad_norm": 1.616634488105774,
      "learning_rate": 0.00017815207947142033,
      "loss": 1.163,
      "step": 3130
    },
    {
      "epoch": 0.44017995220019684,
      "grad_norm": 1.490604281425476,
      "learning_rate": 0.00017829690104213648,
      "loss": 1.1135,
      "step": 3131
    },
    {
      "epoch": 0.4403205398566006,
      "grad_norm": 1.7612413167953491,
      "learning_rate": 0.0001784412994804404,
      "loss": 1.0438,
      "step": 3132
    },
    {
      "epoch": 0.44046112751300437,
      "grad_norm": 1.5427476167678833,
      "learning_rate": 0.00017858527400597352,
      "loss": 1.1609,
      "step": 3133
    },
    {
      "epoch": 0.4406017151694081,
      "grad_norm": 1.5783510208129883,
      "learning_rate": 0.00017872882384066817,
      "loss": 1.0844,
      "step": 3134
    },
    {
      "epoch": 0.4407423028258119,
      "grad_norm": 1.3439441919326782,
      "learning_rate": 0.0001788719482087517,
      "loss": 1.2233,
      "step": 3135
    },
    {
      "epoch": 0.4408828904822157,
      "grad_norm": 1.73897385597229,
      "learning_rate": 0.000179014646336751,
      "loss": 1.0857,
      "step": 3136
    },
    {
      "epoch": 0.4410234781386194,
      "grad_norm": 1.4559074640274048,
      "learning_rate": 0.00017915691745349623,
      "loss": 1.1644,
      "step": 3137
    },
    {
      "epoch": 0.4411640657950232,
      "grad_norm": 1.6197588443756104,
      "learning_rate": 0.00017929876079012525,
      "loss": 1.0571,
      "step": 3138
    },
    {
      "epoch": 0.44130465345142694,
      "grad_norm": 1.8033051490783691,
      "learning_rate": 0.00017944017558008777,
      "loss": 1.1248,
      "step": 3139
    },
    {
      "epoch": 0.44144524110783073,
      "grad_norm": 1.418507695198059,
      "learning_rate": 0.00017958116105914947,
      "loss": 1.0852,
      "step": 3140
    },
    {
      "epoch": 0.4415858287642345,
      "grad_norm": 1.5166536569595337,
      "learning_rate": 0.00017972171646539605,
      "loss": 1.2762,
      "step": 3141
    },
    {
      "epoch": 0.44172641642063826,
      "grad_norm": 1.3859754800796509,
      "learning_rate": 0.00017986184103923749,
      "loss": 1.0951,
      "step": 3142
    },
    {
      "epoch": 0.44186700407704205,
      "grad_norm": 1.6361781358718872,
      "learning_rate": 0.00018000153402341194,
      "loss": 1.1077,
      "step": 3143
    },
    {
      "epoch": 0.4420075917334458,
      "grad_norm": 1.547247290611267,
      "learning_rate": 0.00018014079466299014,
      "loss": 1.1124,
      "step": 3144
    },
    {
      "epoch": 0.44214817938984957,
      "grad_norm": 1.6467697620391846,
      "learning_rate": 0.0001802796222053792,
      "loss": 1.0968,
      "step": 3145
    },
    {
      "epoch": 0.44228876704625336,
      "grad_norm": 1.768883466720581,
      "learning_rate": 0.00018041801590032674,
      "loss": 1.1389,
      "step": 3146
    },
    {
      "epoch": 0.4424293547026571,
      "grad_norm": 1.5373023748397827,
      "learning_rate": 0.000180555974999925,
      "loss": 0.9806,
      "step": 3147
    },
    {
      "epoch": 0.4425699423590609,
      "grad_norm": 1.6406632661819458,
      "learning_rate": 0.00018069349875861497,
      "loss": 0.9474,
      "step": 3148
    },
    {
      "epoch": 0.4427105300154646,
      "grad_norm": 1.4863826036453247,
      "learning_rate": 0.00018083058643319016,
      "loss": 1.1377,
      "step": 3149
    },
    {
      "epoch": 0.4428511176718684,
      "grad_norm": 1.6950080394744873,
      "learning_rate": 0.0001809672372828009,
      "loss": 0.9573,
      "step": 3150
    },
    {
      "epoch": 0.4429917053282722,
      "grad_norm": 1.5309185981750488,
      "learning_rate": 0.00018110345056895807,
      "loss": 1.221,
      "step": 3151
    },
    {
      "epoch": 0.44313229298467594,
      "grad_norm": 1.3764957189559937,
      "learning_rate": 0.00018123922555553736,
      "loss": 1.0282,
      "step": 3152
    },
    {
      "epoch": 0.4432728806410797,
      "grad_norm": 1.8881162405014038,
      "learning_rate": 0.00018137456150878303,
      "loss": 1.1413,
      "step": 3153
    },
    {
      "epoch": 0.44341346829748346,
      "grad_norm": 1.823763132095337,
      "learning_rate": 0.000181509457697312,
      "loss": 1.1625,
      "step": 3154
    },
    {
      "epoch": 0.44355405595388725,
      "grad_norm": 1.7852914333343506,
      "learning_rate": 0.00018164391339211785,
      "loss": 1.0166,
      "step": 3155
    },
    {
      "epoch": 0.44369464361029104,
      "grad_norm": 1.6736303567886353,
      "learning_rate": 0.0001817779278665745,
      "loss": 1.0317,
      "step": 3156
    },
    {
      "epoch": 0.4438352312666948,
      "grad_norm": 1.7487742900848389,
      "learning_rate": 0.0001819115003964405,
      "loss": 1.0743,
      "step": 3157
    },
    {
      "epoch": 0.44397581892309856,
      "grad_norm": 1.8748531341552734,
      "learning_rate": 0.00018204463025986258,
      "loss": 0.9309,
      "step": 3158
    },
    {
      "epoch": 0.4441164065795023,
      "grad_norm": 1.6438868045806885,
      "learning_rate": 0.0001821773167373799,
      "loss": 0.9782,
      "step": 3159
    },
    {
      "epoch": 0.4442569942359061,
      "grad_norm": 1.8003995418548584,
      "learning_rate": 0.00018230955911192767,
      "loss": 1.1098,
      "step": 3160
    },
    {
      "epoch": 0.4443975818923099,
      "grad_norm": 1.7574056386947632,
      "learning_rate": 0.00018244135666884117,
      "loss": 1.0497,
      "step": 3161
    },
    {
      "epoch": 0.4445381695487136,
      "grad_norm": 1.724685788154602,
      "learning_rate": 0.00018257270869585942,
      "loss": 1.0861,
      "step": 3162
    },
    {
      "epoch": 0.4446787572051174,
      "grad_norm": 1.6409759521484375,
      "learning_rate": 0.00018270361448312943,
      "loss": 0.9698,
      "step": 3163
    },
    {
      "epoch": 0.44481934486152114,
      "grad_norm": 1.5991286039352417,
      "learning_rate": 0.00018283407332320964,
      "loss": 1.1503,
      "step": 3164
    },
    {
      "epoch": 0.4449599325179249,
      "grad_norm": 1.7041174173355103,
      "learning_rate": 0.0001829640845110738,
      "loss": 1.182,
      "step": 3165
    },
    {
      "epoch": 0.4451005201743287,
      "grad_norm": 1.5778220891952515,
      "learning_rate": 0.00018309364734411495,
      "loss": 1.0526,
      "step": 3166
    },
    {
      "epoch": 0.44524110783073245,
      "grad_norm": 1.4940539598464966,
      "learning_rate": 0.0001832227611221492,
      "loss": 1.2699,
      "step": 3167
    },
    {
      "epoch": 0.44538169548713624,
      "grad_norm": 1.4692035913467407,
      "learning_rate": 0.00018335142514741936,
      "loss": 1.1065,
      "step": 3168
    },
    {
      "epoch": 0.44552228314354,
      "grad_norm": 1.7302888631820679,
      "learning_rate": 0.00018347963872459883,
      "loss": 1.0809,
      "step": 3169
    },
    {
      "epoch": 0.44566287079994377,
      "grad_norm": 1.297942042350769,
      "learning_rate": 0.00018360740116079522,
      "loss": 1.1226,
      "step": 3170
    },
    {
      "epoch": 0.44580345845634756,
      "grad_norm": 1.5318225622177124,
      "learning_rate": 0.00018373471176555439,
      "loss": 1.2389,
      "step": 3171
    },
    {
      "epoch": 0.4459440461127513,
      "grad_norm": 1.512954831123352,
      "learning_rate": 0.00018386156985086388,
      "loss": 1.0093,
      "step": 3172
    },
    {
      "epoch": 0.4460846337691551,
      "grad_norm": 1.490644931793213,
      "learning_rate": 0.0001839879747311566,
      "loss": 1.2256,
      "step": 3173
    },
    {
      "epoch": 0.4462252214255588,
      "grad_norm": 1.6254115104675293,
      "learning_rate": 0.00018411392572331494,
      "loss": 1.1278,
      "step": 3174
    },
    {
      "epoch": 0.4463658090819626,
      "grad_norm": 1.8583766222000122,
      "learning_rate": 0.000184239422146674,
      "loss": 0.9626,
      "step": 3175
    },
    {
      "epoch": 0.4465063967383664,
      "grad_norm": 1.541916847229004,
      "learning_rate": 0.00018436446332302566,
      "loss": 1.1256,
      "step": 3176
    },
    {
      "epoch": 0.44664698439477013,
      "grad_norm": 1.5708916187286377,
      "learning_rate": 0.0001844890485766217,
      "loss": 1.0938,
      "step": 3177
    },
    {
      "epoch": 0.4467875720511739,
      "grad_norm": 1.436668038368225,
      "learning_rate": 0.00018461317723417818,
      "loss": 1.2472,
      "step": 3178
    },
    {
      "epoch": 0.44692815970757765,
      "grad_norm": 1.648244023323059,
      "learning_rate": 0.00018473684862487847,
      "loss": 1.1366,
      "step": 3179
    },
    {
      "epoch": 0.44706874736398144,
      "grad_norm": 1.9605716466903687,
      "learning_rate": 0.00018486006208037726,
      "loss": 1.105,
      "step": 3180
    },
    {
      "epoch": 0.44720933502038523,
      "grad_norm": 1.8758617639541626,
      "learning_rate": 0.0001849828169348038,
      "loss": 1.0359,
      "step": 3181
    },
    {
      "epoch": 0.44734992267678897,
      "grad_norm": 1.5094432830810547,
      "learning_rate": 0.00018510511252476584,
      "loss": 1.1292,
      "step": 3182
    },
    {
      "epoch": 0.44749051033319276,
      "grad_norm": 1.493501901626587,
      "learning_rate": 0.00018522694818935316,
      "loss": 1.2736,
      "step": 3183
    },
    {
      "epoch": 0.4476310979895965,
      "grad_norm": 1.835216999053955,
      "learning_rate": 0.00018534832327014104,
      "loss": 1.1216,
      "step": 3184
    },
    {
      "epoch": 0.4477716856460003,
      "grad_norm": 1.773746132850647,
      "learning_rate": 0.0001854692371111936,
      "loss": 1.1062,
      "step": 3185
    },
    {
      "epoch": 0.4479122733024041,
      "grad_norm": 1.5245671272277832,
      "learning_rate": 0.000185589689059068,
      "loss": 1.056,
      "step": 3186
    },
    {
      "epoch": 0.4480528609588078,
      "grad_norm": 1.5524650812149048,
      "learning_rate": 0.00018570967846281723,
      "loss": 1.0005,
      "step": 3187
    },
    {
      "epoch": 0.4481934486152116,
      "grad_norm": 1.5923912525177002,
      "learning_rate": 0.00018582920467399424,
      "loss": 1.1461,
      "step": 3188
    },
    {
      "epoch": 0.44833403627161533,
      "grad_norm": 1.5916200876235962,
      "learning_rate": 0.00018594826704665488,
      "loss": 1.0975,
      "step": 3189
    },
    {
      "epoch": 0.4484746239280191,
      "grad_norm": 1.7979869842529297,
      "learning_rate": 0.00018606686493736186,
      "loss": 1.2152,
      "step": 3190
    },
    {
      "epoch": 0.4486152115844229,
      "grad_norm": 1.5986148118972778,
      "learning_rate": 0.00018618499770518812,
      "loss": 1.099,
      "step": 3191
    },
    {
      "epoch": 0.44875579924082665,
      "grad_norm": 1.5275638103485107,
      "learning_rate": 0.00018630266471171986,
      "loss": 0.9873,
      "step": 3192
    },
    {
      "epoch": 0.44889638689723044,
      "grad_norm": 1.644615650177002,
      "learning_rate": 0.00018641986532106085,
      "loss": 1.1044,
      "step": 3193
    },
    {
      "epoch": 0.44903697455363417,
      "grad_norm": 1.4990376234054565,
      "learning_rate": 0.00018653659889983487,
      "loss": 1.0023,
      "step": 3194
    },
    {
      "epoch": 0.44917756221003796,
      "grad_norm": 1.587220549583435,
      "learning_rate": 0.00018665286481719012,
      "loss": 1.0466,
      "step": 3195
    },
    {
      "epoch": 0.44931814986644175,
      "grad_norm": 2.2069644927978516,
      "learning_rate": 0.0001867686624448017,
      "loss": 0.8871,
      "step": 3196
    },
    {
      "epoch": 0.4494587375228455,
      "grad_norm": 1.339219093322754,
      "learning_rate": 0.0001868839911568757,
      "loss": 1.1782,
      "step": 3197
    },
    {
      "epoch": 0.4495993251792493,
      "grad_norm": 1.5183438062667847,
      "learning_rate": 0.0001869988503301522,
      "loss": 1.0702,
      "step": 3198
    },
    {
      "epoch": 0.449739912835653,
      "grad_norm": 2.235534906387329,
      "learning_rate": 0.00018711323934390893,
      "loss": 1.0879,
      "step": 3199
    },
    {
      "epoch": 0.4498805004920568,
      "grad_norm": 1.6622740030288696,
      "learning_rate": 0.00018722715757996417,
      "loss": 1.0724,
      "step": 3200
    },
    {
      "epoch": 0.4500210881484606,
      "grad_norm": 1.6693710088729858,
      "learning_rate": 0.00018734060442268072,
      "loss": 1.1912,
      "step": 3201
    },
    {
      "epoch": 0.4501616758048643,
      "grad_norm": 1.5303916931152344,
      "learning_rate": 0.0001874535792589686,
      "loss": 1.0853,
      "step": 3202
    },
    {
      "epoch": 0.4503022634612681,
      "grad_norm": 1.6946380138397217,
      "learning_rate": 0.00018756608147828886,
      "loss": 1.0903,
      "step": 3203
    },
    {
      "epoch": 0.45044285111767185,
      "grad_norm": 1.9255988597869873,
      "learning_rate": 0.0001876781104726565,
      "loss": 1.1121,
      "step": 3204
    },
    {
      "epoch": 0.45058343877407564,
      "grad_norm": 1.5013493299484253,
      "learning_rate": 0.00018778966563664406,
      "loss": 1.1031,
      "step": 3205
    },
    {
      "epoch": 0.45072402643047943,
      "grad_norm": 1.6668682098388672,
      "learning_rate": 0.0001879007463673846,
      "loss": 1.1178,
      "step": 3206
    },
    {
      "epoch": 0.45086461408688316,
      "grad_norm": 1.3627811670303345,
      "learning_rate": 0.00018801135206457544,
      "loss": 1.0869,
      "step": 3207
    },
    {
      "epoch": 0.45100520174328695,
      "grad_norm": 1.5147308111190796,
      "learning_rate": 0.00018812148213048053,
      "loss": 1.1673,
      "step": 3208
    },
    {
      "epoch": 0.4511457893996907,
      "grad_norm": 1.482494592666626,
      "learning_rate": 0.00018823113596993477,
      "loss": 1.1447,
      "step": 3209
    },
    {
      "epoch": 0.4512863770560945,
      "grad_norm": 1.5497442483901978,
      "learning_rate": 0.0001883403129903464,
      "loss": 1.1314,
      "step": 3210
    },
    {
      "epoch": 0.45142696471249827,
      "grad_norm": 1.8608015775680542,
      "learning_rate": 0.00018844901260170054,
      "loss": 1.2097,
      "step": 3211
    },
    {
      "epoch": 0.451567552368902,
      "grad_norm": 1.6804125308990479,
      "learning_rate": 0.0001885572342165623,
      "loss": 1.1551,
      "step": 3212
    },
    {
      "epoch": 0.4517081400253058,
      "grad_norm": 1.8375053405761719,
      "learning_rate": 0.00018866497725008005,
      "loss": 1.0948,
      "step": 3213
    },
    {
      "epoch": 0.4518487276817095,
      "grad_norm": 1.8146287202835083,
      "learning_rate": 0.0001887722411199885,
      "loss": 1.0983,
      "step": 3214
    },
    {
      "epoch": 0.4519893153381133,
      "grad_norm": 1.48105788230896,
      "learning_rate": 0.00018887902524661183,
      "loss": 1.1592,
      "step": 3215
    },
    {
      "epoch": 0.4521299029945171,
      "grad_norm": 1.6806939840316772,
      "learning_rate": 0.00018898532905286684,
      "loss": 1.0345,
      "step": 3216
    },
    {
      "epoch": 0.45227049065092084,
      "grad_norm": 1.4273369312286377,
      "learning_rate": 0.00018909115196426602,
      "loss": 1.2144,
      "step": 3217
    },
    {
      "epoch": 0.45241107830732463,
      "grad_norm": 1.704586148262024,
      "learning_rate": 0.0001891964934089209,
      "loss": 1.168,
      "step": 3218
    },
    {
      "epoch": 0.45255166596372837,
      "grad_norm": 1.3643442392349243,
      "learning_rate": 0.00018930135281754483,
      "loss": 1.196,
      "step": 3219
    },
    {
      "epoch": 0.45269225362013216,
      "grad_norm": 1.6370229721069336,
      "learning_rate": 0.00018940572962345616,
      "loss": 1.1543,
      "step": 3220
    },
    {
      "epoch": 0.45283284127653595,
      "grad_norm": 1.5378804206848145,
      "learning_rate": 0.00018950962326258128,
      "loss": 1.0578,
      "step": 3221
    },
    {
      "epoch": 0.4529734289329397,
      "grad_norm": 2.111809253692627,
      "learning_rate": 0.00018961303317345792,
      "loss": 1.0438,
      "step": 3222
    },
    {
      "epoch": 0.45311401658934347,
      "grad_norm": 1.8930079936981201,
      "learning_rate": 0.0001897159587972378,
      "loss": 1.0885,
      "step": 3223
    },
    {
      "epoch": 0.4532546042457472,
      "grad_norm": 1.7445110082626343,
      "learning_rate": 0.00018981839957768986,
      "loss": 1.1814,
      "step": 3224
    },
    {
      "epoch": 0.453395191902151,
      "grad_norm": 1.521631121635437,
      "learning_rate": 0.0001899203549612032,
      "loss": 1.1128,
      "step": 3225
    },
    {
      "epoch": 0.4535357795585548,
      "grad_norm": 1.6246145963668823,
      "learning_rate": 0.00019002182439679022,
      "loss": 1.0561,
      "step": 3226
    },
    {
      "epoch": 0.4536763672149585,
      "grad_norm": 1.6023118495941162,
      "learning_rate": 0.00019012280733608935,
      "loss": 1.119,
      "step": 3227
    },
    {
      "epoch": 0.4538169548713623,
      "grad_norm": 1.6071927547454834,
      "learning_rate": 0.0001902233032333683,
      "loss": 1.0498,
      "step": 3228
    },
    {
      "epoch": 0.45395754252776604,
      "grad_norm": 1.6124399900436401,
      "learning_rate": 0.0001903233115455266,
      "loss": 1.1889,
      "step": 3229
    },
    {
      "epoch": 0.45409813018416983,
      "grad_norm": 1.5719568729400635,
      "learning_rate": 0.0001904228317320991,
      "loss": 1.0175,
      "step": 3230
    },
    {
      "epoch": 0.4542387178405736,
      "grad_norm": 1.4730674028396606,
      "learning_rate": 0.00019052186325525835,
      "loss": 1.2442,
      "step": 3231
    },
    {
      "epoch": 0.45437930549697736,
      "grad_norm": 1.5172780752182007,
      "learning_rate": 0.00019062040557981774,
      "loss": 1.0966,
      "step": 3232
    },
    {
      "epoch": 0.45451989315338115,
      "grad_norm": 1.6164716482162476,
      "learning_rate": 0.00019071845817323463,
      "loss": 1.1928,
      "step": 3233
    },
    {
      "epoch": 0.4546604808097849,
      "grad_norm": 1.7357474565505981,
      "learning_rate": 0.0001908160205056127,
      "loss": 0.9199,
      "step": 3234
    },
    {
      "epoch": 0.4548010684661887,
      "grad_norm": 1.487215280532837,
      "learning_rate": 0.00019091309204970528,
      "loss": 1.0954,
      "step": 3235
    },
    {
      "epoch": 0.45494165612259246,
      "grad_norm": 1.4316610097885132,
      "learning_rate": 0.0001910096722809179,
      "loss": 1.1652,
      "step": 3236
    },
    {
      "epoch": 0.4550822437789962,
      "grad_norm": 1.4441157579421997,
      "learning_rate": 0.00019110576067731138,
      "loss": 1.1571,
      "step": 3237
    },
    {
      "epoch": 0.4552228314354,
      "grad_norm": 1.4759703874588013,
      "learning_rate": 0.0001912013567196044,
      "loss": 1.0268,
      "step": 3238
    },
    {
      "epoch": 0.4553634190918037,
      "grad_norm": 1.5219768285751343,
      "learning_rate": 0.00019129645989117644,
      "loss": 1.0422,
      "step": 3239
    },
    {
      "epoch": 0.4555040067482075,
      "grad_norm": 1.5629980564117432,
      "learning_rate": 0.0001913910696780706,
      "loss": 1.1921,
      "step": 3240
    },
    {
      "epoch": 0.4556445944046113,
      "grad_norm": 1.787369728088379,
      "learning_rate": 0.0001914851855689963,
      "loss": 1.1295,
      "step": 3241
    },
    {
      "epoch": 0.45578518206101504,
      "grad_norm": 1.648879051208496,
      "learning_rate": 0.00019157880705533212,
      "loss": 1.0267,
      "step": 3242
    },
    {
      "epoch": 0.4559257697174188,
      "grad_norm": 1.4288392066955566,
      "learning_rate": 0.00019167193363112844,
      "loss": 1.2123,
      "step": 3243
    },
    {
      "epoch": 0.45606635737382256,
      "grad_norm": 1.550159215927124,
      "learning_rate": 0.0001917645647931102,
      "loss": 1.1076,
      "step": 3244
    },
    {
      "epoch": 0.45620694503022635,
      "grad_norm": 1.8001580238342285,
      "learning_rate": 0.00019185670004067982,
      "loss": 1.0703,
      "step": 3245
    },
    {
      "epoch": 0.4563475326866301,
      "grad_norm": 1.5358669757843018,
      "learning_rate": 0.00019194833887591962,
      "loss": 1.0572,
      "step": 3246
    },
    {
      "epoch": 0.4564881203430339,
      "grad_norm": 1.853590488433838,
      "learning_rate": 0.0001920394808035946,
      "loss": 1.2643,
      "step": 3247
    },
    {
      "epoch": 0.45662870799943767,
      "grad_norm": 1.5857511758804321,
      "learning_rate": 0.00019213012533115525,
      "loss": 1.0509,
      "step": 3248
    },
    {
      "epoch": 0.4567692956558414,
      "grad_norm": 1.7067980766296387,
      "learning_rate": 0.00019222027196874006,
      "loss": 1.1287,
      "step": 3249
    },
    {
      "epoch": 0.4569098833122452,
      "grad_norm": 1.4682480096817017,
      "learning_rate": 0.00019230992022917828,
      "loss": 1.2435,
      "step": 3250
    },
    {
      "epoch": 0.4570504709686489,
      "grad_norm": 1.621389389038086,
      "learning_rate": 0.0001923990696279923,
      "loss": 1.1028,
      "step": 3251
    },
    {
      "epoch": 0.4571910586250527,
      "grad_norm": 1.4565461874008179,
      "learning_rate": 0.0001924877196834007,
      "loss": 1.1096,
      "step": 3252
    },
    {
      "epoch": 0.4573316462814565,
      "grad_norm": 1.8926925659179688,
      "learning_rate": 0.0001925758699163205,
      "loss": 1.0667,
      "step": 3253
    },
    {
      "epoch": 0.45747223393786024,
      "grad_norm": 1.7135066986083984,
      "learning_rate": 0.0001926635198503699,
      "loss": 1.2322,
      "step": 3254
    },
    {
      "epoch": 0.45761282159426403,
      "grad_norm": 1.778018832206726,
      "learning_rate": 0.0001927506690118707,
      "loss": 1.0635,
      "step": 3255
    },
    {
      "epoch": 0.45775340925066776,
      "grad_norm": 1.9006184339523315,
      "learning_rate": 0.00019283731692985116,
      "loss": 1.1052,
      "step": 3256
    },
    {
      "epoch": 0.45789399690707155,
      "grad_norm": 1.7689692974090576,
      "learning_rate": 0.0001929234631360482,
      "loss": 1.1002,
      "step": 3257
    },
    {
      "epoch": 0.45803458456347534,
      "grad_norm": 1.8175965547561646,
      "learning_rate": 0.00019300910716491028,
      "loss": 1.0062,
      "step": 3258
    },
    {
      "epoch": 0.4581751722198791,
      "grad_norm": 1.686124324798584,
      "learning_rate": 0.00019309424855359946,
      "loss": 1.1502,
      "step": 3259
    },
    {
      "epoch": 0.45831575987628287,
      "grad_norm": 1.4275610446929932,
      "learning_rate": 0.0001931788868419944,
      "loss": 1.0208,
      "step": 3260
    },
    {
      "epoch": 0.4584563475326866,
      "grad_norm": 1.71065092086792,
      "learning_rate": 0.00019326302157269252,
      "loss": 1.162,
      "step": 3261
    },
    {
      "epoch": 0.4585969351890904,
      "grad_norm": 1.79793119430542,
      "learning_rate": 0.00019334665229101266,
      "loss": 1.0939,
      "step": 3262
    },
    {
      "epoch": 0.4587375228454942,
      "grad_norm": 1.6609662771224976,
      "learning_rate": 0.00019342977854499722,
      "loss": 1.1467,
      "step": 3263
    },
    {
      "epoch": 0.4588781105018979,
      "grad_norm": 1.4693876504898071,
      "learning_rate": 0.00019351239988541513,
      "loss": 1.1271,
      "step": 3264
    },
    {
      "epoch": 0.4590186981583017,
      "grad_norm": 1.364440679550171,
      "learning_rate": 0.0001935945158657637,
      "loss": 0.957,
      "step": 3265
    },
    {
      "epoch": 0.45915928581470544,
      "grad_norm": 1.5585960149765015,
      "learning_rate": 0.00019367612604227158,
      "loss": 1.2777,
      "step": 3266
    },
    {
      "epoch": 0.45929987347110923,
      "grad_norm": 1.707665205001831,
      "learning_rate": 0.0001937572299739006,
      "loss": 1.0127,
      "step": 3267
    },
    {
      "epoch": 0.459440461127513,
      "grad_norm": 1.6099849939346313,
      "learning_rate": 0.00019383782722234868,
      "loss": 1.0456,
      "step": 3268
    },
    {
      "epoch": 0.45958104878391676,
      "grad_norm": 1.5259515047073364,
      "learning_rate": 0.00019391791735205182,
      "loss": 1.1028,
      "step": 3269
    },
    {
      "epoch": 0.45972163644032055,
      "grad_norm": 1.4775805473327637,
      "learning_rate": 0.0001939974999301866,
      "loss": 1.1509,
      "step": 3270
    },
    {
      "epoch": 0.4598622240967243,
      "grad_norm": 1.508345603942871,
      "learning_rate": 0.00019407657452667266,
      "loss": 1.0823,
      "step": 3271
    },
    {
      "epoch": 0.46000281175312807,
      "grad_norm": 1.668908715248108,
      "learning_rate": 0.0001941551407141746,
      "loss": 1.0316,
      "step": 3272
    },
    {
      "epoch": 0.46014339940953186,
      "grad_norm": 1.753269910812378,
      "learning_rate": 0.00019423319806810492,
      "loss": 1.1348,
      "step": 3273
    },
    {
      "epoch": 0.4602839870659356,
      "grad_norm": 1.9424091577529907,
      "learning_rate": 0.00019431074616662556,
      "loss": 1.0163,
      "step": 3274
    },
    {
      "epoch": 0.4604245747223394,
      "grad_norm": 1.740915060043335,
      "learning_rate": 0.00019438778459065093,
      "loss": 1.0535,
      "step": 3275
    },
    {
      "epoch": 0.4605651623787431,
      "grad_norm": 1.5911599397659302,
      "learning_rate": 0.00019446431292384966,
      "loss": 1.0331,
      "step": 3276
    },
    {
      "epoch": 0.4607057500351469,
      "grad_norm": 1.8266139030456543,
      "learning_rate": 0.00019454033075264708,
      "loss": 1.2023,
      "step": 3277
    },
    {
      "epoch": 0.4608463376915507,
      "grad_norm": 1.6196670532226562,
      "learning_rate": 0.00019461583766622721,
      "loss": 1.2538,
      "step": 3278
    },
    {
      "epoch": 0.46098692534795443,
      "grad_norm": 1.5268537998199463,
      "learning_rate": 0.00019469083325653546,
      "loss": 1.3717,
      "step": 3279
    },
    {
      "epoch": 0.4611275130043582,
      "grad_norm": 1.8436176776885986,
      "learning_rate": 0.00019476531711828024,
      "loss": 0.9883,
      "step": 3280
    },
    {
      "epoch": 0.46126810066076196,
      "grad_norm": 1.7099618911743164,
      "learning_rate": 0.0001948392888489357,
      "loss": 1.259,
      "step": 3281
    },
    {
      "epoch": 0.46140868831716575,
      "grad_norm": 1.6978356838226318,
      "learning_rate": 0.0001949127480487434,
      "loss": 1.2455,
      "step": 3282
    },
    {
      "epoch": 0.46154927597356954,
      "grad_norm": 1.585282564163208,
      "learning_rate": 0.000194985694320715,
      "loss": 1.1067,
      "step": 3283
    },
    {
      "epoch": 0.4616898636299733,
      "grad_norm": 2.46761155128479,
      "learning_rate": 0.00019505812727063383,
      "loss": 0.9913,
      "step": 3284
    },
    {
      "epoch": 0.46183045128637706,
      "grad_norm": 1.6439449787139893,
      "learning_rate": 0.00019513004650705757,
      "loss": 1.2007,
      "step": 3285
    },
    {
      "epoch": 0.4619710389427808,
      "grad_norm": 1.6804428100585938,
      "learning_rate": 0.00019520145164131995,
      "loss": 1.1194,
      "step": 3286
    },
    {
      "epoch": 0.4621116265991846,
      "grad_norm": 1.465842843055725,
      "learning_rate": 0.0001952723422875331,
      "loss": 1.1938,
      "step": 3287
    },
    {
      "epoch": 0.4622522142555884,
      "grad_norm": 1.48807692527771,
      "learning_rate": 0.00019534271806258952,
      "loss": 1.0302,
      "step": 3288
    },
    {
      "epoch": 0.4623928019119921,
      "grad_norm": 1.6693642139434814,
      "learning_rate": 0.00019541257858616417,
      "loss": 1.1879,
      "step": 3289
    },
    {
      "epoch": 0.4625333895683959,
      "grad_norm": 1.7700566053390503,
      "learning_rate": 0.00019548192348071655,
      "loss": 0.9621,
      "step": 3290
    },
    {
      "epoch": 0.46267397722479964,
      "grad_norm": 1.6643636226654053,
      "learning_rate": 0.00019555075237149265,
      "loss": 1.1579,
      "step": 3291
    },
    {
      "epoch": 0.4628145648812034,
      "grad_norm": 1.480509877204895,
      "learning_rate": 0.0001956190648865272,
      "loss": 1.1954,
      "step": 3292
    },
    {
      "epoch": 0.4629551525376072,
      "grad_norm": 1.5868602991104126,
      "learning_rate": 0.00019568686065664544,
      "loss": 0.976,
      "step": 3293
    },
    {
      "epoch": 0.46309574019401095,
      "grad_norm": 1.495326042175293,
      "learning_rate": 0.00019575413931546517,
      "loss": 1.1031,
      "step": 3294
    },
    {
      "epoch": 0.46323632785041474,
      "grad_norm": 1.5598524808883667,
      "learning_rate": 0.00019582090049939877,
      "loss": 1.0029,
      "step": 3295
    },
    {
      "epoch": 0.4633769155068185,
      "grad_norm": 1.406623363494873,
      "learning_rate": 0.0001958871438476553,
      "loss": 1.2575,
      "step": 3296
    },
    {
      "epoch": 0.46351750316322227,
      "grad_norm": 1.5317660570144653,
      "learning_rate": 0.00019595286900224212,
      "loss": 1.333,
      "step": 3297
    },
    {
      "epoch": 0.46365809081962606,
      "grad_norm": 1.6207188367843628,
      "learning_rate": 0.00019601807560796713,
      "loss": 1.1224,
      "step": 3298
    },
    {
      "epoch": 0.4637986784760298,
      "grad_norm": 1.5104868412017822,
      "learning_rate": 0.00019608276331244053,
      "loss": 1.0109,
      "step": 3299
    },
    {
      "epoch": 0.4639392661324336,
      "grad_norm": 1.3576815128326416,
      "learning_rate": 0.00019614693176607683,
      "loss": 1.1908,
      "step": 3300
    },
    {
      "epoch": 0.4640798537888373,
      "grad_norm": 1.4787133932113647,
      "learning_rate": 0.00019621058062209654,
      "loss": 1.139,
      "step": 3301
    },
    {
      "epoch": 0.4642204414452411,
      "grad_norm": 1.6530110836029053,
      "learning_rate": 0.0001962737095365283,
      "loss": 1.0969,
      "step": 3302
    },
    {
      "epoch": 0.4643610291016449,
      "grad_norm": 1.628546118736267,
      "learning_rate": 0.0001963363181682106,
      "loss": 1.1805,
      "step": 3303
    },
    {
      "epoch": 0.46450161675804863,
      "grad_norm": 1.8981812000274658,
      "learning_rate": 0.0001963984061787937,
      "loss": 1.2018,
      "step": 3304
    },
    {
      "epoch": 0.4646422044144524,
      "grad_norm": 1.9972995519638062,
      "learning_rate": 0.0001964599732327412,
      "loss": 1.1625,
      "step": 3305
    },
    {
      "epoch": 0.46478279207085615,
      "grad_norm": 1.6538752317428589,
      "learning_rate": 0.0001965210189973323,
      "loss": 1.2088,
      "step": 3306
    },
    {
      "epoch": 0.46492337972725994,
      "grad_norm": 1.7131551504135132,
      "learning_rate": 0.0001965815431426632,
      "loss": 1.1188,
      "step": 3307
    },
    {
      "epoch": 0.46506396738366373,
      "grad_norm": 1.598732352256775,
      "learning_rate": 0.00019664154534164912,
      "loss": 1.0579,
      "step": 3308
    },
    {
      "epoch": 0.46520455504006747,
      "grad_norm": 2.1298539638519287,
      "learning_rate": 0.00019670102527002587,
      "loss": 1.265,
      "step": 3309
    },
    {
      "epoch": 0.46534514269647126,
      "grad_norm": 1.5860410928726196,
      "learning_rate": 0.00019675998260635183,
      "loss": 1.2048,
      "step": 3310
    },
    {
      "epoch": 0.465485730352875,
      "grad_norm": 1.5889828205108643,
      "learning_rate": 0.0001968184170320096,
      "loss": 1.1385,
      "step": 3311
    },
    {
      "epoch": 0.4656263180092788,
      "grad_norm": 1.518093466758728,
      "learning_rate": 0.00019687632823120753,
      "loss": 1.1532,
      "step": 3312
    },
    {
      "epoch": 0.4657669056656826,
      "grad_norm": 1.5684781074523926,
      "learning_rate": 0.00019693371589098177,
      "loss": 1.0994,
      "step": 3313
    },
    {
      "epoch": 0.4659074933220863,
      "grad_norm": 1.7670962810516357,
      "learning_rate": 0.00019699057970119766,
      "loss": 1.0397,
      "step": 3314
    },
    {
      "epoch": 0.4660480809784901,
      "grad_norm": 1.4377338886260986,
      "learning_rate": 0.00019704691935455162,
      "loss": 1.0747,
      "step": 3315
    },
    {
      "epoch": 0.46618866863489383,
      "grad_norm": 1.7336262464523315,
      "learning_rate": 0.00019710273454657265,
      "loss": 1.2266,
      "step": 3316
    },
    {
      "epoch": 0.4663292562912976,
      "grad_norm": 1.4929819107055664,
      "learning_rate": 0.00019715802497562415,
      "loss": 1.0479,
      "step": 3317
    },
    {
      "epoch": 0.4664698439477014,
      "grad_norm": 1.9454610347747803,
      "learning_rate": 0.00019721279034290524,
      "loss": 1.1156,
      "step": 3318
    },
    {
      "epoch": 0.46661043160410515,
      "grad_norm": 1.6175256967544556,
      "learning_rate": 0.00019726703035245283,
      "loss": 1.0831,
      "step": 3319
    },
    {
      "epoch": 0.46675101926050894,
      "grad_norm": 1.4404327869415283,
      "learning_rate": 0.00019732074471114278,
      "loss": 1.1626,
      "step": 3320
    },
    {
      "epoch": 0.46689160691691267,
      "grad_norm": 1.7768429517745972,
      "learning_rate": 0.00019737393312869178,
      "loss": 1.1174,
      "step": 3321
    },
    {
      "epoch": 0.46703219457331646,
      "grad_norm": 1.607519507408142,
      "learning_rate": 0.00019742659531765878,
      "loss": 1.1314,
      "step": 3322
    },
    {
      "epoch": 0.46717278222972025,
      "grad_norm": 1.6154793500900269,
      "learning_rate": 0.00019747873099344656,
      "loss": 1.2256,
      "step": 3323
    },
    {
      "epoch": 0.467313369886124,
      "grad_norm": 1.8441311120986938,
      "learning_rate": 0.0001975303398743033,
      "loss": 0.9434,
      "step": 3324
    },
    {
      "epoch": 0.4674539575425278,
      "grad_norm": 1.520572543144226,
      "learning_rate": 0.00019758142168132417,
      "loss": 1.0673,
      "step": 3325
    },
    {
      "epoch": 0.4675945451989315,
      "grad_norm": 1.595366358757019,
      "learning_rate": 0.00019763197613845255,
      "loss": 1.0884,
      "step": 3326
    },
    {
      "epoch": 0.4677351328553353,
      "grad_norm": 2.324237585067749,
      "learning_rate": 0.00019768200297248193,
      "loss": 0.9224,
      "step": 3327
    },
    {
      "epoch": 0.4678757205117391,
      "grad_norm": 1.6157039403915405,
      "learning_rate": 0.00019773150191305705,
      "loss": 1.1784,
      "step": 3328
    },
    {
      "epoch": 0.4680163081681428,
      "grad_norm": 1.874991536140442,
      "learning_rate": 0.00019778047269267558,
      "loss": 1.1103,
      "step": 3329
    },
    {
      "epoch": 0.4681568958245466,
      "grad_norm": 1.5767600536346436,
      "learning_rate": 0.00019782891504668943,
      "loss": 1.2781,
      "step": 3330
    },
    {
      "epoch": 0.46829748348095035,
      "grad_norm": 1.5732533931732178,
      "learning_rate": 0.00019787682871330627,
      "loss": 1.2103,
      "step": 3331
    },
    {
      "epoch": 0.46843807113735414,
      "grad_norm": 1.6820166110992432,
      "learning_rate": 0.00019792421343359085,
      "loss": 1.0244,
      "step": 3332
    },
    {
      "epoch": 0.46857865879375793,
      "grad_norm": 1.403633952140808,
      "learning_rate": 0.0001979710689514665,
      "loss": 1.2601,
      "step": 3333
    },
    {
      "epoch": 0.46871924645016166,
      "grad_norm": 1.613083004951477,
      "learning_rate": 0.00019801739501371646,
      "loss": 0.9558,
      "step": 3334
    },
    {
      "epoch": 0.46885983410656545,
      "grad_norm": 1.753831386566162,
      "learning_rate": 0.0001980631913699852,
      "loss": 1.0277,
      "step": 3335
    },
    {
      "epoch": 0.4690004217629692,
      "grad_norm": 1.5759246349334717,
      "learning_rate": 0.00019810845777277995,
      "loss": 1.0917,
      "step": 3336
    },
    {
      "epoch": 0.469141009419373,
      "grad_norm": 1.6166754961013794,
      "learning_rate": 0.00019815319397747177,
      "loss": 1.0711,
      "step": 3337
    },
    {
      "epoch": 0.46928159707577677,
      "grad_norm": 1.6785099506378174,
      "learning_rate": 0.00019819739974229715,
      "loss": 0.9907,
      "step": 3338
    },
    {
      "epoch": 0.4694221847321805,
      "grad_norm": 1.4258477687835693,
      "learning_rate": 0.00019824107482835905,
      "loss": 1.1573,
      "step": 3339
    },
    {
      "epoch": 0.4695627723885843,
      "grad_norm": 1.6837691068649292,
      "learning_rate": 0.00019828421899962853,
      "loss": 1.1816,
      "step": 3340
    },
    {
      "epoch": 0.469703360044988,
      "grad_norm": 1.6297104358673096,
      "learning_rate": 0.00019832683202294557,
      "loss": 0.954,
      "step": 3341
    },
    {
      "epoch": 0.4698439477013918,
      "grad_norm": 1.6349260807037354,
      "learning_rate": 0.00019836891366802078,
      "loss": 1.2856,
      "step": 3342
    },
    {
      "epoch": 0.4699845353577956,
      "grad_norm": 1.4602775573730469,
      "learning_rate": 0.0001984104637074363,
      "loss": 1.0721,
      "step": 3343
    },
    {
      "epoch": 0.47012512301419934,
      "grad_norm": 1.5848400592803955,
      "learning_rate": 0.00019845148191664734,
      "loss": 1.0537,
      "step": 3344
    },
    {
      "epoch": 0.47026571067060313,
      "grad_norm": 1.6370667219161987,
      "learning_rate": 0.00019849196807398306,
      "loss": 1.0044,
      "step": 3345
    },
    {
      "epoch": 0.47040629832700687,
      "grad_norm": 1.5269997119903564,
      "learning_rate": 0.00019853192196064807,
      "loss": 1.1678,
      "step": 3346
    },
    {
      "epoch": 0.47054688598341066,
      "grad_norm": 1.982067584991455,
      "learning_rate": 0.0001985713433607234,
      "loss": 1.1119,
      "step": 3347
    },
    {
      "epoch": 0.47068747363981445,
      "grad_norm": 1.565603494644165,
      "learning_rate": 0.00019861023206116775,
      "loss": 1.2034,
      "step": 3348
    },
    {
      "epoch": 0.4708280612962182,
      "grad_norm": 1.4361058473587036,
      "learning_rate": 0.00019864858785181868,
      "loss": 1.0927,
      "step": 3349
    },
    {
      "epoch": 0.47096864895262197,
      "grad_norm": 1.5688005685806274,
      "learning_rate": 0.00019868641052539368,
      "loss": 1.2383,
      "step": 3350
    },
    {
      "epoch": 0.4711092366090257,
      "grad_norm": 1.4878824949264526,
      "learning_rate": 0.00019872369987749132,
      "loss": 1.0869,
      "step": 3351
    },
    {
      "epoch": 0.4712498242654295,
      "grad_norm": 1.5821523666381836,
      "learning_rate": 0.0001987604557065923,
      "loss": 1.0749,
      "step": 3352
    },
    {
      "epoch": 0.4713904119218333,
      "grad_norm": 1.6060481071472168,
      "learning_rate": 0.00019879667781406063,
      "loss": 1.099,
      "step": 3353
    },
    {
      "epoch": 0.471530999578237,
      "grad_norm": 1.5122332572937012,
      "learning_rate": 0.0001988323660041447,
      "loss": 1.1725,
      "step": 3354
    },
    {
      "epoch": 0.4716715872346408,
      "grad_norm": 1.6417362689971924,
      "learning_rate": 0.00019886752008397828,
      "loss": 1.149,
      "step": 3355
    },
    {
      "epoch": 0.47181217489104454,
      "grad_norm": 1.414832353591919,
      "learning_rate": 0.00019890213986358148,
      "loss": 1.0392,
      "step": 3356
    },
    {
      "epoch": 0.47195276254744833,
      "grad_norm": 1.7045929431915283,
      "learning_rate": 0.00019893622515586198,
      "loss": 0.9764,
      "step": 3357
    },
    {
      "epoch": 0.4720933502038521,
      "grad_norm": 1.4909868240356445,
      "learning_rate": 0.00019896977577661592,
      "loss": 1.3567,
      "step": 3358
    },
    {
      "epoch": 0.47223393786025586,
      "grad_norm": 1.5450488328933716,
      "learning_rate": 0.00019900279154452897,
      "loss": 1.1675,
      "step": 3359
    },
    {
      "epoch": 0.47237452551665965,
      "grad_norm": 1.6774107217788696,
      "learning_rate": 0.00019903527228117706,
      "loss": 1.0206,
      "step": 3360
    },
    {
      "epoch": 0.4725151131730634,
      "grad_norm": 1.4228885173797607,
      "learning_rate": 0.0001990672178110278,
      "loss": 1.0393,
      "step": 3361
    },
    {
      "epoch": 0.4726557008294672,
      "grad_norm": 1.5823191404342651,
      "learning_rate": 0.00019909862796144094,
      "loss": 1.2527,
      "step": 3362
    },
    {
      "epoch": 0.47279628848587096,
      "grad_norm": 1.5499451160430908,
      "learning_rate": 0.00019912950256266966,
      "loss": 0.9385,
      "step": 3363
    },
    {
      "epoch": 0.4729368761422747,
      "grad_norm": 1.5034055709838867,
      "learning_rate": 0.00019915984144786133,
      "loss": 1.1385,
      "step": 3364
    },
    {
      "epoch": 0.4730774637986785,
      "grad_norm": 1.7582358121871948,
      "learning_rate": 0.00019918964445305842,
      "loss": 1.0718,
      "step": 3365
    },
    {
      "epoch": 0.4732180514550822,
      "grad_norm": 1.5986504554748535,
      "learning_rate": 0.00019921891141719945,
      "loss": 1.1471,
      "step": 3366
    },
    {
      "epoch": 0.473358639111486,
      "grad_norm": 1.540013074874878,
      "learning_rate": 0.0001992476421821197,
      "loss": 1.1587,
      "step": 3367
    },
    {
      "epoch": 0.4734992267678898,
      "grad_norm": 1.4725242853164673,
      "learning_rate": 0.00019927583659255235,
      "loss": 1.1817,
      "step": 3368
    },
    {
      "epoch": 0.47363981442429354,
      "grad_norm": 2.007878065109253,
      "learning_rate": 0.00019930349449612895,
      "loss": 1.1868,
      "step": 3369
    },
    {
      "epoch": 0.4737804020806973,
      "grad_norm": 1.5524427890777588,
      "learning_rate": 0.00019933061574338068,
      "loss": 1.1385,
      "step": 3370
    },
    {
      "epoch": 0.47392098973710106,
      "grad_norm": 1.7926955223083496,
      "learning_rate": 0.00019935720018773872,
      "loss": 1.0657,
      "step": 3371
    },
    {
      "epoch": 0.47406157739350485,
      "grad_norm": 1.639826774597168,
      "learning_rate": 0.00019938324768553533,
      "loss": 1.053,
      "step": 3372
    },
    {
      "epoch": 0.47420216504990864,
      "grad_norm": 1.911899447441101,
      "learning_rate": 0.0001994087580960045,
      "loss": 1.1328,
      "step": 3373
    },
    {
      "epoch": 0.4743427527063124,
      "grad_norm": 1.2169060707092285,
      "learning_rate": 0.0001994337312812828,
      "loss": 1.1943,
      "step": 3374
    },
    {
      "epoch": 0.47448334036271617,
      "grad_norm": 1.5074080228805542,
      "learning_rate": 0.00019945816710641002,
      "loss": 1.1349,
      "step": 3375
    },
    {
      "epoch": 0.4746239280191199,
      "grad_norm": 1.6493173837661743,
      "learning_rate": 0.00019948206543933005,
      "loss": 1.2151,
      "step": 3376
    },
    {
      "epoch": 0.4747645156755237,
      "grad_norm": 1.6262887716293335,
      "learning_rate": 0.00019950542615089132,
      "loss": 1.1828,
      "step": 3377
    },
    {
      "epoch": 0.4749051033319275,
      "grad_norm": 1.377705454826355,
      "learning_rate": 0.00019952824911484784,
      "loss": 1.0237,
      "step": 3378
    },
    {
      "epoch": 0.4750456909883312,
      "grad_norm": 1.5686708688735962,
      "learning_rate": 0.0001995505342078597,
      "loss": 1.0045,
      "step": 3379
    },
    {
      "epoch": 0.475186278644735,
      "grad_norm": 1.5970723628997803,
      "learning_rate": 0.00019957228130949365,
      "loss": 1.0428,
      "step": 3380
    },
    {
      "epoch": 0.47532686630113874,
      "grad_norm": 1.6592744588851929,
      "learning_rate": 0.00019959349030222395,
      "loss": 1.1687,
      "step": 3381
    },
    {
      "epoch": 0.47546745395754253,
      "grad_norm": 1.4892263412475586,
      "learning_rate": 0.00019961416107143286,
      "loss": 1.2041,
      "step": 3382
    },
    {
      "epoch": 0.4756080416139463,
      "grad_norm": 1.5162616968154907,
      "learning_rate": 0.00019963429350541137,
      "loss": 1.0068,
      "step": 3383
    },
    {
      "epoch": 0.47574862927035005,
      "grad_norm": 2.102450370788574,
      "learning_rate": 0.00019965388749535964,
      "loss": 1.2301,
      "step": 3384
    },
    {
      "epoch": 0.47588921692675384,
      "grad_norm": 1.6222028732299805,
      "learning_rate": 0.0001996729429353878,
      "loss": 0.9122,
      "step": 3385
    },
    {
      "epoch": 0.4760298045831576,
      "grad_norm": 1.7526503801345825,
      "learning_rate": 0.0001996914597225164,
      "loss": 1.2955,
      "step": 3386
    },
    {
      "epoch": 0.47617039223956137,
      "grad_norm": 1.7738004922866821,
      "learning_rate": 0.00019970943775667686,
      "loss": 1.4021,
      "step": 3387
    },
    {
      "epoch": 0.47631097989596516,
      "grad_norm": 1.603786587715149,
      "learning_rate": 0.0001997268769407123,
      "loss": 1.1692,
      "step": 3388
    },
    {
      "epoch": 0.4764515675523689,
      "grad_norm": 1.8415170907974243,
      "learning_rate": 0.0001997437771803778,
      "loss": 1.1525,
      "step": 3389
    },
    {
      "epoch": 0.4765921552087727,
      "grad_norm": 1.7206870317459106,
      "learning_rate": 0.00019976013838434096,
      "loss": 1.0135,
      "step": 3390
    },
    {
      "epoch": 0.4767327428651764,
      "grad_norm": 1.6455782651901245,
      "learning_rate": 0.00019977596046418257,
      "loss": 1.0813,
      "step": 3391
    },
    {
      "epoch": 0.4768733305215802,
      "grad_norm": 1.3065447807312012,
      "learning_rate": 0.00019979124333439682,
      "loss": 1.0208,
      "step": 3392
    },
    {
      "epoch": 0.477013918177984,
      "grad_norm": 1.605247139930725,
      "learning_rate": 0.00019980598691239206,
      "loss": 1.0776,
      "step": 3393
    },
    {
      "epoch": 0.47715450583438773,
      "grad_norm": 1.5926387310028076,
      "learning_rate": 0.00019982019111849087,
      "loss": 1.2742,
      "step": 3394
    },
    {
      "epoch": 0.4772950934907915,
      "grad_norm": 1.6697275638580322,
      "learning_rate": 0.00019983385587593093,
      "loss": 1.1789,
      "step": 3395
    },
    {
      "epoch": 0.47743568114719526,
      "grad_norm": 1.5768369436264038,
      "learning_rate": 0.00019984698111086505,
      "loss": 1.1894,
      "step": 3396
    },
    {
      "epoch": 0.47757626880359905,
      "grad_norm": 1.8101179599761963,
      "learning_rate": 0.00019985956675236177,
      "loss": 1.0172,
      "step": 3397
    },
    {
      "epoch": 0.47771685646000284,
      "grad_norm": 1.585906982421875,
      "learning_rate": 0.00019987161273240579,
      "loss": 1.1674,
      "step": 3398
    },
    {
      "epoch": 0.47785744411640657,
      "grad_norm": 1.5611917972564697,
      "learning_rate": 0.0001998831189858981,
      "loss": 1.1056,
      "step": 3399
    },
    {
      "epoch": 0.47799803177281036,
      "grad_norm": 1.4002258777618408,
      "learning_rate": 0.0001998940854506566,
      "loss": 1.2217,
      "step": 3400
    },
    {
      "epoch": 0.4781386194292141,
      "grad_norm": 1.6283776760101318,
      "learning_rate": 0.0001999045120674163,
      "loss": 1.0639,
      "step": 3401
    },
    {
      "epoch": 0.4782792070856179,
      "grad_norm": 1.602163314819336,
      "learning_rate": 0.0001999143987798296,
      "loss": 1.0796,
      "step": 3402
    },
    {
      "epoch": 0.4784197947420217,
      "grad_norm": 1.6284319162368774,
      "learning_rate": 0.00019992374553446667,
      "loss": 1.1798,
      "step": 3403
    },
    {
      "epoch": 0.4785603823984254,
      "grad_norm": 1.6477832794189453,
      "learning_rate": 0.0001999325522808158,
      "loss": 1.159,
      "step": 3404
    },
    {
      "epoch": 0.4787009700548292,
      "grad_norm": 1.3184211254119873,
      "learning_rate": 0.0001999408189712835,
      "loss": 1.0582,
      "step": 3405
    },
    {
      "epoch": 0.47884155771123293,
      "grad_norm": 1.6036186218261719,
      "learning_rate": 0.0001999485455611949,
      "loss": 1.1424,
      "step": 3406
    },
    {
      "epoch": 0.4789821453676367,
      "grad_norm": 1.478720784187317,
      "learning_rate": 0.00019995573200879397,
      "loss": 1.1076,
      "step": 3407
    },
    {
      "epoch": 0.4791227330240405,
      "grad_norm": 1.2861512899398804,
      "learning_rate": 0.0001999623782752436,
      "loss": 0.9757,
      "step": 3408
    },
    {
      "epoch": 0.47926332068044425,
      "grad_norm": 1.4720852375030518,
      "learning_rate": 0.00019996848432462608,
      "loss": 1.3591,
      "step": 3409
    },
    {
      "epoch": 0.47940390833684804,
      "grad_norm": 1.7304606437683105,
      "learning_rate": 0.00019997405012394306,
      "loss": 0.8975,
      "step": 3410
    },
    {
      "epoch": 0.4795444959932518,
      "grad_norm": 1.7161039113998413,
      "learning_rate": 0.00019997907564311583,
      "loss": 1.1041,
      "step": 3411
    },
    {
      "epoch": 0.47968508364965556,
      "grad_norm": 1.5034428834915161,
      "learning_rate": 0.0001999835608549854,
      "loss": 1.1375,
      "step": 3412
    },
    {
      "epoch": 0.47982567130605935,
      "grad_norm": 1.8445419073104858,
      "learning_rate": 0.0001999875057353129,
      "loss": 1.0335,
      "step": 3413
    },
    {
      "epoch": 0.4799662589624631,
      "grad_norm": 1.7106130123138428,
      "learning_rate": 0.00019999091026277928,
      "loss": 1.1542,
      "step": 3414
    },
    {
      "epoch": 0.4801068466188669,
      "grad_norm": 1.4896001815795898,
      "learning_rate": 0.0001999937744189858,
      "loss": 1.0498,
      "step": 3415
    },
    {
      "epoch": 0.4802474342752706,
      "grad_norm": 1.699540138244629,
      "learning_rate": 0.00019999609818845398,
      "loss": 1.1212,
      "step": 3416
    },
    {
      "epoch": 0.4803880219316744,
      "grad_norm": 1.411373257637024,
      "learning_rate": 0.00019999788155862573,
      "loss": 1.1356,
      "step": 3417
    },
    {
      "epoch": 0.4805286095880782,
      "grad_norm": 1.4942293167114258,
      "learning_rate": 0.0001999991245198633,
      "loss": 1.1498,
      "step": 3418
    },
    {
      "epoch": 0.4806691972444819,
      "grad_norm": 1.5200082063674927,
      "learning_rate": 0.00019999982706544954,
      "loss": 1.125,
      "step": 3419
    },
    {
      "epoch": 0.4808097849008857,
      "grad_norm": 1.6338897943496704,
      "learning_rate": 0.00019999998919158768,
      "loss": 1.1915,
      "step": 3420
    },
    {
      "epoch": 0.48095037255728945,
      "grad_norm": 1.7035242319107056,
      "learning_rate": 0.0001999996108974016,
      "loss": 1.1257,
      "step": 3421
    },
    {
      "epoch": 0.48109096021369324,
      "grad_norm": 1.7155303955078125,
      "learning_rate": 0.00019999869218493568,
      "loss": 1.0703,
      "step": 3422
    },
    {
      "epoch": 0.48123154787009703,
      "grad_norm": 1.9401447772979736,
      "learning_rate": 0.00019999723305915484,
      "loss": 0.9731,
      "step": 3423
    },
    {
      "epoch": 0.48137213552650077,
      "grad_norm": 1.555639624595642,
      "learning_rate": 0.00019999523352794442,
      "loss": 1.0495,
      "step": 3424
    },
    {
      "epoch": 0.48151272318290456,
      "grad_norm": 1.5827713012695312,
      "learning_rate": 0.0001999926936021104,
      "loss": 1.0696,
      "step": 3425
    },
    {
      "epoch": 0.4816533108393083,
      "grad_norm": 2.227073907852173,
      "learning_rate": 0.00019998961329537897,
      "loss": 1.192,
      "step": 3426
    },
    {
      "epoch": 0.4817938984957121,
      "grad_norm": 1.5356168746948242,
      "learning_rate": 0.00019998599262439679,
      "loss": 1.115,
      "step": 3427
    },
    {
      "epoch": 0.48193448615211587,
      "grad_norm": 1.6472971439361572,
      "learning_rate": 0.00019998183160873067,
      "loss": 1.0981,
      "step": 3428
    },
    {
      "epoch": 0.4820750738085196,
      "grad_norm": 1.6813911199569702,
      "learning_rate": 0.0001999771302708676,
      "loss": 1.135,
      "step": 3429
    },
    {
      "epoch": 0.4822156614649234,
      "grad_norm": 1.636351227760315,
      "learning_rate": 0.00019997188863621457,
      "loss": 1.0808,
      "step": 3430
    },
    {
      "epoch": 0.48235624912132713,
      "grad_norm": 1.7487175464630127,
      "learning_rate": 0.00019996610673309845,
      "loss": 1.2532,
      "step": 3431
    },
    {
      "epoch": 0.4824968367777309,
      "grad_norm": 1.7238794565200806,
      "learning_rate": 0.0001999597845927658,
      "loss": 1.0662,
      "step": 3432
    },
    {
      "epoch": 0.4826374244341347,
      "grad_norm": 1.5761266946792603,
      "learning_rate": 0.00019995292224938278,
      "loss": 1.0931,
      "step": 3433
    },
    {
      "epoch": 0.48277801209053844,
      "grad_norm": 1.469896912574768,
      "learning_rate": 0.00019994551974003488,
      "loss": 1.169,
      "step": 3434
    },
    {
      "epoch": 0.48291859974694223,
      "grad_norm": 1.6635735034942627,
      "learning_rate": 0.00019993757710472677,
      "loss": 1.1513,
      "step": 3435
    },
    {
      "epoch": 0.48305918740334597,
      "grad_norm": 1.376404047012329,
      "learning_rate": 0.00019992909438638204,
      "loss": 1.2194,
      "step": 3436
    },
    {
      "epoch": 0.48319977505974976,
      "grad_norm": 1.6121550798416138,
      "learning_rate": 0.0001999200716308431,
      "loss": 1.1513,
      "step": 3437
    },
    {
      "epoch": 0.48334036271615355,
      "grad_norm": 1.567616581916809,
      "learning_rate": 0.0001999105088868707,
      "loss": 1.2189,
      "step": 3438
    },
    {
      "epoch": 0.4834809503725573,
      "grad_norm": 1.4781194925308228,
      "learning_rate": 0.00019990040620614387,
      "loss": 1.2117,
      "step": 3439
    },
    {
      "epoch": 0.4836215380289611,
      "grad_norm": 1.7202019691467285,
      "learning_rate": 0.00019988976364325957,
      "loss": 1.0559,
      "step": 3440
    },
    {
      "epoch": 0.4837621256853648,
      "grad_norm": 1.6376553773880005,
      "learning_rate": 0.00019987858125573236,
      "loss": 0.8842,
      "step": 3441
    },
    {
      "epoch": 0.4839027133417686,
      "grad_norm": 1.376672387123108,
      "learning_rate": 0.00019986685910399418,
      "loss": 1.2571,
      "step": 3442
    },
    {
      "epoch": 0.4840433009981724,
      "grad_norm": 1.311773419380188,
      "learning_rate": 0.0001998545972513939,
      "loss": 0.973,
      "step": 3443
    },
    {
      "epoch": 0.4841838886545761,
      "grad_norm": 1.3390560150146484,
      "learning_rate": 0.00019984179576419705,
      "loss": 1.0776,
      "step": 3444
    },
    {
      "epoch": 0.4843244763109799,
      "grad_norm": 1.8996272087097168,
      "learning_rate": 0.00019982845471158548,
      "loss": 1.0858,
      "step": 3445
    },
    {
      "epoch": 0.48446506396738365,
      "grad_norm": 1.4217652082443237,
      "learning_rate": 0.000199814574165657,
      "loss": 1.3005,
      "step": 3446
    },
    {
      "epoch": 0.48460565162378744,
      "grad_norm": 1.466475486755371,
      "learning_rate": 0.0001998001542014249,
      "loss": 1.1148,
      "step": 3447
    },
    {
      "epoch": 0.4847462392801912,
      "grad_norm": 1.4954991340637207,
      "learning_rate": 0.00019978519489681756,
      "loss": 0.9326,
      "step": 3448
    },
    {
      "epoch": 0.48488682693659496,
      "grad_norm": 1.9856195449829102,
      "learning_rate": 0.00019976969633267815,
      "loss": 1.1484,
      "step": 3449
    },
    {
      "epoch": 0.48502741459299875,
      "grad_norm": 1.6031893491744995,
      "learning_rate": 0.00019975365859276406,
      "loss": 1.0996,
      "step": 3450
    },
    {
      "epoch": 0.4851680022494025,
      "grad_norm": 1.8179928064346313,
      "learning_rate": 0.0001997370817637465,
      "loss": 1.0882,
      "step": 3451
    },
    {
      "epoch": 0.4853085899058063,
      "grad_norm": 1.552886962890625,
      "learning_rate": 0.00019971996593521008,
      "loss": 1.2606,
      "step": 3452
    },
    {
      "epoch": 0.48544917756221,
      "grad_norm": 1.5957696437835693,
      "learning_rate": 0.00019970231119965215,
      "loss": 0.9991,
      "step": 3453
    },
    {
      "epoch": 0.4855897652186138,
      "grad_norm": 1.5699517726898193,
      "learning_rate": 0.0001996841176524825,
      "loss": 1.1576,
      "step": 3454
    },
    {
      "epoch": 0.4857303528750176,
      "grad_norm": 1.7023415565490723,
      "learning_rate": 0.00019966538539202287,
      "loss": 1.0385,
      "step": 3455
    },
    {
      "epoch": 0.4858709405314213,
      "grad_norm": 1.7766003608703613,
      "learning_rate": 0.0001996461145195061,
      "loss": 1.0329,
      "step": 3456
    },
    {
      "epoch": 0.4860115281878251,
      "grad_norm": 2.0209293365478516,
      "learning_rate": 0.00019962630513907596,
      "loss": 1.1715,
      "step": 3457
    },
    {
      "epoch": 0.48615211584422885,
      "grad_norm": 1.506122350692749,
      "learning_rate": 0.0001996059573577864,
      "loss": 1.1564,
      "step": 3458
    },
    {
      "epoch": 0.48629270350063264,
      "grad_norm": 1.3697240352630615,
      "learning_rate": 0.00019958507128560096,
      "loss": 1.1852,
      "step": 3459
    },
    {
      "epoch": 0.48643329115703643,
      "grad_norm": 1.672161340713501,
      "learning_rate": 0.00019956364703539218,
      "loss": 1.2014,
      "step": 3460
    },
    {
      "epoch": 0.48657387881344016,
      "grad_norm": 1.8319733142852783,
      "learning_rate": 0.00019954168472294118,
      "loss": 1.0503,
      "step": 3461
    },
    {
      "epoch": 0.48671446646984395,
      "grad_norm": 1.5017945766448975,
      "learning_rate": 0.00019951918446693668,
      "loss": 1.0806,
      "step": 3462
    },
    {
      "epoch": 0.4868550541262477,
      "grad_norm": 1.6148134469985962,
      "learning_rate": 0.0001994961463889747,
      "loss": 1.2438,
      "step": 3463
    },
    {
      "epoch": 0.4869956417826515,
      "grad_norm": 1.7978739738464355,
      "learning_rate": 0.00019947257061355767,
      "loss": 0.9002,
      "step": 3464
    },
    {
      "epoch": 0.48713622943905527,
      "grad_norm": 1.3880726099014282,
      "learning_rate": 0.0001994484572680939,
      "loss": 1.3301,
      "step": 3465
    },
    {
      "epoch": 0.487276817095459,
      "grad_norm": 1.6025991439819336,
      "learning_rate": 0.00019942380648289688,
      "loss": 0.9888,
      "step": 3466
    },
    {
      "epoch": 0.4874174047518628,
      "grad_norm": 1.3286842107772827,
      "learning_rate": 0.00019939861839118441,
      "loss": 1.181,
      "step": 3467
    },
    {
      "epoch": 0.4875579924082665,
      "grad_norm": 1.515671968460083,
      "learning_rate": 0.0001993728931290781,
      "loss": 1.2186,
      "step": 3468
    },
    {
      "epoch": 0.4876985800646703,
      "grad_norm": 1.512807011604309,
      "learning_rate": 0.0001993466308356025,
      "loss": 1.1193,
      "step": 3469
    },
    {
      "epoch": 0.4878391677210741,
      "grad_norm": 1.4050400257110596,
      "learning_rate": 0.00019931983165268438,
      "loss": 1.1505,
      "step": 3470
    },
    {
      "epoch": 0.48797975537747784,
      "grad_norm": 1.9276859760284424,
      "learning_rate": 0.000199292495725152,
      "loss": 1.0555,
      "step": 3471
    },
    {
      "epoch": 0.48812034303388163,
      "grad_norm": 1.5585317611694336,
      "learning_rate": 0.00019926462320073429,
      "loss": 1.181,
      "step": 3472
    },
    {
      "epoch": 0.48826093069028537,
      "grad_norm": 1.4719743728637695,
      "learning_rate": 0.00019923621423006006,
      "loss": 1.0434,
      "step": 3473
    },
    {
      "epoch": 0.48840151834668916,
      "grad_norm": 1.5573036670684814,
      "learning_rate": 0.00019920726896665723,
      "loss": 0.9643,
      "step": 3474
    },
    {
      "epoch": 0.48854210600309295,
      "grad_norm": 1.8796683549880981,
      "learning_rate": 0.00019917778756695177,
      "loss": 1.1414,
      "step": 3475
    },
    {
      "epoch": 0.4886826936594967,
      "grad_norm": 1.4987471103668213,
      "learning_rate": 0.00019914777019026727,
      "loss": 1.2388,
      "step": 3476
    },
    {
      "epoch": 0.48882328131590047,
      "grad_norm": 1.4048137664794922,
      "learning_rate": 0.0001991172169988237,
      "loss": 1.1397,
      "step": 3477
    },
    {
      "epoch": 0.4889638689723042,
      "grad_norm": 1.7440463304519653,
      "learning_rate": 0.00019908612815773682,
      "loss": 1.2548,
      "step": 3478
    },
    {
      "epoch": 0.489104456628708,
      "grad_norm": 1.610785722732544,
      "learning_rate": 0.00019905450383501697,
      "loss": 1.1152,
      "step": 3479
    },
    {
      "epoch": 0.4892450442851118,
      "grad_norm": 1.7589292526245117,
      "learning_rate": 0.00019902234420156849,
      "loss": 1.0287,
      "step": 3480
    },
    {
      "epoch": 0.4893856319415155,
      "grad_norm": 1.8279848098754883,
      "learning_rate": 0.00019898964943118857,
      "loss": 0.9825,
      "step": 3481
    },
    {
      "epoch": 0.4895262195979193,
      "grad_norm": 2.0163145065307617,
      "learning_rate": 0.00019895641970056643,
      "loss": 1.0598,
      "step": 3482
    },
    {
      "epoch": 0.48966680725432304,
      "grad_norm": 1.760863184928894,
      "learning_rate": 0.00019892265518928228,
      "loss": 1.041,
      "step": 3483
    },
    {
      "epoch": 0.48980739491072683,
      "grad_norm": 1.4557090997695923,
      "learning_rate": 0.00019888835607980644,
      "loss": 1.0896,
      "step": 3484
    },
    {
      "epoch": 0.4899479825671306,
      "grad_norm": 1.3904736042022705,
      "learning_rate": 0.0001988535225574983,
      "loss": 1.1218,
      "step": 3485
    },
    {
      "epoch": 0.49008857022353436,
      "grad_norm": 1.5134161710739136,
      "learning_rate": 0.0001988181548106053,
      "loss": 0.9855,
      "step": 3486
    },
    {
      "epoch": 0.49022915787993815,
      "grad_norm": 1.5753072500228882,
      "learning_rate": 0.00019878225303026197,
      "loss": 1.1725,
      "step": 3487
    },
    {
      "epoch": 0.4903697455363419,
      "grad_norm": 1.6741448640823364,
      "learning_rate": 0.00019874581741048878,
      "loss": 1.0701,
      "step": 3488
    },
    {
      "epoch": 0.4905103331927457,
      "grad_norm": 1.6311262845993042,
      "learning_rate": 0.00019870884814819135,
      "loss": 1.1096,
      "step": 3489
    },
    {
      "epoch": 0.49065092084914946,
      "grad_norm": 1.5495127439498901,
      "learning_rate": 0.00019867134544315903,
      "loss": 1.1031,
      "step": 3490
    },
    {
      "epoch": 0.4907915085055532,
      "grad_norm": 1.4549237489700317,
      "learning_rate": 0.00019863330949806415,
      "loss": 1.2845,
      "step": 3491
    },
    {
      "epoch": 0.490932096161957,
      "grad_norm": 1.4402852058410645,
      "learning_rate": 0.00019859474051846064,
      "loss": 1.0099,
      "step": 3492
    },
    {
      "epoch": 0.4910726838183607,
      "grad_norm": 1.6489686965942383,
      "learning_rate": 0.00019855563871278316,
      "loss": 1.1968,
      "step": 3493
    },
    {
      "epoch": 0.4912132714747645,
      "grad_norm": 1.7465388774871826,
      "learning_rate": 0.00019851600429234584,
      "loss": 1.1014,
      "step": 3494
    },
    {
      "epoch": 0.4913538591311683,
      "grad_norm": 1.3914461135864258,
      "learning_rate": 0.00019847583747134118,
      "loss": 1.1588,
      "step": 3495
    },
    {
      "epoch": 0.49149444678757204,
      "grad_norm": 1.6746124029159546,
      "learning_rate": 0.0001984351384668388,
      "loss": 1.0578,
      "step": 3496
    },
    {
      "epoch": 0.4916350344439758,
      "grad_norm": 1.4225691556930542,
      "learning_rate": 0.0001983939074987845,
      "loss": 1.0351,
      "step": 3497
    },
    {
      "epoch": 0.49177562210037956,
      "grad_norm": 1.4606013298034668,
      "learning_rate": 0.00019835214478999878,
      "loss": 1.0754,
      "step": 3498
    },
    {
      "epoch": 0.49191620975678335,
      "grad_norm": 1.2552516460418701,
      "learning_rate": 0.00019830985056617587,
      "loss": 1.2484,
      "step": 3499
    },
    {
      "epoch": 0.49205679741318714,
      "grad_norm": 1.3750171661376953,
      "learning_rate": 0.00019826702505588238,
      "loss": 1.0905,
      "step": 3500
    },
    {
      "epoch": 0.49205679741318714,
      "eval_loss": 1.1756529808044434,
      "eval_runtime": 771.4931,
      "eval_samples_per_second": 16.392,
      "eval_steps_per_second": 8.196,
      "step": 3500
    },
    {
      "epoch": 0.4921973850695909,
      "grad_norm": 1.7237358093261719,
      "learning_rate": 0.00019822366849055604,
      "loss": 1.0848,
      "step": 3501
    },
    {
      "epoch": 0.49233797272599467,
      "grad_norm": 1.8151134252548218,
      "learning_rate": 0.0001981797811045046,
      "loss": 1.1212,
      "step": 3502
    },
    {
      "epoch": 0.4924785603823984,
      "grad_norm": 1.531727910041809,
      "learning_rate": 0.00019813536313490447,
      "loss": 1.057,
      "step": 3503
    },
    {
      "epoch": 0.4926191480388022,
      "grad_norm": 1.536389946937561,
      "learning_rate": 0.00019809041482179936,
      "loss": 1.1396,
      "step": 3504
    },
    {
      "epoch": 0.492759735695206,
      "grad_norm": 1.9208904504776,
      "learning_rate": 0.00019804493640809912,
      "loss": 0.8549,
      "step": 3505
    },
    {
      "epoch": 0.4929003233516097,
      "grad_norm": 1.4396376609802246,
      "learning_rate": 0.00019799892813957844,
      "loss": 1.1724,
      "step": 3506
    },
    {
      "epoch": 0.4930409110080135,
      "grad_norm": 1.7213244438171387,
      "learning_rate": 0.00019795239026487526,
      "loss": 1.0804,
      "step": 3507
    },
    {
      "epoch": 0.49318149866441724,
      "grad_norm": 1.7996331453323364,
      "learning_rate": 0.00019790532303548986,
      "loss": 1.2397,
      "step": 3508
    },
    {
      "epoch": 0.49332208632082103,
      "grad_norm": 1.8622909784317017,
      "learning_rate": 0.00019785772670578307,
      "loss": 1.1637,
      "step": 3509
    },
    {
      "epoch": 0.4934626739772248,
      "grad_norm": 1.4775292873382568,
      "learning_rate": 0.00019780960153297517,
      "loss": 1.2897,
      "step": 3510
    },
    {
      "epoch": 0.49360326163362855,
      "grad_norm": 2.015002965927124,
      "learning_rate": 0.00019776094777714437,
      "loss": 1.1114,
      "step": 3511
    },
    {
      "epoch": 0.49374384929003234,
      "grad_norm": 1.4867494106292725,
      "learning_rate": 0.0001977117657012256,
      "loss": 1.0002,
      "step": 3512
    },
    {
      "epoch": 0.4938844369464361,
      "grad_norm": 1.809673547744751,
      "learning_rate": 0.0001976620555710087,
      "loss": 1.2189,
      "step": 3513
    },
    {
      "epoch": 0.49402502460283987,
      "grad_norm": 1.4428997039794922,
      "learning_rate": 0.00019761181765513738,
      "loss": 1.1148,
      "step": 3514
    },
    {
      "epoch": 0.49416561225924366,
      "grad_norm": 1.4905405044555664,
      "learning_rate": 0.00019756105222510758,
      "loss": 1.1767,
      "step": 3515
    },
    {
      "epoch": 0.4943061999156474,
      "grad_norm": 1.3486416339874268,
      "learning_rate": 0.00019750975955526608,
      "loss": 1.1153,
      "step": 3516
    },
    {
      "epoch": 0.4944467875720512,
      "grad_norm": 1.7866111993789673,
      "learning_rate": 0.00019745793992280883,
      "loss": 1.2051,
      "step": 3517
    },
    {
      "epoch": 0.4945873752284549,
      "grad_norm": 1.443369746208191,
      "learning_rate": 0.0001974055936077798,
      "loss": 1.036,
      "step": 3518
    },
    {
      "epoch": 0.4947279628848587,
      "grad_norm": 1.4128276109695435,
      "learning_rate": 0.000197352720893069,
      "loss": 1.2127,
      "step": 3519
    },
    {
      "epoch": 0.4948685505412625,
      "grad_norm": 1.3866122961044312,
      "learning_rate": 0.0001972993220644115,
      "loss": 1.1853,
      "step": 3520
    },
    {
      "epoch": 0.49500913819766623,
      "grad_norm": 1.383903980255127,
      "learning_rate": 0.0001972453974103854,
      "loss": 1.3033,
      "step": 3521
    },
    {
      "epoch": 0.49514972585407,
      "grad_norm": 1.4273645877838135,
      "learning_rate": 0.00019719094722241048,
      "loss": 1.0272,
      "step": 3522
    },
    {
      "epoch": 0.49529031351047376,
      "grad_norm": 1.469364881515503,
      "learning_rate": 0.0001971359717947467,
      "loss": 1.155,
      "step": 3523
    },
    {
      "epoch": 0.49543090116687755,
      "grad_norm": 1.6330296993255615,
      "learning_rate": 0.00019708047142449244,
      "loss": 1.2756,
      "step": 3524
    },
    {
      "epoch": 0.49557148882328134,
      "grad_norm": 1.8993473052978516,
      "learning_rate": 0.0001970244464115831,
      "loss": 1.1049,
      "step": 3525
    },
    {
      "epoch": 0.49571207647968507,
      "grad_norm": 1.6570795774459839,
      "learning_rate": 0.00019696789705878916,
      "loss": 1.2472,
      "step": 3526
    },
    {
      "epoch": 0.49585266413608886,
      "grad_norm": 1.444677472114563,
      "learning_rate": 0.0001969108236717149,
      "loss": 1.1325,
      "step": 3527
    },
    {
      "epoch": 0.4959932517924926,
      "grad_norm": 1.5004249811172485,
      "learning_rate": 0.00019685322655879658,
      "loss": 1.0813,
      "step": 3528
    },
    {
      "epoch": 0.4961338394488964,
      "grad_norm": 1.3776262998580933,
      "learning_rate": 0.00019679510603130064,
      "loss": 1.3089,
      "step": 3529
    },
    {
      "epoch": 0.4962744271053002,
      "grad_norm": 1.8351444005966187,
      "learning_rate": 0.00019673646240332235,
      "loss": 1.1045,
      "step": 3530
    },
    {
      "epoch": 0.4964150147617039,
      "grad_norm": 1.5332343578338623,
      "learning_rate": 0.00019667729599178374,
      "loss": 1.1828,
      "step": 3531
    },
    {
      "epoch": 0.4965556024181077,
      "grad_norm": 1.5084378719329834,
      "learning_rate": 0.00019661760711643225,
      "loss": 0.9543,
      "step": 3532
    },
    {
      "epoch": 0.49669619007451143,
      "grad_norm": 1.5066108703613281,
      "learning_rate": 0.00019655739609983867,
      "loss": 1.232,
      "step": 3533
    },
    {
      "epoch": 0.4968367777309152,
      "grad_norm": 1.3503634929656982,
      "learning_rate": 0.00019649666326739565,
      "loss": 1.113,
      "step": 3534
    },
    {
      "epoch": 0.496977365387319,
      "grad_norm": 1.854359745979309,
      "learning_rate": 0.00019643540894731572,
      "loss": 1.2639,
      "step": 3535
    },
    {
      "epoch": 0.49711795304372275,
      "grad_norm": 1.7596244812011719,
      "learning_rate": 0.00019637363347062975,
      "loss": 1.0485,
      "step": 3536
    },
    {
      "epoch": 0.49725854070012654,
      "grad_norm": 1.497044324874878,
      "learning_rate": 0.00019631133717118505,
      "loss": 1.1247,
      "step": 3537
    },
    {
      "epoch": 0.4973991283565303,
      "grad_norm": 1.6081379652023315,
      "learning_rate": 0.00019624852038564345,
      "loss": 1.152,
      "step": 3538
    },
    {
      "epoch": 0.49753971601293406,
      "grad_norm": 1.4462971687316895,
      "learning_rate": 0.00019618518345347974,
      "loss": 1.1622,
      "step": 3539
    },
    {
      "epoch": 0.49768030366933785,
      "grad_norm": 1.5896004438400269,
      "learning_rate": 0.00019612132671697954,
      "loss": 1.04,
      "step": 3540
    },
    {
      "epoch": 0.4978208913257416,
      "grad_norm": 1.5084689855575562,
      "learning_rate": 0.00019605695052123768,
      "loss": 1.1197,
      "step": 3541
    },
    {
      "epoch": 0.4979614789821454,
      "grad_norm": 1.7922512292861938,
      "learning_rate": 0.0001959920552141563,
      "loss": 1.2242,
      "step": 3542
    },
    {
      "epoch": 0.4981020666385491,
      "grad_norm": 1.847695231437683,
      "learning_rate": 0.0001959266411464428,
      "loss": 1.0199,
      "step": 3543
    },
    {
      "epoch": 0.4982426542949529,
      "grad_norm": 1.5519657135009766,
      "learning_rate": 0.0001958607086716082,
      "loss": 1.071,
      "step": 3544
    },
    {
      "epoch": 0.4983832419513567,
      "grad_norm": 2.008594512939453,
      "learning_rate": 0.00019579425814596497,
      "loss": 1.1112,
      "step": 3545
    },
    {
      "epoch": 0.4985238296077604,
      "grad_norm": 1.805629014968872,
      "learning_rate": 0.00019572728992862533,
      "loss": 1.1659,
      "step": 3546
    },
    {
      "epoch": 0.4986644172641642,
      "grad_norm": 1.6197986602783203,
      "learning_rate": 0.00019565980438149914,
      "loss": 1.1983,
      "step": 3547
    },
    {
      "epoch": 0.49880500492056795,
      "grad_norm": 1.4621025323867798,
      "learning_rate": 0.0001955918018692921,
      "loss": 1.1069,
      "step": 3548
    },
    {
      "epoch": 0.49894559257697174,
      "grad_norm": 1.6799734830856323,
      "learning_rate": 0.00019552328275950363,
      "loss": 1.0126,
      "step": 3549
    },
    {
      "epoch": 0.49908618023337553,
      "grad_norm": 1.523012638092041,
      "learning_rate": 0.00019545424742242499,
      "loss": 1.0866,
      "step": 3550
    },
    {
      "epoch": 0.49922676788977927,
      "grad_norm": 1.6338179111480713,
      "learning_rate": 0.0001953846962311371,
      "loss": 1.093,
      "step": 3551
    },
    {
      "epoch": 0.49936735554618306,
      "grad_norm": 1.3568872213363647,
      "learning_rate": 0.00019531462956150894,
      "loss": 1.0566,
      "step": 3552
    },
    {
      "epoch": 0.4995079432025868,
      "grad_norm": 1.4436982870101929,
      "learning_rate": 0.00019524404779219497,
      "loss": 1.2121,
      "step": 3553
    },
    {
      "epoch": 0.4996485308589906,
      "grad_norm": 1.3861697912216187,
      "learning_rate": 0.00019517295130463348,
      "loss": 1.2984,
      "step": 3554
    },
    {
      "epoch": 0.49978911851539437,
      "grad_norm": 1.5952372550964355,
      "learning_rate": 0.00019510134048304433,
      "loss": 0.9834,
      "step": 3555
    },
    {
      "epoch": 0.4999297061717981,
      "grad_norm": 1.620132327079773,
      "learning_rate": 0.0001950292157144271,
      "loss": 1.1332,
      "step": 3556
    },
    {
      "epoch": 0.5000702938282019,
      "grad_norm": 1.512007474899292,
      "learning_rate": 0.00019495657738855869,
      "loss": 1.0817,
      "step": 3557
    },
    {
      "epoch": 0.5002108814846057,
      "grad_norm": 1.6786788702011108,
      "learning_rate": 0.00019488342589799138,
      "loss": 1.2406,
      "step": 3558
    },
    {
      "epoch": 0.5003514691410094,
      "grad_norm": 1.6350194215774536,
      "learning_rate": 0.00019480976163805078,
      "loss": 0.9232,
      "step": 3559
    },
    {
      "epoch": 0.5004920567974132,
      "grad_norm": 1.6930019855499268,
      "learning_rate": 0.00019473558500683358,
      "loss": 1.2865,
      "step": 3560
    },
    {
      "epoch": 0.5006326444538169,
      "grad_norm": 1.4427614212036133,
      "learning_rate": 0.0001946608964052054,
      "loss": 1.2478,
      "step": 3561
    },
    {
      "epoch": 0.5007732321102207,
      "grad_norm": 1.7015951871871948,
      "learning_rate": 0.0001945856962367986,
      "loss": 1.1991,
      "step": 3562
    },
    {
      "epoch": 0.5009138197666245,
      "grad_norm": 2.0140883922576904,
      "learning_rate": 0.0001945099849080103,
      "loss": 0.9701,
      "step": 3563
    },
    {
      "epoch": 0.5010544074230282,
      "grad_norm": 1.9378361701965332,
      "learning_rate": 0.00019443376282799997,
      "loss": 1.1062,
      "step": 3564
    },
    {
      "epoch": 0.501194995079432,
      "grad_norm": 1.4531147480010986,
      "learning_rate": 0.00019435703040868722,
      "loss": 1.127,
      "step": 3565
    },
    {
      "epoch": 0.5013355827358358,
      "grad_norm": 1.6864938735961914,
      "learning_rate": 0.0001942797880647496,
      "loss": 1.2018,
      "step": 3566
    },
    {
      "epoch": 0.5014761703922396,
      "grad_norm": 1.5693116188049316,
      "learning_rate": 0.00019420203621362063,
      "loss": 1.2246,
      "step": 3567
    },
    {
      "epoch": 0.5016167580486434,
      "grad_norm": 2.065089464187622,
      "learning_rate": 0.0001941237752754871,
      "loss": 1.1075,
      "step": 3568
    },
    {
      "epoch": 0.501757345705047,
      "grad_norm": 1.5827796459197998,
      "learning_rate": 0.0001940450056732871,
      "loss": 1.3158,
      "step": 3569
    },
    {
      "epoch": 0.5018979333614508,
      "grad_norm": 1.71378493309021,
      "learning_rate": 0.00019396572783270753,
      "loss": 1.1185,
      "step": 3570
    },
    {
      "epoch": 0.5020385210178546,
      "grad_norm": 1.411409854888916,
      "learning_rate": 0.0001938859421821821,
      "loss": 1.0248,
      "step": 3571
    },
    {
      "epoch": 0.5021791086742584,
      "grad_norm": 1.702235460281372,
      "learning_rate": 0.00019380564915288866,
      "loss": 1.0443,
      "step": 3572
    },
    {
      "epoch": 0.5023196963306622,
      "grad_norm": 1.5470056533813477,
      "learning_rate": 0.00019372484917874717,
      "loss": 1.064,
      "step": 3573
    },
    {
      "epoch": 0.5024602839870659,
      "grad_norm": 1.5414520502090454,
      "learning_rate": 0.00019364354269641704,
      "loss": 1.2807,
      "step": 3574
    },
    {
      "epoch": 0.5026008716434697,
      "grad_norm": 1.5100198984146118,
      "learning_rate": 0.00019356173014529522,
      "loss": 1.308,
      "step": 3575
    },
    {
      "epoch": 0.5027414592998735,
      "grad_norm": 1.5485409498214722,
      "learning_rate": 0.0001934794119675133,
      "loss": 1.0455,
      "step": 3576
    },
    {
      "epoch": 0.5028820469562773,
      "grad_norm": 1.469976544380188,
      "learning_rate": 0.00019339658860793553,
      "loss": 1.1031,
      "step": 3577
    },
    {
      "epoch": 0.503022634612681,
      "grad_norm": 1.8500107526779175,
      "learning_rate": 0.00019331326051415628,
      "loss": 1.0567,
      "step": 3578
    },
    {
      "epoch": 0.5031632222690847,
      "grad_norm": 1.6106845140457153,
      "learning_rate": 0.0001932294281364975,
      "loss": 1.1954,
      "step": 3579
    },
    {
      "epoch": 0.5033038099254885,
      "grad_norm": 1.7810707092285156,
      "learning_rate": 0.00019314509192800645,
      "loss": 1.06,
      "step": 3580
    },
    {
      "epoch": 0.5034443975818923,
      "grad_norm": 1.5509262084960938,
      "learning_rate": 0.00019306025234445314,
      "loss": 1.1953,
      "step": 3581
    },
    {
      "epoch": 0.5035849852382961,
      "grad_norm": 1.5146313905715942,
      "learning_rate": 0.00019297490984432808,
      "loss": 1.1797,
      "step": 3582
    },
    {
      "epoch": 0.5037255728946999,
      "grad_norm": 1.679510235786438,
      "learning_rate": 0.0001928890648888395,
      "loss": 1.0985,
      "step": 3583
    },
    {
      "epoch": 0.5038661605511036,
      "grad_norm": 1.5291295051574707,
      "learning_rate": 0.000192802717941911,
      "loss": 1.3009,
      "step": 3584
    },
    {
      "epoch": 0.5040067482075073,
      "grad_norm": 1.523842215538025,
      "learning_rate": 0.00019271586947017903,
      "loss": 1.235,
      "step": 3585
    },
    {
      "epoch": 0.5041473358639111,
      "grad_norm": 1.4023487567901611,
      "learning_rate": 0.0001926285199429906,
      "loss": 1.1918,
      "step": 3586
    },
    {
      "epoch": 0.5042879235203149,
      "grad_norm": 1.9775969982147217,
      "learning_rate": 0.0001925406698324002,
      "loss": 1.0506,
      "step": 3587
    },
    {
      "epoch": 0.5044285111767187,
      "grad_norm": 1.2975975275039673,
      "learning_rate": 0.00019245231961316782,
      "loss": 1.1475,
      "step": 3588
    },
    {
      "epoch": 0.5045690988331224,
      "grad_norm": 1.9386390447616577,
      "learning_rate": 0.000192363469762756,
      "loss": 1.1351,
      "step": 3589
    },
    {
      "epoch": 0.5047096864895262,
      "grad_norm": 1.4826356172561646,
      "learning_rate": 0.00019227412076132752,
      "loss": 1.1056,
      "step": 3590
    },
    {
      "epoch": 0.50485027414593,
      "grad_norm": 1.5296432971954346,
      "learning_rate": 0.0001921842730917425,
      "loss": 1.1189,
      "step": 3591
    },
    {
      "epoch": 0.5049908618023338,
      "grad_norm": 1.8712908029556274,
      "learning_rate": 0.00019209392723955614,
      "loss": 1.0707,
      "step": 3592
    },
    {
      "epoch": 0.5051314494587376,
      "grad_norm": 1.9690479040145874,
      "learning_rate": 0.00019200308369301573,
      "loss": 1.0886,
      "step": 3593
    },
    {
      "epoch": 0.5052720371151412,
      "grad_norm": 1.47812020778656,
      "learning_rate": 0.00019191174294305846,
      "loss": 1.1917,
      "step": 3594
    },
    {
      "epoch": 0.505412624771545,
      "grad_norm": 1.6938480138778687,
      "learning_rate": 0.00019181990548330826,
      "loss": 1.0703,
      "step": 3595
    },
    {
      "epoch": 0.5055532124279488,
      "grad_norm": 1.5146780014038086,
      "learning_rate": 0.00019172757181007354,
      "loss": 1.2952,
      "step": 3596
    },
    {
      "epoch": 0.5056938000843526,
      "grad_norm": 1.4516061544418335,
      "learning_rate": 0.00019163474242234419,
      "loss": 1.1758,
      "step": 3597
    },
    {
      "epoch": 0.5058343877407564,
      "grad_norm": 1.773118019104004,
      "learning_rate": 0.00019154141782178928,
      "loss": 1.0024,
      "step": 3598
    },
    {
      "epoch": 0.5059749753971601,
      "grad_norm": 1.5232595205307007,
      "learning_rate": 0.00019144759851275387,
      "loss": 1.2407,
      "step": 3599
    },
    {
      "epoch": 0.5061155630535639,
      "grad_norm": 1.5919992923736572,
      "learning_rate": 0.00019135328500225667,
      "loss": 1.1946,
      "step": 3600
    },
    {
      "epoch": 0.5062561507099677,
      "grad_norm": 1.7809345722198486,
      "learning_rate": 0.00019125847779998708,
      "loss": 1.0593,
      "step": 3601
    },
    {
      "epoch": 0.5063967383663714,
      "grad_norm": 1.6356592178344727,
      "learning_rate": 0.0001911631774183026,
      "loss": 1.1708,
      "step": 3602
    },
    {
      "epoch": 0.5065373260227752,
      "grad_norm": 1.5724273920059204,
      "learning_rate": 0.0001910673843722259,
      "loss": 1.157,
      "step": 3603
    },
    {
      "epoch": 0.5066779136791789,
      "grad_norm": 1.6084489822387695,
      "learning_rate": 0.0001909710991794421,
      "loss": 1.1888,
      "step": 3604
    },
    {
      "epoch": 0.5068185013355827,
      "grad_norm": 1.4469398260116577,
      "learning_rate": 0.000190874322360296,
      "loss": 0.9837,
      "step": 3605
    },
    {
      "epoch": 0.5069590889919865,
      "grad_norm": 1.3576964139938354,
      "learning_rate": 0.0001907770544377893,
      "loss": 1.1889,
      "step": 3606
    },
    {
      "epoch": 0.5070996766483903,
      "grad_norm": 1.7188951969146729,
      "learning_rate": 0.0001906792959375777,
      "loss": 0.9657,
      "step": 3607
    },
    {
      "epoch": 0.5072402643047941,
      "grad_norm": 1.7029569149017334,
      "learning_rate": 0.00019058104738796798,
      "loss": 1.1444,
      "step": 3608
    },
    {
      "epoch": 0.5073808519611978,
      "grad_norm": 2.33640456199646,
      "learning_rate": 0.00019048230931991534,
      "loss": 1.2293,
      "step": 3609
    },
    {
      "epoch": 0.5075214396176015,
      "grad_norm": 1.683066725730896,
      "learning_rate": 0.00019038308226702048,
      "loss": 1.1839,
      "step": 3610
    },
    {
      "epoch": 0.5076620272740053,
      "grad_norm": 1.7087126970291138,
      "learning_rate": 0.00019028336676552663,
      "loss": 1.2019,
      "step": 3611
    },
    {
      "epoch": 0.5078026149304091,
      "grad_norm": 1.789767861366272,
      "learning_rate": 0.0001901831633543166,
      "loss": 1.0006,
      "step": 3612
    },
    {
      "epoch": 0.5079432025868129,
      "grad_norm": 1.5411628484725952,
      "learning_rate": 0.00019008247257491009,
      "loss": 1.0269,
      "step": 3613
    },
    {
      "epoch": 0.5080837902432166,
      "grad_norm": 1.6936371326446533,
      "learning_rate": 0.0001899812949714606,
      "loss": 1.0978,
      "step": 3614
    },
    {
      "epoch": 0.5082243778996204,
      "grad_norm": 1.6971980333328247,
      "learning_rate": 0.00018987963109075255,
      "loss": 1.1974,
      "step": 3615
    },
    {
      "epoch": 0.5083649655560242,
      "grad_norm": 1.5394200086593628,
      "learning_rate": 0.0001897774814821982,
      "loss": 1.3009,
      "step": 3616
    },
    {
      "epoch": 0.508505553212428,
      "grad_norm": 1.7504249811172485,
      "learning_rate": 0.00018967484669783495,
      "loss": 1.0563,
      "step": 3617
    },
    {
      "epoch": 0.5086461408688318,
      "grad_norm": 1.6234488487243652,
      "learning_rate": 0.000189571727292322,
      "loss": 1.1178,
      "step": 3618
    },
    {
      "epoch": 0.5087867285252354,
      "grad_norm": 1.6255872249603271,
      "learning_rate": 0.0001894681238229377,
      "loss": 1.01,
      "step": 3619
    },
    {
      "epoch": 0.5089273161816392,
      "grad_norm": 1.4783170223236084,
      "learning_rate": 0.00018936403684957629,
      "loss": 1.1807,
      "step": 3620
    },
    {
      "epoch": 0.509067903838043,
      "grad_norm": 1.5387217998504639,
      "learning_rate": 0.000189259466934745,
      "loss": 1.0924,
      "step": 3621
    },
    {
      "epoch": 0.5092084914944468,
      "grad_norm": 1.6021257638931274,
      "learning_rate": 0.000189154414643561,
      "loss": 1.2175,
      "step": 3622
    },
    {
      "epoch": 0.5093490791508506,
      "grad_norm": 1.6728668212890625,
      "learning_rate": 0.00018904888054374817,
      "loss": 1.0964,
      "step": 3623
    },
    {
      "epoch": 0.5094896668072543,
      "grad_norm": 1.4190289974212646,
      "learning_rate": 0.0001889428652056344,
      "loss": 1.2169,
      "step": 3624
    },
    {
      "epoch": 0.5096302544636581,
      "grad_norm": 1.6314561367034912,
      "learning_rate": 0.00018883636920214816,
      "loss": 1.1413,
      "step": 3625
    },
    {
      "epoch": 0.5097708421200619,
      "grad_norm": 1.6979494094848633,
      "learning_rate": 0.00018872939310881558,
      "loss": 1.0401,
      "step": 3626
    },
    {
      "epoch": 0.5099114297764656,
      "grad_norm": 1.96439790725708,
      "learning_rate": 0.0001886219375037572,
      "loss": 1.2187,
      "step": 3627
    },
    {
      "epoch": 0.5100520174328694,
      "grad_norm": 1.4975608587265015,
      "learning_rate": 0.00018851400296768508,
      "loss": 1.2921,
      "step": 3628
    },
    {
      "epoch": 0.5101926050892731,
      "grad_norm": 1.5817891359329224,
      "learning_rate": 0.0001884055900838994,
      "loss": 1.1487,
      "step": 3629
    },
    {
      "epoch": 0.5103331927456769,
      "grad_norm": 1.6842440366744995,
      "learning_rate": 0.0001882966994382856,
      "loss": 1.3805,
      "step": 3630
    },
    {
      "epoch": 0.5104737804020807,
      "grad_norm": 1.6529258489608765,
      "learning_rate": 0.0001881873316193108,
      "loss": 1.0708,
      "step": 3631
    },
    {
      "epoch": 0.5106143680584845,
      "grad_norm": 1.581998348236084,
      "learning_rate": 0.00018807748721802096,
      "loss": 1.168,
      "step": 3632
    },
    {
      "epoch": 0.5107549557148883,
      "grad_norm": 1.7644962072372437,
      "learning_rate": 0.00018796716682803775,
      "loss": 1.0663,
      "step": 3633
    },
    {
      "epoch": 0.510895543371292,
      "grad_norm": 1.568817138671875,
      "learning_rate": 0.00018785637104555496,
      "loss": 1.1909,
      "step": 3634
    },
    {
      "epoch": 0.5110361310276957,
      "grad_norm": 1.4183634519577026,
      "learning_rate": 0.00018774510046933558,
      "loss": 1.2442,
      "step": 3635
    },
    {
      "epoch": 0.5111767186840995,
      "grad_norm": 1.6716878414154053,
      "learning_rate": 0.00018763335570070848,
      "loss": 1.073,
      "step": 3636
    },
    {
      "epoch": 0.5113173063405033,
      "grad_norm": 1.737038254737854,
      "learning_rate": 0.0001875211373435652,
      "loss": 1.3673,
      "step": 3637
    },
    {
      "epoch": 0.5114578939969071,
      "grad_norm": 1.5697096586227417,
      "learning_rate": 0.00018740844600435657,
      "loss": 1.1554,
      "step": 3638
    },
    {
      "epoch": 0.5115984816533108,
      "grad_norm": 1.3224068880081177,
      "learning_rate": 0.00018729528229208964,
      "loss": 1.2692,
      "step": 3639
    },
    {
      "epoch": 0.5117390693097146,
      "grad_norm": 1.9877614974975586,
      "learning_rate": 0.00018718164681832405,
      "loss": 1.1216,
      "step": 3640
    },
    {
      "epoch": 0.5118796569661184,
      "grad_norm": 1.9939446449279785,
      "learning_rate": 0.00018706754019716919,
      "loss": 1.1583,
      "step": 3641
    },
    {
      "epoch": 0.5120202446225222,
      "grad_norm": 2.0314769744873047,
      "learning_rate": 0.00018695296304528043,
      "loss": 1.0589,
      "step": 3642
    },
    {
      "epoch": 0.512160832278926,
      "grad_norm": 1.5715000629425049,
      "learning_rate": 0.00018683791598185607,
      "loss": 1.1363,
      "step": 3643
    },
    {
      "epoch": 0.5123014199353296,
      "grad_norm": 1.4694231748580933,
      "learning_rate": 0.00018672239962863382,
      "loss": 1.2592,
      "step": 3644
    },
    {
      "epoch": 0.5124420075917334,
      "grad_norm": 1.603505253791809,
      "learning_rate": 0.00018660641460988776,
      "loss": 1.1111,
      "step": 3645
    },
    {
      "epoch": 0.5125825952481372,
      "grad_norm": 1.4830131530761719,
      "learning_rate": 0.00018648996155242443,
      "loss": 1.1194,
      "step": 3646
    },
    {
      "epoch": 0.512723182904541,
      "grad_norm": 1.3449971675872803,
      "learning_rate": 0.00018637304108557997,
      "loss": 1.0715,
      "step": 3647
    },
    {
      "epoch": 0.5128637705609448,
      "grad_norm": 2.0291547775268555,
      "learning_rate": 0.00018625565384121625,
      "loss": 1.0919,
      "step": 3648
    },
    {
      "epoch": 0.5130043582173485,
      "grad_norm": 2.139681100845337,
      "learning_rate": 0.00018613780045371813,
      "loss": 1.0865,
      "step": 3649
    },
    {
      "epoch": 0.5131449458737523,
      "grad_norm": 1.7162889242172241,
      "learning_rate": 0.00018601948155998917,
      "loss": 1.1391,
      "step": 3650
    },
    {
      "epoch": 0.513285533530156,
      "grad_norm": 1.5887316465377808,
      "learning_rate": 0.00018590069779944883,
      "loss": 1.0121,
      "step": 3651
    },
    {
      "epoch": 0.5134261211865598,
      "grad_norm": 1.5829851627349854,
      "learning_rate": 0.00018578144981402868,
      "loss": 1.1865,
      "step": 3652
    },
    {
      "epoch": 0.5135667088429636,
      "grad_norm": 1.59730064868927,
      "learning_rate": 0.00018566173824816927,
      "loss": 1.1183,
      "step": 3653
    },
    {
      "epoch": 0.5137072964993673,
      "grad_norm": 1.5172289609909058,
      "learning_rate": 0.00018554156374881622,
      "loss": 1.1443,
      "step": 3654
    },
    {
      "epoch": 0.5138478841557711,
      "grad_norm": 1.6195428371429443,
      "learning_rate": 0.00018542092696541707,
      "loss": 1.0919,
      "step": 3655
    },
    {
      "epoch": 0.5139884718121749,
      "grad_norm": 1.8147934675216675,
      "learning_rate": 0.00018529982854991752,
      "loss": 1.0918,
      "step": 3656
    },
    {
      "epoch": 0.5141290594685787,
      "grad_norm": 1.7218654155731201,
      "learning_rate": 0.0001851782691567582,
      "loss": 1.0603,
      "step": 3657
    },
    {
      "epoch": 0.5142696471249825,
      "grad_norm": 1.5269746780395508,
      "learning_rate": 0.0001850562494428707,
      "loss": 1.2032,
      "step": 3658
    },
    {
      "epoch": 0.5144102347813861,
      "grad_norm": 1.6778831481933594,
      "learning_rate": 0.0001849337700676745,
      "loss": 1.2608,
      "step": 3659
    },
    {
      "epoch": 0.5145508224377899,
      "grad_norm": 1.8773914575576782,
      "learning_rate": 0.00018481083169307315,
      "loss": 0.9957,
      "step": 3660
    },
    {
      "epoch": 0.5146914100941937,
      "grad_norm": 1.8511583805084229,
      "learning_rate": 0.00018468743498345066,
      "loss": 1.0964,
      "step": 3661
    },
    {
      "epoch": 0.5148319977505975,
      "grad_norm": 1.5718145370483398,
      "learning_rate": 0.00018456358060566798,
      "loss": 1.1992,
      "step": 3662
    },
    {
      "epoch": 0.5149725854070013,
      "grad_norm": 1.4890737533569336,
      "learning_rate": 0.00018443926922905935,
      "loss": 1.0996,
      "step": 3663
    },
    {
      "epoch": 0.515113173063405,
      "grad_norm": 1.593003273010254,
      "learning_rate": 0.00018431450152542892,
      "loss": 1.0941,
      "step": 3664
    },
    {
      "epoch": 0.5152537607198088,
      "grad_norm": 1.6489965915679932,
      "learning_rate": 0.0001841892781690467,
      "loss": 1.1548,
      "step": 3665
    },
    {
      "epoch": 0.5153943483762126,
      "grad_norm": 1.9510236978530884,
      "learning_rate": 0.00018406359983664532,
      "loss": 1.0404,
      "step": 3666
    },
    {
      "epoch": 0.5155349360326164,
      "grad_norm": 1.3901524543762207,
      "learning_rate": 0.00018393746720741594,
      "loss": 1.1374,
      "step": 3667
    },
    {
      "epoch": 0.5156755236890201,
      "grad_norm": 1.4264352321624756,
      "learning_rate": 0.00018381088096300517,
      "loss": 1.2286,
      "step": 3668
    },
    {
      "epoch": 0.5158161113454238,
      "grad_norm": 1.6209874153137207,
      "learning_rate": 0.00018368384178751076,
      "loss": 0.956,
      "step": 3669
    },
    {
      "epoch": 0.5159566990018276,
      "grad_norm": 1.5290089845657349,
      "learning_rate": 0.0001835563503674784,
      "loss": 1.1457,
      "step": 3670
    },
    {
      "epoch": 0.5160972866582314,
      "grad_norm": 1.6175745725631714,
      "learning_rate": 0.0001834284073918976,
      "loss": 1.0331,
      "step": 3671
    },
    {
      "epoch": 0.5162378743146352,
      "grad_norm": 1.7426071166992188,
      "learning_rate": 0.00018330001355219844,
      "loss": 1.148,
      "step": 3672
    },
    {
      "epoch": 0.516378461971039,
      "grad_norm": 1.5506643056869507,
      "learning_rate": 0.00018317116954224725,
      "loss": 1.1275,
      "step": 3673
    },
    {
      "epoch": 0.5165190496274427,
      "grad_norm": 1.831276774406433,
      "learning_rate": 0.0001830418760583434,
      "loss": 1.1967,
      "step": 3674
    },
    {
      "epoch": 0.5166596372838465,
      "grad_norm": 1.8346151113510132,
      "learning_rate": 0.00018291213379921512,
      "loss": 1.2469,
      "step": 3675
    },
    {
      "epoch": 0.5168002249402502,
      "grad_norm": 1.7666515111923218,
      "learning_rate": 0.0001827819434660162,
      "loss": 1.0932,
      "step": 3676
    },
    {
      "epoch": 0.516940812596654,
      "grad_norm": 1.5704768896102905,
      "learning_rate": 0.00018265130576232159,
      "loss": 1.1803,
      "step": 3677
    },
    {
      "epoch": 0.5170814002530578,
      "grad_norm": 1.6016086339950562,
      "learning_rate": 0.00018252022139412412,
      "loss": 1.2399,
      "step": 3678
    },
    {
      "epoch": 0.5172219879094615,
      "grad_norm": 1.5247554779052734,
      "learning_rate": 0.00018238869106983047,
      "loss": 1.3207,
      "step": 3679
    },
    {
      "epoch": 0.5173625755658653,
      "grad_norm": 1.4733017683029175,
      "learning_rate": 0.00018225671550025724,
      "loss": 1.1654,
      "step": 3680
    },
    {
      "epoch": 0.5175031632222691,
      "grad_norm": 1.467795968055725,
      "learning_rate": 0.00018212429539862744,
      "loss": 1.1118,
      "step": 3681
    },
    {
      "epoch": 0.5176437508786729,
      "grad_norm": 1.6559836864471436,
      "learning_rate": 0.00018199143148056617,
      "loss": 1.0065,
      "step": 3682
    },
    {
      "epoch": 0.5177843385350767,
      "grad_norm": 1.6089259386062622,
      "learning_rate": 0.00018185812446409714,
      "loss": 1.116,
      "step": 3683
    },
    {
      "epoch": 0.5179249261914803,
      "grad_norm": 1.6406182050704956,
      "learning_rate": 0.00018172437506963867,
      "loss": 1.2553,
      "step": 3684
    },
    {
      "epoch": 0.5180655138478841,
      "grad_norm": 1.5384609699249268,
      "learning_rate": 0.00018159018401999976,
      "loss": 1.1363,
      "step": 3685
    },
    {
      "epoch": 0.5182061015042879,
      "grad_norm": 1.3827362060546875,
      "learning_rate": 0.00018145555204037613,
      "loss": 1.2461,
      "step": 3686
    },
    {
      "epoch": 0.5183466891606917,
      "grad_norm": 1.5396732091903687,
      "learning_rate": 0.0001813204798583465,
      "loss": 0.9303,
      "step": 3687
    },
    {
      "epoch": 0.5184872768170955,
      "grad_norm": 1.610592007637024,
      "learning_rate": 0.00018118496820386848,
      "loss": 1.0675,
      "step": 3688
    },
    {
      "epoch": 0.5186278644734992,
      "grad_norm": 1.447862982749939,
      "learning_rate": 0.00018104901780927467,
      "loss": 1.1218,
      "step": 3689
    },
    {
      "epoch": 0.518768452129903,
      "grad_norm": 1.5255844593048096,
      "learning_rate": 0.00018091262940926863,
      "loss": 1.1819,
      "step": 3690
    },
    {
      "epoch": 0.5189090397863068,
      "grad_norm": 1.5965325832366943,
      "learning_rate": 0.0001807758037409211,
      "loss": 1.1483,
      "step": 3691
    },
    {
      "epoch": 0.5190496274427105,
      "grad_norm": 1.5772151947021484,
      "learning_rate": 0.000180638541543666,
      "loss": 1.1193,
      "step": 3692
    },
    {
      "epoch": 0.5191902150991143,
      "grad_norm": 1.511913537979126,
      "learning_rate": 0.00018050084355929614,
      "loss": 1.1919,
      "step": 3693
    },
    {
      "epoch": 0.519330802755518,
      "grad_norm": 1.2857669591903687,
      "learning_rate": 0.00018036271053195938,
      "loss": 1.2676,
      "step": 3694
    },
    {
      "epoch": 0.5194713904119218,
      "grad_norm": 1.6920479536056519,
      "learning_rate": 0.00018022414320815496,
      "loss": 0.9719,
      "step": 3695
    },
    {
      "epoch": 0.5196119780683256,
      "grad_norm": 1.5320823192596436,
      "learning_rate": 0.0001800851423367288,
      "loss": 1.146,
      "step": 3696
    },
    {
      "epoch": 0.5197525657247294,
      "grad_norm": 1.56381356716156,
      "learning_rate": 0.00017994570866886996,
      "loss": 1.0892,
      "step": 3697
    },
    {
      "epoch": 0.5198931533811332,
      "grad_norm": 1.542754054069519,
      "learning_rate": 0.00017980584295810648,
      "loss": 1.1128,
      "step": 3698
    },
    {
      "epoch": 0.5200337410375369,
      "grad_norm": 1.510730266571045,
      "learning_rate": 0.0001796655459603011,
      "loss": 1.1293,
      "step": 3699
    },
    {
      "epoch": 0.5201743286939406,
      "grad_norm": 1.4919191598892212,
      "learning_rate": 0.00017952481843364746,
      "loss": 1.1843,
      "step": 3700
    },
    {
      "epoch": 0.5203149163503444,
      "grad_norm": 1.5339435338974,
      "learning_rate": 0.0001793836611386657,
      "loss": 1.1307,
      "step": 3701
    },
    {
      "epoch": 0.5204555040067482,
      "grad_norm": 1.5453394651412964,
      "learning_rate": 0.00017924207483819866,
      "loss": 1.0485,
      "step": 3702
    },
    {
      "epoch": 0.520596091663152,
      "grad_norm": 1.5457124710083008,
      "learning_rate": 0.00017910006029740755,
      "loss": 1.2811,
      "step": 3703
    },
    {
      "epoch": 0.5207366793195557,
      "grad_norm": 1.6885244846343994,
      "learning_rate": 0.00017895761828376798,
      "loss": 1.0263,
      "step": 3704
    },
    {
      "epoch": 0.5208772669759595,
      "grad_norm": 2.411804676055908,
      "learning_rate": 0.0001788147495670655,
      "loss": 1.1201,
      "step": 3705
    },
    {
      "epoch": 0.5210178546323633,
      "grad_norm": 1.7698750495910645,
      "learning_rate": 0.00017867145491939183,
      "loss": 1.0149,
      "step": 3706
    },
    {
      "epoch": 0.5211584422887671,
      "grad_norm": 1.6677958965301514,
      "learning_rate": 0.00017852773511514047,
      "loss": 1.035,
      "step": 3707
    },
    {
      "epoch": 0.5212990299451709,
      "grad_norm": 1.5712441205978394,
      "learning_rate": 0.00017838359093100257,
      "loss": 1.0839,
      "step": 3708
    },
    {
      "epoch": 0.5214396176015745,
      "grad_norm": 1.6297316551208496,
      "learning_rate": 0.00017823902314596256,
      "loss": 0.8921,
      "step": 3709
    },
    {
      "epoch": 0.5215802052579783,
      "grad_norm": 1.6683827638626099,
      "learning_rate": 0.00017809403254129432,
      "loss": 1.017,
      "step": 3710
    },
    {
      "epoch": 0.5217207929143821,
      "grad_norm": 1.6705636978149414,
      "learning_rate": 0.00017794861990055657,
      "loss": 1.065,
      "step": 3711
    },
    {
      "epoch": 0.5218613805707859,
      "grad_norm": 1.6101211309432983,
      "learning_rate": 0.000177802786009589,
      "loss": 1.0109,
      "step": 3712
    },
    {
      "epoch": 0.5220019682271897,
      "grad_norm": 1.480465054512024,
      "learning_rate": 0.0001776565316565075,
      "loss": 1.1689,
      "step": 3713
    },
    {
      "epoch": 0.5221425558835934,
      "grad_norm": 1.8111977577209473,
      "learning_rate": 0.00017750985763170042,
      "loss": 1.1529,
      "step": 3714
    },
    {
      "epoch": 0.5222831435399972,
      "grad_norm": 1.8700212240219116,
      "learning_rate": 0.0001773627647278242,
      "loss": 1.0397,
      "step": 3715
    },
    {
      "epoch": 0.522423731196401,
      "grad_norm": 1.4079864025115967,
      "learning_rate": 0.0001772152537397988,
      "loss": 1.1157,
      "step": 3716
    },
    {
      "epoch": 0.5225643188528047,
      "grad_norm": 1.6956493854522705,
      "learning_rate": 0.00017706732546480373,
      "loss": 1.2418,
      "step": 3717
    },
    {
      "epoch": 0.5227049065092085,
      "grad_norm": 1.600470781326294,
      "learning_rate": 0.0001769189807022734,
      "loss": 1.2451,
      "step": 3718
    },
    {
      "epoch": 0.5228454941656122,
      "grad_norm": 1.7283469438552856,
      "learning_rate": 0.00017677022025389336,
      "loss": 1.0379,
      "step": 3719
    },
    {
      "epoch": 0.522986081822016,
      "grad_norm": 1.5760300159454346,
      "learning_rate": 0.0001766210449235952,
      "loss": 1.0231,
      "step": 3720
    },
    {
      "epoch": 0.5231266694784198,
      "grad_norm": 1.3661226034164429,
      "learning_rate": 0.00017647145551755294,
      "loss": 1.1109,
      "step": 3721
    },
    {
      "epoch": 0.5232672571348236,
      "grad_norm": 2.0329251289367676,
      "learning_rate": 0.00017632145284417802,
      "loss": 1.1614,
      "step": 3722
    },
    {
      "epoch": 0.5234078447912274,
      "grad_norm": 1.792861819267273,
      "learning_rate": 0.00017617103771411574,
      "loss": 1.1219,
      "step": 3723
    },
    {
      "epoch": 0.523548432447631,
      "grad_norm": 1.59397292137146,
      "learning_rate": 0.00017602021094023992,
      "loss": 1.0663,
      "step": 3724
    },
    {
      "epoch": 0.5236890201040348,
      "grad_norm": 1.5984725952148438,
      "learning_rate": 0.00017586897333764933,
      "loss": 0.9552,
      "step": 3725
    },
    {
      "epoch": 0.5238296077604386,
      "grad_norm": 1.7898902893066406,
      "learning_rate": 0.0001757173257236626,
      "loss": 1.0919,
      "step": 3726
    },
    {
      "epoch": 0.5239701954168424,
      "grad_norm": 1.7190055847167969,
      "learning_rate": 0.00017556526891781456,
      "loss": 1.2271,
      "step": 3727
    },
    {
      "epoch": 0.5241107830732462,
      "grad_norm": 1.5536561012268066,
      "learning_rate": 0.00017541280374185108,
      "loss": 1.0933,
      "step": 3728
    },
    {
      "epoch": 0.5242513707296499,
      "grad_norm": 1.3654345273971558,
      "learning_rate": 0.00017525993101972503,
      "loss": 1.1123,
      "step": 3729
    },
    {
      "epoch": 0.5243919583860537,
      "grad_norm": 1.278737187385559,
      "learning_rate": 0.00017510665157759178,
      "loss": 1.19,
      "step": 3730
    },
    {
      "epoch": 0.5245325460424575,
      "grad_norm": 1.7194167375564575,
      "learning_rate": 0.00017495296624380478,
      "loss": 1.1024,
      "step": 3731
    },
    {
      "epoch": 0.5246731336988613,
      "grad_norm": 1.5114794969558716,
      "learning_rate": 0.00017479887584891072,
      "loss": 1.2039,
      "step": 3732
    },
    {
      "epoch": 0.524813721355265,
      "grad_norm": 1.6135969161987305,
      "learning_rate": 0.0001746443812256456,
      "loss": 1.156,
      "step": 3733
    },
    {
      "epoch": 0.5249543090116687,
      "grad_norm": 1.4902197122573853,
      "learning_rate": 0.00017448948320892985,
      "loss": 0.9093,
      "step": 3734
    },
    {
      "epoch": 0.5250948966680725,
      "grad_norm": 1.6908851861953735,
      "learning_rate": 0.00017433418263586395,
      "loss": 1.2655,
      "step": 3735
    },
    {
      "epoch": 0.5252354843244763,
      "grad_norm": 1.8719606399536133,
      "learning_rate": 0.00017417848034572377,
      "loss": 0.9476,
      "step": 3736
    },
    {
      "epoch": 0.5253760719808801,
      "grad_norm": 1.3934838771820068,
      "learning_rate": 0.0001740223771799562,
      "loss": 1.1807,
      "step": 3737
    },
    {
      "epoch": 0.5255166596372839,
      "grad_norm": 1.8158665895462036,
      "learning_rate": 0.00017386587398217476,
      "loss": 1.0777,
      "step": 3738
    },
    {
      "epoch": 0.5256572472936876,
      "grad_norm": 1.6443907022476196,
      "learning_rate": 0.00017370897159815445,
      "loss": 1.1393,
      "step": 3739
    },
    {
      "epoch": 0.5257978349500914,
      "grad_norm": 1.4934128522872925,
      "learning_rate": 0.00017355167087582785,
      "loss": 1.1473,
      "step": 3740
    },
    {
      "epoch": 0.5259384226064951,
      "grad_norm": 1.484321117401123,
      "learning_rate": 0.00017339397266527993,
      "loss": 1.2714,
      "step": 3741
    },
    {
      "epoch": 0.5260790102628989,
      "grad_norm": 1.7539621591567993,
      "learning_rate": 0.00017323587781874425,
      "loss": 1.0408,
      "step": 3742
    },
    {
      "epoch": 0.5262195979193027,
      "grad_norm": 1.7769300937652588,
      "learning_rate": 0.00017307738719059734,
      "loss": 1.0113,
      "step": 3743
    },
    {
      "epoch": 0.5263601855757064,
      "grad_norm": 1.5248358249664307,
      "learning_rate": 0.00017291850163735504,
      "loss": 1.0663,
      "step": 3744
    },
    {
      "epoch": 0.5265007732321102,
      "grad_norm": 1.5893051624298096,
      "learning_rate": 0.00017275922201766703,
      "loss": 1.1187,
      "step": 3745
    },
    {
      "epoch": 0.526641360888514,
      "grad_norm": 1.5556881427764893,
      "learning_rate": 0.00017259954919231313,
      "loss": 1.1429,
      "step": 3746
    },
    {
      "epoch": 0.5267819485449178,
      "grad_norm": 1.7362316846847534,
      "learning_rate": 0.0001724394840241976,
      "loss": 0.8313,
      "step": 3747
    },
    {
      "epoch": 0.5269225362013216,
      "grad_norm": 1.4000568389892578,
      "learning_rate": 0.0001722790273783454,
      "loss": 1.2054,
      "step": 3748
    },
    {
      "epoch": 0.5270631238577252,
      "grad_norm": 1.6014084815979004,
      "learning_rate": 0.00017211818012189678,
      "loss": 1.1406,
      "step": 3749
    },
    {
      "epoch": 0.527203711514129,
      "grad_norm": 1.7928180694580078,
      "learning_rate": 0.00017195694312410327,
      "loss": 0.9521,
      "step": 3750
    },
    {
      "epoch": 0.5273442991705328,
      "grad_norm": 1.7550559043884277,
      "learning_rate": 0.00017179531725632237,
      "loss": 1.197,
      "step": 3751
    },
    {
      "epoch": 0.5274848868269366,
      "grad_norm": 1.5872974395751953,
      "learning_rate": 0.00017163330339201326,
      "loss": 0.9784,
      "step": 3752
    },
    {
      "epoch": 0.5276254744833404,
      "grad_norm": 1.5067297220230103,
      "learning_rate": 0.00017147090240673176,
      "loss": 1.2294,
      "step": 3753
    },
    {
      "epoch": 0.5277660621397441,
      "grad_norm": 1.994153618812561,
      "learning_rate": 0.00017130811517812613,
      "loss": 1.0431,
      "step": 3754
    },
    {
      "epoch": 0.5279066497961479,
      "grad_norm": 1.6700845956802368,
      "learning_rate": 0.00017114494258593153,
      "loss": 1.0173,
      "step": 3755
    },
    {
      "epoch": 0.5280472374525517,
      "grad_norm": 1.4686076641082764,
      "learning_rate": 0.00017098138551196597,
      "loss": 1.2655,
      "step": 3756
    },
    {
      "epoch": 0.5281878251089555,
      "grad_norm": 1.843608021736145,
      "learning_rate": 0.00017081744484012524,
      "loss": 1.1475,
      "step": 3757
    },
    {
      "epoch": 0.5283284127653592,
      "grad_norm": 1.4970977306365967,
      "learning_rate": 0.0001706531214563781,
      "loss": 1.2761,
      "step": 3758
    },
    {
      "epoch": 0.5284690004217629,
      "grad_norm": 1.4942389726638794,
      "learning_rate": 0.00017048841624876175,
      "loss": 1.0549,
      "step": 3759
    },
    {
      "epoch": 0.5286095880781667,
      "grad_norm": 1.9837214946746826,
      "learning_rate": 0.0001703233301073765,
      "loss": 1.1074,
      "step": 3760
    },
    {
      "epoch": 0.5287501757345705,
      "grad_norm": 1.4138529300689697,
      "learning_rate": 0.00017015786392438157,
      "loss": 1.0698,
      "step": 3761
    },
    {
      "epoch": 0.5288907633909743,
      "grad_norm": 1.7966243028640747,
      "learning_rate": 0.00016999201859399,
      "loss": 1.0821,
      "step": 3762
    },
    {
      "epoch": 0.5290313510473781,
      "grad_norm": 1.5200825929641724,
      "learning_rate": 0.00016982579501246381,
      "loss": 1.2001,
      "step": 3763
    },
    {
      "epoch": 0.5291719387037818,
      "grad_norm": 1.7835664749145508,
      "learning_rate": 0.00016965919407810893,
      "loss": 1.2699,
      "step": 3764
    },
    {
      "epoch": 0.5293125263601856,
      "grad_norm": 1.5412695407867432,
      "learning_rate": 0.0001694922166912709,
      "loss": 1.2011,
      "step": 3765
    },
    {
      "epoch": 0.5294531140165893,
      "grad_norm": 1.5364247560501099,
      "learning_rate": 0.0001693248637543295,
      "loss": 1.1738,
      "step": 3766
    },
    {
      "epoch": 0.5295937016729931,
      "grad_norm": 1.2982698678970337,
      "learning_rate": 0.0001691571361716942,
      "loss": 1.0911,
      "step": 3767
    },
    {
      "epoch": 0.5297342893293969,
      "grad_norm": 1.4120033979415894,
      "learning_rate": 0.00016898903484979886,
      "loss": 1.276,
      "step": 3768
    },
    {
      "epoch": 0.5298748769858006,
      "grad_norm": 1.358357310295105,
      "learning_rate": 0.00016882056069709753,
      "loss": 1.0731,
      "step": 3769
    },
    {
      "epoch": 0.5300154646422044,
      "grad_norm": 1.4677215814590454,
      "learning_rate": 0.00016865171462405868,
      "loss": 1.1043,
      "step": 3770
    },
    {
      "epoch": 0.5301560522986082,
      "grad_norm": 1.3956471681594849,
      "learning_rate": 0.00016848249754316107,
      "loss": 1.17,
      "step": 3771
    },
    {
      "epoch": 0.530296639955012,
      "grad_norm": 1.487593173980713,
      "learning_rate": 0.00016831291036888802,
      "loss": 0.9351,
      "step": 3772
    },
    {
      "epoch": 0.5304372276114158,
      "grad_norm": 1.3656667470932007,
      "learning_rate": 0.00016814295401772352,
      "loss": 1.134,
      "step": 3773
    },
    {
      "epoch": 0.5305778152678194,
      "grad_norm": 1.3995112180709839,
      "learning_rate": 0.00016797262940814606,
      "loss": 1.1797,
      "step": 3774
    },
    {
      "epoch": 0.5307184029242232,
      "grad_norm": 1.5755138397216797,
      "learning_rate": 0.00016780193746062473,
      "loss": 1.0099,
      "step": 3775
    },
    {
      "epoch": 0.530858990580627,
      "grad_norm": 1.6843140125274658,
      "learning_rate": 0.0001676308790976135,
      "loss": 1.1974,
      "step": 3776
    },
    {
      "epoch": 0.5309995782370308,
      "grad_norm": 1.6219100952148438,
      "learning_rate": 0.00016745945524354667,
      "loss": 1.1849,
      "step": 3777
    },
    {
      "epoch": 0.5311401658934346,
      "grad_norm": 1.6722092628479004,
      "learning_rate": 0.00016728766682483375,
      "loss": 1.1103,
      "step": 3778
    },
    {
      "epoch": 0.5312807535498383,
      "grad_norm": 1.6675746440887451,
      "learning_rate": 0.00016711551476985425,
      "loss": 1.0696,
      "step": 3779
    },
    {
      "epoch": 0.5314213412062421,
      "grad_norm": 1.5984926223754883,
      "learning_rate": 0.00016694300000895303,
      "loss": 1.1587,
      "step": 3780
    },
    {
      "epoch": 0.5315619288626459,
      "grad_norm": 1.3499037027359009,
      "learning_rate": 0.000166770123474435,
      "loss": 1.0533,
      "step": 3781
    },
    {
      "epoch": 0.5317025165190497,
      "grad_norm": 1.5308693647384644,
      "learning_rate": 0.00016659688610056025,
      "loss": 0.9687,
      "step": 3782
    },
    {
      "epoch": 0.5318431041754534,
      "grad_norm": 1.3356037139892578,
      "learning_rate": 0.00016642328882353878,
      "loss": 1.1602,
      "step": 3783
    },
    {
      "epoch": 0.5319836918318571,
      "grad_norm": 1.6446813344955444,
      "learning_rate": 0.00016624933258152567,
      "loss": 1.1751,
      "step": 3784
    },
    {
      "epoch": 0.5321242794882609,
      "grad_norm": 1.6401786804199219,
      "learning_rate": 0.0001660750183146159,
      "loss": 1.106,
      "step": 3785
    },
    {
      "epoch": 0.5322648671446647,
      "grad_norm": 1.5615326166152954,
      "learning_rate": 0.00016590034696483942,
      "loss": 1.1535,
      "step": 3786
    },
    {
      "epoch": 0.5324054548010685,
      "grad_norm": 1.5413111448287964,
      "learning_rate": 0.00016572531947615563,
      "loss": 1.0503,
      "step": 3787
    },
    {
      "epoch": 0.5325460424574723,
      "grad_norm": 1.7092329263687134,
      "learning_rate": 0.00016554993679444887,
      "loss": 1.0392,
      "step": 3788
    },
    {
      "epoch": 0.532686630113876,
      "grad_norm": 1.8237522840499878,
      "learning_rate": 0.00016537419986752282,
      "loss": 1.2031,
      "step": 3789
    },
    {
      "epoch": 0.5328272177702797,
      "grad_norm": 1.5319064855575562,
      "learning_rate": 0.00016519810964509582,
      "loss": 1.0445,
      "step": 3790
    },
    {
      "epoch": 0.5329678054266835,
      "grad_norm": 1.5245063304901123,
      "learning_rate": 0.00016502166707879507,
      "loss": 1.0524,
      "step": 3791
    },
    {
      "epoch": 0.5331083930830873,
      "grad_norm": 2.1862645149230957,
      "learning_rate": 0.00016484487312215233,
      "loss": 1.1641,
      "step": 3792
    },
    {
      "epoch": 0.5332489807394911,
      "grad_norm": 1.57451593875885,
      "learning_rate": 0.00016466772873059812,
      "loss": 1.169,
      "step": 3793
    },
    {
      "epoch": 0.5333895683958948,
      "grad_norm": 1.454856276512146,
      "learning_rate": 0.00016449023486145688,
      "loss": 1.1194,
      "step": 3794
    },
    {
      "epoch": 0.5335301560522986,
      "grad_norm": 1.5088508129119873,
      "learning_rate": 0.00016431239247394173,
      "loss": 1.3244,
      "step": 3795
    },
    {
      "epoch": 0.5336707437087024,
      "grad_norm": 1.4693565368652344,
      "learning_rate": 0.00016413420252914894,
      "loss": 1.1766,
      "step": 3796
    },
    {
      "epoch": 0.5338113313651062,
      "grad_norm": 1.4929934740066528,
      "learning_rate": 0.00016395566599005355,
      "loss": 1.1265,
      "step": 3797
    },
    {
      "epoch": 0.53395191902151,
      "grad_norm": 2.540339231491089,
      "learning_rate": 0.00016377678382150323,
      "loss": 1.1736,
      "step": 3798
    },
    {
      "epoch": 0.5340925066779136,
      "grad_norm": 1.635434627532959,
      "learning_rate": 0.00016359755699021382,
      "loss": 1.1947,
      "step": 3799
    },
    {
      "epoch": 0.5342330943343174,
      "grad_norm": 1.3419463634490967,
      "learning_rate": 0.00016341798646476346,
      "loss": 1.3158,
      "step": 3800
    },
    {
      "epoch": 0.5343736819907212,
      "grad_norm": 1.6953282356262207,
      "learning_rate": 0.0001632380732155881,
      "loss": 1.0691,
      "step": 3801
    },
    {
      "epoch": 0.534514269647125,
      "grad_norm": 1.6255332231521606,
      "learning_rate": 0.00016305781821497538,
      "loss": 1.1085,
      "step": 3802
    },
    {
      "epoch": 0.5346548573035288,
      "grad_norm": 1.321670651435852,
      "learning_rate": 0.00016287722243706033,
      "loss": 1.0756,
      "step": 3803
    },
    {
      "epoch": 0.5347954449599325,
      "grad_norm": 1.3633695840835571,
      "learning_rate": 0.000162696286857819,
      "loss": 1.1416,
      "step": 3804
    },
    {
      "epoch": 0.5349360326163363,
      "grad_norm": 1.5280017852783203,
      "learning_rate": 0.0001625150124550645,
      "loss": 1.1669,
      "step": 3805
    },
    {
      "epoch": 0.5350766202727401,
      "grad_norm": 1.5169228315353394,
      "learning_rate": 0.0001623334002084404,
      "loss": 1.3298,
      "step": 3806
    },
    {
      "epoch": 0.5352172079291438,
      "grad_norm": 1.666818380355835,
      "learning_rate": 0.0001621514510994164,
      "loss": 1.0442,
      "step": 3807
    },
    {
      "epoch": 0.5353577955855476,
      "grad_norm": 1.4644503593444824,
      "learning_rate": 0.00016196916611128253,
      "loss": 1.0691,
      "step": 3808
    },
    {
      "epoch": 0.5354983832419513,
      "grad_norm": 1.4461878538131714,
      "learning_rate": 0.00016178654622914414,
      "loss": 1.064,
      "step": 3809
    },
    {
      "epoch": 0.5356389708983551,
      "grad_norm": 1.706080675125122,
      "learning_rate": 0.0001616035924399161,
      "loss": 1.1756,
      "step": 3810
    },
    {
      "epoch": 0.5357795585547589,
      "grad_norm": 1.576417088508606,
      "learning_rate": 0.00016142030573231808,
      "loss": 1.2379,
      "step": 3811
    },
    {
      "epoch": 0.5359201462111627,
      "grad_norm": 1.7323365211486816,
      "learning_rate": 0.0001612366870968688,
      "loss": 1.2363,
      "step": 3812
    },
    {
      "epoch": 0.5360607338675665,
      "grad_norm": 1.6116310358047485,
      "learning_rate": 0.00016105273752588088,
      "loss": 1.1556,
      "step": 3813
    },
    {
      "epoch": 0.5362013215239702,
      "grad_norm": 1.5729904174804688,
      "learning_rate": 0.00016086845801345514,
      "loss": 1.1097,
      "step": 3814
    },
    {
      "epoch": 0.5363419091803739,
      "grad_norm": 1.6259440183639526,
      "learning_rate": 0.00016068384955547562,
      "loss": 1.2376,
      "step": 3815
    },
    {
      "epoch": 0.5364824968367777,
      "grad_norm": 2.170084238052368,
      "learning_rate": 0.0001604989131496043,
      "loss": 1.2361,
      "step": 3816
    },
    {
      "epoch": 0.5366230844931815,
      "grad_norm": 1.7361100912094116,
      "learning_rate": 0.0001603136497952749,
      "loss": 0.9918,
      "step": 3817
    },
    {
      "epoch": 0.5367636721495853,
      "grad_norm": 1.3526750802993774,
      "learning_rate": 0.0001601280604936886,
      "loss": 1.2948,
      "step": 3818
    },
    {
      "epoch": 0.536904259805989,
      "grad_norm": 1.7053451538085938,
      "learning_rate": 0.0001599421462478076,
      "loss": 1.139,
      "step": 3819
    },
    {
      "epoch": 0.5370448474623928,
      "grad_norm": 1.5709898471832275,
      "learning_rate": 0.0001597559080623506,
      "loss": 1.1139,
      "step": 3820
    },
    {
      "epoch": 0.5371854351187966,
      "grad_norm": 1.640308141708374,
      "learning_rate": 0.00015956934694378653,
      "loss": 1.1465,
      "step": 3821
    },
    {
      "epoch": 0.5373260227752004,
      "grad_norm": 1.461578369140625,
      "learning_rate": 0.00015938246390032984,
      "loss": 1.1153,
      "step": 3822
    },
    {
      "epoch": 0.5374666104316042,
      "grad_norm": 1.7849466800689697,
      "learning_rate": 0.00015919525994193437,
      "loss": 1.0736,
      "step": 3823
    },
    {
      "epoch": 0.5376071980880078,
      "grad_norm": 1.673755168914795,
      "learning_rate": 0.00015900773608028878,
      "loss": 1.0466,
      "step": 3824
    },
    {
      "epoch": 0.5377477857444116,
      "grad_norm": 1.698654294013977,
      "learning_rate": 0.00015881989332881,
      "loss": 1.2141,
      "step": 3825
    },
    {
      "epoch": 0.5378883734008154,
      "grad_norm": 1.4475127458572388,
      "learning_rate": 0.0001586317327026387,
      "loss": 1.0518,
      "step": 3826
    },
    {
      "epoch": 0.5380289610572192,
      "grad_norm": 1.4675617218017578,
      "learning_rate": 0.000158443255218633,
      "loss": 1.0976,
      "step": 3827
    },
    {
      "epoch": 0.538169548713623,
      "grad_norm": 1.7141369581222534,
      "learning_rate": 0.000158254461895364,
      "loss": 1.063,
      "step": 3828
    },
    {
      "epoch": 0.5383101363700267,
      "grad_norm": 1.5986168384552002,
      "learning_rate": 0.00015806535375310907,
      "loss": 1.0443,
      "step": 3829
    },
    {
      "epoch": 0.5384507240264305,
      "grad_norm": 2.0895659923553467,
      "learning_rate": 0.00015787593181384722,
      "loss": 1.0899,
      "step": 3830
    },
    {
      "epoch": 0.5385913116828343,
      "grad_norm": 1.99961256980896,
      "learning_rate": 0.0001576861971012531,
      "loss": 1.0945,
      "step": 3831
    },
    {
      "epoch": 0.538731899339238,
      "grad_norm": 2.195162057876587,
      "learning_rate": 0.000157496150640692,
      "loss": 1.1704,
      "step": 3832
    },
    {
      "epoch": 0.5388724869956418,
      "grad_norm": 1.7708661556243896,
      "learning_rate": 0.00015730579345921356,
      "loss": 0.9426,
      "step": 3833
    },
    {
      "epoch": 0.5390130746520455,
      "grad_norm": 1.459641933441162,
      "learning_rate": 0.0001571151265855468,
      "loss": 1.1044,
      "step": 3834
    },
    {
      "epoch": 0.5391536623084493,
      "grad_norm": 1.4998329877853394,
      "learning_rate": 0.00015692415105009443,
      "loss": 1.0277,
      "step": 3835
    },
    {
      "epoch": 0.5392942499648531,
      "grad_norm": 1.427986979484558,
      "learning_rate": 0.00015673286788492715,
      "loss": 1.2146,
      "step": 3836
    },
    {
      "epoch": 0.5394348376212569,
      "grad_norm": 1.5996015071868896,
      "learning_rate": 0.0001565412781237782,
      "loss": 0.9671,
      "step": 3837
    },
    {
      "epoch": 0.5395754252776607,
      "grad_norm": 1.6934775114059448,
      "learning_rate": 0.00015634938280203762,
      "loss": 1.0562,
      "step": 3838
    },
    {
      "epoch": 0.5397160129340643,
      "grad_norm": 1.7432687282562256,
      "learning_rate": 0.00015615718295674687,
      "loss": 1.0186,
      "step": 3839
    },
    {
      "epoch": 0.5398566005904681,
      "grad_norm": 1.9276608228683472,
      "learning_rate": 0.0001559646796265931,
      "loss": 1.2101,
      "step": 3840
    },
    {
      "epoch": 0.5399971882468719,
      "grad_norm": 1.5071669816970825,
      "learning_rate": 0.00015577187385190362,
      "loss": 1.2018,
      "step": 3841
    },
    {
      "epoch": 0.5401377759032757,
      "grad_norm": 1.6039589643478394,
      "learning_rate": 0.00015557876667464,
      "loss": 1.2119,
      "step": 3842
    },
    {
      "epoch": 0.5402783635596795,
      "grad_norm": 1.446941614151001,
      "learning_rate": 0.00015538535913839287,
      "loss": 1.3036,
      "step": 3843
    },
    {
      "epoch": 0.5404189512160832,
      "grad_norm": 1.304337739944458,
      "learning_rate": 0.00015519165228837593,
      "loss": 1.2947,
      "step": 3844
    },
    {
      "epoch": 0.540559538872487,
      "grad_norm": 1.6457767486572266,
      "learning_rate": 0.0001549976471714206,
      "loss": 1.0668,
      "step": 3845
    },
    {
      "epoch": 0.5407001265288908,
      "grad_norm": 1.6387271881103516,
      "learning_rate": 0.0001548033448359698,
      "loss": 1.0992,
      "step": 3846
    },
    {
      "epoch": 0.5408407141852946,
      "grad_norm": 1.7398921251296997,
      "learning_rate": 0.00015460874633207335,
      "loss": 0.9465,
      "step": 3847
    },
    {
      "epoch": 0.5409813018416983,
      "grad_norm": 1.522489070892334,
      "learning_rate": 0.0001544138527113809,
      "loss": 1.1254,
      "step": 3848
    },
    {
      "epoch": 0.541121889498102,
      "grad_norm": 1.6085925102233887,
      "learning_rate": 0.0001542186650271375,
      "loss": 1.1453,
      "step": 3849
    },
    {
      "epoch": 0.5412624771545058,
      "grad_norm": 1.5238244533538818,
      "learning_rate": 0.00015402318433417692,
      "loss": 1.1563,
      "step": 3850
    },
    {
      "epoch": 0.5414030648109096,
      "grad_norm": 1.7735174894332886,
      "learning_rate": 0.00015382741168891702,
      "loss": 1.0203,
      "step": 3851
    },
    {
      "epoch": 0.5415436524673134,
      "grad_norm": 1.3820545673370361,
      "learning_rate": 0.00015363134814935285,
      "loss": 0.9648,
      "step": 3852
    },
    {
      "epoch": 0.5416842401237172,
      "grad_norm": 1.6669459342956543,
      "learning_rate": 0.00015343499477505178,
      "loss": 1.1267,
      "step": 3853
    },
    {
      "epoch": 0.5418248277801209,
      "grad_norm": 1.4683483839035034,
      "learning_rate": 0.0001532383526271475,
      "loss": 1.2097,
      "step": 3854
    },
    {
      "epoch": 0.5419654154365247,
      "grad_norm": 1.6453440189361572,
      "learning_rate": 0.00015304142276833423,
      "loss": 1.2153,
      "step": 3855
    },
    {
      "epoch": 0.5421060030929284,
      "grad_norm": 1.7566879987716675,
      "learning_rate": 0.00015284420626286114,
      "loss": 1.1859,
      "step": 3856
    },
    {
      "epoch": 0.5422465907493322,
      "grad_norm": 1.6332367658615112,
      "learning_rate": 0.00015264670417652632,
      "loss": 1.0997,
      "step": 3857
    },
    {
      "epoch": 0.542387178405736,
      "grad_norm": 1.7005887031555176,
      "learning_rate": 0.0001524489175766713,
      "loss": 1.1071,
      "step": 3858
    },
    {
      "epoch": 0.5425277660621397,
      "grad_norm": 1.577656626701355,
      "learning_rate": 0.00015225084753217523,
      "loss": 1.0889,
      "step": 3859
    },
    {
      "epoch": 0.5426683537185435,
      "grad_norm": 1.64665687084198,
      "learning_rate": 0.0001520524951134491,
      "loss": 1.0065,
      "step": 3860
    },
    {
      "epoch": 0.5428089413749473,
      "grad_norm": 1.4895246028900146,
      "learning_rate": 0.00015185386139242958,
      "loss": 1.1516,
      "step": 3861
    },
    {
      "epoch": 0.5429495290313511,
      "grad_norm": 1.5270977020263672,
      "learning_rate": 0.00015165494744257395,
      "loss": 1.2059,
      "step": 3862
    },
    {
      "epoch": 0.5430901166877549,
      "grad_norm": 1.5819623470306396,
      "learning_rate": 0.0001514557543388537,
      "loss": 1.3285,
      "step": 3863
    },
    {
      "epoch": 0.5432307043441585,
      "grad_norm": 1.476211667060852,
      "learning_rate": 0.00015125628315774901,
      "loss": 0.9638,
      "step": 3864
    },
    {
      "epoch": 0.5433712920005623,
      "grad_norm": 1.8835612535476685,
      "learning_rate": 0.0001510565349772427,
      "loss": 0.9825,
      "step": 3865
    },
    {
      "epoch": 0.5435118796569661,
      "grad_norm": 1.47813081741333,
      "learning_rate": 0.0001508565108768147,
      "loss": 1.0895,
      "step": 3866
    },
    {
      "epoch": 0.5436524673133699,
      "grad_norm": 1.8078404664993286,
      "learning_rate": 0.00015065621193743601,
      "loss": 0.9923,
      "step": 3867
    },
    {
      "epoch": 0.5437930549697737,
      "grad_norm": 1.805268406867981,
      "learning_rate": 0.00015045563924156303,
      "loss": 0.9991,
      "step": 3868
    },
    {
      "epoch": 0.5439336426261774,
      "grad_norm": 1.8015936613082886,
      "learning_rate": 0.0001502547938731313,
      "loss": 1.0614,
      "step": 3869
    },
    {
      "epoch": 0.5440742302825812,
      "grad_norm": 1.578975796699524,
      "learning_rate": 0.00015005367691755023,
      "loss": 1.0953,
      "step": 3870
    },
    {
      "epoch": 0.544214817938985,
      "grad_norm": 1.7438080310821533,
      "learning_rate": 0.00014985228946169684,
      "loss": 0.975,
      "step": 3871
    },
    {
      "epoch": 0.5443554055953888,
      "grad_norm": 2.175262928009033,
      "learning_rate": 0.00014965063259391,
      "loss": 0.9503,
      "step": 3872
    },
    {
      "epoch": 0.5444959932517925,
      "grad_norm": 1.775667428970337,
      "learning_rate": 0.0001494487074039846,
      "loss": 1.222,
      "step": 3873
    },
    {
      "epoch": 0.5446365809081962,
      "grad_norm": 2.0063724517822266,
      "learning_rate": 0.00014924651498316526,
      "loss": 1.2206,
      "step": 3874
    },
    {
      "epoch": 0.5447771685646,
      "grad_norm": 1.8759170770645142,
      "learning_rate": 0.00014904405642414138,
      "loss": 1.1133,
      "step": 3875
    },
    {
      "epoch": 0.5449177562210038,
      "grad_norm": 1.7773388624191284,
      "learning_rate": 0.00014884133282104002,
      "loss": 1.1712,
      "step": 3876
    },
    {
      "epoch": 0.5450583438774076,
      "grad_norm": 1.4942837953567505,
      "learning_rate": 0.00014863834526942103,
      "loss": 0.9756,
      "step": 3877
    },
    {
      "epoch": 0.5451989315338114,
      "grad_norm": 1.6313179731369019,
      "learning_rate": 0.00014843509486627026,
      "loss": 1.0317,
      "step": 3878
    },
    {
      "epoch": 0.5453395191902151,
      "grad_norm": 1.5798286199569702,
      "learning_rate": 0.00014823158270999462,
      "loss": 1.1392,
      "step": 3879
    },
    {
      "epoch": 0.5454801068466189,
      "grad_norm": 1.5830914974212646,
      "learning_rate": 0.00014802780990041508,
      "loss": 0.9787,
      "step": 3880
    },
    {
      "epoch": 0.5456206945030226,
      "grad_norm": 1.6928977966308594,
      "learning_rate": 0.0001478237775387616,
      "loss": 1.1732,
      "step": 3881
    },
    {
      "epoch": 0.5457612821594264,
      "grad_norm": 1.8092249631881714,
      "learning_rate": 0.0001476194867276664,
      "loss": 1.1699,
      "step": 3882
    },
    {
      "epoch": 0.5459018698158302,
      "grad_norm": 1.8961039781570435,
      "learning_rate": 0.00014741493857115895,
      "loss": 1.0818,
      "step": 3883
    },
    {
      "epoch": 0.5460424574722339,
      "grad_norm": 1.442639708518982,
      "learning_rate": 0.00014721013417465895,
      "loss": 0.9733,
      "step": 3884
    },
    {
      "epoch": 0.5461830451286377,
      "grad_norm": 1.5416443347930908,
      "learning_rate": 0.00014700507464497124,
      "loss": 1.0759,
      "step": 3885
    },
    {
      "epoch": 0.5463236327850415,
      "grad_norm": 1.5705790519714355,
      "learning_rate": 0.00014679976109027926,
      "loss": 1.2461,
      "step": 3886
    },
    {
      "epoch": 0.5464642204414453,
      "grad_norm": 1.3455380201339722,
      "learning_rate": 0.00014659419462013942,
      "loss": 1.0694,
      "step": 3887
    },
    {
      "epoch": 0.5466048080978491,
      "grad_norm": 1.7763983011245728,
      "learning_rate": 0.00014638837634547462,
      "loss": 1.2408,
      "step": 3888
    },
    {
      "epoch": 0.5467453957542527,
      "grad_norm": 1.4022117853164673,
      "learning_rate": 0.00014618230737856885,
      "loss": 1.2195,
      "step": 3889
    },
    {
      "epoch": 0.5468859834106565,
      "grad_norm": 1.4808353185653687,
      "learning_rate": 0.00014597598883306088,
      "loss": 1.0538,
      "step": 3890
    },
    {
      "epoch": 0.5470265710670603,
      "grad_norm": 1.6522396802902222,
      "learning_rate": 0.00014576942182393819,
      "loss": 1.079,
      "step": 3891
    },
    {
      "epoch": 0.5471671587234641,
      "grad_norm": 1.50790536403656,
      "learning_rate": 0.00014556260746753088,
      "loss": 1.1547,
      "step": 3892
    },
    {
      "epoch": 0.5473077463798679,
      "grad_norm": 1.7406824827194214,
      "learning_rate": 0.00014535554688150585,
      "loss": 1.0822,
      "step": 3893
    },
    {
      "epoch": 0.5474483340362716,
      "grad_norm": 1.5230892896652222,
      "learning_rate": 0.000145148241184861,
      "loss": 1.1105,
      "step": 3894
    },
    {
      "epoch": 0.5475889216926754,
      "grad_norm": 1.4690616130828857,
      "learning_rate": 0.00014494069149791828,
      "loss": 1.1483,
      "step": 3895
    },
    {
      "epoch": 0.5477295093490792,
      "grad_norm": 1.6596863269805908,
      "learning_rate": 0.0001447328989423187,
      "loss": 1.2293,
      "step": 3896
    },
    {
      "epoch": 0.547870097005483,
      "grad_norm": 1.5453368425369263,
      "learning_rate": 0.00014452486464101535,
      "loss": 0.9686,
      "step": 3897
    },
    {
      "epoch": 0.5480106846618867,
      "grad_norm": 1.5397090911865234,
      "learning_rate": 0.0001443165897182683,
      "loss": 1.355,
      "step": 3898
    },
    {
      "epoch": 0.5481512723182904,
      "grad_norm": 1.4836153984069824,
      "learning_rate": 0.00014410807529963743,
      "loss": 1.0929,
      "step": 3899
    },
    {
      "epoch": 0.5482918599746942,
      "grad_norm": 1.4231551885604858,
      "learning_rate": 0.00014389932251197731,
      "loss": 1.2543,
      "step": 3900
    },
    {
      "epoch": 0.548432447631098,
      "grad_norm": 1.5932199954986572,
      "learning_rate": 0.00014369033248343033,
      "loss": 0.9643,
      "step": 3901
    },
    {
      "epoch": 0.5485730352875018,
      "grad_norm": 1.4862078428268433,
      "learning_rate": 0.00014348110634342155,
      "loss": 1.2286,
      "step": 3902
    },
    {
      "epoch": 0.5487136229439056,
      "grad_norm": 1.5863451957702637,
      "learning_rate": 0.0001432716452226514,
      "loss": 1.1582,
      "step": 3903
    },
    {
      "epoch": 0.5488542106003093,
      "grad_norm": 1.6338907480239868,
      "learning_rate": 0.00014306195025309064,
      "loss": 1.3017,
      "step": 3904
    },
    {
      "epoch": 0.548994798256713,
      "grad_norm": 1.5038765668869019,
      "learning_rate": 0.0001428520225679734,
      "loss": 1.04,
      "step": 3905
    },
    {
      "epoch": 0.5491353859131168,
      "grad_norm": 1.4763274192810059,
      "learning_rate": 0.000142641863301792,
      "loss": 1.1083,
      "step": 3906
    },
    {
      "epoch": 0.5492759735695206,
      "grad_norm": 1.6350760459899902,
      "learning_rate": 0.00014243147359028967,
      "loss": 1.1975,
      "step": 3907
    },
    {
      "epoch": 0.5494165612259244,
      "grad_norm": 1.7605444192886353,
      "learning_rate": 0.00014222085457045553,
      "loss": 1.059,
      "step": 3908
    },
    {
      "epoch": 0.5495571488823281,
      "grad_norm": 1.466586709022522,
      "learning_rate": 0.00014201000738051737,
      "loss": 1.1299,
      "step": 3909
    },
    {
      "epoch": 0.5496977365387319,
      "grad_norm": 1.7486028671264648,
      "learning_rate": 0.00014179893315993675,
      "loss": 1.0002,
      "step": 3910
    },
    {
      "epoch": 0.5498383241951357,
      "grad_norm": 1.4624099731445312,
      "learning_rate": 0.0001415876330494015,
      "loss": 1.0942,
      "step": 3911
    },
    {
      "epoch": 0.5499789118515395,
      "grad_norm": 1.619309425354004,
      "learning_rate": 0.00014137610819082064,
      "loss": 0.9939,
      "step": 3912
    },
    {
      "epoch": 0.5501194995079433,
      "grad_norm": 1.647270679473877,
      "learning_rate": 0.00014116435972731753,
      "loss": 1.1556,
      "step": 3913
    },
    {
      "epoch": 0.5502600871643469,
      "grad_norm": 1.4949188232421875,
      "learning_rate": 0.00014095238880322411,
      "loss": 1.2394,
      "step": 3914
    },
    {
      "epoch": 0.5504006748207507,
      "grad_norm": 1.807557225227356,
      "learning_rate": 0.0001407401965640745,
      "loss": 1.1138,
      "step": 3915
    },
    {
      "epoch": 0.5505412624771545,
      "grad_norm": 1.4974621534347534,
      "learning_rate": 0.00014052778415659861,
      "loss": 1.1778,
      "step": 3916
    },
    {
      "epoch": 0.5506818501335583,
      "grad_norm": 1.4404653310775757,
      "learning_rate": 0.00014031515272871654,
      "loss": 1.2562,
      "step": 3917
    },
    {
      "epoch": 0.5508224377899621,
      "grad_norm": 1.3669326305389404,
      "learning_rate": 0.00014010230342953182,
      "loss": 0.9315,
      "step": 3918
    },
    {
      "epoch": 0.5509630254463658,
      "grad_norm": 1.6525205373764038,
      "learning_rate": 0.00013988923740932554,
      "loss": 1.1936,
      "step": 3919
    },
    {
      "epoch": 0.5511036131027696,
      "grad_norm": 1.8875749111175537,
      "learning_rate": 0.00013967595581954972,
      "loss": 1.0422,
      "step": 3920
    },
    {
      "epoch": 0.5512442007591734,
      "grad_norm": 1.531981348991394,
      "learning_rate": 0.00013946245981282166,
      "loss": 1.2123,
      "step": 3921
    },
    {
      "epoch": 0.5513847884155771,
      "grad_norm": 1.55530846118927,
      "learning_rate": 0.00013924875054291728,
      "loss": 1.349,
      "step": 3922
    },
    {
      "epoch": 0.5515253760719809,
      "grad_norm": 1.7464920282363892,
      "learning_rate": 0.00013903482916476513,
      "loss": 1.0606,
      "step": 3923
    },
    {
      "epoch": 0.5516659637283846,
      "grad_norm": 1.5669474601745605,
      "learning_rate": 0.00013882069683443968,
      "loss": 1.0327,
      "step": 3924
    },
    {
      "epoch": 0.5518065513847884,
      "grad_norm": 1.4222705364227295,
      "learning_rate": 0.00013860635470915603,
      "loss": 1.0018,
      "step": 3925
    },
    {
      "epoch": 0.5519471390411922,
      "grad_norm": 1.3238657712936401,
      "learning_rate": 0.0001383918039472624,
      "loss": 1.2434,
      "step": 3926
    },
    {
      "epoch": 0.552087726697596,
      "grad_norm": 1.8229690790176392,
      "learning_rate": 0.00013817704570823512,
      "loss": 1.1299,
      "step": 3927
    },
    {
      "epoch": 0.5522283143539998,
      "grad_norm": 1.527869462966919,
      "learning_rate": 0.00013796208115267114,
      "loss": 0.953,
      "step": 3928
    },
    {
      "epoch": 0.5523689020104035,
      "grad_norm": 1.3534142971038818,
      "learning_rate": 0.00013774691144228312,
      "loss": 1.1512,
      "step": 3929
    },
    {
      "epoch": 0.5525094896668072,
      "grad_norm": 1.6637139320373535,
      "learning_rate": 0.00013753153773989177,
      "loss": 1.0237,
      "step": 3930
    },
    {
      "epoch": 0.552650077323211,
      "grad_norm": 1.475791335105896,
      "learning_rate": 0.0001373159612094205,
      "loss": 1.2538,
      "step": 3931
    },
    {
      "epoch": 0.5527906649796148,
      "grad_norm": 1.4248042106628418,
      "learning_rate": 0.00013710018301588893,
      "loss": 1.0571,
      "step": 3932
    },
    {
      "epoch": 0.5529312526360186,
      "grad_norm": 1.6820420026779175,
      "learning_rate": 0.00013688420432540635,
      "loss": 0.9716,
      "step": 3933
    },
    {
      "epoch": 0.5530718402924223,
      "grad_norm": 1.5674381256103516,
      "learning_rate": 0.00013666802630516567,
      "loss": 1.2184,
      "step": 3934
    },
    {
      "epoch": 0.5532124279488261,
      "grad_norm": 1.5818829536437988,
      "learning_rate": 0.00013645165012343688,
      "loss": 1.0481,
      "step": 3935
    },
    {
      "epoch": 0.5533530156052299,
      "grad_norm": 1.56309175491333,
      "learning_rate": 0.00013623507694956102,
      "loss": 1.0375,
      "step": 3936
    },
    {
      "epoch": 0.5534936032616337,
      "grad_norm": 1.7113338708877563,
      "learning_rate": 0.0001360183079539436,
      "loss": 1.1702,
      "step": 3937
    },
    {
      "epoch": 0.5536341909180374,
      "grad_norm": 1.703328013420105,
      "learning_rate": 0.00013580134430804866,
      "loss": 1.0707,
      "step": 3938
    },
    {
      "epoch": 0.5537747785744411,
      "grad_norm": 1.4956313371658325,
      "learning_rate": 0.0001355841871843917,
      "loss": 1.2238,
      "step": 3939
    },
    {
      "epoch": 0.5539153662308449,
      "grad_norm": 1.680105209350586,
      "learning_rate": 0.00013536683775653422,
      "loss": 1.2278,
      "step": 3940
    },
    {
      "epoch": 0.5540559538872487,
      "grad_norm": 1.55063796043396,
      "learning_rate": 0.00013514929719907678,
      "loss": 1.0527,
      "step": 3941
    },
    {
      "epoch": 0.5541965415436525,
      "grad_norm": 1.355248212814331,
      "learning_rate": 0.00013493156668765302,
      "loss": 1.0801,
      "step": 3942
    },
    {
      "epoch": 0.5543371292000563,
      "grad_norm": 1.7224987745285034,
      "learning_rate": 0.00013471364739892284,
      "loss": 1.1422,
      "step": 3943
    },
    {
      "epoch": 0.55447771685646,
      "grad_norm": 1.6128566265106201,
      "learning_rate": 0.00013449554051056655,
      "loss": 1.1973,
      "step": 3944
    },
    {
      "epoch": 0.5546183045128638,
      "grad_norm": 1.7153887748718262,
      "learning_rate": 0.00013427724720127822,
      "loss": 1.046,
      "step": 3945
    },
    {
      "epoch": 0.5547588921692675,
      "grad_norm": 1.62929368019104,
      "learning_rate": 0.00013405876865075952,
      "loss": 1.2837,
      "step": 3946
    },
    {
      "epoch": 0.5548994798256713,
      "grad_norm": 1.4206769466400146,
      "learning_rate": 0.00013384010603971288,
      "loss": 1.2518,
      "step": 3947
    },
    {
      "epoch": 0.5550400674820751,
      "grad_norm": 1.7167932987213135,
      "learning_rate": 0.00013362126054983567,
      "loss": 1.0929,
      "step": 3948
    },
    {
      "epoch": 0.5551806551384788,
      "grad_norm": 1.9676324129104614,
      "learning_rate": 0.0001334022333638135,
      "loss": 1.017,
      "step": 3949
    },
    {
      "epoch": 0.5553212427948826,
      "grad_norm": 1.7059996128082275,
      "learning_rate": 0.00013318302566531405,
      "loss": 1.2151,
      "step": 3950
    },
    {
      "epoch": 0.5554618304512864,
      "grad_norm": 1.6943539381027222,
      "learning_rate": 0.00013296363863898037,
      "loss": 1.1996,
      "step": 3951
    },
    {
      "epoch": 0.5556024181076902,
      "grad_norm": 1.588172197341919,
      "learning_rate": 0.00013274407347042448,
      "loss": 1.0493,
      "step": 3952
    },
    {
      "epoch": 0.555743005764094,
      "grad_norm": 1.566749930381775,
      "learning_rate": 0.0001325243313462216,
      "loss": 1.0695,
      "step": 3953
    },
    {
      "epoch": 0.5558835934204976,
      "grad_norm": 1.77114999294281,
      "learning_rate": 0.00013230441345390273,
      "loss": 1.141,
      "step": 3954
    },
    {
      "epoch": 0.5560241810769014,
      "grad_norm": 1.5325640439987183,
      "learning_rate": 0.0001320843209819492,
      "loss": 1.2529,
      "step": 3955
    },
    {
      "epoch": 0.5561647687333052,
      "grad_norm": 1.328894019126892,
      "learning_rate": 0.0001318640551197852,
      "loss": 1.1602,
      "step": 3956
    },
    {
      "epoch": 0.556305356389709,
      "grad_norm": 1.6925562620162964,
      "learning_rate": 0.00013164361705777283,
      "loss": 1.0591,
      "step": 3957
    },
    {
      "epoch": 0.5564459440461128,
      "grad_norm": 1.4138293266296387,
      "learning_rate": 0.00013142300798720395,
      "loss": 1.1272,
      "step": 3958
    },
    {
      "epoch": 0.5565865317025165,
      "grad_norm": 1.6659469604492188,
      "learning_rate": 0.00013120222910029515,
      "loss": 1.08,
      "step": 3959
    },
    {
      "epoch": 0.5567271193589203,
      "grad_norm": 1.79518461227417,
      "learning_rate": 0.0001309812815901803,
      "loss": 1.1633,
      "step": 3960
    },
    {
      "epoch": 0.5568677070153241,
      "grad_norm": 1.5644574165344238,
      "learning_rate": 0.00013076016665090513,
      "loss": 1.1582,
      "step": 3961
    },
    {
      "epoch": 0.5570082946717279,
      "grad_norm": 1.4507501125335693,
      "learning_rate": 0.00013053888547741953,
      "loss": 1.1284,
      "step": 3962
    },
    {
      "epoch": 0.5571488823281316,
      "grad_norm": 1.7567702531814575,
      "learning_rate": 0.00013031743926557215,
      "loss": 1.0889,
      "step": 3963
    },
    {
      "epoch": 0.5572894699845353,
      "grad_norm": 1.5538691282272339,
      "learning_rate": 0.00013009582921210354,
      "loss": 0.9984,
      "step": 3964
    },
    {
      "epoch": 0.5574300576409391,
      "grad_norm": 1.7582006454467773,
      "learning_rate": 0.0001298740565146396,
      "loss": 1.1015,
      "step": 3965
    },
    {
      "epoch": 0.5575706452973429,
      "grad_norm": 1.521117091178894,
      "learning_rate": 0.00012965212237168504,
      "loss": 1.1838,
      "step": 3966
    },
    {
      "epoch": 0.5577112329537467,
      "grad_norm": 2.1228201389312744,
      "learning_rate": 0.00012943002798261732,
      "loss": 1.1935,
      "step": 3967
    },
    {
      "epoch": 0.5578518206101505,
      "grad_norm": 1.7819856405258179,
      "learning_rate": 0.00012920777454767976,
      "loss": 0.9513,
      "step": 3968
    },
    {
      "epoch": 0.5579924082665542,
      "grad_norm": 1.4721482992172241,
      "learning_rate": 0.00012898536326797524,
      "loss": 0.9879,
      "step": 3969
    },
    {
      "epoch": 0.558132995922958,
      "grad_norm": 1.5510343313217163,
      "learning_rate": 0.0001287627953454597,
      "loss": 1.143,
      "step": 3970
    },
    {
      "epoch": 0.5582735835793617,
      "grad_norm": 1.7081395387649536,
      "learning_rate": 0.00012854007198293533,
      "loss": 1.1305,
      "step": 3971
    },
    {
      "epoch": 0.5584141712357655,
      "grad_norm": 1.5295957326889038,
      "learning_rate": 0.00012831719438404494,
      "loss": 1.1284,
      "step": 3972
    },
    {
      "epoch": 0.5585547588921693,
      "grad_norm": 1.4552247524261475,
      "learning_rate": 0.0001280941637532642,
      "loss": 1.1684,
      "step": 3973
    },
    {
      "epoch": 0.558695346548573,
      "grad_norm": 1.5246106386184692,
      "learning_rate": 0.00012787098129589627,
      "loss": 1.1401,
      "step": 3974
    },
    {
      "epoch": 0.5588359342049768,
      "grad_norm": 1.6140977144241333,
      "learning_rate": 0.00012764764821806442,
      "loss": 1.1534,
      "step": 3975
    },
    {
      "epoch": 0.5589765218613806,
      "grad_norm": 1.8850557804107666,
      "learning_rate": 0.00012742416572670648,
      "loss": 1.1321,
      "step": 3976
    },
    {
      "epoch": 0.5591171095177844,
      "grad_norm": 1.4362857341766357,
      "learning_rate": 0.00012720053502956707,
      "loss": 1.2769,
      "step": 3977
    },
    {
      "epoch": 0.559257697174188,
      "grad_norm": 2.1125741004943848,
      "learning_rate": 0.00012697675733519225,
      "loss": 0.9734,
      "step": 3978
    },
    {
      "epoch": 0.5593982848305918,
      "grad_norm": 1.383237600326538,
      "learning_rate": 0.00012675283385292206,
      "loss": 1.0679,
      "step": 3979
    },
    {
      "epoch": 0.5595388724869956,
      "grad_norm": 1.5852957963943481,
      "learning_rate": 0.00012652876579288496,
      "loss": 0.9735,
      "step": 3980
    },
    {
      "epoch": 0.5596794601433994,
      "grad_norm": 1.5995701551437378,
      "learning_rate": 0.00012630455436599017,
      "loss": 1.0902,
      "step": 3981
    },
    {
      "epoch": 0.5598200477998032,
      "grad_norm": 1.5200456380844116,
      "learning_rate": 0.00012608020078392213,
      "loss": 0.9442,
      "step": 3982
    },
    {
      "epoch": 0.5599606354562069,
      "grad_norm": 1.3112149238586426,
      "learning_rate": 0.000125855706259133,
      "loss": 1.2304,
      "step": 3983
    },
    {
      "epoch": 0.5601012231126107,
      "grad_norm": 1.7582054138183594,
      "learning_rate": 0.00012563107200483739,
      "loss": 1.0576,
      "step": 3984
    },
    {
      "epoch": 0.5602418107690145,
      "grad_norm": 1.3468613624572754,
      "learning_rate": 0.00012540629923500426,
      "loss": 0.9773,
      "step": 3985
    },
    {
      "epoch": 0.5603823984254183,
      "grad_norm": 1.548923134803772,
      "learning_rate": 0.00012518138916435168,
      "loss": 1.0694,
      "step": 3986
    },
    {
      "epoch": 0.560522986081822,
      "grad_norm": 1.6743748188018799,
      "learning_rate": 0.00012495634300833928,
      "loss": 0.9958,
      "step": 3987
    },
    {
      "epoch": 0.5606635737382257,
      "grad_norm": 1.6062607765197754,
      "learning_rate": 0.00012473116198316266,
      "loss": 1.1341,
      "step": 3988
    },
    {
      "epoch": 0.5608041613946295,
      "grad_norm": 1.6284127235412598,
      "learning_rate": 0.00012450584730574574,
      "loss": 1.1989,
      "step": 3989
    },
    {
      "epoch": 0.5609447490510333,
      "grad_norm": 1.6897327899932861,
      "learning_rate": 0.00012428040019373503,
      "loss": 1.127,
      "step": 3990
    },
    {
      "epoch": 0.5610853367074371,
      "grad_norm": 1.421090006828308,
      "learning_rate": 0.00012405482186549267,
      "loss": 1.0254,
      "step": 3991
    },
    {
      "epoch": 0.5612259243638409,
      "grad_norm": 1.6002284288406372,
      "learning_rate": 0.00012382911354008995,
      "loss": 0.9916,
      "step": 3992
    },
    {
      "epoch": 0.5613665120202446,
      "grad_norm": 1.5964062213897705,
      "learning_rate": 0.00012360327643730075,
      "loss": 1.2777,
      "step": 3993
    },
    {
      "epoch": 0.5615070996766484,
      "grad_norm": 1.5343259572982788,
      "learning_rate": 0.0001233773117775946,
      "loss": 1.1566,
      "step": 3994
    },
    {
      "epoch": 0.5616476873330521,
      "grad_norm": 1.9964089393615723,
      "learning_rate": 0.00012315122078213072,
      "loss": 1.0836,
      "step": 3995
    },
    {
      "epoch": 0.5617882749894559,
      "grad_norm": 1.665576696395874,
      "learning_rate": 0.00012292500467275092,
      "loss": 1.0852,
      "step": 3996
    },
    {
      "epoch": 0.5619288626458597,
      "grad_norm": 1.567152976989746,
      "learning_rate": 0.00012269866467197322,
      "loss": 1.1021,
      "step": 3997
    },
    {
      "epoch": 0.5620694503022634,
      "grad_norm": 1.8434057235717773,
      "learning_rate": 0.00012247220200298494,
      "loss": 0.9909,
      "step": 3998
    },
    {
      "epoch": 0.5622100379586672,
      "grad_norm": 1.3561681509017944,
      "learning_rate": 0.0001222456178896366,
      "loss": 1.1852,
      "step": 3999
    },
    {
      "epoch": 0.562350625615071,
      "grad_norm": 1.6375491619110107,
      "learning_rate": 0.00012201891355643498,
      "loss": 1.2658,
      "step": 4000
    },
    {
      "epoch": 0.562350625615071,
      "eval_loss": 1.1635297536849976,
      "eval_runtime": 771.6759,
      "eval_samples_per_second": 16.388,
      "eval_steps_per_second": 8.194,
      "step": 4000
    },
    {
      "epoch": 0.5624912132714748,
      "grad_norm": 1.624438762664795,
      "learning_rate": 0.00012179209022853654,
      "loss": 1.0555,
      "step": 4001
    },
    {
      "epoch": 0.5626318009278786,
      "grad_norm": 1.8508399724960327,
      "learning_rate": 0.0001215651491317405,
      "loss": 1.1328,
      "step": 4002
    },
    {
      "epoch": 0.5627723885842822,
      "grad_norm": 2.248283863067627,
      "learning_rate": 0.00012133809149248321,
      "loss": 1.139,
      "step": 4003
    },
    {
      "epoch": 0.562912976240686,
      "grad_norm": 2.1197385787963867,
      "learning_rate": 0.00012111091853783014,
      "loss": 1.0557,
      "step": 4004
    },
    {
      "epoch": 0.5630535638970898,
      "grad_norm": 1.6341224908828735,
      "learning_rate": 0.00012088363149547039,
      "loss": 1.1225,
      "step": 4005
    },
    {
      "epoch": 0.5631941515534936,
      "grad_norm": 1.6008939743041992,
      "learning_rate": 0.00012065623159370926,
      "loss": 1.1455,
      "step": 4006
    },
    {
      "epoch": 0.5633347392098974,
      "grad_norm": 1.8846298456192017,
      "learning_rate": 0.00012042872006146249,
      "loss": 1.0211,
      "step": 4007
    },
    {
      "epoch": 0.5634753268663011,
      "grad_norm": 1.8133467435836792,
      "learning_rate": 0.00012020109812824847,
      "loss": 1.0416,
      "step": 4008
    },
    {
      "epoch": 0.5636159145227049,
      "grad_norm": 1.6468199491500854,
      "learning_rate": 0.00011997336702418266,
      "loss": 1.2404,
      "step": 4009
    },
    {
      "epoch": 0.5637565021791087,
      "grad_norm": 1.9869403839111328,
      "learning_rate": 0.00011974552797997039,
      "loss": 1.0478,
      "step": 4010
    },
    {
      "epoch": 0.5638970898355125,
      "grad_norm": 1.3261923789978027,
      "learning_rate": 0.00011951758222690021,
      "loss": 1.1583,
      "step": 4011
    },
    {
      "epoch": 0.5640376774919162,
      "grad_norm": 1.650475263595581,
      "learning_rate": 0.0001192895309968376,
      "loss": 1.1306,
      "step": 4012
    },
    {
      "epoch": 0.5641782651483199,
      "grad_norm": 1.4865940809249878,
      "learning_rate": 0.00011906137552221766,
      "loss": 0.9445,
      "step": 4013
    },
    {
      "epoch": 0.5643188528047237,
      "grad_norm": 1.3799567222595215,
      "learning_rate": 0.00011883311703603919,
      "loss": 1.0271,
      "step": 4014
    },
    {
      "epoch": 0.5644594404611275,
      "grad_norm": 1.3713600635528564,
      "learning_rate": 0.00011860475677185756,
      "loss": 1.1148,
      "step": 4015
    },
    {
      "epoch": 0.5646000281175313,
      "grad_norm": 1.6336877346038818,
      "learning_rate": 0.00011837629596377829,
      "loss": 0.9557,
      "step": 4016
    },
    {
      "epoch": 0.5647406157739351,
      "grad_norm": 1.601557970046997,
      "learning_rate": 0.00011814773584644995,
      "loss": 1.1583,
      "step": 4017
    },
    {
      "epoch": 0.5648812034303388,
      "grad_norm": 1.618944525718689,
      "learning_rate": 0.00011791907765505815,
      "loss": 1.1211,
      "step": 4018
    },
    {
      "epoch": 0.5650217910867426,
      "grad_norm": 1.4385606050491333,
      "learning_rate": 0.0001176903226253183,
      "loss": 1.1319,
      "step": 4019
    },
    {
      "epoch": 0.5651623787431463,
      "grad_norm": 1.8210099935531616,
      "learning_rate": 0.00011746147199346931,
      "loss": 1.2691,
      "step": 4020
    },
    {
      "epoch": 0.5653029663995501,
      "grad_norm": 1.6339364051818848,
      "learning_rate": 0.00011723252699626648,
      "loss": 1.1182,
      "step": 4021
    },
    {
      "epoch": 0.5654435540559539,
      "grad_norm": 1.8090896606445312,
      "learning_rate": 0.00011700348887097532,
      "loss": 1.0659,
      "step": 4022
    },
    {
      "epoch": 0.5655841417123576,
      "grad_norm": 1.5306227207183838,
      "learning_rate": 0.00011677435885536454,
      "loss": 1.0094,
      "step": 4023
    },
    {
      "epoch": 0.5657247293687614,
      "grad_norm": 1.6284476518630981,
      "learning_rate": 0.00011654513818769954,
      "loss": 1.0256,
      "step": 4024
    },
    {
      "epoch": 0.5658653170251652,
      "grad_norm": 2.015366554260254,
      "learning_rate": 0.00011631582810673534,
      "loss": 1.1606,
      "step": 4025
    },
    {
      "epoch": 0.566005904681569,
      "grad_norm": 1.5720336437225342,
      "learning_rate": 0.00011608642985171046,
      "loss": 1.1766,
      "step": 4026
    },
    {
      "epoch": 0.5661464923379728,
      "grad_norm": 1.4904803037643433,
      "learning_rate": 0.0001158569446623398,
      "loss": 1.2244,
      "step": 4027
    },
    {
      "epoch": 0.5662870799943764,
      "grad_norm": 1.5877892971038818,
      "learning_rate": 0.00011562737377880814,
      "loss": 0.9824,
      "step": 4028
    },
    {
      "epoch": 0.5664276676507802,
      "grad_norm": 2.1488122940063477,
      "learning_rate": 0.00011539771844176342,
      "loss": 1.1136,
      "step": 4029
    },
    {
      "epoch": 0.566568255307184,
      "grad_norm": 1.5014896392822266,
      "learning_rate": 0.00011516797989230956,
      "loss": 1.2437,
      "step": 4030
    },
    {
      "epoch": 0.5667088429635878,
      "grad_norm": 1.3258026838302612,
      "learning_rate": 0.00011493815937200094,
      "loss": 1.112,
      "step": 4031
    },
    {
      "epoch": 0.5668494306199916,
      "grad_norm": 1.4623006582260132,
      "learning_rate": 0.00011470825812283417,
      "loss": 0.9097,
      "step": 4032
    },
    {
      "epoch": 0.5669900182763953,
      "grad_norm": 1.5426688194274902,
      "learning_rate": 0.00011447827738724269,
      "loss": 1.1655,
      "step": 4033
    },
    {
      "epoch": 0.5671306059327991,
      "grad_norm": 1.58692467212677,
      "learning_rate": 0.00011424821840808895,
      "loss": 0.9751,
      "step": 4034
    },
    {
      "epoch": 0.5672711935892029,
      "grad_norm": 1.3573929071426392,
      "learning_rate": 0.00011401808242865894,
      "loss": 1.2641,
      "step": 4035
    },
    {
      "epoch": 0.5674117812456067,
      "grad_norm": 1.460105299949646,
      "learning_rate": 0.00011378787069265415,
      "loss": 1.0974,
      "step": 4036
    },
    {
      "epoch": 0.5675523689020104,
      "grad_norm": 1.286098837852478,
      "learning_rate": 0.00011355758444418592,
      "loss": 1.0506,
      "step": 4037
    },
    {
      "epoch": 0.5676929565584141,
      "grad_norm": 1.4736844301223755,
      "learning_rate": 0.00011332722492776774,
      "loss": 1.1226,
      "step": 4038
    },
    {
      "epoch": 0.5678335442148179,
      "grad_norm": 1.400517225265503,
      "learning_rate": 0.00011309679338830983,
      "loss": 1.0403,
      "step": 4039
    },
    {
      "epoch": 0.5679741318712217,
      "grad_norm": 1.496596336364746,
      "learning_rate": 0.00011286629107111086,
      "loss": 1.2497,
      "step": 4040
    },
    {
      "epoch": 0.5681147195276255,
      "grad_norm": 1.4562294483184814,
      "learning_rate": 0.00011263571922185245,
      "loss": 1.2057,
      "step": 4041
    },
    {
      "epoch": 0.5682553071840293,
      "grad_norm": 1.5912797451019287,
      "learning_rate": 0.00011240507908659191,
      "loss": 1.1049,
      "step": 4042
    },
    {
      "epoch": 0.568395894840433,
      "grad_norm": 1.5452648401260376,
      "learning_rate": 0.00011217437191175554,
      "loss": 1.0454,
      "step": 4043
    },
    {
      "epoch": 0.5685364824968367,
      "grad_norm": 1.6715019941329956,
      "learning_rate": 0.00011194359894413185,
      "loss": 1.3876,
      "step": 4044
    },
    {
      "epoch": 0.5686770701532405,
      "grad_norm": 1.839421272277832,
      "learning_rate": 0.00011171276143086502,
      "loss": 1.2129,
      "step": 4045
    },
    {
      "epoch": 0.5688176578096443,
      "grad_norm": 1.5027565956115723,
      "learning_rate": 0.00011148186061944805,
      "loss": 1.0731,
      "step": 4046
    },
    {
      "epoch": 0.5689582454660481,
      "grad_norm": 1.7460906505584717,
      "learning_rate": 0.000111250897757716,
      "loss": 1.0549,
      "step": 4047
    },
    {
      "epoch": 0.5690988331224518,
      "grad_norm": 1.6669596433639526,
      "learning_rate": 0.00011101987409383934,
      "loss": 1.0306,
      "step": 4048
    },
    {
      "epoch": 0.5692394207788556,
      "grad_norm": 1.4240697622299194,
      "learning_rate": 0.00011078879087631673,
      "loss": 1.2094,
      "step": 4049
    },
    {
      "epoch": 0.5693800084352594,
      "grad_norm": 1.5358521938323975,
      "learning_rate": 0.00011055764935396938,
      "loss": 1.2325,
      "step": 4050
    },
    {
      "epoch": 0.5695205960916632,
      "grad_norm": 1.4270415306091309,
      "learning_rate": 0.00011032645077593288,
      "loss": 1.0587,
      "step": 4051
    },
    {
      "epoch": 0.569661183748067,
      "grad_norm": 2.9819393157958984,
      "learning_rate": 0.00011009519639165169,
      "loss": 1.188,
      "step": 4052
    },
    {
      "epoch": 0.5698017714044706,
      "grad_norm": 1.8607054948806763,
      "learning_rate": 0.00010986388745087135,
      "loss": 1.0192,
      "step": 4053
    },
    {
      "epoch": 0.5699423590608744,
      "grad_norm": 1.5495058298110962,
      "learning_rate": 0.0001096325252036329,
      "loss": 1.1255,
      "step": 4054
    },
    {
      "epoch": 0.5700829467172782,
      "grad_norm": 1.585081696510315,
      "learning_rate": 0.00010940111090026477,
      "loss": 1.0341,
      "step": 4055
    },
    {
      "epoch": 0.570223534373682,
      "grad_norm": 1.5388468503952026,
      "learning_rate": 0.00010916964579137725,
      "loss": 0.9729,
      "step": 4056
    },
    {
      "epoch": 0.5703641220300858,
      "grad_norm": 1.5102996826171875,
      "learning_rate": 0.00010893813112785467,
      "loss": 1.0614,
      "step": 4057
    },
    {
      "epoch": 0.5705047096864895,
      "grad_norm": 1.9239026308059692,
      "learning_rate": 0.00010870656816084983,
      "loss": 1.0869,
      "step": 4058
    },
    {
      "epoch": 0.5706452973428933,
      "grad_norm": 1.6632723808288574,
      "learning_rate": 0.00010847495814177593,
      "loss": 1.1031,
      "step": 4059
    },
    {
      "epoch": 0.570785884999297,
      "grad_norm": 1.9787684679031372,
      "learning_rate": 0.00010824330232230089,
      "loss": 1.0795,
      "step": 4060
    },
    {
      "epoch": 0.5709264726557008,
      "grad_norm": 1.3629584312438965,
      "learning_rate": 0.00010801160195433972,
      "loss": 1.2766,
      "step": 4061
    },
    {
      "epoch": 0.5710670603121046,
      "grad_norm": 1.5499821901321411,
      "learning_rate": 0.00010777985829004881,
      "loss": 0.9935,
      "step": 4062
    },
    {
      "epoch": 0.5712076479685083,
      "grad_norm": 1.7588714361190796,
      "learning_rate": 0.00010754807258181787,
      "loss": 1.0969,
      "step": 4063
    },
    {
      "epoch": 0.5713482356249121,
      "grad_norm": 1.5832879543304443,
      "learning_rate": 0.00010731624608226427,
      "loss": 1.3267,
      "step": 4064
    },
    {
      "epoch": 0.5714888232813159,
      "grad_norm": 1.6579564809799194,
      "learning_rate": 0.00010708438004422543,
      "loss": 1.0284,
      "step": 4065
    },
    {
      "epoch": 0.5716294109377197,
      "grad_norm": 1.4698227643966675,
      "learning_rate": 0.00010685247572075303,
      "loss": 1.1115,
      "step": 4066
    },
    {
      "epoch": 0.5717699985941235,
      "grad_norm": 1.4323146343231201,
      "learning_rate": 0.00010662053436510504,
      "loss": 1.0304,
      "step": 4067
    },
    {
      "epoch": 0.5719105862505272,
      "grad_norm": 1.3872884511947632,
      "learning_rate": 0.00010638855723073989,
      "loss": 0.9836,
      "step": 4068
    },
    {
      "epoch": 0.5720511739069309,
      "grad_norm": 1.7440104484558105,
      "learning_rate": 0.0001061565455713093,
      "loss": 1.3509,
      "step": 4069
    },
    {
      "epoch": 0.5721917615633347,
      "grad_norm": 1.6638835668563843,
      "learning_rate": 0.00010592450064065159,
      "loss": 1.0616,
      "step": 4070
    },
    {
      "epoch": 0.5723323492197385,
      "grad_norm": 1.874626874923706,
      "learning_rate": 0.00010569242369278491,
      "loss": 1.0749,
      "step": 4071
    },
    {
      "epoch": 0.5724729368761423,
      "grad_norm": 1.3097511529922485,
      "learning_rate": 0.00010546031598190027,
      "loss": 0.9199,
      "step": 4072
    },
    {
      "epoch": 0.572613524532546,
      "grad_norm": 1.534995198249817,
      "learning_rate": 0.00010522817876235509,
      "loss": 1.2265,
      "step": 4073
    },
    {
      "epoch": 0.5727541121889498,
      "grad_norm": 1.5412578582763672,
      "learning_rate": 0.00010499601328866626,
      "loss": 1.2338,
      "step": 4074
    },
    {
      "epoch": 0.5728946998453536,
      "grad_norm": 1.59591805934906,
      "learning_rate": 0.00010476382081550338,
      "loss": 1.1714,
      "step": 4075
    },
    {
      "epoch": 0.5730352875017574,
      "grad_norm": 1.4012467861175537,
      "learning_rate": 0.00010453160259768171,
      "loss": 0.918,
      "step": 4076
    },
    {
      "epoch": 0.5731758751581612,
      "grad_norm": 1.4918253421783447,
      "learning_rate": 0.00010429935989015595,
      "loss": 1.2636,
      "step": 4077
    },
    {
      "epoch": 0.5733164628145648,
      "grad_norm": 1.2853596210479736,
      "learning_rate": 0.00010406709394801304,
      "loss": 1.091,
      "step": 4078
    },
    {
      "epoch": 0.5734570504709686,
      "grad_norm": 1.900497555732727,
      "learning_rate": 0.00010383480602646551,
      "loss": 1.069,
      "step": 4079
    },
    {
      "epoch": 0.5735976381273724,
      "grad_norm": 1.5572820901870728,
      "learning_rate": 0.00010360249738084443,
      "loss": 1.1396,
      "step": 4080
    },
    {
      "epoch": 0.5737382257837762,
      "grad_norm": 1.4134215116500854,
      "learning_rate": 0.00010337016926659337,
      "loss": 1.1554,
      "step": 4081
    },
    {
      "epoch": 0.57387881344018,
      "grad_norm": 1.8869800567626953,
      "learning_rate": 0.0001031378229392606,
      "loss": 1.106,
      "step": 4082
    },
    {
      "epoch": 0.5740194010965837,
      "grad_norm": 1.4702309370040894,
      "learning_rate": 0.00010290545965449318,
      "loss": 1.0756,
      "step": 4083
    },
    {
      "epoch": 0.5741599887529875,
      "grad_norm": 1.518019676208496,
      "learning_rate": 0.00010267308066802941,
      "loss": 1.1607,
      "step": 4084
    },
    {
      "epoch": 0.5743005764093913,
      "grad_norm": 1.3203870058059692,
      "learning_rate": 0.00010244068723569302,
      "loss": 1.2398,
      "step": 4085
    },
    {
      "epoch": 0.574441164065795,
      "grad_norm": 1.4370421171188354,
      "learning_rate": 0.00010220828061338525,
      "loss": 1.1538,
      "step": 4086
    },
    {
      "epoch": 0.5745817517221988,
      "grad_norm": 1.8046213388442993,
      "learning_rate": 0.00010197586205707894,
      "loss": 1.0626,
      "step": 4087
    },
    {
      "epoch": 0.5747223393786025,
      "grad_norm": 1.2604687213897705,
      "learning_rate": 0.00010174343282281134,
      "loss": 1.0449,
      "step": 4088
    },
    {
      "epoch": 0.5748629270350063,
      "grad_norm": 1.3485549688339233,
      "learning_rate": 0.00010151099416667742,
      "loss": 1.169,
      "step": 4089
    },
    {
      "epoch": 0.5750035146914101,
      "grad_norm": 1.2802033424377441,
      "learning_rate": 0.00010127854734482311,
      "loss": 1.2495,
      "step": 4090
    },
    {
      "epoch": 0.5751441023478139,
      "grad_norm": 1.610844373703003,
      "learning_rate": 0.00010104609361343828,
      "loss": 0.8887,
      "step": 4091
    },
    {
      "epoch": 0.5752846900042177,
      "grad_norm": 1.995839238166809,
      "learning_rate": 0.00010081363422875031,
      "loss": 1.0287,
      "step": 4092
    },
    {
      "epoch": 0.5754252776606213,
      "grad_norm": 2.0530014038085938,
      "learning_rate": 0.00010058117044701716,
      "loss": 1.2103,
      "step": 4093
    },
    {
      "epoch": 0.5755658653170251,
      "grad_norm": 1.7005441188812256,
      "learning_rate": 0.00010034870352452055,
      "loss": 1.0509,
      "step": 4094
    },
    {
      "epoch": 0.5757064529734289,
      "grad_norm": 1.5908674001693726,
      "learning_rate": 0.00010011623471755891,
      "loss": 0.9264,
      "step": 4095
    },
    {
      "epoch": 0.5758470406298327,
      "grad_norm": 1.5426108837127686,
      "learning_rate": 9.988376528244117e-05,
      "loss": 1.3266,
      "step": 4096
    },
    {
      "epoch": 0.5759876282862365,
      "grad_norm": 1.809288740158081,
      "learning_rate": 9.965129647547962e-05,
      "loss": 1.0901,
      "step": 4097
    },
    {
      "epoch": 0.5761282159426402,
      "grad_norm": 1.5840275287628174,
      "learning_rate": 9.941882955298291e-05,
      "loss": 1.1056,
      "step": 4098
    },
    {
      "epoch": 0.576268803599044,
      "grad_norm": 1.5523747205734253,
      "learning_rate": 9.918636577124966e-05,
      "loss": 1.161,
      "step": 4099
    },
    {
      "epoch": 0.5764093912554478,
      "grad_norm": 1.6288014650344849,
      "learning_rate": 9.895390638656187e-05,
      "loss": 1.2052,
      "step": 4100
    },
    {
      "epoch": 0.5765499789118516,
      "grad_norm": 1.627454400062561,
      "learning_rate": 9.872145265517695e-05,
      "loss": 1.134,
      "step": 4101
    },
    {
      "epoch": 0.5766905665682553,
      "grad_norm": 1.3928477764129639,
      "learning_rate": 9.848900583332257e-05,
      "loss": 1.1645,
      "step": 4102
    },
    {
      "epoch": 0.576831154224659,
      "grad_norm": 1.5905396938323975,
      "learning_rate": 9.825656717718874e-05,
      "loss": 1.0821,
      "step": 4103
    },
    {
      "epoch": 0.5769717418810628,
      "grad_norm": 1.4843705892562866,
      "learning_rate": 9.802413794292112e-05,
      "loss": 1.0503,
      "step": 4104
    },
    {
      "epoch": 0.5771123295374666,
      "grad_norm": 1.5523444414138794,
      "learning_rate": 9.779171938661491e-05,
      "loss": 0.9415,
      "step": 4105
    },
    {
      "epoch": 0.5772529171938704,
      "grad_norm": 1.7932676076889038,
      "learning_rate": 9.755931276430705e-05,
      "loss": 1.1594,
      "step": 4106
    },
    {
      "epoch": 0.5773935048502742,
      "grad_norm": 1.5670524835586548,
      "learning_rate": 9.732691933197057e-05,
      "loss": 1.1582,
      "step": 4107
    },
    {
      "epoch": 0.5775340925066779,
      "grad_norm": 1.4571954011917114,
      "learning_rate": 9.709454034550697e-05,
      "loss": 1.2033,
      "step": 4108
    },
    {
      "epoch": 0.5776746801630817,
      "grad_norm": 1.6463483572006226,
      "learning_rate": 9.686217706073946e-05,
      "loss": 1.2277,
      "step": 4109
    },
    {
      "epoch": 0.5778152678194854,
      "grad_norm": 1.4486931562423706,
      "learning_rate": 9.662983073340658e-05,
      "loss": 0.9291,
      "step": 4110
    },
    {
      "epoch": 0.5779558554758892,
      "grad_norm": 1.3631370067596436,
      "learning_rate": 9.639750261915564e-05,
      "loss": 1.1951,
      "step": 4111
    },
    {
      "epoch": 0.578096443132293,
      "grad_norm": 1.9223905801773071,
      "learning_rate": 9.616519397353455e-05,
      "loss": 1.0936,
      "step": 4112
    },
    {
      "epoch": 0.5782370307886967,
      "grad_norm": 1.637665867805481,
      "learning_rate": 9.593290605198713e-05,
      "loss": 1.1062,
      "step": 4113
    },
    {
      "epoch": 0.5783776184451005,
      "grad_norm": 1.324052333831787,
      "learning_rate": 9.570064010984413e-05,
      "loss": 1.085,
      "step": 4114
    },
    {
      "epoch": 0.5785182061015043,
      "grad_norm": 1.7659406661987305,
      "learning_rate": 9.546839740231837e-05,
      "loss": 1.1038,
      "step": 4115
    },
    {
      "epoch": 0.5786587937579081,
      "grad_norm": 1.6049100160598755,
      "learning_rate": 9.523617918449678e-05,
      "loss": 1.0342,
      "step": 4116
    },
    {
      "epoch": 0.5787993814143119,
      "grad_norm": 1.6284385919570923,
      "learning_rate": 9.500398671133381e-05,
      "loss": 0.977,
      "step": 4117
    },
    {
      "epoch": 0.5789399690707155,
      "grad_norm": 1.3733383417129517,
      "learning_rate": 9.477182123764488e-05,
      "loss": 1.0997,
      "step": 4118
    },
    {
      "epoch": 0.5790805567271193,
      "grad_norm": 1.642594575881958,
      "learning_rate": 9.453968401809989e-05,
      "loss": 1.1188,
      "step": 4119
    },
    {
      "epoch": 0.5792211443835231,
      "grad_norm": 1.4414639472961426,
      "learning_rate": 9.430757630721516e-05,
      "loss": 1.0948,
      "step": 4120
    },
    {
      "epoch": 0.5793617320399269,
      "grad_norm": 1.6329948902130127,
      "learning_rate": 9.40754993593484e-05,
      "loss": 1.0896,
      "step": 4121
    },
    {
      "epoch": 0.5795023196963307,
      "grad_norm": 1.4446980953216553,
      "learning_rate": 9.384345442869077e-05,
      "loss": 1.2178,
      "step": 4122
    },
    {
      "epoch": 0.5796429073527344,
      "grad_norm": 1.5405209064483643,
      "learning_rate": 9.361144276926019e-05,
      "loss": 1.2474,
      "step": 4123
    },
    {
      "epoch": 0.5797834950091382,
      "grad_norm": 1.4495494365692139,
      "learning_rate": 9.337946563489512e-05,
      "loss": 1.1533,
      "step": 4124
    },
    {
      "epoch": 0.579924082665542,
      "grad_norm": 1.5312670469284058,
      "learning_rate": 9.314752427924703e-05,
      "loss": 1.0668,
      "step": 4125
    },
    {
      "epoch": 0.5800646703219458,
      "grad_norm": 1.2855480909347534,
      "learning_rate": 9.291561995577454e-05,
      "loss": 1.0934,
      "step": 4126
    },
    {
      "epoch": 0.5802052579783495,
      "grad_norm": 2.12532377243042,
      "learning_rate": 9.268375391773588e-05,
      "loss": 1.1042,
      "step": 4127
    },
    {
      "epoch": 0.5803458456347532,
      "grad_norm": 1.3586033582687378,
      "learning_rate": 9.245192741818221e-05,
      "loss": 1.163,
      "step": 4128
    },
    {
      "epoch": 0.580486433291157,
      "grad_norm": 1.3961461782455444,
      "learning_rate": 9.222014170995117e-05,
      "loss": 1.074,
      "step": 4129
    },
    {
      "epoch": 0.5806270209475608,
      "grad_norm": 1.438223123550415,
      "learning_rate": 9.198839804566034e-05,
      "loss": 1.2003,
      "step": 4130
    },
    {
      "epoch": 0.5807676086039646,
      "grad_norm": 1.408590316772461,
      "learning_rate": 9.175669767769918e-05,
      "loss": 1.1695,
      "step": 4131
    },
    {
      "epoch": 0.5809081962603684,
      "grad_norm": 1.4102329015731812,
      "learning_rate": 9.152504185822423e-05,
      "loss": 1.1762,
      "step": 4132
    },
    {
      "epoch": 0.5810487839167721,
      "grad_norm": 1.578930139541626,
      "learning_rate": 9.129343183915025e-05,
      "loss": 0.8909,
      "step": 4133
    },
    {
      "epoch": 0.5811893715731759,
      "grad_norm": 1.9491959810256958,
      "learning_rate": 9.106186887214532e-05,
      "loss": 1.0047,
      "step": 4134
    },
    {
      "epoch": 0.5813299592295796,
      "grad_norm": 1.7259621620178223,
      "learning_rate": 9.083035420862292e-05,
      "loss": 1.0757,
      "step": 4135
    },
    {
      "epoch": 0.5814705468859834,
      "grad_norm": 1.429060697555542,
      "learning_rate": 9.05988890997353e-05,
      "loss": 1.0324,
      "step": 4136
    },
    {
      "epoch": 0.5816111345423872,
      "grad_norm": 1.6342957019805908,
      "learning_rate": 9.03674747963671e-05,
      "loss": 1.2348,
      "step": 4137
    },
    {
      "epoch": 0.5817517221987909,
      "grad_norm": 1.7282531261444092,
      "learning_rate": 9.013611254912871e-05,
      "loss": 1.1168,
      "step": 4138
    },
    {
      "epoch": 0.5818923098551947,
      "grad_norm": 1.4712570905685425,
      "learning_rate": 8.990480360834838e-05,
      "loss": 1.1137,
      "step": 4139
    },
    {
      "epoch": 0.5820328975115985,
      "grad_norm": 1.3941638469696045,
      "learning_rate": 8.967354922406727e-05,
      "loss": 1.0554,
      "step": 4140
    },
    {
      "epoch": 0.5821734851680023,
      "grad_norm": 2.155773162841797,
      "learning_rate": 8.94423506460307e-05,
      "loss": 1.2755,
      "step": 4141
    },
    {
      "epoch": 0.5823140728244061,
      "grad_norm": 1.3591934442520142,
      "learning_rate": 8.921120912368326e-05,
      "loss": 1.0909,
      "step": 4142
    },
    {
      "epoch": 0.5824546604808097,
      "grad_norm": 1.3323320150375366,
      "learning_rate": 8.898012590616083e-05,
      "loss": 0.9737,
      "step": 4143
    },
    {
      "epoch": 0.5825952481372135,
      "grad_norm": 1.5060689449310303,
      "learning_rate": 8.874910224228407e-05,
      "loss": 0.9405,
      "step": 4144
    },
    {
      "epoch": 0.5827358357936173,
      "grad_norm": 1.3919918537139893,
      "learning_rate": 8.851813938055192e-05,
      "loss": 1.2147,
      "step": 4145
    },
    {
      "epoch": 0.5828764234500211,
      "grad_norm": 1.5048372745513916,
      "learning_rate": 8.828723856913505e-05,
      "loss": 1.3283,
      "step": 4146
    },
    {
      "epoch": 0.5830170111064249,
      "grad_norm": 1.612302303314209,
      "learning_rate": 8.805640105586823e-05,
      "loss": 0.9325,
      "step": 4147
    },
    {
      "epoch": 0.5831575987628286,
      "grad_norm": 1.6926614046096802,
      "learning_rate": 8.782562808824462e-05,
      "loss": 1.209,
      "step": 4148
    },
    {
      "epoch": 0.5832981864192324,
      "grad_norm": 1.5206537246704102,
      "learning_rate": 8.759492091340817e-05,
      "loss": 1.032,
      "step": 4149
    },
    {
      "epoch": 0.5834387740756362,
      "grad_norm": 1.4960602521896362,
      "learning_rate": 8.736428077814752e-05,
      "loss": 1.0302,
      "step": 4150
    },
    {
      "epoch": 0.58357936173204,
      "grad_norm": 1.4819774627685547,
      "learning_rate": 8.71337089288893e-05,
      "loss": 1.1802,
      "step": 4151
    },
    {
      "epoch": 0.5837199493884437,
      "grad_norm": 1.4674115180969238,
      "learning_rate": 8.690320661169024e-05,
      "loss": 0.9929,
      "step": 4152
    },
    {
      "epoch": 0.5838605370448474,
      "grad_norm": 1.745369791984558,
      "learning_rate": 8.667277507223222e-05,
      "loss": 1.0825,
      "step": 4153
    },
    {
      "epoch": 0.5840011247012512,
      "grad_norm": 1.622039794921875,
      "learning_rate": 8.644241555581424e-05,
      "loss": 1.1253,
      "step": 4154
    },
    {
      "epoch": 0.584141712357655,
      "grad_norm": 1.5848309993743896,
      "learning_rate": 8.621212930734591e-05,
      "loss": 1.1849,
      "step": 4155
    },
    {
      "epoch": 0.5842823000140588,
      "grad_norm": 1.486699104309082,
      "learning_rate": 8.598191757134104e-05,
      "loss": 1.173,
      "step": 4156
    },
    {
      "epoch": 0.5844228876704626,
      "grad_norm": 1.4568766355514526,
      "learning_rate": 8.575178159191113e-05,
      "loss": 1.019,
      "step": 4157
    },
    {
      "epoch": 0.5845634753268663,
      "grad_norm": 1.5060558319091797,
      "learning_rate": 8.552172261275739e-05,
      "loss": 1.1796,
      "step": 4158
    },
    {
      "epoch": 0.58470406298327,
      "grad_norm": 1.4735040664672852,
      "learning_rate": 8.52917418771659e-05,
      "loss": 1.181,
      "step": 4159
    },
    {
      "epoch": 0.5848446506396738,
      "grad_norm": 1.4132533073425293,
      "learning_rate": 8.506184062799914e-05,
      "loss": 1.1975,
      "step": 4160
    },
    {
      "epoch": 0.5849852382960776,
      "grad_norm": 1.7201894521713257,
      "learning_rate": 8.483202010769042e-05,
      "loss": 1.0339,
      "step": 4161
    },
    {
      "epoch": 0.5851258259524814,
      "grad_norm": 1.7111510038375854,
      "learning_rate": 8.460228155823684e-05,
      "loss": 1.1094,
      "step": 4162
    },
    {
      "epoch": 0.5852664136088851,
      "grad_norm": 1.7137900590896606,
      "learning_rate": 8.437262622119183e-05,
      "loss": 1.046,
      "step": 4163
    },
    {
      "epoch": 0.5854070012652889,
      "grad_norm": 1.4121006727218628,
      "learning_rate": 8.414305533766016e-05,
      "loss": 1.1409,
      "step": 4164
    },
    {
      "epoch": 0.5855475889216927,
      "grad_norm": 1.4923151731491089,
      "learning_rate": 8.391357014828971e-05,
      "loss": 1.2139,
      "step": 4165
    },
    {
      "epoch": 0.5856881765780965,
      "grad_norm": 1.6963143348693848,
      "learning_rate": 8.368417189326466e-05,
      "loss": 1.0116,
      "step": 4166
    },
    {
      "epoch": 0.5858287642345003,
      "grad_norm": 1.4766533374786377,
      "learning_rate": 8.345486181230062e-05,
      "loss": 1.3042,
      "step": 4167
    },
    {
      "epoch": 0.5859693518909039,
      "grad_norm": 1.6993334293365479,
      "learning_rate": 8.322564114463554e-05,
      "loss": 1.1781,
      "step": 4168
    },
    {
      "epoch": 0.5861099395473077,
      "grad_norm": 1.5286372900009155,
      "learning_rate": 8.299651112902476e-05,
      "loss": 1.0995,
      "step": 4169
    },
    {
      "epoch": 0.5862505272037115,
      "grad_norm": 1.6968472003936768,
      "learning_rate": 8.276747300373358e-05,
      "loss": 0.9882,
      "step": 4170
    },
    {
      "epoch": 0.5863911148601153,
      "grad_norm": 1.577603816986084,
      "learning_rate": 8.253852800653075e-05,
      "loss": 1.1235,
      "step": 4171
    },
    {
      "epoch": 0.5865317025165191,
      "grad_norm": 1.4103081226348877,
      "learning_rate": 8.230967737468168e-05,
      "loss": 1.0779,
      "step": 4172
    },
    {
      "epoch": 0.5866722901729228,
      "grad_norm": 1.3782298564910889,
      "learning_rate": 8.2080922344942e-05,
      "loss": 0.9131,
      "step": 4173
    },
    {
      "epoch": 0.5868128778293266,
      "grad_norm": 1.3994282484054565,
      "learning_rate": 8.185226415355e-05,
      "loss": 1.183,
      "step": 4174
    },
    {
      "epoch": 0.5869534654857304,
      "grad_norm": 1.3962469100952148,
      "learning_rate": 8.162370403622187e-05,
      "loss": 1.1618,
      "step": 4175
    },
    {
      "epoch": 0.5870940531421341,
      "grad_norm": 1.8983590602874756,
      "learning_rate": 8.139524322814252e-05,
      "loss": 1.2075,
      "step": 4176
    },
    {
      "epoch": 0.5872346407985379,
      "grad_norm": 1.3825422525405884,
      "learning_rate": 8.116688296396089e-05,
      "loss": 0.9856,
      "step": 4177
    },
    {
      "epoch": 0.5873752284549416,
      "grad_norm": 1.5633920431137085,
      "learning_rate": 8.093862447778242e-05,
      "loss": 0.9431,
      "step": 4178
    },
    {
      "epoch": 0.5875158161113454,
      "grad_norm": 1.4687788486480713,
      "learning_rate": 8.071046900316248e-05,
      "loss": 1.0768,
      "step": 4179
    },
    {
      "epoch": 0.5876564037677492,
      "grad_norm": 1.5420085191726685,
      "learning_rate": 8.048241777309977e-05,
      "loss": 1.0422,
      "step": 4180
    },
    {
      "epoch": 0.587796991424153,
      "grad_norm": 1.728670597076416,
      "learning_rate": 8.025447202002979e-05,
      "loss": 1.1709,
      "step": 4181
    },
    {
      "epoch": 0.5879375790805568,
      "grad_norm": 1.5436347723007202,
      "learning_rate": 8.00266329758173e-05,
      "loss": 1.1001,
      "step": 4182
    },
    {
      "epoch": 0.5880781667369605,
      "grad_norm": 1.4828145503997803,
      "learning_rate": 7.979890187175167e-05,
      "loss": 1.2927,
      "step": 4183
    },
    {
      "epoch": 0.5882187543933642,
      "grad_norm": 1.6836767196655273,
      "learning_rate": 7.957127993853768e-05,
      "loss": 1.15,
      "step": 4184
    },
    {
      "epoch": 0.588359342049768,
      "grad_norm": 1.7820947170257568,
      "learning_rate": 7.934376840629063e-05,
      "loss": 1.0307,
      "step": 4185
    },
    {
      "epoch": 0.5884999297061718,
      "grad_norm": 1.8901464939117432,
      "learning_rate": 7.911636850452975e-05,
      "loss": 1.098,
      "step": 4186
    },
    {
      "epoch": 0.5886405173625756,
      "grad_norm": 1.4528756141662598,
      "learning_rate": 7.888908146216993e-05,
      "loss": 1.2327,
      "step": 4187
    },
    {
      "epoch": 0.5887811050189793,
      "grad_norm": 1.5124706029891968,
      "learning_rate": 7.866190850751685e-05,
      "loss": 1.2133,
      "step": 4188
    },
    {
      "epoch": 0.5889216926753831,
      "grad_norm": 1.548366904258728,
      "learning_rate": 7.843485086825946e-05,
      "loss": 0.9843,
      "step": 4189
    },
    {
      "epoch": 0.5890622803317869,
      "grad_norm": 1.487217903137207,
      "learning_rate": 7.820790977146353e-05,
      "loss": 1.0401,
      "step": 4190
    },
    {
      "epoch": 0.5892028679881907,
      "grad_norm": 1.6446367502212524,
      "learning_rate": 7.798108644356517e-05,
      "loss": 1.1581,
      "step": 4191
    },
    {
      "epoch": 0.5893434556445944,
      "grad_norm": 1.5782257318496704,
      "learning_rate": 7.775438211036353e-05,
      "loss": 1.2773,
      "step": 4192
    },
    {
      "epoch": 0.5894840433009981,
      "grad_norm": 1.4850808382034302,
      "learning_rate": 7.752779799701504e-05,
      "loss": 1.2556,
      "step": 4193
    },
    {
      "epoch": 0.5896246309574019,
      "grad_norm": 1.48939847946167,
      "learning_rate": 7.730133532802695e-05,
      "loss": 1.0362,
      "step": 4194
    },
    {
      "epoch": 0.5897652186138057,
      "grad_norm": 1.4075407981872559,
      "learning_rate": 7.707499532724916e-05,
      "loss": 1.1842,
      "step": 4195
    },
    {
      "epoch": 0.5899058062702095,
      "grad_norm": 1.3069419860839844,
      "learning_rate": 7.684877921786933e-05,
      "loss": 1.0605,
      "step": 4196
    },
    {
      "epoch": 0.5900463939266133,
      "grad_norm": 1.3781691789627075,
      "learning_rate": 7.662268822240544e-05,
      "loss": 1.0563,
      "step": 4197
    },
    {
      "epoch": 0.590186981583017,
      "grad_norm": 1.4723843336105347,
      "learning_rate": 7.639672356269932e-05,
      "loss": 1.2222,
      "step": 4198
    },
    {
      "epoch": 0.5903275692394208,
      "grad_norm": 1.4538120031356812,
      "learning_rate": 7.617088645991004e-05,
      "loss": 1.2281,
      "step": 4199
    },
    {
      "epoch": 0.5904681568958245,
      "grad_norm": 1.5857374668121338,
      "learning_rate": 7.594517813450749e-05,
      "loss": 1.3078,
      "step": 4200
    },
    {
      "epoch": 0.5906087445522283,
      "grad_norm": 1.321958303451538,
      "learning_rate": 7.571959980626497e-05,
      "loss": 1.0851,
      "step": 4201
    },
    {
      "epoch": 0.5907493322086321,
      "grad_norm": 1.4515509605407715,
      "learning_rate": 7.549415269425444e-05,
      "loss": 1.2119,
      "step": 4202
    },
    {
      "epoch": 0.5908899198650358,
      "grad_norm": 1.2492576837539673,
      "learning_rate": 7.526883801683751e-05,
      "loss": 1.2103,
      "step": 4203
    },
    {
      "epoch": 0.5910305075214396,
      "grad_norm": 1.4944450855255127,
      "learning_rate": 7.504365699166061e-05,
      "loss": 1.2808,
      "step": 4204
    },
    {
      "epoch": 0.5911710951778434,
      "grad_norm": 1.5426299571990967,
      "learning_rate": 7.481861083564845e-05,
      "loss": 1.0778,
      "step": 4205
    },
    {
      "epoch": 0.5913116828342472,
      "grad_norm": 1.504692792892456,
      "learning_rate": 7.459370076499579e-05,
      "loss": 1.2755,
      "step": 4206
    },
    {
      "epoch": 0.591452270490651,
      "grad_norm": 1.4401805400848389,
      "learning_rate": 7.436892799516268e-05,
      "loss": 1.1253,
      "step": 4207
    },
    {
      "epoch": 0.5915928581470546,
      "grad_norm": 1.610812783241272,
      "learning_rate": 7.414429374086697e-05,
      "loss": 1.2199,
      "step": 4208
    },
    {
      "epoch": 0.5917334458034584,
      "grad_norm": 1.4524190425872803,
      "learning_rate": 7.391979921607794e-05,
      "loss": 1.0122,
      "step": 4209
    },
    {
      "epoch": 0.5918740334598622,
      "grad_norm": 1.5610370635986328,
      "learning_rate": 7.369544563400998e-05,
      "loss": 1.0609,
      "step": 4210
    },
    {
      "epoch": 0.592014621116266,
      "grad_norm": 1.5946533679962158,
      "learning_rate": 7.34712342071152e-05,
      "loss": 1.1239,
      "step": 4211
    },
    {
      "epoch": 0.5921552087726698,
      "grad_norm": 1.3448432683944702,
      "learning_rate": 7.324716614707782e-05,
      "loss": 1.0841,
      "step": 4212
    },
    {
      "epoch": 0.5922957964290735,
      "grad_norm": 1.3389289379119873,
      "learning_rate": 7.30232426648079e-05,
      "loss": 1.2661,
      "step": 4213
    },
    {
      "epoch": 0.5924363840854773,
      "grad_norm": 1.8335050344467163,
      "learning_rate": 7.279946497043301e-05,
      "loss": 1.2074,
      "step": 4214
    },
    {
      "epoch": 0.5925769717418811,
      "grad_norm": 1.3863948583602905,
      "learning_rate": 7.25758342732936e-05,
      "loss": 0.9329,
      "step": 4215
    },
    {
      "epoch": 0.5927175593982849,
      "grad_norm": 1.6306954622268677,
      "learning_rate": 7.235235178193556e-05,
      "loss": 1.0845,
      "step": 4216
    },
    {
      "epoch": 0.5928581470546886,
      "grad_norm": 1.680195689201355,
      "learning_rate": 7.21290187041038e-05,
      "loss": 1.0328,
      "step": 4217
    },
    {
      "epoch": 0.5929987347110923,
      "grad_norm": 1.634758710861206,
      "learning_rate": 7.190583624673597e-05,
      "loss": 1.0657,
      "step": 4218
    },
    {
      "epoch": 0.5931393223674961,
      "grad_norm": 1.4600855112075806,
      "learning_rate": 7.168280561595524e-05,
      "loss": 1.2085,
      "step": 4219
    },
    {
      "epoch": 0.5932799100238999,
      "grad_norm": 1.551268458366394,
      "learning_rate": 7.145992801706456e-05,
      "loss": 1.1168,
      "step": 4220
    },
    {
      "epoch": 0.5934204976803037,
      "grad_norm": 1.5092949867248535,
      "learning_rate": 7.123720465454046e-05,
      "loss": 1.0605,
      "step": 4221
    },
    {
      "epoch": 0.5935610853367075,
      "grad_norm": 1.298520565032959,
      "learning_rate": 7.101463673202485e-05,
      "loss": 1.2685,
      "step": 4222
    },
    {
      "epoch": 0.5937016729931112,
      "grad_norm": 1.4333882331848145,
      "learning_rate": 7.079222545232014e-05,
      "loss": 1.1297,
      "step": 4223
    },
    {
      "epoch": 0.593842260649515,
      "grad_norm": 1.3803248405456543,
      "learning_rate": 7.056997201738274e-05,
      "loss": 1.0207,
      "step": 4224
    },
    {
      "epoch": 0.5939828483059187,
      "grad_norm": 1.6230905055999756,
      "learning_rate": 7.034787762831501e-05,
      "loss": 1.2458,
      "step": 4225
    },
    {
      "epoch": 0.5941234359623225,
      "grad_norm": 1.5969089269638062,
      "learning_rate": 7.012594348536064e-05,
      "loss": 1.0949,
      "step": 4226
    },
    {
      "epoch": 0.5942640236187263,
      "grad_norm": 1.8327897787094116,
      "learning_rate": 6.990417078789645e-05,
      "loss": 1.1725,
      "step": 4227
    },
    {
      "epoch": 0.59440461127513,
      "grad_norm": 1.5417488813400269,
      "learning_rate": 6.968256073442782e-05,
      "loss": 1.0514,
      "step": 4228
    },
    {
      "epoch": 0.5945451989315338,
      "grad_norm": 1.3674736022949219,
      "learning_rate": 6.946111452258065e-05,
      "loss": 1.16,
      "step": 4229
    },
    {
      "epoch": 0.5946857865879376,
      "grad_norm": 1.4234743118286133,
      "learning_rate": 6.923983334909505e-05,
      "loss": 1.0813,
      "step": 4230
    },
    {
      "epoch": 0.5948263742443414,
      "grad_norm": 1.443596601486206,
      "learning_rate": 6.90187184098196e-05,
      "loss": 1.0994,
      "step": 4231
    },
    {
      "epoch": 0.5949669619007452,
      "grad_norm": 1.4384586811065674,
      "learning_rate": 6.879777089970499e-05,
      "loss": 1.1105,
      "step": 4232
    },
    {
      "epoch": 0.5951075495571488,
      "grad_norm": 1.5228984355926514,
      "learning_rate": 6.857699201279613e-05,
      "loss": 1.0453,
      "step": 4233
    },
    {
      "epoch": 0.5952481372135526,
      "grad_norm": 1.6803926229476929,
      "learning_rate": 6.835638294222727e-05,
      "loss": 1.1505,
      "step": 4234
    },
    {
      "epoch": 0.5953887248699564,
      "grad_norm": 1.5883737802505493,
      "learning_rate": 6.813594488021477e-05,
      "loss": 1.2517,
      "step": 4235
    },
    {
      "epoch": 0.5955293125263602,
      "grad_norm": 1.278574824333191,
      "learning_rate": 6.79156790180509e-05,
      "loss": 1.0957,
      "step": 4236
    },
    {
      "epoch": 0.595669900182764,
      "grad_norm": 1.339820146560669,
      "learning_rate": 6.76955865460974e-05,
      "loss": 1.0507,
      "step": 4237
    },
    {
      "epoch": 0.5958104878391677,
      "grad_norm": 1.5273261070251465,
      "learning_rate": 6.747566865377852e-05,
      "loss": 1.0611,
      "step": 4238
    },
    {
      "epoch": 0.5959510754955715,
      "grad_norm": 1.330202341079712,
      "learning_rate": 6.725592652957541e-05,
      "loss": 1.1236,
      "step": 4239
    },
    {
      "epoch": 0.5960916631519753,
      "grad_norm": 1.4519484043121338,
      "learning_rate": 6.703636136101976e-05,
      "loss": 1.163,
      "step": 4240
    },
    {
      "epoch": 0.596232250808379,
      "grad_norm": 1.3539050817489624,
      "learning_rate": 6.681697433468602e-05,
      "loss": 1.0255,
      "step": 4241
    },
    {
      "epoch": 0.5963728384647828,
      "grad_norm": 1.6199321746826172,
      "learning_rate": 6.659776663618654e-05,
      "loss": 1.0746,
      "step": 4242
    },
    {
      "epoch": 0.5965134261211865,
      "grad_norm": 1.912966251373291,
      "learning_rate": 6.637873945016442e-05,
      "loss": 1.1574,
      "step": 4243
    },
    {
      "epoch": 0.5966540137775903,
      "grad_norm": 1.4175331592559814,
      "learning_rate": 6.615989396028721e-05,
      "loss": 1.3179,
      "step": 4244
    },
    {
      "epoch": 0.5967946014339941,
      "grad_norm": 1.4059772491455078,
      "learning_rate": 6.59412313492407e-05,
      "loss": 1.0878,
      "step": 4245
    },
    {
      "epoch": 0.5969351890903979,
      "grad_norm": 1.4151118993759155,
      "learning_rate": 6.572275279872175e-05,
      "loss": 0.9676,
      "step": 4246
    },
    {
      "epoch": 0.5970757767468017,
      "grad_norm": 1.2971704006195068,
      "learning_rate": 6.550445948943343e-05,
      "loss": 1.2852,
      "step": 4247
    },
    {
      "epoch": 0.5972163644032054,
      "grad_norm": 1.711425542831421,
      "learning_rate": 6.52863526010773e-05,
      "loss": 1.2415,
      "step": 4248
    },
    {
      "epoch": 0.5973569520596091,
      "grad_norm": 1.4828789234161377,
      "learning_rate": 6.506843331234711e-05,
      "loss": 1.0048,
      "step": 4249
    },
    {
      "epoch": 0.5974975397160129,
      "grad_norm": 1.6645047664642334,
      "learning_rate": 6.485070280092311e-05,
      "loss": 1.1059,
      "step": 4250
    },
    {
      "epoch": 0.5976381273724167,
      "grad_norm": 1.4339182376861572,
      "learning_rate": 6.463316224346584e-05,
      "loss": 0.9151,
      "step": 4251
    },
    {
      "epoch": 0.5977787150288205,
      "grad_norm": 1.2656444311141968,
      "learning_rate": 6.441581281560836e-05,
      "loss": 1.2756,
      "step": 4252
    },
    {
      "epoch": 0.5979193026852242,
      "grad_norm": 1.4918078184127808,
      "learning_rate": 6.419865569195158e-05,
      "loss": 0.9547,
      "step": 4253
    },
    {
      "epoch": 0.598059890341628,
      "grad_norm": 1.5096545219421387,
      "learning_rate": 6.398169204605636e-05,
      "loss": 1.0977,
      "step": 4254
    },
    {
      "epoch": 0.5982004779980318,
      "grad_norm": 1.4800612926483154,
      "learning_rate": 6.376492305043898e-05,
      "loss": 0.9854,
      "step": 4255
    },
    {
      "epoch": 0.5983410656544356,
      "grad_norm": 1.5839406251907349,
      "learning_rate": 6.354834987656326e-05,
      "loss": 1.0785,
      "step": 4256
    },
    {
      "epoch": 0.5984816533108394,
      "grad_norm": 1.3532739877700806,
      "learning_rate": 6.333197369483446e-05,
      "loss": 1.1667,
      "step": 4257
    },
    {
      "epoch": 0.598622240967243,
      "grad_norm": 1.7125253677368164,
      "learning_rate": 6.311579567459356e-05,
      "loss": 1.1104,
      "step": 4258
    },
    {
      "epoch": 0.5987628286236468,
      "grad_norm": 1.6467697620391846,
      "learning_rate": 6.289981698411111e-05,
      "loss": 0.9622,
      "step": 4259
    },
    {
      "epoch": 0.5989034162800506,
      "grad_norm": 1.2817507982254028,
      "learning_rate": 6.268403879057955e-05,
      "loss": 1.1612,
      "step": 4260
    },
    {
      "epoch": 0.5990440039364544,
      "grad_norm": 1.6554582118988037,
      "learning_rate": 6.246846226010832e-05,
      "loss": 1.1409,
      "step": 4261
    },
    {
      "epoch": 0.5991845915928582,
      "grad_norm": 3.393735885620117,
      "learning_rate": 6.225308855771696e-05,
      "loss": 1.1174,
      "step": 4262
    },
    {
      "epoch": 0.5993251792492619,
      "grad_norm": 1.4198832511901855,
      "learning_rate": 6.203791884732885e-05,
      "loss": 0.9806,
      "step": 4263
    },
    {
      "epoch": 0.5994657669056657,
      "grad_norm": 1.4474982023239136,
      "learning_rate": 6.182295429176512e-05,
      "loss": 1.1521,
      "step": 4264
    },
    {
      "epoch": 0.5996063545620695,
      "grad_norm": 1.3669469356536865,
      "learning_rate": 6.160819605273757e-05,
      "loss": 1.1952,
      "step": 4265
    },
    {
      "epoch": 0.5997469422184732,
      "grad_norm": 1.734615445137024,
      "learning_rate": 6.139364529084397e-05,
      "loss": 1.2274,
      "step": 4266
    },
    {
      "epoch": 0.599887529874877,
      "grad_norm": 1.590633511543274,
      "learning_rate": 6.117930316556038e-05,
      "loss": 1.0488,
      "step": 4267
    },
    {
      "epoch": 0.6000281175312807,
      "grad_norm": 1.8271090984344482,
      "learning_rate": 6.0965170835235006e-05,
      "loss": 1.1787,
      "step": 4268
    },
    {
      "epoch": 0.6001687051876845,
      "grad_norm": 1.7865246534347534,
      "learning_rate": 6.075124945708277e-05,
      "loss": 0.9497,
      "step": 4269
    },
    {
      "epoch": 0.6003092928440883,
      "grad_norm": 1.5280213356018066,
      "learning_rate": 6.053754018717839e-05,
      "loss": 0.8985,
      "step": 4270
    },
    {
      "epoch": 0.6004498805004921,
      "grad_norm": 1.6126213073730469,
      "learning_rate": 6.0324044180450335e-05,
      "loss": 0.9155,
      "step": 4271
    },
    {
      "epoch": 0.6005904681568959,
      "grad_norm": 1.3436758518218994,
      "learning_rate": 6.0110762590674695e-05,
      "loss": 1.3437,
      "step": 4272
    },
    {
      "epoch": 0.6007310558132996,
      "grad_norm": 1.6500016450881958,
      "learning_rate": 5.9897696570468175e-05,
      "loss": 1.2177,
      "step": 4273
    },
    {
      "epoch": 0.6008716434697033,
      "grad_norm": 1.4086854457855225,
      "learning_rate": 5.9684847271283454e-05,
      "loss": 1.0449,
      "step": 4274
    },
    {
      "epoch": 0.6010122311261071,
      "grad_norm": 1.70358145236969,
      "learning_rate": 5.9472215843401524e-05,
      "loss": 1.065,
      "step": 4275
    },
    {
      "epoch": 0.6011528187825109,
      "grad_norm": 1.5281661748886108,
      "learning_rate": 5.925980343592566e-05,
      "loss": 1.0655,
      "step": 4276
    },
    {
      "epoch": 0.6012934064389147,
      "grad_norm": 1.5017807483673096,
      "learning_rate": 5.90476111967758e-05,
      "loss": 1.1654,
      "step": 4277
    },
    {
      "epoch": 0.6014339940953184,
      "grad_norm": 1.312555193901062,
      "learning_rate": 5.883564027268251e-05,
      "loss": 1.0035,
      "step": 4278
    },
    {
      "epoch": 0.6015745817517222,
      "grad_norm": 1.4056788682937622,
      "learning_rate": 5.862389180917942e-05,
      "loss": 1.1789,
      "step": 4279
    },
    {
      "epoch": 0.601715169408126,
      "grad_norm": 1.2841614484786987,
      "learning_rate": 5.841236695059855e-05,
      "loss": 1.1538,
      "step": 4280
    },
    {
      "epoch": 0.6018557570645298,
      "grad_norm": 1.3331403732299805,
      "learning_rate": 5.8201066840063324e-05,
      "loss": 1.0438,
      "step": 4281
    },
    {
      "epoch": 0.6019963447209336,
      "grad_norm": 1.2809027433395386,
      "learning_rate": 5.798999261948261e-05,
      "loss": 1.229,
      "step": 4282
    },
    {
      "epoch": 0.6021369323773372,
      "grad_norm": 1.3463876247406006,
      "learning_rate": 5.77791454295447e-05,
      "loss": 1.0916,
      "step": 4283
    },
    {
      "epoch": 0.602277520033741,
      "grad_norm": 1.333112120628357,
      "learning_rate": 5.75685264097103e-05,
      "loss": 1.0738,
      "step": 4284
    },
    {
      "epoch": 0.6024181076901448,
      "grad_norm": 1.573660135269165,
      "learning_rate": 5.7358136698207985e-05,
      "loss": 1.1672,
      "step": 4285
    },
    {
      "epoch": 0.6025586953465486,
      "grad_norm": 1.2619086503982544,
      "learning_rate": 5.7147977432026654e-05,
      "loss": 1.0527,
      "step": 4286
    },
    {
      "epoch": 0.6026992830029524,
      "grad_norm": 1.3653135299682617,
      "learning_rate": 5.6938049746909496e-05,
      "loss": 1.182,
      "step": 4287
    },
    {
      "epoch": 0.6028398706593561,
      "grad_norm": 1.4287258386611938,
      "learning_rate": 5.6728354777348656e-05,
      "loss": 1.2136,
      "step": 4288
    },
    {
      "epoch": 0.6029804583157599,
      "grad_norm": 1.5081720352172852,
      "learning_rate": 5.651889365657852e-05,
      "loss": 1.0803,
      "step": 4289
    },
    {
      "epoch": 0.6031210459721636,
      "grad_norm": 1.4072084426879883,
      "learning_rate": 5.6309667516569653e-05,
      "loss": 1.1081,
      "step": 4290
    },
    {
      "epoch": 0.6032616336285674,
      "grad_norm": 1.4997000694274902,
      "learning_rate": 5.6100677488022925e-05,
      "loss": 1.18,
      "step": 4291
    },
    {
      "epoch": 0.6034022212849712,
      "grad_norm": 1.32863450050354,
      "learning_rate": 5.589192470036258e-05,
      "loss": 1.1851,
      "step": 4292
    },
    {
      "epoch": 0.6035428089413749,
      "grad_norm": 1.9259086847305298,
      "learning_rate": 5.568341028173169e-05,
      "loss": 0.9879,
      "step": 4293
    },
    {
      "epoch": 0.6036833965977787,
      "grad_norm": 1.9060866832733154,
      "learning_rate": 5.547513535898471e-05,
      "loss": 1.2086,
      "step": 4294
    },
    {
      "epoch": 0.6038239842541825,
      "grad_norm": 1.3560576438903809,
      "learning_rate": 5.526710105768143e-05,
      "loss": 1.0508,
      "step": 4295
    },
    {
      "epoch": 0.6039645719105863,
      "grad_norm": 1.6633834838867188,
      "learning_rate": 5.5059308502081766e-05,
      "loss": 1.1788,
      "step": 4296
    },
    {
      "epoch": 0.6041051595669901,
      "grad_norm": 1.2527902126312256,
      "learning_rate": 5.485175881513907e-05,
      "loss": 1.1101,
      "step": 4297
    },
    {
      "epoch": 0.6042457472233937,
      "grad_norm": 1.5427803993225098,
      "learning_rate": 5.464445311849412e-05,
      "loss": 1.2126,
      "step": 4298
    },
    {
      "epoch": 0.6043863348797975,
      "grad_norm": 1.3695780038833618,
      "learning_rate": 5.4437392532469346e-05,
      "loss": 1.1707,
      "step": 4299
    },
    {
      "epoch": 0.6045269225362013,
      "grad_norm": 1.4725677967071533,
      "learning_rate": 5.423057817606187e-05,
      "loss": 1.0278,
      "step": 4300
    },
    {
      "epoch": 0.6046675101926051,
      "grad_norm": 1.4721102714538574,
      "learning_rate": 5.4024011166939115e-05,
      "loss": 1.2705,
      "step": 4301
    },
    {
      "epoch": 0.6048080978490089,
      "grad_norm": 1.4243910312652588,
      "learning_rate": 5.381769262143128e-05,
      "loss": 1.1247,
      "step": 4302
    },
    {
      "epoch": 0.6049486855054126,
      "grad_norm": 1.537965178489685,
      "learning_rate": 5.361162365452538e-05,
      "loss": 1.1777,
      "step": 4303
    },
    {
      "epoch": 0.6050892731618164,
      "grad_norm": 1.5104551315307617,
      "learning_rate": 5.340580537986074e-05,
      "loss": 1.1311,
      "step": 4304
    },
    {
      "epoch": 0.6052298608182202,
      "grad_norm": 1.5846277475357056,
      "learning_rate": 5.320023890972079e-05,
      "loss": 1.1517,
      "step": 4305
    },
    {
      "epoch": 0.605370448474624,
      "grad_norm": 1.3748670816421509,
      "learning_rate": 5.299492535502881e-05,
      "loss": 1.0802,
      "step": 4306
    },
    {
      "epoch": 0.6055110361310277,
      "grad_norm": 1.4671114683151245,
      "learning_rate": 5.278986582534109e-05,
      "loss": 1.0319,
      "step": 4307
    },
    {
      "epoch": 0.6056516237874314,
      "grad_norm": 1.4248442649841309,
      "learning_rate": 5.258506142884111e-05,
      "loss": 1.1191,
      "step": 4308
    },
    {
      "epoch": 0.6057922114438352,
      "grad_norm": 1.5168888568878174,
      "learning_rate": 5.2380513272333595e-05,
      "loss": 1.1237,
      "step": 4309
    },
    {
      "epoch": 0.605932799100239,
      "grad_norm": 1.2091078758239746,
      "learning_rate": 5.217622246123861e-05,
      "loss": 1.0993,
      "step": 4310
    },
    {
      "epoch": 0.6060733867566428,
      "grad_norm": 1.4522743225097656,
      "learning_rate": 5.197219009958488e-05,
      "loss": 0.9898,
      "step": 4311
    },
    {
      "epoch": 0.6062139744130466,
      "grad_norm": 1.5619193315505981,
      "learning_rate": 5.1768417290005355e-05,
      "loss": 1.0768,
      "step": 4312
    },
    {
      "epoch": 0.6063545620694503,
      "grad_norm": 1.407865047454834,
      "learning_rate": 5.156490513372981e-05,
      "loss": 1.1652,
      "step": 4313
    },
    {
      "epoch": 0.606495149725854,
      "grad_norm": 1.693955898284912,
      "learning_rate": 5.1361654730579125e-05,
      "loss": 1.1942,
      "step": 4314
    },
    {
      "epoch": 0.6066357373822578,
      "grad_norm": 1.4608654975891113,
      "learning_rate": 5.115866717896005e-05,
      "loss": 1.0096,
      "step": 4315
    },
    {
      "epoch": 0.6067763250386616,
      "grad_norm": 1.3502553701400757,
      "learning_rate": 5.095594357585869e-05,
      "loss": 1.1782,
      "step": 4316
    },
    {
      "epoch": 0.6069169126950654,
      "grad_norm": 1.8047771453857422,
      "learning_rate": 5.0753485016834726e-05,
      "loss": 1.0787,
      "step": 4317
    },
    {
      "epoch": 0.6070575003514691,
      "grad_norm": 1.4042876958847046,
      "learning_rate": 5.055129259601562e-05,
      "loss": 1.1699,
      "step": 4318
    },
    {
      "epoch": 0.6071980880078729,
      "grad_norm": 1.5897648334503174,
      "learning_rate": 5.0349367406089976e-05,
      "loss": 0.9878,
      "step": 4319
    },
    {
      "epoch": 0.6073386756642767,
      "grad_norm": 1.5452066659927368,
      "learning_rate": 5.0147710538303114e-05,
      "loss": 1.1491,
      "step": 4320
    },
    {
      "epoch": 0.6074792633206805,
      "grad_norm": 1.414280652999878,
      "learning_rate": 4.99463230824499e-05,
      "loss": 1.158,
      "step": 4321
    },
    {
      "epoch": 0.6076198509770843,
      "grad_norm": 1.6704879999160767,
      "learning_rate": 4.974520612686868e-05,
      "loss": 1.1028,
      "step": 4322
    },
    {
      "epoch": 0.6077604386334879,
      "grad_norm": 1.3957321643829346,
      "learning_rate": 4.954436075843711e-05,
      "loss": 1.2535,
      "step": 4323
    },
    {
      "epoch": 0.6079010262898917,
      "grad_norm": 1.5514394044876099,
      "learning_rate": 4.934378806256404e-05,
      "loss": 1.0344,
      "step": 4324
    },
    {
      "epoch": 0.6080416139462955,
      "grad_norm": 1.401624083518982,
      "learning_rate": 4.914348912318536e-05,
      "loss": 1.1864,
      "step": 4325
    },
    {
      "epoch": 0.6081822016026993,
      "grad_norm": 1.3931816816329956,
      "learning_rate": 4.8943465022757364e-05,
      "loss": 1.1757,
      "step": 4326
    },
    {
      "epoch": 0.6083227892591031,
      "grad_norm": 1.2539271116256714,
      "learning_rate": 4.874371684225104e-05,
      "loss": 1.2593,
      "step": 4327
    },
    {
      "epoch": 0.6084633769155068,
      "grad_norm": 1.4128599166870117,
      "learning_rate": 4.854424566114629e-05,
      "loss": 1.1155,
      "step": 4328
    },
    {
      "epoch": 0.6086039645719106,
      "grad_norm": 1.410922646522522,
      "learning_rate": 4.8345052557426177e-05,
      "loss": 1.2657,
      "step": 4329
    },
    {
      "epoch": 0.6087445522283144,
      "grad_norm": 1.2792892456054688,
      "learning_rate": 4.814613860757039e-05,
      "loss": 1.1646,
      "step": 4330
    },
    {
      "epoch": 0.6088851398847182,
      "grad_norm": 1.4834266901016235,
      "learning_rate": 4.7947504886551045e-05,
      "loss": 1.1957,
      "step": 4331
    },
    {
      "epoch": 0.6090257275411219,
      "grad_norm": 1.4080910682678223,
      "learning_rate": 4.7749152467824824e-05,
      "loss": 1.197,
      "step": 4332
    },
    {
      "epoch": 0.6091663151975256,
      "grad_norm": 1.384048581123352,
      "learning_rate": 4.755108242332876e-05,
      "loss": 1.1367,
      "step": 4333
    },
    {
      "epoch": 0.6093069028539294,
      "grad_norm": 1.4815808534622192,
      "learning_rate": 4.735329582347376e-05,
      "loss": 1.0719,
      "step": 4334
    },
    {
      "epoch": 0.6094474905103332,
      "grad_norm": 1.3955570459365845,
      "learning_rate": 4.715579373713892e-05,
      "loss": 1.2964,
      "step": 4335
    },
    {
      "epoch": 0.609588078166737,
      "grad_norm": 1.7314403057098389,
      "learning_rate": 4.6958577231665766e-05,
      "loss": 1.1602,
      "step": 4336
    },
    {
      "epoch": 0.6097286658231408,
      "grad_norm": 1.6436820030212402,
      "learning_rate": 4.676164737285263e-05,
      "loss": 0.9942,
      "step": 4337
    },
    {
      "epoch": 0.6098692534795445,
      "grad_norm": 1.6054106950759888,
      "learning_rate": 4.65650052249482e-05,
      "loss": 0.9992,
      "step": 4338
    },
    {
      "epoch": 0.6100098411359482,
      "grad_norm": 1.6610833406448364,
      "learning_rate": 4.636865185064728e-05,
      "loss": 0.9908,
      "step": 4339
    },
    {
      "epoch": 0.610150428792352,
      "grad_norm": 1.2309097051620483,
      "learning_rate": 4.6172588311083095e-05,
      "loss": 1.0814,
      "step": 4340
    },
    {
      "epoch": 0.6102910164487558,
      "grad_norm": 1.5919324159622192,
      "learning_rate": 4.597681566582298e-05,
      "loss": 1.2145,
      "step": 4341
    },
    {
      "epoch": 0.6104316041051596,
      "grad_norm": 2.1186587810516357,
      "learning_rate": 4.578133497286265e-05,
      "loss": 1.059,
      "step": 4342
    },
    {
      "epoch": 0.6105721917615633,
      "grad_norm": 1.7373801469802856,
      "learning_rate": 4.5586147288619174e-05,
      "loss": 0.9895,
      "step": 4343
    },
    {
      "epoch": 0.6107127794179671,
      "grad_norm": 1.206427812576294,
      "learning_rate": 4.5391253667926724e-05,
      "loss": 1.1094,
      "step": 4344
    },
    {
      "epoch": 0.6108533670743709,
      "grad_norm": 1.6961253881454468,
      "learning_rate": 4.5196655164030164e-05,
      "loss": 0.8964,
      "step": 4345
    },
    {
      "epoch": 0.6109939547307747,
      "grad_norm": 1.405066967010498,
      "learning_rate": 4.500235282857948e-05,
      "loss": 1.0301,
      "step": 4346
    },
    {
      "epoch": 0.6111345423871785,
      "grad_norm": 1.3108572959899902,
      "learning_rate": 4.4808347711624056e-05,
      "loss": 1.0792,
      "step": 4347
    },
    {
      "epoch": 0.6112751300435821,
      "grad_norm": 1.4207971096038818,
      "learning_rate": 4.461464086160728e-05,
      "loss": 1.0716,
      "step": 4348
    },
    {
      "epoch": 0.6114157176999859,
      "grad_norm": 1.5100897550582886,
      "learning_rate": 4.4421233325359976e-05,
      "loss": 1.1736,
      "step": 4349
    },
    {
      "epoch": 0.6115563053563897,
      "grad_norm": 1.2307566404342651,
      "learning_rate": 4.42281261480965e-05,
      "loss": 1.1635,
      "step": 4350
    },
    {
      "epoch": 0.6116968930127935,
      "grad_norm": 1.4345271587371826,
      "learning_rate": 4.403532037340696e-05,
      "loss": 0.9486,
      "step": 4351
    },
    {
      "epoch": 0.6118374806691973,
      "grad_norm": 1.508758306503296,
      "learning_rate": 4.38428170432532e-05,
      "loss": 1.0451,
      "step": 4352
    },
    {
      "epoch": 0.611978068325601,
      "grad_norm": 1.4468878507614136,
      "learning_rate": 4.3650617197962454e-05,
      "loss": 1.0542,
      "step": 4353
    },
    {
      "epoch": 0.6121186559820048,
      "grad_norm": 1.8240516185760498,
      "learning_rate": 4.345872187622187e-05,
      "loss": 0.8715,
      "step": 4354
    },
    {
      "epoch": 0.6122592436384086,
      "grad_norm": 1.5071755647659302,
      "learning_rate": 4.326713211507285e-05,
      "loss": 0.8966,
      "step": 4355
    },
    {
      "epoch": 0.6123998312948123,
      "grad_norm": 1.2899247407913208,
      "learning_rate": 4.30758489499057e-05,
      "loss": 0.9891,
      "step": 4356
    },
    {
      "epoch": 0.6125404189512161,
      "grad_norm": 1.4941260814666748,
      "learning_rate": 4.288487341445317e-05,
      "loss": 1.0794,
      "step": 4357
    },
    {
      "epoch": 0.6126810066076198,
      "grad_norm": 1.3990248441696167,
      "learning_rate": 4.269420654078658e-05,
      "loss": 1.2177,
      "step": 4358
    },
    {
      "epoch": 0.6128215942640236,
      "grad_norm": 1.5790759325027466,
      "learning_rate": 4.2503849359308115e-05,
      "loss": 1.1138,
      "step": 4359
    },
    {
      "epoch": 0.6129621819204274,
      "grad_norm": 1.5905839204788208,
      "learning_rate": 4.2313802898746816e-05,
      "loss": 1.0246,
      "step": 4360
    },
    {
      "epoch": 0.6131027695768312,
      "grad_norm": 1.678378701210022,
      "learning_rate": 4.2124068186152896e-05,
      "loss": 0.9704,
      "step": 4361
    },
    {
      "epoch": 0.613243357233235,
      "grad_norm": 1.3668862581253052,
      "learning_rate": 4.1934646246891004e-05,
      "loss": 1.0314,
      "step": 4362
    },
    {
      "epoch": 0.6133839448896387,
      "grad_norm": 1.6395385265350342,
      "learning_rate": 4.174553810463604e-05,
      "loss": 0.9342,
      "step": 4363
    },
    {
      "epoch": 0.6135245325460424,
      "grad_norm": 1.4189099073410034,
      "learning_rate": 4.1556744781366964e-05,
      "loss": 1.0732,
      "step": 4364
    },
    {
      "epoch": 0.6136651202024462,
      "grad_norm": 1.5765348672866821,
      "learning_rate": 4.136826729736136e-05,
      "loss": 1.0833,
      "step": 4365
    },
    {
      "epoch": 0.61380570785885,
      "grad_norm": 1.3064919710159302,
      "learning_rate": 4.118010667119013e-05,
      "loss": 1.0798,
      "step": 4366
    },
    {
      "epoch": 0.6139462955152538,
      "grad_norm": 1.5052297115325928,
      "learning_rate": 4.099226391971135e-05,
      "loss": 1.1089,
      "step": 4367
    },
    {
      "epoch": 0.6140868831716575,
      "grad_norm": 1.5134916305541992,
      "learning_rate": 4.080474005806553e-05,
      "loss": 1.0525,
      "step": 4368
    },
    {
      "epoch": 0.6142274708280613,
      "grad_norm": 1.6728014945983887,
      "learning_rate": 4.0617536099670286e-05,
      "loss": 1.219,
      "step": 4369
    },
    {
      "epoch": 0.6143680584844651,
      "grad_norm": 1.2799314260482788,
      "learning_rate": 4.043065305621353e-05,
      "loss": 1.1585,
      "step": 4370
    },
    {
      "epoch": 0.6145086461408689,
      "grad_norm": 1.5163085460662842,
      "learning_rate": 4.024409193764945e-05,
      "loss": 1.0563,
      "step": 4371
    },
    {
      "epoch": 0.6146492337972727,
      "grad_norm": 1.4219701290130615,
      "learning_rate": 4.005785375219239e-05,
      "loss": 1.3444,
      "step": 4372
    },
    {
      "epoch": 0.6147898214536763,
      "grad_norm": 1.4664735794067383,
      "learning_rate": 3.987193950631144e-05,
      "loss": 0.8697,
      "step": 4373
    },
    {
      "epoch": 0.6149304091100801,
      "grad_norm": 1.428406834602356,
      "learning_rate": 3.968635020472522e-05,
      "loss": 1.1891,
      "step": 4374
    },
    {
      "epoch": 0.6150709967664839,
      "grad_norm": 1.193264126777649,
      "learning_rate": 3.950108685039586e-05,
      "loss": 1.0278,
      "step": 4375
    },
    {
      "epoch": 0.6152115844228877,
      "grad_norm": 1.486804723739624,
      "learning_rate": 3.9316150444524305e-05,
      "loss": 0.9753,
      "step": 4376
    },
    {
      "epoch": 0.6153521720792915,
      "grad_norm": 1.509149193763733,
      "learning_rate": 3.9131541986544995e-05,
      "loss": 1.0236,
      "step": 4377
    },
    {
      "epoch": 0.6154927597356952,
      "grad_norm": 1.5526857376098633,
      "learning_rate": 3.8947262474119254e-05,
      "loss": 1.0404,
      "step": 4378
    },
    {
      "epoch": 0.615633347392099,
      "grad_norm": 1.4888659715652466,
      "learning_rate": 3.876331290313112e-05,
      "loss": 1.2512,
      "step": 4379
    },
    {
      "epoch": 0.6157739350485028,
      "grad_norm": 1.4944264888763428,
      "learning_rate": 3.8579694267681984e-05,
      "loss": 0.9484,
      "step": 4380
    },
    {
      "epoch": 0.6159145227049065,
      "grad_norm": 1.5568236112594604,
      "learning_rate": 3.839640756008397e-05,
      "loss": 0.9685,
      "step": 4381
    },
    {
      "epoch": 0.6160551103613103,
      "grad_norm": 1.7347071170806885,
      "learning_rate": 3.821345377085608e-05,
      "loss": 1.1157,
      "step": 4382
    },
    {
      "epoch": 0.616195698017714,
      "grad_norm": 1.4222285747528076,
      "learning_rate": 3.8030833888717463e-05,
      "loss": 1.1036,
      "step": 4383
    },
    {
      "epoch": 0.6163362856741178,
      "grad_norm": 1.3943883180618286,
      "learning_rate": 3.78485489005836e-05,
      "loss": 1.1418,
      "step": 4384
    },
    {
      "epoch": 0.6164768733305216,
      "grad_norm": 1.461656093597412,
      "learning_rate": 3.766659979155972e-05,
      "loss": 1.074,
      "step": 4385
    },
    {
      "epoch": 0.6166174609869254,
      "grad_norm": 1.9054063558578491,
      "learning_rate": 3.748498754493563e-05,
      "loss": 1.0386,
      "step": 4386
    },
    {
      "epoch": 0.6167580486433292,
      "grad_norm": 1.4297500848770142,
      "learning_rate": 3.7303713142180906e-05,
      "loss": 1.1283,
      "step": 4387
    },
    {
      "epoch": 0.6168986362997328,
      "grad_norm": 1.3975093364715576,
      "learning_rate": 3.7122777562939805e-05,
      "loss": 1.0754,
      "step": 4388
    },
    {
      "epoch": 0.6170392239561366,
      "grad_norm": 1.1918034553527832,
      "learning_rate": 3.6942181785024655e-05,
      "loss": 1.0718,
      "step": 4389
    },
    {
      "epoch": 0.6171798116125404,
      "grad_norm": 1.3748321533203125,
      "learning_rate": 3.676192678441198e-05,
      "loss": 1.1667,
      "step": 4390
    },
    {
      "epoch": 0.6173203992689442,
      "grad_norm": 1.579368233680725,
      "learning_rate": 3.658201353523656e-05,
      "loss": 1.2213,
      "step": 4391
    },
    {
      "epoch": 0.617460986925348,
      "grad_norm": 1.3240174055099487,
      "learning_rate": 3.6402443009786246e-05,
      "loss": 1.0808,
      "step": 4392
    },
    {
      "epoch": 0.6176015745817517,
      "grad_norm": 1.5125865936279297,
      "learning_rate": 3.62232161784969e-05,
      "loss": 1.0488,
      "step": 4393
    },
    {
      "epoch": 0.6177421622381555,
      "grad_norm": 1.395004153251648,
      "learning_rate": 3.604433400994659e-05,
      "loss": 1.1963,
      "step": 4394
    },
    {
      "epoch": 0.6178827498945593,
      "grad_norm": 1.421115756034851,
      "learning_rate": 3.5865797470851e-05,
      "loss": 1.2428,
      "step": 4395
    },
    {
      "epoch": 0.6180233375509631,
      "grad_norm": 1.8728317022323608,
      "learning_rate": 3.568760752605841e-05,
      "loss": 1.0845,
      "step": 4396
    },
    {
      "epoch": 0.6181639252073667,
      "grad_norm": 1.3712613582611084,
      "learning_rate": 3.550976513854316e-05,
      "loss": 1.1193,
      "step": 4397
    },
    {
      "epoch": 0.6183045128637705,
      "grad_norm": 1.6290863752365112,
      "learning_rate": 3.5332271269401785e-05,
      "loss": 1.1232,
      "step": 4398
    },
    {
      "epoch": 0.6184451005201743,
      "grad_norm": 1.431323766708374,
      "learning_rate": 3.5155126877847724e-05,
      "loss": 1.0867,
      "step": 4399
    },
    {
      "epoch": 0.6185856881765781,
      "grad_norm": 1.7410309314727783,
      "learning_rate": 3.4978332921204984e-05,
      "loss": 1.0688,
      "step": 4400
    },
    {
      "epoch": 0.6187262758329819,
      "grad_norm": 1.4703388214111328,
      "learning_rate": 3.48018903549044e-05,
      "loss": 1.1206,
      "step": 4401
    },
    {
      "epoch": 0.6188668634893856,
      "grad_norm": 1.3409868478775024,
      "learning_rate": 3.462580013247716e-05,
      "loss": 1.1791,
      "step": 4402
    },
    {
      "epoch": 0.6190074511457894,
      "grad_norm": 1.4749587774276733,
      "learning_rate": 3.445006320555113e-05,
      "loss": 1.018,
      "step": 4403
    },
    {
      "epoch": 0.6191480388021932,
      "grad_norm": 1.4152321815490723,
      "learning_rate": 3.427468052384448e-05,
      "loss": 1.1396,
      "step": 4404
    },
    {
      "epoch": 0.619288626458597,
      "grad_norm": 1.3408496379852295,
      "learning_rate": 3.40996530351607e-05,
      "loss": 1.0889,
      "step": 4405
    },
    {
      "epoch": 0.6194292141150007,
      "grad_norm": 1.318666934967041,
      "learning_rate": 3.392498168538401e-05,
      "loss": 1.0756,
      "step": 4406
    },
    {
      "epoch": 0.6195698017714044,
      "grad_norm": 1.5575569868087769,
      "learning_rate": 3.37506674184744e-05,
      "loss": 1.0809,
      "step": 4407
    },
    {
      "epoch": 0.6197103894278082,
      "grad_norm": 1.5042906999588013,
      "learning_rate": 3.357671117646129e-05,
      "loss": 1.1034,
      "step": 4408
    },
    {
      "epoch": 0.619850977084212,
      "grad_norm": 1.5212104320526123,
      "learning_rate": 3.3403113899439934e-05,
      "loss": 1.1415,
      "step": 4409
    },
    {
      "epoch": 0.6199915647406158,
      "grad_norm": 1.6616339683532715,
      "learning_rate": 3.3229876525565004e-05,
      "loss": 1.1151,
      "step": 4410
    },
    {
      "epoch": 0.6201321523970196,
      "grad_norm": 1.4756731986999512,
      "learning_rate": 3.305699999104697e-05,
      "loss": 1.08,
      "step": 4411
    },
    {
      "epoch": 0.6202727400534233,
      "grad_norm": 1.259914755821228,
      "learning_rate": 3.2884485230145864e-05,
      "loss": 1.0971,
      "step": 4412
    },
    {
      "epoch": 0.620413327709827,
      "grad_norm": 1.5841761827468872,
      "learning_rate": 3.2712333175166365e-05,
      "loss": 1.0999,
      "step": 4413
    },
    {
      "epoch": 0.6205539153662308,
      "grad_norm": 1.6461697816848755,
      "learning_rate": 3.254054475645326e-05,
      "loss": 1.2192,
      "step": 4414
    },
    {
      "epoch": 0.6206945030226346,
      "grad_norm": 1.424811601638794,
      "learning_rate": 3.236912090238656e-05,
      "loss": 1.1659,
      "step": 4415
    },
    {
      "epoch": 0.6208350906790384,
      "grad_norm": 1.339572548866272,
      "learning_rate": 3.219806253937534e-05,
      "loss": 1.2613,
      "step": 4416
    },
    {
      "epoch": 0.6209756783354421,
      "grad_norm": 1.477257490158081,
      "learning_rate": 3.202737059185398e-05,
      "loss": 0.974,
      "step": 4417
    },
    {
      "epoch": 0.6211162659918459,
      "grad_norm": 1.2759066820144653,
      "learning_rate": 3.1857045982276556e-05,
      "loss": 1.0652,
      "step": 4418
    },
    {
      "epoch": 0.6212568536482497,
      "grad_norm": 1.7240900993347168,
      "learning_rate": 3.168708963111198e-05,
      "loss": 1.079,
      "step": 4419
    },
    {
      "epoch": 0.6213974413046535,
      "grad_norm": 1.2336928844451904,
      "learning_rate": 3.1517502456839146e-05,
      "loss": 1.1758,
      "step": 4420
    },
    {
      "epoch": 0.6215380289610573,
      "grad_norm": 1.5001429319381714,
      "learning_rate": 3.13482853759413e-05,
      "loss": 1.1288,
      "step": 4421
    },
    {
      "epoch": 0.6216786166174609,
      "grad_norm": 1.4633482694625854,
      "learning_rate": 3.117943930290246e-05,
      "loss": 1.1878,
      "step": 4422
    },
    {
      "epoch": 0.6218192042738647,
      "grad_norm": 1.8321245908737183,
      "learning_rate": 3.101096515020119e-05,
      "loss": 1.0304,
      "step": 4423
    },
    {
      "epoch": 0.6219597919302685,
      "grad_norm": 1.5590680837631226,
      "learning_rate": 3.084286382830591e-05,
      "loss": 1.0027,
      "step": 4424
    },
    {
      "epoch": 0.6221003795866723,
      "grad_norm": 1.5889335870742798,
      "learning_rate": 3.067513624567042e-05,
      "loss": 1.1256,
      "step": 4425
    },
    {
      "epoch": 0.6222409672430761,
      "grad_norm": 1.6196380853652954,
      "learning_rate": 3.0507783308729167e-05,
      "loss": 1.1894,
      "step": 4426
    },
    {
      "epoch": 0.6223815548994798,
      "grad_norm": 1.3884514570236206,
      "learning_rate": 3.0340805921891113e-05,
      "loss": 1.0617,
      "step": 4427
    },
    {
      "epoch": 0.6225221425558836,
      "grad_norm": 1.5002044439315796,
      "learning_rate": 3.017420498753638e-05,
      "loss": 1.0097,
      "step": 4428
    },
    {
      "epoch": 0.6226627302122874,
      "grad_norm": 1.5755399465560913,
      "learning_rate": 3.0007981406009976e-05,
      "loss": 1.0326,
      "step": 4429
    },
    {
      "epoch": 0.6228033178686911,
      "grad_norm": 1.6131588220596313,
      "learning_rate": 2.984213607561841e-05,
      "loss": 0.9766,
      "step": 4430
    },
    {
      "epoch": 0.6229439055250949,
      "grad_norm": 1.8061624765396118,
      "learning_rate": 2.9676669892623634e-05,
      "loss": 1.1878,
      "step": 4431
    },
    {
      "epoch": 0.6230844931814986,
      "grad_norm": 1.4408483505249023,
      "learning_rate": 2.951158375123838e-05,
      "loss": 0.9999,
      "step": 4432
    },
    {
      "epoch": 0.6232250808379024,
      "grad_norm": 1.4647165536880493,
      "learning_rate": 2.9346878543621814e-05,
      "loss": 1.0783,
      "step": 4433
    },
    {
      "epoch": 0.6233656684943062,
      "grad_norm": 1.4452180862426758,
      "learning_rate": 2.9182555159874813e-05,
      "loss": 1.0876,
      "step": 4434
    },
    {
      "epoch": 0.62350625615071,
      "grad_norm": 1.4254707098007202,
      "learning_rate": 2.901861448803408e-05,
      "loss": 1.1282,
      "step": 4435
    },
    {
      "epoch": 0.6236468438071138,
      "grad_norm": 1.4154839515686035,
      "learning_rate": 2.885505741406852e-05,
      "loss": 1.0489,
      "step": 4436
    },
    {
      "epoch": 0.6237874314635174,
      "grad_norm": 1.6588928699493408,
      "learning_rate": 2.8691884821873926e-05,
      "loss": 1.0371,
      "step": 4437
    },
    {
      "epoch": 0.6239280191199212,
      "grad_norm": 1.3315945863723755,
      "learning_rate": 2.8529097593268218e-05,
      "loss": 1.1163,
      "step": 4438
    },
    {
      "epoch": 0.624068606776325,
      "grad_norm": 1.4353880882263184,
      "learning_rate": 2.836669660798691e-05,
      "loss": 1.127,
      "step": 4439
    },
    {
      "epoch": 0.6242091944327288,
      "grad_norm": 1.4644019603729248,
      "learning_rate": 2.8204682743677623e-05,
      "loss": 1.074,
      "step": 4440
    },
    {
      "epoch": 0.6243497820891326,
      "grad_norm": 1.5801353454589844,
      "learning_rate": 2.8043056875896712e-05,
      "loss": 1.0783,
      "step": 4441
    },
    {
      "epoch": 0.6244903697455363,
      "grad_norm": 1.2935967445373535,
      "learning_rate": 2.7881819878103288e-05,
      "loss": 1.1393,
      "step": 4442
    },
    {
      "epoch": 0.6246309574019401,
      "grad_norm": 1.3892353773117065,
      "learning_rate": 2.7720972621654696e-05,
      "loss": 1.134,
      "step": 4443
    },
    {
      "epoch": 0.6247715450583439,
      "grad_norm": 1.6180310249328613,
      "learning_rate": 2.7560515975802436e-05,
      "loss": 1.2264,
      "step": 4444
    },
    {
      "epoch": 0.6249121327147477,
      "grad_norm": 1.6356141567230225,
      "learning_rate": 2.7400450807686928e-05,
      "loss": 1.1083,
      "step": 4445
    },
    {
      "epoch": 0.6250527203711514,
      "grad_norm": 1.3472166061401367,
      "learning_rate": 2.7240777982332954e-05,
      "loss": 1.0145,
      "step": 4446
    },
    {
      "epoch": 0.6251933080275551,
      "grad_norm": 1.5314542055130005,
      "learning_rate": 2.708149836264514e-05,
      "loss": 1.1817,
      "step": 4447
    },
    {
      "epoch": 0.6253338956839589,
      "grad_norm": 1.6723191738128662,
      "learning_rate": 2.6922612809402646e-05,
      "loss": 1.0435,
      "step": 4448
    },
    {
      "epoch": 0.6254744833403627,
      "grad_norm": 1.5202618837356567,
      "learning_rate": 2.6764122181255747e-05,
      "loss": 1.1042,
      "step": 4449
    },
    {
      "epoch": 0.6256150709967665,
      "grad_norm": 1.5254162549972534,
      "learning_rate": 2.660602733472011e-05,
      "loss": 1.0888,
      "step": 4450
    },
    {
      "epoch": 0.6257556586531703,
      "grad_norm": 1.1581802368164062,
      "learning_rate": 2.6448329124172267e-05,
      "loss": 1.11,
      "step": 4451
    },
    {
      "epoch": 0.625896246309574,
      "grad_norm": 1.7343603372573853,
      "learning_rate": 2.6291028401845596e-05,
      "loss": 0.9883,
      "step": 4452
    },
    {
      "epoch": 0.6260368339659778,
      "grad_norm": 1.4901602268218994,
      "learning_rate": 2.613412601782529e-05,
      "loss": 1.0347,
      "step": 4453
    },
    {
      "epoch": 0.6261774216223815,
      "grad_norm": 1.299425721168518,
      "learning_rate": 2.597762282004379e-05,
      "loss": 1.1432,
      "step": 4454
    },
    {
      "epoch": 0.6263180092787853,
      "grad_norm": 1.2473987340927124,
      "learning_rate": 2.5821519654276406e-05,
      "loss": 1.0536,
      "step": 4455
    },
    {
      "epoch": 0.6264585969351891,
      "grad_norm": 1.4938387870788574,
      "learning_rate": 2.5665817364136114e-05,
      "loss": 1.1717,
      "step": 4456
    },
    {
      "epoch": 0.6265991845915928,
      "grad_norm": 1.4258654117584229,
      "learning_rate": 2.551051679107016e-05,
      "loss": 1.0852,
      "step": 4457
    },
    {
      "epoch": 0.6267397722479966,
      "grad_norm": 1.5060728788375854,
      "learning_rate": 2.5355618774354516e-05,
      "loss": 1.1235,
      "step": 4458
    },
    {
      "epoch": 0.6268803599044004,
      "grad_norm": 1.405777931213379,
      "learning_rate": 2.5201124151089272e-05,
      "loss": 1.1388,
      "step": 4459
    },
    {
      "epoch": 0.6270209475608042,
      "grad_norm": 1.2915644645690918,
      "learning_rate": 2.5047033756195338e-05,
      "loss": 1.0087,
      "step": 4460
    },
    {
      "epoch": 0.627161535217208,
      "grad_norm": 1.4761492013931274,
      "learning_rate": 2.489334842240826e-05,
      "loss": 0.9806,
      "step": 4461
    },
    {
      "epoch": 0.6273021228736116,
      "grad_norm": 1.554646611213684,
      "learning_rate": 2.474006898027502e-05,
      "loss": 1.1853,
      "step": 4462
    },
    {
      "epoch": 0.6274427105300154,
      "grad_norm": 1.4308911561965942,
      "learning_rate": 2.458719625814897e-05,
      "loss": 1.11,
      "step": 4463
    },
    {
      "epoch": 0.6275832981864192,
      "grad_norm": 1.541159987449646,
      "learning_rate": 2.443473108218547e-05,
      "loss": 1.0438,
      "step": 4464
    },
    {
      "epoch": 0.627723885842823,
      "grad_norm": 1.5582081079483032,
      "learning_rate": 2.428267427633739e-05,
      "loss": 1.1809,
      "step": 4465
    },
    {
      "epoch": 0.6278644734992268,
      "grad_norm": 1.399914026260376,
      "learning_rate": 2.413102666235083e-05,
      "loss": 1.0133,
      "step": 4466
    },
    {
      "epoch": 0.6280050611556305,
      "grad_norm": 1.46014404296875,
      "learning_rate": 2.3979789059760062e-05,
      "loss": 0.9893,
      "step": 4467
    },
    {
      "epoch": 0.6281456488120343,
      "grad_norm": 1.634110450744629,
      "learning_rate": 2.382896228588425e-05,
      "loss": 1.2178,
      "step": 4468
    },
    {
      "epoch": 0.6282862364684381,
      "grad_norm": 1.2450031042099,
      "learning_rate": 2.367854715582203e-05,
      "loss": 1.2112,
      "step": 4469
    },
    {
      "epoch": 0.6284268241248419,
      "grad_norm": 1.5941013097763062,
      "learning_rate": 2.352854448244719e-05,
      "loss": 1.1152,
      "step": 4470
    },
    {
      "epoch": 0.6285674117812456,
      "grad_norm": 1.389519453048706,
      "learning_rate": 2.3378955076404863e-05,
      "loss": 1.1643,
      "step": 4471
    },
    {
      "epoch": 0.6287079994376493,
      "grad_norm": 1.4938688278198242,
      "learning_rate": 2.3229779746106693e-05,
      "loss": 1.1388,
      "step": 4472
    },
    {
      "epoch": 0.6288485870940531,
      "grad_norm": 1.5804443359375,
      "learning_rate": 2.3081019297726582e-05,
      "loss": 1.0678,
      "step": 4473
    },
    {
      "epoch": 0.6289891747504569,
      "grad_norm": 1.3976081609725952,
      "learning_rate": 2.293267453519642e-05,
      "loss": 1.0578,
      "step": 4474
    },
    {
      "epoch": 0.6291297624068607,
      "grad_norm": 1.397852897644043,
      "learning_rate": 2.2784746260201184e-05,
      "loss": 0.9588,
      "step": 4475
    },
    {
      "epoch": 0.6292703500632645,
      "grad_norm": 1.3752132654190063,
      "learning_rate": 2.2637235272175796e-05,
      "loss": 1.1867,
      "step": 4476
    },
    {
      "epoch": 0.6294109377196682,
      "grad_norm": 1.6590983867645264,
      "learning_rate": 2.249014236829967e-05,
      "loss": 1.1895,
      "step": 4477
    },
    {
      "epoch": 0.629551525376072,
      "grad_norm": 1.4390580654144287,
      "learning_rate": 2.2343468343492524e-05,
      "loss": 1.0843,
      "step": 4478
    },
    {
      "epoch": 0.6296921130324757,
      "grad_norm": 1.3901548385620117,
      "learning_rate": 2.2197213990411116e-05,
      "loss": 1.0618,
      "step": 4479
    },
    {
      "epoch": 0.6298327006888795,
      "grad_norm": 1.5120019912719727,
      "learning_rate": 2.2051380099443452e-05,
      "loss": 1.0617,
      "step": 4480
    },
    {
      "epoch": 0.6299732883452833,
      "grad_norm": 1.3477047681808472,
      "learning_rate": 2.1905967458705722e-05,
      "loss": 1.1747,
      "step": 4481
    },
    {
      "epoch": 0.630113876001687,
      "grad_norm": 1.4019070863723755,
      "learning_rate": 2.1760976854037474e-05,
      "loss": 1.0612,
      "step": 4482
    },
    {
      "epoch": 0.6302544636580908,
      "grad_norm": 1.5615360736846924,
      "learning_rate": 2.161640906899749e-05,
      "loss": 1.255,
      "step": 4483
    },
    {
      "epoch": 0.6303950513144946,
      "grad_norm": 1.4424104690551758,
      "learning_rate": 2.1472264884859527e-05,
      "loss": 1.0222,
      "step": 4484
    },
    {
      "epoch": 0.6305356389708984,
      "grad_norm": 1.6282355785369873,
      "learning_rate": 2.1328545080608264e-05,
      "loss": 1.0052,
      "step": 4485
    },
    {
      "epoch": 0.6306762266273022,
      "grad_norm": 1.7765384912490845,
      "learning_rate": 2.11852504329345e-05,
      "loss": 1.0043,
      "step": 4486
    },
    {
      "epoch": 0.6308168142837058,
      "grad_norm": 1.6934775114059448,
      "learning_rate": 2.104238171623213e-05,
      "loss": 1.033,
      "step": 4487
    },
    {
      "epoch": 0.6309574019401096,
      "grad_norm": 1.3477107286453247,
      "learning_rate": 2.0899939702592498e-05,
      "loss": 0.9866,
      "step": 4488
    },
    {
      "epoch": 0.6310979895965134,
      "grad_norm": 1.3790810108184814,
      "learning_rate": 2.0757925161801394e-05,
      "loss": 1.0059,
      "step": 4489
    },
    {
      "epoch": 0.6312385772529172,
      "grad_norm": 1.6331619024276733,
      "learning_rate": 2.061633886133435e-05,
      "loss": 1.2265,
      "step": 4490
    },
    {
      "epoch": 0.631379164909321,
      "grad_norm": 1.5564173460006714,
      "learning_rate": 2.0475181566352586e-05,
      "loss": 1.0317,
      "step": 4491
    },
    {
      "epoch": 0.6315197525657247,
      "grad_norm": 1.4913239479064941,
      "learning_rate": 2.033445403969889e-05,
      "loss": 0.971,
      "step": 4492
    },
    {
      "epoch": 0.6316603402221285,
      "grad_norm": 1.848768949508667,
      "learning_rate": 2.0194157041893612e-05,
      "loss": 0.9984,
      "step": 4493
    },
    {
      "epoch": 0.6318009278785323,
      "grad_norm": 1.6827806234359741,
      "learning_rate": 2.0054291331130005e-05,
      "loss": 1.1791,
      "step": 4494
    },
    {
      "epoch": 0.631941515534936,
      "grad_norm": 1.4217039346694946,
      "learning_rate": 1.99148576632713e-05,
      "loss": 0.9814,
      "step": 4495
    },
    {
      "epoch": 0.6320821031913398,
      "grad_norm": 1.7841097116470337,
      "learning_rate": 1.977585679184515e-05,
      "loss": 1.0978,
      "step": 4496
    },
    {
      "epoch": 0.6322226908477435,
      "grad_norm": 1.4245531558990479,
      "learning_rate": 1.9637289468040577e-05,
      "loss": 1.2075,
      "step": 4497
    },
    {
      "epoch": 0.6323632785041473,
      "grad_norm": 1.5407109260559082,
      "learning_rate": 1.949915644070398e-05,
      "loss": 1.0965,
      "step": 4498
    },
    {
      "epoch": 0.6325038661605511,
      "grad_norm": 1.3557565212249756,
      "learning_rate": 1.936145845633407e-05,
      "loss": 1.1531,
      "step": 4499
    },
    {
      "epoch": 0.6326444538169549,
      "grad_norm": 1.6637272834777832,
      "learning_rate": 1.922419625907892e-05,
      "loss": 0.9441,
      "step": 4500
    },
    {
      "epoch": 0.6326444538169549,
      "eval_loss": 1.144382119178772,
      "eval_runtime": 771.578,
      "eval_samples_per_second": 16.39,
      "eval_steps_per_second": 8.195,
      "step": 4500
    },
    {
      "epoch": 0.6327850414733587,
      "grad_norm": 1.7313671112060547,
      "learning_rate": 1.9087370590731424e-05,
      "loss": 1.1638,
      "step": 4501
    },
    {
      "epoch": 0.6329256291297624,
      "grad_norm": 1.792948842048645,
      "learning_rate": 1.89509821907254e-05,
      "loss": 1.0081,
      "step": 4502
    },
    {
      "epoch": 0.6330662167861661,
      "grad_norm": 1.8421803712844849,
      "learning_rate": 1.8815031796131532e-05,
      "loss": 1.1638,
      "step": 4503
    },
    {
      "epoch": 0.6332068044425699,
      "grad_norm": 1.4263116121292114,
      "learning_rate": 1.8679520141653583e-05,
      "loss": 1.1547,
      "step": 4504
    },
    {
      "epoch": 0.6333473920989737,
      "grad_norm": 1.7271922826766968,
      "learning_rate": 1.8544447959623846e-05,
      "loss": 1.0875,
      "step": 4505
    },
    {
      "epoch": 0.6334879797553775,
      "grad_norm": 1.4782007932662964,
      "learning_rate": 1.8409815980000324e-05,
      "loss": 1.2143,
      "step": 4506
    },
    {
      "epoch": 0.6336285674117812,
      "grad_norm": 1.273679494857788,
      "learning_rate": 1.827562493036139e-05,
      "loss": 1.0547,
      "step": 4507
    },
    {
      "epoch": 0.633769155068185,
      "grad_norm": 1.466796636581421,
      "learning_rate": 1.8141875535902907e-05,
      "loss": 0.9773,
      "step": 4508
    },
    {
      "epoch": 0.6339097427245888,
      "grad_norm": 1.3641548156738281,
      "learning_rate": 1.8008568519433876e-05,
      "loss": 1.3103,
      "step": 4509
    },
    {
      "epoch": 0.6340503303809926,
      "grad_norm": 1.385964274406433,
      "learning_rate": 1.787570460137259e-05,
      "loss": 0.8734,
      "step": 4510
    },
    {
      "epoch": 0.6341909180373964,
      "grad_norm": 1.3315889835357666,
      "learning_rate": 1.774328449974273e-05,
      "loss": 1.1553,
      "step": 4511
    },
    {
      "epoch": 0.6343315056938,
      "grad_norm": 1.5484455823898315,
      "learning_rate": 1.7611308930169625e-05,
      "loss": 1.0502,
      "step": 4512
    },
    {
      "epoch": 0.6344720933502038,
      "grad_norm": 1.416001319885254,
      "learning_rate": 1.747977860587585e-05,
      "loss": 1.0369,
      "step": 4513
    },
    {
      "epoch": 0.6346126810066076,
      "grad_norm": 1.665939450263977,
      "learning_rate": 1.7348694237678498e-05,
      "loss": 1.0582,
      "step": 4514
    },
    {
      "epoch": 0.6347532686630114,
      "grad_norm": 1.1654421091079712,
      "learning_rate": 1.7218056533983895e-05,
      "loss": 1.1158,
      "step": 4515
    },
    {
      "epoch": 0.6348938563194152,
      "grad_norm": 1.3930370807647705,
      "learning_rate": 1.708786620078483e-05,
      "loss": 1.1427,
      "step": 4516
    },
    {
      "epoch": 0.6350344439758189,
      "grad_norm": 1.6332199573516846,
      "learning_rate": 1.695812394165669e-05,
      "loss": 1.1899,
      "step": 4517
    },
    {
      "epoch": 0.6351750316322227,
      "grad_norm": 1.2831356525421143,
      "learning_rate": 1.682883045775281e-05,
      "loss": 1.1198,
      "step": 4518
    },
    {
      "epoch": 0.6353156192886265,
      "grad_norm": 1.3898485898971558,
      "learning_rate": 1.6699986447801607e-05,
      "loss": 1.1126,
      "step": 4519
    },
    {
      "epoch": 0.6354562069450302,
      "grad_norm": 1.5956634283065796,
      "learning_rate": 1.6571592608102382e-05,
      "loss": 1.1473,
      "step": 4520
    },
    {
      "epoch": 0.635596794601434,
      "grad_norm": 1.5226705074310303,
      "learning_rate": 1.6443649632521606e-05,
      "loss": 0.8555,
      "step": 4521
    },
    {
      "epoch": 0.6357373822578377,
      "grad_norm": 1.470116138458252,
      "learning_rate": 1.6316158212489318e-05,
      "loss": 1.1396,
      "step": 4522
    },
    {
      "epoch": 0.6358779699142415,
      "grad_norm": 1.3038480281829834,
      "learning_rate": 1.6189119036994925e-05,
      "loss": 1.0424,
      "step": 4523
    },
    {
      "epoch": 0.6360185575706453,
      "grad_norm": 1.4289804697036743,
      "learning_rate": 1.6062532792584018e-05,
      "loss": 1.1232,
      "step": 4524
    },
    {
      "epoch": 0.6361591452270491,
      "grad_norm": 1.5107018947601318,
      "learning_rate": 1.5936400163354782e-05,
      "loss": 0.9122,
      "step": 4525
    },
    {
      "epoch": 0.6362997328834529,
      "grad_norm": 1.58344304561615,
      "learning_rate": 1.5810721830953333e-05,
      "loss": 1.1532,
      "step": 4526
    },
    {
      "epoch": 0.6364403205398566,
      "grad_norm": 1.6025232076644897,
      "learning_rate": 1.5685498474571116e-05,
      "loss": 1.104,
      "step": 4527
    },
    {
      "epoch": 0.6365809081962603,
      "grad_norm": 1.7390124797821045,
      "learning_rate": 1.5560730770940658e-05,
      "loss": 0.9467,
      "step": 4528
    },
    {
      "epoch": 0.6367214958526641,
      "grad_norm": 1.6282975673675537,
      "learning_rate": 1.5436419394332057e-05,
      "loss": 1.1049,
      "step": 4529
    },
    {
      "epoch": 0.6368620835090679,
      "grad_norm": 1.5621891021728516,
      "learning_rate": 1.5312565016549452e-05,
      "loss": 1.0868,
      "step": 4530
    },
    {
      "epoch": 0.6370026711654717,
      "grad_norm": 1.3707304000854492,
      "learning_rate": 1.5189168306926926e-05,
      "loss": 1.239,
      "step": 4531
    },
    {
      "epoch": 0.6371432588218754,
      "grad_norm": 1.301941990852356,
      "learning_rate": 1.506622993232546e-05,
      "loss": 1.1568,
      "step": 4532
    },
    {
      "epoch": 0.6372838464782792,
      "grad_norm": 1.4280186891555786,
      "learning_rate": 1.4943750557129378e-05,
      "loss": 1.1396,
      "step": 4533
    },
    {
      "epoch": 0.637424434134683,
      "grad_norm": 1.5440527200698853,
      "learning_rate": 1.4821730843241898e-05,
      "loss": 1.04,
      "step": 4534
    },
    {
      "epoch": 0.6375650217910868,
      "grad_norm": 1.3729318380355835,
      "learning_rate": 1.4700171450082433e-05,
      "loss": 1.1302,
      "step": 4535
    },
    {
      "epoch": 0.6377056094474906,
      "grad_norm": 1.4124313592910767,
      "learning_rate": 1.457907303458298e-05,
      "loss": 1.0684,
      "step": 4536
    },
    {
      "epoch": 0.6378461971038942,
      "grad_norm": 1.416968584060669,
      "learning_rate": 1.4458436251183804e-05,
      "loss": 1.1076,
      "step": 4537
    },
    {
      "epoch": 0.637986784760298,
      "grad_norm": 1.7226933240890503,
      "learning_rate": 1.433826175183085e-05,
      "loss": 1.0278,
      "step": 4538
    },
    {
      "epoch": 0.6381273724167018,
      "grad_norm": 1.4787588119506836,
      "learning_rate": 1.4218550185971325e-05,
      "loss": 1.2679,
      "step": 4539
    },
    {
      "epoch": 0.6382679600731056,
      "grad_norm": 1.3610107898712158,
      "learning_rate": 1.4099302200551213e-05,
      "loss": 0.9747,
      "step": 4540
    },
    {
      "epoch": 0.6384085477295094,
      "grad_norm": 1.4667015075683594,
      "learning_rate": 1.3980518440010926e-05,
      "loss": 1.1394,
      "step": 4541
    },
    {
      "epoch": 0.6385491353859131,
      "grad_norm": 1.4055155515670776,
      "learning_rate": 1.3862199546281939e-05,
      "loss": 1.0955,
      "step": 4542
    },
    {
      "epoch": 0.6386897230423169,
      "grad_norm": 1.345955729484558,
      "learning_rate": 1.3744346158783682e-05,
      "loss": 0.9937,
      "step": 4543
    },
    {
      "epoch": 0.6388303106987206,
      "grad_norm": 1.8019993305206299,
      "learning_rate": 1.362695891442013e-05,
      "loss": 1.027,
      "step": 4544
    },
    {
      "epoch": 0.6389708983551244,
      "grad_norm": 1.4690574407577515,
      "learning_rate": 1.3510038447575623e-05,
      "loss": 1.2568,
      "step": 4545
    },
    {
      "epoch": 0.6391114860115282,
      "grad_norm": 1.3037898540496826,
      "learning_rate": 1.3393585390112285e-05,
      "loss": 1.0033,
      "step": 4546
    },
    {
      "epoch": 0.6392520736679319,
      "grad_norm": 1.509144902229309,
      "learning_rate": 1.3277600371366173e-05,
      "loss": 1.2401,
      "step": 4547
    },
    {
      "epoch": 0.6393926613243357,
      "grad_norm": 1.4416965246200562,
      "learning_rate": 1.3162084018143961e-05,
      "loss": 1.1529,
      "step": 4548
    },
    {
      "epoch": 0.6395332489807395,
      "grad_norm": 1.3746495246887207,
      "learning_rate": 1.3047036954719672e-05,
      "loss": 0.9523,
      "step": 4549
    },
    {
      "epoch": 0.6396738366371433,
      "grad_norm": 1.6035786867141724,
      "learning_rate": 1.2932459802830887e-05,
      "loss": 0.941,
      "step": 4550
    },
    {
      "epoch": 0.6398144242935471,
      "grad_norm": 1.4460724592208862,
      "learning_rate": 1.2818353181675913e-05,
      "loss": 1.1436,
      "step": 4551
    },
    {
      "epoch": 0.6399550119499507,
      "grad_norm": 1.4307199716567993,
      "learning_rate": 1.270471770791044e-05,
      "loss": 1.104,
      "step": 4552
    },
    {
      "epoch": 0.6400955996063545,
      "grad_norm": 1.3028912544250488,
      "learning_rate": 1.2591553995643468e-05,
      "loss": 1.1008,
      "step": 4553
    },
    {
      "epoch": 0.6402361872627583,
      "grad_norm": 1.4883086681365967,
      "learning_rate": 1.2478862656434765e-05,
      "loss": 0.9387,
      "step": 4554
    },
    {
      "epoch": 0.6403767749191621,
      "grad_norm": 1.5039490461349487,
      "learning_rate": 1.2366644299291563e-05,
      "loss": 1.121,
      "step": 4555
    },
    {
      "epoch": 0.6405173625755659,
      "grad_norm": 1.7084461450576782,
      "learning_rate": 1.2254899530664465e-05,
      "loss": 1.128,
      "step": 4556
    },
    {
      "epoch": 0.6406579502319696,
      "grad_norm": 1.6071950197219849,
      "learning_rate": 1.2143628954445163e-05,
      "loss": 1.131,
      "step": 4557
    },
    {
      "epoch": 0.6407985378883734,
      "grad_norm": 1.3377387523651123,
      "learning_rate": 1.2032833171962265e-05,
      "loss": 1.1283,
      "step": 4558
    },
    {
      "epoch": 0.6409391255447772,
      "grad_norm": 1.3342350721359253,
      "learning_rate": 1.1922512781979022e-05,
      "loss": 1.1275,
      "step": 4559
    },
    {
      "epoch": 0.641079713201181,
      "grad_norm": 1.3437042236328125,
      "learning_rate": 1.1812668380689296e-05,
      "loss": 1.0719,
      "step": 4560
    },
    {
      "epoch": 0.6412203008575847,
      "grad_norm": 1.1669013500213623,
      "learning_rate": 1.1703300561714482e-05,
      "loss": 1.0583,
      "step": 4561
    },
    {
      "epoch": 0.6413608885139884,
      "grad_norm": 1.6965230703353882,
      "learning_rate": 1.1594409916100535e-05,
      "loss": 1.0856,
      "step": 4562
    },
    {
      "epoch": 0.6415014761703922,
      "grad_norm": 1.28836190700531,
      "learning_rate": 1.1485997032314954e-05,
      "loss": 1.1146,
      "step": 4563
    },
    {
      "epoch": 0.641642063826796,
      "grad_norm": 1.6541627645492554,
      "learning_rate": 1.1378062496242826e-05,
      "loss": 1.158,
      "step": 4564
    },
    {
      "epoch": 0.6417826514831998,
      "grad_norm": 1.5767271518707275,
      "learning_rate": 1.1270606891184543e-05,
      "loss": 1.1874,
      "step": 4565
    },
    {
      "epoch": 0.6419232391396036,
      "grad_norm": 1.4117753505706787,
      "learning_rate": 1.1163630797851842e-05,
      "loss": 1.1423,
      "step": 4566
    },
    {
      "epoch": 0.6420638267960073,
      "grad_norm": 1.603371262550354,
      "learning_rate": 1.1057134794365598e-05,
      "loss": 1.2485,
      "step": 4567
    },
    {
      "epoch": 0.642204414452411,
      "grad_norm": 1.601109266281128,
      "learning_rate": 1.0951119456251901e-05,
      "loss": 1.0469,
      "step": 4568
    },
    {
      "epoch": 0.6423450021088148,
      "grad_norm": 1.455797553062439,
      "learning_rate": 1.0845585356439092e-05,
      "loss": 0.9041,
      "step": 4569
    },
    {
      "epoch": 0.6424855897652186,
      "grad_norm": 1.7134190797805786,
      "learning_rate": 1.0740533065254964e-05,
      "loss": 0.9472,
      "step": 4570
    },
    {
      "epoch": 0.6426261774216224,
      "grad_norm": 1.4496278762817383,
      "learning_rate": 1.0635963150423744e-05,
      "loss": 1.1634,
      "step": 4571
    },
    {
      "epoch": 0.6427667650780261,
      "grad_norm": 1.4793556928634644,
      "learning_rate": 1.0531876177062338e-05,
      "loss": 1.192,
      "step": 4572
    },
    {
      "epoch": 0.6429073527344299,
      "grad_norm": 1.8215618133544922,
      "learning_rate": 1.0428272707678033e-05,
      "loss": 0.9175,
      "step": 4573
    },
    {
      "epoch": 0.6430479403908337,
      "grad_norm": 1.357853889465332,
      "learning_rate": 1.0325153302165091e-05,
      "loss": 1.1276,
      "step": 4574
    },
    {
      "epoch": 0.6431885280472375,
      "grad_norm": 1.4100955724716187,
      "learning_rate": 1.0222518517801805e-05,
      "loss": 1.2328,
      "step": 4575
    },
    {
      "epoch": 0.6433291157036413,
      "grad_norm": 1.7632883787155151,
      "learning_rate": 1.0120368909247558e-05,
      "loss": 0.9749,
      "step": 4576
    },
    {
      "epoch": 0.6434697033600449,
      "grad_norm": 1.3649660348892212,
      "learning_rate": 1.00187050285394e-05,
      "loss": 1.259,
      "step": 4577
    },
    {
      "epoch": 0.6436102910164487,
      "grad_norm": 1.4080471992492676,
      "learning_rate": 9.917527425089912e-06,
      "loss": 1.0207,
      "step": 4578
    },
    {
      "epoch": 0.6437508786728525,
      "grad_norm": 1.5512540340423584,
      "learning_rate": 9.81683664568348e-06,
      "loss": 1.1094,
      "step": 4579
    },
    {
      "epoch": 0.6438914663292563,
      "grad_norm": 1.2668583393096924,
      "learning_rate": 9.71663323447345e-06,
      "loss": 1.0997,
      "step": 4580
    },
    {
      "epoch": 0.6440320539856601,
      "grad_norm": 2.0123953819274902,
      "learning_rate": 9.61691773297948e-06,
      "loss": 1.0402,
      "step": 4581
    },
    {
      "epoch": 0.6441726416420638,
      "grad_norm": 1.5434449911117554,
      "learning_rate": 9.51769068008469e-06,
      "loss": 1.0305,
      "step": 4582
    },
    {
      "epoch": 0.6443132292984676,
      "grad_norm": 1.3106489181518555,
      "learning_rate": 9.418952612032061e-06,
      "loss": 1.1233,
      "step": 4583
    },
    {
      "epoch": 0.6444538169548714,
      "grad_norm": 1.4043315649032593,
      "learning_rate": 9.320704062422415e-06,
      "loss": 1.0474,
      "step": 4584
    },
    {
      "epoch": 0.6445944046112752,
      "grad_norm": 1.2796956300735474,
      "learning_rate": 9.222945562210695e-06,
      "loss": 1.1592,
      "step": 4585
    },
    {
      "epoch": 0.6447349922676789,
      "grad_norm": 1.4754557609558105,
      "learning_rate": 9.125677639703988e-06,
      "loss": 1.1417,
      "step": 4586
    },
    {
      "epoch": 0.6448755799240826,
      "grad_norm": 1.4679827690124512,
      "learning_rate": 9.028900820557973e-06,
      "loss": 1.2849,
      "step": 4587
    },
    {
      "epoch": 0.6450161675804864,
      "grad_norm": 2.0796046257019043,
      "learning_rate": 8.932615627774165e-06,
      "loss": 0.7722,
      "step": 4588
    },
    {
      "epoch": 0.6451567552368902,
      "grad_norm": 1.3975369930267334,
      "learning_rate": 8.83682258169738e-06,
      "loss": 1.1146,
      "step": 4589
    },
    {
      "epoch": 0.645297342893294,
      "grad_norm": 1.2121435403823853,
      "learning_rate": 8.741522200012952e-06,
      "loss": 1.0864,
      "step": 4590
    },
    {
      "epoch": 0.6454379305496978,
      "grad_norm": 1.8791464567184448,
      "learning_rate": 8.646714997743376e-06,
      "loss": 1.0714,
      "step": 4591
    },
    {
      "epoch": 0.6455785182061015,
      "grad_norm": 1.4890104532241821,
      "learning_rate": 8.552401487246164e-06,
      "loss": 0.9364,
      "step": 4592
    },
    {
      "epoch": 0.6457191058625052,
      "grad_norm": 1.4233893156051636,
      "learning_rate": 8.45858217821075e-06,
      "loss": 0.8702,
      "step": 4593
    },
    {
      "epoch": 0.645859693518909,
      "grad_norm": 1.4284602403640747,
      "learning_rate": 8.365257577655806e-06,
      "loss": 0.9725,
      "step": 4594
    },
    {
      "epoch": 0.6460002811753128,
      "grad_norm": 1.4677550792694092,
      "learning_rate": 8.272428189926563e-06,
      "loss": 1.1689,
      "step": 4595
    },
    {
      "epoch": 0.6461408688317166,
      "grad_norm": 1.4904371500015259,
      "learning_rate": 8.180094516691728e-06,
      "loss": 0.8672,
      "step": 4596
    },
    {
      "epoch": 0.6462814564881203,
      "grad_norm": 1.3578132390975952,
      "learning_rate": 8.088257056941528e-06,
      "loss": 1.1481,
      "step": 4597
    },
    {
      "epoch": 0.6464220441445241,
      "grad_norm": 1.5984734296798706,
      "learning_rate": 7.996916306984293e-06,
      "loss": 0.9219,
      "step": 4598
    },
    {
      "epoch": 0.6465626318009279,
      "grad_norm": 1.6134288311004639,
      "learning_rate": 7.906072760443927e-06,
      "loss": 1.0042,
      "step": 4599
    },
    {
      "epoch": 0.6467032194573317,
      "grad_norm": 1.3699004650115967,
      "learning_rate": 7.815726908257537e-06,
      "loss": 1.0077,
      "step": 4600
    },
    {
      "epoch": 0.6468438071137355,
      "grad_norm": 1.4933918714523315,
      "learning_rate": 7.725879238672518e-06,
      "loss": 1.0974,
      "step": 4601
    },
    {
      "epoch": 0.6469843947701391,
      "grad_norm": 1.5371718406677246,
      "learning_rate": 7.636530237243999e-06,
      "loss": 0.9565,
      "step": 4602
    },
    {
      "epoch": 0.6471249824265429,
      "grad_norm": 1.4520262479782104,
      "learning_rate": 7.547680386832257e-06,
      "loss": 1.2959,
      "step": 4603
    },
    {
      "epoch": 0.6472655700829467,
      "grad_norm": 1.5093024969100952,
      "learning_rate": 7.459330167599798e-06,
      "loss": 1.0456,
      "step": 4604
    },
    {
      "epoch": 0.6474061577393505,
      "grad_norm": 1.5406694412231445,
      "learning_rate": 7.371480057009406e-06,
      "loss": 0.9843,
      "step": 4605
    },
    {
      "epoch": 0.6475467453957543,
      "grad_norm": 1.4007389545440674,
      "learning_rate": 7.284130529820987e-06,
      "loss": 1.0817,
      "step": 4606
    },
    {
      "epoch": 0.647687333052158,
      "grad_norm": 1.5947946310043335,
      "learning_rate": 7.197282058089072e-06,
      "loss": 1.2235,
      "step": 4607
    },
    {
      "epoch": 0.6478279207085618,
      "grad_norm": 1.4811285734176636,
      "learning_rate": 7.110935111160544e-06,
      "loss": 1.2335,
      "step": 4608
    },
    {
      "epoch": 0.6479685083649656,
      "grad_norm": 1.744096279144287,
      "learning_rate": 7.025090155671932e-06,
      "loss": 1.1784,
      "step": 4609
    },
    {
      "epoch": 0.6481090960213693,
      "grad_norm": 1.4991068840026855,
      "learning_rate": 6.939747655546847e-06,
      "loss": 1.0812,
      "step": 4610
    },
    {
      "epoch": 0.6482496836777731,
      "grad_norm": 1.5459321737289429,
      "learning_rate": 6.854908071993638e-06,
      "loss": 1.1034,
      "step": 4611
    },
    {
      "epoch": 0.6483902713341768,
      "grad_norm": 1.5247722864151,
      "learning_rate": 6.770571863502506e-06,
      "loss": 1.015,
      "step": 4612
    },
    {
      "epoch": 0.6485308589905806,
      "grad_norm": 1.4616103172302246,
      "learning_rate": 6.686739485843707e-06,
      "loss": 1.0399,
      "step": 4613
    },
    {
      "epoch": 0.6486714466469844,
      "grad_norm": 2.0909390449523926,
      "learning_rate": 6.603411392064496e-06,
      "loss": 1.1416,
      "step": 4614
    },
    {
      "epoch": 0.6488120343033882,
      "grad_norm": 1.4743515253067017,
      "learning_rate": 6.520588032486685e-06,
      "loss": 1.2245,
      "step": 4615
    },
    {
      "epoch": 0.648952621959792,
      "grad_norm": 1.6310267448425293,
      "learning_rate": 6.438269854704848e-06,
      "loss": 0.9659,
      "step": 4616
    },
    {
      "epoch": 0.6490932096161957,
      "grad_norm": 1.4004312753677368,
      "learning_rate": 6.3564573035829965e-06,
      "loss": 0.9982,
      "step": 4617
    },
    {
      "epoch": 0.6492337972725994,
      "grad_norm": 1.4322093725204468,
      "learning_rate": 6.275150821252906e-06,
      "loss": 1.1035,
      "step": 4618
    },
    {
      "epoch": 0.6493743849290032,
      "grad_norm": 1.4936329126358032,
      "learning_rate": 6.194350847111385e-06,
      "loss": 1.2566,
      "step": 4619
    },
    {
      "epoch": 0.649514972585407,
      "grad_norm": 1.7867822647094727,
      "learning_rate": 6.114057817817942e-06,
      "loss": 1.1337,
      "step": 4620
    },
    {
      "epoch": 0.6496555602418108,
      "grad_norm": 1.4738588333129883,
      "learning_rate": 6.034272167292488e-06,
      "loss": 1.1463,
      "step": 4621
    },
    {
      "epoch": 0.6497961478982145,
      "grad_norm": 1.6060329675674438,
      "learning_rate": 5.954994326712992e-06,
      "loss": 1.0886,
      "step": 4622
    },
    {
      "epoch": 0.6499367355546183,
      "grad_norm": 1.4173438549041748,
      "learning_rate": 5.876224724512891e-06,
      "loss": 1.0507,
      "step": 4623
    },
    {
      "epoch": 0.6500773232110221,
      "grad_norm": 1.3353832960128784,
      "learning_rate": 5.797963786379357e-06,
      "loss": 1.0758,
      "step": 4624
    },
    {
      "epoch": 0.6502179108674259,
      "grad_norm": 1.28609037399292,
      "learning_rate": 5.72021193525043e-06,
      "loss": 1.1155,
      "step": 4625
    },
    {
      "epoch": 0.6503584985238297,
      "grad_norm": 1.2950166463851929,
      "learning_rate": 5.642969591312863e-06,
      "loss": 1.228,
      "step": 4626
    },
    {
      "epoch": 0.6504990861802333,
      "grad_norm": 1.2037038803100586,
      "learning_rate": 5.566237172000077e-06,
      "loss": 1.3189,
      "step": 4627
    },
    {
      "epoch": 0.6506396738366371,
      "grad_norm": 1.3204855918884277,
      "learning_rate": 5.490015091989709e-06,
      "loss": 1.0453,
      "step": 4628
    },
    {
      "epoch": 0.6507802614930409,
      "grad_norm": 1.8723152875900269,
      "learning_rate": 5.414303763201412e-06,
      "loss": 1.0591,
      "step": 4629
    },
    {
      "epoch": 0.6509208491494447,
      "grad_norm": 1.461222767829895,
      "learning_rate": 5.339103594794692e-06,
      "loss": 1.0136,
      "step": 4630
    },
    {
      "epoch": 0.6510614368058485,
      "grad_norm": 1.4480640888214111,
      "learning_rate": 5.264414993166433e-06,
      "loss": 1.1268,
      "step": 4631
    },
    {
      "epoch": 0.6512020244622522,
      "grad_norm": 1.3429391384124756,
      "learning_rate": 5.190238361949207e-06,
      "loss": 1.123,
      "step": 4632
    },
    {
      "epoch": 0.651342612118656,
      "grad_norm": 1.43754243850708,
      "learning_rate": 5.116574102008664e-06,
      "loss": 1.3193,
      "step": 4633
    },
    {
      "epoch": 0.6514831997750598,
      "grad_norm": 1.3405563831329346,
      "learning_rate": 5.0434226114413175e-06,
      "loss": 1.313,
      "step": 4634
    },
    {
      "epoch": 0.6516237874314635,
      "grad_norm": 1.277026891708374,
      "learning_rate": 4.970784285572938e-06,
      "loss": 1.2259,
      "step": 4635
    },
    {
      "epoch": 0.6517643750878673,
      "grad_norm": 1.3763902187347412,
      "learning_rate": 4.898659516955695e-06,
      "loss": 1.1217,
      "step": 4636
    },
    {
      "epoch": 0.651904962744271,
      "grad_norm": 1.3666484355926514,
      "learning_rate": 4.827048695366576e-06,
      "loss": 1.0983,
      "step": 4637
    },
    {
      "epoch": 0.6520455504006748,
      "grad_norm": 1.3865193128585815,
      "learning_rate": 4.75595220780507e-06,
      "loss": 0.9801,
      "step": 4638
    },
    {
      "epoch": 0.6521861380570786,
      "grad_norm": 1.4418766498565674,
      "learning_rate": 4.685370438491088e-06,
      "loss": 0.8953,
      "step": 4639
    },
    {
      "epoch": 0.6523267257134824,
      "grad_norm": 1.837701439857483,
      "learning_rate": 4.615303768862888e-06,
      "loss": 1.0736,
      "step": 4640
    },
    {
      "epoch": 0.6524673133698862,
      "grad_norm": 1.4642831087112427,
      "learning_rate": 4.5457525775750905e-06,
      "loss": 1.1013,
      "step": 4641
    },
    {
      "epoch": 0.6526079010262898,
      "grad_norm": 1.7053608894348145,
      "learning_rate": 4.476717240496364e-06,
      "loss": 1.0683,
      "step": 4642
    },
    {
      "epoch": 0.6527484886826936,
      "grad_norm": 1.3616359233856201,
      "learning_rate": 4.40819813070793e-06,
      "loss": 1.0951,
      "step": 4643
    },
    {
      "epoch": 0.6528890763390974,
      "grad_norm": 1.636445164680481,
      "learning_rate": 4.3401956185008865e-06,
      "loss": 1.1668,
      "step": 4644
    },
    {
      "epoch": 0.6530296639955012,
      "grad_norm": 1.447555422782898,
      "learning_rate": 4.272710071374708e-06,
      "loss": 1.1322,
      "step": 4645
    },
    {
      "epoch": 0.653170251651905,
      "grad_norm": 1.4610812664031982,
      "learning_rate": 4.205741854035061e-06,
      "loss": 1.189,
      "step": 4646
    },
    {
      "epoch": 0.6533108393083087,
      "grad_norm": 1.6126530170440674,
      "learning_rate": 4.139291328391826e-06,
      "loss": 1.1342,
      "step": 4647
    },
    {
      "epoch": 0.6534514269647125,
      "grad_norm": 1.4832618236541748,
      "learning_rate": 4.0733588535571964e-06,
      "loss": 1.2393,
      "step": 4648
    },
    {
      "epoch": 0.6535920146211163,
      "grad_norm": 1.3505134582519531,
      "learning_rate": 4.007944785843754e-06,
      "loss": 1.1831,
      "step": 4649
    },
    {
      "epoch": 0.6537326022775201,
      "grad_norm": 1.5674325227737427,
      "learning_rate": 3.943049478762306e-06,
      "loss": 1.1255,
      "step": 4650
    },
    {
      "epoch": 0.6538731899339238,
      "grad_norm": 1.4148610830307007,
      "learning_rate": 3.878673283020506e-06,
      "loss": 1.1137,
      "step": 4651
    },
    {
      "epoch": 0.6540137775903275,
      "grad_norm": 1.5694575309753418,
      "learning_rate": 3.814816546520305e-06,
      "loss": 1.0304,
      "step": 4652
    },
    {
      "epoch": 0.6541543652467313,
      "grad_norm": 2.029987335205078,
      "learning_rate": 3.7514796143565233e-06,
      "loss": 1.0763,
      "step": 4653
    },
    {
      "epoch": 0.6542949529031351,
      "grad_norm": 1.4760479927062988,
      "learning_rate": 3.688662828814993e-06,
      "loss": 1.1008,
      "step": 4654
    },
    {
      "epoch": 0.6544355405595389,
      "grad_norm": 1.6851032972335815,
      "learning_rate": 3.626366529370273e-06,
      "loss": 1.0602,
      "step": 4655
    },
    {
      "epoch": 0.6545761282159427,
      "grad_norm": 1.3899202346801758,
      "learning_rate": 3.5645910526843186e-06,
      "loss": 1.1824,
      "step": 4656
    },
    {
      "epoch": 0.6547167158723464,
      "grad_norm": 1.371256709098816,
      "learning_rate": 3.5033367326043896e-06,
      "loss": 1.1281,
      "step": 4657
    },
    {
      "epoch": 0.6548573035287502,
      "grad_norm": 1.3207546472549438,
      "learning_rate": 3.442603900161334e-06,
      "loss": 1.2554,
      "step": 4658
    },
    {
      "epoch": 0.654997891185154,
      "grad_norm": 1.6351217031478882,
      "learning_rate": 3.382392883567753e-06,
      "loss": 1.0777,
      "step": 4659
    },
    {
      "epoch": 0.6551384788415577,
      "grad_norm": 1.4840818643569946,
      "learning_rate": 3.322704008216282e-06,
      "loss": 1.1817,
      "step": 4660
    },
    {
      "epoch": 0.6552790664979615,
      "grad_norm": 1.4984583854675293,
      "learning_rate": 3.263537596677657e-06,
      "loss": 1.0334,
      "step": 4661
    },
    {
      "epoch": 0.6554196541543652,
      "grad_norm": 1.592917561531067,
      "learning_rate": 3.204893968699385e-06,
      "loss": 1.1682,
      "step": 4662
    },
    {
      "epoch": 0.655560241810769,
      "grad_norm": 1.3989580869674683,
      "learning_rate": 3.1467734412034656e-06,
      "loss": 0.9429,
      "step": 4663
    },
    {
      "epoch": 0.6557008294671728,
      "grad_norm": 1.6415146589279175,
      "learning_rate": 3.0891763282851038e-06,
      "loss": 1.1336,
      "step": 4664
    },
    {
      "epoch": 0.6558414171235766,
      "grad_norm": 1.6393038034439087,
      "learning_rate": 3.032102941210857e-06,
      "loss": 1.0681,
      "step": 4665
    },
    {
      "epoch": 0.6559820047799804,
      "grad_norm": 1.443495750427246,
      "learning_rate": 2.9755535884169237e-06,
      "loss": 0.9845,
      "step": 4666
    },
    {
      "epoch": 0.656122592436384,
      "grad_norm": 1.740159273147583,
      "learning_rate": 2.919528575507546e-06,
      "loss": 1.1334,
      "step": 4667
    },
    {
      "epoch": 0.6562631800927878,
      "grad_norm": 1.5363390445709229,
      "learning_rate": 2.8640282052533553e-06,
      "loss": 1.0782,
      "step": 4668
    },
    {
      "epoch": 0.6564037677491916,
      "grad_norm": 1.5020028352737427,
      "learning_rate": 2.809052777589538e-06,
      "loss": 0.9812,
      "step": 4669
    },
    {
      "epoch": 0.6565443554055954,
      "grad_norm": 1.9931001663208008,
      "learning_rate": 2.7546025896146633e-06,
      "loss": 1.1206,
      "step": 4670
    },
    {
      "epoch": 0.6566849430619992,
      "grad_norm": 1.311684012413025,
      "learning_rate": 2.700677935588536e-06,
      "loss": 1.1235,
      "step": 4671
    },
    {
      "epoch": 0.6568255307184029,
      "grad_norm": 1.5941035747528076,
      "learning_rate": 2.6472791069309776e-06,
      "loss": 1.1965,
      "step": 4672
    },
    {
      "epoch": 0.6569661183748067,
      "grad_norm": 1.464877963066101,
      "learning_rate": 2.594406392220261e-06,
      "loss": 1.0991,
      "step": 4673
    },
    {
      "epoch": 0.6571067060312105,
      "grad_norm": 1.4147855043411255,
      "learning_rate": 2.542060077191177e-06,
      "loss": 1.0422,
      "step": 4674
    },
    {
      "epoch": 0.6572472936876143,
      "grad_norm": 1.6108964681625366,
      "learning_rate": 2.490240444733949e-06,
      "loss": 0.9241,
      "step": 4675
    },
    {
      "epoch": 0.657387881344018,
      "grad_norm": 1.400757074356079,
      "learning_rate": 2.4389477748924194e-06,
      "loss": 0.9684,
      "step": 4676
    },
    {
      "epoch": 0.6575284690004217,
      "grad_norm": 1.3087785243988037,
      "learning_rate": 2.388182344862633e-06,
      "loss": 0.9708,
      "step": 4677
    },
    {
      "epoch": 0.6576690566568255,
      "grad_norm": 1.4986070394515991,
      "learning_rate": 2.3379444289913567e-06,
      "loss": 1.0709,
      "step": 4678
    },
    {
      "epoch": 0.6578096443132293,
      "grad_norm": 1.687068223953247,
      "learning_rate": 2.288234298774461e-06,
      "loss": 1.0933,
      "step": 4679
    },
    {
      "epoch": 0.6579502319696331,
      "grad_norm": 1.565130591392517,
      "learning_rate": 2.2390522228555977e-06,
      "loss": 1.138,
      "step": 4680
    },
    {
      "epoch": 0.6580908196260369,
      "grad_norm": 1.3261088132858276,
      "learning_rate": 2.190398467024868e-06,
      "loss": 0.992,
      "step": 4681
    },
    {
      "epoch": 0.6582314072824406,
      "grad_norm": 1.3626514673233032,
      "learning_rate": 2.1422732942169676e-06,
      "loss": 1.0854,
      "step": 4682
    },
    {
      "epoch": 0.6583719949388444,
      "grad_norm": 1.4164628982543945,
      "learning_rate": 2.094676964510167e-06,
      "loss": 0.8984,
      "step": 4683
    },
    {
      "epoch": 0.6585125825952481,
      "grad_norm": 1.611080527305603,
      "learning_rate": 2.0476097351247446e-06,
      "loss": 1.0359,
      "step": 4684
    },
    {
      "epoch": 0.6586531702516519,
      "grad_norm": 1.4140105247497559,
      "learning_rate": 2.0010718604215884e-06,
      "loss": 1.0064,
      "step": 4685
    },
    {
      "epoch": 0.6587937579080557,
      "grad_norm": 1.3111251592636108,
      "learning_rate": 1.9550635919009075e-06,
      "loss": 1.1599,
      "step": 4686
    },
    {
      "epoch": 0.6589343455644594,
      "grad_norm": 1.3239078521728516,
      "learning_rate": 1.909585178200679e-06,
      "loss": 1.0376,
      "step": 4687
    },
    {
      "epoch": 0.6590749332208632,
      "grad_norm": 1.3969831466674805,
      "learning_rate": 1.864636865095537e-06,
      "loss": 1.0467,
      "step": 4688
    },
    {
      "epoch": 0.659215520877267,
      "grad_norm": 1.521179437637329,
      "learning_rate": 1.820218895495418e-06,
      "loss": 1.1343,
      "step": 4689
    },
    {
      "epoch": 0.6593561085336708,
      "grad_norm": 1.7416496276855469,
      "learning_rate": 1.7763315094439847e-06,
      "loss": 1.2626,
      "step": 4690
    },
    {
      "epoch": 0.6594966961900746,
      "grad_norm": 1.2517189979553223,
      "learning_rate": 1.7329749441176268e-06,
      "loss": 1.0251,
      "step": 4691
    },
    {
      "epoch": 0.6596372838464782,
      "grad_norm": 1.5434406995773315,
      "learning_rate": 1.6901494338241397e-06,
      "loss": 1.136,
      "step": 4692
    },
    {
      "epoch": 0.659777871502882,
      "grad_norm": 1.3562062978744507,
      "learning_rate": 1.6478552100012257e-06,
      "loss": 1.218,
      "step": 4693
    },
    {
      "epoch": 0.6599184591592858,
      "grad_norm": 1.8169102668762207,
      "learning_rate": 1.606092501215517e-06,
      "loss": 1.0194,
      "step": 4694
    },
    {
      "epoch": 0.6600590468156896,
      "grad_norm": 1.404701828956604,
      "learning_rate": 1.5648615331612104e-06,
      "loss": 1.0888,
      "step": 4695
    },
    {
      "epoch": 0.6601996344720934,
      "grad_norm": 1.4749436378479004,
      "learning_rate": 1.5241625286588567e-06,
      "loss": 1.1874,
      "step": 4696
    },
    {
      "epoch": 0.6603402221284971,
      "grad_norm": 1.4246598482131958,
      "learning_rate": 1.4839957076542066e-06,
      "loss": 1.127,
      "step": 4697
    },
    {
      "epoch": 0.6604808097849009,
      "grad_norm": 1.2662742137908936,
      "learning_rate": 1.4443612872168777e-06,
      "loss": 1.1556,
      "step": 4698
    },
    {
      "epoch": 0.6606213974413047,
      "grad_norm": 1.3867974281311035,
      "learning_rate": 1.405259481539367e-06,
      "loss": 1.1098,
      "step": 4699
    },
    {
      "epoch": 0.6607619850977084,
      "grad_norm": 1.5200914144515991,
      "learning_rate": 1.3666905019358856e-06,
      "loss": 1.143,
      "step": 4700
    },
    {
      "epoch": 0.6609025727541122,
      "grad_norm": 1.4861524105072021,
      "learning_rate": 1.3286545568409802e-06,
      "loss": 1.1899,
      "step": 4701
    },
    {
      "epoch": 0.6610431604105159,
      "grad_norm": 1.3704124689102173,
      "learning_rate": 1.2911518518086695e-06,
      "loss": 1.2642,
      "step": 4702
    },
    {
      "epoch": 0.6611837480669197,
      "grad_norm": 1.318520188331604,
      "learning_rate": 1.2541825895112213e-06,
      "loss": 1.2359,
      "step": 4703
    },
    {
      "epoch": 0.6613243357233235,
      "grad_norm": 1.5450632572174072,
      "learning_rate": 1.217746969738065e-06,
      "loss": 1.0623,
      "step": 4704
    },
    {
      "epoch": 0.6614649233797273,
      "grad_norm": 1.5264941453933716,
      "learning_rate": 1.1818451893947368e-06,
      "loss": 1.1317,
      "step": 4705
    },
    {
      "epoch": 0.6616055110361311,
      "grad_norm": 1.8133238554000854,
      "learning_rate": 1.1464774425017366e-06,
      "loss": 1.2198,
      "step": 4706
    },
    {
      "epoch": 0.6617460986925348,
      "grad_norm": 1.4382520914077759,
      "learning_rate": 1.1116439201935614e-06,
      "loss": 1.1941,
      "step": 4707
    },
    {
      "epoch": 0.6618866863489385,
      "grad_norm": 1.512198567390442,
      "learning_rate": 1.077344810717751e-06,
      "loss": 1.2951,
      "step": 4708
    },
    {
      "epoch": 0.6620272740053423,
      "grad_norm": 1.3102061748504639,
      "learning_rate": 1.0435802994336108e-06,
      "loss": 1.0848,
      "step": 4709
    },
    {
      "epoch": 0.6621678616617461,
      "grad_norm": 1.4544934034347534,
      "learning_rate": 1.0103505688114245e-06,
      "loss": 1.1525,
      "step": 4710
    },
    {
      "epoch": 0.6623084493181499,
      "grad_norm": 1.369935393333435,
      "learning_rate": 9.776557984315315e-07,
      "loss": 1.1115,
      "step": 4711
    },
    {
      "epoch": 0.6624490369745536,
      "grad_norm": 1.4719254970550537,
      "learning_rate": 9.454961649830396e-07,
      "loss": 1.158,
      "step": 4712
    },
    {
      "epoch": 0.6625896246309574,
      "grad_norm": 1.5282942056655884,
      "learning_rate": 9.138718422632253e-07,
      "loss": 1.1534,
      "step": 4713
    },
    {
      "epoch": 0.6627302122873612,
      "grad_norm": 1.7642723321914673,
      "learning_rate": 8.827830011762905e-07,
      "loss": 0.9751,
      "step": 4714
    },
    {
      "epoch": 0.662870799943765,
      "grad_norm": 1.4575263261795044,
      "learning_rate": 8.522298097327408e-07,
      "loss": 1.0893,
      "step": 4715
    },
    {
      "epoch": 0.6630113876001688,
      "grad_norm": 1.3493950366973877,
      "learning_rate": 8.222124330482639e-07,
      "loss": 1.0619,
      "step": 4716
    },
    {
      "epoch": 0.6631519752565724,
      "grad_norm": 1.2762203216552734,
      "learning_rate": 7.927310333428195e-07,
      "loss": 1.2318,
      "step": 4717
    },
    {
      "epoch": 0.6632925629129762,
      "grad_norm": 1.3016866445541382,
      "learning_rate": 7.637857699399176e-07,
      "loss": 1.159,
      "step": 4718
    },
    {
      "epoch": 0.66343315056938,
      "grad_norm": 1.3848686218261719,
      "learning_rate": 7.353767992657079e-07,
      "loss": 1.1093,
      "step": 4719
    },
    {
      "epoch": 0.6635737382257838,
      "grad_norm": 1.5514755249023438,
      "learning_rate": 7.075042748480143e-07,
      "loss": 1.1296,
      "step": 4720
    },
    {
      "epoch": 0.6637143258821876,
      "grad_norm": 1.6539583206176758,
      "learning_rate": 6.801683473156683e-07,
      "loss": 1.0772,
      "step": 4721
    },
    {
      "epoch": 0.6638549135385913,
      "grad_norm": 1.7444967031478882,
      "learning_rate": 6.533691643975326e-07,
      "loss": 1.2007,
      "step": 4722
    },
    {
      "epoch": 0.6639955011949951,
      "grad_norm": 1.4749362468719482,
      "learning_rate": 6.27106870921923e-07,
      "loss": 1.0723,
      "step": 4723
    },
    {
      "epoch": 0.6641360888513989,
      "grad_norm": 1.5300449132919312,
      "learning_rate": 6.01381608815621e-07,
      "loss": 1.0595,
      "step": 4724
    },
    {
      "epoch": 0.6642766765078026,
      "grad_norm": 1.2021002769470215,
      "learning_rate": 5.761935171031407e-07,
      "loss": 1.2158,
      "step": 4725
    },
    {
      "epoch": 0.6644172641642064,
      "grad_norm": 1.4134711027145386,
      "learning_rate": 5.515427319060739e-07,
      "loss": 1.0113,
      "step": 4726
    },
    {
      "epoch": 0.6645578518206101,
      "grad_norm": 1.3078596591949463,
      "learning_rate": 5.274293864423463e-07,
      "loss": 1.0031,
      "step": 4727
    },
    {
      "epoch": 0.6646984394770139,
      "grad_norm": 1.4784148931503296,
      "learning_rate": 5.03853611025329e-07,
      "loss": 1.1576,
      "step": 4728
    },
    {
      "epoch": 0.6648390271334177,
      "grad_norm": 1.6909754276275635,
      "learning_rate": 4.80815533063339e-07,
      "loss": 0.9528,
      "step": 4729
    },
    {
      "epoch": 0.6649796147898215,
      "grad_norm": 1.5353033542633057,
      "learning_rate": 4.583152770588406e-07,
      "loss": 1.0678,
      "step": 4730
    },
    {
      "epoch": 0.6651202024462253,
      "grad_norm": 1.5983922481536865,
      "learning_rate": 4.3635296460781127e-07,
      "loss": 1.0682,
      "step": 4731
    },
    {
      "epoch": 0.665260790102629,
      "grad_norm": 1.45863676071167,
      "learning_rate": 4.149287143990765e-07,
      "loss": 1.1654,
      "step": 4732
    },
    {
      "epoch": 0.6654013777590327,
      "grad_norm": 1.607346534729004,
      "learning_rate": 3.940426422136101e-07,
      "loss": 1.1677,
      "step": 4733
    },
    {
      "epoch": 0.6655419654154365,
      "grad_norm": 1.5031582117080688,
      "learning_rate": 3.736948609240343e-07,
      "loss": 1.0674,
      "step": 4734
    },
    {
      "epoch": 0.6656825530718403,
      "grad_norm": 1.5838440656661987,
      "learning_rate": 3.5388548049392065e-07,
      "loss": 1.0731,
      "step": 4735
    },
    {
      "epoch": 0.6658231407282441,
      "grad_norm": 1.3321393728256226,
      "learning_rate": 3.3461460797715726e-07,
      "loss": 1.1096,
      "step": 4736
    },
    {
      "epoch": 0.6659637283846478,
      "grad_norm": 1.3840361833572388,
      "learning_rate": 3.158823475174821e-07,
      "loss": 0.8806,
      "step": 4737
    },
    {
      "epoch": 0.6661043160410516,
      "grad_norm": 1.4491748809814453,
      "learning_rate": 2.9768880034788395e-07,
      "loss": 1.1457,
      "step": 4738
    },
    {
      "epoch": 0.6662449036974554,
      "grad_norm": 1.6087952852249146,
      "learning_rate": 2.80034064789958e-07,
      "loss": 1.06,
      "step": 4739
    },
    {
      "epoch": 0.6663854913538592,
      "grad_norm": 1.4784824848175049,
      "learning_rate": 2.629182362535065e-07,
      "loss": 1.2269,
      "step": 4740
    },
    {
      "epoch": 0.666526079010263,
      "grad_norm": 1.3393865823745728,
      "learning_rate": 2.4634140723595044e-07,
      "loss": 1.0946,
      "step": 4741
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 1.4266458749771118,
      "learning_rate": 2.303036673218628e-07,
      "loss": 1.2321,
      "step": 4742
    },
    {
      "epoch": 0.6668072543230704,
      "grad_norm": 1.5118038654327393,
      "learning_rate": 2.1480510318245828e-07,
      "loss": 0.9111,
      "step": 4743
    },
    {
      "epoch": 0.6669478419794742,
      "grad_norm": 1.5277137756347656,
      "learning_rate": 1.9984579857512675e-07,
      "loss": 0.9602,
      "step": 4744
    },
    {
      "epoch": 0.667088429635878,
      "grad_norm": 1.620111107826233,
      "learning_rate": 1.8542583434300042e-07,
      "loss": 1.0505,
      "step": 4745
    },
    {
      "epoch": 0.6672290172922818,
      "grad_norm": 1.9711358547210693,
      "learning_rate": 1.7154528841452078e-07,
      "loss": 0.9652,
      "step": 4746
    },
    {
      "epoch": 0.6673696049486855,
      "grad_norm": 1.443314552307129,
      "learning_rate": 1.582042358029723e-07,
      "loss": 1.115,
      "step": 4747
    },
    {
      "epoch": 0.6675101926050893,
      "grad_norm": 1.417747974395752,
      "learning_rate": 1.4540274860612712e-07,
      "loss": 1.0308,
      "step": 4748
    },
    {
      "epoch": 0.667650780261493,
      "grad_norm": 1.6341999769210815,
      "learning_rate": 1.3314089600583445e-07,
      "loss": 1.1707,
      "step": 4749
    },
    {
      "epoch": 0.6677913679178968,
      "grad_norm": 1.4526551961898804,
      "learning_rate": 1.2141874426763177e-07,
      "loss": 0.9883,
      "step": 4750
    },
    {
      "epoch": 0.6679319555743006,
      "grad_norm": 1.5006890296936035,
      "learning_rate": 1.102363567404452e-07,
      "loss": 1.0936,
      "step": 4751
    },
    {
      "epoch": 0.6680725432307043,
      "grad_norm": 1.4973688125610352,
      "learning_rate": 9.959379385613421e-08,
      "loss": 1.0619,
      "step": 4752
    },
    {
      "epoch": 0.6682131308871081,
      "grad_norm": 1.4991475343704224,
      "learning_rate": 8.949111312931413e-08,
      "loss": 1.1532,
      "step": 4753
    },
    {
      "epoch": 0.6683537185435119,
      "grad_norm": 1.224552035331726,
      "learning_rate": 7.992836915692303e-08,
      "loss": 1.0841,
      "step": 4754
    },
    {
      "epoch": 0.6684943061999157,
      "grad_norm": 1.6060876846313477,
      "learning_rate": 7.090561361796644e-08,
      "loss": 1.3704,
      "step": 4755
    },
    {
      "epoch": 0.6686348938563195,
      "grad_norm": 1.951820969581604,
      "learning_rate": 6.242289527326195e-08,
      "loss": 1.038,
      "step": 4756
    },
    {
      "epoch": 0.6687754815127231,
      "grad_norm": 1.2801499366760254,
      "learning_rate": 5.4480259965139504e-08,
      "loss": 1.2058,
      "step": 4757
    },
    {
      "epoch": 0.6689160691691269,
      "grad_norm": 1.5191859006881714,
      "learning_rate": 4.707775061723041e-08,
      "loss": 1.1005,
      "step": 4758
    },
    {
      "epoch": 0.6690566568255307,
      "grad_norm": 1.6534703969955444,
      "learning_rate": 4.021540723421202e-08,
      "loss": 1.1093,
      "step": 4759
    },
    {
      "epoch": 0.6691972444819345,
      "grad_norm": 1.3649096488952637,
      "learning_rate": 3.389326690156347e-08,
      "loss": 1.1869,
      "step": 4760
    },
    {
      "epoch": 0.6693378321383383,
      "grad_norm": 1.4506586790084839,
      "learning_rate": 2.8111363785432442e-08,
      "loss": 1.1674,
      "step": 4761
    },
    {
      "epoch": 0.669478419794742,
      "grad_norm": 1.6341016292572021,
      "learning_rate": 2.2869729132413142e-08,
      "loss": 1.1764,
      "step": 4762
    },
    {
      "epoch": 0.6696190074511458,
      "grad_norm": 1.539791226387024,
      "learning_rate": 1.8168391269346442e-08,
      "loss": 1.1462,
      "step": 4763
    },
    {
      "epoch": 0.6697595951075496,
      "grad_norm": 1.467381477355957,
      "learning_rate": 1.4007375603219963e-08,
      "loss": 1.1724,
      "step": 4764
    },
    {
      "epoch": 0.6699001827639534,
      "grad_norm": 1.3140398263931274,
      "learning_rate": 1.038670462103486e-08,
      "loss": 1.0921,
      "step": 4765
    },
    {
      "epoch": 0.6700407704203571,
      "grad_norm": 1.3499788045883179,
      "learning_rate": 7.306397889617067e-09,
      "loss": 1.1207,
      "step": 4766
    },
    {
      "epoch": 0.6701813580767608,
      "grad_norm": 1.4730892181396484,
      "learning_rate": 4.766472055572901e-09,
      "loss": 1.018,
      "step": 4767
    },
    {
      "epoch": 0.6703219457331646,
      "grad_norm": 1.5563009977340698,
      "learning_rate": 2.7669408451780344e-09,
      "loss": 1.1369,
      "step": 4768
    },
    {
      "epoch": 0.6704625333895684,
      "grad_norm": 1.4129835367202759,
      "learning_rate": 1.3078150643219822e-09,
      "loss": 1.2097,
      "step": 4769
    },
    {
      "epoch": 0.6706031210459722,
      "grad_norm": 1.3660987615585327,
      "learning_rate": 3.8910259839708417e-10,
      "loss": 1.0155,
      "step": 4770
    },
    {
      "epoch": 0.670743708702376,
      "grad_norm": 1.4899345636367798,
      "learning_rate": 1.0808412320706396e-11,
      "loss": 0.9879,
      "step": 4771
    },
    {
      "epoch": 0.6708842963587797,
      "grad_norm": 1.5905715227127075,
      "learning_rate": 1.7293455047973083e-10,
      "loss": 1.0982,
      "step": 4772
    },
    {
      "epoch": 0.6710248840151835,
      "grad_norm": 1.6500169038772583,
      "learning_rate": 8.754801367083509e-10,
      "loss": 1.0685,
      "step": 4773
    },
    {
      "epoch": 0.6711654716715872,
      "grad_norm": 1.5946232080459595,
      "learning_rate": 2.1184413742880717e-09,
      "loss": 0.9942,
      "step": 4774
    },
    {
      "epoch": 0.671306059327991,
      "grad_norm": 1.3676609992980957,
      "learning_rate": 3.901811546036527e-09,
      "loss": 1.0881,
      "step": 4775
    },
    {
      "epoch": 0.6714466469843948,
      "grad_norm": 1.4879956245422363,
      "learning_rate": 6.225581014229764e-09,
      "loss": 1.106,
      "step": 4776
    },
    {
      "epoch": 0.6715872346407985,
      "grad_norm": 1.283400058746338,
      "learning_rate": 9.089737220746574e-09,
      "loss": 1.1082,
      "step": 4777
    },
    {
      "epoch": 0.6717278222972023,
      "grad_norm": 1.402266025543213,
      "learning_rate": 1.2494264687124002e-08,
      "loss": 1.179,
      "step": 4778
    },
    {
      "epoch": 0.6718684099536061,
      "grad_norm": 1.5371780395507812,
      "learning_rate": 1.643914501457955e-08,
      "loss": 1.4179,
      "step": 4779
    },
    {
      "epoch": 0.6720089976100099,
      "grad_norm": 1.7128407955169678,
      "learning_rate": 2.0924356884188812e-08,
      "loss": 1.1054,
      "step": 4780
    },
    {
      "epoch": 0.6721495852664137,
      "grad_norm": 1.6817388534545898,
      "learning_rate": 2.594987605694099e-08,
      "loss": 1.1167,
      "step": 4781
    },
    {
      "epoch": 0.6722901729228173,
      "grad_norm": 1.3260242938995361,
      "learning_rate": 3.151567537391653e-08,
      "loss": 1.1149,
      "step": 4782
    },
    {
      "epoch": 0.6724307605792211,
      "grad_norm": 1.6274785995483398,
      "learning_rate": 3.7621724756398135e-08,
      "loss": 1.0968,
      "step": 4783
    },
    {
      "epoch": 0.6725713482356249,
      "grad_norm": 1.423259973526001,
      "learning_rate": 4.426799120605951e-08,
      "loss": 1.0457,
      "step": 4784
    },
    {
      "epoch": 0.6727119358920287,
      "grad_norm": 1.5450254678726196,
      "learning_rate": 5.1454438805098595e-08,
      "loss": 1.1616,
      "step": 4785
    },
    {
      "epoch": 0.6728525235484325,
      "grad_norm": 1.4109565019607544,
      "learning_rate": 5.9181028716503994e-08,
      "loss": 1.2159,
      "step": 4786
    },
    {
      "epoch": 0.6729931112048362,
      "grad_norm": 2.057067394256592,
      "learning_rate": 6.744771918422155e-08,
      "loss": 1.1119,
      "step": 4787
    },
    {
      "epoch": 0.67313369886124,
      "grad_norm": 1.486525297164917,
      "learning_rate": 7.625446553335413e-08,
      "loss": 1.0102,
      "step": 4788
    },
    {
      "epoch": 0.6732742865176438,
      "grad_norm": 1.398446798324585,
      "learning_rate": 8.560122017042815e-08,
      "loss": 1.2336,
      "step": 4789
    },
    {
      "epoch": 0.6734148741740476,
      "grad_norm": 1.437994122505188,
      "learning_rate": 9.548793258372657e-08,
      "loss": 1.097,
      "step": 4790
    },
    {
      "epoch": 0.6735554618304513,
      "grad_norm": 1.5355684757232666,
      "learning_rate": 1.0591454934339995e-07,
      "loss": 1.0989,
      "step": 4791
    },
    {
      "epoch": 0.673696049486855,
      "grad_norm": 1.6747028827667236,
      "learning_rate": 1.1688101410191054e-07,
      "loss": 1.1462,
      "step": 4792
    },
    {
      "epoch": 0.6738366371432588,
      "grad_norm": 1.411176323890686,
      "learning_rate": 1.2838726759422105e-07,
      "loss": 1.0849,
      "step": 4793
    },
    {
      "epoch": 0.6739772247996626,
      "grad_norm": 1.586918830871582,
      "learning_rate": 1.4043324763822751e-07,
      "loss": 1.2526,
      "step": 4794
    },
    {
      "epoch": 0.6741178124560664,
      "grad_norm": 1.6840449571609497,
      "learning_rate": 1.5301888913497043e-07,
      "loss": 1.0416,
      "step": 4795
    },
    {
      "epoch": 0.6742584001124702,
      "grad_norm": 1.3676695823669434,
      "learning_rate": 1.6614412406907864e-07,
      "loss": 1.1633,
      "step": 4796
    },
    {
      "epoch": 0.6743989877688739,
      "grad_norm": 1.2136064767837524,
      "learning_rate": 1.798088815091137e-07,
      "loss": 1.1335,
      "step": 4797
    },
    {
      "epoch": 0.6745395754252776,
      "grad_norm": 1.3521567583084106,
      "learning_rate": 1.9401308760796934e-07,
      "loss": 0.9272,
      "step": 4798
    },
    {
      "epoch": 0.6746801630816814,
      "grad_norm": 1.639989972114563,
      "learning_rate": 2.0875666560316033e-07,
      "loss": 1.0096,
      "step": 4799
    },
    {
      "epoch": 0.6748207507380852,
      "grad_norm": 2.395617723464966,
      "learning_rate": 2.2403953581744407e-07,
      "loss": 1.1075,
      "step": 4800
    },
    {
      "epoch": 0.674961338394489,
      "grad_norm": 1.3341600894927979,
      "learning_rate": 2.3986161565904273e-07,
      "loss": 1.175,
      "step": 4801
    },
    {
      "epoch": 0.6751019260508927,
      "grad_norm": 1.3201384544372559,
      "learning_rate": 2.562228196222205e-07,
      "loss": 1.2108,
      "step": 4802
    },
    {
      "epoch": 0.6752425137072965,
      "grad_norm": 2.0240724086761475,
      "learning_rate": 2.7312305928769435e-07,
      "loss": 1.0369,
      "step": 4803
    },
    {
      "epoch": 0.6753831013637003,
      "grad_norm": 1.5471067428588867,
      "learning_rate": 2.9056224332312254e-07,
      "loss": 1.0714,
      "step": 4804
    },
    {
      "epoch": 0.6755236890201041,
      "grad_norm": 1.4260435104370117,
      "learning_rate": 3.0854027748359327e-07,
      "loss": 1.1911,
      "step": 4805
    },
    {
      "epoch": 0.6756642766765079,
      "grad_norm": 1.3810187578201294,
      "learning_rate": 3.2705706461219064e-07,
      "loss": 0.9947,
      "step": 4806
    },
    {
      "epoch": 0.6758048643329115,
      "grad_norm": 1.7384076118469238,
      "learning_rate": 3.461125046403502e-07,
      "loss": 1.0661,
      "step": 4807
    },
    {
      "epoch": 0.6759454519893153,
      "grad_norm": 1.3636423349380493,
      "learning_rate": 3.657064945886468e-07,
      "loss": 1.1536,
      "step": 4808
    },
    {
      "epoch": 0.6760860396457191,
      "grad_norm": 1.394768238067627,
      "learning_rate": 3.8583892856715044e-07,
      "loss": 0.9558,
      "step": 4809
    },
    {
      "epoch": 0.6762266273021229,
      "grad_norm": 1.4544743299484253,
      "learning_rate": 4.0650969777605854e-07,
      "loss": 1.0731,
      "step": 4810
    },
    {
      "epoch": 0.6763672149585267,
      "grad_norm": 1.448643445968628,
      "learning_rate": 4.2771869050636236e-07,
      "loss": 1.1305,
      "step": 4811
    },
    {
      "epoch": 0.6765078026149304,
      "grad_norm": 1.528549075126648,
      "learning_rate": 4.494657921403134e-07,
      "loss": 1.2954,
      "step": 4812
    },
    {
      "epoch": 0.6766483902713342,
      "grad_norm": 1.4048672914505005,
      "learning_rate": 4.717508851521446e-07,
      "loss": 1.0428,
      "step": 4813
    },
    {
      "epoch": 0.676788977927738,
      "grad_norm": 1.736676573753357,
      "learning_rate": 4.945738491086926e-07,
      "loss": 1.0756,
      "step": 4814
    },
    {
      "epoch": 0.6769295655841417,
      "grad_norm": 1.454715609550476,
      "learning_rate": 5.179345606699748e-07,
      "loss": 1.2006,
      "step": 4815
    },
    {
      "epoch": 0.6770701532405454,
      "grad_norm": 1.4325737953186035,
      "learning_rate": 5.418328935899553e-07,
      "loss": 1.0174,
      "step": 4816
    },
    {
      "epoch": 0.6772107408969492,
      "grad_norm": 1.606271505355835,
      "learning_rate": 5.662687187172111e-07,
      "loss": 1.0904,
      "step": 4817
    },
    {
      "epoch": 0.677351328553353,
      "grad_norm": 1.5662051439285278,
      "learning_rate": 5.912419039954986e-07,
      "loss": 1.1234,
      "step": 4818
    },
    {
      "epoch": 0.6774919162097568,
      "grad_norm": 1.3490843772888184,
      "learning_rate": 6.167523144646747e-07,
      "loss": 1.1351,
      "step": 4819
    },
    {
      "epoch": 0.6776325038661606,
      "grad_norm": 1.4742964506149292,
      "learning_rate": 6.427998122612855e-07,
      "loss": 0.9752,
      "step": 4820
    },
    {
      "epoch": 0.6777730915225643,
      "grad_norm": 1.4627928733825684,
      "learning_rate": 6.693842566193209e-07,
      "loss": 0.9268,
      "step": 4821
    },
    {
      "epoch": 0.677913679178968,
      "grad_norm": 1.3610886335372925,
      "learning_rate": 6.965055038710367e-07,
      "loss": 1.2903,
      "step": 4822
    },
    {
      "epoch": 0.6780542668353718,
      "grad_norm": 1.4165807962417603,
      "learning_rate": 7.241634074476866e-07,
      "loss": 1.0223,
      "step": 4823
    },
    {
      "epoch": 0.6781948544917756,
      "grad_norm": 1.4492093324661255,
      "learning_rate": 7.523578178802892e-07,
      "loss": 1.0594,
      "step": 4824
    },
    {
      "epoch": 0.6783354421481794,
      "grad_norm": 1.3083932399749756,
      "learning_rate": 7.810885828005821e-07,
      "loss": 1.1191,
      "step": 4825
    },
    {
      "epoch": 0.6784760298045831,
      "grad_norm": 1.3295800685882568,
      "learning_rate": 8.103555469415657e-07,
      "loss": 1.1115,
      "step": 4826
    },
    {
      "epoch": 0.6786166174609869,
      "grad_norm": 1.2235029935836792,
      "learning_rate": 8.40158552138659e-07,
      "loss": 1.1101,
      "step": 4827
    },
    {
      "epoch": 0.6787572051173907,
      "grad_norm": 1.4327434301376343,
      "learning_rate": 8.704974373303532e-07,
      "loss": 1.038,
      "step": 4828
    },
    {
      "epoch": 0.6788977927737945,
      "grad_norm": 1.3832898139953613,
      "learning_rate": 9.013720385590563e-07,
      "loss": 1.0429,
      "step": 4829
    },
    {
      "epoch": 0.6790383804301983,
      "grad_norm": 1.3806769847869873,
      "learning_rate": 9.327821889722033e-07,
      "loss": 1.13,
      "step": 4830
    },
    {
      "epoch": 0.6791789680866019,
      "grad_norm": 1.3715819120407104,
      "learning_rate": 9.64727718822922e-07,
      "loss": 1.072,
      "step": 4831
    },
    {
      "epoch": 0.6793195557430057,
      "grad_norm": 1.2648341655731201,
      "learning_rate": 9.972084554710437e-07,
      "loss": 1.0836,
      "step": 4832
    },
    {
      "epoch": 0.6794601433994095,
      "grad_norm": 1.4952607154846191,
      "learning_rate": 1.0302242233840576e-06,
      "loss": 0.9527,
      "step": 4833
    },
    {
      "epoch": 0.6796007310558133,
      "grad_norm": 1.6180636882781982,
      "learning_rate": 1.0637748441379991e-06,
      "loss": 1.1066,
      "step": 4834
    },
    {
      "epoch": 0.6797413187122171,
      "grad_norm": 1.424631953239441,
      "learning_rate": 1.097860136418516e-06,
      "loss": 1.0384,
      "step": 4835
    },
    {
      "epoch": 0.6798819063686208,
      "grad_norm": 1.2948923110961914,
      "learning_rate": 1.1324799160217448e-06,
      "loss": 1.0634,
      "step": 4836
    },
    {
      "epoch": 0.6800224940250246,
      "grad_norm": 1.404331088066101,
      "learning_rate": 1.1676339958552774e-06,
      "loss": 1.106,
      "step": 4837
    },
    {
      "epoch": 0.6801630816814284,
      "grad_norm": 1.3394215106964111,
      "learning_rate": 1.2033221859393594e-06,
      "loss": 1.2129,
      "step": 4838
    },
    {
      "epoch": 0.6803036693378322,
      "grad_norm": 1.4706966876983643,
      "learning_rate": 1.239544293407724e-06,
      "loss": 1.1644,
      "step": 4839
    },
    {
      "epoch": 0.6804442569942359,
      "grad_norm": 1.8317556381225586,
      "learning_rate": 1.2763001225087112e-06,
      "loss": 1.0744,
      "step": 4840
    },
    {
      "epoch": 0.6805848446506396,
      "grad_norm": 1.466599464416504,
      "learning_rate": 1.3135894746063248e-06,
      "loss": 1.0021,
      "step": 4841
    },
    {
      "epoch": 0.6807254323070434,
      "grad_norm": 1.6144347190856934,
      "learning_rate": 1.351412148181297e-06,
      "loss": 1.1979,
      "step": 4842
    },
    {
      "epoch": 0.6808660199634472,
      "grad_norm": 1.4847089052200317,
      "learning_rate": 1.3897679388322316e-06,
      "loss": 0.9999,
      "step": 4843
    },
    {
      "epoch": 0.681006607619851,
      "grad_norm": 1.3977535963058472,
      "learning_rate": 1.4286566392766155e-06,
      "loss": 0.9053,
      "step": 4844
    },
    {
      "epoch": 0.6811471952762548,
      "grad_norm": 1.4604250192642212,
      "learning_rate": 1.4680780393519166e-06,
      "loss": 1.2084,
      "step": 4845
    },
    {
      "epoch": 0.6812877829326585,
      "grad_norm": 1.298667311668396,
      "learning_rate": 1.5080319260169173e-06,
      "loss": 0.9536,
      "step": 4846
    },
    {
      "epoch": 0.6814283705890622,
      "grad_norm": 1.48374605178833,
      "learning_rate": 1.5485180833526903e-06,
      "loss": 1.1024,
      "step": 4847
    },
    {
      "epoch": 0.681568958245466,
      "grad_norm": 1.3797065019607544,
      "learning_rate": 1.589536292563687e-06,
      "loss": 1.0937,
      "step": 4848
    },
    {
      "epoch": 0.6817095459018698,
      "grad_norm": 1.4217805862426758,
      "learning_rate": 1.6310863319792368e-06,
      "loss": 1.0705,
      "step": 4849
    },
    {
      "epoch": 0.6818501335582736,
      "grad_norm": 1.6601895093917847,
      "learning_rate": 1.6731679770544351e-06,
      "loss": 0.9814,
      "step": 4850
    },
    {
      "epoch": 0.6819907212146773,
      "grad_norm": 1.4799463748931885,
      "learning_rate": 1.7157810003714858e-06,
      "loss": 0.9547,
      "step": 4851
    },
    {
      "epoch": 0.6821313088710811,
      "grad_norm": 1.3784663677215576,
      "learning_rate": 1.758925171640935e-06,
      "loss": 1.0076,
      "step": 4852
    },
    {
      "epoch": 0.6822718965274849,
      "grad_norm": 1.480427861213684,
      "learning_rate": 1.8026002577028356e-06,
      "loss": 0.9393,
      "step": 4853
    },
    {
      "epoch": 0.6824124841838887,
      "grad_norm": 1.6656707525253296,
      "learning_rate": 1.8468060225282136e-06,
      "loss": 1.1451,
      "step": 4854
    },
    {
      "epoch": 0.6825530718402925,
      "grad_norm": 1.343040943145752,
      "learning_rate": 1.8915422272200778e-06,
      "loss": 1.109,
      "step": 4855
    },
    {
      "epoch": 0.6826936594966961,
      "grad_norm": 1.7481480836868286,
      "learning_rate": 1.936808630014775e-06,
      "loss": 1.1319,
      "step": 4856
    },
    {
      "epoch": 0.6828342471530999,
      "grad_norm": 1.3604086637496948,
      "learning_rate": 1.9826049862835425e-06,
      "loss": 0.993,
      "step": 4857
    },
    {
      "epoch": 0.6829748348095037,
      "grad_norm": 1.3665908575057983,
      "learning_rate": 2.028931048533489e-06,
      "loss": 1.1978,
      "step": 4858
    },
    {
      "epoch": 0.6831154224659075,
      "grad_norm": 1.3871029615402222,
      "learning_rate": 2.0757865664091435e-06,
      "loss": 1.0338,
      "step": 4859
    },
    {
      "epoch": 0.6832560101223113,
      "grad_norm": 1.3779269456863403,
      "learning_rate": 2.123171286693737e-06,
      "loss": 1.2347,
      "step": 4860
    },
    {
      "epoch": 0.683396597778715,
      "grad_norm": 1.5447933673858643,
      "learning_rate": 2.1710849533105314e-06,
      "loss": 1.1246,
      "step": 4861
    },
    {
      "epoch": 0.6835371854351188,
      "grad_norm": 1.4880609512329102,
      "learning_rate": 2.219527307324398e-06,
      "loss": 1.2627,
      "step": 4862
    },
    {
      "epoch": 0.6836777730915226,
      "grad_norm": 1.5202250480651855,
      "learning_rate": 2.2684980869429717e-06,
      "loss": 1.0265,
      "step": 4863
    },
    {
      "epoch": 0.6838183607479263,
      "grad_norm": 1.786994457244873,
      "learning_rate": 2.3179970275180708e-06,
      "loss": 1.1445,
      "step": 4864
    },
    {
      "epoch": 0.6839589484043301,
      "grad_norm": 1.5646347999572754,
      "learning_rate": 2.3680238615474413e-06,
      "loss": 1.0806,
      "step": 4865
    },
    {
      "epoch": 0.6840995360607338,
      "grad_norm": 1.4246294498443604,
      "learning_rate": 2.4185783186758683e-06,
      "loss": 1.1352,
      "step": 4866
    },
    {
      "epoch": 0.6842401237171376,
      "grad_norm": 1.3302407264709473,
      "learning_rate": 2.46966012569666e-06,
      "loss": 1.0226,
      "step": 4867
    },
    {
      "epoch": 0.6843807113735414,
      "grad_norm": 1.4979499578475952,
      "learning_rate": 2.5212690065534284e-06,
      "loss": 1.0313,
      "step": 4868
    },
    {
      "epoch": 0.6845212990299452,
      "grad_norm": 1.3192657232284546,
      "learning_rate": 2.573404682341185e-06,
      "loss": 1.1196,
      "step": 4869
    },
    {
      "epoch": 0.684661886686349,
      "grad_norm": 1.2882829904556274,
      "learning_rate": 2.6260668713082306e-06,
      "loss": 1.1772,
      "step": 4870
    },
    {
      "epoch": 0.6848024743427527,
      "grad_norm": 1.2195637226104736,
      "learning_rate": 2.679255288857241e-06,
      "loss": 1.0661,
      "step": 4871
    },
    {
      "epoch": 0.6849430619991564,
      "grad_norm": 1.5533759593963623,
      "learning_rate": 2.7329696475471677e-06,
      "loss": 1.1361,
      "step": 4872
    },
    {
      "epoch": 0.6850836496555602,
      "grad_norm": 1.3703093528747559,
      "learning_rate": 2.787209657094736e-06,
      "loss": 0.9354,
      "step": 4873
    },
    {
      "epoch": 0.685224237311964,
      "grad_norm": 1.5518101453781128,
      "learning_rate": 2.841975024375909e-06,
      "loss": 1.0978,
      "step": 4874
    },
    {
      "epoch": 0.6853648249683678,
      "grad_norm": 1.6186693906784058,
      "learning_rate": 2.8972654534273223e-06,
      "loss": 1.1612,
      "step": 4875
    },
    {
      "epoch": 0.6855054126247715,
      "grad_norm": 1.4209234714508057,
      "learning_rate": 2.9530806454483804e-06,
      "loss": 0.9515,
      "step": 4876
    },
    {
      "epoch": 0.6856460002811753,
      "grad_norm": 1.3870513439178467,
      "learning_rate": 3.0094202988023012e-06,
      "loss": 0.9679,
      "step": 4877
    },
    {
      "epoch": 0.6857865879375791,
      "grad_norm": 1.9350088834762573,
      "learning_rate": 3.0662841090182583e-06,
      "loss": 1.0131,
      "step": 4878
    },
    {
      "epoch": 0.6859271755939829,
      "grad_norm": 1.4833595752716064,
      "learning_rate": 3.123671768792491e-06,
      "loss": 1.0554,
      "step": 4879
    },
    {
      "epoch": 0.6860677632503867,
      "grad_norm": 1.5023751258850098,
      "learning_rate": 3.181582967990404e-06,
      "loss": 1.1384,
      "step": 4880
    },
    {
      "epoch": 0.6862083509067903,
      "grad_norm": 1.7142322063446045,
      "learning_rate": 3.2400173936481426e-06,
      "loss": 1.0682,
      "step": 4881
    },
    {
      "epoch": 0.6863489385631941,
      "grad_norm": 1.638667106628418,
      "learning_rate": 3.298974729974169e-06,
      "loss": 1.2382,
      "step": 4882
    },
    {
      "epoch": 0.6864895262195979,
      "grad_norm": 1.3759998083114624,
      "learning_rate": 3.358454658350896e-06,
      "loss": 1.0849,
      "step": 4883
    },
    {
      "epoch": 0.6866301138760017,
      "grad_norm": 1.610538363456726,
      "learning_rate": 3.4184568573367825e-06,
      "loss": 1.0991,
      "step": 4884
    },
    {
      "epoch": 0.6867707015324055,
      "grad_norm": 1.5742664337158203,
      "learning_rate": 3.4789810026676805e-06,
      "loss": 1.1161,
      "step": 4885
    },
    {
      "epoch": 0.6869112891888092,
      "grad_norm": 1.3952248096466064,
      "learning_rate": 3.540026767258775e-06,
      "loss": 0.9984,
      "step": 4886
    },
    {
      "epoch": 0.687051876845213,
      "grad_norm": 1.6505029201507568,
      "learning_rate": 3.601593821206306e-06,
      "loss": 1.1329,
      "step": 4887
    },
    {
      "epoch": 0.6871924645016168,
      "grad_norm": 1.4024291038513184,
      "learning_rate": 3.663681831789323e-06,
      "loss": 1.1517,
      "step": 4888
    },
    {
      "epoch": 0.6873330521580205,
      "grad_norm": 1.4667930603027344,
      "learning_rate": 3.7262904634717042e-06,
      "loss": 1.0541,
      "step": 4889
    },
    {
      "epoch": 0.6874736398144243,
      "grad_norm": 1.6159905195236206,
      "learning_rate": 3.7894193779034915e-06,
      "loss": 0.9491,
      "step": 4890
    },
    {
      "epoch": 0.687614227470828,
      "grad_norm": 1.610817313194275,
      "learning_rate": 3.8530682339231624e-06,
      "loss": 1.1599,
      "step": 4891
    },
    {
      "epoch": 0.6877548151272318,
      "grad_norm": 1.3115578889846802,
      "learning_rate": 3.917236687559433e-06,
      "loss": 1.1472,
      "step": 4892
    },
    {
      "epoch": 0.6878954027836356,
      "grad_norm": 1.3280036449432373,
      "learning_rate": 3.981924392032898e-06,
      "loss": 1.0343,
      "step": 4893
    },
    {
      "epoch": 0.6880359904400394,
      "grad_norm": 1.4637577533721924,
      "learning_rate": 4.047130997757864e-06,
      "loss": 0.9651,
      "step": 4894
    },
    {
      "epoch": 0.6881765780964432,
      "grad_norm": 1.3350638151168823,
      "learning_rate": 4.112856152344702e-06,
      "loss": 0.9595,
      "step": 4895
    },
    {
      "epoch": 0.6883171657528468,
      "grad_norm": 1.706178069114685,
      "learning_rate": 4.179099500601158e-06,
      "loss": 0.964,
      "step": 4896
    },
    {
      "epoch": 0.6884577534092506,
      "grad_norm": 1.468935251235962,
      "learning_rate": 4.245860684534853e-06,
      "loss": 1.0509,
      "step": 4897
    },
    {
      "epoch": 0.6885983410656544,
      "grad_norm": 1.3223748207092285,
      "learning_rate": 4.313139343354589e-06,
      "loss": 0.8702,
      "step": 4898
    },
    {
      "epoch": 0.6887389287220582,
      "grad_norm": 1.3585405349731445,
      "learning_rate": 4.380935113472751e-06,
      "loss": 1.2057,
      "step": 4899
    },
    {
      "epoch": 0.688879516378462,
      "grad_norm": 1.9479950666427612,
      "learning_rate": 4.4492476285073135e-06,
      "loss": 0.9199,
      "step": 4900
    },
    {
      "epoch": 0.6890201040348657,
      "grad_norm": 1.6740117073059082,
      "learning_rate": 4.51807651928351e-06,
      "loss": 1.1804,
      "step": 4901
    },
    {
      "epoch": 0.6891606916912695,
      "grad_norm": 1.4004918336868286,
      "learning_rate": 4.587421413835824e-06,
      "loss": 1.1684,
      "step": 4902
    },
    {
      "epoch": 0.6893012793476733,
      "grad_norm": 1.275559902191162,
      "learning_rate": 4.657281937410462e-06,
      "loss": 1.2093,
      "step": 4903
    },
    {
      "epoch": 0.6894418670040771,
      "grad_norm": 1.2343933582305908,
      "learning_rate": 4.7276577124668934e-06,
      "loss": 1.1817,
      "step": 4904
    },
    {
      "epoch": 0.6895824546604808,
      "grad_norm": 1.6704858541488647,
      "learning_rate": 4.798548358680044e-06,
      "loss": 1.1479,
      "step": 4905
    },
    {
      "epoch": 0.6897230423168845,
      "grad_norm": 1.5787972211837769,
      "learning_rate": 4.869953492942447e-06,
      "loss": 1.0217,
      "step": 4906
    },
    {
      "epoch": 0.6898636299732883,
      "grad_norm": 1.471778392791748,
      "learning_rate": 4.941872729366126e-06,
      "loss": 0.998,
      "step": 4907
    },
    {
      "epoch": 0.6900042176296921,
      "grad_norm": 1.7716648578643799,
      "learning_rate": 5.014305679285047e-06,
      "loss": 0.9712,
      "step": 4908
    },
    {
      "epoch": 0.6901448052860959,
      "grad_norm": 1.3268769979476929,
      "learning_rate": 5.087251951256611e-06,
      "loss": 1.0978,
      "step": 4909
    },
    {
      "epoch": 0.6902853929424997,
      "grad_norm": 1.3656537532806396,
      "learning_rate": 5.1607111510643015e-06,
      "loss": 1.1487,
      "step": 4910
    },
    {
      "epoch": 0.6904259805989034,
      "grad_norm": 1.5626206398010254,
      "learning_rate": 5.2346828817197325e-06,
      "loss": 1.0883,
      "step": 4911
    },
    {
      "epoch": 0.6905665682553072,
      "grad_norm": 1.411900520324707,
      "learning_rate": 5.309166743464544e-06,
      "loss": 1.1233,
      "step": 4912
    },
    {
      "epoch": 0.690707155911711,
      "grad_norm": 1.370728611946106,
      "learning_rate": 5.384162333772769e-06,
      "loss": 1.0077,
      "step": 4913
    },
    {
      "epoch": 0.6908477435681147,
      "grad_norm": 1.7694209814071655,
      "learning_rate": 5.45966924735295e-06,
      "loss": 1.062,
      "step": 4914
    },
    {
      "epoch": 0.6909883312245185,
      "grad_norm": 1.4260045289993286,
      "learning_rate": 5.535687076150264e-06,
      "loss": 1.1748,
      "step": 4915
    },
    {
      "epoch": 0.6911289188809222,
      "grad_norm": 1.2463020086288452,
      "learning_rate": 5.6122154093490645e-06,
      "loss": 1.2363,
      "step": 4916
    },
    {
      "epoch": 0.691269506537326,
      "grad_norm": 1.3865596055984497,
      "learning_rate": 5.6892538333744525e-06,
      "loss": 1.1048,
      "step": 4917
    },
    {
      "epoch": 0.6914100941937298,
      "grad_norm": 1.3396198749542236,
      "learning_rate": 5.7668019318950935e-06,
      "loss": 1.0793,
      "step": 4918
    },
    {
      "epoch": 0.6915506818501336,
      "grad_norm": 1.3271517753601074,
      "learning_rate": 5.844859285825355e-06,
      "loss": 0.8818,
      "step": 4919
    },
    {
      "epoch": 0.6916912695065374,
      "grad_norm": 1.6613982915878296,
      "learning_rate": 5.923425473327349e-06,
      "loss": 1.1887,
      "step": 4920
    },
    {
      "epoch": 0.691831857162941,
      "grad_norm": 1.4619628190994263,
      "learning_rate": 6.002500069813366e-06,
      "loss": 1.0152,
      "step": 4921
    },
    {
      "epoch": 0.6919724448193448,
      "grad_norm": 1.462469458580017,
      "learning_rate": 6.082082647948173e-06,
      "loss": 1.0543,
      "step": 4922
    },
    {
      "epoch": 0.6921130324757486,
      "grad_norm": 1.2704381942749023,
      "learning_rate": 6.16217277765131e-06,
      "loss": 1.1018,
      "step": 4923
    },
    {
      "epoch": 0.6922536201321524,
      "grad_norm": 1.4149150848388672,
      "learning_rate": 6.242770026099387e-06,
      "loss": 1.018,
      "step": 4924
    },
    {
      "epoch": 0.6923942077885562,
      "grad_norm": 1.5590381622314453,
      "learning_rate": 6.323873957728432e-06,
      "loss": 1.1326,
      "step": 4925
    },
    {
      "epoch": 0.6925347954449599,
      "grad_norm": 1.3625110387802124,
      "learning_rate": 6.405484134236217e-06,
      "loss": 1.1023,
      "step": 4926
    },
    {
      "epoch": 0.6926753831013637,
      "grad_norm": 1.3481576442718506,
      "learning_rate": 6.487600114584902e-06,
      "loss": 1.2319,
      "step": 4927
    },
    {
      "epoch": 0.6928159707577675,
      "grad_norm": 1.5925836563110352,
      "learning_rate": 6.570221455002801e-06,
      "loss": 1.0656,
      "step": 4928
    },
    {
      "epoch": 0.6929565584141713,
      "grad_norm": 1.3884526491165161,
      "learning_rate": 6.653347708987345e-06,
      "loss": 1.1777,
      "step": 4929
    },
    {
      "epoch": 0.693097146070575,
      "grad_norm": 1.2569427490234375,
      "learning_rate": 6.736978427307439e-06,
      "loss": 1.1852,
      "step": 4930
    },
    {
      "epoch": 0.6932377337269787,
      "grad_norm": 1.6543160676956177,
      "learning_rate": 6.821113158005587e-06,
      "loss": 1.2967,
      "step": 4931
    },
    {
      "epoch": 0.6933783213833825,
      "grad_norm": 1.375723123550415,
      "learning_rate": 6.9057514464005435e-06,
      "loss": 1.1711,
      "step": 4932
    },
    {
      "epoch": 0.6935189090397863,
      "grad_norm": 1.5626740455627441,
      "learning_rate": 6.990892835089746e-06,
      "loss": 1.0695,
      "step": 4933
    },
    {
      "epoch": 0.6936594966961901,
      "grad_norm": 1.4267864227294922,
      "learning_rate": 7.0765368639517105e-06,
      "loss": 1.0664,
      "step": 4934
    },
    {
      "epoch": 0.6938000843525939,
      "grad_norm": 1.713644027709961,
      "learning_rate": 7.1626830701488566e-06,
      "loss": 1.0623,
      "step": 4935
    },
    {
      "epoch": 0.6939406720089976,
      "grad_norm": 1.309842824935913,
      "learning_rate": 7.249330988129321e-06,
      "loss": 1.0045,
      "step": 4936
    },
    {
      "epoch": 0.6940812596654014,
      "grad_norm": 1.3423738479614258,
      "learning_rate": 7.336480149630087e-06,
      "loss": 1.1984,
      "step": 4937
    },
    {
      "epoch": 0.6942218473218051,
      "grad_norm": 1.5318446159362793,
      "learning_rate": 7.424130083679459e-06,
      "loss": 0.9874,
      "step": 4938
    },
    {
      "epoch": 0.6943624349782089,
      "grad_norm": 1.309071660041809,
      "learning_rate": 7.5122803165992735e-06,
      "loss": 1.1949,
      "step": 4939
    },
    {
      "epoch": 0.6945030226346127,
      "grad_norm": 1.7288638353347778,
      "learning_rate": 7.600930372007698e-06,
      "loss": 1.128,
      "step": 4940
    },
    {
      "epoch": 0.6946436102910164,
      "grad_norm": 1.4791160821914673,
      "learning_rate": 7.690079770821756e-06,
      "loss": 1.0597,
      "step": 4941
    },
    {
      "epoch": 0.6947841979474202,
      "grad_norm": 1.5862897634506226,
      "learning_rate": 7.779728031259859e-06,
      "loss": 1.0841,
      "step": 4942
    },
    {
      "epoch": 0.694924785603824,
      "grad_norm": 1.4965940713882446,
      "learning_rate": 7.869874668844745e-06,
      "loss": 1.054,
      "step": 4943
    },
    {
      "epoch": 0.6950653732602278,
      "grad_norm": 1.4169597625732422,
      "learning_rate": 7.960519196405414e-06,
      "loss": 0.9758,
      "step": 4944
    },
    {
      "epoch": 0.6952059609166316,
      "grad_norm": 1.3030412197113037,
      "learning_rate": 8.051661124080335e-06,
      "loss": 1.1896,
      "step": 4945
    },
    {
      "epoch": 0.6953465485730352,
      "grad_norm": 1.3293075561523438,
      "learning_rate": 8.143299959320195e-06,
      "loss": 1.1278,
      "step": 4946
    },
    {
      "epoch": 0.695487136229439,
      "grad_norm": 1.5375946760177612,
      "learning_rate": 8.23543520688974e-06,
      "loss": 1.1875,
      "step": 4947
    },
    {
      "epoch": 0.6956277238858428,
      "grad_norm": 1.407959222793579,
      "learning_rate": 8.328066368871545e-06,
      "loss": 1.094,
      "step": 4948
    },
    {
      "epoch": 0.6957683115422466,
      "grad_norm": 1.3725889921188354,
      "learning_rate": 8.421192944667855e-06,
      "loss": 1.1208,
      "step": 4949
    },
    {
      "epoch": 0.6959088991986504,
      "grad_norm": 1.5156421661376953,
      "learning_rate": 8.51481443100367e-06,
      "loss": 1.121,
      "step": 4950
    },
    {
      "epoch": 0.6960494868550541,
      "grad_norm": 1.2263962030410767,
      "learning_rate": 8.608930321929398e-06,
      "loss": 1.2018,
      "step": 4951
    },
    {
      "epoch": 0.6961900745114579,
      "grad_norm": 1.7076478004455566,
      "learning_rate": 8.703540108823571e-06,
      "loss": 0.887,
      "step": 4952
    },
    {
      "epoch": 0.6963306621678617,
      "grad_norm": 1.2285683155059814,
      "learning_rate": 8.798643280395558e-06,
      "loss": 1.0297,
      "step": 4953
    },
    {
      "epoch": 0.6964712498242654,
      "grad_norm": 1.5435336828231812,
      "learning_rate": 8.894239322688635e-06,
      "loss": 0.9392,
      "step": 4954
    },
    {
      "epoch": 0.6966118374806692,
      "grad_norm": 1.5103533267974854,
      "learning_rate": 8.990327719082037e-06,
      "loss": 1.037,
      "step": 4955
    },
    {
      "epoch": 0.6967524251370729,
      "grad_norm": 1.5330473184585571,
      "learning_rate": 9.086907950294688e-06,
      "loss": 1.2102,
      "step": 4956
    },
    {
      "epoch": 0.6968930127934767,
      "grad_norm": 1.408018946647644,
      "learning_rate": 9.183979494387263e-06,
      "loss": 0.9182,
      "step": 4957
    },
    {
      "epoch": 0.6970336004498805,
      "grad_norm": 1.4160250425338745,
      "learning_rate": 9.281541826765361e-06,
      "loss": 1.2034,
      "step": 4958
    },
    {
      "epoch": 0.6971741881062843,
      "grad_norm": 1.3084138631820679,
      "learning_rate": 9.379594420182236e-06,
      "loss": 0.893,
      "step": 4959
    },
    {
      "epoch": 0.6973147757626881,
      "grad_norm": 1.3996903896331787,
      "learning_rate": 9.478136744741694e-06,
      "loss": 1.1761,
      "step": 4960
    },
    {
      "epoch": 0.6974553634190918,
      "grad_norm": 1.3063985109329224,
      "learning_rate": 9.577168267900849e-06,
      "loss": 1.1896,
      "step": 4961
    },
    {
      "epoch": 0.6975959510754955,
      "grad_norm": 1.208673119544983,
      "learning_rate": 9.676688454473404e-06,
      "loss": 0.9968,
      "step": 4962
    },
    {
      "epoch": 0.6977365387318993,
      "grad_norm": 1.190706491470337,
      "learning_rate": 9.776696766631665e-06,
      "loss": 1.2813,
      "step": 4963
    },
    {
      "epoch": 0.6978771263883031,
      "grad_norm": 1.3440732955932617,
      "learning_rate": 9.877192663910573e-06,
      "loss": 1.1604,
      "step": 4964
    },
    {
      "epoch": 0.6980177140447069,
      "grad_norm": 1.575817584991455,
      "learning_rate": 9.978175603209816e-06,
      "loss": 0.9985,
      "step": 4965
    },
    {
      "epoch": 0.6981583017011106,
      "grad_norm": 1.6623871326446533,
      "learning_rate": 1.0079645038796759e-05,
      "loss": 1.0497,
      "step": 4966
    },
    {
      "epoch": 0.6982988893575144,
      "grad_norm": 1.7487361431121826,
      "learning_rate": 1.0181600422310134e-05,
      "loss": 1.1212,
      "step": 4967
    },
    {
      "epoch": 0.6984394770139182,
      "grad_norm": 1.443806767463684,
      "learning_rate": 1.0284041202762185e-05,
      "loss": 1.0074,
      "step": 4968
    },
    {
      "epoch": 0.698580064670322,
      "grad_norm": 1.591076374053955,
      "learning_rate": 1.0386966826542056e-05,
      "loss": 1.0727,
      "step": 4969
    },
    {
      "epoch": 0.6987206523267258,
      "grad_norm": 1.5440260171890259,
      "learning_rate": 1.0490376737418706e-05,
      "loss": 1.1079,
      "step": 4970
    },
    {
      "epoch": 0.6988612399831294,
      "grad_norm": 1.3839133977890015,
      "learning_rate": 1.059427037654388e-05,
      "loss": 1.0365,
      "step": 4971
    },
    {
      "epoch": 0.6990018276395332,
      "grad_norm": 1.5883028507232666,
      "learning_rate": 1.0698647182455113e-05,
      "loss": 0.9077,
      "step": 4972
    },
    {
      "epoch": 0.699142415295937,
      "grad_norm": 1.5616477727890015,
      "learning_rate": 1.0803506591079105e-05,
      "loss": 1.1939,
      "step": 4973
    },
    {
      "epoch": 0.6992830029523408,
      "grad_norm": 1.4395846128463745,
      "learning_rate": 1.090884803573392e-05,
      "loss": 1.1671,
      "step": 4974
    },
    {
      "epoch": 0.6994235906087446,
      "grad_norm": 1.4219413995742798,
      "learning_rate": 1.1014670947133154e-05,
      "loss": 1.1285,
      "step": 4975
    },
    {
      "epoch": 0.6995641782651483,
      "grad_norm": 1.2796146869659424,
      "learning_rate": 1.1120974753388147e-05,
      "loss": 1.0766,
      "step": 4976
    },
    {
      "epoch": 0.6997047659215521,
      "grad_norm": 1.3047541379928589,
      "learning_rate": 1.1227758880011475e-05,
      "loss": 1.0575,
      "step": 4977
    },
    {
      "epoch": 0.6998453535779559,
      "grad_norm": 1.451591968536377,
      "learning_rate": 1.1335022749919933e-05,
      "loss": 1.0217,
      "step": 4978
    },
    {
      "epoch": 0.6999859412343596,
      "grad_norm": 1.3717801570892334,
      "learning_rate": 1.1442765783437714e-05,
      "loss": 1.176,
      "step": 4979
    },
    {
      "epoch": 0.7001265288907634,
      "grad_norm": 1.3661319017410278,
      "learning_rate": 1.1550987398299418e-05,
      "loss": 1.0162,
      "step": 4980
    },
    {
      "epoch": 0.7002671165471671,
      "grad_norm": 1.8349156379699707,
      "learning_rate": 1.165968700965363e-05,
      "loss": 1.1374,
      "step": 4981
    },
    {
      "epoch": 0.7004077042035709,
      "grad_norm": 1.5677218437194824,
      "learning_rate": 1.1768864030065174e-05,
      "loss": 1.1755,
      "step": 4982
    },
    {
      "epoch": 0.7005482918599747,
      "grad_norm": 1.4944318532943726,
      "learning_rate": 1.1878517869519402e-05,
      "loss": 1.1223,
      "step": 4983
    },
    {
      "epoch": 0.7006888795163785,
      "grad_norm": 1.523879885673523,
      "learning_rate": 1.1988647935424646e-05,
      "loss": 0.9827,
      "step": 4984
    },
    {
      "epoch": 0.7008294671727823,
      "grad_norm": 1.4091252088546753,
      "learning_rate": 1.2099253632615304e-05,
      "loss": 1.163,
      "step": 4985
    },
    {
      "epoch": 0.700970054829186,
      "grad_norm": 1.3410515785217285,
      "learning_rate": 1.2210334363355913e-05,
      "loss": 1.152,
      "step": 4986
    },
    {
      "epoch": 0.7011106424855897,
      "grad_norm": 1.3381253480911255,
      "learning_rate": 1.2321889527343478e-05,
      "loss": 1.0834,
      "step": 4987
    },
    {
      "epoch": 0.7012512301419935,
      "grad_norm": 1.248390793800354,
      "learning_rate": 1.243391852171114e-05,
      "loss": 1.1429,
      "step": 4988
    },
    {
      "epoch": 0.7013918177983973,
      "grad_norm": 1.5188485383987427,
      "learning_rate": 1.2546420741031396e-05,
      "loss": 0.9806,
      "step": 4989
    },
    {
      "epoch": 0.7015324054548011,
      "grad_norm": 1.5692591667175293,
      "learning_rate": 1.2659395577319221e-05,
      "loss": 1.2571,
      "step": 4990
    },
    {
      "epoch": 0.7016729931112048,
      "grad_norm": 1.438883900642395,
      "learning_rate": 1.2772842420035769e-05,
      "loss": 0.9191,
      "step": 4991
    },
    {
      "epoch": 0.7018135807676086,
      "grad_norm": 1.63066828250885,
      "learning_rate": 1.2886760656091145e-05,
      "loss": 0.9441,
      "step": 4992
    },
    {
      "epoch": 0.7019541684240124,
      "grad_norm": 1.4363404512405396,
      "learning_rate": 1.3001149669847757e-05,
      "loss": 1.0677,
      "step": 4993
    },
    {
      "epoch": 0.7020947560804162,
      "grad_norm": 1.4282046556472778,
      "learning_rate": 1.3116008843124295e-05,
      "loss": 0.9988,
      "step": 4994
    },
    {
      "epoch": 0.70223534373682,
      "grad_norm": 1.3177841901779175,
      "learning_rate": 1.3231337555198287e-05,
      "loss": 1.1169,
      "step": 4995
    },
    {
      "epoch": 0.7023759313932236,
      "grad_norm": 1.5245767831802368,
      "learning_rate": 1.3347135182809888e-05,
      "loss": 1.1572,
      "step": 4996
    },
    {
      "epoch": 0.7025165190496274,
      "grad_norm": 1.398707628250122,
      "learning_rate": 1.3463401100165097e-05,
      "loss": 1.2045,
      "step": 4997
    },
    {
      "epoch": 0.7026571067060312,
      "grad_norm": 1.5760347843170166,
      "learning_rate": 1.3580134678939105e-05,
      "loss": 1.0518,
      "step": 4998
    },
    {
      "epoch": 0.702797694362435,
      "grad_norm": 2.0028562545776367,
      "learning_rate": 1.3697335288280055e-05,
      "loss": 1.1424,
      "step": 4999
    },
    {
      "epoch": 0.7029382820188388,
      "grad_norm": 1.320900797843933,
      "learning_rate": 1.3815002294811963e-05,
      "loss": 1.0824,
      "step": 5000
    },
    {
      "epoch": 0.7029382820188388,
      "eval_loss": 1.141364574432373,
      "eval_runtime": 771.192,
      "eval_samples_per_second": 16.398,
      "eval_steps_per_second": 8.199,
      "step": 5000
    },
    {
      "epoch": 0.7030788696752425,
      "grad_norm": 2.921252965927124,
      "learning_rate": 1.3933135062638059e-05,
      "loss": 1.0814,
      "step": 5001
    },
    {
      "epoch": 0.7032194573316463,
      "grad_norm": 1.404411792755127,
      "learning_rate": 1.4051732953345064e-05,
      "loss": 0.9691,
      "step": 5002
    },
    {
      "epoch": 0.70336004498805,
      "grad_norm": 1.3602023124694824,
      "learning_rate": 1.4170795326005815e-05,
      "loss": 1.0298,
      "step": 5003
    },
    {
      "epoch": 0.7035006326444538,
      "grad_norm": 1.749941110610962,
      "learning_rate": 1.4290321537182693e-05,
      "loss": 1.2724,
      "step": 5004
    },
    {
      "epoch": 0.7036412203008576,
      "grad_norm": 1.4249433279037476,
      "learning_rate": 1.4410310940931971e-05,
      "loss": 1.1439,
      "step": 5005
    },
    {
      "epoch": 0.7037818079572613,
      "grad_norm": 1.5607659816741943,
      "learning_rate": 1.453076288880637e-05,
      "loss": 0.9836,
      "step": 5006
    },
    {
      "epoch": 0.7039223956136651,
      "grad_norm": 1.560676097869873,
      "learning_rate": 1.4651676729858999e-05,
      "loss": 1.1375,
      "step": 5007
    },
    {
      "epoch": 0.7040629832700689,
      "grad_norm": 1.3218481540679932,
      "learning_rate": 1.4773051810646821e-05,
      "loss": 1.1077,
      "step": 5008
    },
    {
      "epoch": 0.7042035709264727,
      "grad_norm": 1.455933690071106,
      "learning_rate": 1.4894887475234066e-05,
      "loss": 0.9806,
      "step": 5009
    },
    {
      "epoch": 0.7043441585828765,
      "grad_norm": 1.4135358333587646,
      "learning_rate": 1.501718306519615e-05,
      "loss": 1.2568,
      "step": 5010
    },
    {
      "epoch": 0.7044847462392801,
      "grad_norm": 1.3584680557250977,
      "learning_rate": 1.5139937919622793e-05,
      "loss": 1.1591,
      "step": 5011
    },
    {
      "epoch": 0.7046253338956839,
      "grad_norm": 1.4605896472930908,
      "learning_rate": 1.5263151375121442e-05,
      "loss": 1.1445,
      "step": 5012
    },
    {
      "epoch": 0.7047659215520877,
      "grad_norm": 1.4781129360198975,
      "learning_rate": 1.5386822765821773e-05,
      "loss": 1.1332,
      "step": 5013
    },
    {
      "epoch": 0.7049065092084915,
      "grad_norm": 1.4243106842041016,
      "learning_rate": 1.5510951423378272e-05,
      "loss": 1.0218,
      "step": 5014
    },
    {
      "epoch": 0.7050470968648953,
      "grad_norm": 1.5827454328536987,
      "learning_rate": 1.563553667697437e-05,
      "loss": 0.9792,
      "step": 5015
    },
    {
      "epoch": 0.705187684521299,
      "grad_norm": 1.4305530786514282,
      "learning_rate": 1.5760577853325975e-05,
      "loss": 1.0405,
      "step": 5016
    },
    {
      "epoch": 0.7053282721777028,
      "grad_norm": 1.9864399433135986,
      "learning_rate": 1.588607427668498e-05,
      "loss": 0.9106,
      "step": 5017
    },
    {
      "epoch": 0.7054688598341066,
      "grad_norm": 1.4926599264144897,
      "learning_rate": 1.6012025268843335e-05,
      "loss": 1.044,
      "step": 5018
    },
    {
      "epoch": 0.7056094474905104,
      "grad_norm": 1.4765375852584839,
      "learning_rate": 1.6138430149136196e-05,
      "loss": 1.1018,
      "step": 5019
    },
    {
      "epoch": 0.7057500351469141,
      "grad_norm": 1.2326396703720093,
      "learning_rate": 1.6265288234445542e-05,
      "loss": 1.2444,
      "step": 5020
    },
    {
      "epoch": 0.7058906228033178,
      "grad_norm": 1.570879578590393,
      "learning_rate": 1.639259883920471e-05,
      "loss": 1.0997,
      "step": 5021
    },
    {
      "epoch": 0.7060312104597216,
      "grad_norm": 1.5067118406295776,
      "learning_rate": 1.6520361275401242e-05,
      "loss": 1.1035,
      "step": 5022
    },
    {
      "epoch": 0.7061717981161254,
      "grad_norm": 1.4903509616851807,
      "learning_rate": 1.66485748525806e-05,
      "loss": 1.1649,
      "step": 5023
    },
    {
      "epoch": 0.7063123857725292,
      "grad_norm": 1.4378719329833984,
      "learning_rate": 1.6777238877850764e-05,
      "loss": 1.0717,
      "step": 5024
    },
    {
      "epoch": 0.706452973428933,
      "grad_norm": 1.4192732572555542,
      "learning_rate": 1.690635265588494e-05,
      "loss": 1.0854,
      "step": 5025
    },
    {
      "epoch": 0.7065935610853367,
      "grad_norm": 1.4854215383529663,
      "learning_rate": 1.7035915488926247e-05,
      "loss": 0.9708,
      "step": 5026
    },
    {
      "epoch": 0.7067341487417405,
      "grad_norm": 1.8635921478271484,
      "learning_rate": 1.7165926676790413e-05,
      "loss": 1.034,
      "step": 5027
    },
    {
      "epoch": 0.7068747363981442,
      "grad_norm": 1.4041444063186646,
      "learning_rate": 1.7296385516870484e-05,
      "loss": 1.1052,
      "step": 5028
    },
    {
      "epoch": 0.707015324054548,
      "grad_norm": 1.3712893724441528,
      "learning_rate": 1.7427291304140502e-05,
      "loss": 1.0678,
      "step": 5029
    },
    {
      "epoch": 0.7071559117109518,
      "grad_norm": 1.4774610996246338,
      "learning_rate": 1.755864333115892e-05,
      "loss": 1.1693,
      "step": 5030
    },
    {
      "epoch": 0.7072964993673555,
      "grad_norm": 1.4219928979873657,
      "learning_rate": 1.7690440888072292e-05,
      "loss": 1.0206,
      "step": 5031
    },
    {
      "epoch": 0.7074370870237593,
      "grad_norm": 1.7145816087722778,
      "learning_rate": 1.7822683262620077e-05,
      "loss": 1.2017,
      "step": 5032
    },
    {
      "epoch": 0.7075776746801631,
      "grad_norm": 1.6930980682373047,
      "learning_rate": 1.7955369740137307e-05,
      "loss": 1.0946,
      "step": 5033
    },
    {
      "epoch": 0.7077182623365669,
      "grad_norm": 1.4460967779159546,
      "learning_rate": 1.8088499603559515e-05,
      "loss": 0.9596,
      "step": 5034
    },
    {
      "epoch": 0.7078588499929707,
      "grad_norm": 1.4091733694076538,
      "learning_rate": 1.8222072133425504e-05,
      "loss": 1.2007,
      "step": 5035
    },
    {
      "epoch": 0.7079994376493743,
      "grad_norm": 1.4032169580459595,
      "learning_rate": 1.8356086607882082e-05,
      "loss": 0.9837,
      "step": 5036
    },
    {
      "epoch": 0.7081400253057781,
      "grad_norm": 1.2292309999465942,
      "learning_rate": 1.849054230268792e-05,
      "loss": 1.1476,
      "step": 5037
    },
    {
      "epoch": 0.7082806129621819,
      "grad_norm": 1.7575289011001587,
      "learning_rate": 1.862543849121703e-05,
      "loss": 1.1933,
      "step": 5038
    },
    {
      "epoch": 0.7084212006185857,
      "grad_norm": 1.3454101085662842,
      "learning_rate": 1.8760774444462603e-05,
      "loss": 1.3912,
      "step": 5039
    },
    {
      "epoch": 0.7085617882749895,
      "grad_norm": 1.3507156372070312,
      "learning_rate": 1.88965494310419e-05,
      "loss": 1.0795,
      "step": 5040
    },
    {
      "epoch": 0.7087023759313932,
      "grad_norm": 1.691261887550354,
      "learning_rate": 1.903276271719908e-05,
      "loss": 1.0935,
      "step": 5041
    },
    {
      "epoch": 0.708842963587797,
      "grad_norm": 1.258744239807129,
      "learning_rate": 1.9169413566809802e-05,
      "loss": 1.1848,
      "step": 5042
    },
    {
      "epoch": 0.7089835512442008,
      "grad_norm": 1.3457298278808594,
      "learning_rate": 1.930650124138501e-05,
      "loss": 1.0482,
      "step": 5043
    },
    {
      "epoch": 0.7091241389006046,
      "grad_norm": 1.5675537586212158,
      "learning_rate": 1.944402500007487e-05,
      "loss": 1.0465,
      "step": 5044
    },
    {
      "epoch": 0.7092647265570083,
      "grad_norm": 1.7646533250808716,
      "learning_rate": 1.958198409967329e-05,
      "loss": 0.996,
      "step": 5045
    },
    {
      "epoch": 0.709405314213412,
      "grad_norm": 1.2808046340942383,
      "learning_rate": 1.9720377794620824e-05,
      "loss": 0.9918,
      "step": 5046
    },
    {
      "epoch": 0.7095459018698158,
      "grad_norm": 1.482823371887207,
      "learning_rate": 1.9859205337009766e-05,
      "loss": 0.9829,
      "step": 5047
    },
    {
      "epoch": 0.7096864895262196,
      "grad_norm": 1.3885877132415771,
      "learning_rate": 1.9998465976587988e-05,
      "loss": 1.2135,
      "step": 5048
    },
    {
      "epoch": 0.7098270771826234,
      "grad_norm": 1.5537773370742798,
      "learning_rate": 2.013815896076259e-05,
      "loss": 1.023,
      "step": 5049
    },
    {
      "epoch": 0.7099676648390272,
      "grad_norm": 1.4322688579559326,
      "learning_rate": 2.0278283534603915e-05,
      "loss": 0.9707,
      "step": 5050
    },
    {
      "epoch": 0.7101082524954309,
      "grad_norm": 1.4919099807739258,
      "learning_rate": 2.0418838940850515e-05,
      "loss": 1.1296,
      "step": 5051
    },
    {
      "epoch": 0.7102488401518346,
      "grad_norm": 1.4127334356307983,
      "learning_rate": 2.055982441991211e-05,
      "loss": 1.0238,
      "step": 5052
    },
    {
      "epoch": 0.7103894278082384,
      "grad_norm": 1.2954659461975098,
      "learning_rate": 2.0701239209874755e-05,
      "loss": 1.1308,
      "step": 5053
    },
    {
      "epoch": 0.7105300154646422,
      "grad_norm": 1.5916810035705566,
      "learning_rate": 2.084308254650379e-05,
      "loss": 1.0773,
      "step": 5054
    },
    {
      "epoch": 0.710670603121046,
      "grad_norm": 1.2937370538711548,
      "learning_rate": 2.0985353663248907e-05,
      "loss": 1.0939,
      "step": 5055
    },
    {
      "epoch": 0.7108111907774497,
      "grad_norm": 1.322975993156433,
      "learning_rate": 2.1128051791248192e-05,
      "loss": 1.0298,
      "step": 5056
    },
    {
      "epoch": 0.7109517784338535,
      "grad_norm": 1.3694440126419067,
      "learning_rate": 2.1271176159331907e-05,
      "loss": 1.0945,
      "step": 5057
    },
    {
      "epoch": 0.7110923660902573,
      "grad_norm": 1.310219407081604,
      "learning_rate": 2.141472599402644e-05,
      "loss": 1.0216,
      "step": 5058
    },
    {
      "epoch": 0.7112329537466611,
      "grad_norm": 1.4444305896759033,
      "learning_rate": 2.1558700519559572e-05,
      "loss": 1.0777,
      "step": 5059
    },
    {
      "epoch": 0.7113735414030649,
      "grad_norm": 1.2830818891525269,
      "learning_rate": 2.17030989578635e-05,
      "loss": 1.0599,
      "step": 5060
    },
    {
      "epoch": 0.7115141290594685,
      "grad_norm": 1.2233829498291016,
      "learning_rate": 2.1847920528579634e-05,
      "loss": 1.0878,
      "step": 5061
    },
    {
      "epoch": 0.7116547167158723,
      "grad_norm": 1.6383781433105469,
      "learning_rate": 2.19931644490627e-05,
      "loss": 1.0903,
      "step": 5062
    },
    {
      "epoch": 0.7117953043722761,
      "grad_norm": 1.5202101469039917,
      "learning_rate": 2.2138829934384807e-05,
      "loss": 1.1201,
      "step": 5063
    },
    {
      "epoch": 0.7119358920286799,
      "grad_norm": 1.4271974563598633,
      "learning_rate": 2.2284916197340345e-05,
      "loss": 1.0491,
      "step": 5064
    },
    {
      "epoch": 0.7120764796850837,
      "grad_norm": 1.3867093324661255,
      "learning_rate": 2.2431422448449014e-05,
      "loss": 1.0811,
      "step": 5065
    },
    {
      "epoch": 0.7122170673414874,
      "grad_norm": 1.3822288513183594,
      "learning_rate": 2.2578347895961194e-05,
      "loss": 0.9684,
      "step": 5066
    },
    {
      "epoch": 0.7123576549978912,
      "grad_norm": 1.500529408454895,
      "learning_rate": 2.272569174586203e-05,
      "loss": 1.2278,
      "step": 5067
    },
    {
      "epoch": 0.712498242654295,
      "grad_norm": 1.4182180166244507,
      "learning_rate": 2.2873453201875218e-05,
      "loss": 1.0373,
      "step": 5068
    },
    {
      "epoch": 0.7126388303106987,
      "grad_norm": 1.4086917638778687,
      "learning_rate": 2.302163146546772e-05,
      "loss": 1.2571,
      "step": 5069
    },
    {
      "epoch": 0.7127794179671025,
      "grad_norm": 1.4072638750076294,
      "learning_rate": 2.317022573585401e-05,
      "loss": 1.0523,
      "step": 5070
    },
    {
      "epoch": 0.7129200056235062,
      "grad_norm": 1.6621103286743164,
      "learning_rate": 2.3319235210000245e-05,
      "loss": 1.0472,
      "step": 5071
    },
    {
      "epoch": 0.71306059327991,
      "grad_norm": 1.6470837593078613,
      "learning_rate": 2.3468659082629208e-05,
      "loss": 1.0403,
      "step": 5072
    },
    {
      "epoch": 0.7132011809363138,
      "grad_norm": 1.7782124280929565,
      "learning_rate": 2.3618496546223467e-05,
      "loss": 1.1622,
      "step": 5073
    },
    {
      "epoch": 0.7133417685927176,
      "grad_norm": 1.3854808807373047,
      "learning_rate": 2.376874679103085e-05,
      "loss": 0.9912,
      "step": 5074
    },
    {
      "epoch": 0.7134823562491214,
      "grad_norm": 1.9716534614562988,
      "learning_rate": 2.3919409005068626e-05,
      "loss": 0.9869,
      "step": 5075
    },
    {
      "epoch": 0.713622943905525,
      "grad_norm": 1.594011902809143,
      "learning_rate": 2.407048237412738e-05,
      "loss": 1.0166,
      "step": 5076
    },
    {
      "epoch": 0.7137635315619288,
      "grad_norm": 1.4897948503494263,
      "learning_rate": 2.4221966081775817e-05,
      "loss": 1.1136,
      "step": 5077
    },
    {
      "epoch": 0.7139041192183326,
      "grad_norm": 1.3209940195083618,
      "learning_rate": 2.437385930936511e-05,
      "loss": 1.3008,
      "step": 5078
    },
    {
      "epoch": 0.7140447068747364,
      "grad_norm": 1.4511011838912964,
      "learning_rate": 2.4526161236033295e-05,
      "loss": 0.9831,
      "step": 5079
    },
    {
      "epoch": 0.7141852945311402,
      "grad_norm": 1.413233757019043,
      "learning_rate": 2.4678871038709727e-05,
      "loss": 1.084,
      "step": 5080
    },
    {
      "epoch": 0.7143258821875439,
      "grad_norm": 1.5195097923278809,
      "learning_rate": 2.48319878921195e-05,
      "loss": 0.9602,
      "step": 5081
    },
    {
      "epoch": 0.7144664698439477,
      "grad_norm": 1.5137646198272705,
      "learning_rate": 2.498551096878782e-05,
      "loss": 1.072,
      "step": 5082
    },
    {
      "epoch": 0.7146070575003515,
      "grad_norm": 1.4378291368484497,
      "learning_rate": 2.5139439439045022e-05,
      "loss": 1.0683,
      "step": 5083
    },
    {
      "epoch": 0.7147476451567553,
      "grad_norm": 1.3300946950912476,
      "learning_rate": 2.529377247102993e-05,
      "loss": 1.2189,
      "step": 5084
    },
    {
      "epoch": 0.714888232813159,
      "grad_norm": 1.6513850688934326,
      "learning_rate": 2.5448509230695407e-05,
      "loss": 1.02,
      "step": 5085
    },
    {
      "epoch": 0.7150288204695627,
      "grad_norm": 1.4555330276489258,
      "learning_rate": 2.560364888181267e-05,
      "loss": 1.0585,
      "step": 5086
    },
    {
      "epoch": 0.7151694081259665,
      "grad_norm": 1.6769942045211792,
      "learning_rate": 2.575919058597531e-05,
      "loss": 1.1365,
      "step": 5087
    },
    {
      "epoch": 0.7153099957823703,
      "grad_norm": 1.74495530128479,
      "learning_rate": 2.591513350260426e-05,
      "loss": 1.054,
      "step": 5088
    },
    {
      "epoch": 0.7154505834387741,
      "grad_norm": 1.4007655382156372,
      "learning_rate": 2.607147678895222e-05,
      "loss": 1.0112,
      "step": 5089
    },
    {
      "epoch": 0.7155911710951779,
      "grad_norm": 1.4773788452148438,
      "learning_rate": 2.6228219600108084e-05,
      "loss": 1.1799,
      "step": 5090
    },
    {
      "epoch": 0.7157317587515816,
      "grad_norm": 1.2975196838378906,
      "learning_rate": 2.6385361089002092e-05,
      "loss": 1.0219,
      "step": 5091
    },
    {
      "epoch": 0.7158723464079854,
      "grad_norm": 1.5171356201171875,
      "learning_rate": 2.6542900406409267e-05,
      "loss": 1.0044,
      "step": 5092
    },
    {
      "epoch": 0.7160129340643892,
      "grad_norm": 1.717629075050354,
      "learning_rate": 2.6700836700955023e-05,
      "loss": 1.0921,
      "step": 5093
    },
    {
      "epoch": 0.7161535217207929,
      "grad_norm": 1.4404336214065552,
      "learning_rate": 2.6859169119119663e-05,
      "loss": 1.0503,
      "step": 5094
    },
    {
      "epoch": 0.7162941093771967,
      "grad_norm": 1.4715195894241333,
      "learning_rate": 2.7017896805242425e-05,
      "loss": 0.8386,
      "step": 5095
    },
    {
      "epoch": 0.7164346970336004,
      "grad_norm": 1.3613841533660889,
      "learning_rate": 2.717701890152653e-05,
      "loss": 1.23,
      "step": 5096
    },
    {
      "epoch": 0.7165752846900042,
      "grad_norm": 1.4388577938079834,
      "learning_rate": 2.7336534548043735e-05,
      "loss": 0.9571,
      "step": 5097
    },
    {
      "epoch": 0.716715872346408,
      "grad_norm": 1.5045839548110962,
      "learning_rate": 2.7496442882738972e-05,
      "loss": 1.1862,
      "step": 5098
    },
    {
      "epoch": 0.7168564600028118,
      "grad_norm": 1.5927168130874634,
      "learning_rate": 2.765674304143501e-05,
      "loss": 1.1197,
      "step": 5099
    },
    {
      "epoch": 0.7169970476592156,
      "grad_norm": 1.5803582668304443,
      "learning_rate": 2.7817434157837118e-05,
      "loss": 1.0155,
      "step": 5100
    },
    {
      "epoch": 0.7171376353156192,
      "grad_norm": 1.4054594039916992,
      "learning_rate": 2.7978515363537628e-05,
      "loss": 0.9355,
      "step": 5101
    },
    {
      "epoch": 0.717278222972023,
      "grad_norm": 1.7124093770980835,
      "learning_rate": 2.813998578802124e-05,
      "loss": 1.1495,
      "step": 5102
    },
    {
      "epoch": 0.7174188106284268,
      "grad_norm": 1.5772989988327026,
      "learning_rate": 2.8301844558668377e-05,
      "loss": 0.9562,
      "step": 5103
    },
    {
      "epoch": 0.7175593982848306,
      "grad_norm": 1.535008430480957,
      "learning_rate": 2.846409080076161e-05,
      "loss": 0.9287,
      "step": 5104
    },
    {
      "epoch": 0.7176999859412344,
      "grad_norm": 1.448206901550293,
      "learning_rate": 2.8626723637489073e-05,
      "loss": 1.168,
      "step": 5105
    },
    {
      "epoch": 0.7178405735976381,
      "grad_norm": 1.3722233772277832,
      "learning_rate": 2.8789742189949742e-05,
      "loss": 0.9557,
      "step": 5106
    },
    {
      "epoch": 0.7179811612540419,
      "grad_norm": 1.2610207796096802,
      "learning_rate": 2.895314557715816e-05,
      "loss": 1.1261,
      "step": 5107
    },
    {
      "epoch": 0.7181217489104457,
      "grad_norm": 1.7515277862548828,
      "learning_rate": 2.91169329160491e-05,
      "loss": 1.0638,
      "step": 5108
    },
    {
      "epoch": 0.7182623365668495,
      "grad_norm": 1.4402310848236084,
      "learning_rate": 2.9281103321482273e-05,
      "loss": 0.9887,
      "step": 5109
    },
    {
      "epoch": 0.7184029242232532,
      "grad_norm": 1.3183423280715942,
      "learning_rate": 2.9445655906247728e-05,
      "loss": 1.0038,
      "step": 5110
    },
    {
      "epoch": 0.7185435118796569,
      "grad_norm": 1.498550295829773,
      "learning_rate": 2.9610589781069254e-05,
      "loss": 1.0943,
      "step": 5111
    },
    {
      "epoch": 0.7186840995360607,
      "grad_norm": 1.6317098140716553,
      "learning_rate": 2.9775904054610936e-05,
      "loss": 1.1004,
      "step": 5112
    },
    {
      "epoch": 0.7188246871924645,
      "grad_norm": 1.2800447940826416,
      "learning_rate": 2.994159783348065e-05,
      "loss": 1.0246,
      "step": 5113
    },
    {
      "epoch": 0.7189652748488683,
      "grad_norm": 1.4673877954483032,
      "learning_rate": 3.0107670222235405e-05,
      "loss": 1.1634,
      "step": 5114
    },
    {
      "epoch": 0.7191058625052721,
      "grad_norm": 1.601762056350708,
      "learning_rate": 3.0274120323386202e-05,
      "loss": 1.111,
      "step": 5115
    },
    {
      "epoch": 0.7192464501616758,
      "grad_norm": 1.1524897813796997,
      "learning_rate": 3.044094723740273e-05,
      "loss": 1.2334,
      "step": 5116
    },
    {
      "epoch": 0.7193870378180796,
      "grad_norm": 1.3889868259429932,
      "learning_rate": 3.0608150062718235e-05,
      "loss": 1.1294,
      "step": 5117
    },
    {
      "epoch": 0.7195276254744833,
      "grad_norm": 1.346666932106018,
      "learning_rate": 3.0775727895734954e-05,
      "loss": 0.9094,
      "step": 5118
    },
    {
      "epoch": 0.7196682131308871,
      "grad_norm": 1.5472166538238525,
      "learning_rate": 3.094367983082772e-05,
      "loss": 1.0214,
      "step": 5119
    },
    {
      "epoch": 0.7198088007872909,
      "grad_norm": 1.690266489982605,
      "learning_rate": 3.111200496035005e-05,
      "loss": 1.0553,
      "step": 5120
    },
    {
      "epoch": 0.7199493884436946,
      "grad_norm": 1.325559139251709,
      "learning_rate": 3.128070237463904e-05,
      "loss": 1.2053,
      "step": 5121
    },
    {
      "epoch": 0.7200899761000984,
      "grad_norm": 1.5367271900177002,
      "learning_rate": 3.144977116201892e-05,
      "loss": 1.2625,
      "step": 5122
    },
    {
      "epoch": 0.7202305637565022,
      "grad_norm": 1.5380946397781372,
      "learning_rate": 3.161921040880794e-05,
      "loss": 0.9639,
      "step": 5123
    },
    {
      "epoch": 0.720371151412906,
      "grad_norm": 1.455798625946045,
      "learning_rate": 3.1789019199321715e-05,
      "loss": 1.2015,
      "step": 5124
    },
    {
      "epoch": 0.7205117390693098,
      "grad_norm": 1.4700946807861328,
      "learning_rate": 3.195919661587893e-05,
      "loss": 1.0272,
      "step": 5125
    },
    {
      "epoch": 0.7206523267257134,
      "grad_norm": 1.510307788848877,
      "learning_rate": 3.212974173880615e-05,
      "loss": 1.0911,
      "step": 5126
    },
    {
      "epoch": 0.7207929143821172,
      "grad_norm": 1.444344401359558,
      "learning_rate": 3.230065364644276e-05,
      "loss": 0.8661,
      "step": 5127
    },
    {
      "epoch": 0.720933502038521,
      "grad_norm": 1.534865379333496,
      "learning_rate": 3.247193141514585e-05,
      "loss": 1.1059,
      "step": 5128
    },
    {
      "epoch": 0.7210740896949248,
      "grad_norm": 1.663016676902771,
      "learning_rate": 3.2643574119295825e-05,
      "loss": 1.0328,
      "step": 5129
    },
    {
      "epoch": 0.7212146773513286,
      "grad_norm": 1.5684951543807983,
      "learning_rate": 3.281558083130003e-05,
      "loss": 1.0684,
      "step": 5130
    },
    {
      "epoch": 0.7213552650077323,
      "grad_norm": 1.5656399726867676,
      "learning_rate": 3.2987950621599506e-05,
      "loss": 1.1641,
      "step": 5131
    },
    {
      "epoch": 0.7214958526641361,
      "grad_norm": 1.46066415309906,
      "learning_rate": 3.316068255867265e-05,
      "loss": 1.168,
      "step": 5132
    },
    {
      "epoch": 0.7216364403205399,
      "grad_norm": 1.4967045783996582,
      "learning_rate": 3.333377570904086e-05,
      "loss": 1.1019,
      "step": 5133
    },
    {
      "epoch": 0.7217770279769437,
      "grad_norm": 1.5539056062698364,
      "learning_rate": 3.3507229137273476e-05,
      "loss": 1.1405,
      "step": 5134
    },
    {
      "epoch": 0.7219176156333474,
      "grad_norm": 1.2928998470306396,
      "learning_rate": 3.368104190599283e-05,
      "loss": 1.0688,
      "step": 5135
    },
    {
      "epoch": 0.7220582032897511,
      "grad_norm": 1.4684479236602783,
      "learning_rate": 3.3855213075879125e-05,
      "loss": 1.1063,
      "step": 5136
    },
    {
      "epoch": 0.7221987909461549,
      "grad_norm": 1.4775816202163696,
      "learning_rate": 3.402974170567629e-05,
      "loss": 0.9497,
      "step": 5137
    },
    {
      "epoch": 0.7223393786025587,
      "grad_norm": 1.2297914028167725,
      "learning_rate": 3.4204626852195476e-05,
      "loss": 1.3326,
      "step": 5138
    },
    {
      "epoch": 0.7224799662589625,
      "grad_norm": 1.7451540231704712,
      "learning_rate": 3.437986757032213e-05,
      "loss": 1.0723,
      "step": 5139
    },
    {
      "epoch": 0.7226205539153663,
      "grad_norm": 1.451252818107605,
      "learning_rate": 3.455546291301969e-05,
      "loss": 0.916,
      "step": 5140
    },
    {
      "epoch": 0.72276114157177,
      "grad_norm": 1.5200122594833374,
      "learning_rate": 3.473141193133471e-05,
      "loss": 0.9239,
      "step": 5141
    },
    {
      "epoch": 0.7229017292281738,
      "grad_norm": 1.4452885389328003,
      "learning_rate": 3.4907713674403255e-05,
      "loss": 1.06,
      "step": 5142
    },
    {
      "epoch": 0.7230423168845775,
      "grad_norm": 1.349530577659607,
      "learning_rate": 3.5084367189454635e-05,
      "loss": 1.1144,
      "step": 5143
    },
    {
      "epoch": 0.7231829045409813,
      "grad_norm": 1.6087793111801147,
      "learning_rate": 3.526137152181723e-05,
      "loss": 1.0064,
      "step": 5144
    },
    {
      "epoch": 0.7233234921973851,
      "grad_norm": 1.643750548362732,
      "learning_rate": 3.543872571492357e-05,
      "loss": 1.2159,
      "step": 5145
    },
    {
      "epoch": 0.7234640798537888,
      "grad_norm": 1.3270338773727417,
      "learning_rate": 3.5616428810315306e-05,
      "loss": 1.2163,
      "step": 5146
    },
    {
      "epoch": 0.7236046675101926,
      "grad_norm": 1.4748362302780151,
      "learning_rate": 3.579447984764896e-05,
      "loss": 1.0708,
      "step": 5147
    },
    {
      "epoch": 0.7237452551665964,
      "grad_norm": 1.4146596193313599,
      "learning_rate": 3.597287786470043e-05,
      "loss": 1.1576,
      "step": 5148
    },
    {
      "epoch": 0.7238858428230002,
      "grad_norm": 1.4727798700332642,
      "learning_rate": 3.615162189737006e-05,
      "loss": 1.1099,
      "step": 5149
    },
    {
      "epoch": 0.724026430479404,
      "grad_norm": 1.5401214361190796,
      "learning_rate": 3.6330710979689076e-05,
      "loss": 1.1022,
      "step": 5150
    },
    {
      "epoch": 0.7241670181358076,
      "grad_norm": 1.4448827505111694,
      "learning_rate": 3.6510144143823445e-05,
      "loss": 0.9661,
      "step": 5151
    },
    {
      "epoch": 0.7243076057922114,
      "grad_norm": 1.6585123538970947,
      "learning_rate": 3.668992042007976e-05,
      "loss": 1.0055,
      "step": 5152
    },
    {
      "epoch": 0.7244481934486152,
      "grad_norm": 1.6157805919647217,
      "learning_rate": 3.6870038836910404e-05,
      "loss": 1.0071,
      "step": 5153
    },
    {
      "epoch": 0.724588781105019,
      "grad_norm": 1.384234070777893,
      "learning_rate": 3.705049842091858e-05,
      "loss": 1.2116,
      "step": 5154
    },
    {
      "epoch": 0.7247293687614228,
      "grad_norm": 1.5986098051071167,
      "learning_rate": 3.72312981968642e-05,
      "loss": 1.1031,
      "step": 5155
    },
    {
      "epoch": 0.7248699564178265,
      "grad_norm": 1.358021855354309,
      "learning_rate": 3.741243718766841e-05,
      "loss": 1.0434,
      "step": 5156
    },
    {
      "epoch": 0.7250105440742303,
      "grad_norm": 1.9547767639160156,
      "learning_rate": 3.7593914414418795e-05,
      "loss": 1.0219,
      "step": 5157
    },
    {
      "epoch": 0.7251511317306341,
      "grad_norm": 1.8195929527282715,
      "learning_rate": 3.7775728896375816e-05,
      "loss": 0.9131,
      "step": 5158
    },
    {
      "epoch": 0.7252917193870378,
      "grad_norm": 1.6590352058410645,
      "learning_rate": 3.795787965097698e-05,
      "loss": 1.1712,
      "step": 5159
    },
    {
      "epoch": 0.7254323070434416,
      "grad_norm": 1.4661225080490112,
      "learning_rate": 3.8140365693842054e-05,
      "loss": 1.1105,
      "step": 5160
    },
    {
      "epoch": 0.7255728946998453,
      "grad_norm": 1.2051458358764648,
      "learning_rate": 3.8323186038779677e-05,
      "loss": 1.0182,
      "step": 5161
    },
    {
      "epoch": 0.7257134823562491,
      "grad_norm": 1.5070620775222778,
      "learning_rate": 3.850633969779131e-05,
      "loss": 1.0924,
      "step": 5162
    },
    {
      "epoch": 0.7258540700126529,
      "grad_norm": 1.2599800825119019,
      "learning_rate": 3.868982568107725e-05,
      "loss": 1.0049,
      "step": 5163
    },
    {
      "epoch": 0.7259946576690567,
      "grad_norm": 1.8337546586990356,
      "learning_rate": 3.887364299704185e-05,
      "loss": 1.1996,
      "step": 5164
    },
    {
      "epoch": 0.7261352453254605,
      "grad_norm": 1.5559680461883545,
      "learning_rate": 3.90577906522987e-05,
      "loss": 1.0626,
      "step": 5165
    },
    {
      "epoch": 0.7262758329818642,
      "grad_norm": 1.6130889654159546,
      "learning_rate": 3.924226765167665e-05,
      "loss": 1.0393,
      "step": 5166
    },
    {
      "epoch": 0.726416420638268,
      "grad_norm": 1.3451756238937378,
      "learning_rate": 3.942707299822437e-05,
      "loss": 1.2778,
      "step": 5167
    },
    {
      "epoch": 0.7265570082946717,
      "grad_norm": 1.284605622291565,
      "learning_rate": 3.96122056932157e-05,
      "loss": 1.0482,
      "step": 5168
    },
    {
      "epoch": 0.7266975959510755,
      "grad_norm": 1.5336910486221313,
      "learning_rate": 3.979766473615623e-05,
      "loss": 1.04,
      "step": 5169
    },
    {
      "epoch": 0.7268381836074793,
      "grad_norm": 1.6023290157318115,
      "learning_rate": 3.998344912478733e-05,
      "loss": 1.2351,
      "step": 5170
    },
    {
      "epoch": 0.726978771263883,
      "grad_norm": 1.3640739917755127,
      "learning_rate": 4.016955785509234e-05,
      "loss": 1.0787,
      "step": 5171
    },
    {
      "epoch": 0.7271193589202868,
      "grad_norm": 1.4293287992477417,
      "learning_rate": 4.035598992130172e-05,
      "loss": 1.1902,
      "step": 5172
    },
    {
      "epoch": 0.7272599465766906,
      "grad_norm": 1.5437184572219849,
      "learning_rate": 4.054274431589844e-05,
      "loss": 1.1501,
      "step": 5173
    },
    {
      "epoch": 0.7274005342330944,
      "grad_norm": 1.4636178016662598,
      "learning_rate": 4.072982002962398e-05,
      "loss": 1.1431,
      "step": 5174
    },
    {
      "epoch": 0.7275411218894982,
      "grad_norm": 1.6322813034057617,
      "learning_rate": 4.0917216051483054e-05,
      "loss": 1.0938,
      "step": 5175
    },
    {
      "epoch": 0.7276817095459018,
      "grad_norm": 1.5582051277160645,
      "learning_rate": 4.110493136874892e-05,
      "loss": 0.9558,
      "step": 5176
    },
    {
      "epoch": 0.7278222972023056,
      "grad_norm": 1.4727298021316528,
      "learning_rate": 4.129296496697019e-05,
      "loss": 1.1515,
      "step": 5177
    },
    {
      "epoch": 0.7279628848587094,
      "grad_norm": 1.4713739156723022,
      "learning_rate": 4.1481315829974964e-05,
      "loss": 1.2103,
      "step": 5178
    },
    {
      "epoch": 0.7281034725151132,
      "grad_norm": 1.272148847579956,
      "learning_rate": 4.166998293987636e-05,
      "loss": 1.2469,
      "step": 5179
    },
    {
      "epoch": 0.728244060171517,
      "grad_norm": 1.4540225267410278,
      "learning_rate": 4.185896527707929e-05,
      "loss": 1.0139,
      "step": 5180
    },
    {
      "epoch": 0.7283846478279207,
      "grad_norm": 1.55671226978302,
      "learning_rate": 4.2048261820284394e-05,
      "loss": 1.1012,
      "step": 5181
    },
    {
      "epoch": 0.7285252354843245,
      "grad_norm": 1.511956810951233,
      "learning_rate": 4.2237871546495004e-05,
      "loss": 1.0108,
      "step": 5182
    },
    {
      "epoch": 0.7286658231407283,
      "grad_norm": 1.5407170057296753,
      "learning_rate": 4.242779343102108e-05,
      "loss": 1.1732,
      "step": 5183
    },
    {
      "epoch": 0.728806410797132,
      "grad_norm": 1.5406509637832642,
      "learning_rate": 4.261802644748605e-05,
      "loss": 1.0743,
      "step": 5184
    },
    {
      "epoch": 0.7289469984535358,
      "grad_norm": 1.6264762878417969,
      "learning_rate": 4.280856956783219e-05,
      "loss": 1.2639,
      "step": 5185
    },
    {
      "epoch": 0.7290875861099395,
      "grad_norm": 1.5373024940490723,
      "learning_rate": 4.2999421762325666e-05,
      "loss": 0.9919,
      "step": 5186
    },
    {
      "epoch": 0.7292281737663433,
      "grad_norm": 1.5172511339187622,
      "learning_rate": 4.3190581999561864e-05,
      "loss": 0.9366,
      "step": 5187
    },
    {
      "epoch": 0.7293687614227471,
      "grad_norm": 1.5667402744293213,
      "learning_rate": 4.3382049246472344e-05,
      "loss": 1.152,
      "step": 5188
    },
    {
      "epoch": 0.7295093490791509,
      "grad_norm": 1.6167898178100586,
      "learning_rate": 4.357382246832877e-05,
      "loss": 1.0644,
      "step": 5189
    },
    {
      "epoch": 0.7296499367355547,
      "grad_norm": 2.0274605751037598,
      "learning_rate": 4.3765900628750037e-05,
      "loss": 1.0915,
      "step": 5190
    },
    {
      "epoch": 0.7297905243919584,
      "grad_norm": 1.5673877000808716,
      "learning_rate": 4.3958282689706145e-05,
      "loss": 1.1608,
      "step": 5191
    },
    {
      "epoch": 0.7299311120483621,
      "grad_norm": 1.4108526706695557,
      "learning_rate": 4.4150967611525215e-05,
      "loss": 1.146,
      "step": 5192
    },
    {
      "epoch": 0.7300716997047659,
      "grad_norm": 1.3137731552124023,
      "learning_rate": 4.4343954352898986e-05,
      "loss": 1.0727,
      "step": 5193
    },
    {
      "epoch": 0.7302122873611697,
      "grad_norm": 1.4181054830551147,
      "learning_rate": 4.453724187088775e-05,
      "loss": 1.0271,
      "step": 5194
    },
    {
      "epoch": 0.7303528750175735,
      "grad_norm": 1.2049615383148193,
      "learning_rate": 4.473082912092592e-05,
      "loss": 1.0495,
      "step": 5195
    },
    {
      "epoch": 0.7304934626739772,
      "grad_norm": 1.536152720451355,
      "learning_rate": 4.492471505682898e-05,
      "loss": 1.2221,
      "step": 5196
    },
    {
      "epoch": 0.730634050330381,
      "grad_norm": 1.558337688446045,
      "learning_rate": 4.511889863079782e-05,
      "loss": 1.1,
      "step": 5197
    },
    {
      "epoch": 0.7307746379867848,
      "grad_norm": 1.4691828489303589,
      "learning_rate": 4.5313378793424387e-05,
      "loss": 1.185,
      "step": 5198
    },
    {
      "epoch": 0.7309152256431886,
      "grad_norm": 1.5632073879241943,
      "learning_rate": 4.5508154493698676e-05,
      "loss": 1.0999,
      "step": 5199
    },
    {
      "epoch": 0.7310558132995923,
      "grad_norm": 1.2636840343475342,
      "learning_rate": 4.57032246790128e-05,
      "loss": 1.0136,
      "step": 5200
    },
    {
      "epoch": 0.731196400955996,
      "grad_norm": 1.4621537923812866,
      "learning_rate": 4.5898588295168175e-05,
      "loss": 1.0899,
      "step": 5201
    },
    {
      "epoch": 0.7313369886123998,
      "grad_norm": 1.8799059391021729,
      "learning_rate": 4.609424428637956e-05,
      "loss": 1.1148,
      "step": 5202
    },
    {
      "epoch": 0.7314775762688036,
      "grad_norm": 1.3494035005569458,
      "learning_rate": 4.6290191595282085e-05,
      "loss": 1.0717,
      "step": 5203
    },
    {
      "epoch": 0.7316181639252074,
      "grad_norm": 1.827477216720581,
      "learning_rate": 4.648642916293685e-05,
      "loss": 1.1522,
      "step": 5204
    },
    {
      "epoch": 0.7317587515816112,
      "grad_norm": 1.4591060876846313,
      "learning_rate": 4.6682955928836046e-05,
      "loss": 0.9169,
      "step": 5205
    },
    {
      "epoch": 0.7318993392380149,
      "grad_norm": 1.7474573850631714,
      "learning_rate": 4.687977083090852e-05,
      "loss": 1.0636,
      "step": 5206
    },
    {
      "epoch": 0.7320399268944187,
      "grad_norm": 1.3763030767440796,
      "learning_rate": 4.707687280552688e-05,
      "loss": 0.9897,
      "step": 5207
    },
    {
      "epoch": 0.7321805145508224,
      "grad_norm": 1.477509617805481,
      "learning_rate": 4.7274260787511596e-05,
      "loss": 1.0353,
      "step": 5208
    },
    {
      "epoch": 0.7323211022072262,
      "grad_norm": 1.5268422365188599,
      "learning_rate": 4.747193371013819e-05,
      "loss": 1.1415,
      "step": 5209
    },
    {
      "epoch": 0.73246168986363,
      "grad_norm": 1.4810867309570312,
      "learning_rate": 4.766989050514149e-05,
      "loss": 0.9924,
      "step": 5210
    },
    {
      "epoch": 0.7326022775200337,
      "grad_norm": 1.2574759721755981,
      "learning_rate": 4.78681301027226e-05,
      "loss": 1.1319,
      "step": 5211
    },
    {
      "epoch": 0.7327428651764375,
      "grad_norm": 1.3029568195343018,
      "learning_rate": 4.806665143155463e-05,
      "loss": 0.9147,
      "step": 5212
    },
    {
      "epoch": 0.7328834528328413,
      "grad_norm": 1.1912997961044312,
      "learning_rate": 4.8265453418787874e-05,
      "loss": 0.9996,
      "step": 5213
    },
    {
      "epoch": 0.7330240404892451,
      "grad_norm": 1.5119787454605103,
      "learning_rate": 4.84645349900554e-05,
      "loss": 1.0442,
      "step": 5214
    },
    {
      "epoch": 0.7331646281456489,
      "grad_norm": 1.5476953983306885,
      "learning_rate": 4.866389506948028e-05,
      "loss": 1.1573,
      "step": 5215
    },
    {
      "epoch": 0.7333052158020525,
      "grad_norm": 1.7250189781188965,
      "learning_rate": 4.8863532579679864e-05,
      "loss": 1.172,
      "step": 5216
    },
    {
      "epoch": 0.7334458034584563,
      "grad_norm": 1.4740110635757446,
      "learning_rate": 4.9063446441772345e-05,
      "loss": 1.1451,
      "step": 5217
    },
    {
      "epoch": 0.7335863911148601,
      "grad_norm": 1.4685451984405518,
      "learning_rate": 4.9263635575382464e-05,
      "loss": 0.9052,
      "step": 5218
    },
    {
      "epoch": 0.7337269787712639,
      "grad_norm": 1.3665598630905151,
      "learning_rate": 4.946409889864719e-05,
      "loss": 1.1949,
      "step": 5219
    },
    {
      "epoch": 0.7338675664276677,
      "grad_norm": 1.2414376735687256,
      "learning_rate": 4.966483532822233e-05,
      "loss": 1.3086,
      "step": 5220
    },
    {
      "epoch": 0.7340081540840714,
      "grad_norm": 1.3362243175506592,
      "learning_rate": 4.9865843779286836e-05,
      "loss": 1.1393,
      "step": 5221
    },
    {
      "epoch": 0.7341487417404752,
      "grad_norm": 1.4964226484298706,
      "learning_rate": 5.0067123165550065e-05,
      "loss": 1.2226,
      "step": 5222
    },
    {
      "epoch": 0.734289329396879,
      "grad_norm": 1.6839885711669922,
      "learning_rate": 5.0268672399257496e-05,
      "loss": 1.0207,
      "step": 5223
    },
    {
      "epoch": 0.7344299170532828,
      "grad_norm": 1.3886445760726929,
      "learning_rate": 5.0470490391195855e-05,
      "loss": 1.0347,
      "step": 5224
    },
    {
      "epoch": 0.7345705047096865,
      "grad_norm": 1.3482694625854492,
      "learning_rate": 5.067257605069955e-05,
      "loss": 0.9163,
      "step": 5225
    },
    {
      "epoch": 0.7347110923660902,
      "grad_norm": 1.3079732656478882,
      "learning_rate": 5.087492828565651e-05,
      "loss": 0.8394,
      "step": 5226
    },
    {
      "epoch": 0.734851680022494,
      "grad_norm": 1.5347014665603638,
      "learning_rate": 5.1077546002513844e-05,
      "loss": 1.2028,
      "step": 5227
    },
    {
      "epoch": 0.7349922676788978,
      "grad_norm": 1.355405330657959,
      "learning_rate": 5.1280428106284564e-05,
      "loss": 1.1252,
      "step": 5228
    },
    {
      "epoch": 0.7351328553353016,
      "grad_norm": 1.5107446908950806,
      "learning_rate": 5.148357350055199e-05,
      "loss": 1.2608,
      "step": 5229
    },
    {
      "epoch": 0.7352734429917054,
      "grad_norm": 1.4972312450408936,
      "learning_rate": 5.1686981087476985e-05,
      "loss": 1.1534,
      "step": 5230
    },
    {
      "epoch": 0.7354140306481091,
      "grad_norm": 1.6418769359588623,
      "learning_rate": 5.18906497678038e-05,
      "loss": 1.1178,
      "step": 5231
    },
    {
      "epoch": 0.7355546183045129,
      "grad_norm": 1.2796071767807007,
      "learning_rate": 5.209457844086524e-05,
      "loss": 1.1217,
      "step": 5232
    },
    {
      "epoch": 0.7356952059609166,
      "grad_norm": 1.331477403640747,
      "learning_rate": 5.229876600458916e-05,
      "loss": 1.0963,
      "step": 5233
    },
    {
      "epoch": 0.7358357936173204,
      "grad_norm": 1.2709838151931763,
      "learning_rate": 5.2503211355504376e-05,
      "loss": 1.148,
      "step": 5234
    },
    {
      "epoch": 0.7359763812737241,
      "grad_norm": 1.4944126605987549,
      "learning_rate": 5.270791338874652e-05,
      "loss": 1.1024,
      "step": 5235
    },
    {
      "epoch": 0.7361169689301279,
      "grad_norm": 1.4201240539550781,
      "learning_rate": 5.291287099806409e-05,
      "loss": 1.1055,
      "step": 5236
    },
    {
      "epoch": 0.7362575565865317,
      "grad_norm": 1.3540663719177246,
      "learning_rate": 5.311808307582438e-05,
      "loss": 1.1678,
      "step": 5237
    },
    {
      "epoch": 0.7363981442429355,
      "grad_norm": 1.458297610282898,
      "learning_rate": 5.332354851301935e-05,
      "loss": 1.0456,
      "step": 5238
    },
    {
      "epoch": 0.7365387318993393,
      "grad_norm": 1.384600043296814,
      "learning_rate": 5.3529266199272364e-05,
      "loss": 0.8495,
      "step": 5239
    },
    {
      "epoch": 0.736679319555743,
      "grad_norm": 1.3977196216583252,
      "learning_rate": 5.373523502284263e-05,
      "loss": 1.0036,
      "step": 5240
    },
    {
      "epoch": 0.7368199072121467,
      "grad_norm": 1.8023971319198608,
      "learning_rate": 5.3941453870632664e-05,
      "loss": 0.9685,
      "step": 5241
    },
    {
      "epoch": 0.7369604948685505,
      "grad_norm": 1.710236668586731,
      "learning_rate": 5.4147921628194085e-05,
      "loss": 1.0166,
      "step": 5242
    },
    {
      "epoch": 0.7371010825249543,
      "grad_norm": 1.4299685955047607,
      "learning_rate": 5.4354637179732894e-05,
      "loss": 1.1145,
      "step": 5243
    },
    {
      "epoch": 0.7372416701813581,
      "grad_norm": 1.84723699092865,
      "learning_rate": 5.4561599408116114e-05,
      "loss": 0.8735,
      "step": 5244
    },
    {
      "epoch": 0.7373822578377618,
      "grad_norm": 1.4263161420822144,
      "learning_rate": 5.476880719487767e-05,
      "loss": 0.9656,
      "step": 5245
    },
    {
      "epoch": 0.7375228454941656,
      "grad_norm": 1.2079124450683594,
      "learning_rate": 5.4976259420224306e-05,
      "loss": 1.2032,
      "step": 5246
    },
    {
      "epoch": 0.7376634331505694,
      "grad_norm": 1.5209518671035767,
      "learning_rate": 5.518395496304237e-05,
      "loss": 0.9543,
      "step": 5247
    },
    {
      "epoch": 0.7378040208069732,
      "grad_norm": 1.504423975944519,
      "learning_rate": 5.539189270090235e-05,
      "loss": 1.0302,
      "step": 5248
    },
    {
      "epoch": 0.737944608463377,
      "grad_norm": 1.409773588180542,
      "learning_rate": 5.560007151006633e-05,
      "loss": 1.0991,
      "step": 5249
    },
    {
      "epoch": 0.7380851961197806,
      "grad_norm": 1.3150978088378906,
      "learning_rate": 5.5808490265493904e-05,
      "loss": 1.2769,
      "step": 5250
    },
    {
      "epoch": 0.7382257837761844,
      "grad_norm": 1.6275994777679443,
      "learning_rate": 5.6017147840847484e-05,
      "loss": 1.0682,
      "step": 5251
    },
    {
      "epoch": 0.7383663714325882,
      "grad_norm": 1.52139413356781,
      "learning_rate": 5.6226043108498996e-05,
      "loss": 1.0706,
      "step": 5252
    },
    {
      "epoch": 0.738506959088992,
      "grad_norm": 1.513227105140686,
      "learning_rate": 5.643517493953583e-05,
      "loss": 1.1548,
      "step": 5253
    },
    {
      "epoch": 0.7386475467453958,
      "grad_norm": 1.5025076866149902,
      "learning_rate": 5.664454220376696e-05,
      "loss": 1.0072,
      "step": 5254
    },
    {
      "epoch": 0.7387881344017995,
      "grad_norm": 1.4733140468597412,
      "learning_rate": 5.6854143769729006e-05,
      "loss": 1.1774,
      "step": 5255
    },
    {
      "epoch": 0.7389287220582033,
      "grad_norm": 1.3341091871261597,
      "learning_rate": 5.7063978504692385e-05,
      "loss": 1.1234,
      "step": 5256
    },
    {
      "epoch": 0.739069309714607,
      "grad_norm": 1.5044634342193604,
      "learning_rate": 5.727404527466726e-05,
      "loss": 1.1379,
      "step": 5257
    },
    {
      "epoch": 0.7392098973710108,
      "grad_norm": 1.9237945079803467,
      "learning_rate": 5.748434294441046e-05,
      "loss": 1.0836,
      "step": 5258
    },
    {
      "epoch": 0.7393504850274146,
      "grad_norm": 1.4697003364562988,
      "learning_rate": 5.7694870377429885e-05,
      "loss": 0.9938,
      "step": 5259
    },
    {
      "epoch": 0.7394910726838183,
      "grad_norm": 1.345574140548706,
      "learning_rate": 5.7905626435992864e-05,
      "loss": 1.1154,
      "step": 5260
    },
    {
      "epoch": 0.7396316603402221,
      "grad_norm": 1.536887288093567,
      "learning_rate": 5.811660998113051e-05,
      "loss": 1.0866,
      "step": 5261
    },
    {
      "epoch": 0.7397722479966259,
      "grad_norm": 1.560431718826294,
      "learning_rate": 5.832781987264477e-05,
      "loss": 0.981,
      "step": 5262
    },
    {
      "epoch": 0.7399128356530297,
      "grad_norm": 1.4894108772277832,
      "learning_rate": 5.853925496911432e-05,
      "loss": 1.2522,
      "step": 5263
    },
    {
      "epoch": 0.7400534233094335,
      "grad_norm": 1.8384276628494263,
      "learning_rate": 5.875091412790082e-05,
      "loss": 0.9499,
      "step": 5264
    },
    {
      "epoch": 0.7401940109658371,
      "grad_norm": 1.4205719232559204,
      "learning_rate": 5.8962796205154856e-05,
      "loss": 1.0791,
      "step": 5265
    },
    {
      "epoch": 0.7403345986222409,
      "grad_norm": 1.2460823059082031,
      "learning_rate": 5.9174900055823e-05,
      "loss": 1.1109,
      "step": 5266
    },
    {
      "epoch": 0.7404751862786447,
      "grad_norm": 1.2720468044281006,
      "learning_rate": 5.938722453365214e-05,
      "loss": 1.1744,
      "step": 5267
    },
    {
      "epoch": 0.7406157739350485,
      "grad_norm": 1.388856053352356,
      "learning_rate": 5.959976849119803e-05,
      "loss": 1.069,
      "step": 5268
    },
    {
      "epoch": 0.7407563615914523,
      "grad_norm": 1.4284722805023193,
      "learning_rate": 5.981253077982961e-05,
      "loss": 0.9496,
      "step": 5269
    },
    {
      "epoch": 0.740896949247856,
      "grad_norm": 1.309838056564331,
      "learning_rate": 6.002551024973613e-05,
      "loss": 1.0792,
      "step": 5270
    },
    {
      "epoch": 0.7410375369042598,
      "grad_norm": 1.3103395700454712,
      "learning_rate": 6.023870574993312e-05,
      "loss": 1.0517,
      "step": 5271
    },
    {
      "epoch": 0.7411781245606636,
      "grad_norm": 1.6839008331298828,
      "learning_rate": 6.0452116128268646e-05,
      "loss": 1.1073,
      "step": 5272
    },
    {
      "epoch": 0.7413187122170674,
      "grad_norm": 1.708111047744751,
      "learning_rate": 6.0665740231429344e-05,
      "loss": 1.2783,
      "step": 5273
    },
    {
      "epoch": 0.7414592998734711,
      "grad_norm": 1.569547176361084,
      "learning_rate": 6.0879576904947524e-05,
      "loss": 1.0418,
      "step": 5274
    },
    {
      "epoch": 0.7415998875298748,
      "grad_norm": 1.365343689918518,
      "learning_rate": 6.10936249932057e-05,
      "loss": 1.1208,
      "step": 5275
    },
    {
      "epoch": 0.7417404751862786,
      "grad_norm": 1.5469835996627808,
      "learning_rate": 6.130788333944441e-05,
      "loss": 1.0736,
      "step": 5276
    },
    {
      "epoch": 0.7418810628426824,
      "grad_norm": 1.509407877922058,
      "learning_rate": 6.152235078576836e-05,
      "loss": 0.994,
      "step": 5277
    },
    {
      "epoch": 0.7420216504990862,
      "grad_norm": 1.6288894414901733,
      "learning_rate": 6.17370261731511e-05,
      "loss": 0.9378,
      "step": 5278
    },
    {
      "epoch": 0.74216223815549,
      "grad_norm": 1.3841811418533325,
      "learning_rate": 6.195190834144357e-05,
      "loss": 1.1678,
      "step": 5279
    },
    {
      "epoch": 0.7423028258118937,
      "grad_norm": 1.5142526626586914,
      "learning_rate": 6.216699612937856e-05,
      "loss": 1.0074,
      "step": 5280
    },
    {
      "epoch": 0.7424434134682975,
      "grad_norm": 1.692191481590271,
      "learning_rate": 6.238228837457782e-05,
      "loss": 1.09,
      "step": 5281
    },
    {
      "epoch": 0.7425840011247012,
      "grad_norm": 1.4265773296356201,
      "learning_rate": 6.259778391355821e-05,
      "loss": 1.1086,
      "step": 5282
    },
    {
      "epoch": 0.742724588781105,
      "grad_norm": 1.3299533128738403,
      "learning_rate": 6.281348158173788e-05,
      "loss": 1.176,
      "step": 5283
    },
    {
      "epoch": 0.7428651764375088,
      "grad_norm": 1.3546382188796997,
      "learning_rate": 6.30293802134425e-05,
      "loss": 1.0979,
      "step": 5284
    },
    {
      "epoch": 0.7430057640939125,
      "grad_norm": 1.338637113571167,
      "learning_rate": 6.324547864191237e-05,
      "loss": 1.1407,
      "step": 5285
    },
    {
      "epoch": 0.7431463517503163,
      "grad_norm": 1.392252802848816,
      "learning_rate": 6.346177569930688e-05,
      "loss": 1.0595,
      "step": 5286
    },
    {
      "epoch": 0.7432869394067201,
      "grad_norm": 1.4773297309875488,
      "learning_rate": 6.367827021671314e-05,
      "loss": 1.0706,
      "step": 5287
    },
    {
      "epoch": 0.7434275270631239,
      "grad_norm": 1.4831146001815796,
      "learning_rate": 6.389496102415044e-05,
      "loss": 0.9707,
      "step": 5288
    },
    {
      "epoch": 0.7435681147195277,
      "grad_norm": 1.9262382984161377,
      "learning_rate": 6.411184695057755e-05,
      "loss": 0.9962,
      "step": 5289
    },
    {
      "epoch": 0.7437087023759313,
      "grad_norm": 1.5699487924575806,
      "learning_rate": 6.43289268238987e-05,
      "loss": 1.3045,
      "step": 5290
    },
    {
      "epoch": 0.7438492900323351,
      "grad_norm": 1.419165849685669,
      "learning_rate": 6.454619947096998e-05,
      "loss": 1.2838,
      "step": 5291
    },
    {
      "epoch": 0.7439898776887389,
      "grad_norm": 1.3697706460952759,
      "learning_rate": 6.476366371760558e-05,
      "loss": 1.2424,
      "step": 5292
    },
    {
      "epoch": 0.7441304653451427,
      "grad_norm": 1.4661139249801636,
      "learning_rate": 6.498131838858489e-05,
      "loss": 1.069,
      "step": 5293
    },
    {
      "epoch": 0.7442710530015465,
      "grad_norm": 1.272864580154419,
      "learning_rate": 6.519916230765702e-05,
      "loss": 1.222,
      "step": 5294
    },
    {
      "epoch": 0.7444116406579502,
      "grad_norm": 1.586114764213562,
      "learning_rate": 6.541719429754954e-05,
      "loss": 1.1174,
      "step": 5295
    },
    {
      "epoch": 0.744552228314354,
      "grad_norm": 1.3049507141113281,
      "learning_rate": 6.563541317997311e-05,
      "loss": 1.2108,
      "step": 5296
    },
    {
      "epoch": 0.7446928159707578,
      "grad_norm": 1.2726781368255615,
      "learning_rate": 6.585381777562796e-05,
      "loss": 1.1541,
      "step": 5297
    },
    {
      "epoch": 0.7448334036271615,
      "grad_norm": 1.6503196954727173,
      "learning_rate": 6.607240690421159e-05,
      "loss": 0.8664,
      "step": 5298
    },
    {
      "epoch": 0.7449739912835653,
      "grad_norm": 1.3925321102142334,
      "learning_rate": 6.629117938442365e-05,
      "loss": 0.9434,
      "step": 5299
    },
    {
      "epoch": 0.745114578939969,
      "grad_norm": 1.32786226272583,
      "learning_rate": 6.651013403397307e-05,
      "loss": 1.1509,
      "step": 5300
    },
    {
      "epoch": 0.7452551665963728,
      "grad_norm": 1.7618815898895264,
      "learning_rate": 6.672926966958429e-05,
      "loss": 1.1058,
      "step": 5301
    },
    {
      "epoch": 0.7453957542527766,
      "grad_norm": 1.4670830965042114,
      "learning_rate": 6.694858510700351e-05,
      "loss": 1.0922,
      "step": 5302
    },
    {
      "epoch": 0.7455363419091804,
      "grad_norm": 1.4868276119232178,
      "learning_rate": 6.71680791610057e-05,
      "loss": 1.2107,
      "step": 5303
    },
    {
      "epoch": 0.7456769295655842,
      "grad_norm": 1.5144661664962769,
      "learning_rate": 6.73877506454003e-05,
      "loss": 1.0172,
      "step": 5304
    },
    {
      "epoch": 0.7458175172219879,
      "grad_norm": 1.5091643333435059,
      "learning_rate": 6.760759837303732e-05,
      "loss": 0.9419,
      "step": 5305
    },
    {
      "epoch": 0.7459581048783916,
      "grad_norm": 1.604432463645935,
      "learning_rate": 6.782762115581534e-05,
      "loss": 0.8777,
      "step": 5306
    },
    {
      "epoch": 0.7460986925347954,
      "grad_norm": 2.0736241340637207,
      "learning_rate": 6.804781780468619e-05,
      "loss": 1.0613,
      "step": 5307
    },
    {
      "epoch": 0.7462392801911992,
      "grad_norm": 1.6338483095169067,
      "learning_rate": 6.826818712966232e-05,
      "loss": 1.1345,
      "step": 5308
    },
    {
      "epoch": 0.746379867847603,
      "grad_norm": 1.8005859851837158,
      "learning_rate": 6.848872793982299e-05,
      "loss": 1.3495,
      "step": 5309
    },
    {
      "epoch": 0.7465204555040067,
      "grad_norm": 1.5873109102249146,
      "learning_rate": 6.870943904332055e-05,
      "loss": 1.0473,
      "step": 5310
    },
    {
      "epoch": 0.7466610431604105,
      "grad_norm": 1.210644006729126,
      "learning_rate": 6.893031924738751e-05,
      "loss": 1.1673,
      "step": 5311
    },
    {
      "epoch": 0.7468016308168143,
      "grad_norm": 1.6030234098434448,
      "learning_rate": 6.915136735834225e-05,
      "loss": 1.0541,
      "step": 5312
    },
    {
      "epoch": 0.7469422184732181,
      "grad_norm": 1.4622976779937744,
      "learning_rate": 6.937258218159521e-05,
      "loss": 1.0677,
      "step": 5313
    },
    {
      "epoch": 0.7470828061296219,
      "grad_norm": 1.5559067726135254,
      "learning_rate": 6.959396252165692e-05,
      "loss": 1.1503,
      "step": 5314
    },
    {
      "epoch": 0.7472233937860255,
      "grad_norm": 1.5478278398513794,
      "learning_rate": 6.981550718214286e-05,
      "loss": 1.0233,
      "step": 5315
    },
    {
      "epoch": 0.7473639814424293,
      "grad_norm": 1.650622844696045,
      "learning_rate": 7.003721496578007e-05,
      "loss": 1.0245,
      "step": 5316
    },
    {
      "epoch": 0.7475045690988331,
      "grad_norm": 1.3995699882507324,
      "learning_rate": 7.025908467441497e-05,
      "loss": 1.0992,
      "step": 5317
    },
    {
      "epoch": 0.7476451567552369,
      "grad_norm": 1.6078470945358276,
      "learning_rate": 7.04811151090183e-05,
      "loss": 0.894,
      "step": 5318
    },
    {
      "epoch": 0.7477857444116407,
      "grad_norm": 1.4208348989486694,
      "learning_rate": 7.070330506969228e-05,
      "loss": 1.0284,
      "step": 5319
    },
    {
      "epoch": 0.7479263320680444,
      "grad_norm": 1.3263992071151733,
      "learning_rate": 7.092565335567709e-05,
      "loss": 1.1211,
      "step": 5320
    },
    {
      "epoch": 0.7480669197244482,
      "grad_norm": 1.5520212650299072,
      "learning_rate": 7.114815876535715e-05,
      "loss": 0.9622,
      "step": 5321
    },
    {
      "epoch": 0.748207507380852,
      "grad_norm": 1.786577582359314,
      "learning_rate": 7.137082009626816e-05,
      "loss": 1.2021,
      "step": 5322
    },
    {
      "epoch": 0.7483480950372557,
      "grad_norm": 1.4136769771575928,
      "learning_rate": 7.159363614510292e-05,
      "loss": 1.0787,
      "step": 5323
    },
    {
      "epoch": 0.7484886826936595,
      "grad_norm": 1.4533040523529053,
      "learning_rate": 7.181660570771758e-05,
      "loss": 1.1176,
      "step": 5324
    },
    {
      "epoch": 0.7486292703500632,
      "grad_norm": 1.6941392421722412,
      "learning_rate": 7.203972757913968e-05,
      "loss": 0.9721,
      "step": 5325
    },
    {
      "epoch": 0.748769858006467,
      "grad_norm": 1.5779014825820923,
      "learning_rate": 7.2263000553573e-05,
      "loss": 1.035,
      "step": 5326
    },
    {
      "epoch": 0.7489104456628708,
      "grad_norm": 1.4030929803848267,
      "learning_rate": 7.248642342440486e-05,
      "loss": 0.9646,
      "step": 5327
    },
    {
      "epoch": 0.7490510333192746,
      "grad_norm": 1.5416651964187622,
      "learning_rate": 7.270999498421252e-05,
      "loss": 1.0297,
      "step": 5328
    },
    {
      "epoch": 0.7491916209756784,
      "grad_norm": 1.4988633394241333,
      "learning_rate": 7.293371402476954e-05,
      "loss": 1.0317,
      "step": 5329
    },
    {
      "epoch": 0.749332208632082,
      "grad_norm": 1.3487820625305176,
      "learning_rate": 7.315757933705296e-05,
      "loss": 0.9959,
      "step": 5330
    },
    {
      "epoch": 0.7494727962884858,
      "grad_norm": 1.6135162115097046,
      "learning_rate": 7.338158971124901e-05,
      "loss": 1.2789,
      "step": 5331
    },
    {
      "epoch": 0.7496133839448896,
      "grad_norm": 1.5050746202468872,
      "learning_rate": 7.360574393675946e-05,
      "loss": 1.0207,
      "step": 5332
    },
    {
      "epoch": 0.7497539716012934,
      "grad_norm": 1.3999042510986328,
      "learning_rate": 7.383004080220968e-05,
      "loss": 1.2135,
      "step": 5333
    },
    {
      "epoch": 0.7498945592576972,
      "grad_norm": 1.4998618364334106,
      "learning_rate": 7.40544790954537e-05,
      "loss": 0.9559,
      "step": 5334
    },
    {
      "epoch": 0.7500351469141009,
      "grad_norm": 1.7631990909576416,
      "learning_rate": 7.427905760358074e-05,
      "loss": 1.0201,
      "step": 5335
    },
    {
      "epoch": 0.7501757345705047,
      "grad_norm": 1.6888673305511475,
      "learning_rate": 7.450377511292328e-05,
      "loss": 1.1137,
      "step": 5336
    },
    {
      "epoch": 0.7503163222269085,
      "grad_norm": 1.4081411361694336,
      "learning_rate": 7.472863040906172e-05,
      "loss": 1.0339,
      "step": 5337
    },
    {
      "epoch": 0.7504569098833123,
      "grad_norm": 1.4289932250976562,
      "learning_rate": 7.495362227683276e-05,
      "loss": 1.2098,
      "step": 5338
    },
    {
      "epoch": 0.750597497539716,
      "grad_norm": 1.2752811908721924,
      "learning_rate": 7.517874950033394e-05,
      "loss": 1.2272,
      "step": 5339
    },
    {
      "epoch": 0.7507380851961197,
      "grad_norm": 1.626650094985962,
      "learning_rate": 7.540401086293189e-05,
      "loss": 1.1869,
      "step": 5340
    },
    {
      "epoch": 0.7508786728525235,
      "grad_norm": 1.7253526449203491,
      "learning_rate": 7.562940514726862e-05,
      "loss": 1.1179,
      "step": 5341
    },
    {
      "epoch": 0.7510192605089273,
      "grad_norm": 1.4361252784729004,
      "learning_rate": 7.585493113526746e-05,
      "loss": 1.1858,
      "step": 5342
    },
    {
      "epoch": 0.7511598481653311,
      "grad_norm": 1.8434196710586548,
      "learning_rate": 7.608058760813953e-05,
      "loss": 1.2294,
      "step": 5343
    },
    {
      "epoch": 0.7513004358217349,
      "grad_norm": 1.3862097263336182,
      "learning_rate": 7.630637334639172e-05,
      "loss": 1.0727,
      "step": 5344
    },
    {
      "epoch": 0.7514410234781386,
      "grad_norm": 1.4383560419082642,
      "learning_rate": 7.653228712983158e-05,
      "loss": 1.1135,
      "step": 5345
    },
    {
      "epoch": 0.7515816111345424,
      "grad_norm": 1.2651022672653198,
      "learning_rate": 7.675832773757548e-05,
      "loss": 1.1502,
      "step": 5346
    },
    {
      "epoch": 0.7517221987909462,
      "grad_norm": 1.4223278760910034,
      "learning_rate": 7.698449394805336e-05,
      "loss": 0.9655,
      "step": 5347
    },
    {
      "epoch": 0.7518627864473499,
      "grad_norm": 1.3136907815933228,
      "learning_rate": 7.721078453901694e-05,
      "loss": 1.195,
      "step": 5348
    },
    {
      "epoch": 0.7520033741037537,
      "grad_norm": 1.37447190284729,
      "learning_rate": 7.74371982875461e-05,
      "loss": 1.2629,
      "step": 5349
    },
    {
      "epoch": 0.7521439617601574,
      "grad_norm": 1.3895930051803589,
      "learning_rate": 7.766373397005477e-05,
      "loss": 1.0433,
      "step": 5350
    },
    {
      "epoch": 0.7522845494165612,
      "grad_norm": 1.425257682800293,
      "learning_rate": 7.789039036229743e-05,
      "loss": 1.0938,
      "step": 5351
    },
    {
      "epoch": 0.752425137072965,
      "grad_norm": 1.4701745510101318,
      "learning_rate": 7.81171662393773e-05,
      "loss": 1.0487,
      "step": 5352
    },
    {
      "epoch": 0.7525657247293688,
      "grad_norm": 1.2588233947753906,
      "learning_rate": 7.834406037575125e-05,
      "loss": 1.1234,
      "step": 5353
    },
    {
      "epoch": 0.7527063123857726,
      "grad_norm": 1.3957531452178955,
      "learning_rate": 7.857107154523666e-05,
      "loss": 1.0568,
      "step": 5354
    },
    {
      "epoch": 0.7528469000421762,
      "grad_norm": 1.5003902912139893,
      "learning_rate": 7.87981985210194e-05,
      "loss": 0.9328,
      "step": 5355
    },
    {
      "epoch": 0.75298748769858,
      "grad_norm": 1.2984360456466675,
      "learning_rate": 7.902544007565862e-05,
      "loss": 1.2342,
      "step": 5356
    },
    {
      "epoch": 0.7531280753549838,
      "grad_norm": 2.816767930984497,
      "learning_rate": 7.92527949810952e-05,
      "loss": 1.1077,
      "step": 5357
    },
    {
      "epoch": 0.7532686630113876,
      "grad_norm": 1.3727279901504517,
      "learning_rate": 7.948026200865635e-05,
      "loss": 1.1311,
      "step": 5358
    },
    {
      "epoch": 0.7534092506677914,
      "grad_norm": 1.4927536249160767,
      "learning_rate": 7.970783992906391e-05,
      "loss": 1.3186,
      "step": 5359
    },
    {
      "epoch": 0.7535498383241951,
      "grad_norm": 1.2997090816497803,
      "learning_rate": 7.993552751244074e-05,
      "loss": 1.3134,
      "step": 5360
    },
    {
      "epoch": 0.7536904259805989,
      "grad_norm": 1.5226658582687378,
      "learning_rate": 8.016332352831672e-05,
      "loss": 0.9519,
      "step": 5361
    },
    {
      "epoch": 0.7538310136370027,
      "grad_norm": 1.3433340787887573,
      "learning_rate": 8.039122674563526e-05,
      "loss": 1.1808,
      "step": 5362
    },
    {
      "epoch": 0.7539716012934065,
      "grad_norm": 1.4863228797912598,
      "learning_rate": 8.061923593276143e-05,
      "loss": 1.0358,
      "step": 5363
    },
    {
      "epoch": 0.7541121889498102,
      "grad_norm": 1.4247759580612183,
      "learning_rate": 8.084734985748674e-05,
      "loss": 0.9879,
      "step": 5364
    },
    {
      "epoch": 0.7542527766062139,
      "grad_norm": 1.6492676734924316,
      "learning_rate": 8.10755672870376e-05,
      "loss": 0.9264,
      "step": 5365
    },
    {
      "epoch": 0.7543933642626177,
      "grad_norm": 1.3319462537765503,
      "learning_rate": 8.130388698807995e-05,
      "loss": 1.0222,
      "step": 5366
    },
    {
      "epoch": 0.7545339519190215,
      "grad_norm": 1.3230377435684204,
      "learning_rate": 8.153230772672762e-05,
      "loss": 1.1963,
      "step": 5367
    },
    {
      "epoch": 0.7546745395754253,
      "grad_norm": 1.388152837753296,
      "learning_rate": 8.17608282685487e-05,
      "loss": 1.1979,
      "step": 5368
    },
    {
      "epoch": 0.7548151272318291,
      "grad_norm": 1.4801323413848877,
      "learning_rate": 8.198944737857161e-05,
      "loss": 1.0715,
      "step": 5369
    },
    {
      "epoch": 0.7549557148882328,
      "grad_norm": 1.2793148756027222,
      "learning_rate": 8.221816382129158e-05,
      "loss": 1.0964,
      "step": 5370
    },
    {
      "epoch": 0.7550963025446366,
      "grad_norm": 1.6060523986816406,
      "learning_rate": 8.244697636067885e-05,
      "loss": 1.3248,
      "step": 5371
    },
    {
      "epoch": 0.7552368902010403,
      "grad_norm": 1.2606897354125977,
      "learning_rate": 8.267588376018361e-05,
      "loss": 1.0864,
      "step": 5372
    },
    {
      "epoch": 0.7553774778574441,
      "grad_norm": 1.4110023975372314,
      "learning_rate": 8.290488478274366e-05,
      "loss": 1.1036,
      "step": 5373
    },
    {
      "epoch": 0.7555180655138479,
      "grad_norm": 1.4287656545639038,
      "learning_rate": 8.313397819079076e-05,
      "loss": 1.101,
      "step": 5374
    },
    {
      "epoch": 0.7556586531702516,
      "grad_norm": 1.4051427841186523,
      "learning_rate": 8.33631627462573e-05,
      "loss": 1.0241,
      "step": 5375
    },
    {
      "epoch": 0.7557992408266554,
      "grad_norm": 1.691651463508606,
      "learning_rate": 8.359243721058373e-05,
      "loss": 1.0697,
      "step": 5376
    },
    {
      "epoch": 0.7559398284830592,
      "grad_norm": 1.4664592742919922,
      "learning_rate": 8.382180034472358e-05,
      "loss": 1.183,
      "step": 5377
    },
    {
      "epoch": 0.756080416139463,
      "grad_norm": 1.6471012830734253,
      "learning_rate": 8.405125090915176e-05,
      "loss": 1.1393,
      "step": 5378
    },
    {
      "epoch": 0.7562210037958668,
      "grad_norm": 1.4669466018676758,
      "learning_rate": 8.428078766387098e-05,
      "loss": 0.9799,
      "step": 5379
    },
    {
      "epoch": 0.7563615914522704,
      "grad_norm": 1.552144169807434,
      "learning_rate": 8.451040936841763e-05,
      "loss": 0.9142,
      "step": 5380
    },
    {
      "epoch": 0.7565021791086742,
      "grad_norm": 1.3698182106018066,
      "learning_rate": 8.474011478186926e-05,
      "loss": 0.956,
      "step": 5381
    },
    {
      "epoch": 0.756642766765078,
      "grad_norm": 1.4861034154891968,
      "learning_rate": 8.496990266285097e-05,
      "loss": 1.1571,
      "step": 5382
    },
    {
      "epoch": 0.7567833544214818,
      "grad_norm": 1.2533947229385376,
      "learning_rate": 8.519977176954204e-05,
      "loss": 1.0939,
      "step": 5383
    },
    {
      "epoch": 0.7569239420778856,
      "grad_norm": 1.4379841089248657,
      "learning_rate": 8.542972085968351e-05,
      "loss": 1.0383,
      "step": 5384
    },
    {
      "epoch": 0.7570645297342893,
      "grad_norm": 1.3614472150802612,
      "learning_rate": 8.565974869058307e-05,
      "loss": 1.1258,
      "step": 5385
    },
    {
      "epoch": 0.7572051173906931,
      "grad_norm": 1.4784165620803833,
      "learning_rate": 8.588985401912346e-05,
      "loss": 1.1339,
      "step": 5386
    },
    {
      "epoch": 0.7573457050470969,
      "grad_norm": 1.5052164793014526,
      "learning_rate": 8.612003560176894e-05,
      "loss": 1.0927,
      "step": 5387
    },
    {
      "epoch": 0.7574862927035007,
      "grad_norm": 1.5336036682128906,
      "learning_rate": 8.635029219457111e-05,
      "loss": 1.1738,
      "step": 5388
    },
    {
      "epoch": 0.7576268803599044,
      "grad_norm": 1.4074761867523193,
      "learning_rate": 8.658062255317642e-05,
      "loss": 1.1922,
      "step": 5389
    },
    {
      "epoch": 0.7577674680163081,
      "grad_norm": 1.7060742378234863,
      "learning_rate": 8.68110254328327e-05,
      "loss": 1.0477,
      "step": 5390
    },
    {
      "epoch": 0.7579080556727119,
      "grad_norm": 1.4848105907440186,
      "learning_rate": 8.704149958839581e-05,
      "loss": 1.0868,
      "step": 5391
    },
    {
      "epoch": 0.7580486433291157,
      "grad_norm": 1.3744168281555176,
      "learning_rate": 8.727204377433641e-05,
      "loss": 1.0198,
      "step": 5392
    },
    {
      "epoch": 0.7581892309855195,
      "grad_norm": 1.6432400941848755,
      "learning_rate": 8.750265674474675e-05,
      "loss": 1.2183,
      "step": 5393
    },
    {
      "epoch": 0.7583298186419233,
      "grad_norm": 1.2895923852920532,
      "learning_rate": 8.773333725334717e-05,
      "loss": 1.0391,
      "step": 5394
    },
    {
      "epoch": 0.758470406298327,
      "grad_norm": 1.765105962753296,
      "learning_rate": 8.796408405349367e-05,
      "loss": 1.1423,
      "step": 5395
    },
    {
      "epoch": 0.7586109939547308,
      "grad_norm": 1.5386505126953125,
      "learning_rate": 8.819489589818302e-05,
      "loss": 1.0627,
      "step": 5396
    },
    {
      "epoch": 0.7587515816111345,
      "grad_norm": 1.520706295967102,
      "learning_rate": 8.842577154006095e-05,
      "loss": 1.0724,
      "step": 5397
    },
    {
      "epoch": 0.7588921692675383,
      "grad_norm": 1.347184658050537,
      "learning_rate": 8.865670973142878e-05,
      "loss": 1.1944,
      "step": 5398
    },
    {
      "epoch": 0.7590327569239421,
      "grad_norm": 1.3330023288726807,
      "learning_rate": 8.888770922424928e-05,
      "loss": 1.0366,
      "step": 5399
    },
    {
      "epoch": 0.7591733445803458,
      "grad_norm": 1.4517403841018677,
      "learning_rate": 8.911876877015413e-05,
      "loss": 1.1249,
      "step": 5400
    },
    {
      "epoch": 0.7593139322367496,
      "grad_norm": 1.5430153608322144,
      "learning_rate": 8.934988712045041e-05,
      "loss": 1.0316,
      "step": 5401
    },
    {
      "epoch": 0.7594545198931534,
      "grad_norm": 1.3863049745559692,
      "learning_rate": 8.95810630261273e-05,
      "loss": 1.111,
      "step": 5402
    },
    {
      "epoch": 0.7595951075495572,
      "grad_norm": 1.4324114322662354,
      "learning_rate": 8.98122952378636e-05,
      "loss": 0.9958,
      "step": 5403
    },
    {
      "epoch": 0.759735695205961,
      "grad_norm": 1.8085713386535645,
      "learning_rate": 9.004358250603278e-05,
      "loss": 1.0262,
      "step": 5404
    },
    {
      "epoch": 0.7598762828623646,
      "grad_norm": 1.7172257900238037,
      "learning_rate": 9.02749235807113e-05,
      "loss": 1.1631,
      "step": 5405
    },
    {
      "epoch": 0.7600168705187684,
      "grad_norm": 1.7382696866989136,
      "learning_rate": 9.05063172116852e-05,
      "loss": 1.0444,
      "step": 5406
    },
    {
      "epoch": 0.7601574581751722,
      "grad_norm": 1.5063060522079468,
      "learning_rate": 9.073776214845595e-05,
      "loss": 1.0788,
      "step": 5407
    },
    {
      "epoch": 0.760298045831576,
      "grad_norm": 1.4869520664215088,
      "learning_rate": 9.096925714024793e-05,
      "loss": 0.9771,
      "step": 5408
    },
    {
      "epoch": 0.7604386334879798,
      "grad_norm": 1.5705692768096924,
      "learning_rate": 9.120080093601504e-05,
      "loss": 0.9444,
      "step": 5409
    },
    {
      "epoch": 0.7605792211443835,
      "grad_norm": 1.7625998258590698,
      "learning_rate": 9.143239228444739e-05,
      "loss": 1.0834,
      "step": 5410
    },
    {
      "epoch": 0.7607198088007873,
      "grad_norm": 1.4511125087738037,
      "learning_rate": 9.166402993397814e-05,
      "loss": 1.1545,
      "step": 5411
    },
    {
      "epoch": 0.7608603964571911,
      "grad_norm": 1.3516312837600708,
      "learning_rate": 9.189571263279022e-05,
      "loss": 1.1695,
      "step": 5412
    },
    {
      "epoch": 0.7610009841135948,
      "grad_norm": 1.3404661417007446,
      "learning_rate": 9.212743912882294e-05,
      "loss": 0.9308,
      "step": 5413
    },
    {
      "epoch": 0.7611415717699986,
      "grad_norm": 1.6325422525405884,
      "learning_rate": 9.235920816977958e-05,
      "loss": 0.9425,
      "step": 5414
    },
    {
      "epoch": 0.7612821594264023,
      "grad_norm": 1.9405518770217896,
      "learning_rate": 9.25910185031323e-05,
      "loss": 1.0087,
      "step": 5415
    },
    {
      "epoch": 0.7614227470828061,
      "grad_norm": 1.3001155853271484,
      "learning_rate": 9.282286887613138e-05,
      "loss": 0.9242,
      "step": 5416
    },
    {
      "epoch": 0.7615633347392099,
      "grad_norm": 1.376131534576416,
      "learning_rate": 9.305475803580992e-05,
      "loss": 1.1825,
      "step": 5417
    },
    {
      "epoch": 0.7617039223956137,
      "grad_norm": 1.3605438470840454,
      "learning_rate": 9.328668472899167e-05,
      "loss": 1.1496,
      "step": 5418
    },
    {
      "epoch": 0.7618445100520175,
      "grad_norm": 1.3806957006454468,
      "learning_rate": 9.351864770229748e-05,
      "loss": 1.0801,
      "step": 5419
    },
    {
      "epoch": 0.7619850977084212,
      "grad_norm": 1.3381540775299072,
      "learning_rate": 9.375064570215219e-05,
      "loss": 1.0056,
      "step": 5420
    },
    {
      "epoch": 0.762125685364825,
      "grad_norm": 1.2808128595352173,
      "learning_rate": 9.398267747479113e-05,
      "loss": 1.1029,
      "step": 5421
    },
    {
      "epoch": 0.7622662730212287,
      "grad_norm": 1.443454623222351,
      "learning_rate": 9.421474176626785e-05,
      "loss": 1.1954,
      "step": 5422
    },
    {
      "epoch": 0.7624068606776325,
      "grad_norm": 1.3334343433380127,
      "learning_rate": 9.44468373224589e-05,
      "loss": 0.9464,
      "step": 5423
    },
    {
      "epoch": 0.7625474483340363,
      "grad_norm": 1.7124524116516113,
      "learning_rate": 9.46789628890731e-05,
      "loss": 1.1066,
      "step": 5424
    },
    {
      "epoch": 0.76268803599044,
      "grad_norm": 1.3430200815200806,
      "learning_rate": 9.491111721165642e-05,
      "loss": 1.203,
      "step": 5425
    },
    {
      "epoch": 0.7628286236468438,
      "grad_norm": 1.8050833940505981,
      "learning_rate": 9.514329903559958e-05,
      "loss": 0.9956,
      "step": 5426
    },
    {
      "epoch": 0.7629692113032476,
      "grad_norm": 1.441994547843933,
      "learning_rate": 9.537550710614467e-05,
      "loss": 1.1703,
      "step": 5427
    },
    {
      "epoch": 0.7631097989596514,
      "grad_norm": 1.4922010898590088,
      "learning_rate": 9.560774016839196e-05,
      "loss": 1.0965,
      "step": 5428
    },
    {
      "epoch": 0.7632503866160552,
      "grad_norm": 1.528695821762085,
      "learning_rate": 9.583999696730646e-05,
      "loss": 1.0414,
      "step": 5429
    },
    {
      "epoch": 0.7633909742724588,
      "grad_norm": 1.420166254043579,
      "learning_rate": 9.607227624772562e-05,
      "loss": 1.0444,
      "step": 5430
    },
    {
      "epoch": 0.7635315619288626,
      "grad_norm": 1.4705331325531006,
      "learning_rate": 9.630457675436432e-05,
      "loss": 1.0866,
      "step": 5431
    },
    {
      "epoch": 0.7636721495852664,
      "grad_norm": 1.4203083515167236,
      "learning_rate": 9.653689723182331e-05,
      "loss": 1.1992,
      "step": 5432
    },
    {
      "epoch": 0.7638127372416702,
      "grad_norm": 1.2495427131652832,
      "learning_rate": 9.676923642459588e-05,
      "loss": 0.9442,
      "step": 5433
    },
    {
      "epoch": 0.763953324898074,
      "grad_norm": 1.5685738325119019,
      "learning_rate": 9.700159307707297e-05,
      "loss": 1.1436,
      "step": 5434
    },
    {
      "epoch": 0.7640939125544777,
      "grad_norm": 1.4802474975585938,
      "learning_rate": 9.723396593355239e-05,
      "loss": 0.9595,
      "step": 5435
    },
    {
      "epoch": 0.7642345002108815,
      "grad_norm": 1.3096531629562378,
      "learning_rate": 9.746635373824373e-05,
      "loss": 1.2968,
      "step": 5436
    },
    {
      "epoch": 0.7643750878672853,
      "grad_norm": 1.2808921337127686,
      "learning_rate": 9.769875523527585e-05,
      "loss": 1.1368,
      "step": 5437
    },
    {
      "epoch": 0.764515675523689,
      "grad_norm": 1.4281468391418457,
      "learning_rate": 9.793116916870377e-05,
      "loss": 1.1838,
      "step": 5438
    },
    {
      "epoch": 0.7646562631800928,
      "grad_norm": 1.3643267154693604,
      "learning_rate": 9.816359428251515e-05,
      "loss": 1.1702,
      "step": 5439
    },
    {
      "epoch": 0.7647968508364965,
      "grad_norm": 1.211414098739624,
      "learning_rate": 9.839602932063718e-05,
      "loss": 1.1102,
      "step": 5440
    },
    {
      "epoch": 0.7649374384929003,
      "grad_norm": 1.510221004486084,
      "learning_rate": 9.862847302694405e-05,
      "loss": 1.0786,
      "step": 5441
    },
    {
      "epoch": 0.7650780261493041,
      "grad_norm": 1.563236117362976,
      "learning_rate": 9.886092414526187e-05,
      "loss": 1.0723,
      "step": 5442
    },
    {
      "epoch": 0.7652186138057079,
      "grad_norm": 1.2483630180358887,
      "learning_rate": 9.909338141937796e-05,
      "loss": 1.1784,
      "step": 5443
    },
    {
      "epoch": 0.7653592014621117,
      "grad_norm": 1.6528750658035278,
      "learning_rate": 9.932584359304567e-05,
      "loss": 1.0173,
      "step": 5444
    },
    {
      "epoch": 0.7654997891185154,
      "grad_norm": 1.2788829803466797,
      "learning_rate": 9.9558309409992e-05,
      "loss": 1.3163,
      "step": 5445
    },
    {
      "epoch": 0.7656403767749191,
      "grad_norm": 1.342461109161377,
      "learning_rate": 9.979077761392431e-05,
      "loss": 0.958,
      "step": 5446
    },
    {
      "epoch": 0.7657809644313229,
      "grad_norm": 1.3864604234695435,
      "learning_rate": 0.00010002324694853702,
      "loss": 1.2684,
      "step": 5447
    },
    {
      "epoch": 0.7659215520877267,
      "grad_norm": 1.4951329231262207,
      "learning_rate": 0.00010025571615751832,
      "loss": 1.1209,
      "step": 5448
    },
    {
      "epoch": 0.7660621397441305,
      "grad_norm": 1.557915210723877,
      "learning_rate": 0.00010048818398455775,
      "loss": 1.1252,
      "step": 5449
    },
    {
      "epoch": 0.7662027274005342,
      "grad_norm": 1.3347430229187012,
      "learning_rate": 0.00010072064917335105,
      "loss": 1.1372,
      "step": 5450
    },
    {
      "epoch": 0.766343315056938,
      "grad_norm": 1.3139671087265015,
      "learning_rate": 0.00010095311046760951,
      "loss": 1.0534,
      "step": 5451
    },
    {
      "epoch": 0.7664839027133418,
      "grad_norm": 1.4455411434173584,
      "learning_rate": 0.00010118556661106491,
      "loss": 1.1592,
      "step": 5452
    },
    {
      "epoch": 0.7666244903697456,
      "grad_norm": 1.511698603630066,
      "learning_rate": 0.00010141801634747636,
      "loss": 1.1127,
      "step": 5453
    },
    {
      "epoch": 0.7667650780261493,
      "grad_norm": 1.485671877861023,
      "learning_rate": 0.00010165045842063858,
      "loss": 1.0226,
      "step": 5454
    },
    {
      "epoch": 0.766905665682553,
      "grad_norm": 1.340990662574768,
      "learning_rate": 0.00010188289157438712,
      "loss": 1.1478,
      "step": 5455
    },
    {
      "epoch": 0.7670462533389568,
      "grad_norm": 1.311906099319458,
      "learning_rate": 0.00010211531455260579,
      "loss": 1.2151,
      "step": 5456
    },
    {
      "epoch": 0.7671868409953606,
      "grad_norm": 1.4611523151397705,
      "learning_rate": 0.00010234772609923349,
      "loss": 1.0312,
      "step": 5457
    },
    {
      "epoch": 0.7673274286517644,
      "grad_norm": 1.4277955293655396,
      "learning_rate": 0.0001025801249582706,
      "loss": 1.0284,
      "step": 5458
    },
    {
      "epoch": 0.7674680163081682,
      "grad_norm": 1.5118184089660645,
      "learning_rate": 0.00010281250987378675,
      "loss": 1.1734,
      "step": 5459
    },
    {
      "epoch": 0.7676086039645719,
      "grad_norm": 1.6353014707565308,
      "learning_rate": 0.00010304487958992643,
      "loss": 1.0172,
      "step": 5460
    },
    {
      "epoch": 0.7677491916209757,
      "grad_norm": 1.3926796913146973,
      "learning_rate": 0.00010327723285091586,
      "loss": 1.0346,
      "step": 5461
    },
    {
      "epoch": 0.7678897792773794,
      "grad_norm": 1.3487756252288818,
      "learning_rate": 0.00010350956840107114,
      "loss": 0.9866,
      "step": 5462
    },
    {
      "epoch": 0.7680303669337832,
      "grad_norm": 1.634243130683899,
      "learning_rate": 0.00010374188498480341,
      "loss": 1.141,
      "step": 5463
    },
    {
      "epoch": 0.768170954590187,
      "grad_norm": 1.2520289421081543,
      "learning_rate": 0.00010397418134662655,
      "loss": 1.2076,
      "step": 5464
    },
    {
      "epoch": 0.7683115422465907,
      "grad_norm": 1.4183924198150635,
      "learning_rate": 0.00010420645623116365,
      "loss": 1.1634,
      "step": 5465
    },
    {
      "epoch": 0.7684521299029945,
      "grad_norm": 1.5784406661987305,
      "learning_rate": 0.00010443870838315371,
      "loss": 1.1497,
      "step": 5466
    },
    {
      "epoch": 0.7685927175593983,
      "grad_norm": 1.6441493034362793,
      "learning_rate": 0.00010467093654745912,
      "loss": 1.154,
      "step": 5467
    },
    {
      "epoch": 0.7687333052158021,
      "grad_norm": 1.6765402555465698,
      "learning_rate": 0.00010490313946907153,
      "loss": 0.9609,
      "step": 5468
    },
    {
      "epoch": 0.7688738928722059,
      "grad_norm": 1.3366221189498901,
      "learning_rate": 0.00010513531589311854,
      "loss": 1.0256,
      "step": 5469
    },
    {
      "epoch": 0.7690144805286095,
      "grad_norm": 1.5933122634887695,
      "learning_rate": 0.00010536746456487189,
      "loss": 1.1484,
      "step": 5470
    },
    {
      "epoch": 0.7691550681850133,
      "grad_norm": 2.1714155673980713,
      "learning_rate": 0.00010559958422975289,
      "loss": 0.9501,
      "step": 5471
    },
    {
      "epoch": 0.7692956558414171,
      "grad_norm": 1.487620234489441,
      "learning_rate": 0.00010583167363333903,
      "loss": 1.0847,
      "step": 5472
    },
    {
      "epoch": 0.7694362434978209,
      "grad_norm": 1.4001129865646362,
      "learning_rate": 0.00010606373152137237,
      "loss": 1.1535,
      "step": 5473
    },
    {
      "epoch": 0.7695768311542247,
      "grad_norm": 1.4177616834640503,
      "learning_rate": 0.00010629575663976463,
      "loss": 1.1038,
      "step": 5474
    },
    {
      "epoch": 0.7697174188106284,
      "grad_norm": 1.5543391704559326,
      "learning_rate": 0.00010652774773460476,
      "loss": 1.0954,
      "step": 5475
    },
    {
      "epoch": 0.7698580064670322,
      "grad_norm": 1.3415855169296265,
      "learning_rate": 0.00010675970355216561,
      "loss": 1.0247,
      "step": 5476
    },
    {
      "epoch": 0.769998594123436,
      "grad_norm": 1.6205615997314453,
      "learning_rate": 0.00010699162283891047,
      "loss": 1.1377,
      "step": 5477
    },
    {
      "epoch": 0.7701391817798398,
      "grad_norm": 1.3740359544754028,
      "learning_rate": 0.00010722350434150061,
      "loss": 1.1851,
      "step": 5478
    },
    {
      "epoch": 0.7702797694362435,
      "grad_norm": 1.5614345073699951,
      "learning_rate": 0.00010745534680680114,
      "loss": 1.1796,
      "step": 5479
    },
    {
      "epoch": 0.7704203570926472,
      "grad_norm": 2.0259976387023926,
      "learning_rate": 0.00010768714898188756,
      "loss": 1.0921,
      "step": 5480
    },
    {
      "epoch": 0.770560944749051,
      "grad_norm": 1.4283287525177002,
      "learning_rate": 0.00010791890961405422,
      "loss": 1.0881,
      "step": 5481
    },
    {
      "epoch": 0.7707015324054548,
      "grad_norm": 1.8582700490951538,
      "learning_rate": 0.00010815062745081923,
      "loss": 1.1271,
      "step": 5482
    },
    {
      "epoch": 0.7708421200618586,
      "grad_norm": 1.3672877550125122,
      "learning_rate": 0.0001083823012399322,
      "loss": 1.2893,
      "step": 5483
    },
    {
      "epoch": 0.7709827077182624,
      "grad_norm": 1.3410208225250244,
      "learning_rate": 0.00010861392972938073,
      "loss": 1.206,
      "step": 5484
    },
    {
      "epoch": 0.7711232953746661,
      "grad_norm": 1.5196852684020996,
      "learning_rate": 0.00010884551166739707,
      "loss": 1.0374,
      "step": 5485
    },
    {
      "epoch": 0.7712638830310699,
      "grad_norm": 1.8199872970581055,
      "learning_rate": 0.0001090770458024656,
      "loss": 1.1083,
      "step": 5486
    },
    {
      "epoch": 0.7714044706874736,
      "grad_norm": 1.5110101699829102,
      "learning_rate": 0.00010930853088332866,
      "loss": 0.936,
      "step": 5487
    },
    {
      "epoch": 0.7715450583438774,
      "grad_norm": 1.4139792919158936,
      "learning_rate": 0.00010953996565899314,
      "loss": 1.0146,
      "step": 5488
    },
    {
      "epoch": 0.7716856460002812,
      "grad_norm": 1.3802932500839233,
      "learning_rate": 0.00010977134887873885,
      "loss": 0.9961,
      "step": 5489
    },
    {
      "epoch": 0.7718262336566849,
      "grad_norm": 1.2879211902618408,
      "learning_rate": 0.00011000267929212378,
      "loss": 1.0622,
      "step": 5490
    },
    {
      "epoch": 0.7719668213130887,
      "grad_norm": 1.4274189472198486,
      "learning_rate": 0.00011023395564899062,
      "loss": 1.1386,
      "step": 5491
    },
    {
      "epoch": 0.7721074089694925,
      "grad_norm": 1.9872201681137085,
      "learning_rate": 0.00011046517669947538,
      "loss": 1.0529,
      "step": 5492
    },
    {
      "epoch": 0.7722479966258963,
      "grad_norm": 1.5062114000320435,
      "learning_rate": 0.000110696341194012,
      "loss": 1.1821,
      "step": 5493
    },
    {
      "epoch": 0.7723885842823001,
      "grad_norm": 1.1962296962738037,
      "learning_rate": 0.00011092744788334095,
      "loss": 1.1499,
      "step": 5494
    },
    {
      "epoch": 0.7725291719387037,
      "grad_norm": 1.3849213123321533,
      "learning_rate": 0.00011115849551851407,
      "loss": 1.2284,
      "step": 5495
    },
    {
      "epoch": 0.7726697595951075,
      "grad_norm": 1.3935222625732422,
      "learning_rate": 0.00011138948285090283,
      "loss": 1.1122,
      "step": 5496
    },
    {
      "epoch": 0.7728103472515113,
      "grad_norm": 1.4547971487045288,
      "learning_rate": 0.00011162040863220496,
      "loss": 1.0737,
      "step": 5497
    },
    {
      "epoch": 0.7729509349079151,
      "grad_norm": 1.1961450576782227,
      "learning_rate": 0.00011185127161445048,
      "loss": 0.9877,
      "step": 5498
    },
    {
      "epoch": 0.7730915225643189,
      "grad_norm": 1.2929692268371582,
      "learning_rate": 0.00011208207055000823,
      "loss": 1.0615,
      "step": 5499
    },
    {
      "epoch": 0.7732321102207226,
      "grad_norm": 1.6532726287841797,
      "learning_rate": 0.00011231280419159426,
      "loss": 1.0553,
      "step": 5500
    },
    {
      "epoch": 0.7732321102207226,
      "eval_loss": 1.1459760665893555,
      "eval_runtime": 771.5148,
      "eval_samples_per_second": 16.391,
      "eval_steps_per_second": 8.196,
      "step": 5500
    },
    {
      "epoch": 0.7733726978771264,
      "grad_norm": 1.2698248624801636,
      "learning_rate": 0.0001125434712922766,
      "loss": 1.1914,
      "step": 5501
    },
    {
      "epoch": 0.7735132855335302,
      "grad_norm": 1.7252577543258667,
      "learning_rate": 0.00011277407060548373,
      "loss": 1.1052,
      "step": 5502
    },
    {
      "epoch": 0.773653873189934,
      "grad_norm": 1.3226394653320312,
      "learning_rate": 0.00011300460088500944,
      "loss": 1.114,
      "step": 5503
    },
    {
      "epoch": 0.7737944608463377,
      "grad_norm": 1.6742491722106934,
      "learning_rate": 0.00011323506088502111,
      "loss": 1.001,
      "step": 5504
    },
    {
      "epoch": 0.7739350485027414,
      "grad_norm": 1.4988573789596558,
      "learning_rate": 0.00011346544936006629,
      "loss": 0.9822,
      "step": 5505
    },
    {
      "epoch": 0.7740756361591452,
      "grad_norm": 1.6405117511749268,
      "learning_rate": 0.00011369576506507869,
      "loss": 1.0969,
      "step": 5506
    },
    {
      "epoch": 0.774216223815549,
      "grad_norm": 1.5313422679901123,
      "learning_rate": 0.00011392600675538474,
      "loss": 1.2033,
      "step": 5507
    },
    {
      "epoch": 0.7743568114719528,
      "grad_norm": 1.4461616277694702,
      "learning_rate": 0.00011415617318671198,
      "loss": 1.0984,
      "step": 5508
    },
    {
      "epoch": 0.7744973991283566,
      "grad_norm": 1.629564642906189,
      "learning_rate": 0.00011438626311519411,
      "loss": 1.0671,
      "step": 5509
    },
    {
      "epoch": 0.7746379867847603,
      "grad_norm": 1.3945621252059937,
      "learning_rate": 0.00011461627529737771,
      "loss": 1.1941,
      "step": 5510
    },
    {
      "epoch": 0.774778574441164,
      "grad_norm": 1.4565906524658203,
      "learning_rate": 0.0001148462084902306,
      "loss": 1.1012,
      "step": 5511
    },
    {
      "epoch": 0.7749191620975678,
      "grad_norm": 1.569475769996643,
      "learning_rate": 0.00011507606145114657,
      "loss": 0.9551,
      "step": 5512
    },
    {
      "epoch": 0.7750597497539716,
      "grad_norm": 1.548414707183838,
      "learning_rate": 0.00011530583293795392,
      "loss": 1.051,
      "step": 5513
    },
    {
      "epoch": 0.7752003374103754,
      "grad_norm": 1.4310904741287231,
      "learning_rate": 0.00011553552170892018,
      "loss": 1.2146,
      "step": 5514
    },
    {
      "epoch": 0.7753409250667791,
      "grad_norm": 2.152693271636963,
      "learning_rate": 0.00011576512652276037,
      "loss": 0.9179,
      "step": 5515
    },
    {
      "epoch": 0.7754815127231829,
      "grad_norm": 1.5413626432418823,
      "learning_rate": 0.00011599464613864368,
      "loss": 1.1686,
      "step": 5516
    },
    {
      "epoch": 0.7756221003795867,
      "grad_norm": 1.6530675888061523,
      "learning_rate": 0.00011622407931619927,
      "loss": 1.1397,
      "step": 5517
    },
    {
      "epoch": 0.7757626880359905,
      "grad_norm": 1.4358413219451904,
      "learning_rate": 0.00011645342481552301,
      "loss": 1.0134,
      "step": 5518
    },
    {
      "epoch": 0.7759032756923943,
      "grad_norm": 1.4288928508758545,
      "learning_rate": 0.00011668268139718559,
      "loss": 1.2383,
      "step": 5519
    },
    {
      "epoch": 0.7760438633487979,
      "grad_norm": 1.7640436887741089,
      "learning_rate": 0.00011691184782223735,
      "loss": 0.9707,
      "step": 5520
    },
    {
      "epoch": 0.7761844510052017,
      "grad_norm": 1.6170525550842285,
      "learning_rate": 0.00011714092285221674,
      "loss": 1.1632,
      "step": 5521
    },
    {
      "epoch": 0.7763250386616055,
      "grad_norm": 1.582384705543518,
      "learning_rate": 0.00011736990524915508,
      "loss": 1.1946,
      "step": 5522
    },
    {
      "epoch": 0.7764656263180093,
      "grad_norm": 1.4584747552871704,
      "learning_rate": 0.00011759879377558485,
      "loss": 1.1679,
      "step": 5523
    },
    {
      "epoch": 0.7766062139744131,
      "grad_norm": 1.5388820171356201,
      "learning_rate": 0.00011782758719454612,
      "loss": 1.2476,
      "step": 5524
    },
    {
      "epoch": 0.7767468016308168,
      "grad_norm": 1.3941351175308228,
      "learning_rate": 0.00011805628426959266,
      "loss": 1.3018,
      "step": 5525
    },
    {
      "epoch": 0.7768873892872206,
      "grad_norm": 1.4740849733352661,
      "learning_rate": 0.00011828488376479828,
      "loss": 0.9744,
      "step": 5526
    },
    {
      "epoch": 0.7770279769436244,
      "grad_norm": 1.610359787940979,
      "learning_rate": 0.00011851338444476527,
      "loss": 1.1878,
      "step": 5527
    },
    {
      "epoch": 0.7771685646000281,
      "grad_norm": 1.3510148525238037,
      "learning_rate": 0.00011874178507462921,
      "loss": 1.0118,
      "step": 5528
    },
    {
      "epoch": 0.7773091522564319,
      "grad_norm": 1.4790215492248535,
      "learning_rate": 0.00011897008442006654,
      "loss": 1.1149,
      "step": 5529
    },
    {
      "epoch": 0.7774497399128356,
      "grad_norm": 1.4522075653076172,
      "learning_rate": 0.00011919828124730106,
      "loss": 1.0955,
      "step": 5530
    },
    {
      "epoch": 0.7775903275692394,
      "grad_norm": 1.5414754152297974,
      "learning_rate": 0.00011942637432311044,
      "loss": 1.1956,
      "step": 5531
    },
    {
      "epoch": 0.7777309152256432,
      "grad_norm": 1.4726097583770752,
      "learning_rate": 0.0001196543624148337,
      "loss": 1.2357,
      "step": 5532
    },
    {
      "epoch": 0.777871502882047,
      "grad_norm": 1.6013164520263672,
      "learning_rate": 0.00011988224429037622,
      "loss": 1.0539,
      "step": 5533
    },
    {
      "epoch": 0.7780120905384508,
      "grad_norm": 1.358528971672058,
      "learning_rate": 0.0001201100187182179,
      "loss": 0.9743,
      "step": 5534
    },
    {
      "epoch": 0.7781526781948545,
      "grad_norm": 1.4335037469863892,
      "learning_rate": 0.00012033768446741964,
      "loss": 1.1016,
      "step": 5535
    },
    {
      "epoch": 0.7782932658512582,
      "grad_norm": 1.4006026983261108,
      "learning_rate": 0.00012056524030762916,
      "loss": 1.0348,
      "step": 5536
    },
    {
      "epoch": 0.778433853507662,
      "grad_norm": 1.6717196702957153,
      "learning_rate": 0.00012079268500908833,
      "loss": 1.0769,
      "step": 5537
    },
    {
      "epoch": 0.7785744411640658,
      "grad_norm": 1.7739191055297852,
      "learning_rate": 0.00012102001734263962,
      "loss": 1.1113,
      "step": 5538
    },
    {
      "epoch": 0.7787150288204696,
      "grad_norm": 1.748705267906189,
      "learning_rate": 0.00012124723607973256,
      "loss": 1.0496,
      "step": 5539
    },
    {
      "epoch": 0.7788556164768733,
      "grad_norm": 1.5208414793014526,
      "learning_rate": 0.00012147433999243134,
      "loss": 1.1185,
      "step": 5540
    },
    {
      "epoch": 0.7789962041332771,
      "grad_norm": 1.5571465492248535,
      "learning_rate": 0.00012170132785341959,
      "loss": 0.9324,
      "step": 5541
    },
    {
      "epoch": 0.7791367917896809,
      "grad_norm": 1.5662784576416016,
      "learning_rate": 0.00012192819843600865,
      "loss": 1.1571,
      "step": 5542
    },
    {
      "epoch": 0.7792773794460847,
      "grad_norm": 1.7923420667648315,
      "learning_rate": 0.00012215495051414395,
      "loss": 1.1033,
      "step": 5543
    },
    {
      "epoch": 0.7794179671024885,
      "grad_norm": 1.4136308431625366,
      "learning_rate": 0.00012238158286241113,
      "loss": 1.0235,
      "step": 5544
    },
    {
      "epoch": 0.7795585547588921,
      "grad_norm": 1.5282459259033203,
      "learning_rate": 0.0001226080942560422,
      "loss": 1.0131,
      "step": 5545
    },
    {
      "epoch": 0.7796991424152959,
      "grad_norm": 1.3774656057357788,
      "learning_rate": 0.00012283448347092394,
      "loss": 1.0862,
      "step": 5546
    },
    {
      "epoch": 0.7798397300716997,
      "grad_norm": 1.3188681602478027,
      "learning_rate": 0.00012306074928360275,
      "loss": 1.1468,
      "step": 5547
    },
    {
      "epoch": 0.7799803177281035,
      "grad_norm": 1.6134953498840332,
      "learning_rate": 0.00012328689047129204,
      "loss": 1.0852,
      "step": 5548
    },
    {
      "epoch": 0.7801209053845073,
      "grad_norm": 1.786989688873291,
      "learning_rate": 0.00012351290581187873,
      "loss": 1.0076,
      "step": 5549
    },
    {
      "epoch": 0.780261493040911,
      "grad_norm": 1.4669002294540405,
      "learning_rate": 0.00012373879408392972,
      "loss": 1.1699,
      "step": 5550
    },
    {
      "epoch": 0.7804020806973148,
      "grad_norm": 1.602731704711914,
      "learning_rate": 0.0001239645540666992,
      "loss": 1.2429,
      "step": 5551
    },
    {
      "epoch": 0.7805426683537185,
      "grad_norm": 1.4792585372924805,
      "learning_rate": 0.00012419018454013373,
      "loss": 1.0552,
      "step": 5552
    },
    {
      "epoch": 0.7806832560101223,
      "grad_norm": 1.660774827003479,
      "learning_rate": 0.00012441568428488023,
      "loss": 1.0405,
      "step": 5553
    },
    {
      "epoch": 0.7808238436665261,
      "grad_norm": 1.5376135110855103,
      "learning_rate": 0.0001246410520822925,
      "loss": 1.2104,
      "step": 5554
    },
    {
      "epoch": 0.7809644313229298,
      "grad_norm": 1.5293513536453247,
      "learning_rate": 0.00012486628671443679,
      "loss": 1.0458,
      "step": 5555
    },
    {
      "epoch": 0.7811050189793336,
      "grad_norm": 1.7144984006881714,
      "learning_rate": 0.00012509138696409927,
      "loss": 0.9319,
      "step": 5556
    },
    {
      "epoch": 0.7812456066357374,
      "grad_norm": 1.2997870445251465,
      "learning_rate": 0.0001253163516147923,
      "loss": 1.1437,
      "step": 5557
    },
    {
      "epoch": 0.7813861942921412,
      "grad_norm": 1.3640278577804565,
      "learning_rate": 0.00012554117945076093,
      "loss": 1.0736,
      "step": 5558
    },
    {
      "epoch": 0.781526781948545,
      "grad_norm": 1.3622573614120483,
      "learning_rate": 0.00012576586925699013,
      "loss": 1.0605,
      "step": 5559
    },
    {
      "epoch": 0.7816673696049486,
      "grad_norm": 1.6923670768737793,
      "learning_rate": 0.00012599041981920997,
      "loss": 1.149,
      "step": 5560
    },
    {
      "epoch": 0.7818079572613524,
      "grad_norm": 1.5631389617919922,
      "learning_rate": 0.0001262148299239034,
      "loss": 1.1489,
      "step": 5561
    },
    {
      "epoch": 0.7819485449177562,
      "grad_norm": 1.3515472412109375,
      "learning_rate": 0.00012643909835831281,
      "loss": 1.1279,
      "step": 5562
    },
    {
      "epoch": 0.78208913257416,
      "grad_norm": 1.6097798347473145,
      "learning_rate": 0.00012666322391044572,
      "loss": 0.9772,
      "step": 5563
    },
    {
      "epoch": 0.7822297202305638,
      "grad_norm": 1.2947033643722534,
      "learning_rate": 0.0001268872053690819,
      "loss": 1.1287,
      "step": 5564
    },
    {
      "epoch": 0.7823703078869675,
      "grad_norm": 1.4198120832443237,
      "learning_rate": 0.00012711104152378,
      "loss": 1.0299,
      "step": 5565
    },
    {
      "epoch": 0.7825108955433713,
      "grad_norm": 1.415968656539917,
      "learning_rate": 0.00012733473116488374,
      "loss": 1.1621,
      "step": 5566
    },
    {
      "epoch": 0.7826514831997751,
      "grad_norm": 1.5678163766860962,
      "learning_rate": 0.00012755827308352872,
      "loss": 0.9933,
      "step": 5567
    },
    {
      "epoch": 0.7827920708561789,
      "grad_norm": 1.6019206047058105,
      "learning_rate": 0.00012778166607164886,
      "loss": 1.0861,
      "step": 5568
    },
    {
      "epoch": 0.7829326585125826,
      "grad_norm": 1.6614123582839966,
      "learning_rate": 0.00012800490892198274,
      "loss": 1.1858,
      "step": 5569
    },
    {
      "epoch": 0.7830732461689863,
      "grad_norm": 1.6095753908157349,
      "learning_rate": 0.000128228000428081,
      "loss": 1.0735,
      "step": 5570
    },
    {
      "epoch": 0.7832138338253901,
      "grad_norm": 1.421250820159912,
      "learning_rate": 0.00012845093938431098,
      "loss": 1.0489,
      "step": 5571
    },
    {
      "epoch": 0.7833544214817939,
      "grad_norm": 1.5175213813781738,
      "learning_rate": 0.00012867372458586567,
      "loss": 1.1077,
      "step": 5572
    },
    {
      "epoch": 0.7834950091381977,
      "grad_norm": 1.3379660844802856,
      "learning_rate": 0.00012889635482876824,
      "loss": 0.9964,
      "step": 5573
    },
    {
      "epoch": 0.7836355967946015,
      "grad_norm": 1.5404894351959229,
      "learning_rate": 0.00012911882890987946,
      "loss": 1.0018,
      "step": 5574
    },
    {
      "epoch": 0.7837761844510052,
      "grad_norm": 1.65001380443573,
      "learning_rate": 0.00012934114562690406,
      "loss": 1.2196,
      "step": 5575
    },
    {
      "epoch": 0.783916772107409,
      "grad_norm": 1.322904109954834,
      "learning_rate": 0.0001295633037783972,
      "loss": 1.0365,
      "step": 5576
    },
    {
      "epoch": 0.7840573597638127,
      "grad_norm": 1.5621763467788696,
      "learning_rate": 0.00012978530216377078,
      "loss": 1.0583,
      "step": 5577
    },
    {
      "epoch": 0.7841979474202165,
      "grad_norm": 1.376332402229309,
      "learning_rate": 0.00013000713958330077,
      "loss": 1.0072,
      "step": 5578
    },
    {
      "epoch": 0.7843385350766203,
      "grad_norm": 1.296595573425293,
      "learning_rate": 0.00013022881483813186,
      "loss": 1.0079,
      "step": 5579
    },
    {
      "epoch": 0.784479122733024,
      "grad_norm": 1.4044209718704224,
      "learning_rate": 0.0001304503267302863,
      "loss": 1.0362,
      "step": 5580
    },
    {
      "epoch": 0.7846197103894278,
      "grad_norm": 1.5517892837524414,
      "learning_rate": 0.0001306716740626685,
      "loss": 1.0055,
      "step": 5581
    },
    {
      "epoch": 0.7847602980458316,
      "grad_norm": 1.451204538345337,
      "learning_rate": 0.00013089285563907242,
      "loss": 1.0962,
      "step": 5582
    },
    {
      "epoch": 0.7849008857022354,
      "grad_norm": 1.4309781789779663,
      "learning_rate": 0.00013111387026418765,
      "loss": 1.1922,
      "step": 5583
    },
    {
      "epoch": 0.7850414733586392,
      "grad_norm": 1.6148037910461426,
      "learning_rate": 0.00013133471674360613,
      "loss": 1.1475,
      "step": 5584
    },
    {
      "epoch": 0.7851820610150428,
      "grad_norm": 1.5660749673843384,
      "learning_rate": 0.00013155539388382852,
      "loss": 1.0952,
      "step": 5585
    },
    {
      "epoch": 0.7853226486714466,
      "grad_norm": 1.5160824060440063,
      "learning_rate": 0.0001317759004922705,
      "loss": 1.0711,
      "step": 5586
    },
    {
      "epoch": 0.7854632363278504,
      "grad_norm": 1.8241794109344482,
      "learning_rate": 0.0001319962353772695,
      "loss": 1.0456,
      "step": 5587
    },
    {
      "epoch": 0.7856038239842542,
      "grad_norm": 1.465441346168518,
      "learning_rate": 0.00013221639734809062,
      "loss": 0.9879,
      "step": 5588
    },
    {
      "epoch": 0.785744411640658,
      "grad_norm": 1.572426676750183,
      "learning_rate": 0.00013243638521493432,
      "loss": 1.0694,
      "step": 5589
    },
    {
      "epoch": 0.7858849992970617,
      "grad_norm": 1.533159613609314,
      "learning_rate": 0.00013265619778894054,
      "loss": 1.2315,
      "step": 5590
    },
    {
      "epoch": 0.7860255869534655,
      "grad_norm": 1.3051337003707886,
      "learning_rate": 0.00013287583388219788,
      "loss": 1.2876,
      "step": 5591
    },
    {
      "epoch": 0.7861661746098693,
      "grad_norm": 1.730096697807312,
      "learning_rate": 0.00013309529230774798,
      "loss": 1.039,
      "step": 5592
    },
    {
      "epoch": 0.786306762266273,
      "grad_norm": 1.4026286602020264,
      "learning_rate": 0.00013331457187959274,
      "loss": 1.0175,
      "step": 5593
    },
    {
      "epoch": 0.7864473499226768,
      "grad_norm": 1.7236859798431396,
      "learning_rate": 0.0001335336714127007,
      "loss": 0.9509,
      "step": 5594
    },
    {
      "epoch": 0.7865879375790805,
      "grad_norm": 1.5127487182617188,
      "learning_rate": 0.0001337525897230133,
      "loss": 1.1074,
      "step": 5595
    },
    {
      "epoch": 0.7867285252354843,
      "grad_norm": 1.946820616722107,
      "learning_rate": 0.00013397132562745117,
      "loss": 1.1296,
      "step": 5596
    },
    {
      "epoch": 0.7868691128918881,
      "grad_norm": 1.4466369152069092,
      "learning_rate": 0.00013418987794392145,
      "loss": 1.0444,
      "step": 5597
    },
    {
      "epoch": 0.7870097005482919,
      "grad_norm": 1.4259413480758667,
      "learning_rate": 0.00013440824549132215,
      "loss": 1.087,
      "step": 5598
    },
    {
      "epoch": 0.7871502882046957,
      "grad_norm": 1.3837573528289795,
      "learning_rate": 0.0001346264270895511,
      "loss": 1.0958,
      "step": 5599
    },
    {
      "epoch": 0.7872908758610994,
      "grad_norm": 1.7041758298873901,
      "learning_rate": 0.0001348444215595103,
      "loss": 1.0768,
      "step": 5600
    },
    {
      "epoch": 0.7874314635175031,
      "grad_norm": 1.7288165092468262,
      "learning_rate": 0.00013506222772311316,
      "loss": 1.2027,
      "step": 5601
    },
    {
      "epoch": 0.7875720511739069,
      "grad_norm": 1.2921909093856812,
      "learning_rate": 0.0001352798444032908,
      "loss": 1.0523,
      "step": 5602
    },
    {
      "epoch": 0.7877126388303107,
      "grad_norm": 1.8259844779968262,
      "learning_rate": 0.00013549727042399832,
      "loss": 1.0083,
      "step": 5603
    },
    {
      "epoch": 0.7878532264867145,
      "grad_norm": 1.414042353630066,
      "learning_rate": 0.000135714504610221,
      "loss": 1.0231,
      "step": 5604
    },
    {
      "epoch": 0.7879938141431182,
      "grad_norm": 1.3684697151184082,
      "learning_rate": 0.00013593154578798157,
      "loss": 1.0622,
      "step": 5605
    },
    {
      "epoch": 0.788134401799522,
      "grad_norm": 1.401868224143982,
      "learning_rate": 0.00013614839278434457,
      "loss": 0.9551,
      "step": 5606
    },
    {
      "epoch": 0.7882749894559258,
      "grad_norm": 1.4144128561019897,
      "learning_rate": 0.00013636504442742512,
      "loss": 1.1625,
      "step": 5607
    },
    {
      "epoch": 0.7884155771123296,
      "grad_norm": 1.4678175449371338,
      "learning_rate": 0.0001365814995463936,
      "loss": 1.1232,
      "step": 5608
    },
    {
      "epoch": 0.7885561647687334,
      "grad_norm": 1.3464306592941284,
      "learning_rate": 0.00013679775697148194,
      "loss": 0.9356,
      "step": 5609
    },
    {
      "epoch": 0.788696752425137,
      "grad_norm": 1.4150291681289673,
      "learning_rate": 0.0001370138155339914,
      "loss": 1.2191,
      "step": 5610
    },
    {
      "epoch": 0.7888373400815408,
      "grad_norm": 1.5973212718963623,
      "learning_rate": 0.00013722967406629732,
      "loss": 0.9093,
      "step": 5611
    },
    {
      "epoch": 0.7889779277379446,
      "grad_norm": 1.2328665256500244,
      "learning_rate": 0.00013744533140185622,
      "loss": 1.1888,
      "step": 5612
    },
    {
      "epoch": 0.7891185153943484,
      "grad_norm": 1.6444063186645508,
      "learning_rate": 0.00013766078637521183,
      "loss": 0.8679,
      "step": 5613
    },
    {
      "epoch": 0.7892591030507522,
      "grad_norm": 1.3145257234573364,
      "learning_rate": 0.00013787603782200146,
      "loss": 1.1918,
      "step": 5614
    },
    {
      "epoch": 0.7893996907071559,
      "grad_norm": 1.5414947271347046,
      "learning_rate": 0.00013809108457896275,
      "loss": 1.0688,
      "step": 5615
    },
    {
      "epoch": 0.7895402783635597,
      "grad_norm": 1.3908318281173706,
      "learning_rate": 0.00013830592548393916,
      "loss": 1.0648,
      "step": 5616
    },
    {
      "epoch": 0.7896808660199635,
      "grad_norm": 1.480819582939148,
      "learning_rate": 0.00013852055937588618,
      "loss": 1.2854,
      "step": 5617
    },
    {
      "epoch": 0.7898214536763672,
      "grad_norm": 1.3167513608932495,
      "learning_rate": 0.00013873498509487896,
      "loss": 1.1598,
      "step": 5618
    },
    {
      "epoch": 0.789962041332771,
      "grad_norm": 1.723859429359436,
      "learning_rate": 0.00013894920148211707,
      "loss": 1.2355,
      "step": 5619
    },
    {
      "epoch": 0.7901026289891747,
      "grad_norm": 1.5559337139129639,
      "learning_rate": 0.00013916320737993154,
      "loss": 1.2829,
      "step": 5620
    },
    {
      "epoch": 0.7902432166455785,
      "grad_norm": 1.6864264011383057,
      "learning_rate": 0.0001393770016317908,
      "loss": 1.1167,
      "step": 5621
    },
    {
      "epoch": 0.7903838043019823,
      "grad_norm": 1.5447458028793335,
      "learning_rate": 0.00013959058308230717,
      "loss": 1.0327,
      "step": 5622
    },
    {
      "epoch": 0.7905243919583861,
      "grad_norm": 1.7024704217910767,
      "learning_rate": 0.00013980395057724282,
      "loss": 1.1748,
      "step": 5623
    },
    {
      "epoch": 0.7906649796147899,
      "grad_norm": 1.7572596073150635,
      "learning_rate": 0.00014001710296351677,
      "loss": 1.1019,
      "step": 5624
    },
    {
      "epoch": 0.7908055672711936,
      "grad_norm": 1.7711588144302368,
      "learning_rate": 0.0001402300390892094,
      "loss": 1.0094,
      "step": 5625
    },
    {
      "epoch": 0.7909461549275973,
      "grad_norm": 1.3348605632781982,
      "learning_rate": 0.00014044275780357107,
      "loss": 1.0278,
      "step": 5626
    },
    {
      "epoch": 0.7910867425840011,
      "grad_norm": 1.4561105966567993,
      "learning_rate": 0.0001406552579570264,
      "loss": 0.9291,
      "step": 5627
    },
    {
      "epoch": 0.7912273302404049,
      "grad_norm": 1.5254851579666138,
      "learning_rate": 0.0001408675384011808,
      "loss": 0.9715,
      "step": 5628
    },
    {
      "epoch": 0.7913679178968087,
      "grad_norm": 1.6428312063217163,
      "learning_rate": 0.00014107959798882792,
      "loss": 1.1775,
      "step": 5629
    },
    {
      "epoch": 0.7915085055532124,
      "grad_norm": 1.477127194404602,
      "learning_rate": 0.00014129143557395447,
      "loss": 1.0734,
      "step": 5630
    },
    {
      "epoch": 0.7916490932096162,
      "grad_norm": 1.3617712259292603,
      "learning_rate": 0.00014150305001174685,
      "loss": 1.0805,
      "step": 5631
    },
    {
      "epoch": 0.79178968086602,
      "grad_norm": 1.3348193168640137,
      "learning_rate": 0.00014171444015859754,
      "loss": 0.9355,
      "step": 5632
    },
    {
      "epoch": 0.7919302685224238,
      "grad_norm": 1.544110655784607,
      "learning_rate": 0.00014192560487211097,
      "loss": 1.247,
      "step": 5633
    },
    {
      "epoch": 0.7920708561788276,
      "grad_norm": 1.5258915424346924,
      "learning_rate": 0.0001421365430111103,
      "loss": 1.0826,
      "step": 5634
    },
    {
      "epoch": 0.7922114438352312,
      "grad_norm": 1.6430495977401733,
      "learning_rate": 0.0001423472534356428,
      "loss": 1.0478,
      "step": 5635
    },
    {
      "epoch": 0.792352031491635,
      "grad_norm": 1.5684338808059692,
      "learning_rate": 0.0001425577350069859,
      "loss": 0.9938,
      "step": 5636
    },
    {
      "epoch": 0.7924926191480388,
      "grad_norm": 1.5994559526443481,
      "learning_rate": 0.00014276798658765483,
      "loss": 1.0314,
      "step": 5637
    },
    {
      "epoch": 0.7926332068044426,
      "grad_norm": 1.4762688875198364,
      "learning_rate": 0.000142978007041407,
      "loss": 0.9539,
      "step": 5638
    },
    {
      "epoch": 0.7927737944608464,
      "grad_norm": 1.5123330354690552,
      "learning_rate": 0.00014318779523324904,
      "loss": 1.0758,
      "step": 5639
    },
    {
      "epoch": 0.7929143821172501,
      "grad_norm": 1.7013332843780518,
      "learning_rate": 0.00014339735002944285,
      "loss": 1.0451,
      "step": 5640
    },
    {
      "epoch": 0.7930549697736539,
      "grad_norm": 1.6365585327148438,
      "learning_rate": 0.00014360667029751138,
      "loss": 1.03,
      "step": 5641
    },
    {
      "epoch": 0.7931955574300577,
      "grad_norm": 1.7630963325500488,
      "learning_rate": 0.0001438157549062456,
      "loss": 1.0132,
      "step": 5642
    },
    {
      "epoch": 0.7933361450864614,
      "grad_norm": 1.7885102033615112,
      "learning_rate": 0.0001440246027257097,
      "loss": 0.9106,
      "step": 5643
    },
    {
      "epoch": 0.7934767327428652,
      "grad_norm": 1.5088754892349243,
      "learning_rate": 0.00014423321262724693,
      "loss": 1.07,
      "step": 5644
    },
    {
      "epoch": 0.7936173203992689,
      "grad_norm": 1.746026873588562,
      "learning_rate": 0.00014444158348348742,
      "loss": 1.1331,
      "step": 5645
    },
    {
      "epoch": 0.7937579080556727,
      "grad_norm": 1.6333361864089966,
      "learning_rate": 0.00014464971416835252,
      "loss": 1.1466,
      "step": 5646
    },
    {
      "epoch": 0.7938984957120765,
      "grad_norm": 1.7398219108581543,
      "learning_rate": 0.0001448576035570612,
      "loss": 1.0189,
      "step": 5647
    },
    {
      "epoch": 0.7940390833684803,
      "grad_norm": 1.4198212623596191,
      "learning_rate": 0.0001450652505261372,
      "loss": 1.1773,
      "step": 5648
    },
    {
      "epoch": 0.794179671024884,
      "grad_norm": 1.8787436485290527,
      "learning_rate": 0.00014527265395341375,
      "loss": 1.2396,
      "step": 5649
    },
    {
      "epoch": 0.7943202586812877,
      "grad_norm": 1.4699925184249878,
      "learning_rate": 0.00014547981271804085,
      "loss": 1.0412,
      "step": 5650
    },
    {
      "epoch": 0.7944608463376915,
      "grad_norm": 1.6392946243286133,
      "learning_rate": 0.00014568672570048992,
      "loss": 1.0506,
      "step": 5651
    },
    {
      "epoch": 0.7946014339940953,
      "grad_norm": 1.3838471174240112,
      "learning_rate": 0.00014589339178256095,
      "loss": 1.2437,
      "step": 5652
    },
    {
      "epoch": 0.7947420216504991,
      "grad_norm": 1.4227930307388306,
      "learning_rate": 0.00014609980984738868,
      "loss": 1.1783,
      "step": 5653
    },
    {
      "epoch": 0.7948826093069028,
      "grad_norm": 1.7484363317489624,
      "learning_rate": 0.0001463059787794478,
      "loss": 0.9739,
      "step": 5654
    },
    {
      "epoch": 0.7950231969633066,
      "grad_norm": 1.5847816467285156,
      "learning_rate": 0.00014651189746455888,
      "loss": 1.1094,
      "step": 5655
    },
    {
      "epoch": 0.7951637846197104,
      "grad_norm": 1.541422724723816,
      "learning_rate": 0.00014671756478989588,
      "loss": 1.1106,
      "step": 5656
    },
    {
      "epoch": 0.7953043722761142,
      "grad_norm": 1.5829159021377563,
      "learning_rate": 0.00014692297964399032,
      "loss": 0.9962,
      "step": 5657
    },
    {
      "epoch": 0.795444959932518,
      "grad_norm": 1.32750403881073,
      "learning_rate": 0.00014712814091673892,
      "loss": 1.0505,
      "step": 5658
    },
    {
      "epoch": 0.7955855475889216,
      "grad_norm": 1.6743700504302979,
      "learning_rate": 0.0001473330474994079,
      "loss": 1.0747,
      "step": 5659
    },
    {
      "epoch": 0.7957261352453254,
      "grad_norm": 1.4660942554473877,
      "learning_rate": 0.00014753769828464034,
      "loss": 1.3035,
      "step": 5660
    },
    {
      "epoch": 0.7958667229017292,
      "grad_norm": 1.260180115699768,
      "learning_rate": 0.00014774209216646193,
      "loss": 1.0845,
      "step": 5661
    },
    {
      "epoch": 0.796007310558133,
      "grad_norm": 1.5140784978866577,
      "learning_rate": 0.00014794622804028653,
      "loss": 1.1241,
      "step": 5662
    },
    {
      "epoch": 0.7961478982145368,
      "grad_norm": 1.3824011087417603,
      "learning_rate": 0.0001481501048029218,
      "loss": 1.1292,
      "step": 5663
    },
    {
      "epoch": 0.7962884858709405,
      "grad_norm": 1.6494665145874023,
      "learning_rate": 0.00014835372135257657,
      "loss": 1.0817,
      "step": 5664
    },
    {
      "epoch": 0.7964290735273443,
      "grad_norm": 1.3390727043151855,
      "learning_rate": 0.00014855707658886557,
      "loss": 1.0441,
      "step": 5665
    },
    {
      "epoch": 0.7965696611837481,
      "grad_norm": 2.166602611541748,
      "learning_rate": 0.00014876016941281513,
      "loss": 1.404,
      "step": 5666
    },
    {
      "epoch": 0.7967102488401518,
      "grad_norm": 1.5688440799713135,
      "learning_rate": 0.00014896299872687075,
      "loss": 1.2379,
      "step": 5667
    },
    {
      "epoch": 0.7968508364965556,
      "grad_norm": 2.0063350200653076,
      "learning_rate": 0.00014916556343490115,
      "loss": 1.1094,
      "step": 5668
    },
    {
      "epoch": 0.7969914241529593,
      "grad_norm": 1.9332396984100342,
      "learning_rate": 0.0001493678624422058,
      "loss": 1.1097,
      "step": 5669
    },
    {
      "epoch": 0.7971320118093631,
      "grad_norm": 2.1014201641082764,
      "learning_rate": 0.00014956989465551921,
      "loss": 1.1234,
      "step": 5670
    },
    {
      "epoch": 0.7972725994657669,
      "grad_norm": 1.3399276733398438,
      "learning_rate": 0.00014977165898301807,
      "loss": 1.1089,
      "step": 5671
    },
    {
      "epoch": 0.7974131871221707,
      "grad_norm": 1.5425918102264404,
      "learning_rate": 0.00014997315433432716,
      "loss": 1.0159,
      "step": 5672
    },
    {
      "epoch": 0.7975537747785745,
      "grad_norm": 1.4200782775878906,
      "learning_rate": 0.00015017437962052457,
      "loss": 0.9066,
      "step": 5673
    },
    {
      "epoch": 0.7976943624349782,
      "grad_norm": 1.7302223443984985,
      "learning_rate": 0.0001503753337541473,
      "loss": 1.0471,
      "step": 5674
    },
    {
      "epoch": 0.797834950091382,
      "grad_norm": 1.2505970001220703,
      "learning_rate": 0.00015057601564919874,
      "loss": 1.2027,
      "step": 5675
    },
    {
      "epoch": 0.7979755377477857,
      "grad_norm": 1.2446095943450928,
      "learning_rate": 0.00015077642422115276,
      "loss": 1.1713,
      "step": 5676
    },
    {
      "epoch": 0.7981161254041895,
      "grad_norm": 1.4951272010803223,
      "learning_rate": 0.00015097655838696103,
      "loss": 1.0648,
      "step": 5677
    },
    {
      "epoch": 0.7982567130605933,
      "grad_norm": 1.7118096351623535,
      "learning_rate": 0.00015117641706505726,
      "loss": 1.1926,
      "step": 5678
    },
    {
      "epoch": 0.798397300716997,
      "grad_norm": 1.3575928211212158,
      "learning_rate": 0.0001513759991753644,
      "loss": 1.2677,
      "step": 5679
    },
    {
      "epoch": 0.7985378883734008,
      "grad_norm": 1.6323256492614746,
      "learning_rate": 0.00015157530363930035,
      "loss": 0.989,
      "step": 5680
    },
    {
      "epoch": 0.7986784760298046,
      "grad_norm": 1.3392562866210938,
      "learning_rate": 0.0001517743293797832,
      "loss": 1.2168,
      "step": 5681
    },
    {
      "epoch": 0.7988190636862084,
      "grad_norm": 1.3770902156829834,
      "learning_rate": 0.00015197307532123676,
      "loss": 1.2245,
      "step": 5682
    },
    {
      "epoch": 0.7989596513426122,
      "grad_norm": 1.6267471313476562,
      "learning_rate": 0.00015217154038959797,
      "loss": 1.1198,
      "step": 5683
    },
    {
      "epoch": 0.7991002389990158,
      "grad_norm": 1.3441733121871948,
      "learning_rate": 0.000152369723512321,
      "loss": 1.1828,
      "step": 5684
    },
    {
      "epoch": 0.7992408266554196,
      "grad_norm": 1.4427199363708496,
      "learning_rate": 0.00015256762361838384,
      "loss": 1.2098,
      "step": 5685
    },
    {
      "epoch": 0.7993814143118234,
      "grad_norm": 2.031794786453247,
      "learning_rate": 0.00015276523963829395,
      "loss": 1.0708,
      "step": 5686
    },
    {
      "epoch": 0.7995220019682272,
      "grad_norm": 1.4735536575317383,
      "learning_rate": 0.00015296257050409398,
      "loss": 1.0756,
      "step": 5687
    },
    {
      "epoch": 0.799662589624631,
      "grad_norm": 1.6772555112838745,
      "learning_rate": 0.00015315961514936812,
      "loss": 1.268,
      "step": 5688
    },
    {
      "epoch": 0.7998031772810347,
      "grad_norm": 1.6445684432983398,
      "learning_rate": 0.00015335637250924654,
      "loss": 1.178,
      "step": 5689
    },
    {
      "epoch": 0.7999437649374385,
      "grad_norm": 1.5837604999542236,
      "learning_rate": 0.0001535528415204123,
      "loss": 1.0395,
      "step": 5690
    },
    {
      "epoch": 0.8000843525938423,
      "grad_norm": 1.5402393341064453,
      "learning_rate": 0.00015374902112110715,
      "loss": 1.1315,
      "step": 5691
    },
    {
      "epoch": 0.800224940250246,
      "grad_norm": 1.525162935256958,
      "learning_rate": 0.0001539449102511364,
      "loss": 0.9369,
      "step": 5692
    },
    {
      "epoch": 0.8003655279066498,
      "grad_norm": 1.4625253677368164,
      "learning_rate": 0.00015414050785187526,
      "loss": 1.2137,
      "step": 5693
    },
    {
      "epoch": 0.8005061155630535,
      "grad_norm": 1.7157238721847534,
      "learning_rate": 0.00015433581286627442,
      "loss": 1.0715,
      "step": 5694
    },
    {
      "epoch": 0.8006467032194573,
      "grad_norm": 1.378341794013977,
      "learning_rate": 0.00015453082423886564,
      "loss": 1.087,
      "step": 5695
    },
    {
      "epoch": 0.8007872908758611,
      "grad_norm": 1.3470582962036133,
      "learning_rate": 0.00015472554091576813,
      "loss": 1.1046,
      "step": 5696
    },
    {
      "epoch": 0.8009278785322649,
      "grad_norm": 1.4451959133148193,
      "learning_rate": 0.00015491996184469287,
      "loss": 1.4123,
      "step": 5697
    },
    {
      "epoch": 0.8010684661886687,
      "grad_norm": 1.6420955657958984,
      "learning_rate": 0.0001551140859749495,
      "loss": 1.1789,
      "step": 5698
    },
    {
      "epoch": 0.8012090538450723,
      "grad_norm": 1.397789716720581,
      "learning_rate": 0.000155307912257452,
      "loss": 1.0206,
      "step": 5699
    },
    {
      "epoch": 0.8013496415014761,
      "grad_norm": 1.521833896636963,
      "learning_rate": 0.00015550143964472354,
      "loss": 1.1297,
      "step": 5700
    },
    {
      "epoch": 0.8014902291578799,
      "grad_norm": 1.5013799667358398,
      "learning_rate": 0.0001556946670909023,
      "loss": 1.0724,
      "step": 5701
    },
    {
      "epoch": 0.8016308168142837,
      "grad_norm": 1.7038522958755493,
      "learning_rate": 0.00015588759355174823,
      "loss": 1.0359,
      "step": 5702
    },
    {
      "epoch": 0.8017714044706875,
      "grad_norm": 1.7611379623413086,
      "learning_rate": 0.00015608021798464722,
      "loss": 1.0374,
      "step": 5703
    },
    {
      "epoch": 0.8019119921270912,
      "grad_norm": 1.465014934539795,
      "learning_rate": 0.0001562725393486176,
      "loss": 1.0924,
      "step": 5704
    },
    {
      "epoch": 0.802052579783495,
      "grad_norm": 2.1919872760772705,
      "learning_rate": 0.00015646455660431552,
      "loss": 1.1464,
      "step": 5705
    },
    {
      "epoch": 0.8021931674398988,
      "grad_norm": 1.4380505084991455,
      "learning_rate": 0.00015665626871404044,
      "loss": 1.1747,
      "step": 5706
    },
    {
      "epoch": 0.8023337550963026,
      "grad_norm": 1.7741841077804565,
      "learning_rate": 0.00015684767464174145,
      "loss": 1.0196,
      "step": 5707
    },
    {
      "epoch": 0.8024743427527063,
      "grad_norm": 1.659224271774292,
      "learning_rate": 0.00015703877335302154,
      "loss": 1.1425,
      "step": 5708
    },
    {
      "epoch": 0.80261493040911,
      "grad_norm": 1.545297622680664,
      "learning_rate": 0.00015722956381514425,
      "loss": 1.1056,
      "step": 5709
    },
    {
      "epoch": 0.8027555180655138,
      "grad_norm": 1.6022683382034302,
      "learning_rate": 0.00015742004499703932,
      "loss": 1.0439,
      "step": 5710
    },
    {
      "epoch": 0.8028961057219176,
      "grad_norm": 1.3503202199935913,
      "learning_rate": 0.00015761021586930754,
      "loss": 1.0151,
      "step": 5711
    },
    {
      "epoch": 0.8030366933783214,
      "grad_norm": 1.5832364559173584,
      "learning_rate": 0.0001578000754042267,
      "loss": 1.1397,
      "step": 5712
    },
    {
      "epoch": 0.8031772810347252,
      "grad_norm": 1.6288056373596191,
      "learning_rate": 0.00015798962257575726,
      "loss": 1.0815,
      "step": 5713
    },
    {
      "epoch": 0.8033178686911289,
      "grad_norm": 1.432247519493103,
      "learning_rate": 0.00015817885635954738,
      "loss": 1.1311,
      "step": 5714
    },
    {
      "epoch": 0.8034584563475327,
      "grad_norm": 1.627925157546997,
      "learning_rate": 0.00015836777573293969,
      "loss": 1.2746,
      "step": 5715
    },
    {
      "epoch": 0.8035990440039364,
      "grad_norm": 1.5419883728027344,
      "learning_rate": 0.00015855637967497485,
      "loss": 0.9149,
      "step": 5716
    },
    {
      "epoch": 0.8037396316603402,
      "grad_norm": 1.4279956817626953,
      "learning_rate": 0.0001587446671663988,
      "loss": 1.1017,
      "step": 5717
    },
    {
      "epoch": 0.803880219316744,
      "grad_norm": 1.568623661994934,
      "learning_rate": 0.0001589326371896678,
      "loss": 1.1843,
      "step": 5718
    },
    {
      "epoch": 0.8040208069731477,
      "grad_norm": 1.4893977642059326,
      "learning_rate": 0.00015912028872895343,
      "loss": 1.243,
      "step": 5719
    },
    {
      "epoch": 0.8041613946295515,
      "grad_norm": 1.5769963264465332,
      "learning_rate": 0.00015930762077014847,
      "loss": 1.0853,
      "step": 5720
    },
    {
      "epoch": 0.8043019822859553,
      "grad_norm": 1.6960009336471558,
      "learning_rate": 0.0001594946323008724,
      "loss": 1.0485,
      "step": 5721
    },
    {
      "epoch": 0.8044425699423591,
      "grad_norm": 1.4665942192077637,
      "learning_rate": 0.00015968132231047682,
      "loss": 1.2148,
      "step": 5722
    },
    {
      "epoch": 0.8045831575987629,
      "grad_norm": 1.5795809030532837,
      "learning_rate": 0.00015986768979005085,
      "loss": 1.0435,
      "step": 5723
    },
    {
      "epoch": 0.8047237452551665,
      "grad_norm": 1.5913575887680054,
      "learning_rate": 0.0001600537337324266,
      "loss": 1.2155,
      "step": 5724
    },
    {
      "epoch": 0.8048643329115703,
      "grad_norm": 1.5109450817108154,
      "learning_rate": 0.00016023945313218462,
      "loss": 1.1125,
      "step": 5725
    },
    {
      "epoch": 0.8050049205679741,
      "grad_norm": 1.451585292816162,
      "learning_rate": 0.00016042484698565977,
      "loss": 0.9053,
      "step": 5726
    },
    {
      "epoch": 0.8051455082243779,
      "grad_norm": 1.5580374002456665,
      "learning_rate": 0.0001606099142909454,
      "loss": 1.114,
      "step": 5727
    },
    {
      "epoch": 0.8052860958807817,
      "grad_norm": 1.4874242544174194,
      "learning_rate": 0.00016079465404790039,
      "loss": 1.061,
      "step": 5728
    },
    {
      "epoch": 0.8054266835371854,
      "grad_norm": 1.4909095764160156,
      "learning_rate": 0.00016097906525815338,
      "loss": 1.0107,
      "step": 5729
    },
    {
      "epoch": 0.8055672711935892,
      "grad_norm": 1.5545469522476196,
      "learning_rate": 0.00016116314692510856,
      "loss": 1.0742,
      "step": 5730
    },
    {
      "epoch": 0.805707858849993,
      "grad_norm": 1.5215692520141602,
      "learning_rate": 0.00016134689805395112,
      "loss": 1.1531,
      "step": 5731
    },
    {
      "epoch": 0.8058484465063968,
      "grad_norm": 1.5538671016693115,
      "learning_rate": 0.00016153031765165244,
      "loss": 1.0998,
      "step": 5732
    },
    {
      "epoch": 0.8059890341628005,
      "grad_norm": 1.512155294418335,
      "learning_rate": 0.00016171340472697553,
      "loss": 1.0497,
      "step": 5733
    },
    {
      "epoch": 0.8061296218192042,
      "grad_norm": 1.3926509618759155,
      "learning_rate": 0.00016189615829048098,
      "loss": 1.2417,
      "step": 5734
    },
    {
      "epoch": 0.806270209475608,
      "grad_norm": 2.133988380432129,
      "learning_rate": 0.00016207857735453063,
      "loss": 1.1426,
      "step": 5735
    },
    {
      "epoch": 0.8064107971320118,
      "grad_norm": 1.5682400465011597,
      "learning_rate": 0.00016226066093329516,
      "loss": 1.1925,
      "step": 5736
    },
    {
      "epoch": 0.8065513847884156,
      "grad_norm": 1.5079593658447266,
      "learning_rate": 0.00016244240804275765,
      "loss": 1.2005,
      "step": 5737
    },
    {
      "epoch": 0.8066919724448194,
      "grad_norm": 1.4995418787002563,
      "learning_rate": 0.00016262381770071964,
      "loss": 1.1118,
      "step": 5738
    },
    {
      "epoch": 0.8068325601012231,
      "grad_norm": 1.5487691164016724,
      "learning_rate": 0.00016280488892680644,
      "loss": 0.9962,
      "step": 5739
    },
    {
      "epoch": 0.8069731477576269,
      "grad_norm": 1.5999518632888794,
      "learning_rate": 0.0001629856207424721,
      "loss": 0.9394,
      "step": 5740
    },
    {
      "epoch": 0.8071137354140306,
      "grad_norm": 1.5059882402420044,
      "learning_rate": 0.0001631660121710052,
      "loss": 1.075,
      "step": 5741
    },
    {
      "epoch": 0.8072543230704344,
      "grad_norm": 1.2887698411941528,
      "learning_rate": 0.00016334606223753363,
      "loss": 1.0792,
      "step": 5742
    },
    {
      "epoch": 0.8073949107268382,
      "grad_norm": 1.6343480348587036,
      "learning_rate": 0.0001635257699690301,
      "loss": 1.1148,
      "step": 5743
    },
    {
      "epoch": 0.8075354983832419,
      "grad_norm": 1.7865283489227295,
      "learning_rate": 0.0001637051343943173,
      "loss": 1.0572,
      "step": 5744
    },
    {
      "epoch": 0.8076760860396457,
      "grad_norm": 1.4392718076705933,
      "learning_rate": 0.00016388415454407374,
      "loss": 1.2039,
      "step": 5745
    },
    {
      "epoch": 0.8078166736960495,
      "grad_norm": 1.3723490238189697,
      "learning_rate": 0.00016406282945083747,
      "loss": 1.2145,
      "step": 5746
    },
    {
      "epoch": 0.8079572613524533,
      "grad_norm": 1.7210584878921509,
      "learning_rate": 0.00016424115814901326,
      "loss": 1.0788,
      "step": 5747
    },
    {
      "epoch": 0.8080978490088571,
      "grad_norm": 1.3565744161605835,
      "learning_rate": 0.0001644191396748764,
      "loss": 1.2574,
      "step": 5748
    },
    {
      "epoch": 0.8082384366652607,
      "grad_norm": 1.523324966430664,
      "learning_rate": 0.00016459677306657837,
      "loss": 1.0979,
      "step": 5749
    },
    {
      "epoch": 0.8083790243216645,
      "grad_norm": 1.4342013597488403,
      "learning_rate": 0.0001647740573641522,
      "loss": 1.1804,
      "step": 5750
    },
    {
      "epoch": 0.8085196119780683,
      "grad_norm": 1.5282158851623535,
      "learning_rate": 0.00016495099160951737,
      "loss": 1.1458,
      "step": 5751
    },
    {
      "epoch": 0.8086601996344721,
      "grad_norm": 1.5015190839767456,
      "learning_rate": 0.00016512757484648502,
      "loss": 1.1253,
      "step": 5752
    },
    {
      "epoch": 0.8088007872908759,
      "grad_norm": 1.8936903476715088,
      "learning_rate": 0.00016530380612076373,
      "loss": 1.0385,
      "step": 5753
    },
    {
      "epoch": 0.8089413749472796,
      "grad_norm": 1.670353889465332,
      "learning_rate": 0.00016547968447996327,
      "loss": 1.0397,
      "step": 5754
    },
    {
      "epoch": 0.8090819626036834,
      "grad_norm": 1.7460968494415283,
      "learning_rate": 0.0001656552089736015,
      "loss": 1.0467,
      "step": 5755
    },
    {
      "epoch": 0.8092225502600872,
      "grad_norm": 1.4712988138198853,
      "learning_rate": 0.00016583037865310822,
      "loss": 1.1452,
      "step": 5756
    },
    {
      "epoch": 0.809363137916491,
      "grad_norm": 2.1142280101776123,
      "learning_rate": 0.0001660051925718307,
      "loss": 1.2491,
      "step": 5757
    },
    {
      "epoch": 0.8095037255728947,
      "grad_norm": 1.9158499240875244,
      "learning_rate": 0.00016617964978503892,
      "loss": 1.2036,
      "step": 5758
    },
    {
      "epoch": 0.8096443132292984,
      "grad_norm": 1.5129985809326172,
      "learning_rate": 0.00016635374934993057,
      "loss": 1.082,
      "step": 5759
    },
    {
      "epoch": 0.8097849008857022,
      "grad_norm": 1.4786715507507324,
      "learning_rate": 0.00016652749032563591,
      "loss": 1.0775,
      "step": 5760
    },
    {
      "epoch": 0.809925488542106,
      "grad_norm": 1.5206594467163086,
      "learning_rate": 0.00016670087177322377,
      "loss": 1.0725,
      "step": 5761
    },
    {
      "epoch": 0.8100660761985098,
      "grad_norm": 1.438468337059021,
      "learning_rate": 0.00016687389275570492,
      "loss": 1.1579,
      "step": 5762
    },
    {
      "epoch": 0.8102066638549136,
      "grad_norm": 1.5131474733352661,
      "learning_rate": 0.00016704655233803903,
      "loss": 1.1815,
      "step": 5763
    },
    {
      "epoch": 0.8103472515113173,
      "grad_norm": 1.5328601598739624,
      "learning_rate": 0.00016721884958713863,
      "loss": 1.1084,
      "step": 5764
    },
    {
      "epoch": 0.810487839167721,
      "grad_norm": 1.290543794631958,
      "learning_rate": 0.00016739078357187382,
      "loss": 1.1824,
      "step": 5765
    },
    {
      "epoch": 0.8106284268241248,
      "grad_norm": 1.3997935056686401,
      "learning_rate": 0.00016756235336307866,
      "loss": 1.0045,
      "step": 5766
    },
    {
      "epoch": 0.8107690144805286,
      "grad_norm": 1.455236554145813,
      "learning_rate": 0.00016773355803355498,
      "loss": 1.3117,
      "step": 5767
    },
    {
      "epoch": 0.8109096021369324,
      "grad_norm": 1.3410011529922485,
      "learning_rate": 0.00016790439665807775,
      "loss": 1.1312,
      "step": 5768
    },
    {
      "epoch": 0.8110501897933361,
      "grad_norm": 1.5433731079101562,
      "learning_rate": 0.0001680748683134003,
      "loss": 1.134,
      "step": 5769
    },
    {
      "epoch": 0.8111907774497399,
      "grad_norm": 1.546483039855957,
      "learning_rate": 0.00016824497207825888,
      "loss": 0.9889,
      "step": 5770
    },
    {
      "epoch": 0.8113313651061437,
      "grad_norm": 1.4178482294082642,
      "learning_rate": 0.0001684147070333784,
      "loss": 1.1241,
      "step": 5771
    },
    {
      "epoch": 0.8114719527625475,
      "grad_norm": 1.326355218887329,
      "learning_rate": 0.00016858407226147648,
      "loss": 0.9307,
      "step": 5772
    },
    {
      "epoch": 0.8116125404189513,
      "grad_norm": 1.414809226989746,
      "learning_rate": 0.00016875306684726858,
      "loss": 1.0763,
      "step": 5773
    },
    {
      "epoch": 0.8117531280753549,
      "grad_norm": 1.6696404218673706,
      "learning_rate": 0.00016892168987747382,
      "loss": 1.18,
      "step": 5774
    },
    {
      "epoch": 0.8118937157317587,
      "grad_norm": 1.3377764225006104,
      "learning_rate": 0.00016908994044081895,
      "loss": 1.1563,
      "step": 5775
    },
    {
      "epoch": 0.8120343033881625,
      "grad_norm": 1.3739420175552368,
      "learning_rate": 0.0001692578176280436,
      "loss": 0.9673,
      "step": 5776
    },
    {
      "epoch": 0.8121748910445663,
      "grad_norm": 1.4684382677078247,
      "learning_rate": 0.00016942532053190517,
      "loss": 1.16,
      "step": 5777
    },
    {
      "epoch": 0.8123154787009701,
      "grad_norm": 1.4818003177642822,
      "learning_rate": 0.00016959244824718393,
      "loss": 1.0684,
      "step": 5778
    },
    {
      "epoch": 0.8124560663573738,
      "grad_norm": 1.4691050052642822,
      "learning_rate": 0.00016975919987068742,
      "loss": 1.0208,
      "step": 5779
    },
    {
      "epoch": 0.8125966540137776,
      "grad_norm": 1.3566340208053589,
      "learning_rate": 0.00016992557450125627,
      "loss": 1.1719,
      "step": 5780
    },
    {
      "epoch": 0.8127372416701814,
      "grad_norm": 1.4202677011489868,
      "learning_rate": 0.00017009157123976765,
      "loss": 1.1659,
      "step": 5781
    },
    {
      "epoch": 0.8128778293265851,
      "grad_norm": 1.711361050605774,
      "learning_rate": 0.00017025718918914161,
      "loss": 1.1488,
      "step": 5782
    },
    {
      "epoch": 0.8130184169829889,
      "grad_norm": 1.3953537940979004,
      "learning_rate": 0.00017042242745434508,
      "loss": 1.0681,
      "step": 5783
    },
    {
      "epoch": 0.8131590046393926,
      "grad_norm": 1.4220561981201172,
      "learning_rate": 0.00017058728514239636,
      "loss": 1.1057,
      "step": 5784
    },
    {
      "epoch": 0.8132995922957964,
      "grad_norm": 1.9043923616409302,
      "learning_rate": 0.00017075176136237133,
      "loss": 1.0527,
      "step": 5785
    },
    {
      "epoch": 0.8134401799522002,
      "grad_norm": 1.6898139715194702,
      "learning_rate": 0.0001709158552254068,
      "loss": 0.9847,
      "step": 5786
    },
    {
      "epoch": 0.813580767608604,
      "grad_norm": 1.3999412059783936,
      "learning_rate": 0.0001710795658447061,
      "loss": 1.1591,
      "step": 5787
    },
    {
      "epoch": 0.8137213552650078,
      "grad_norm": 1.3460698127746582,
      "learning_rate": 0.00017124289233554372,
      "loss": 1.22,
      "step": 5788
    },
    {
      "epoch": 0.8138619429214115,
      "grad_norm": 1.371561050415039,
      "learning_rate": 0.00017140583381526986,
      "loss": 1.0945,
      "step": 5789
    },
    {
      "epoch": 0.8140025305778152,
      "grad_norm": 1.5573794841766357,
      "learning_rate": 0.00017156838940331575,
      "loss": 1.0005,
      "step": 5790
    },
    {
      "epoch": 0.814143118234219,
      "grad_norm": 1.608441948890686,
      "learning_rate": 0.00017173055822119793,
      "loss": 0.9757,
      "step": 5791
    },
    {
      "epoch": 0.8142837058906228,
      "grad_norm": 1.4796730279922485,
      "learning_rate": 0.00017189233939252262,
      "loss": 1.0384,
      "step": 5792
    },
    {
      "epoch": 0.8144242935470266,
      "grad_norm": 1.5651180744171143,
      "learning_rate": 0.00017205373204299182,
      "loss": 0.9544,
      "step": 5793
    },
    {
      "epoch": 0.8145648812034303,
      "grad_norm": 1.9007986783981323,
      "learning_rate": 0.00017221473530040662,
      "loss": 1.0585,
      "step": 5794
    },
    {
      "epoch": 0.8147054688598341,
      "grad_norm": 1.4783306121826172,
      "learning_rate": 0.00017237534829467256,
      "loss": 1.0407,
      "step": 5795
    },
    {
      "epoch": 0.8148460565162379,
      "grad_norm": 1.6285356283187866,
      "learning_rate": 0.00017253557015780433,
      "loss": 1.1033,
      "step": 5796
    },
    {
      "epoch": 0.8149866441726417,
      "grad_norm": 1.7649298906326294,
      "learning_rate": 0.00017269540002393016,
      "loss": 1.2908,
      "step": 5797
    },
    {
      "epoch": 0.8151272318290455,
      "grad_norm": 1.793298363685608,
      "learning_rate": 0.0001728548370292972,
      "loss": 1.0274,
      "step": 5798
    },
    {
      "epoch": 0.8152678194854491,
      "grad_norm": 1.7948448657989502,
      "learning_rate": 0.00017301388031227524,
      "loss": 1.0774,
      "step": 5799
    },
    {
      "epoch": 0.8154084071418529,
      "grad_norm": 2.266706705093384,
      "learning_rate": 0.0001731725290133617,
      "loss": 1.0117,
      "step": 5800
    },
    {
      "epoch": 0.8155489947982567,
      "grad_norm": 1.4295145273208618,
      "learning_rate": 0.00017333078227518703,
      "loss": 1.073,
      "step": 5801
    },
    {
      "epoch": 0.8156895824546605,
      "grad_norm": 1.2857609987258911,
      "learning_rate": 0.0001734886392425183,
      "loss": 1.029,
      "step": 5802
    },
    {
      "epoch": 0.8158301701110643,
      "grad_norm": 1.5508610010147095,
      "learning_rate": 0.0001736460990622639,
      "loss": 1.0891,
      "step": 5803
    },
    {
      "epoch": 0.815970757767468,
      "grad_norm": 1.4223897457122803,
      "learning_rate": 0.00017380316088347933,
      "loss": 1.1008,
      "step": 5804
    },
    {
      "epoch": 0.8161113454238718,
      "grad_norm": 1.5619823932647705,
      "learning_rate": 0.00017395982385737016,
      "loss": 1.0046,
      "step": 5805
    },
    {
      "epoch": 0.8162519330802755,
      "grad_norm": 1.4877030849456787,
      "learning_rate": 0.00017411608713729815,
      "loss": 1.1834,
      "step": 5806
    },
    {
      "epoch": 0.8163925207366793,
      "grad_norm": 1.4040204286575317,
      "learning_rate": 0.0001742719498787843,
      "loss": 1.1257,
      "step": 5807
    },
    {
      "epoch": 0.8165331083930831,
      "grad_norm": 1.7330501079559326,
      "learning_rate": 0.0001744274112395146,
      "loss": 1.1737,
      "step": 5808
    },
    {
      "epoch": 0.8166736960494868,
      "grad_norm": 1.8112233877182007,
      "learning_rate": 0.00017458247037934442,
      "loss": 1.0542,
      "step": 5809
    },
    {
      "epoch": 0.8168142837058906,
      "grad_norm": 1.6762362718582153,
      "learning_rate": 0.00017473712646030257,
      "loss": 1.0094,
      "step": 5810
    },
    {
      "epoch": 0.8169548713622944,
      "grad_norm": 1.546201467514038,
      "learning_rate": 0.00017489137864659568,
      "loss": 1.0544,
      "step": 5811
    },
    {
      "epoch": 0.8170954590186982,
      "grad_norm": 1.520206093788147,
      "learning_rate": 0.00017504522610461388,
      "loss": 1.0969,
      "step": 5812
    },
    {
      "epoch": 0.817236046675102,
      "grad_norm": 1.6897534132003784,
      "learning_rate": 0.00017519866800293386,
      "loss": 1.1973,
      "step": 5813
    },
    {
      "epoch": 0.8173766343315056,
      "grad_norm": 1.4974918365478516,
      "learning_rate": 0.00017535170351232473,
      "loss": 1.1151,
      "step": 5814
    },
    {
      "epoch": 0.8175172219879094,
      "grad_norm": 1.4645960330963135,
      "learning_rate": 0.00017550433180575103,
      "loss": 1.239,
      "step": 5815
    },
    {
      "epoch": 0.8176578096443132,
      "grad_norm": 1.5788077116012573,
      "learning_rate": 0.00017565655205837837,
      "loss": 1.0783,
      "step": 5816
    },
    {
      "epoch": 0.817798397300717,
      "grad_norm": 1.5918527841567993,
      "learning_rate": 0.0001758083634475777,
      "loss": 1.1872,
      "step": 5817
    },
    {
      "epoch": 0.8179389849571208,
      "grad_norm": 1.7419222593307495,
      "learning_rate": 0.00017595976515292932,
      "loss": 1.0214,
      "step": 5818
    },
    {
      "epoch": 0.8180795726135245,
      "grad_norm": 1.3909293413162231,
      "learning_rate": 0.0001761107563562272,
      "loss": 1.1353,
      "step": 5819
    },
    {
      "epoch": 0.8182201602699283,
      "grad_norm": 1.3777424097061157,
      "learning_rate": 0.00017626133624148448,
      "loss": 1.0974,
      "step": 5820
    },
    {
      "epoch": 0.8183607479263321,
      "grad_norm": 1.5204271078109741,
      "learning_rate": 0.00017641150399493678,
      "loss": 0.9377,
      "step": 5821
    },
    {
      "epoch": 0.8185013355827359,
      "grad_norm": 1.4435449838638306,
      "learning_rate": 0.00017656125880504656,
      "loss": 1.1271,
      "step": 5822
    },
    {
      "epoch": 0.8186419232391396,
      "grad_norm": 1.4259570837020874,
      "learning_rate": 0.00017671059986250866,
      "loss": 1.0431,
      "step": 5823
    },
    {
      "epoch": 0.8187825108955433,
      "grad_norm": 1.399613380432129,
      "learning_rate": 0.00017685952636025337,
      "loss": 1.1579,
      "step": 5824
    },
    {
      "epoch": 0.8189230985519471,
      "grad_norm": 1.6581363677978516,
      "learning_rate": 0.00017700803749345182,
      "loss": 1.1227,
      "step": 5825
    },
    {
      "epoch": 0.8190636862083509,
      "grad_norm": 1.6004389524459839,
      "learning_rate": 0.00017715613245951927,
      "loss": 1.0415,
      "step": 5826
    },
    {
      "epoch": 0.8192042738647547,
      "grad_norm": 1.548508644104004,
      "learning_rate": 0.0001773038104581204,
      "loss": 1.0832,
      "step": 5827
    },
    {
      "epoch": 0.8193448615211585,
      "grad_norm": 1.577166199684143,
      "learning_rate": 0.0001774510706911733,
      "loss": 1.0706,
      "step": 5828
    },
    {
      "epoch": 0.8194854491775622,
      "grad_norm": 1.4846118688583374,
      "learning_rate": 0.00017759791236285376,
      "loss": 1.0193,
      "step": 5829
    },
    {
      "epoch": 0.819626036833966,
      "grad_norm": 1.518710970878601,
      "learning_rate": 0.00017774433467959905,
      "loss": 1.0304,
      "step": 5830
    },
    {
      "epoch": 0.8197666244903697,
      "grad_norm": 1.6334466934204102,
      "learning_rate": 0.00017789033685011355,
      "loss": 1.0469,
      "step": 5831
    },
    {
      "epoch": 0.8199072121467735,
      "grad_norm": 1.352867841720581,
      "learning_rate": 0.00017803591808537145,
      "loss": 1.3038,
      "step": 5832
    },
    {
      "epoch": 0.8200477998031773,
      "grad_norm": 1.4934836626052856,
      "learning_rate": 0.00017818107759862248,
      "loss": 1.1838,
      "step": 5833
    },
    {
      "epoch": 0.820188387459581,
      "grad_norm": 1.9229391813278198,
      "learning_rate": 0.00017832581460539468,
      "loss": 0.9266,
      "step": 5834
    },
    {
      "epoch": 0.8203289751159848,
      "grad_norm": 1.6088701486587524,
      "learning_rate": 0.0001784701283234998,
      "loss": 1.0511,
      "step": 5835
    },
    {
      "epoch": 0.8204695627723886,
      "grad_norm": 1.6429166793823242,
      "learning_rate": 0.00017861401797303724,
      "loss": 0.9433,
      "step": 5836
    },
    {
      "epoch": 0.8206101504287924,
      "grad_norm": 1.5431374311447144,
      "learning_rate": 0.00017875748277639808,
      "loss": 1.1237,
      "step": 5837
    },
    {
      "epoch": 0.8207507380851962,
      "grad_norm": 1.9579063653945923,
      "learning_rate": 0.00017890052195826894,
      "loss": 1.2331,
      "step": 5838
    },
    {
      "epoch": 0.8208913257415998,
      "grad_norm": 1.5910776853561401,
      "learning_rate": 0.00017904313474563721,
      "loss": 1.1749,
      "step": 5839
    },
    {
      "epoch": 0.8210319133980036,
      "grad_norm": 1.8251785039901733,
      "learning_rate": 0.00017918532036779422,
      "loss": 1.2289,
      "step": 5840
    },
    {
      "epoch": 0.8211725010544074,
      "grad_norm": 1.4791548252105713,
      "learning_rate": 0.00017932707805633994,
      "loss": 1.1391,
      "step": 5841
    },
    {
      "epoch": 0.8213130887108112,
      "grad_norm": 1.7422658205032349,
      "learning_rate": 0.00017946840704518687,
      "loss": 1.1383,
      "step": 5842
    },
    {
      "epoch": 0.821453676367215,
      "grad_norm": 1.9155553579330444,
      "learning_rate": 0.00017960930657056427,
      "loss": 1.0473,
      "step": 5843
    },
    {
      "epoch": 0.8215942640236187,
      "grad_norm": 1.4329042434692383,
      "learning_rate": 0.00017974977587102272,
      "loss": 1.0367,
      "step": 5844
    },
    {
      "epoch": 0.8217348516800225,
      "grad_norm": 1.9214993715286255,
      "learning_rate": 0.00017988981418743714,
      "loss": 1.0691,
      "step": 5845
    },
    {
      "epoch": 0.8218754393364263,
      "grad_norm": 1.3206554651260376,
      "learning_rate": 0.0001800294207630119,
      "loss": 1.1629,
      "step": 5846
    },
    {
      "epoch": 0.82201602699283,
      "grad_norm": 1.4965832233428955,
      "learning_rate": 0.00018016859484328487,
      "loss": 1.0505,
      "step": 5847
    },
    {
      "epoch": 0.8221566146492338,
      "grad_norm": 1.573905110359192,
      "learning_rate": 0.00018030733567613085,
      "loss": 1.0702,
      "step": 5848
    },
    {
      "epoch": 0.8222972023056375,
      "grad_norm": 1.425039291381836,
      "learning_rate": 0.00018044564251176607,
      "loss": 1.1299,
      "step": 5849
    },
    {
      "epoch": 0.8224377899620413,
      "grad_norm": 1.5792194604873657,
      "learning_rate": 0.00018058351460275218,
      "loss": 1.0587,
      "step": 5850
    },
    {
      "epoch": 0.8225783776184451,
      "grad_norm": 1.4262478351593018,
      "learning_rate": 0.00018072095120400027,
      "loss": 1.1629,
      "step": 5851
    },
    {
      "epoch": 0.8227189652748489,
      "grad_norm": 1.4793559312820435,
      "learning_rate": 0.00018085795157277517,
      "loss": 1.1445,
      "step": 5852
    },
    {
      "epoch": 0.8228595529312527,
      "grad_norm": 1.4962836503982544,
      "learning_rate": 0.0001809945149686987,
      "loss": 1.2628,
      "step": 5853
    },
    {
      "epoch": 0.8230001405876564,
      "grad_norm": 1.6014152765274048,
      "learning_rate": 0.00018113064065375443,
      "loss": 1.0906,
      "step": 5854
    },
    {
      "epoch": 0.8231407282440601,
      "grad_norm": 2.052616834640503,
      "learning_rate": 0.00018126632789229157,
      "loss": 1.0755,
      "step": 5855
    },
    {
      "epoch": 0.8232813159004639,
      "grad_norm": 1.4809298515319824,
      "learning_rate": 0.00018140157595102866,
      "loss": 1.1324,
      "step": 5856
    },
    {
      "epoch": 0.8234219035568677,
      "grad_norm": 1.9319586753845215,
      "learning_rate": 0.00018153638409905732,
      "loss": 1.2302,
      "step": 5857
    },
    {
      "epoch": 0.8235624912132715,
      "grad_norm": 1.6330680847167969,
      "learning_rate": 0.00018167075160784715,
      "loss": 0.9988,
      "step": 5858
    },
    {
      "epoch": 0.8237030788696752,
      "grad_norm": 1.4707117080688477,
      "learning_rate": 0.0001818046777512487,
      "loss": 0.8977,
      "step": 5859
    },
    {
      "epoch": 0.823843666526079,
      "grad_norm": 1.407244324684143,
      "learning_rate": 0.00018193816180549764,
      "loss": 1.1299,
      "step": 5860
    },
    {
      "epoch": 0.8239842541824828,
      "grad_norm": 1.7882940769195557,
      "learning_rate": 0.00018207120304921905,
      "loss": 1.1249,
      "step": 5861
    },
    {
      "epoch": 0.8241248418388866,
      "grad_norm": 1.556605339050293,
      "learning_rate": 0.0001822038007634308,
      "loss": 1.0504,
      "step": 5862
    },
    {
      "epoch": 0.8242654294952904,
      "grad_norm": 1.4826678037643433,
      "learning_rate": 0.00018233595423154816,
      "loss": 1.0265,
      "step": 5863
    },
    {
      "epoch": 0.824406017151694,
      "grad_norm": 1.488984227180481,
      "learning_rate": 0.00018246766273938647,
      "loss": 1.1522,
      "step": 5864
    },
    {
      "epoch": 0.8245466048080978,
      "grad_norm": 1.414135217666626,
      "learning_rate": 0.00018259892557516617,
      "loss": 1.0611,
      "step": 5865
    },
    {
      "epoch": 0.8246871924645016,
      "grad_norm": 1.7302489280700684,
      "learning_rate": 0.0001827297420295163,
      "loss": 1.0318,
      "step": 5866
    },
    {
      "epoch": 0.8248277801209054,
      "grad_norm": 1.473582148551941,
      "learning_rate": 0.000182860111395478,
      "loss": 1.2567,
      "step": 5867
    },
    {
      "epoch": 0.8249683677773092,
      "grad_norm": 1.6083279848098755,
      "learning_rate": 0.00018299003296850863,
      "loss": 1.1232,
      "step": 5868
    },
    {
      "epoch": 0.8251089554337129,
      "grad_norm": 1.3910558223724365,
      "learning_rate": 0.00018311950604648556,
      "loss": 1.0135,
      "step": 5869
    },
    {
      "epoch": 0.8252495430901167,
      "grad_norm": 1.622670292854309,
      "learning_rate": 0.00018324852992970976,
      "loss": 1.0938,
      "step": 5870
    },
    {
      "epoch": 0.8253901307465205,
      "grad_norm": 1.5981030464172363,
      "learning_rate": 0.00018337710392091018,
      "loss": 1.0627,
      "step": 5871
    },
    {
      "epoch": 0.8255307184029242,
      "grad_norm": 1.694669485092163,
      "learning_rate": 0.00018350522732524645,
      "loss": 1.1464,
      "step": 5872
    },
    {
      "epoch": 0.825671306059328,
      "grad_norm": 1.4717962741851807,
      "learning_rate": 0.00018363289945031358,
      "loss": 1.1185,
      "step": 5873
    },
    {
      "epoch": 0.8258118937157317,
      "grad_norm": 1.7109782695770264,
      "learning_rate": 0.00018376011960614561,
      "loss": 1.2647,
      "step": 5874
    },
    {
      "epoch": 0.8259524813721355,
      "grad_norm": 1.5914998054504395,
      "learning_rate": 0.00018388688710521876,
      "loss": 1.0978,
      "step": 5875
    },
    {
      "epoch": 0.8260930690285393,
      "grad_norm": 1.7676033973693848,
      "learning_rate": 0.00018401320126245564,
      "loss": 1.0879,
      "step": 5876
    },
    {
      "epoch": 0.8262336566849431,
      "grad_norm": 1.522753119468689,
      "learning_rate": 0.00018413906139522874,
      "loss": 1.0275,
      "step": 5877
    },
    {
      "epoch": 0.8263742443413469,
      "grad_norm": 1.5342464447021484,
      "learning_rate": 0.00018426446682336434,
      "loss": 1.1688,
      "step": 5878
    },
    {
      "epoch": 0.8265148319977506,
      "grad_norm": 1.4747552871704102,
      "learning_rate": 0.0001843894168691459,
      "loss": 1.139,
      "step": 5879
    },
    {
      "epoch": 0.8266554196541543,
      "grad_norm": 1.3897693157196045,
      "learning_rate": 0.00018451391085731787,
      "loss": 1.078,
      "step": 5880
    },
    {
      "epoch": 0.8267960073105581,
      "grad_norm": 1.469297170639038,
      "learning_rate": 0.00018463794811508937,
      "loss": 1.1189,
      "step": 5881
    },
    {
      "epoch": 0.8269365949669619,
      "grad_norm": 1.9336961507797241,
      "learning_rate": 0.00018476152797213796,
      "loss": 1.1462,
      "step": 5882
    },
    {
      "epoch": 0.8270771826233657,
      "grad_norm": 1.4648991823196411,
      "learning_rate": 0.00018488464976061257,
      "loss": 1.041,
      "step": 5883
    },
    {
      "epoch": 0.8272177702797694,
      "grad_norm": 1.5434668064117432,
      "learning_rate": 0.0001850073128151382,
      "loss": 1.0703,
      "step": 5884
    },
    {
      "epoch": 0.8273583579361732,
      "grad_norm": 2.090346097946167,
      "learning_rate": 0.00018512951647281872,
      "loss": 0.9833,
      "step": 5885
    },
    {
      "epoch": 0.827498945592577,
      "grad_norm": 1.825767993927002,
      "learning_rate": 0.00018525126007324052,
      "loss": 1.0855,
      "step": 5886
    },
    {
      "epoch": 0.8276395332489808,
      "grad_norm": 1.5066967010498047,
      "learning_rate": 0.00018537254295847644,
      "loss": 1.0051,
      "step": 5887
    },
    {
      "epoch": 0.8277801209053846,
      "grad_norm": 1.7884889841079712,
      "learning_rate": 0.00018549336447308906,
      "loss": 1.0878,
      "step": 5888
    },
    {
      "epoch": 0.8279207085617882,
      "grad_norm": 1.7422735691070557,
      "learning_rate": 0.0001856137239641341,
      "loss": 1.0817,
      "step": 5889
    },
    {
      "epoch": 0.828061296218192,
      "grad_norm": 1.4196370840072632,
      "learning_rate": 0.00018573362078116477,
      "loss": 0.9833,
      "step": 5890
    },
    {
      "epoch": 0.8282018838745958,
      "grad_norm": 1.492509126663208,
      "learning_rate": 0.00018585305427623383,
      "loss": 1.1259,
      "step": 5891
    },
    {
      "epoch": 0.8283424715309996,
      "grad_norm": 1.4626104831695557,
      "learning_rate": 0.00018597202380389837,
      "loss": 1.1801,
      "step": 5892
    },
    {
      "epoch": 0.8284830591874034,
      "grad_norm": 1.3039798736572266,
      "learning_rate": 0.000186090528721223,
      "loss": 1.0526,
      "step": 5893
    },
    {
      "epoch": 0.8286236468438071,
      "grad_norm": 1.6295067071914673,
      "learning_rate": 0.0001862085683877829,
      "loss": 1.0156,
      "step": 5894
    },
    {
      "epoch": 0.8287642345002109,
      "grad_norm": 1.4658045768737793,
      "learning_rate": 0.00018632614216566767,
      "loss": 1.0141,
      "step": 5895
    },
    {
      "epoch": 0.8289048221566147,
      "grad_norm": 1.3608834743499756,
      "learning_rate": 0.00018644324941948458,
      "loss": 1.1922,
      "step": 5896
    },
    {
      "epoch": 0.8290454098130184,
      "grad_norm": 1.6327542066574097,
      "learning_rate": 0.00018655988951636227,
      "loss": 1.1705,
      "step": 5897
    },
    {
      "epoch": 0.8291859974694222,
      "grad_norm": 1.682275414466858,
      "learning_rate": 0.00018667606182595382,
      "loss": 1.0147,
      "step": 5898
    },
    {
      "epoch": 0.8293265851258259,
      "grad_norm": 1.3682678937911987,
      "learning_rate": 0.00018679176572044035,
      "loss": 1.0439,
      "step": 5899
    },
    {
      "epoch": 0.8294671727822297,
      "grad_norm": 1.4350112676620483,
      "learning_rate": 0.00018690700057453442,
      "loss": 1.1576,
      "step": 5900
    },
    {
      "epoch": 0.8296077604386335,
      "grad_norm": 1.8254261016845703,
      "learning_rate": 0.00018702176576548357,
      "loss": 1.1657,
      "step": 5901
    },
    {
      "epoch": 0.8297483480950373,
      "grad_norm": 1.3804636001586914,
      "learning_rate": 0.00018713606067307308,
      "loss": 1.1713,
      "step": 5902
    },
    {
      "epoch": 0.8298889357514411,
      "grad_norm": 1.5870754718780518,
      "learning_rate": 0.00018724988467963013,
      "loss": 1.1485,
      "step": 5903
    },
    {
      "epoch": 0.8300295234078447,
      "grad_norm": 1.4452159404754639,
      "learning_rate": 0.00018736323717002655,
      "loss": 1.2034,
      "step": 5904
    },
    {
      "epoch": 0.8301701110642485,
      "grad_norm": 1.2495286464691162,
      "learning_rate": 0.0001874761175316824,
      "loss": 1.1278,
      "step": 5905
    },
    {
      "epoch": 0.8303106987206523,
      "grad_norm": 1.479017972946167,
      "learning_rate": 0.00018758852515456912,
      "loss": 1.0619,
      "step": 5906
    },
    {
      "epoch": 0.8304512863770561,
      "grad_norm": 1.2515045404434204,
      "learning_rate": 0.00018770045943121308,
      "loss": 1.3192,
      "step": 5907
    },
    {
      "epoch": 0.8305918740334599,
      "grad_norm": 1.4598861932754517,
      "learning_rate": 0.0001878119197566985,
      "loss": 1.0373,
      "step": 5908
    },
    {
      "epoch": 0.8307324616898636,
      "grad_norm": 1.5429744720458984,
      "learning_rate": 0.00018792290552867137,
      "loss": 1.1546,
      "step": 5909
    },
    {
      "epoch": 0.8308730493462674,
      "grad_norm": 1.488864779472351,
      "learning_rate": 0.00018803341614734157,
      "loss": 1.198,
      "step": 5910
    },
    {
      "epoch": 0.8310136370026712,
      "grad_norm": 1.7490814924240112,
      "learning_rate": 0.00018814345101548745,
      "loss": 1.1159,
      "step": 5911
    },
    {
      "epoch": 0.831154224659075,
      "grad_norm": 1.3731272220611572,
      "learning_rate": 0.0001882530095384581,
      "loss": 1.1029,
      "step": 5912
    },
    {
      "epoch": 0.8312948123154787,
      "grad_norm": 1.7752777338027954,
      "learning_rate": 0.00018836209112417689,
      "loss": 1.0501,
      "step": 5913
    },
    {
      "epoch": 0.8314353999718824,
      "grad_norm": 1.815822720527649,
      "learning_rate": 0.0001884706951831447,
      "loss": 1.0411,
      "step": 5914
    },
    {
      "epoch": 0.8315759876282862,
      "grad_norm": 1.8113523721694946,
      "learning_rate": 0.0001885788211284431,
      "loss": 1.0958,
      "step": 5915
    },
    {
      "epoch": 0.83171657528469,
      "grad_norm": 1.3298790454864502,
      "learning_rate": 0.00018868646837573745,
      "loss": 1.2689,
      "step": 5916
    },
    {
      "epoch": 0.8318571629410938,
      "grad_norm": 1.665807843208313,
      "learning_rate": 0.00018879363634328002,
      "loss": 1.1082,
      "step": 5917
    },
    {
      "epoch": 0.8319977505974976,
      "grad_norm": 1.4741203784942627,
      "learning_rate": 0.00018890032445191324,
      "loss": 1.1303,
      "step": 5918
    },
    {
      "epoch": 0.8321383382539013,
      "grad_norm": 1.8953452110290527,
      "learning_rate": 0.00018900653212507295,
      "loss": 1.1444,
      "step": 5919
    },
    {
      "epoch": 0.8322789259103051,
      "grad_norm": 1.4035890102386475,
      "learning_rate": 0.00018911225878879127,
      "loss": 1.1062,
      "step": 5920
    },
    {
      "epoch": 0.8324195135667088,
      "grad_norm": 1.5970633029937744,
      "learning_rate": 0.0001892175038716995,
      "loss": 1.1629,
      "step": 5921
    },
    {
      "epoch": 0.8325601012231126,
      "grad_norm": 1.4602802991867065,
      "learning_rate": 0.00018932226680503202,
      "loss": 1.0447,
      "step": 5922
    },
    {
      "epoch": 0.8327006888795164,
      "grad_norm": 1.3861241340637207,
      "learning_rate": 0.00018942654702262845,
      "loss": 1.11,
      "step": 5923
    },
    {
      "epoch": 0.8328412765359201,
      "grad_norm": 1.307676076889038,
      "learning_rate": 0.00018953034396093732,
      "loss": 1.1178,
      "step": 5924
    },
    {
      "epoch": 0.8329818641923239,
      "grad_norm": 1.42476487159729,
      "learning_rate": 0.00018963365705901877,
      "loss": 1.1591,
      "step": 5925
    },
    {
      "epoch": 0.8331224518487277,
      "grad_norm": 1.5130529403686523,
      "learning_rate": 0.00018973648575854767,
      "loss": 1.0831,
      "step": 5926
    },
    {
      "epoch": 0.8332630395051315,
      "grad_norm": 1.4839224815368652,
      "learning_rate": 0.00018983882950381696,
      "loss": 1.0765,
      "step": 5927
    },
    {
      "epoch": 0.8334036271615353,
      "grad_norm": 1.8331331014633179,
      "learning_rate": 0.00018994068774174017,
      "loss": 1.0919,
      "step": 5928
    },
    {
      "epoch": 0.8335442148179389,
      "grad_norm": 1.7166328430175781,
      "learning_rate": 0.00019004205992185436,
      "loss": 1.0143,
      "step": 5929
    },
    {
      "epoch": 0.8336848024743427,
      "grad_norm": 1.4868155717849731,
      "learning_rate": 0.00019014294549632386,
      "loss": 1.2003,
      "step": 5930
    },
    {
      "epoch": 0.8338253901307465,
      "grad_norm": 1.4680668115615845,
      "learning_rate": 0.00019024334391994243,
      "loss": 0.9848,
      "step": 5931
    },
    {
      "epoch": 0.8339659777871503,
      "grad_norm": 1.7208677530288696,
      "learning_rate": 0.00019034325465013644,
      "loss": 1.1249,
      "step": 5932
    },
    {
      "epoch": 0.8341065654435541,
      "grad_norm": 1.4576504230499268,
      "learning_rate": 0.000190442677146968,
      "loss": 1.2359,
      "step": 5933
    },
    {
      "epoch": 0.8342471530999578,
      "grad_norm": 1.7475160360336304,
      "learning_rate": 0.00019054161087313765,
      "loss": 1.0916,
      "step": 5934
    },
    {
      "epoch": 0.8343877407563616,
      "grad_norm": 1.4630885124206543,
      "learning_rate": 0.00019064005529398728,
      "loss": 1.2059,
      "step": 5935
    },
    {
      "epoch": 0.8345283284127654,
      "grad_norm": 1.412188172340393,
      "learning_rate": 0.00019073800987750344,
      "loss": 1.1068,
      "step": 5936
    },
    {
      "epoch": 0.8346689160691692,
      "grad_norm": 1.4575237035751343,
      "learning_rate": 0.00019083547409431926,
      "loss": 1.1048,
      "step": 5937
    },
    {
      "epoch": 0.8348095037255729,
      "grad_norm": 1.4687055349349976,
      "learning_rate": 0.00019093244741771848,
      "loss": 1.2573,
      "step": 5938
    },
    {
      "epoch": 0.8349500913819766,
      "grad_norm": 1.555798888206482,
      "learning_rate": 0.0001910289293236375,
      "loss": 1.096,
      "step": 5939
    },
    {
      "epoch": 0.8350906790383804,
      "grad_norm": 1.6410865783691406,
      "learning_rate": 0.0001911249192906682,
      "loss": 1.3766,
      "step": 5940
    },
    {
      "epoch": 0.8352312666947842,
      "grad_norm": 1.7532354593276978,
      "learning_rate": 0.00019122041680006145,
      "loss": 0.9955,
      "step": 5941
    },
    {
      "epoch": 0.835371854351188,
      "grad_norm": 1.5114244222640991,
      "learning_rate": 0.0001913154213357292,
      "loss": 1.2178,
      "step": 5942
    },
    {
      "epoch": 0.8355124420075918,
      "grad_norm": 1.2675491571426392,
      "learning_rate": 0.00019140993238424758,
      "loss": 1.2431,
      "step": 5943
    },
    {
      "epoch": 0.8356530296639955,
      "grad_norm": 1.5359234809875488,
      "learning_rate": 0.00019150394943485957,
      "loss": 1.0918,
      "step": 5944
    },
    {
      "epoch": 0.8357936173203993,
      "grad_norm": 1.6032987833023071,
      "learning_rate": 0.00019159747197947783,
      "loss": 1.0715,
      "step": 5945
    },
    {
      "epoch": 0.835934204976803,
      "grad_norm": 1.3906646966934204,
      "learning_rate": 0.00019169049951268762,
      "loss": 1.1211,
      "step": 5946
    },
    {
      "epoch": 0.8360747926332068,
      "grad_norm": 1.6246758699417114,
      "learning_rate": 0.0001917830315317491,
      "loss": 1.0676,
      "step": 5947
    },
    {
      "epoch": 0.8362153802896106,
      "grad_norm": 1.6799509525299072,
      "learning_rate": 0.00019187506753660032,
      "loss": 0.9654,
      "step": 5948
    },
    {
      "epoch": 0.8363559679460143,
      "grad_norm": 1.578428030014038,
      "learning_rate": 0.00019196660702986004,
      "loss": 1.089,
      "step": 5949
    },
    {
      "epoch": 0.8364965556024181,
      "grad_norm": 1.628301739692688,
      "learning_rate": 0.00019205764951683017,
      "loss": 1.126,
      "step": 5950
    },
    {
      "epoch": 0.8366371432588219,
      "grad_norm": 1.625022292137146,
      "learning_rate": 0.0001921481945054985,
      "loss": 1.1229,
      "step": 5951
    },
    {
      "epoch": 0.8367777309152257,
      "grad_norm": 1.4488615989685059,
      "learning_rate": 0.00019223824150654153,
      "loss": 1.1204,
      "step": 5952
    },
    {
      "epoch": 0.8369183185716295,
      "grad_norm": 1.4631729125976562,
      "learning_rate": 0.00019232779003332677,
      "loss": 1.1359,
      "step": 5953
    },
    {
      "epoch": 0.8370589062280331,
      "grad_norm": 1.4217629432678223,
      "learning_rate": 0.00019241683960191593,
      "loss": 1.0841,
      "step": 5954
    },
    {
      "epoch": 0.8371994938844369,
      "grad_norm": 1.4967546463012695,
      "learning_rate": 0.0001925053897310669,
      "loss": 1.0006,
      "step": 5955
    },
    {
      "epoch": 0.8373400815408407,
      "grad_norm": 1.4561681747436523,
      "learning_rate": 0.00019259343994223663,
      "loss": 1.1762,
      "step": 5956
    },
    {
      "epoch": 0.8374806691972445,
      "grad_norm": 1.746760368347168,
      "learning_rate": 0.000192680989759584,
      "loss": 0.97,
      "step": 5957
    },
    {
      "epoch": 0.8376212568536483,
      "grad_norm": 2.1877620220184326,
      "learning_rate": 0.00019276803870997192,
      "loss": 1.0829,
      "step": 5958
    },
    {
      "epoch": 0.837761844510052,
      "grad_norm": 1.5877281427383423,
      "learning_rate": 0.00019285458632296995,
      "loss": 1.0482,
      "step": 5959
    },
    {
      "epoch": 0.8379024321664558,
      "grad_norm": 1.8219314813613892,
      "learning_rate": 0.00019294063213085732,
      "loss": 1.1726,
      "step": 5960
    },
    {
      "epoch": 0.8380430198228596,
      "grad_norm": 1.5504529476165771,
      "learning_rate": 0.00019302617566862486,
      "loss": 1.0722,
      "step": 5961
    },
    {
      "epoch": 0.8381836074792633,
      "grad_norm": 1.4696111679077148,
      "learning_rate": 0.00019311121647397803,
      "loss": 1.0718,
      "step": 5962
    },
    {
      "epoch": 0.8383241951356671,
      "grad_norm": 2.116523265838623,
      "learning_rate": 0.0001931957540873388,
      "loss": 1.2589,
      "step": 5963
    },
    {
      "epoch": 0.8384647827920708,
      "grad_norm": 1.643581509590149,
      "learning_rate": 0.00019327978805184866,
      "loss": 1.0774,
      "step": 5964
    },
    {
      "epoch": 0.8386053704484746,
      "grad_norm": 1.359900712966919,
      "learning_rate": 0.00019336331791337103,
      "loss": 1.2336,
      "step": 5965
    },
    {
      "epoch": 0.8387459581048784,
      "grad_norm": 1.8663924932479858,
      "learning_rate": 0.0001934463432204936,
      "loss": 1.1978,
      "step": 5966
    },
    {
      "epoch": 0.8388865457612822,
      "grad_norm": 1.9023118019104004,
      "learning_rate": 0.00019352886352453039,
      "loss": 1.0289,
      "step": 5967
    },
    {
      "epoch": 0.839027133417686,
      "grad_norm": 1.4507088661193848,
      "learning_rate": 0.00019361087837952513,
      "loss": 1.2631,
      "step": 5968
    },
    {
      "epoch": 0.8391677210740897,
      "grad_norm": 1.7893881797790527,
      "learning_rate": 0.00019369238734225276,
      "loss": 0.9952,
      "step": 5969
    },
    {
      "epoch": 0.8393083087304934,
      "grad_norm": 1.6266541481018066,
      "learning_rate": 0.00019377338997222221,
      "loss": 1.0321,
      "step": 5970
    },
    {
      "epoch": 0.8394488963868972,
      "grad_norm": 1.7825284004211426,
      "learning_rate": 0.00019385388583167875,
      "loss": 1.1235,
      "step": 5971
    },
    {
      "epoch": 0.839589484043301,
      "grad_norm": 1.3919070959091187,
      "learning_rate": 0.0001939338744856063,
      "loss": 1.1165,
      "step": 5972
    },
    {
      "epoch": 0.8397300716997048,
      "grad_norm": 1.5269519090652466,
      "learning_rate": 0.00019401335550172996,
      "loss": 1.1512,
      "step": 5973
    },
    {
      "epoch": 0.8398706593561085,
      "grad_norm": 1.780094861984253,
      "learning_rate": 0.00019409232845051818,
      "loss": 1.2934,
      "step": 5974
    },
    {
      "epoch": 0.8400112470125123,
      "grad_norm": 1.9017629623413086,
      "learning_rate": 0.0001941707929051848,
      "loss": 1.2326,
      "step": 5975
    },
    {
      "epoch": 0.8401518346689161,
      "grad_norm": 1.9500749111175537,
      "learning_rate": 0.00019424874844169217,
      "loss": 1.0793,
      "step": 5976
    },
    {
      "epoch": 0.8402924223253199,
      "grad_norm": 1.6347146034240723,
      "learning_rate": 0.00019432619463875258,
      "loss": 1.0544,
      "step": 5977
    },
    {
      "epoch": 0.8404330099817237,
      "grad_norm": 1.9184274673461914,
      "learning_rate": 0.00019440313107783094,
      "loss": 1.1067,
      "step": 5978
    },
    {
      "epoch": 0.8405735976381273,
      "grad_norm": 1.6473196744918823,
      "learning_rate": 0.00019447955734314712,
      "loss": 1.1278,
      "step": 5979
    },
    {
      "epoch": 0.8407141852945311,
      "grad_norm": 1.5821847915649414,
      "learning_rate": 0.00019455547302167794,
      "loss": 1.0496,
      "step": 5980
    },
    {
      "epoch": 0.8408547729509349,
      "grad_norm": 1.5718687772750854,
      "learning_rate": 0.00019463087770315974,
      "loss": 1.1368,
      "step": 5981
    },
    {
      "epoch": 0.8409953606073387,
      "grad_norm": 1.4894031286239624,
      "learning_rate": 0.00019470577098009017,
      "loss": 1.0662,
      "step": 5982
    },
    {
      "epoch": 0.8411359482637425,
      "grad_norm": 1.4276140928268433,
      "learning_rate": 0.00019478015244773058,
      "loss": 1.2244,
      "step": 5983
    },
    {
      "epoch": 0.8412765359201462,
      "grad_norm": 1.6701074838638306,
      "learning_rate": 0.00019485402170410856,
      "loss": 1.1319,
      "step": 5984
    },
    {
      "epoch": 0.84141712357655,
      "grad_norm": 1.5479501485824585,
      "learning_rate": 0.00019492737835001954,
      "loss": 1.0419,
      "step": 5985
    },
    {
      "epoch": 0.8415577112329538,
      "grad_norm": 1.5314816236495972,
      "learning_rate": 0.00019500022198902904,
      "loss": 1.0545,
      "step": 5986
    },
    {
      "epoch": 0.8416982988893575,
      "grad_norm": 1.6646918058395386,
      "learning_rate": 0.0001950725522274755,
      "loss": 1.1497,
      "step": 5987
    },
    {
      "epoch": 0.8418388865457613,
      "grad_norm": 1.5840744972229004,
      "learning_rate": 0.0001951443686744713,
      "loss": 1.0571,
      "step": 5988
    },
    {
      "epoch": 0.841979474202165,
      "grad_norm": 1.5935938358306885,
      "learning_rate": 0.000195215670941906,
      "loss": 1.1626,
      "step": 5989
    },
    {
      "epoch": 0.8421200618585688,
      "grad_norm": 1.530623197555542,
      "learning_rate": 0.00019528645864444743,
      "loss": 1.2604,
      "step": 5990
    },
    {
      "epoch": 0.8422606495149726,
      "grad_norm": 1.503002405166626,
      "learning_rate": 0.00019535673139954435,
      "loss": 1.1846,
      "step": 5991
    },
    {
      "epoch": 0.8424012371713764,
      "grad_norm": 1.63442063331604,
      "learning_rate": 0.00019542648882742863,
      "loss": 1.0003,
      "step": 5992
    },
    {
      "epoch": 0.8425418248277802,
      "grad_norm": 1.8872716426849365,
      "learning_rate": 0.00019549573055111695,
      "loss": 1.106,
      "step": 5993
    },
    {
      "epoch": 0.8426824124841839,
      "grad_norm": 1.5691394805908203,
      "learning_rate": 0.0001955644561964127,
      "loss": 0.9473,
      "step": 5994
    },
    {
      "epoch": 0.8428230001405876,
      "grad_norm": 1.606658697128296,
      "learning_rate": 0.00019563266539190862,
      "loss": 1.0766,
      "step": 5995
    },
    {
      "epoch": 0.8429635877969914,
      "grad_norm": 1.498555064201355,
      "learning_rate": 0.0001957003577689883,
      "loss": 1.1991,
      "step": 5996
    },
    {
      "epoch": 0.8431041754533952,
      "grad_norm": 1.5831938982009888,
      "learning_rate": 0.00019576753296182835,
      "loss": 1.15,
      "step": 5997
    },
    {
      "epoch": 0.843244763109799,
      "grad_norm": 1.7592809200286865,
      "learning_rate": 0.00019583419060740034,
      "loss": 1.0714,
      "step": 5998
    },
    {
      "epoch": 0.8433853507662027,
      "grad_norm": 2.051140785217285,
      "learning_rate": 0.00019590033034547267,
      "loss": 0.8927,
      "step": 5999
    },
    {
      "epoch": 0.8435259384226065,
      "grad_norm": 1.625166893005371,
      "learning_rate": 0.00019596595181861292,
      "loss": 1.2475,
      "step": 6000
    },
    {
      "epoch": 0.8435259384226065,
      "eval_loss": 1.1662399768829346,
      "eval_runtime": 771.5332,
      "eval_samples_per_second": 16.391,
      "eval_steps_per_second": 8.195,
      "step": 6000
    },
    {
      "epoch": 0.8436665260790103,
      "grad_norm": 1.4842838048934937,
      "learning_rate": 0.0001960310546721891,
      "loss": 1.257,
      "step": 6001
    },
    {
      "epoch": 0.8438071137354141,
      "grad_norm": 1.6678732633590698,
      "learning_rate": 0.00019609563855437208,
      "loss": 1.2974,
      "step": 6002
    },
    {
      "epoch": 0.8439477013918179,
      "grad_norm": 1.6141902208328247,
      "learning_rate": 0.0001961597031161376,
      "loss": 1.2132,
      "step": 6003
    },
    {
      "epoch": 0.8440882890482215,
      "grad_norm": 1.4225987195968628,
      "learning_rate": 0.00019622324801126756,
      "loss": 1.1282,
      "step": 6004
    },
    {
      "epoch": 0.8442288767046253,
      "grad_norm": 1.6598849296569824,
      "learning_rate": 0.00019628627289635243,
      "loss": 1.1275,
      "step": 6005
    },
    {
      "epoch": 0.8443694643610291,
      "grad_norm": 1.4414657354354858,
      "learning_rate": 0.00019634877743079286,
      "loss": 1.2339,
      "step": 6006
    },
    {
      "epoch": 0.8445100520174329,
      "grad_norm": 1.4277397394180298,
      "learning_rate": 0.00019641076127680163,
      "loss": 1.018,
      "step": 6007
    },
    {
      "epoch": 0.8446506396738367,
      "grad_norm": 1.5345675945281982,
      "learning_rate": 0.00019647222409940545,
      "loss": 1.1138,
      "step": 6008
    },
    {
      "epoch": 0.8447912273302404,
      "grad_norm": 1.487858533859253,
      "learning_rate": 0.00019653316556644663,
      "loss": 1.099,
      "step": 6009
    },
    {
      "epoch": 0.8449318149866442,
      "grad_norm": 2.1305909156799316,
      "learning_rate": 0.00019659358534858504,
      "loss": 1.1004,
      "step": 6010
    },
    {
      "epoch": 0.845072402643048,
      "grad_norm": 1.5892137289047241,
      "learning_rate": 0.00019665348311929994,
      "loss": 1.0144,
      "step": 6011
    },
    {
      "epoch": 0.8452129902994517,
      "grad_norm": 1.9201478958129883,
      "learning_rate": 0.00019671285855489158,
      "loss": 1.2703,
      "step": 6012
    },
    {
      "epoch": 0.8453535779558555,
      "grad_norm": 1.4777084589004517,
      "learning_rate": 0.00019677171133448287,
      "loss": 1.1731,
      "step": 6013
    },
    {
      "epoch": 0.8454941656122592,
      "grad_norm": 1.856252908706665,
      "learning_rate": 0.0001968300411400215,
      "loss": 1.0934,
      "step": 6014
    },
    {
      "epoch": 0.845634753268663,
      "grad_norm": 1.4859765768051147,
      "learning_rate": 0.00019688784765628132,
      "loss": 1.1316,
      "step": 6015
    },
    {
      "epoch": 0.8457753409250668,
      "grad_norm": 1.5976370573043823,
      "learning_rate": 0.0001969451305708641,
      "loss": 1.1116,
      "step": 6016
    },
    {
      "epoch": 0.8459159285814706,
      "grad_norm": 1.9214733839035034,
      "learning_rate": 0.00019700188957420128,
      "loss": 1.1111,
      "step": 6017
    },
    {
      "epoch": 0.8460565162378744,
      "grad_norm": 1.5786629915237427,
      "learning_rate": 0.00019705812435955565,
      "loss": 1.0094,
      "step": 6018
    },
    {
      "epoch": 0.846197103894278,
      "grad_norm": 1.405492901802063,
      "learning_rate": 0.00019711383462302302,
      "loss": 1.2884,
      "step": 6019
    },
    {
      "epoch": 0.8463376915506818,
      "grad_norm": 1.6673741340637207,
      "learning_rate": 0.00019716902006353368,
      "loss": 1.1475,
      "step": 6020
    },
    {
      "epoch": 0.8464782792070856,
      "grad_norm": 1.6119686365127563,
      "learning_rate": 0.0001972236803828543,
      "loss": 1.1302,
      "step": 6021
    },
    {
      "epoch": 0.8466188668634894,
      "grad_norm": 1.6133556365966797,
      "learning_rate": 0.00019727781528558938,
      "loss": 1.0906,
      "step": 6022
    },
    {
      "epoch": 0.8467594545198932,
      "grad_norm": 1.5618221759796143,
      "learning_rate": 0.0001973314244791829,
      "loss": 1.1336,
      "step": 6023
    },
    {
      "epoch": 0.8469000421762969,
      "grad_norm": 1.5144115686416626,
      "learning_rate": 0.0001973845076739198,
      "loss": 1.3238,
      "step": 6024
    },
    {
      "epoch": 0.8470406298327007,
      "grad_norm": 1.664770483970642,
      "learning_rate": 0.00019743706458292772,
      "loss": 1.2125,
      "step": 6025
    },
    {
      "epoch": 0.8471812174891045,
      "grad_norm": 1.7499644756317139,
      "learning_rate": 0.00019748909492217832,
      "loss": 1.0376,
      "step": 6026
    },
    {
      "epoch": 0.8473218051455083,
      "grad_norm": 1.5143380165100098,
      "learning_rate": 0.00019754059841048922,
      "loss": 0.9377,
      "step": 6027
    },
    {
      "epoch": 0.847462392801912,
      "grad_norm": 1.4634941816329956,
      "learning_rate": 0.0001975915747695249,
      "loss": 1.1752,
      "step": 6028
    },
    {
      "epoch": 0.8476029804583157,
      "grad_norm": 1.4417890310287476,
      "learning_rate": 0.00019764202372379877,
      "loss": 1.1128,
      "step": 6029
    },
    {
      "epoch": 0.8477435681147195,
      "grad_norm": 1.5765267610549927,
      "learning_rate": 0.00019769194500067442,
      "loss": 1.1419,
      "step": 6030
    },
    {
      "epoch": 0.8478841557711233,
      "grad_norm": 1.7081711292266846,
      "learning_rate": 0.00019774133833036712,
      "loss": 0.9049,
      "step": 6031
    },
    {
      "epoch": 0.8480247434275271,
      "grad_norm": 2.105071783065796,
      "learning_rate": 0.00019779020344594522,
      "loss": 1.0262,
      "step": 6032
    },
    {
      "epoch": 0.8481653310839309,
      "grad_norm": 1.5579105615615845,
      "learning_rate": 0.0001978385400833316,
      "loss": 1.1631,
      "step": 6033
    },
    {
      "epoch": 0.8483059187403346,
      "grad_norm": 1.4506206512451172,
      "learning_rate": 0.00019788634798130537,
      "loss": 1.0906,
      "step": 6034
    },
    {
      "epoch": 0.8484465063967384,
      "grad_norm": 1.814211130142212,
      "learning_rate": 0.00019793362688150283,
      "loss": 1.1152,
      "step": 6035
    },
    {
      "epoch": 0.8485870940531421,
      "grad_norm": 1.6775009632110596,
      "learning_rate": 0.00019798037652841915,
      "loss": 1.0888,
      "step": 6036
    },
    {
      "epoch": 0.8487276817095459,
      "grad_norm": 1.5924047231674194,
      "learning_rate": 0.00019802659666940967,
      "loss": 1.1347,
      "step": 6037
    },
    {
      "epoch": 0.8488682693659497,
      "grad_norm": 1.4933830499649048,
      "learning_rate": 0.00019807228705469147,
      "loss": 1.4288,
      "step": 6038
    },
    {
      "epoch": 0.8490088570223534,
      "grad_norm": 1.468808650970459,
      "learning_rate": 0.00019811744743734422,
      "loss": 1.3121,
      "step": 6039
    },
    {
      "epoch": 0.8491494446787572,
      "grad_norm": 1.7285569906234741,
      "learning_rate": 0.00019816207757331213,
      "loss": 1.2607,
      "step": 6040
    },
    {
      "epoch": 0.849290032335161,
      "grad_norm": 1.5500061511993408,
      "learning_rate": 0.00019820617722140482,
      "loss": 1.29,
      "step": 6041
    },
    {
      "epoch": 0.8494306199915648,
      "grad_norm": 1.5596911907196045,
      "learning_rate": 0.00019824974614329882,
      "loss": 1.2475,
      "step": 6042
    },
    {
      "epoch": 0.8495712076479686,
      "grad_norm": 1.52439284324646,
      "learning_rate": 0.00019829278410353872,
      "loss": 1.0256,
      "step": 6043
    },
    {
      "epoch": 0.8497117953043722,
      "grad_norm": 1.3950785398483276,
      "learning_rate": 0.0001983352908695387,
      "loss": 1.0468,
      "step": 6044
    },
    {
      "epoch": 0.849852382960776,
      "grad_norm": 1.4521901607513428,
      "learning_rate": 0.00019837726621158345,
      "loss": 1.1392,
      "step": 6045
    },
    {
      "epoch": 0.8499929706171798,
      "grad_norm": 1.8365533351898193,
      "learning_rate": 0.00019841870990282975,
      "loss": 1.1933,
      "step": 6046
    },
    {
      "epoch": 0.8501335582735836,
      "grad_norm": 1.5154433250427246,
      "learning_rate": 0.0001984596217193074,
      "loss": 1.18,
      "step": 6047
    },
    {
      "epoch": 0.8502741459299874,
      "grad_norm": 2.003979444503784,
      "learning_rate": 0.00019850000143992056,
      "loss": 1.2098,
      "step": 6048
    },
    {
      "epoch": 0.8504147335863911,
      "grad_norm": 1.3231438398361206,
      "learning_rate": 0.00019853984884644907,
      "loss": 1.1443,
      "step": 6049
    },
    {
      "epoch": 0.8505553212427949,
      "grad_norm": 1.6560726165771484,
      "learning_rate": 0.00019857916372354942,
      "loss": 1.2798,
      "step": 6050
    },
    {
      "epoch": 0.8506959088991987,
      "grad_norm": 1.386256217956543,
      "learning_rate": 0.00019861794585875593,
      "loss": 1.2171,
      "step": 6051
    },
    {
      "epoch": 0.8508364965556025,
      "grad_norm": 1.5850603580474854,
      "learning_rate": 0.0001986561950424821,
      "loss": 1.0832,
      "step": 6052
    },
    {
      "epoch": 0.8509770842120062,
      "grad_norm": 1.8223907947540283,
      "learning_rate": 0.00019869391106802154,
      "loss": 1.1332,
      "step": 6053
    },
    {
      "epoch": 0.8511176718684099,
      "grad_norm": 1.850203275680542,
      "learning_rate": 0.0001987310937315491,
      "loss": 1.0981,
      "step": 6054
    },
    {
      "epoch": 0.8512582595248137,
      "grad_norm": 1.5282913446426392,
      "learning_rate": 0.00019876774283212215,
      "loss": 1.2067,
      "step": 6055
    },
    {
      "epoch": 0.8513988471812175,
      "grad_norm": 1.5437836647033691,
      "learning_rate": 0.00019880385817168145,
      "loss": 1.2575,
      "step": 6056
    },
    {
      "epoch": 0.8515394348376213,
      "grad_norm": 1.6665188074111938,
      "learning_rate": 0.0001988394395550524,
      "loss": 1.0578,
      "step": 6057
    },
    {
      "epoch": 0.8516800224940251,
      "grad_norm": 1.5002418756484985,
      "learning_rate": 0.00019887448678994587,
      "loss": 1.0129,
      "step": 6058
    },
    {
      "epoch": 0.8518206101504288,
      "grad_norm": 1.425171136856079,
      "learning_rate": 0.00019890899968695952,
      "loss": 0.9764,
      "step": 6059
    },
    {
      "epoch": 0.8519611978068325,
      "grad_norm": 1.493937373161316,
      "learning_rate": 0.00019894297805957858,
      "loss": 1.2348,
      "step": 6060
    },
    {
      "epoch": 0.8521017854632363,
      "grad_norm": 1.3702573776245117,
      "learning_rate": 0.000198976421724177,
      "loss": 1.065,
      "step": 6061
    },
    {
      "epoch": 0.8522423731196401,
      "grad_norm": 1.671837329864502,
      "learning_rate": 0.00019900933050001846,
      "loss": 1.0886,
      "step": 6062
    },
    {
      "epoch": 0.8523829607760439,
      "grad_norm": 1.6466151475906372,
      "learning_rate": 0.00019904170420925716,
      "loss": 1.1992,
      "step": 6063
    },
    {
      "epoch": 0.8525235484324476,
      "grad_norm": 1.5970338582992554,
      "learning_rate": 0.00019907354267693897,
      "loss": 1.2508,
      "step": 6064
    },
    {
      "epoch": 0.8526641360888514,
      "grad_norm": 1.5661513805389404,
      "learning_rate": 0.00019910484573100242,
      "loss": 1.0371,
      "step": 6065
    },
    {
      "epoch": 0.8528047237452552,
      "grad_norm": 1.5002433061599731,
      "learning_rate": 0.0001991356132022793,
      "loss": 1.1311,
      "step": 6066
    },
    {
      "epoch": 0.852945311401659,
      "grad_norm": 1.5863333940505981,
      "learning_rate": 0.000199165844924496,
      "loss": 1.1388,
      "step": 6067
    },
    {
      "epoch": 0.8530858990580626,
      "grad_norm": 1.8095427751541138,
      "learning_rate": 0.00019919554073427408,
      "loss": 1.2299,
      "step": 6068
    },
    {
      "epoch": 0.8532264867144664,
      "grad_norm": 1.475136160850525,
      "learning_rate": 0.0001992247004711314,
      "loss": 1.0152,
      "step": 6069
    },
    {
      "epoch": 0.8533670743708702,
      "grad_norm": 1.5142878293991089,
      "learning_rate": 0.00019925332397748277,
      "loss": 1.0129,
      "step": 6070
    },
    {
      "epoch": 0.853507662027274,
      "grad_norm": 1.3030288219451904,
      "learning_rate": 0.00019928141109864088,
      "loss": 1.2281,
      "step": 6071
    },
    {
      "epoch": 0.8536482496836778,
      "grad_norm": 1.9224004745483398,
      "learning_rate": 0.00019930896168281727,
      "loss": 1.2788,
      "step": 6072
    },
    {
      "epoch": 0.8537888373400815,
      "grad_norm": 1.5901966094970703,
      "learning_rate": 0.00019933597558112294,
      "loss": 1.1711,
      "step": 6073
    },
    {
      "epoch": 0.8539294249964853,
      "grad_norm": 1.7138621807098389,
      "learning_rate": 0.00019936245264756924,
      "loss": 0.997,
      "step": 6074
    },
    {
      "epoch": 0.8540700126528891,
      "grad_norm": 1.5857902765274048,
      "learning_rate": 0.00019938839273906877,
      "loss": 1.0957,
      "step": 6075
    },
    {
      "epoch": 0.8542106003092929,
      "grad_norm": 1.6597809791564941,
      "learning_rate": 0.00019941379571543596,
      "loss": 1.1066,
      "step": 6076
    },
    {
      "epoch": 0.8543511879656966,
      "grad_norm": 1.6365077495574951,
      "learning_rate": 0.00019943866143938793,
      "loss": 1.2367,
      "step": 6077
    },
    {
      "epoch": 0.8544917756221003,
      "grad_norm": 1.5892115831375122,
      "learning_rate": 0.0001994629897765453,
      "loss": 1.2084,
      "step": 6078
    },
    {
      "epoch": 0.8546323632785041,
      "grad_norm": 1.6594325304031372,
      "learning_rate": 0.00019948678059543271,
      "loss": 1.0576,
      "step": 6079
    },
    {
      "epoch": 0.8547729509349079,
      "grad_norm": 1.4591797590255737,
      "learning_rate": 0.00019951003376747974,
      "loss": 1.1482,
      "step": 6080
    },
    {
      "epoch": 0.8549135385913117,
      "grad_norm": 1.597563624382019,
      "learning_rate": 0.00019953274916702154,
      "loss": 0.9287,
      "step": 6081
    },
    {
      "epoch": 0.8550541262477155,
      "grad_norm": 1.4245452880859375,
      "learning_rate": 0.0001995549266712994,
      "loss": 1.0531,
      "step": 6082
    },
    {
      "epoch": 0.8551947139041192,
      "grad_norm": 1.6714739799499512,
      "learning_rate": 0.00019957656616046164,
      "loss": 1.1701,
      "step": 6083
    },
    {
      "epoch": 0.855335301560523,
      "grad_norm": 1.4690399169921875,
      "learning_rate": 0.00019959766751756402,
      "loss": 0.991,
      "step": 6084
    },
    {
      "epoch": 0.8554758892169267,
      "grad_norm": 1.9270836114883423,
      "learning_rate": 0.00019961823062857048,
      "loss": 1.3126,
      "step": 6085
    },
    {
      "epoch": 0.8556164768733305,
      "grad_norm": 1.595717191696167,
      "learning_rate": 0.0001996382553823538,
      "loss": 1.0591,
      "step": 6086
    },
    {
      "epoch": 0.8557570645297343,
      "grad_norm": 1.4572665691375732,
      "learning_rate": 0.00019965774167069613,
      "loss": 1.1713,
      "step": 6087
    },
    {
      "epoch": 0.855897652186138,
      "grad_norm": 1.725117564201355,
      "learning_rate": 0.00019967668938828962,
      "loss": 1.2278,
      "step": 6088
    },
    {
      "epoch": 0.8560382398425418,
      "grad_norm": 1.3636226654052734,
      "learning_rate": 0.0001996950984327369,
      "loss": 1.044,
      "step": 6089
    },
    {
      "epoch": 0.8561788274989456,
      "grad_norm": 1.5100599527359009,
      "learning_rate": 0.00019971296870455177,
      "loss": 1.2371,
      "step": 6090
    },
    {
      "epoch": 0.8563194151553494,
      "grad_norm": 1.54267418384552,
      "learning_rate": 0.0001997303001071596,
      "loss": 0.8834,
      "step": 6091
    },
    {
      "epoch": 0.8564600028117532,
      "grad_norm": 1.5044151544570923,
      "learning_rate": 0.00019974709254689797,
      "loss": 1.1336,
      "step": 6092
    },
    {
      "epoch": 0.8566005904681568,
      "grad_norm": 1.790626049041748,
      "learning_rate": 0.00019976334593301714,
      "loss": 1.164,
      "step": 6093
    },
    {
      "epoch": 0.8567411781245606,
      "grad_norm": 1.888482928276062,
      "learning_rate": 0.00019977906017768052,
      "loss": 1.0559,
      "step": 6094
    },
    {
      "epoch": 0.8568817657809644,
      "grad_norm": 1.4013934135437012,
      "learning_rate": 0.00019979423519596504,
      "loss": 1.1993,
      "step": 6095
    },
    {
      "epoch": 0.8570223534373682,
      "grad_norm": 1.5337351560592651,
      "learning_rate": 0.00019980887090586188,
      "loss": 1.1365,
      "step": 6096
    },
    {
      "epoch": 0.857162941093772,
      "grad_norm": 1.791982889175415,
      "learning_rate": 0.00019982296722827666,
      "loss": 1.0008,
      "step": 6097
    },
    {
      "epoch": 0.8573035287501757,
      "grad_norm": 1.5414128303527832,
      "learning_rate": 0.00019983652408703002,
      "loss": 1.2672,
      "step": 6098
    },
    {
      "epoch": 0.8574441164065795,
      "grad_norm": 1.4475752115249634,
      "learning_rate": 0.00019984954140885787,
      "loss": 1.162,
      "step": 6099
    },
    {
      "epoch": 0.8575847040629833,
      "grad_norm": 1.5225797891616821,
      "learning_rate": 0.00019986201912341195,
      "loss": 1.094,
      "step": 6100
    },
    {
      "epoch": 0.857725291719387,
      "grad_norm": 1.4602535963058472,
      "learning_rate": 0.0001998739571632602,
      "loss": 1.2062,
      "step": 6101
    },
    {
      "epoch": 0.8578658793757908,
      "grad_norm": 1.4407458305358887,
      "learning_rate": 0.000199885355463887,
      "loss": 1.1281,
      "step": 6102
    },
    {
      "epoch": 0.8580064670321945,
      "grad_norm": 1.4825701713562012,
      "learning_rate": 0.0001998962139636936,
      "loss": 1.052,
      "step": 6103
    },
    {
      "epoch": 0.8581470546885983,
      "grad_norm": 1.655100703239441,
      "learning_rate": 0.00019990653260399844,
      "loss": 1.1039,
      "step": 6104
    },
    {
      "epoch": 0.8582876423450021,
      "grad_norm": 1.3157027959823608,
      "learning_rate": 0.00019991631132903746,
      "loss": 1.0967,
      "step": 6105
    },
    {
      "epoch": 0.8584282300014059,
      "grad_norm": 1.5023869276046753,
      "learning_rate": 0.00019992555008596454,
      "loss": 0.9876,
      "step": 6106
    },
    {
      "epoch": 0.8585688176578097,
      "grad_norm": 1.6842833757400513,
      "learning_rate": 0.00019993424882485146,
      "loss": 1.2737,
      "step": 6107
    },
    {
      "epoch": 0.8587094053142134,
      "grad_norm": 1.5314263105392456,
      "learning_rate": 0.0001999424074986885,
      "loss": 1.165,
      "step": 6108
    },
    {
      "epoch": 0.8588499929706171,
      "grad_norm": 1.4606941938400269,
      "learning_rate": 0.0001999500260633845,
      "loss": 1.1645,
      "step": 6109
    },
    {
      "epoch": 0.8589905806270209,
      "grad_norm": 2.2398877143859863,
      "learning_rate": 0.00019995710447776724,
      "loss": 0.9753,
      "step": 6110
    },
    {
      "epoch": 0.8591311682834247,
      "grad_norm": 1.6762396097183228,
      "learning_rate": 0.00019996364270358346,
      "loss": 1.2541,
      "step": 6111
    },
    {
      "epoch": 0.8592717559398285,
      "grad_norm": 1.6597589254379272,
      "learning_rate": 0.00019996964070549927,
      "loss": 1.1324,
      "step": 6112
    },
    {
      "epoch": 0.8594123435962322,
      "grad_norm": 1.492126703262329,
      "learning_rate": 0.00019997509845110027,
      "loss": 1.2672,
      "step": 6113
    },
    {
      "epoch": 0.859552931252636,
      "grad_norm": 1.693613886833191,
      "learning_rate": 0.00019998001591089168,
      "loss": 1.1572,
      "step": 6114
    },
    {
      "epoch": 0.8596935189090398,
      "grad_norm": 1.6239246129989624,
      "learning_rate": 0.00019998439305829856,
      "loss": 1.107,
      "step": 6115
    },
    {
      "epoch": 0.8598341065654436,
      "grad_norm": 1.3944828510284424,
      "learning_rate": 0.00019998822986966585,
      "loss": 1.1223,
      "step": 6116
    },
    {
      "epoch": 0.8599746942218474,
      "grad_norm": 1.3561604022979736,
      "learning_rate": 0.0001999915263242587,
      "loss": 1.0306,
      "step": 6117
    },
    {
      "epoch": 0.860115281878251,
      "grad_norm": 1.5423094034194946,
      "learning_rate": 0.00019999428240426238,
      "loss": 1.032,
      "step": 6118
    },
    {
      "epoch": 0.8602558695346548,
      "grad_norm": 1.4678648710250854,
      "learning_rate": 0.00019999649809478252,
      "loss": 1.0563,
      "step": 6119
    },
    {
      "epoch": 0.8603964571910586,
      "grad_norm": 1.5015610456466675,
      "learning_rate": 0.00019999817338384497,
      "loss": 1.079,
      "step": 6120
    },
    {
      "epoch": 0.8605370448474624,
      "grad_norm": 1.4384807348251343,
      "learning_rate": 0.0001999993082623962,
      "loss": 1.0564,
      "step": 6121
    },
    {
      "epoch": 0.8606776325038662,
      "grad_norm": 1.2739685773849487,
      "learning_rate": 0.0001999999027243031,
      "loss": 1.2697,
      "step": 6122
    },
    {
      "epoch": 0.8608182201602699,
      "grad_norm": 1.4408586025238037,
      "learning_rate": 0.00019999995676635304,
      "loss": 1.1017,
      "step": 6123
    },
    {
      "epoch": 0.8609588078166737,
      "grad_norm": 1.4765459299087524,
      "learning_rate": 0.000199999470388254,
      "loss": 1.1303,
      "step": 6124
    },
    {
      "epoch": 0.8610993954730775,
      "grad_norm": 1.5434603691101074,
      "learning_rate": 0.00019999844359263445,
      "loss": 1.2655,
      "step": 6125
    },
    {
      "epoch": 0.8612399831294812,
      "grad_norm": 1.4110441207885742,
      "learning_rate": 0.00019999687638504337,
      "loss": 1.0602,
      "step": 6126
    },
    {
      "epoch": 0.861380570785885,
      "grad_norm": 1.7094694375991821,
      "learning_rate": 0.00019999476877395034,
      "loss": 1.056,
      "step": 6127
    },
    {
      "epoch": 0.8615211584422887,
      "grad_norm": 1.4843864440917969,
      "learning_rate": 0.00019999212077074528,
      "loss": 1.1994,
      "step": 6128
    },
    {
      "epoch": 0.8616617460986925,
      "grad_norm": 1.5939384698867798,
      "learning_rate": 0.0001999889323897385,
      "loss": 1.1358,
      "step": 6129
    },
    {
      "epoch": 0.8618023337550963,
      "grad_norm": 1.7701234817504883,
      "learning_rate": 0.00019998520364816074,
      "loss": 1.0937,
      "step": 6130
    },
    {
      "epoch": 0.8619429214115001,
      "grad_norm": 1.7024245262145996,
      "learning_rate": 0.0001999809345661628,
      "loss": 1.1553,
      "step": 6131
    },
    {
      "epoch": 0.8620835090679039,
      "grad_norm": 1.4059864282608032,
      "learning_rate": 0.00019997612516681578,
      "loss": 1.3175,
      "step": 6132
    },
    {
      "epoch": 0.8622240967243076,
      "grad_norm": 1.701541543006897,
      "learning_rate": 0.00019997077547611057,
      "loss": 1.0466,
      "step": 6133
    },
    {
      "epoch": 0.8623646843807113,
      "grad_norm": 1.5355850458145142,
      "learning_rate": 0.00019996488552295798,
      "loss": 1.282,
      "step": 6134
    },
    {
      "epoch": 0.8625052720371151,
      "grad_norm": 1.768146276473999,
      "learning_rate": 0.00019995845533918853,
      "loss": 1.2927,
      "step": 6135
    },
    {
      "epoch": 0.8626458596935189,
      "grad_norm": 1.694832444190979,
      "learning_rate": 0.00019995148495955228,
      "loss": 1.1202,
      "step": 6136
    },
    {
      "epoch": 0.8627864473499227,
      "grad_norm": 1.2775850296020508,
      "learning_rate": 0.00019994397442171856,
      "loss": 1.3053,
      "step": 6137
    },
    {
      "epoch": 0.8629270350063264,
      "grad_norm": 1.7648521661758423,
      "learning_rate": 0.0001999359237662758,
      "loss": 1.2739,
      "step": 6138
    },
    {
      "epoch": 0.8630676226627302,
      "grad_norm": 1.7829097509384155,
      "learning_rate": 0.0001999273330367315,
      "loss": 1.2235,
      "step": 6139
    },
    {
      "epoch": 0.863208210319134,
      "grad_norm": 1.4383522272109985,
      "learning_rate": 0.0001999182022795116,
      "loss": 1.0993,
      "step": 6140
    },
    {
      "epoch": 0.8633487979755378,
      "grad_norm": 1.6176992654800415,
      "learning_rate": 0.0001999085315439606,
      "loss": 1.0698,
      "step": 6141
    },
    {
      "epoch": 0.8634893856319416,
      "grad_norm": 1.6800168752670288,
      "learning_rate": 0.00019989832088234115,
      "loss": 1.0964,
      "step": 6142
    },
    {
      "epoch": 0.8636299732883452,
      "grad_norm": 1.5263217687606812,
      "learning_rate": 0.00019988757034983373,
      "loss": 1.1528,
      "step": 6143
    },
    {
      "epoch": 0.863770560944749,
      "grad_norm": 1.653136134147644,
      "learning_rate": 0.00019987628000453647,
      "loss": 1.061,
      "step": 6144
    },
    {
      "epoch": 0.8639111486011528,
      "grad_norm": 1.7288063764572144,
      "learning_rate": 0.0001998644499074646,
      "loss": 1.0087,
      "step": 6145
    },
    {
      "epoch": 0.8640517362575566,
      "grad_norm": 1.634347915649414,
      "learning_rate": 0.00019985208012255042,
      "loss": 1.1191,
      "step": 6146
    },
    {
      "epoch": 0.8641923239139604,
      "grad_norm": 1.490736484527588,
      "learning_rate": 0.00019983917071664275,
      "loss": 1.2242,
      "step": 6147
    },
    {
      "epoch": 0.8643329115703641,
      "grad_norm": 1.6289029121398926,
      "learning_rate": 0.0001998257217595067,
      "loss": 1.2388,
      "step": 6148
    },
    {
      "epoch": 0.8644734992267679,
      "grad_norm": 1.7319447994232178,
      "learning_rate": 0.00019981173332382315,
      "loss": 1.0814,
      "step": 6149
    },
    {
      "epoch": 0.8646140868831717,
      "grad_norm": 1.7341259717941284,
      "learning_rate": 0.00019979720548518842,
      "loss": 1.0684,
      "step": 6150
    },
    {
      "epoch": 0.8647546745395754,
      "grad_norm": 1.5209671258926392,
      "learning_rate": 0.00019978213832211398,
      "loss": 1.1127,
      "step": 6151
    },
    {
      "epoch": 0.8648952621959792,
      "grad_norm": 1.5926756858825684,
      "learning_rate": 0.00019976653191602578,
      "loss": 1.2912,
      "step": 6152
    },
    {
      "epoch": 0.8650358498523829,
      "grad_norm": 1.4908852577209473,
      "learning_rate": 0.00019975038635126405,
      "loss": 1.0749,
      "step": 6153
    },
    {
      "epoch": 0.8651764375087867,
      "grad_norm": 1.704007625579834,
      "learning_rate": 0.00019973370171508275,
      "loss": 1.1793,
      "step": 6154
    },
    {
      "epoch": 0.8653170251651905,
      "grad_norm": 1.6004278659820557,
      "learning_rate": 0.00019971647809764905,
      "loss": 1.0859,
      "step": 6155
    },
    {
      "epoch": 0.8654576128215943,
      "grad_norm": 1.6006426811218262,
      "learning_rate": 0.00019969871559204276,
      "loss": 1.1265,
      "step": 6156
    },
    {
      "epoch": 0.8655982004779981,
      "grad_norm": 1.5208874940872192,
      "learning_rate": 0.00019968041429425622,
      "loss": 1.0418,
      "step": 6157
    },
    {
      "epoch": 0.8657387881344017,
      "grad_norm": 1.5680872201919556,
      "learning_rate": 0.0001996615743031934,
      "loss": 1.089,
      "step": 6158
    },
    {
      "epoch": 0.8658793757908055,
      "grad_norm": 1.6558834314346313,
      "learning_rate": 0.00019964219572066934,
      "loss": 1.2096,
      "step": 6159
    },
    {
      "epoch": 0.8660199634472093,
      "grad_norm": 1.5796432495117188,
      "learning_rate": 0.00019962227865140988,
      "loss": 1.1234,
      "step": 6160
    },
    {
      "epoch": 0.8661605511036131,
      "grad_norm": 1.5294408798217773,
      "learning_rate": 0.00019960182320305093,
      "loss": 1.1575,
      "step": 6161
    },
    {
      "epoch": 0.8663011387600169,
      "grad_norm": 1.5493537187576294,
      "learning_rate": 0.00019958082948613794,
      "loss": 0.976,
      "step": 6162
    },
    {
      "epoch": 0.8664417264164206,
      "grad_norm": 1.5243096351623535,
      "learning_rate": 0.00019955929761412522,
      "loss": 1.1565,
      "step": 6163
    },
    {
      "epoch": 0.8665823140728244,
      "grad_norm": 1.3936110734939575,
      "learning_rate": 0.00019953722770337534,
      "loss": 1.1434,
      "step": 6164
    },
    {
      "epoch": 0.8667229017292282,
      "grad_norm": 1.9467121362686157,
      "learning_rate": 0.00019951461987315864,
      "loss": 1.1091,
      "step": 6165
    },
    {
      "epoch": 0.866863489385632,
      "grad_norm": 1.4705027341842651,
      "learning_rate": 0.00019949147424565248,
      "loss": 1.2285,
      "step": 6166
    },
    {
      "epoch": 0.8670040770420357,
      "grad_norm": 1.3761117458343506,
      "learning_rate": 0.00019946779094594046,
      "loss": 1.1029,
      "step": 6167
    },
    {
      "epoch": 0.8671446646984394,
      "grad_norm": 1.8962184190750122,
      "learning_rate": 0.00019944357010201203,
      "loss": 1.1313,
      "step": 6168
    },
    {
      "epoch": 0.8672852523548432,
      "grad_norm": 1.8026325702667236,
      "learning_rate": 0.00019941881184476154,
      "loss": 1.216,
      "step": 6169
    },
    {
      "epoch": 0.867425840011247,
      "grad_norm": 1.4864004850387573,
      "learning_rate": 0.00019939351630798768,
      "loss": 1.1557,
      "step": 6170
    },
    {
      "epoch": 0.8675664276676508,
      "grad_norm": 1.52595853805542,
      "learning_rate": 0.00019936768362839265,
      "loss": 1.0061,
      "step": 6171
    },
    {
      "epoch": 0.8677070153240546,
      "grad_norm": 1.7342872619628906,
      "learning_rate": 0.00019934131394558154,
      "loss": 1.1207,
      "step": 6172
    },
    {
      "epoch": 0.8678476029804583,
      "grad_norm": 1.5026956796646118,
      "learning_rate": 0.00019931440740206146,
      "loss": 1.3073,
      "step": 6173
    },
    {
      "epoch": 0.8679881906368621,
      "grad_norm": 1.7497482299804688,
      "learning_rate": 0.00019928696414324093,
      "loss": 1.2266,
      "step": 6174
    },
    {
      "epoch": 0.8681287782932658,
      "grad_norm": 1.3876404762268066,
      "learning_rate": 0.00019925898431742884,
      "loss": 1.0226,
      "step": 6175
    },
    {
      "epoch": 0.8682693659496696,
      "grad_norm": 1.687936782836914,
      "learning_rate": 0.0001992304680758339,
      "loss": 1.2169,
      "step": 6176
    },
    {
      "epoch": 0.8684099536060734,
      "grad_norm": 1.5885920524597168,
      "learning_rate": 0.0001992014155725637,
      "loss": 1.0014,
      "step": 6177
    },
    {
      "epoch": 0.8685505412624771,
      "grad_norm": 1.4930663108825684,
      "learning_rate": 0.00019917182696462386,
      "loss": 1.1776,
      "step": 6178
    },
    {
      "epoch": 0.8686911289188809,
      "grad_norm": 1.3335028886795044,
      "learning_rate": 0.00019914170241191727,
      "loss": 1.2458,
      "step": 6179
    },
    {
      "epoch": 0.8688317165752847,
      "grad_norm": 1.4297091960906982,
      "learning_rate": 0.00019911104207724316,
      "loss": 1.1675,
      "step": 6180
    },
    {
      "epoch": 0.8689723042316885,
      "grad_norm": 1.6465222835540771,
      "learning_rate": 0.0001990798461262962,
      "loss": 1.0303,
      "step": 6181
    },
    {
      "epoch": 0.8691128918880923,
      "grad_norm": 1.4053986072540283,
      "learning_rate": 0.00019904811472766574,
      "loss": 1.0782,
      "step": 6182
    },
    {
      "epoch": 0.8692534795444959,
      "grad_norm": 1.486846923828125,
      "learning_rate": 0.00019901584805283459,
      "loss": 1.0639,
      "step": 6183
    },
    {
      "epoch": 0.8693940672008997,
      "grad_norm": 1.597578525543213,
      "learning_rate": 0.00019898304627617857,
      "loss": 1.1583,
      "step": 6184
    },
    {
      "epoch": 0.8695346548573035,
      "grad_norm": 1.456912636756897,
      "learning_rate": 0.00019894970957496513,
      "loss": 1.0025,
      "step": 6185
    },
    {
      "epoch": 0.8696752425137073,
      "grad_norm": 1.375084400177002,
      "learning_rate": 0.00019891583812935254,
      "loss": 1.2217,
      "step": 6186
    },
    {
      "epoch": 0.8698158301701111,
      "grad_norm": 1.7421189546585083,
      "learning_rate": 0.00019888143212238907,
      "loss": 1.2681,
      "step": 6187
    },
    {
      "epoch": 0.8699564178265148,
      "grad_norm": 1.7346243858337402,
      "learning_rate": 0.00019884649174001175,
      "loss": 1.1399,
      "step": 6188
    },
    {
      "epoch": 0.8700970054829186,
      "grad_norm": 1.4927830696105957,
      "learning_rate": 0.00019881101717104556,
      "loss": 0.9755,
      "step": 6189
    },
    {
      "epoch": 0.8702375931393224,
      "grad_norm": 1.7113436460494995,
      "learning_rate": 0.00019877500860720227,
      "loss": 1.2004,
      "step": 6190
    },
    {
      "epoch": 0.8703781807957262,
      "grad_norm": 1.8939406871795654,
      "learning_rate": 0.0001987384662430795,
      "loss": 1.0836,
      "step": 6191
    },
    {
      "epoch": 0.8705187684521299,
      "grad_norm": 1.5908331871032715,
      "learning_rate": 0.00019870139027615966,
      "loss": 1.0833,
      "step": 6192
    },
    {
      "epoch": 0.8706593561085336,
      "grad_norm": 1.4946908950805664,
      "learning_rate": 0.00019866378090680885,
      "loss": 1.245,
      "step": 6193
    },
    {
      "epoch": 0.8707999437649374,
      "grad_norm": 1.758683443069458,
      "learning_rate": 0.00019862563833827567,
      "loss": 1.1093,
      "step": 6194
    },
    {
      "epoch": 0.8709405314213412,
      "grad_norm": 1.7881842851638794,
      "learning_rate": 0.00019858696277669046,
      "loss": 1.1133,
      "step": 6195
    },
    {
      "epoch": 0.871081119077745,
      "grad_norm": 1.4499465227127075,
      "learning_rate": 0.00019854775443106377,
      "loss": 1.1755,
      "step": 6196
    },
    {
      "epoch": 0.8712217067341488,
      "grad_norm": 1.532599687576294,
      "learning_rate": 0.00019850801351328548,
      "loss": 1.0932,
      "step": 6197
    },
    {
      "epoch": 0.8713622943905525,
      "grad_norm": 1.4890328645706177,
      "learning_rate": 0.00019846774023812364,
      "loss": 1.1525,
      "step": 6198
    },
    {
      "epoch": 0.8715028820469563,
      "grad_norm": 1.4294342994689941,
      "learning_rate": 0.00019842693482322323,
      "loss": 1.2282,
      "step": 6199
    },
    {
      "epoch": 0.87164346970336,
      "grad_norm": 1.4407870769500732,
      "learning_rate": 0.00019838559748910505,
      "loss": 1.178,
      "step": 6200
    },
    {
      "epoch": 0.8717840573597638,
      "grad_norm": 1.6586512327194214,
      "learning_rate": 0.00019834372845916446,
      "loss": 1.1086,
      "step": 6201
    },
    {
      "epoch": 0.8719246450161676,
      "grad_norm": 1.6664986610412598,
      "learning_rate": 0.00019830132795967015,
      "loss": 1.1761,
      "step": 6202
    },
    {
      "epoch": 0.8720652326725713,
      "grad_norm": 1.306622862815857,
      "learning_rate": 0.00019825839621976317,
      "loss": 1.1971,
      "step": 6203
    },
    {
      "epoch": 0.8722058203289751,
      "grad_norm": 1.666748285293579,
      "learning_rate": 0.00019821493347145543,
      "loss": 1.1046,
      "step": 6204
    },
    {
      "epoch": 0.8723464079853789,
      "grad_norm": 1.7657784223556519,
      "learning_rate": 0.00019817093994962837,
      "loss": 1.167,
      "step": 6205
    },
    {
      "epoch": 0.8724869956417827,
      "grad_norm": 1.7931715250015259,
      "learning_rate": 0.00019812641589203202,
      "loss": 0.9289,
      "step": 6206
    },
    {
      "epoch": 0.8726275832981865,
      "grad_norm": 1.6488878726959229,
      "learning_rate": 0.0001980813615392834,
      "loss": 1.1752,
      "step": 6207
    },
    {
      "epoch": 0.8727681709545901,
      "grad_norm": 1.5380927324295044,
      "learning_rate": 0.00019803577713486546,
      "loss": 1.1744,
      "step": 6208
    },
    {
      "epoch": 0.8729087586109939,
      "grad_norm": 1.6321132183074951,
      "learning_rate": 0.00019798966292512558,
      "loss": 1.042,
      "step": 6209
    },
    {
      "epoch": 0.8730493462673977,
      "grad_norm": 1.4531956911087036,
      "learning_rate": 0.0001979430191592744,
      "loss": 0.9495,
      "step": 6210
    },
    {
      "epoch": 0.8731899339238015,
      "grad_norm": 1.510131597518921,
      "learning_rate": 0.0001978958460893843,
      "loss": 1.2615,
      "step": 6211
    },
    {
      "epoch": 0.8733305215802053,
      "grad_norm": 1.550824522972107,
      "learning_rate": 0.00019784814397038822,
      "loss": 1.2603,
      "step": 6212
    },
    {
      "epoch": 0.873471109236609,
      "grad_norm": 1.6592379808425903,
      "learning_rate": 0.000197799913060078,
      "loss": 1.0632,
      "step": 6213
    },
    {
      "epoch": 0.8736116968930128,
      "grad_norm": 1.3620550632476807,
      "learning_rate": 0.0001977511536191035,
      "loss": 1.1808,
      "step": 6214
    },
    {
      "epoch": 0.8737522845494166,
      "grad_norm": 1.4227341413497925,
      "learning_rate": 0.00019770186591097055,
      "loss": 1.1627,
      "step": 6215
    },
    {
      "epoch": 0.8738928722058203,
      "grad_norm": 1.4327707290649414,
      "learning_rate": 0.00019765205020204,
      "loss": 1.0825,
      "step": 6216
    },
    {
      "epoch": 0.8740334598622241,
      "grad_norm": 1.363798975944519,
      "learning_rate": 0.00019760170676152603,
      "loss": 1.3218,
      "step": 6217
    },
    {
      "epoch": 0.8741740475186278,
      "grad_norm": 1.5900293588638306,
      "learning_rate": 0.00019755083586149496,
      "loss": 0.9668,
      "step": 6218
    },
    {
      "epoch": 0.8743146351750316,
      "grad_norm": 1.5782054662704468,
      "learning_rate": 0.00019749943777686347,
      "loss": 1.1599,
      "step": 6219
    },
    {
      "epoch": 0.8744552228314354,
      "grad_norm": 1.372436285018921,
      "learning_rate": 0.00019744751278539727,
      "loss": 1.2888,
      "step": 6220
    },
    {
      "epoch": 0.8745958104878392,
      "grad_norm": 1.6607648134231567,
      "learning_rate": 0.00019739506116770957,
      "loss": 1.0799,
      "step": 6221
    },
    {
      "epoch": 0.874736398144243,
      "grad_norm": 1.4729442596435547,
      "learning_rate": 0.00019734208320725972,
      "loss": 0.9825,
      "step": 6222
    },
    {
      "epoch": 0.8748769858006467,
      "grad_norm": 1.8251850605010986,
      "learning_rate": 0.00019728857919035125,
      "loss": 1.2521,
      "step": 6223
    },
    {
      "epoch": 0.8750175734570504,
      "grad_norm": 1.7067652940750122,
      "learning_rate": 0.00019723454940613088,
      "loss": 1.1862,
      "step": 6224
    },
    {
      "epoch": 0.8751581611134542,
      "grad_norm": 1.5229543447494507,
      "learning_rate": 0.00019717999414658655,
      "loss": 1.1365,
      "step": 6225
    },
    {
      "epoch": 0.875298748769858,
      "grad_norm": 1.4963674545288086,
      "learning_rate": 0.000197124913706546,
      "loss": 1.2076,
      "step": 6226
    },
    {
      "epoch": 0.8754393364262618,
      "grad_norm": 1.6534603834152222,
      "learning_rate": 0.00019706930838367517,
      "loss": 1.2654,
      "step": 6227
    },
    {
      "epoch": 0.8755799240826655,
      "grad_norm": 1.5388236045837402,
      "learning_rate": 0.00019701317847847652,
      "loss": 1.0325,
      "step": 6228
    },
    {
      "epoch": 0.8757205117390693,
      "grad_norm": 1.5922486782073975,
      "learning_rate": 0.00019695652429428754,
      "loss": 1.1181,
      "step": 6229
    },
    {
      "epoch": 0.8758610993954731,
      "grad_norm": 1.5083140134811401,
      "learning_rate": 0.00019689934613727902,
      "loss": 1.1231,
      "step": 6230
    },
    {
      "epoch": 0.8760016870518769,
      "grad_norm": 1.523826241493225,
      "learning_rate": 0.00019684164431645328,
      "loss": 1.213,
      "step": 6231
    },
    {
      "epoch": 0.8761422747082807,
      "grad_norm": 1.4014438390731812,
      "learning_rate": 0.00019678341914364274,
      "loss": 1.2392,
      "step": 6232
    },
    {
      "epoch": 0.8762828623646843,
      "grad_norm": 1.482775330543518,
      "learning_rate": 0.00019672467093350822,
      "loss": 1.178,
      "step": 6233
    },
    {
      "epoch": 0.8764234500210881,
      "grad_norm": 1.5870132446289062,
      "learning_rate": 0.0001966654000035369,
      "loss": 1.1547,
      "step": 6234
    },
    {
      "epoch": 0.8765640376774919,
      "grad_norm": 1.6379015445709229,
      "learning_rate": 0.00019660560667404098,
      "loss": 0.989,
      "step": 6235
    },
    {
      "epoch": 0.8767046253338957,
      "grad_norm": 1.8849948644638062,
      "learning_rate": 0.00019654529126815582,
      "loss": 1.0101,
      "step": 6236
    },
    {
      "epoch": 0.8768452129902995,
      "grad_norm": 1.940879464149475,
      "learning_rate": 0.00019648445411183816,
      "loss": 1.0609,
      "step": 6237
    },
    {
      "epoch": 0.8769858006467032,
      "grad_norm": 1.443382978439331,
      "learning_rate": 0.0001964230955338645,
      "loss": 1.2278,
      "step": 6238
    },
    {
      "epoch": 0.877126388303107,
      "grad_norm": 1.4613780975341797,
      "learning_rate": 0.0001963612158658289,
      "loss": 1.184,
      "step": 6239
    },
    {
      "epoch": 0.8772669759595108,
      "grad_norm": 1.462450385093689,
      "learning_rate": 0.00019629881544214176,
      "loss": 1.1319,
      "step": 6240
    },
    {
      "epoch": 0.8774075636159145,
      "grad_norm": 1.4693776369094849,
      "learning_rate": 0.00019623589460002783,
      "loss": 1.0803,
      "step": 6241
    },
    {
      "epoch": 0.8775481512723183,
      "grad_norm": 1.3492608070373535,
      "learning_rate": 0.00019617245367952403,
      "loss": 1.0576,
      "step": 6242
    },
    {
      "epoch": 0.877688738928722,
      "grad_norm": 1.559926152229309,
      "learning_rate": 0.00019610849302347804,
      "loss": 1.1316,
      "step": 6243
    },
    {
      "epoch": 0.8778293265851258,
      "grad_norm": 1.5201525688171387,
      "learning_rate": 0.00019604401297754635,
      "loss": 1.0137,
      "step": 6244
    },
    {
      "epoch": 0.8779699142415296,
      "grad_norm": 1.4809287786483765,
      "learning_rate": 0.0001959790138901922,
      "loss": 1.149,
      "step": 6245
    },
    {
      "epoch": 0.8781105018979334,
      "grad_norm": 1.5154500007629395,
      "learning_rate": 0.00019591349611268393,
      "loss": 1.2308,
      "step": 6246
    },
    {
      "epoch": 0.8782510895543372,
      "grad_norm": 1.4307491779327393,
      "learning_rate": 0.00019584745999909299,
      "loss": 0.9676,
      "step": 6247
    },
    {
      "epoch": 0.8783916772107409,
      "grad_norm": 1.8054863214492798,
      "learning_rate": 0.00019578090590629188,
      "loss": 1.006,
      "step": 6248
    },
    {
      "epoch": 0.8785322648671446,
      "grad_norm": 1.3877809047698975,
      "learning_rate": 0.0001957138341939527,
      "loss": 1.3351,
      "step": 6249
    },
    {
      "epoch": 0.8786728525235484,
      "grad_norm": 1.55948007106781,
      "learning_rate": 0.00019564624522454448,
      "loss": 0.9729,
      "step": 6250
    },
    {
      "epoch": 0.8788134401799522,
      "grad_norm": 1.6335482597351074,
      "learning_rate": 0.00019557813936333176,
      "loss": 1.1122,
      "step": 6251
    },
    {
      "epoch": 0.878954027836356,
      "grad_norm": 1.6987074613571167,
      "learning_rate": 0.0001955095169783727,
      "loss": 1.1003,
      "step": 6252
    },
    {
      "epoch": 0.8790946154927597,
      "grad_norm": 1.607047438621521,
      "learning_rate": 0.00019544037844051646,
      "loss": 1.1269,
      "step": 6253
    },
    {
      "epoch": 0.8792352031491635,
      "grad_norm": 1.823833703994751,
      "learning_rate": 0.00019537072412340186,
      "loss": 0.9292,
      "step": 6254
    },
    {
      "epoch": 0.8793757908055673,
      "grad_norm": 2.0652248859405518,
      "learning_rate": 0.00019530055440345506,
      "loss": 0.9585,
      "step": 6255
    },
    {
      "epoch": 0.8795163784619711,
      "grad_norm": 1.5573054552078247,
      "learning_rate": 0.00019522986965988745,
      "loss": 1.0223,
      "step": 6256
    },
    {
      "epoch": 0.8796569661183749,
      "grad_norm": 1.3964755535125732,
      "learning_rate": 0.00019515867027469392,
      "loss": 1.1722,
      "step": 6257
    },
    {
      "epoch": 0.8797975537747785,
      "grad_norm": 1.6252235174179077,
      "learning_rate": 0.00019508695663265031,
      "loss": 1.1172,
      "step": 6258
    },
    {
      "epoch": 0.8799381414311823,
      "grad_norm": 1.627620816230774,
      "learning_rate": 0.00019501472912131175,
      "loss": 1.1131,
      "step": 6259
    },
    {
      "epoch": 0.8800787290875861,
      "grad_norm": 2.025327205657959,
      "learning_rate": 0.00019494198813101065,
      "loss": 1.0675,
      "step": 6260
    },
    {
      "epoch": 0.8802193167439899,
      "grad_norm": 1.503050446510315,
      "learning_rate": 0.00019486873405485393,
      "loss": 1.0409,
      "step": 6261
    },
    {
      "epoch": 0.8803599044003937,
      "grad_norm": 1.4362365007400513,
      "learning_rate": 0.00019479496728872167,
      "loss": 1.0607,
      "step": 6262
    },
    {
      "epoch": 0.8805004920567974,
      "grad_norm": 1.410366177558899,
      "learning_rate": 0.0001947206882312644,
      "loss": 1.0454,
      "step": 6263
    },
    {
      "epoch": 0.8806410797132012,
      "grad_norm": 1.5220593214035034,
      "learning_rate": 0.0001946458972839014,
      "loss": 1.2778,
      "step": 6264
    },
    {
      "epoch": 0.880781667369605,
      "grad_norm": 1.7586288452148438,
      "learning_rate": 0.00019457059485081818,
      "loss": 1.2868,
      "step": 6265
    },
    {
      "epoch": 0.8809222550260087,
      "grad_norm": 1.7254953384399414,
      "learning_rate": 0.00019449478133896437,
      "loss": 1.0897,
      "step": 6266
    },
    {
      "epoch": 0.8810628426824125,
      "grad_norm": 1.543230652809143,
      "learning_rate": 0.0001944184571580516,
      "loss": 1.1527,
      "step": 6267
    },
    {
      "epoch": 0.8812034303388162,
      "grad_norm": 1.5498888492584229,
      "learning_rate": 0.00019434162272055148,
      "loss": 1.0726,
      "step": 6268
    },
    {
      "epoch": 0.88134401799522,
      "grad_norm": 1.760201334953308,
      "learning_rate": 0.0001942642784416928,
      "loss": 1.1273,
      "step": 6269
    },
    {
      "epoch": 0.8814846056516238,
      "grad_norm": 1.5934005975723267,
      "learning_rate": 0.00019418642473945986,
      "loss": 1.1804,
      "step": 6270
    },
    {
      "epoch": 0.8816251933080276,
      "grad_norm": 1.4877043962478638,
      "learning_rate": 0.00019410806203459002,
      "loss": 0.961,
      "step": 6271
    },
    {
      "epoch": 0.8817657809644314,
      "grad_norm": 1.5874605178833008,
      "learning_rate": 0.00019402919075057118,
      "loss": 1.0357,
      "step": 6272
    },
    {
      "epoch": 0.881906368620835,
      "grad_norm": 1.8418123722076416,
      "learning_rate": 0.00019394981131363998,
      "loss": 1.0466,
      "step": 6273
    },
    {
      "epoch": 0.8820469562772388,
      "grad_norm": 2.0968949794769287,
      "learning_rate": 0.00019386992415277888,
      "loss": 1.1216,
      "step": 6274
    },
    {
      "epoch": 0.8821875439336426,
      "grad_norm": 1.4984673261642456,
      "learning_rate": 0.0001937895296997145,
      "loss": 1.1645,
      "step": 6275
    },
    {
      "epoch": 0.8823281315900464,
      "grad_norm": 1.4272727966308594,
      "learning_rate": 0.00019370862838891494,
      "loss": 1.0875,
      "step": 6276
    },
    {
      "epoch": 0.8824687192464502,
      "grad_norm": 1.5758718252182007,
      "learning_rate": 0.00019362722065758718,
      "loss": 1.1545,
      "step": 6277
    },
    {
      "epoch": 0.8826093069028539,
      "grad_norm": 1.4487682580947876,
      "learning_rate": 0.00019354530694567528,
      "loss": 1.1255,
      "step": 6278
    },
    {
      "epoch": 0.8827498945592577,
      "grad_norm": 1.723207712173462,
      "learning_rate": 0.00019346288769585772,
      "loss": 1.0867,
      "step": 6279
    },
    {
      "epoch": 0.8828904822156615,
      "grad_norm": 1.7369710206985474,
      "learning_rate": 0.0001933799633535448,
      "loss": 1.267,
      "step": 6280
    },
    {
      "epoch": 0.8830310698720653,
      "grad_norm": 1.5821813344955444,
      "learning_rate": 0.00019329653436687662,
      "loss": 0.9647,
      "step": 6281
    },
    {
      "epoch": 0.883171657528469,
      "grad_norm": 1.5088082551956177,
      "learning_rate": 0.00019321260118672042,
      "loss": 1.1097,
      "step": 6282
    },
    {
      "epoch": 0.8833122451848727,
      "grad_norm": 1.6615917682647705,
      "learning_rate": 0.00019312816426666823,
      "loss": 1.1272,
      "step": 6283
    },
    {
      "epoch": 0.8834528328412765,
      "grad_norm": 1.6318162679672241,
      "learning_rate": 0.0001930432240630344,
      "loss": 1.1957,
      "step": 6284
    },
    {
      "epoch": 0.8835934204976803,
      "grad_norm": 1.699544906616211,
      "learning_rate": 0.00019295778103485303,
      "loss": 1.092,
      "step": 6285
    },
    {
      "epoch": 0.8837340081540841,
      "grad_norm": 1.452155351638794,
      "learning_rate": 0.00019287183564387563,
      "loss": 1.1093,
      "step": 6286
    },
    {
      "epoch": 0.8838745958104879,
      "grad_norm": 1.962667465209961,
      "learning_rate": 0.0001927853883545688,
      "loss": 0.9863,
      "step": 6287
    },
    {
      "epoch": 0.8840151834668916,
      "grad_norm": 1.726178765296936,
      "learning_rate": 0.00019269843963411117,
      "loss": 1.0038,
      "step": 6288
    },
    {
      "epoch": 0.8841557711232954,
      "grad_norm": 1.7817556858062744,
      "learning_rate": 0.00019261098995239123,
      "loss": 0.9891,
      "step": 6289
    },
    {
      "epoch": 0.8842963587796991,
      "grad_norm": 1.5124626159667969,
      "learning_rate": 0.00019252303978200522,
      "loss": 1.1666,
      "step": 6290
    },
    {
      "epoch": 0.8844369464361029,
      "grad_norm": 1.627833366394043,
      "learning_rate": 0.00019243458959825347,
      "loss": 1.1291,
      "step": 6291
    },
    {
      "epoch": 0.8845775340925067,
      "grad_norm": 1.400123953819275,
      "learning_rate": 0.000192345639879139,
      "loss": 0.9903,
      "step": 6292
    },
    {
      "epoch": 0.8847181217489104,
      "grad_norm": 1.5058045387268066,
      "learning_rate": 0.00019225619110536403,
      "loss": 1.1122,
      "step": 6293
    },
    {
      "epoch": 0.8848587094053142,
      "grad_norm": 1.5628327131271362,
      "learning_rate": 0.00019216624376032816,
      "loss": 1.0261,
      "step": 6294
    },
    {
      "epoch": 0.884999297061718,
      "grad_norm": 1.5974706411361694,
      "learning_rate": 0.00019207579833012512,
      "loss": 1.0237,
      "step": 6295
    },
    {
      "epoch": 0.8851398847181218,
      "grad_norm": 1.7940621376037598,
      "learning_rate": 0.0001919848553035404,
      "loss": 1.1585,
      "step": 6296
    },
    {
      "epoch": 0.8852804723745256,
      "grad_norm": 1.5367661714553833,
      "learning_rate": 0.00019189341517204857,
      "loss": 1.069,
      "step": 6297
    },
    {
      "epoch": 0.8854210600309292,
      "grad_norm": 1.9608746767044067,
      "learning_rate": 0.00019180147842981103,
      "loss": 1.0906,
      "step": 6298
    },
    {
      "epoch": 0.885561647687333,
      "grad_norm": 1.4846490621566772,
      "learning_rate": 0.0001917090455736724,
      "loss": 1.2059,
      "step": 6299
    },
    {
      "epoch": 0.8857022353437368,
      "grad_norm": 1.9853787422180176,
      "learning_rate": 0.00019161611710315882,
      "loss": 1.0617,
      "step": 6300
    },
    {
      "epoch": 0.8858428230001406,
      "grad_norm": 1.5920952558517456,
      "learning_rate": 0.0001915226935204745,
      "loss": 1.0634,
      "step": 6301
    },
    {
      "epoch": 0.8859834106565444,
      "grad_norm": 1.5354681015014648,
      "learning_rate": 0.00019142877533049975,
      "loss": 1.1546,
      "step": 6302
    },
    {
      "epoch": 0.8861239983129481,
      "grad_norm": 1.4436358213424683,
      "learning_rate": 0.00019133436304078754,
      "loss": 1.071,
      "step": 6303
    },
    {
      "epoch": 0.8862645859693519,
      "grad_norm": 1.700414776802063,
      "learning_rate": 0.00019123945716156105,
      "loss": 1.0366,
      "step": 6304
    },
    {
      "epoch": 0.8864051736257557,
      "grad_norm": 1.5852826833724976,
      "learning_rate": 0.000191144058205711,
      "loss": 1.0831,
      "step": 6305
    },
    {
      "epoch": 0.8865457612821595,
      "grad_norm": 1.7942229509353638,
      "learning_rate": 0.00019104816668879298,
      "loss": 1.2648,
      "step": 6306
    },
    {
      "epoch": 0.8866863489385632,
      "grad_norm": 1.7961047887802124,
      "learning_rate": 0.00019095178312902405,
      "loss": 1.1179,
      "step": 6307
    },
    {
      "epoch": 0.8868269365949669,
      "grad_norm": 1.5688387155532837,
      "learning_rate": 0.00019085490804728075,
      "loss": 1.082,
      "step": 6308
    },
    {
      "epoch": 0.8869675242513707,
      "grad_norm": 1.7847216129302979,
      "learning_rate": 0.00019075754196709572,
      "loss": 1.176,
      "step": 6309
    },
    {
      "epoch": 0.8871081119077745,
      "grad_norm": 1.7829781770706177,
      "learning_rate": 0.00019065968541465513,
      "loss": 1.1243,
      "step": 6310
    },
    {
      "epoch": 0.8872486995641783,
      "grad_norm": 1.3460321426391602,
      "learning_rate": 0.00019056133891879579,
      "loss": 1.0228,
      "step": 6311
    },
    {
      "epoch": 0.8873892872205821,
      "grad_norm": 1.7017979621887207,
      "learning_rate": 0.00019046250301100199,
      "loss": 1.097,
      "step": 6312
    },
    {
      "epoch": 0.8875298748769858,
      "grad_norm": 1.5155030488967896,
      "learning_rate": 0.00019036317822540336,
      "loss": 1.0929,
      "step": 6313
    },
    {
      "epoch": 0.8876704625333895,
      "grad_norm": 1.5356099605560303,
      "learning_rate": 0.00019026336509877126,
      "loss": 1.0342,
      "step": 6314
    },
    {
      "epoch": 0.8878110501897933,
      "grad_norm": 2.386984348297119,
      "learning_rate": 0.00019016306417051602,
      "loss": 1.0894,
      "step": 6315
    },
    {
      "epoch": 0.8879516378461971,
      "grad_norm": 2.086090326309204,
      "learning_rate": 0.00019006227598268433,
      "loss": 0.9626,
      "step": 6316
    },
    {
      "epoch": 0.8880922255026009,
      "grad_norm": 1.4490245580673218,
      "learning_rate": 0.00018996100107995632,
      "loss": 1.1968,
      "step": 6317
    },
    {
      "epoch": 0.8882328131590046,
      "grad_norm": 1.8368602991104126,
      "learning_rate": 0.00018985924000964195,
      "loss": 1.0438,
      "step": 6318
    },
    {
      "epoch": 0.8883734008154084,
      "grad_norm": 1.5259191989898682,
      "learning_rate": 0.000189756993321679,
      "loss": 1.2751,
      "step": 6319
    },
    {
      "epoch": 0.8885139884718122,
      "grad_norm": 2.003312349319458,
      "learning_rate": 0.0001896542615686291,
      "loss": 1.4439,
      "step": 6320
    },
    {
      "epoch": 0.888654576128216,
      "grad_norm": 1.4312918186187744,
      "learning_rate": 0.00018955104530567587,
      "loss": 1.2727,
      "step": 6321
    },
    {
      "epoch": 0.8887951637846198,
      "grad_norm": 1.3547639846801758,
      "learning_rate": 0.00018944734509062104,
      "loss": 0.9829,
      "step": 6322
    },
    {
      "epoch": 0.8889357514410234,
      "grad_norm": 1.5510088205337524,
      "learning_rate": 0.0001893431614838815,
      "loss": 1.189,
      "step": 6323
    },
    {
      "epoch": 0.8890763390974272,
      "grad_norm": 1.4649487733840942,
      "learning_rate": 0.0001892384950484867,
      "loss": 1.093,
      "step": 6324
    },
    {
      "epoch": 0.889216926753831,
      "grad_norm": 1.478615403175354,
      "learning_rate": 0.00018913334635007573,
      "loss": 1.181,
      "step": 6325
    },
    {
      "epoch": 0.8893575144102348,
      "grad_norm": 1.3818132877349854,
      "learning_rate": 0.00018902771595689325,
      "loss": 1.0702,
      "step": 6326
    },
    {
      "epoch": 0.8894981020666386,
      "grad_norm": 1.664141297340393,
      "learning_rate": 0.00018892160443978752,
      "loss": 1.2168,
      "step": 6327
    },
    {
      "epoch": 0.8896386897230423,
      "grad_norm": 1.3369783163070679,
      "learning_rate": 0.00018881501237220683,
      "loss": 1.1759,
      "step": 6328
    },
    {
      "epoch": 0.8897792773794461,
      "grad_norm": 1.81209135055542,
      "learning_rate": 0.00018870794033019645,
      "loss": 1.0203,
      "step": 6329
    },
    {
      "epoch": 0.8899198650358499,
      "grad_norm": 1.4312351942062378,
      "learning_rate": 0.00018860038889239558,
      "loss": 1.2242,
      "step": 6330
    },
    {
      "epoch": 0.8900604526922536,
      "grad_norm": 1.5400738716125488,
      "learning_rate": 0.00018849235864003386,
      "loss": 1.1914,
      "step": 6331
    },
    {
      "epoch": 0.8902010403486574,
      "grad_norm": 1.4642376899719238,
      "learning_rate": 0.0001883838501569291,
      "loss": 1.3285,
      "step": 6332
    },
    {
      "epoch": 0.8903416280050611,
      "grad_norm": 1.579999566078186,
      "learning_rate": 0.00018827486402948318,
      "loss": 1.1617,
      "step": 6333
    },
    {
      "epoch": 0.8904822156614649,
      "grad_norm": 1.7391313314437866,
      "learning_rate": 0.0001881654008466792,
      "loss": 1.1801,
      "step": 6334
    },
    {
      "epoch": 0.8906228033178687,
      "grad_norm": 1.484275460243225,
      "learning_rate": 0.00018805546120007863,
      "loss": 1.0459,
      "step": 6335
    },
    {
      "epoch": 0.8907633909742725,
      "grad_norm": 1.5246872901916504,
      "learning_rate": 0.0001879450456838177,
      "loss": 1.1465,
      "step": 6336
    },
    {
      "epoch": 0.8909039786306763,
      "grad_norm": 1.6901015043258667,
      "learning_rate": 0.00018783415489460442,
      "loss": 1.012,
      "step": 6337
    },
    {
      "epoch": 0.89104456628708,
      "grad_norm": 1.5391663312911987,
      "learning_rate": 0.0001877227894317152,
      "loss": 1.0717,
      "step": 6338
    },
    {
      "epoch": 0.8911851539434837,
      "grad_norm": 1.3827171325683594,
      "learning_rate": 0.0001876109498969915,
      "loss": 1.1567,
      "step": 6339
    },
    {
      "epoch": 0.8913257415998875,
      "grad_norm": 1.7718429565429688,
      "learning_rate": 0.00018749863689483722,
      "loss": 1.0981,
      "step": 6340
    },
    {
      "epoch": 0.8914663292562913,
      "grad_norm": 1.6265588998794556,
      "learning_rate": 0.00018738585103221473,
      "loss": 0.9565,
      "step": 6341
    },
    {
      "epoch": 0.8916069169126951,
      "grad_norm": 1.3795002698898315,
      "learning_rate": 0.00018727259291864164,
      "loss": 1.1312,
      "step": 6342
    },
    {
      "epoch": 0.8917475045690988,
      "grad_norm": 1.700629711151123,
      "learning_rate": 0.00018715886316618793,
      "loss": 1.0978,
      "step": 6343
    },
    {
      "epoch": 0.8918880922255026,
      "grad_norm": 1.5042741298675537,
      "learning_rate": 0.00018704466238947243,
      "loss": 1.1104,
      "step": 6344
    },
    {
      "epoch": 0.8920286798819064,
      "grad_norm": 1.664193034172058,
      "learning_rate": 0.00018692999120565934,
      "loss": 1.0792,
      "step": 6345
    },
    {
      "epoch": 0.8921692675383102,
      "grad_norm": 1.6032724380493164,
      "learning_rate": 0.00018681485023445518,
      "loss": 1.0361,
      "step": 6346
    },
    {
      "epoch": 0.892309855194714,
      "grad_norm": 1.562300443649292,
      "learning_rate": 0.00018669924009810518,
      "loss": 0.9804,
      "step": 6347
    },
    {
      "epoch": 0.8924504428511176,
      "grad_norm": 1.4831124544143677,
      "learning_rate": 0.00018658316142139007,
      "loss": 1.0278,
      "step": 6348
    },
    {
      "epoch": 0.8925910305075214,
      "grad_norm": 1.561264157295227,
      "learning_rate": 0.00018646661483162286,
      "loss": 1.2374,
      "step": 6349
    },
    {
      "epoch": 0.8927316181639252,
      "grad_norm": 1.4529964923858643,
      "learning_rate": 0.00018634960095864468,
      "loss": 1.185,
      "step": 6350
    },
    {
      "epoch": 0.892872205820329,
      "grad_norm": 1.5258409976959229,
      "learning_rate": 0.00018623212043482277,
      "loss": 1.093,
      "step": 6351
    },
    {
      "epoch": 0.8930127934767328,
      "grad_norm": 1.4549062252044678,
      "learning_rate": 0.0001861141738950456,
      "loss": 1.1798,
      "step": 6352
    },
    {
      "epoch": 0.8931533811331365,
      "grad_norm": 1.5218631029129028,
      "learning_rate": 0.00018599576197672035,
      "loss": 1.1266,
      "step": 6353
    },
    {
      "epoch": 0.8932939687895403,
      "grad_norm": 2.2403485774993896,
      "learning_rate": 0.00018587688531976918,
      "loss": 1.0561,
      "step": 6354
    },
    {
      "epoch": 0.893434556445944,
      "grad_norm": 1.8621560335159302,
      "learning_rate": 0.00018575754456662575,
      "loss": 1.1599,
      "step": 6355
    },
    {
      "epoch": 0.8935751441023478,
      "grad_norm": 1.5919421911239624,
      "learning_rate": 0.0001856377403622318,
      "loss": 1.1737,
      "step": 6356
    },
    {
      "epoch": 0.8937157317587516,
      "grad_norm": 1.4922878742218018,
      "learning_rate": 0.00018551747335403385,
      "loss": 1.0942,
      "step": 6357
    },
    {
      "epoch": 0.8938563194151553,
      "grad_norm": 1.8164589405059814,
      "learning_rate": 0.00018539674419197895,
      "loss": 1.3627,
      "step": 6358
    },
    {
      "epoch": 0.8939969070715591,
      "grad_norm": 1.8989347219467163,
      "learning_rate": 0.00018527555352851246,
      "loss": 1.0352,
      "step": 6359
    },
    {
      "epoch": 0.8941374947279629,
      "grad_norm": 1.9497973918914795,
      "learning_rate": 0.00018515390201857335,
      "loss": 0.9997,
      "step": 6360
    },
    {
      "epoch": 0.8942780823843667,
      "grad_norm": 1.4469410181045532,
      "learning_rate": 0.00018503179031959102,
      "loss": 1.2748,
      "step": 6361
    },
    {
      "epoch": 0.8944186700407705,
      "grad_norm": 1.5533922910690308,
      "learning_rate": 0.0001849092190914821,
      "loss": 0.9936,
      "step": 6362
    },
    {
      "epoch": 0.8945592576971741,
      "grad_norm": 1.5229958295822144,
      "learning_rate": 0.00018478618899664645,
      "loss": 1.0281,
      "step": 6363
    },
    {
      "epoch": 0.8946998453535779,
      "grad_norm": 1.5649421215057373,
      "learning_rate": 0.00018466270069996383,
      "loss": 1.1463,
      "step": 6364
    },
    {
      "epoch": 0.8948404330099817,
      "grad_norm": 1.5594871044158936,
      "learning_rate": 0.0001845387548687901,
      "loss": 1.2037,
      "step": 6365
    },
    {
      "epoch": 0.8949810206663855,
      "grad_norm": 1.7333632707595825,
      "learning_rate": 0.00018441435217295387,
      "loss": 1.0283,
      "step": 6366
    },
    {
      "epoch": 0.8951216083227893,
      "grad_norm": 1.5330479145050049,
      "learning_rate": 0.00018428949328475261,
      "loss": 1.1675,
      "step": 6367
    },
    {
      "epoch": 0.895262195979193,
      "grad_norm": 1.6849862337112427,
      "learning_rate": 0.0001841641788789493,
      "loss": 1.0626,
      "step": 6368
    },
    {
      "epoch": 0.8954027836355968,
      "grad_norm": 1.461786150932312,
      "learning_rate": 0.00018403840963276827,
      "loss": 1.0598,
      "step": 6369
    },
    {
      "epoch": 0.8955433712920006,
      "grad_norm": 1.4823565483093262,
      "learning_rate": 0.0001839121862258925,
      "loss": 1.2585,
      "step": 6370
    },
    {
      "epoch": 0.8956839589484044,
      "grad_norm": 1.6111935377120972,
      "learning_rate": 0.00018378550934045876,
      "loss": 0.994,
      "step": 6371
    },
    {
      "epoch": 0.8958245466048081,
      "grad_norm": 1.6366283893585205,
      "learning_rate": 0.00018365837966105486,
      "loss": 1.0852,
      "step": 6372
    },
    {
      "epoch": 0.8959651342612118,
      "grad_norm": 1.9053990840911865,
      "learning_rate": 0.0001835307978747155,
      "loss": 1.1538,
      "step": 6373
    },
    {
      "epoch": 0.8961057219176156,
      "grad_norm": 1.6906307935714722,
      "learning_rate": 0.00018340276467091862,
      "loss": 1.1129,
      "step": 6374
    },
    {
      "epoch": 0.8962463095740194,
      "grad_norm": 1.6204514503479004,
      "learning_rate": 0.00018327428074158174,
      "loss": 1.0773,
      "step": 6375
    },
    {
      "epoch": 0.8963868972304232,
      "grad_norm": 1.5157361030578613,
      "learning_rate": 0.0001831453467810584,
      "loss": 1.0869,
      "step": 6376
    },
    {
      "epoch": 0.896527484886827,
      "grad_norm": 1.6222038269042969,
      "learning_rate": 0.00018301596348613362,
      "loss": 1.003,
      "step": 6377
    },
    {
      "epoch": 0.8966680725432307,
      "grad_norm": 1.7853180170059204,
      "learning_rate": 0.0001828861315560215,
      "loss": 0.97,
      "step": 6378
    },
    {
      "epoch": 0.8968086601996345,
      "grad_norm": 1.358960509300232,
      "learning_rate": 0.00018275585169235997,
      "loss": 1.195,
      "step": 6379
    },
    {
      "epoch": 0.8969492478560382,
      "grad_norm": 1.5706301927566528,
      "learning_rate": 0.00018262512459920807,
      "loss": 1.2116,
      "step": 6380
    },
    {
      "epoch": 0.897089835512442,
      "grad_norm": 1.6354516744613647,
      "learning_rate": 0.00018249395098304168,
      "loss": 1.0305,
      "step": 6381
    },
    {
      "epoch": 0.8972304231688458,
      "grad_norm": 1.452207326889038,
      "learning_rate": 0.0001823623315527497,
      "loss": 1.2923,
      "step": 6382
    },
    {
      "epoch": 0.8973710108252495,
      "grad_norm": 1.8590906858444214,
      "learning_rate": 0.0001822302670196304,
      "loss": 1.1947,
      "step": 6383
    },
    {
      "epoch": 0.8975115984816533,
      "grad_norm": 1.9794228076934814,
      "learning_rate": 0.00018209775809738745,
      "loss": 1.1399,
      "step": 6384
    },
    {
      "epoch": 0.8976521861380571,
      "grad_norm": 1.7283499240875244,
      "learning_rate": 0.00018196480550212603,
      "loss": 1.0648,
      "step": 6385
    },
    {
      "epoch": 0.8977927737944609,
      "grad_norm": 1.5011584758758545,
      "learning_rate": 0.00018183140995234917,
      "loss": 1.1469,
      "step": 6386
    },
    {
      "epoch": 0.8979333614508647,
      "grad_norm": 1.577842354774475,
      "learning_rate": 0.00018169757216895347,
      "loss": 1.0114,
      "step": 6387
    },
    {
      "epoch": 0.8980739491072683,
      "grad_norm": 1.6186087131500244,
      "learning_rate": 0.00018156329287522552,
      "loss": 1.0746,
      "step": 6388
    },
    {
      "epoch": 0.8982145367636721,
      "grad_norm": 1.5673028230667114,
      "learning_rate": 0.0001814285727968383,
      "loss": 1.1323,
      "step": 6389
    },
    {
      "epoch": 0.8983551244200759,
      "grad_norm": 1.663134217262268,
      "learning_rate": 0.00018129341266184633,
      "loss": 1.1743,
      "step": 6390
    },
    {
      "epoch": 0.8984957120764797,
      "grad_norm": 1.5014607906341553,
      "learning_rate": 0.0001811578132006826,
      "loss": 1.1924,
      "step": 6391
    },
    {
      "epoch": 0.8986362997328835,
      "grad_norm": 1.4714635610580444,
      "learning_rate": 0.0001810217751461542,
      "loss": 1.1135,
      "step": 6392
    },
    {
      "epoch": 0.8987768873892872,
      "grad_norm": 1.5610110759735107,
      "learning_rate": 0.00018088529923343852,
      "loss": 1.2171,
      "step": 6393
    },
    {
      "epoch": 0.898917475045691,
      "grad_norm": 1.5925227403640747,
      "learning_rate": 0.00018074838620007927,
      "loss": 1.0544,
      "step": 6394
    },
    {
      "epoch": 0.8990580627020948,
      "grad_norm": 1.6463463306427002,
      "learning_rate": 0.00018061103678598242,
      "loss": 1.0578,
      "step": 6395
    },
    {
      "epoch": 0.8991986503584986,
      "grad_norm": 1.5895497798919678,
      "learning_rate": 0.00018047325173341193,
      "loss": 1.207,
      "step": 6396
    },
    {
      "epoch": 0.8993392380149023,
      "grad_norm": 1.597874402999878,
      "learning_rate": 0.00018033503178698664,
      "loss": 1.2434,
      "step": 6397
    },
    {
      "epoch": 0.899479825671306,
      "grad_norm": 1.6492507457733154,
      "learning_rate": 0.00018019637769367514,
      "loss": 0.925,
      "step": 6398
    },
    {
      "epoch": 0.8996204133277098,
      "grad_norm": 1.350553274154663,
      "learning_rate": 0.00018005729020279242,
      "loss": 1.0922,
      "step": 6399
    },
    {
      "epoch": 0.8997610009841136,
      "grad_norm": 1.4840247631072998,
      "learning_rate": 0.00017991777006599565,
      "loss": 1.1694,
      "step": 6400
    },
    {
      "epoch": 0.8999015886405174,
      "grad_norm": 1.6107951402664185,
      "learning_rate": 0.00017977781803728014,
      "loss": 1.0152,
      "step": 6401
    },
    {
      "epoch": 0.9000421762969212,
      "grad_norm": 1.5258146524429321,
      "learning_rate": 0.00017963743487297507,
      "loss": 1.2058,
      "step": 6402
    },
    {
      "epoch": 0.9001827639533249,
      "grad_norm": 1.5930966138839722,
      "learning_rate": 0.00017949662133173977,
      "loss": 1.2802,
      "step": 6403
    },
    {
      "epoch": 0.9003233516097287,
      "grad_norm": 1.5087382793426514,
      "learning_rate": 0.0001793553781745593,
      "loss": 1.1449,
      "step": 6404
    },
    {
      "epoch": 0.9004639392661324,
      "grad_norm": 1.8385536670684814,
      "learning_rate": 0.0001792137061647405,
      "loss": 1.0766,
      "step": 6405
    },
    {
      "epoch": 0.9006045269225362,
      "grad_norm": 2.1027214527130127,
      "learning_rate": 0.00017907160606790766,
      "loss": 1.1998,
      "step": 6406
    },
    {
      "epoch": 0.90074511457894,
      "grad_norm": 1.3805711269378662,
      "learning_rate": 0.00017892907865199869,
      "loss": 1.0941,
      "step": 6407
    },
    {
      "epoch": 0.9008857022353437,
      "grad_norm": 1.5898939371109009,
      "learning_rate": 0.000178786124687261,
      "loss": 1.1087,
      "step": 6408
    },
    {
      "epoch": 0.9010262898917475,
      "grad_norm": 1.648245930671692,
      "learning_rate": 0.00017864274494624677,
      "loss": 1.0373,
      "step": 6409
    },
    {
      "epoch": 0.9011668775481513,
      "grad_norm": 1.480545163154602,
      "learning_rate": 0.0001784989402038093,
      "loss": 0.9882,
      "step": 6410
    },
    {
      "epoch": 0.9013074652045551,
      "grad_norm": 1.467873454093933,
      "learning_rate": 0.00017835471123709887,
      "loss": 1.0726,
      "step": 6411
    },
    {
      "epoch": 0.9014480528609589,
      "grad_norm": 1.449565052986145,
      "learning_rate": 0.00017821005882555816,
      "loss": 1.0618,
      "step": 6412
    },
    {
      "epoch": 0.9015886405173625,
      "grad_norm": 1.4734703302383423,
      "learning_rate": 0.00017806498375091837,
      "loss": 1.0533,
      "step": 6413
    },
    {
      "epoch": 0.9017292281737663,
      "grad_norm": 1.5945206880569458,
      "learning_rate": 0.00017791948679719458,
      "loss": 1.0787,
      "step": 6414
    },
    {
      "epoch": 0.9018698158301701,
      "grad_norm": 1.4688513278961182,
      "learning_rate": 0.000177773568750682,
      "loss": 1.1385,
      "step": 6415
    },
    {
      "epoch": 0.9020104034865739,
      "grad_norm": 1.3910737037658691,
      "learning_rate": 0.00017762723039995177,
      "loss": 1.0934,
      "step": 6416
    },
    {
      "epoch": 0.9021509911429777,
      "grad_norm": 1.3325132131576538,
      "learning_rate": 0.0001774804725358459,
      "loss": 1.1799,
      "step": 6417
    },
    {
      "epoch": 0.9022915787993814,
      "grad_norm": 2.0555496215820312,
      "learning_rate": 0.00017733329595147384,
      "loss": 1.0889,
      "step": 6418
    },
    {
      "epoch": 0.9024321664557852,
      "grad_norm": 1.5097869634628296,
      "learning_rate": 0.00017718570144220795,
      "loss": 1.1177,
      "step": 6419
    },
    {
      "epoch": 0.902572754112189,
      "grad_norm": 1.9092854261398315,
      "learning_rate": 0.00017703768980567892,
      "loss": 0.9628,
      "step": 6420
    },
    {
      "epoch": 0.9027133417685927,
      "grad_norm": 1.5738064050674438,
      "learning_rate": 0.000176889261841772,
      "loss": 1.1285,
      "step": 6421
    },
    {
      "epoch": 0.9028539294249965,
      "grad_norm": 1.5052008628845215,
      "learning_rate": 0.00017674041835262193,
      "loss": 1.1053,
      "step": 6422
    },
    {
      "epoch": 0.9029945170814002,
      "grad_norm": 1.4882272481918335,
      "learning_rate": 0.00017659116014260925,
      "loss": 1.2357,
      "step": 6423
    },
    {
      "epoch": 0.903135104737804,
      "grad_norm": 1.8263726234436035,
      "learning_rate": 0.00017644148801835606,
      "loss": 1.055,
      "step": 6424
    },
    {
      "epoch": 0.9032756923942078,
      "grad_norm": 1.932105541229248,
      "learning_rate": 0.00017629140278872075,
      "loss": 1.105,
      "step": 6425
    },
    {
      "epoch": 0.9034162800506116,
      "grad_norm": 1.8241925239562988,
      "learning_rate": 0.00017614090526479442,
      "loss": 0.9397,
      "step": 6426
    },
    {
      "epoch": 0.9035568677070154,
      "grad_norm": 1.509896993637085,
      "learning_rate": 0.00017598999625989675,
      "loss": 1.1698,
      "step": 6427
    },
    {
      "epoch": 0.903697455363419,
      "grad_norm": 1.6506980657577515,
      "learning_rate": 0.00017583867658957041,
      "loss": 0.9559,
      "step": 6428
    },
    {
      "epoch": 0.9038380430198228,
      "grad_norm": 1.7758029699325562,
      "learning_rate": 0.00017568694707157797,
      "loss": 1.0795,
      "step": 6429
    },
    {
      "epoch": 0.9039786306762266,
      "grad_norm": 1.634661078453064,
      "learning_rate": 0.00017553480852589636,
      "loss": 0.9192,
      "step": 6430
    },
    {
      "epoch": 0.9041192183326304,
      "grad_norm": 1.4851421117782593,
      "learning_rate": 0.00017538226177471356,
      "loss": 1.1764,
      "step": 6431
    },
    {
      "epoch": 0.9042598059890342,
      "grad_norm": 2.0490009784698486,
      "learning_rate": 0.00017522930764242336,
      "loss": 0.9901,
      "step": 6432
    },
    {
      "epoch": 0.9044003936454379,
      "grad_norm": 1.7594674825668335,
      "learning_rate": 0.0001750759469556208,
      "loss": 1.0774,
      "step": 6433
    },
    {
      "epoch": 0.9045409813018417,
      "grad_norm": 2.054579019546509,
      "learning_rate": 0.00017492218054309836,
      "loss": 1.2097,
      "step": 6434
    },
    {
      "epoch": 0.9046815689582455,
      "grad_norm": 1.58494234085083,
      "learning_rate": 0.00017476800923584137,
      "loss": 1.1214,
      "step": 6435
    },
    {
      "epoch": 0.9048221566146493,
      "grad_norm": 1.4708749055862427,
      "learning_rate": 0.00017461343386702266,
      "loss": 1.1703,
      "step": 6436
    },
    {
      "epoch": 0.904962744271053,
      "grad_norm": 1.6256266832351685,
      "learning_rate": 0.00017445845527199917,
      "loss": 0.9327,
      "step": 6437
    },
    {
      "epoch": 0.9051033319274567,
      "grad_norm": 1.487226128578186,
      "learning_rate": 0.00017430307428830674,
      "loss": 1.1677,
      "step": 6438
    },
    {
      "epoch": 0.9052439195838605,
      "grad_norm": 2.2381091117858887,
      "learning_rate": 0.00017414729175565597,
      "loss": 1.0257,
      "step": 6439
    },
    {
      "epoch": 0.9053845072402643,
      "grad_norm": 1.6512130498886108,
      "learning_rate": 0.00017399110851592747,
      "loss": 1.0079,
      "step": 6440
    },
    {
      "epoch": 0.9055250948966681,
      "grad_norm": 1.796645164489746,
      "learning_rate": 0.0001738345254131671,
      "loss": 1.0531,
      "step": 6441
    },
    {
      "epoch": 0.9056656825530719,
      "grad_norm": 1.7165007591247559,
      "learning_rate": 0.00017367754329358185,
      "loss": 1.1111,
      "step": 6442
    },
    {
      "epoch": 0.9058062702094756,
      "grad_norm": 1.8889862298965454,
      "learning_rate": 0.00017352016300553545,
      "loss": 1.1374,
      "step": 6443
    },
    {
      "epoch": 0.9059468578658794,
      "grad_norm": 1.535206913948059,
      "learning_rate": 0.0001733623853995427,
      "loss": 1.1941,
      "step": 6444
    },
    {
      "epoch": 0.9060874455222832,
      "grad_norm": 1.6663532257080078,
      "learning_rate": 0.0001732042113282659,
      "loss": 1.0994,
      "step": 6445
    },
    {
      "epoch": 0.9062280331786869,
      "grad_norm": 1.553877830505371,
      "learning_rate": 0.00017304564164651045,
      "loss": 1.2882,
      "step": 6446
    },
    {
      "epoch": 0.9063686208350907,
      "grad_norm": 1.3083009719848633,
      "learning_rate": 0.00017288667721121877,
      "loss": 1.2892,
      "step": 6447
    },
    {
      "epoch": 0.9065092084914944,
      "grad_norm": 1.467901587486267,
      "learning_rate": 0.00017272731888146728,
      "loss": 1.1591,
      "step": 6448
    },
    {
      "epoch": 0.9066497961478982,
      "grad_norm": 1.4598779678344727,
      "learning_rate": 0.00017256756751846063,
      "loss": 1.1777,
      "step": 6449
    },
    {
      "epoch": 0.906790383804302,
      "grad_norm": 1.4959535598754883,
      "learning_rate": 0.000172407423985528,
      "loss": 1.0108,
      "step": 6450
    },
    {
      "epoch": 0.9069309714607058,
      "grad_norm": 1.7344943284988403,
      "learning_rate": 0.00017224688914811774,
      "loss": 1.188,
      "step": 6451
    },
    {
      "epoch": 0.9070715591171096,
      "grad_norm": 1.4673240184783936,
      "learning_rate": 0.00017208596387379262,
      "loss": 1.166,
      "step": 6452
    },
    {
      "epoch": 0.9072121467735133,
      "grad_norm": 1.6872568130493164,
      "learning_rate": 0.00017192464903222554,
      "loss": 1.1948,
      "step": 6453
    },
    {
      "epoch": 0.907352734429917,
      "grad_norm": 1.8632227182388306,
      "learning_rate": 0.00017176294549519518,
      "loss": 1.1661,
      "step": 6454
    },
    {
      "epoch": 0.9074933220863208,
      "grad_norm": 1.4482210874557495,
      "learning_rate": 0.0001716008541365801,
      "loss": 1.1283,
      "step": 6455
    },
    {
      "epoch": 0.9076339097427246,
      "grad_norm": 1.806378960609436,
      "learning_rate": 0.00017143837583235522,
      "loss": 1.0424,
      "step": 6456
    },
    {
      "epoch": 0.9077744973991284,
      "grad_norm": 1.6169887781143188,
      "learning_rate": 0.00017127551146058607,
      "loss": 1.1595,
      "step": 6457
    },
    {
      "epoch": 0.9079150850555321,
      "grad_norm": 1.686741590499878,
      "learning_rate": 0.00017111226190142528,
      "loss": 1.0266,
      "step": 6458
    },
    {
      "epoch": 0.9080556727119359,
      "grad_norm": 1.803505301475525,
      "learning_rate": 0.00017094862803710673,
      "loss": 1.019,
      "step": 6459
    },
    {
      "epoch": 0.9081962603683397,
      "grad_norm": 1.7755167484283447,
      "learning_rate": 0.000170784610751941,
      "loss": 1.0453,
      "step": 6460
    },
    {
      "epoch": 0.9083368480247435,
      "grad_norm": 2.183396577835083,
      "learning_rate": 0.00017062021093231083,
      "loss": 1.1797,
      "step": 6461
    },
    {
      "epoch": 0.9084774356811472,
      "grad_norm": 1.800002932548523,
      "learning_rate": 0.00017045542946666676,
      "loss": 1.1146,
      "step": 6462
    },
    {
      "epoch": 0.9086180233375509,
      "grad_norm": 1.435413122177124,
      "learning_rate": 0.00017029026724552104,
      "loss": 1.1596,
      "step": 6463
    },
    {
      "epoch": 0.9087586109939547,
      "grad_norm": 1.7970134019851685,
      "learning_rate": 0.00017012472516144414,
      "loss": 1.0938,
      "step": 6464
    },
    {
      "epoch": 0.9088991986503585,
      "grad_norm": 1.7299251556396484,
      "learning_rate": 0.00016995880410905918,
      "loss": 1.1357,
      "step": 6465
    },
    {
      "epoch": 0.9090397863067623,
      "grad_norm": 1.965127944946289,
      "learning_rate": 0.00016979250498503736,
      "loss": 1.1147,
      "step": 6466
    },
    {
      "epoch": 0.9091803739631661,
      "grad_norm": 1.747715950012207,
      "learning_rate": 0.00016962582868809318,
      "loss": 1.155,
      "step": 6467
    },
    {
      "epoch": 0.9093209616195698,
      "grad_norm": 1.6939440965652466,
      "learning_rate": 0.00016945877611897894,
      "loss": 1.0453,
      "step": 6468
    },
    {
      "epoch": 0.9094615492759736,
      "grad_norm": 1.6203558444976807,
      "learning_rate": 0.00016929134818048115,
      "loss": 0.9897,
      "step": 6469
    },
    {
      "epoch": 0.9096021369323773,
      "grad_norm": 1.499569296836853,
      "learning_rate": 0.00016912354577741453,
      "loss": 1.0271,
      "step": 6470
    },
    {
      "epoch": 0.9097427245887811,
      "grad_norm": 1.5112069845199585,
      "learning_rate": 0.00016895536981661717,
      "loss": 0.9109,
      "step": 6471
    },
    {
      "epoch": 0.9098833122451849,
      "grad_norm": 1.4448838233947754,
      "learning_rate": 0.00016878682120694626,
      "loss": 1.0678,
      "step": 6472
    },
    {
      "epoch": 0.9100238999015886,
      "grad_norm": 1.3693716526031494,
      "learning_rate": 0.00016861790085927325,
      "loss": 1.152,
      "step": 6473
    },
    {
      "epoch": 0.9101644875579924,
      "grad_norm": 1.5944666862487793,
      "learning_rate": 0.00016844860968647773,
      "loss": 1.0419,
      "step": 6474
    },
    {
      "epoch": 0.9103050752143962,
      "grad_norm": 1.4088836908340454,
      "learning_rate": 0.00016827894860344395,
      "loss": 1.0718,
      "step": 6475
    },
    {
      "epoch": 0.9104456628708,
      "grad_norm": 1.6105141639709473,
      "learning_rate": 0.0001681089185270546,
      "loss": 1.1452,
      "step": 6476
    },
    {
      "epoch": 0.9105862505272038,
      "grad_norm": 1.6385246515274048,
      "learning_rate": 0.0001679385203761873,
      "loss": 1.015,
      "step": 6477
    },
    {
      "epoch": 0.9107268381836074,
      "grad_norm": 1.4777799844741821,
      "learning_rate": 0.00016776775507170835,
      "loss": 0.9971,
      "step": 6478
    },
    {
      "epoch": 0.9108674258400112,
      "grad_norm": 1.528877854347229,
      "learning_rate": 0.00016759662353646795,
      "loss": 1.131,
      "step": 6479
    },
    {
      "epoch": 0.911008013496415,
      "grad_norm": 1.638806700706482,
      "learning_rate": 0.00016742512669529593,
      "loss": 1.1725,
      "step": 6480
    },
    {
      "epoch": 0.9111486011528188,
      "grad_norm": 1.8165045976638794,
      "learning_rate": 0.00016725326547499644,
      "loss": 1.1684,
      "step": 6481
    },
    {
      "epoch": 0.9112891888092226,
      "grad_norm": 1.8175681829452515,
      "learning_rate": 0.0001670810408043422,
      "loss": 1.0095,
      "step": 6482
    },
    {
      "epoch": 0.9114297764656263,
      "grad_norm": 1.5764129161834717,
      "learning_rate": 0.0001669084536140706,
      "loss": 1.0272,
      "step": 6483
    },
    {
      "epoch": 0.9115703641220301,
      "grad_norm": 1.6867483854293823,
      "learning_rate": 0.00016673550483687792,
      "loss": 1.0815,
      "step": 6484
    },
    {
      "epoch": 0.9117109517784339,
      "grad_norm": 1.461264967918396,
      "learning_rate": 0.00016656219540741468,
      "loss": 1.3025,
      "step": 6485
    },
    {
      "epoch": 0.9118515394348377,
      "grad_norm": 1.8908863067626953,
      "learning_rate": 0.00016638852626228042,
      "loss": 0.9165,
      "step": 6486
    },
    {
      "epoch": 0.9119921270912413,
      "grad_norm": 1.4980528354644775,
      "learning_rate": 0.00016621449834001826,
      "loss": 1.0986,
      "step": 6487
    },
    {
      "epoch": 0.9121327147476451,
      "grad_norm": 1.5398097038269043,
      "learning_rate": 0.00016604011258111094,
      "loss": 1.1878,
      "step": 6488
    },
    {
      "epoch": 0.9122733024040489,
      "grad_norm": 1.7722392082214355,
      "learning_rate": 0.00016586536992797466,
      "loss": 1.0674,
      "step": 6489
    },
    {
      "epoch": 0.9124138900604527,
      "grad_norm": 1.5136586427688599,
      "learning_rate": 0.00016569027132495406,
      "loss": 0.9756,
      "step": 6490
    },
    {
      "epoch": 0.9125544777168565,
      "grad_norm": 1.6385295391082764,
      "learning_rate": 0.00016551481771831777,
      "loss": 1.1825,
      "step": 6491
    },
    {
      "epoch": 0.9126950653732602,
      "grad_norm": 1.4922282695770264,
      "learning_rate": 0.0001653390100562529,
      "loss": 1.0636,
      "step": 6492
    },
    {
      "epoch": 0.912835653029664,
      "grad_norm": 1.3083932399749756,
      "learning_rate": 0.0001651628492888599,
      "loss": 1.2061,
      "step": 6493
    },
    {
      "epoch": 0.9129762406860678,
      "grad_norm": 1.4748148918151855,
      "learning_rate": 0.0001649863363681475,
      "loss": 0.9789,
      "step": 6494
    },
    {
      "epoch": 0.9131168283424715,
      "grad_norm": 1.5133260488510132,
      "learning_rate": 0.00016480947224802722,
      "loss": 1.1881,
      "step": 6495
    },
    {
      "epoch": 0.9132574159988753,
      "grad_norm": 1.5259737968444824,
      "learning_rate": 0.00016463225788430918,
      "loss": 1.0954,
      "step": 6496
    },
    {
      "epoch": 0.913398003655279,
      "grad_norm": 1.631985068321228,
      "learning_rate": 0.00016445469423469588,
      "loss": 1.1003,
      "step": 6497
    },
    {
      "epoch": 0.9135385913116828,
      "grad_norm": 1.471838116645813,
      "learning_rate": 0.00016427678225877724,
      "loss": 1.0641,
      "step": 6498
    },
    {
      "epoch": 0.9136791789680866,
      "grad_norm": 1.5136257410049438,
      "learning_rate": 0.00016409852291802594,
      "loss": 1.2235,
      "step": 6499
    },
    {
      "epoch": 0.9138197666244904,
      "grad_norm": 1.5724480152130127,
      "learning_rate": 0.00016391991717579186,
      "loss": 1.1526,
      "step": 6500
    },
    {
      "epoch": 0.9138197666244904,
      "eval_loss": 1.167434811592102,
      "eval_runtime": 771.8258,
      "eval_samples_per_second": 16.385,
      "eval_steps_per_second": 8.192,
      "step": 6500
    },
    {
      "epoch": 0.9139603542808942,
      "grad_norm": 1.565011978149414,
      "learning_rate": 0.00016374096599729678,
      "loss": 1.0654,
      "step": 6501
    },
    {
      "epoch": 0.9141009419372979,
      "grad_norm": 1.6404300928115845,
      "learning_rate": 0.00016356167034962935,
      "loss": 1.1936,
      "step": 6502
    },
    {
      "epoch": 0.9142415295937016,
      "grad_norm": 1.4811471700668335,
      "learning_rate": 0.00016338203120173982,
      "loss": 1.0881,
      "step": 6503
    },
    {
      "epoch": 0.9143821172501054,
      "grad_norm": 1.9758319854736328,
      "learning_rate": 0.00016320204952443472,
      "loss": 1.199,
      "step": 6504
    },
    {
      "epoch": 0.9145227049065092,
      "grad_norm": 1.5140650272369385,
      "learning_rate": 0.0001630217262903719,
      "loss": 1.1846,
      "step": 6505
    },
    {
      "epoch": 0.914663292562913,
      "grad_norm": 1.5420124530792236,
      "learning_rate": 0.00016284106247405443,
      "loss": 1.1598,
      "step": 6506
    },
    {
      "epoch": 0.9148038802193167,
      "grad_norm": 1.5033739805221558,
      "learning_rate": 0.000162660059051827,
      "loss": 1.2326,
      "step": 6507
    },
    {
      "epoch": 0.9149444678757205,
      "grad_norm": 1.7205308675765991,
      "learning_rate": 0.0001624787170018685,
      "loss": 1.2471,
      "step": 6508
    },
    {
      "epoch": 0.9150850555321243,
      "grad_norm": 1.3691825866699219,
      "learning_rate": 0.00016229703730418851,
      "loss": 1.0816,
      "step": 6509
    },
    {
      "epoch": 0.9152256431885281,
      "grad_norm": 1.5032117366790771,
      "learning_rate": 0.00016211502094062117,
      "loss": 1.0879,
      "step": 6510
    },
    {
      "epoch": 0.9153662308449318,
      "grad_norm": 1.8461569547653198,
      "learning_rate": 0.00016193266889482,
      "loss": 1.0341,
      "step": 6511
    },
    {
      "epoch": 0.9155068185013355,
      "grad_norm": 1.7466171979904175,
      "learning_rate": 0.00016174998215225258,
      "loss": 1.0044,
      "step": 6512
    },
    {
      "epoch": 0.9156474061577393,
      "grad_norm": 1.727466106414795,
      "learning_rate": 0.00016156696170019545,
      "loss": 0.9111,
      "step": 6513
    },
    {
      "epoch": 0.9157879938141431,
      "grad_norm": 1.4870117902755737,
      "learning_rate": 0.0001613836085277281,
      "loss": 1.1949,
      "step": 6514
    },
    {
      "epoch": 0.9159285814705469,
      "grad_norm": 2.302819013595581,
      "learning_rate": 0.00016119992362572874,
      "loss": 1.0363,
      "step": 6515
    },
    {
      "epoch": 0.9160691691269507,
      "grad_norm": 1.4231330156326294,
      "learning_rate": 0.00016101590798686815,
      "loss": 1.2416,
      "step": 6516
    },
    {
      "epoch": 0.9162097567833544,
      "grad_norm": 1.4687601327896118,
      "learning_rate": 0.00016083156260560398,
      "loss": 1.1473,
      "step": 6517
    },
    {
      "epoch": 0.9163503444397582,
      "grad_norm": 1.524886965751648,
      "learning_rate": 0.00016064688847817639,
      "loss": 1.2734,
      "step": 6518
    },
    {
      "epoch": 0.916490932096162,
      "grad_norm": 1.5301485061645508,
      "learning_rate": 0.00016046188660260205,
      "loss": 1.046,
      "step": 6519
    },
    {
      "epoch": 0.9166315197525657,
      "grad_norm": 1.6526577472686768,
      "learning_rate": 0.00016027655797866877,
      "loss": 1.1024,
      "step": 6520
    },
    {
      "epoch": 0.9167721074089695,
      "grad_norm": 1.5091158151626587,
      "learning_rate": 0.00016009090360793023,
      "loss": 0.9972,
      "step": 6521
    },
    {
      "epoch": 0.9169126950653732,
      "grad_norm": 1.741461992263794,
      "learning_rate": 0.00015990492449370047,
      "loss": 1.2161,
      "step": 6522
    },
    {
      "epoch": 0.917053282721777,
      "grad_norm": 1.5016896724700928,
      "learning_rate": 0.00015971862164104855,
      "loss": 1.1306,
      "step": 6523
    },
    {
      "epoch": 0.9171938703781808,
      "grad_norm": 1.2519829273223877,
      "learning_rate": 0.0001595319960567932,
      "loss": 1.1693,
      "step": 6524
    },
    {
      "epoch": 0.9173344580345846,
      "grad_norm": 2.0255234241485596,
      "learning_rate": 0.00015934504874949665,
      "loss": 1.0875,
      "step": 6525
    },
    {
      "epoch": 0.9174750456909884,
      "grad_norm": 1.5795334577560425,
      "learning_rate": 0.0001591577807294609,
      "loss": 0.9242,
      "step": 6526
    },
    {
      "epoch": 0.917615633347392,
      "grad_norm": 1.7880427837371826,
      "learning_rate": 0.00015897019300872,
      "loss": 0.9739,
      "step": 6527
    },
    {
      "epoch": 0.9177562210037958,
      "grad_norm": 1.5193064212799072,
      "learning_rate": 0.0001587822866010364,
      "loss": 1.2804,
      "step": 6528
    },
    {
      "epoch": 0.9178968086601996,
      "grad_norm": 1.4684398174285889,
      "learning_rate": 0.00015859406252189464,
      "loss": 1.1127,
      "step": 6529
    },
    {
      "epoch": 0.9180373963166034,
      "grad_norm": 1.530775785446167,
      "learning_rate": 0.00015840552178849598,
      "loss": 1.1877,
      "step": 6530
    },
    {
      "epoch": 0.9181779839730072,
      "grad_norm": 1.643715739250183,
      "learning_rate": 0.000158216665419753,
      "loss": 1.1144,
      "step": 6531
    },
    {
      "epoch": 0.9183185716294109,
      "grad_norm": 1.5237538814544678,
      "learning_rate": 0.00015802749443628413,
      "loss": 1.1303,
      "step": 6532
    },
    {
      "epoch": 0.9184591592858147,
      "grad_norm": 1.4913338422775269,
      "learning_rate": 0.0001578380098604075,
      "loss": 1.2558,
      "step": 6533
    },
    {
      "epoch": 0.9185997469422185,
      "grad_norm": 1.3497978448867798,
      "learning_rate": 0.000157648212716137,
      "loss": 1.1094,
      "step": 6534
    },
    {
      "epoch": 0.9187403345986223,
      "grad_norm": 1.5759609937667847,
      "learning_rate": 0.00015745810402917457,
      "loss": 1.0758,
      "step": 6535
    },
    {
      "epoch": 0.918880922255026,
      "grad_norm": 1.4091315269470215,
      "learning_rate": 0.00015726768482690647,
      "loss": 1.0612,
      "step": 6536
    },
    {
      "epoch": 0.9190215099114297,
      "grad_norm": 1.616421103477478,
      "learning_rate": 0.00015707695613839694,
      "loss": 1.187,
      "step": 6537
    },
    {
      "epoch": 0.9191620975678335,
      "grad_norm": 1.3584909439086914,
      "learning_rate": 0.00015688591899438263,
      "loss": 1.1635,
      "step": 6538
    },
    {
      "epoch": 0.9193026852242373,
      "grad_norm": 1.541855812072754,
      "learning_rate": 0.00015669457442726726,
      "loss": 1.1971,
      "step": 6539
    },
    {
      "epoch": 0.9194432728806411,
      "grad_norm": 1.5813138484954834,
      "learning_rate": 0.00015650292347111578,
      "loss": 1.1337,
      "step": 6540
    },
    {
      "epoch": 0.9195838605370449,
      "grad_norm": 1.5295896530151367,
      "learning_rate": 0.0001563109671616491,
      "loss": 1.0732,
      "step": 6541
    },
    {
      "epoch": 0.9197244481934486,
      "grad_norm": 1.4889997243881226,
      "learning_rate": 0.00015611870653623838,
      "loss": 1.1719,
      "step": 6542
    },
    {
      "epoch": 0.9198650358498524,
      "grad_norm": 1.660107970237732,
      "learning_rate": 0.00015592614263389888,
      "loss": 1.1107,
      "step": 6543
    },
    {
      "epoch": 0.9200056235062561,
      "grad_norm": 1.5469095706939697,
      "learning_rate": 0.00015573327649528524,
      "loss": 1.1196,
      "step": 6544
    },
    {
      "epoch": 0.9201462111626599,
      "grad_norm": 1.55607271194458,
      "learning_rate": 0.0001555401091626858,
      "loss": 1.0613,
      "step": 6545
    },
    {
      "epoch": 0.9202867988190637,
      "grad_norm": 1.8101212978363037,
      "learning_rate": 0.00015534664168001574,
      "loss": 0.9151,
      "step": 6546
    },
    {
      "epoch": 0.9204273864754674,
      "grad_norm": 1.3947913646697998,
      "learning_rate": 0.00015515287509281295,
      "loss": 1.1339,
      "step": 6547
    },
    {
      "epoch": 0.9205679741318712,
      "grad_norm": 1.6096855401992798,
      "learning_rate": 0.00015495881044823154,
      "loss": 1.1346,
      "step": 6548
    },
    {
      "epoch": 0.920708561788275,
      "grad_norm": 1.7149146795272827,
      "learning_rate": 0.00015476444879503642,
      "loss": 1.1264,
      "step": 6549
    },
    {
      "epoch": 0.9208491494446788,
      "grad_norm": 1.4933573007583618,
      "learning_rate": 0.00015456979118359758,
      "loss": 1.2065,
      "step": 6550
    },
    {
      "epoch": 0.9209897371010826,
      "grad_norm": 1.7156918048858643,
      "learning_rate": 0.00015437483866588452,
      "loss": 1.0646,
      "step": 6551
    },
    {
      "epoch": 0.9211303247574862,
      "grad_norm": 1.5422724485397339,
      "learning_rate": 0.00015417959229546005,
      "loss": 1.1819,
      "step": 6552
    },
    {
      "epoch": 0.92127091241389,
      "grad_norm": 1.5522266626358032,
      "learning_rate": 0.00015398405312747583,
      "loss": 1.1185,
      "step": 6553
    },
    {
      "epoch": 0.9214115000702938,
      "grad_norm": 1.4369760751724243,
      "learning_rate": 0.00015378822221866502,
      "loss": 1.0613,
      "step": 6554
    },
    {
      "epoch": 0.9215520877266976,
      "grad_norm": 1.5522005558013916,
      "learning_rate": 0.0001535921006273379,
      "loss": 1.1211,
      "step": 6555
    },
    {
      "epoch": 0.9216926753831014,
      "grad_norm": 1.336404800415039,
      "learning_rate": 0.00015339568941337545,
      "loss": 1.0075,
      "step": 6556
    },
    {
      "epoch": 0.9218332630395051,
      "grad_norm": 1.903290867805481,
      "learning_rate": 0.000153198989638224,
      "loss": 1.197,
      "step": 6557
    },
    {
      "epoch": 0.9219738506959089,
      "grad_norm": 1.6431081295013428,
      "learning_rate": 0.00015300200236488916,
      "loss": 1.2515,
      "step": 6558
    },
    {
      "epoch": 0.9221144383523127,
      "grad_norm": 1.5804325342178345,
      "learning_rate": 0.00015280472865793034,
      "loss": 1.1575,
      "step": 6559
    },
    {
      "epoch": 0.9222550260087164,
      "grad_norm": 1.5077961683273315,
      "learning_rate": 0.00015260716958345483,
      "loss": 1.24,
      "step": 6560
    },
    {
      "epoch": 0.9223956136651202,
      "grad_norm": 1.7798374891281128,
      "learning_rate": 0.00015240932620911237,
      "loss": 1.1609,
      "step": 6561
    },
    {
      "epoch": 0.9225362013215239,
      "grad_norm": 1.3063669204711914,
      "learning_rate": 0.00015221119960408846,
      "loss": 1.213,
      "step": 6562
    },
    {
      "epoch": 0.9226767889779277,
      "grad_norm": 1.4664530754089355,
      "learning_rate": 0.00015201279083909975,
      "loss": 1.2487,
      "step": 6563
    },
    {
      "epoch": 0.9228173766343315,
      "grad_norm": 1.3663679361343384,
      "learning_rate": 0.0001518141009863881,
      "loss": 1.1797,
      "step": 6564
    },
    {
      "epoch": 0.9229579642907353,
      "grad_norm": 1.561635136604309,
      "learning_rate": 0.00015161513111971344,
      "loss": 1.0801,
      "step": 6565
    },
    {
      "epoch": 0.9230985519471391,
      "grad_norm": 1.457321047782898,
      "learning_rate": 0.0001514158823143497,
      "loss": 1.1099,
      "step": 6566
    },
    {
      "epoch": 0.9232391396035428,
      "grad_norm": 1.4330271482467651,
      "learning_rate": 0.000151216355647078,
      "loss": 1.0556,
      "step": 6567
    },
    {
      "epoch": 0.9233797272599465,
      "grad_norm": 1.6265289783477783,
      "learning_rate": 0.00015101655219618116,
      "loss": 0.9606,
      "step": 6568
    },
    {
      "epoch": 0.9235203149163503,
      "grad_norm": 1.5503175258636475,
      "learning_rate": 0.00015081647304143788,
      "loss": 1.0722,
      "step": 6569
    },
    {
      "epoch": 0.9236609025727541,
      "grad_norm": 1.3504834175109863,
      "learning_rate": 0.00015061611926411641,
      "loss": 1.1927,
      "step": 6570
    },
    {
      "epoch": 0.9238014902291579,
      "grad_norm": 1.5498241186141968,
      "learning_rate": 0.00015041549194696934,
      "loss": 1.2345,
      "step": 6571
    },
    {
      "epoch": 0.9239420778855616,
      "grad_norm": 1.5600383281707764,
      "learning_rate": 0.00015021459217422813,
      "loss": 1.308,
      "step": 6572
    },
    {
      "epoch": 0.9240826655419654,
      "grad_norm": 1.4596388339996338,
      "learning_rate": 0.00015001342103159555,
      "loss": 1.0649,
      "step": 6573
    },
    {
      "epoch": 0.9242232531983692,
      "grad_norm": 1.344085931777954,
      "learning_rate": 0.00014981197960624168,
      "loss": 1.2349,
      "step": 6574
    },
    {
      "epoch": 0.924363840854773,
      "grad_norm": 1.69893479347229,
      "learning_rate": 0.00014961026898679706,
      "loss": 1.1538,
      "step": 6575
    },
    {
      "epoch": 0.9245044285111768,
      "grad_norm": 1.6587278842926025,
      "learning_rate": 0.0001494082902633469,
      "loss": 1.157,
      "step": 6576
    },
    {
      "epoch": 0.9246450161675804,
      "grad_norm": 1.5773556232452393,
      "learning_rate": 0.00014920604452742565,
      "loss": 1.0948,
      "step": 6577
    },
    {
      "epoch": 0.9247856038239842,
      "grad_norm": 1.5464980602264404,
      "learning_rate": 0.00014900353287201006,
      "loss": 1.0033,
      "step": 6578
    },
    {
      "epoch": 0.924926191480388,
      "grad_norm": 1.3299825191497803,
      "learning_rate": 0.0001488007563915145,
      "loss": 1.0703,
      "step": 6579
    },
    {
      "epoch": 0.9250667791367918,
      "grad_norm": 1.4213758707046509,
      "learning_rate": 0.00014859771618178487,
      "loss": 1.1262,
      "step": 6580
    },
    {
      "epoch": 0.9252073667931956,
      "grad_norm": 1.4658987522125244,
      "learning_rate": 0.00014839441334009128,
      "loss": 1.0986,
      "step": 6581
    },
    {
      "epoch": 0.9253479544495993,
      "grad_norm": 1.4134169816970825,
      "learning_rate": 0.00014819084896512382,
      "loss": 1.1078,
      "step": 6582
    },
    {
      "epoch": 0.9254885421060031,
      "grad_norm": 1.6320602893829346,
      "learning_rate": 0.00014798702415698626,
      "loss": 1.1775,
      "step": 6583
    },
    {
      "epoch": 0.9256291297624069,
      "grad_norm": 1.6916264295578003,
      "learning_rate": 0.000147782940017189,
      "loss": 1.2929,
      "step": 6584
    },
    {
      "epoch": 0.9257697174188106,
      "grad_norm": 1.5852468013763428,
      "learning_rate": 0.00014757859764864465,
      "loss": 1.105,
      "step": 6585
    },
    {
      "epoch": 0.9259103050752144,
      "grad_norm": 1.8864388465881348,
      "learning_rate": 0.00014737399815566046,
      "loss": 1.1493,
      "step": 6586
    },
    {
      "epoch": 0.9260508927316181,
      "grad_norm": 1.5588845014572144,
      "learning_rate": 0.00014716914264393434,
      "loss": 1.2515,
      "step": 6587
    },
    {
      "epoch": 0.9261914803880219,
      "grad_norm": 1.6230597496032715,
      "learning_rate": 0.00014696403222054725,
      "loss": 1.129,
      "step": 6588
    },
    {
      "epoch": 0.9263320680444257,
      "grad_norm": 1.2616040706634521,
      "learning_rate": 0.00014675866799395736,
      "loss": 1.0251,
      "step": 6589
    },
    {
      "epoch": 0.9264726557008295,
      "grad_norm": 1.3786712884902954,
      "learning_rate": 0.00014655305107399478,
      "loss": 1.3065,
      "step": 6590
    },
    {
      "epoch": 0.9266132433572333,
      "grad_norm": 1.3810410499572754,
      "learning_rate": 0.0001463471825718558,
      "loss": 1.1031,
      "step": 6591
    },
    {
      "epoch": 0.926753831013637,
      "grad_norm": 1.3754889965057373,
      "learning_rate": 0.00014614106360009527,
      "loss": 1.1832,
      "step": 6592
    },
    {
      "epoch": 0.9268944186700407,
      "grad_norm": 1.7965134382247925,
      "learning_rate": 0.00014593469527262226,
      "loss": 1.0986,
      "step": 6593
    },
    {
      "epoch": 0.9270350063264445,
      "grad_norm": 1.4691957235336304,
      "learning_rate": 0.00014572807870469331,
      "loss": 1.396,
      "step": 6594
    },
    {
      "epoch": 0.9271755939828483,
      "grad_norm": 1.400891900062561,
      "learning_rate": 0.00014552121501290645,
      "loss": 1.1136,
      "step": 6595
    },
    {
      "epoch": 0.9273161816392521,
      "grad_norm": 1.7431964874267578,
      "learning_rate": 0.0001453141053151954,
      "loss": 0.9416,
      "step": 6596
    },
    {
      "epoch": 0.9274567692956558,
      "grad_norm": 1.6872656345367432,
      "learning_rate": 0.00014510675073082285,
      "loss": 1.0275,
      "step": 6597
    },
    {
      "epoch": 0.9275973569520596,
      "grad_norm": 1.9617735147476196,
      "learning_rate": 0.00014489915238037523,
      "loss": 1.2307,
      "step": 6598
    },
    {
      "epoch": 0.9277379446084634,
      "grad_norm": 1.7294666767120361,
      "learning_rate": 0.00014469131138575687,
      "loss": 1.26,
      "step": 6599
    },
    {
      "epoch": 0.9278785322648672,
      "grad_norm": 1.4973591566085815,
      "learning_rate": 0.00014448322887018232,
      "loss": 1.0628,
      "step": 6600
    },
    {
      "epoch": 0.928019119921271,
      "grad_norm": 1.625969409942627,
      "learning_rate": 0.00014427490595817193,
      "loss": 1.1354,
      "step": 6601
    },
    {
      "epoch": 0.9281597075776746,
      "grad_norm": 1.6516152620315552,
      "learning_rate": 0.00014406634377554572,
      "loss": 1.1306,
      "step": 6602
    },
    {
      "epoch": 0.9283002952340784,
      "grad_norm": 1.5144785642623901,
      "learning_rate": 0.00014385754344941565,
      "loss": 1.0695,
      "step": 6603
    },
    {
      "epoch": 0.9284408828904822,
      "grad_norm": 1.4733822345733643,
      "learning_rate": 0.00014364850610818164,
      "loss": 1.2314,
      "step": 6604
    },
    {
      "epoch": 0.928581470546886,
      "grad_norm": 1.5547142028808594,
      "learning_rate": 0.00014343923288152356,
      "loss": 1.0417,
      "step": 6605
    },
    {
      "epoch": 0.9287220582032898,
      "grad_norm": 1.5468695163726807,
      "learning_rate": 0.0001432297249003971,
      "loss": 1.069,
      "step": 6606
    },
    {
      "epoch": 0.9288626458596935,
      "grad_norm": 1.4928470849990845,
      "learning_rate": 0.00014301998329702609,
      "loss": 1.0303,
      "step": 6607
    },
    {
      "epoch": 0.9290032335160973,
      "grad_norm": 1.7649849653244019,
      "learning_rate": 0.00014281000920489655,
      "loss": 0.9794,
      "step": 6608
    },
    {
      "epoch": 0.929143821172501,
      "grad_norm": 1.5291699171066284,
      "learning_rate": 0.00014259980375875132,
      "loss": 1.2354,
      "step": 6609
    },
    {
      "epoch": 0.9292844088289048,
      "grad_norm": 1.7141178846359253,
      "learning_rate": 0.00014238936809458393,
      "loss": 1.2122,
      "step": 6610
    },
    {
      "epoch": 0.9294249964853086,
      "grad_norm": 1.5573865175247192,
      "learning_rate": 0.00014217870334963113,
      "loss": 1.1228,
      "step": 6611
    },
    {
      "epoch": 0.9295655841417123,
      "grad_norm": 1.747048258781433,
      "learning_rate": 0.00014196781066236843,
      "loss": 0.9781,
      "step": 6612
    },
    {
      "epoch": 0.9297061717981161,
      "grad_norm": 1.5511046648025513,
      "learning_rate": 0.00014175669117250247,
      "loss": 0.991,
      "step": 6613
    },
    {
      "epoch": 0.9298467594545199,
      "grad_norm": 1.3327819108963013,
      "learning_rate": 0.00014154534602096646,
      "loss": 1.1789,
      "step": 6614
    },
    {
      "epoch": 0.9299873471109237,
      "grad_norm": 1.430248737335205,
      "learning_rate": 0.00014133377634991266,
      "loss": 1.205,
      "step": 6615
    },
    {
      "epoch": 0.9301279347673275,
      "grad_norm": 1.5649933815002441,
      "learning_rate": 0.0001411219833027064,
      "loss": 1.1923,
      "step": 6616
    },
    {
      "epoch": 0.9302685224237311,
      "grad_norm": 1.447715401649475,
      "learning_rate": 0.00014090996802392045,
      "loss": 1.1548,
      "step": 6617
    },
    {
      "epoch": 0.9304091100801349,
      "grad_norm": 1.311497449874878,
      "learning_rate": 0.00014069773165932908,
      "loss": 1.1417,
      "step": 6618
    },
    {
      "epoch": 0.9305496977365387,
      "grad_norm": 1.5603049993515015,
      "learning_rate": 0.00014048527535590041,
      "loss": 1.1132,
      "step": 6619
    },
    {
      "epoch": 0.9306902853929425,
      "grad_norm": 1.2894965410232544,
      "learning_rate": 0.00014027260026179176,
      "loss": 1.1549,
      "step": 6620
    },
    {
      "epoch": 0.9308308730493463,
      "grad_norm": 1.4634639024734497,
      "learning_rate": 0.00014005970752634266,
      "loss": 1.0684,
      "step": 6621
    },
    {
      "epoch": 0.93097146070575,
      "grad_norm": 1.6356134414672852,
      "learning_rate": 0.0001398465983000689,
      "loss": 1.2746,
      "step": 6622
    },
    {
      "epoch": 0.9311120483621538,
      "grad_norm": 1.5023365020751953,
      "learning_rate": 0.00013963327373465624,
      "loss": 1.0939,
      "step": 6623
    },
    {
      "epoch": 0.9312526360185576,
      "grad_norm": 1.3733617067337036,
      "learning_rate": 0.0001394197349829537,
      "loss": 1.1134,
      "step": 6624
    },
    {
      "epoch": 0.9313932236749614,
      "grad_norm": 1.787007212638855,
      "learning_rate": 0.0001392059831989687,
      "loss": 1.3154,
      "step": 6625
    },
    {
      "epoch": 0.9315338113313651,
      "grad_norm": 1.5656317472457886,
      "learning_rate": 0.00013899201953785945,
      "loss": 1.1828,
      "step": 6626
    },
    {
      "epoch": 0.9316743989877688,
      "grad_norm": 1.618103265762329,
      "learning_rate": 0.00013877784515592886,
      "loss": 1.0501,
      "step": 6627
    },
    {
      "epoch": 0.9318149866441726,
      "grad_norm": 1.372018814086914,
      "learning_rate": 0.00013856346121061893,
      "loss": 1.0461,
      "step": 6628
    },
    {
      "epoch": 0.9319555743005764,
      "grad_norm": 1.6968333721160889,
      "learning_rate": 0.00013834886886050467,
      "loss": 1.0995,
      "step": 6629
    },
    {
      "epoch": 0.9320961619569802,
      "grad_norm": 1.5547258853912354,
      "learning_rate": 0.00013813406926528642,
      "loss": 1.208,
      "step": 6630
    },
    {
      "epoch": 0.932236749613384,
      "grad_norm": 1.3797279596328735,
      "learning_rate": 0.0001379190635857853,
      "loss": 1.2998,
      "step": 6631
    },
    {
      "epoch": 0.9323773372697877,
      "grad_norm": 1.4869781732559204,
      "learning_rate": 0.00013770385298393534,
      "loss": 0.9379,
      "step": 6632
    },
    {
      "epoch": 0.9325179249261915,
      "grad_norm": 1.2871925830841064,
      "learning_rate": 0.00013748843862277895,
      "loss": 1.2386,
      "step": 6633
    },
    {
      "epoch": 0.9326585125825952,
      "grad_norm": 1.5122178792953491,
      "learning_rate": 0.00013727282166645931,
      "loss": 1.1996,
      "step": 6634
    },
    {
      "epoch": 0.932799100238999,
      "grad_norm": 1.6147407293319702,
      "learning_rate": 0.00013705700328021402,
      "loss": 0.9634,
      "step": 6635
    },
    {
      "epoch": 0.9329396878954028,
      "grad_norm": 1.6249438524246216,
      "learning_rate": 0.00013684098463036964,
      "loss": 1.0607,
      "step": 6636
    },
    {
      "epoch": 0.9330802755518065,
      "grad_norm": 1.4856665134429932,
      "learning_rate": 0.00013662476688433548,
      "loss": 1.0575,
      "step": 6637
    },
    {
      "epoch": 0.9332208632082103,
      "grad_norm": 1.5484623908996582,
      "learning_rate": 0.00013640835121059584,
      "loss": 1.2473,
      "step": 6638
    },
    {
      "epoch": 0.9333614508646141,
      "grad_norm": 1.6568489074707031,
      "learning_rate": 0.00013619173877870523,
      "loss": 1.1483,
      "step": 6639
    },
    {
      "epoch": 0.9335020385210179,
      "grad_norm": 1.4713611602783203,
      "learning_rate": 0.00013597493075928147,
      "loss": 1.1147,
      "step": 6640
    },
    {
      "epoch": 0.9336426261774217,
      "grad_norm": 1.565947413444519,
      "learning_rate": 0.00013575792832399924,
      "loss": 1.0538,
      "step": 6641
    },
    {
      "epoch": 0.9337832138338253,
      "grad_norm": 1.6453301906585693,
      "learning_rate": 0.00013554073264558406,
      "loss": 1.0557,
      "step": 6642
    },
    {
      "epoch": 0.9339238014902291,
      "grad_norm": 1.94373619556427,
      "learning_rate": 0.00013532334489780524,
      "loss": 1.0798,
      "step": 6643
    },
    {
      "epoch": 0.9340643891466329,
      "grad_norm": 1.5408059358596802,
      "learning_rate": 0.00013510576625547094,
      "loss": 1.0819,
      "step": 6644
    },
    {
      "epoch": 0.9342049768030367,
      "grad_norm": 2.3572540283203125,
      "learning_rate": 0.00013488799789442043,
      "loss": 1.1222,
      "step": 6645
    },
    {
      "epoch": 0.9343455644594405,
      "grad_norm": 1.3312740325927734,
      "learning_rate": 0.00013467004099151808,
      "loss": 1.2408,
      "step": 6646
    },
    {
      "epoch": 0.9344861521158442,
      "grad_norm": 1.5414916276931763,
      "learning_rate": 0.0001344518967246475,
      "loss": 1.0494,
      "step": 6647
    },
    {
      "epoch": 0.934626739772248,
      "grad_norm": 1.6014783382415771,
      "learning_rate": 0.00013423356627270485,
      "loss": 1.0584,
      "step": 6648
    },
    {
      "epoch": 0.9347673274286518,
      "grad_norm": 1.4233450889587402,
      "learning_rate": 0.00013401505081559234,
      "loss": 1.0724,
      "step": 6649
    },
    {
      "epoch": 0.9349079150850556,
      "grad_norm": 1.4477014541625977,
      "learning_rate": 0.00013379635153421224,
      "loss": 1.1561,
      "step": 6650
    },
    {
      "epoch": 0.9350485027414593,
      "grad_norm": 1.6945290565490723,
      "learning_rate": 0.00013357746961045953,
      "loss": 1.1847,
      "step": 6651
    },
    {
      "epoch": 0.935189090397863,
      "grad_norm": 1.5527747869491577,
      "learning_rate": 0.0001333584062272172,
      "loss": 1.1605,
      "step": 6652
    },
    {
      "epoch": 0.9353296780542668,
      "grad_norm": 1.4748785495758057,
      "learning_rate": 0.00013313916256834849,
      "loss": 0.9056,
      "step": 6653
    },
    {
      "epoch": 0.9354702657106706,
      "grad_norm": 1.4621069431304932,
      "learning_rate": 0.00013291973981869045,
      "loss": 1.0786,
      "step": 6654
    },
    {
      "epoch": 0.9356108533670744,
      "grad_norm": 1.5380610227584839,
      "learning_rate": 0.00013270013916404853,
      "loss": 1.1562,
      "step": 6655
    },
    {
      "epoch": 0.9357514410234782,
      "grad_norm": 1.720798373222351,
      "learning_rate": 0.00013248036179118938,
      "loss": 1.1845,
      "step": 6656
    },
    {
      "epoch": 0.9358920286798819,
      "grad_norm": 2.17158842086792,
      "learning_rate": 0.0001322604088878348,
      "loss": 1.1571,
      "step": 6657
    },
    {
      "epoch": 0.9360326163362856,
      "grad_norm": 1.8797101974487305,
      "learning_rate": 0.00013204028164265504,
      "loss": 1.1654,
      "step": 6658
    },
    {
      "epoch": 0.9361732039926894,
      "grad_norm": 1.5245221853256226,
      "learning_rate": 0.00013181998124526265,
      "loss": 1.2828,
      "step": 6659
    },
    {
      "epoch": 0.9363137916490932,
      "grad_norm": 1.5401314496994019,
      "learning_rate": 0.00013159950888620584,
      "loss": 1.1151,
      "step": 6660
    },
    {
      "epoch": 0.936454379305497,
      "grad_norm": 1.884827971458435,
      "learning_rate": 0.00013137886575696238,
      "loss": 1.0169,
      "step": 6661
    },
    {
      "epoch": 0.9365949669619007,
      "grad_norm": 1.424190878868103,
      "learning_rate": 0.00013115805304993218,
      "loss": 1.2529,
      "step": 6662
    },
    {
      "epoch": 0.9367355546183045,
      "grad_norm": 1.7138316631317139,
      "learning_rate": 0.0001309370719584328,
      "loss": 1.0168,
      "step": 6663
    },
    {
      "epoch": 0.9368761422747083,
      "grad_norm": 1.3895223140716553,
      "learning_rate": 0.0001307159236766906,
      "loss": 1.2038,
      "step": 6664
    },
    {
      "epoch": 0.9370167299311121,
      "grad_norm": 1.601276159286499,
      "learning_rate": 0.00013049460939983614,
      "loss": 1.3602,
      "step": 6665
    },
    {
      "epoch": 0.9371573175875159,
      "grad_norm": 1.5713335275650024,
      "learning_rate": 0.0001302731303238969,
      "loss": 1.0731,
      "step": 6666
    },
    {
      "epoch": 0.9372979052439195,
      "grad_norm": 1.5697047710418701,
      "learning_rate": 0.00013005148764579086,
      "loss": 1.19,
      "step": 6667
    },
    {
      "epoch": 0.9374384929003233,
      "grad_norm": 1.6842886209487915,
      "learning_rate": 0.00012982968256332032,
      "loss": 0.8614,
      "step": 6668
    },
    {
      "epoch": 0.9375790805567271,
      "grad_norm": 1.305751919746399,
      "learning_rate": 0.00012960771627516533,
      "loss": 1.1623,
      "step": 6669
    },
    {
      "epoch": 0.9377196682131309,
      "grad_norm": 1.5143744945526123,
      "learning_rate": 0.0001293855899808764,
      "loss": 1.0051,
      "step": 6670
    },
    {
      "epoch": 0.9378602558695347,
      "grad_norm": 1.5461031198501587,
      "learning_rate": 0.00012916330488086983,
      "loss": 1.2341,
      "step": 6671
    },
    {
      "epoch": 0.9380008435259384,
      "grad_norm": 1.3624789714813232,
      "learning_rate": 0.00012894086217641958,
      "loss": 1.0155,
      "step": 6672
    },
    {
      "epoch": 0.9381414311823422,
      "grad_norm": 1.578914761543274,
      "learning_rate": 0.00012871826306965107,
      "loss": 1.1574,
      "step": 6673
    },
    {
      "epoch": 0.938282018838746,
      "grad_norm": 1.7654705047607422,
      "learning_rate": 0.00012849550876353538,
      "loss": 0.9921,
      "step": 6674
    },
    {
      "epoch": 0.9384226064951497,
      "grad_norm": 1.5753889083862305,
      "learning_rate": 0.0001282726004618822,
      "loss": 0.9205,
      "step": 6675
    },
    {
      "epoch": 0.9385631941515535,
      "grad_norm": 1.5041799545288086,
      "learning_rate": 0.0001280495393693334,
      "loss": 1.1751,
      "step": 6676
    },
    {
      "epoch": 0.9387037818079572,
      "grad_norm": 1.7249274253845215,
      "learning_rate": 0.00012782632669135666,
      "loss": 1.0899,
      "step": 6677
    },
    {
      "epoch": 0.938844369464361,
      "grad_norm": 1.395675539970398,
      "learning_rate": 0.00012760296363423872,
      "loss": 1.1737,
      "step": 6678
    },
    {
      "epoch": 0.9389849571207648,
      "grad_norm": 1.6182572841644287,
      "learning_rate": 0.0001273794514050791,
      "loss": 1.1992,
      "step": 6679
    },
    {
      "epoch": 0.9391255447771686,
      "grad_norm": 1.5974676609039307,
      "learning_rate": 0.00012715579121178356,
      "loss": 1.1035,
      "step": 6680
    },
    {
      "epoch": 0.9392661324335724,
      "grad_norm": 1.3990097045898438,
      "learning_rate": 0.00012693198426305693,
      "loss": 1.1031,
      "step": 6681
    },
    {
      "epoch": 0.939406720089976,
      "grad_norm": 1.401865005493164,
      "learning_rate": 0.00012670803176839822,
      "loss": 1.1418,
      "step": 6682
    },
    {
      "epoch": 0.9395473077463798,
      "grad_norm": 1.7556618452072144,
      "learning_rate": 0.00012648393493809189,
      "loss": 1.2828,
      "step": 6683
    },
    {
      "epoch": 0.9396878954027836,
      "grad_norm": 1.5242141485214233,
      "learning_rate": 0.000126259694983203,
      "loss": 1.1224,
      "step": 6684
    },
    {
      "epoch": 0.9398284830591874,
      "grad_norm": 1.6732149124145508,
      "learning_rate": 0.00012603531311557002,
      "loss": 1.1571,
      "step": 6685
    },
    {
      "epoch": 0.9399690707155912,
      "grad_norm": 1.3888534307479858,
      "learning_rate": 0.00012581079054779827,
      "loss": 1.1518,
      "step": 6686
    },
    {
      "epoch": 0.9401096583719949,
      "grad_norm": 1.739017367362976,
      "learning_rate": 0.00012558612849325349,
      "loss": 0.9912,
      "step": 6687
    },
    {
      "epoch": 0.9402502460283987,
      "grad_norm": 1.523190975189209,
      "learning_rate": 0.00012536132816605536,
      "loss": 1.0989,
      "step": 6688
    },
    {
      "epoch": 0.9403908336848025,
      "grad_norm": 1.4798834323883057,
      "learning_rate": 0.00012513639078107018,
      "loss": 1.1494,
      "step": 6689
    },
    {
      "epoch": 0.9405314213412063,
      "grad_norm": 1.617917537689209,
      "learning_rate": 0.00012491131755390611,
      "loss": 1.0681,
      "step": 6690
    },
    {
      "epoch": 0.94067200899761,
      "grad_norm": 1.6774353981018066,
      "learning_rate": 0.0001246861097009042,
      "loss": 1.157,
      "step": 6691
    },
    {
      "epoch": 0.9408125966540137,
      "grad_norm": 1.741906762123108,
      "learning_rate": 0.0001244607684391338,
      "loss": 0.9898,
      "step": 6692
    },
    {
      "epoch": 0.9409531843104175,
      "grad_norm": 1.9367271661758423,
      "learning_rate": 0.000124235294986385,
      "loss": 1.1331,
      "step": 6693
    },
    {
      "epoch": 0.9410937719668213,
      "grad_norm": 1.3999614715576172,
      "learning_rate": 0.0001240096905611623,
      "loss": 1.1235,
      "step": 6694
    },
    {
      "epoch": 0.9412343596232251,
      "grad_norm": 1.841403603553772,
      "learning_rate": 0.00012378395638267803,
      "loss": 1.0328,
      "step": 6695
    },
    {
      "epoch": 0.9413749472796289,
      "grad_norm": 1.751081109046936,
      "learning_rate": 0.00012355809367084564,
      "loss": 1.159,
      "step": 6696
    },
    {
      "epoch": 0.9415155349360326,
      "grad_norm": 1.5646120309829712,
      "learning_rate": 0.0001233321036462733,
      "loss": 1.1647,
      "step": 6697
    },
    {
      "epoch": 0.9416561225924364,
      "grad_norm": 1.5338282585144043,
      "learning_rate": 0.0001231059875302573,
      "loss": 1.134,
      "step": 6698
    },
    {
      "epoch": 0.9417967102488402,
      "grad_norm": 1.315643548965454,
      "learning_rate": 0.00012287974654477492,
      "loss": 1.0973,
      "step": 6699
    },
    {
      "epoch": 0.9419372979052439,
      "grad_norm": 1.6651527881622314,
      "learning_rate": 0.00012265338191247855,
      "loss": 1.1113,
      "step": 6700
    },
    {
      "epoch": 0.9420778855616477,
      "grad_norm": 1.8094213008880615,
      "learning_rate": 0.00012242689485668931,
      "loss": 1.062,
      "step": 6701
    },
    {
      "epoch": 0.9422184732180514,
      "grad_norm": 1.981522798538208,
      "learning_rate": 0.0001222002866013889,
      "loss": 1.1684,
      "step": 6702
    },
    {
      "epoch": 0.9423590608744552,
      "grad_norm": 1.2999273538589478,
      "learning_rate": 0.00012197355837121471,
      "loss": 1.2341,
      "step": 6703
    },
    {
      "epoch": 0.942499648530859,
      "grad_norm": 1.3006291389465332,
      "learning_rate": 0.00012174671139145227,
      "loss": 1.1893,
      "step": 6704
    },
    {
      "epoch": 0.9426402361872628,
      "grad_norm": 1.3505628108978271,
      "learning_rate": 0.00012151974688802892,
      "loss": 1.2378,
      "step": 6705
    },
    {
      "epoch": 0.9427808238436666,
      "grad_norm": 1.2462129592895508,
      "learning_rate": 0.0001212926660875071,
      "loss": 1.0818,
      "step": 6706
    },
    {
      "epoch": 0.9429214115000703,
      "grad_norm": 1.514076590538025,
      "learning_rate": 0.0001210654702170779,
      "loss": 1.1438,
      "step": 6707
    },
    {
      "epoch": 0.943061999156474,
      "grad_norm": 1.5616083145141602,
      "learning_rate": 0.00012083816050455362,
      "loss": 1.0652,
      "step": 6708
    },
    {
      "epoch": 0.9432025868128778,
      "grad_norm": 1.4772124290466309,
      "learning_rate": 0.00012061073817836293,
      "loss": 1.0475,
      "step": 6709
    },
    {
      "epoch": 0.9433431744692816,
      "grad_norm": 1.386083722114563,
      "learning_rate": 0.00012038320446754194,
      "loss": 1.1696,
      "step": 6710
    },
    {
      "epoch": 0.9434837621256854,
      "grad_norm": 1.3619487285614014,
      "learning_rate": 0.00012015556060172939,
      "loss": 0.9349,
      "step": 6711
    },
    {
      "epoch": 0.9436243497820891,
      "grad_norm": 1.3930156230926514,
      "learning_rate": 0.00011992780781115912,
      "loss": 0.9652,
      "step": 6712
    },
    {
      "epoch": 0.9437649374384929,
      "grad_norm": 1.3860338926315308,
      "learning_rate": 0.0001196999473266536,
      "loss": 1.0328,
      "step": 6713
    },
    {
      "epoch": 0.9439055250948967,
      "grad_norm": 1.63722825050354,
      "learning_rate": 0.00011947198037961735,
      "loss": 1.0565,
      "step": 6714
    },
    {
      "epoch": 0.9440461127513005,
      "grad_norm": 1.602644920349121,
      "learning_rate": 0.00011924390820203025,
      "loss": 1.1891,
      "step": 6715
    },
    {
      "epoch": 0.9441867004077042,
      "grad_norm": 1.686791181564331,
      "learning_rate": 0.00011901573202644079,
      "loss": 1.1316,
      "step": 6716
    },
    {
      "epoch": 0.9443272880641079,
      "grad_norm": 1.4192028045654297,
      "learning_rate": 0.00011878745308595973,
      "loss": 1.194,
      "step": 6717
    },
    {
      "epoch": 0.9444678757205117,
      "grad_norm": 1.519303560256958,
      "learning_rate": 0.00011855907261425265,
      "loss": 1.1193,
      "step": 6718
    },
    {
      "epoch": 0.9446084633769155,
      "grad_norm": 1.6665433645248413,
      "learning_rate": 0.00011833059184553417,
      "loss": 1.234,
      "step": 6719
    },
    {
      "epoch": 0.9447490510333193,
      "grad_norm": 1.4482470750808716,
      "learning_rate": 0.00011810201201456141,
      "loss": 1.0141,
      "step": 6720
    },
    {
      "epoch": 0.9448896386897231,
      "grad_norm": 1.4632521867752075,
      "learning_rate": 0.00011787333435662591,
      "loss": 1.2013,
      "step": 6721
    },
    {
      "epoch": 0.9450302263461268,
      "grad_norm": 1.4546656608581543,
      "learning_rate": 0.00011764456010754849,
      "loss": 1.1087,
      "step": 6722
    },
    {
      "epoch": 0.9451708140025306,
      "grad_norm": 1.361142873764038,
      "learning_rate": 0.0001174156905036718,
      "loss": 0.99,
      "step": 6723
    },
    {
      "epoch": 0.9453114016589343,
      "grad_norm": 1.5417817831039429,
      "learning_rate": 0.00011718672678185387,
      "loss": 1.0269,
      "step": 6724
    },
    {
      "epoch": 0.9454519893153381,
      "grad_norm": 1.8072704076766968,
      "learning_rate": 0.00011695767017946149,
      "loss": 1.0243,
      "step": 6725
    },
    {
      "epoch": 0.9455925769717419,
      "grad_norm": 1.928001046180725,
      "learning_rate": 0.00011672852193436283,
      "loss": 1.119,
      "step": 6726
    },
    {
      "epoch": 0.9457331646281456,
      "grad_norm": 1.8044129610061646,
      "learning_rate": 0.00011649928328492176,
      "loss": 1.1347,
      "step": 6727
    },
    {
      "epoch": 0.9458737522845494,
      "grad_norm": 1.4587112665176392,
      "learning_rate": 0.00011626995546999119,
      "loss": 1.1367,
      "step": 6728
    },
    {
      "epoch": 0.9460143399409532,
      "grad_norm": 1.3911974430084229,
      "learning_rate": 0.00011604053972890483,
      "loss": 1.0427,
      "step": 6729
    },
    {
      "epoch": 0.946154927597357,
      "grad_norm": 1.4361572265625,
      "learning_rate": 0.00011581103730147229,
      "loss": 1.1263,
      "step": 6730
    },
    {
      "epoch": 0.9462955152537608,
      "grad_norm": 1.9030529260635376,
      "learning_rate": 0.00011558144942797149,
      "loss": 1.1815,
      "step": 6731
    },
    {
      "epoch": 0.9464361029101644,
      "grad_norm": 1.3918246030807495,
      "learning_rate": 0.00011535177734914205,
      "loss": 1.1804,
      "step": 6732
    },
    {
      "epoch": 0.9465766905665682,
      "grad_norm": 1.457699179649353,
      "learning_rate": 0.00011512202230617888,
      "loss": 1.143,
      "step": 6733
    },
    {
      "epoch": 0.946717278222972,
      "grad_norm": 1.7398134469985962,
      "learning_rate": 0.00011489218554072456,
      "loss": 1.0719,
      "step": 6734
    },
    {
      "epoch": 0.9468578658793758,
      "grad_norm": 1.648774266242981,
      "learning_rate": 0.00011466226829486433,
      "loss": 1.0147,
      "step": 6735
    },
    {
      "epoch": 0.9469984535357796,
      "grad_norm": 1.5459599494934082,
      "learning_rate": 0.00011443227181111786,
      "loss": 0.982,
      "step": 6736
    },
    {
      "epoch": 0.9471390411921833,
      "grad_norm": 1.779218316078186,
      "learning_rate": 0.00011420219733243266,
      "loss": 1.1484,
      "step": 6737
    },
    {
      "epoch": 0.9472796288485871,
      "grad_norm": 1.5598056316375732,
      "learning_rate": 0.00011397204610217813,
      "loss": 1.0961,
      "step": 6738
    },
    {
      "epoch": 0.9474202165049909,
      "grad_norm": 1.3873672485351562,
      "learning_rate": 0.00011374181936413892,
      "loss": 1.1671,
      "step": 6739
    },
    {
      "epoch": 0.9475608041613947,
      "grad_norm": 1.6494487524032593,
      "learning_rate": 0.00011351151836250677,
      "loss": 1.2616,
      "step": 6740
    },
    {
      "epoch": 0.9477013918177984,
      "grad_norm": 1.636997938156128,
      "learning_rate": 0.00011328114434187538,
      "loss": 1.0333,
      "step": 6741
    },
    {
      "epoch": 0.9478419794742021,
      "grad_norm": 1.4334580898284912,
      "learning_rate": 0.00011305069854723291,
      "loss": 1.1627,
      "step": 6742
    },
    {
      "epoch": 0.9479825671306059,
      "grad_norm": 1.933579444885254,
      "learning_rate": 0.00011282018222395543,
      "loss": 1.1236,
      "step": 6743
    },
    {
      "epoch": 0.9481231547870097,
      "grad_norm": 1.56564462184906,
      "learning_rate": 0.00011258959661780029,
      "loss": 1.0975,
      "step": 6744
    },
    {
      "epoch": 0.9482637424434135,
      "grad_norm": 1.415169596672058,
      "learning_rate": 0.00011235894297489875,
      "loss": 1.1032,
      "step": 6745
    },
    {
      "epoch": 0.9484043300998173,
      "grad_norm": 1.733755111694336,
      "learning_rate": 0.0001121282225417501,
      "loss": 1.2372,
      "step": 6746
    },
    {
      "epoch": 0.948544917756221,
      "grad_norm": 1.534226894378662,
      "learning_rate": 0.00011189743656521519,
      "loss": 1.0915,
      "step": 6747
    },
    {
      "epoch": 0.9486855054126248,
      "grad_norm": 1.3613048791885376,
      "learning_rate": 0.00011166658629250784,
      "loss": 1.1283,
      "step": 6748
    },
    {
      "epoch": 0.9488260930690285,
      "grad_norm": 1.3693784475326538,
      "learning_rate": 0.00011143567297119024,
      "loss": 1.1792,
      "step": 6749
    },
    {
      "epoch": 0.9489666807254323,
      "grad_norm": 1.5162739753723145,
      "learning_rate": 0.00011120469784916498,
      "loss": 1.1664,
      "step": 6750
    },
    {
      "epoch": 0.9491072683818361,
      "grad_norm": 1.4609289169311523,
      "learning_rate": 0.00011097366217466876,
      "loss": 1.0354,
      "step": 6751
    },
    {
      "epoch": 0.9492478560382398,
      "grad_norm": 1.3869426250457764,
      "learning_rate": 0.00011074256719626559,
      "loss": 1.1956,
      "step": 6752
    },
    {
      "epoch": 0.9493884436946436,
      "grad_norm": 1.6166478395462036,
      "learning_rate": 0.0001105114141628395,
      "loss": 1.102,
      "step": 6753
    },
    {
      "epoch": 0.9495290313510474,
      "grad_norm": 1.5187437534332275,
      "learning_rate": 0.00011028020432358861,
      "loss": 1.1389,
      "step": 6754
    },
    {
      "epoch": 0.9496696190074512,
      "grad_norm": 1.6310714483261108,
      "learning_rate": 0.0001100489389280185,
      "loss": 1.1234,
      "step": 6755
    },
    {
      "epoch": 0.949810206663855,
      "grad_norm": 2.0720696449279785,
      "learning_rate": 0.00010981761922593403,
      "loss": 1.1317,
      "step": 6756
    },
    {
      "epoch": 0.9499507943202586,
      "grad_norm": 1.4006198644638062,
      "learning_rate": 0.00010958624646743399,
      "loss": 1.0419,
      "step": 6757
    },
    {
      "epoch": 0.9500913819766624,
      "grad_norm": 1.4844857454299927,
      "learning_rate": 0.00010935482190290444,
      "loss": 1.2115,
      "step": 6758
    },
    {
      "epoch": 0.9502319696330662,
      "grad_norm": 1.5137664079666138,
      "learning_rate": 0.00010912334678301043,
      "loss": 1.2168,
      "step": 6759
    },
    {
      "epoch": 0.95037255728947,
      "grad_norm": 1.4789561033248901,
      "learning_rate": 0.00010889182235869098,
      "loss": 1.1862,
      "step": 6760
    },
    {
      "epoch": 0.9505131449458738,
      "grad_norm": 1.3465934991836548,
      "learning_rate": 0.00010866024988115077,
      "loss": 0.9934,
      "step": 6761
    },
    {
      "epoch": 0.9506537326022775,
      "grad_norm": 1.4727259874343872,
      "learning_rate": 0.0001084286306018552,
      "loss": 0.9574,
      "step": 6762
    },
    {
      "epoch": 0.9507943202586813,
      "grad_norm": 1.6159754991531372,
      "learning_rate": 0.00010819696577252203,
      "loss": 1.0951,
      "step": 6763
    },
    {
      "epoch": 0.9509349079150851,
      "grad_norm": 1.6645151376724243,
      "learning_rate": 0.0001079652566451149,
      "loss": 1.2014,
      "step": 6764
    },
    {
      "epoch": 0.9510754955714888,
      "grad_norm": 1.8559683561325073,
      "learning_rate": 0.00010773350447183717,
      "loss": 1.1769,
      "step": 6765
    },
    {
      "epoch": 0.9512160832278926,
      "grad_norm": 1.3991363048553467,
      "learning_rate": 0.00010750171050512531,
      "loss": 1.1729,
      "step": 6766
    },
    {
      "epoch": 0.9513566708842963,
      "grad_norm": 1.3634390830993652,
      "learning_rate": 0.00010726987599764067,
      "loss": 0.9861,
      "step": 6767
    },
    {
      "epoch": 0.9514972585407001,
      "grad_norm": 1.6229690313339233,
      "learning_rate": 0.00010703800220226455,
      "loss": 1.0437,
      "step": 6768
    },
    {
      "epoch": 0.9516378461971039,
      "grad_norm": 1.5484777688980103,
      "learning_rate": 0.00010680609037208959,
      "loss": 0.799,
      "step": 6769
    },
    {
      "epoch": 0.9517784338535077,
      "grad_norm": 1.2131210565567017,
      "learning_rate": 0.00010657414176041523,
      "loss": 1.2553,
      "step": 6770
    },
    {
      "epoch": 0.9519190215099115,
      "grad_norm": 1.4492294788360596,
      "learning_rate": 0.00010634215762073896,
      "loss": 1.0768,
      "step": 6771
    },
    {
      "epoch": 0.9520596091663152,
      "grad_norm": 1.8306665420532227,
      "learning_rate": 0.00010611013920675001,
      "loss": 1.1515,
      "step": 6772
    },
    {
      "epoch": 0.952200196822719,
      "grad_norm": 1.5225064754486084,
      "learning_rate": 0.00010587808777232324,
      "loss": 1.0946,
      "step": 6773
    },
    {
      "epoch": 0.9523407844791227,
      "grad_norm": 1.6275482177734375,
      "learning_rate": 0.0001056460045715124,
      "loss": 1.0644,
      "step": 6774
    },
    {
      "epoch": 0.9524813721355265,
      "grad_norm": 1.4119380712509155,
      "learning_rate": 0.00010541389085854183,
      "loss": 1.2653,
      "step": 6775
    },
    {
      "epoch": 0.9526219597919303,
      "grad_norm": 1.3208789825439453,
      "learning_rate": 0.0001051817478878016,
      "loss": 1.0682,
      "step": 6776
    },
    {
      "epoch": 0.952762547448334,
      "grad_norm": 1.4160929918289185,
      "learning_rate": 0.0001049495769138396,
      "loss": 1.1407,
      "step": 6777
    },
    {
      "epoch": 0.9529031351047378,
      "grad_norm": 1.9084817171096802,
      "learning_rate": 0.0001047173791913551,
      "loss": 1.0365,
      "step": 6778
    },
    {
      "epoch": 0.9530437227611416,
      "grad_norm": 1.748084545135498,
      "learning_rate": 0.00010448515597519211,
      "loss": 1.0539,
      "step": 6779
    },
    {
      "epoch": 0.9531843104175454,
      "grad_norm": 1.683795690536499,
      "learning_rate": 0.00010425290852033167,
      "loss": 1.19,
      "step": 6780
    },
    {
      "epoch": 0.9533248980739492,
      "grad_norm": 1.513442873954773,
      "learning_rate": 0.00010402063808188691,
      "loss": 1.1002,
      "step": 6781
    },
    {
      "epoch": 0.9534654857303528,
      "grad_norm": 1.293067216873169,
      "learning_rate": 0.00010378834591509461,
      "loss": 1.0218,
      "step": 6782
    },
    {
      "epoch": 0.9536060733867566,
      "grad_norm": 1.3904697895050049,
      "learning_rate": 0.00010355603327530868,
      "loss": 1.0566,
      "step": 6783
    },
    {
      "epoch": 0.9537466610431604,
      "grad_norm": 1.8799470663070679,
      "learning_rate": 0.0001033237014179939,
      "loss": 1.1368,
      "step": 6784
    },
    {
      "epoch": 0.9538872486995642,
      "grad_norm": 1.3664745092391968,
      "learning_rate": 0.00010309135159871956,
      "loss": 1.1105,
      "step": 6785
    },
    {
      "epoch": 0.954027836355968,
      "grad_norm": 1.4892727136611938,
      "learning_rate": 0.00010285898507315092,
      "loss": 1.0465,
      "step": 6786
    },
    {
      "epoch": 0.9541684240123717,
      "grad_norm": 1.4610118865966797,
      "learning_rate": 0.00010262660309704437,
      "loss": 1.1689,
      "step": 6787
    },
    {
      "epoch": 0.9543090116687755,
      "grad_norm": 1.3492438793182373,
      "learning_rate": 0.00010239420692623898,
      "loss": 1.3101,
      "step": 6788
    },
    {
      "epoch": 0.9544495993251793,
      "grad_norm": 1.3992981910705566,
      "learning_rate": 0.00010216179781665152,
      "loss": 1.1512,
      "step": 6789
    },
    {
      "epoch": 0.954590186981583,
      "grad_norm": 1.6134124994277954,
      "learning_rate": 0.00010192937702426822,
      "loss": 1.1618,
      "step": 6790
    },
    {
      "epoch": 0.9547307746379868,
      "grad_norm": 1.699350118637085,
      "learning_rate": 0.00010169694580513802,
      "loss": 1.1291,
      "step": 6791
    },
    {
      "epoch": 0.9548713622943905,
      "grad_norm": 1.4614404439926147,
      "learning_rate": 0.00010146450541536665,
      "loss": 1.2838,
      "step": 6792
    },
    {
      "epoch": 0.9550119499507943,
      "grad_norm": 1.4564318656921387,
      "learning_rate": 0.00010123205711110986,
      "loss": 0.9897,
      "step": 6793
    },
    {
      "epoch": 0.9551525376071981,
      "grad_norm": 1.5704106092453003,
      "learning_rate": 0.00010099960214856511,
      "loss": 1.1676,
      "step": 6794
    },
    {
      "epoch": 0.9552931252636019,
      "grad_norm": 1.5085735321044922,
      "learning_rate": 0.00010076714178396664,
      "loss": 1.1776,
      "step": 6795
    },
    {
      "epoch": 0.9554337129200057,
      "grad_norm": 1.6291038990020752,
      "learning_rate": 0.00010053467727357759,
      "loss": 1.0049,
      "step": 6796
    },
    {
      "epoch": 0.9555743005764094,
      "grad_norm": 1.2386654615402222,
      "learning_rate": 0.00010030220987368357,
      "loss": 1.1165,
      "step": 6797
    },
    {
      "epoch": 0.9557148882328131,
      "grad_norm": 1.6076996326446533,
      "learning_rate": 0.000100069740840586,
      "loss": 1.1212,
      "step": 6798
    },
    {
      "epoch": 0.9558554758892169,
      "grad_norm": 1.4445931911468506,
      "learning_rate": 9.983727143059432e-05,
      "loss": 1.2868,
      "step": 6799
    },
    {
      "epoch": 0.9559960635456207,
      "grad_norm": 1.400575041770935,
      "learning_rate": 9.96048029000212e-05,
      "loss": 1.0049,
      "step": 6800
    },
    {
      "epoch": 0.9561366512020245,
      "grad_norm": 1.451474905014038,
      "learning_rate": 9.937233650517393e-05,
      "loss": 1.1804,
      "step": 6801
    },
    {
      "epoch": 0.9562772388584282,
      "grad_norm": 1.3411791324615479,
      "learning_rate": 9.913987350234794e-05,
      "loss": 0.939,
      "step": 6802
    },
    {
      "epoch": 0.956417826514832,
      "grad_norm": 1.490064024925232,
      "learning_rate": 9.890741514782086e-05,
      "loss": 1.0484,
      "step": 6803
    },
    {
      "epoch": 0.9565584141712358,
      "grad_norm": 1.6005958318710327,
      "learning_rate": 9.867496269784502e-05,
      "loss": 1.2615,
      "step": 6804
    },
    {
      "epoch": 0.9566990018276396,
      "grad_norm": 1.4992308616638184,
      "learning_rate": 9.844251740864084e-05,
      "loss": 1.1202,
      "step": 6805
    },
    {
      "epoch": 0.9568395894840434,
      "grad_norm": 1.7090251445770264,
      "learning_rate": 9.821008053639022e-05,
      "loss": 1.1586,
      "step": 6806
    },
    {
      "epoch": 0.956980177140447,
      "grad_norm": 1.6370277404785156,
      "learning_rate": 9.797765333722884e-05,
      "loss": 1.1541,
      "step": 6807
    },
    {
      "epoch": 0.9571207647968508,
      "grad_norm": 1.8294792175292969,
      "learning_rate": 9.774523706724124e-05,
      "loss": 1.2044,
      "step": 6808
    },
    {
      "epoch": 0.9572613524532546,
      "grad_norm": 1.8806610107421875,
      "learning_rate": 9.751283298245226e-05,
      "loss": 1.35,
      "step": 6809
    },
    {
      "epoch": 0.9574019401096584,
      "grad_norm": 1.533388376235962,
      "learning_rate": 9.728044233882063e-05,
      "loss": 1.0307,
      "step": 6810
    },
    {
      "epoch": 0.9575425277660622,
      "grad_norm": 1.2972668409347534,
      "learning_rate": 9.704806639223292e-05,
      "loss": 1.1311,
      "step": 6811
    },
    {
      "epoch": 0.9576831154224659,
      "grad_norm": 1.7908935546875,
      "learning_rate": 9.681570639849611e-05,
      "loss": 1.1479,
      "step": 6812
    },
    {
      "epoch": 0.9578237030788697,
      "grad_norm": 1.4883158206939697,
      "learning_rate": 9.658336361333095e-05,
      "loss": 1.2812,
      "step": 6813
    },
    {
      "epoch": 0.9579642907352734,
      "grad_norm": 1.5619202852249146,
      "learning_rate": 9.635103929236525e-05,
      "loss": 1.1112,
      "step": 6814
    },
    {
      "epoch": 0.9581048783916772,
      "grad_norm": 1.6798022985458374,
      "learning_rate": 9.611873469112695e-05,
      "loss": 1.0436,
      "step": 6815
    },
    {
      "epoch": 0.958245466048081,
      "grad_norm": 1.840875267982483,
      "learning_rate": 9.588645106503749e-05,
      "loss": 1.1965,
      "step": 6816
    },
    {
      "epoch": 0.9583860537044847,
      "grad_norm": 1.2950009107589722,
      "learning_rate": 9.565418966940509e-05,
      "loss": 1.1705,
      "step": 6817
    },
    {
      "epoch": 0.9585266413608885,
      "grad_norm": 1.6205487251281738,
      "learning_rate": 9.542195175941716e-05,
      "loss": 0.9348,
      "step": 6818
    },
    {
      "epoch": 0.9586672290172923,
      "grad_norm": 1.4711651802062988,
      "learning_rate": 9.518973859013522e-05,
      "loss": 1.0652,
      "step": 6819
    },
    {
      "epoch": 0.9588078166736961,
      "grad_norm": 1.694783329963684,
      "learning_rate": 9.495755141648659e-05,
      "loss": 1.1628,
      "step": 6820
    },
    {
      "epoch": 0.9589484043300999,
      "grad_norm": 1.9514949321746826,
      "learning_rate": 9.472539149325769e-05,
      "loss": 1.1664,
      "step": 6821
    },
    {
      "epoch": 0.9590889919865035,
      "grad_norm": 1.5163946151733398,
      "learning_rate": 9.44932600750883e-05,
      "loss": 1.2432,
      "step": 6822
    },
    {
      "epoch": 0.9592295796429073,
      "grad_norm": 1.4111179113388062,
      "learning_rate": 9.426115841646386e-05,
      "loss": 1.1603,
      "step": 6823
    },
    {
      "epoch": 0.9593701672993111,
      "grad_norm": 1.5625234842300415,
      "learning_rate": 9.402908777170905e-05,
      "loss": 1.1082,
      "step": 6824
    },
    {
      "epoch": 0.9595107549557149,
      "grad_norm": 1.6442183256149292,
      "learning_rate": 9.379704939498116e-05,
      "loss": 1.0317,
      "step": 6825
    },
    {
      "epoch": 0.9596513426121187,
      "grad_norm": 1.4944127798080444,
      "learning_rate": 9.35650445402623e-05,
      "loss": 1.0213,
      "step": 6826
    },
    {
      "epoch": 0.9597919302685224,
      "grad_norm": 1.3639795780181885,
      "learning_rate": 9.333307446135452e-05,
      "loss": 0.9715,
      "step": 6827
    },
    {
      "epoch": 0.9599325179249262,
      "grad_norm": 1.783484697341919,
      "learning_rate": 9.310114041187144e-05,
      "loss": 1.0495,
      "step": 6828
    },
    {
      "epoch": 0.96007310558133,
      "grad_norm": 1.6898469924926758,
      "learning_rate": 9.286924364523156e-05,
      "loss": 1.0872,
      "step": 6829
    },
    {
      "epoch": 0.9602136932377338,
      "grad_norm": 1.5557194948196411,
      "learning_rate": 9.263738541465245e-05,
      "loss": 1.218,
      "step": 6830
    },
    {
      "epoch": 0.9603542808941375,
      "grad_norm": 1.5620765686035156,
      "learning_rate": 9.240556697314326e-05,
      "loss": 0.8775,
      "step": 6831
    },
    {
      "epoch": 0.9604948685505412,
      "grad_norm": 1.8593417406082153,
      "learning_rate": 9.217378957349811e-05,
      "loss": 1.1138,
      "step": 6832
    },
    {
      "epoch": 0.960635456206945,
      "grad_norm": 1.6951953172683716,
      "learning_rate": 9.194205446828928e-05,
      "loss": 1.0636,
      "step": 6833
    },
    {
      "epoch": 0.9607760438633488,
      "grad_norm": 1.5085084438323975,
      "learning_rate": 9.171036290986053e-05,
      "loss": 1.0973,
      "step": 6834
    },
    {
      "epoch": 0.9609166315197526,
      "grad_norm": 1.6766443252563477,
      "learning_rate": 9.147871615032025e-05,
      "loss": 1.1821,
      "step": 6835
    },
    {
      "epoch": 0.9610572191761564,
      "grad_norm": 1.3871210813522339,
      "learning_rate": 9.124711544153494e-05,
      "loss": 0.9911,
      "step": 6836
    },
    {
      "epoch": 0.9611978068325601,
      "grad_norm": 1.5403169393539429,
      "learning_rate": 9.10155620351215e-05,
      "loss": 1.1222,
      "step": 6837
    },
    {
      "epoch": 0.9613383944889639,
      "grad_norm": 1.278633952140808,
      "learning_rate": 9.078405718244243e-05,
      "loss": 1.1022,
      "step": 6838
    },
    {
      "epoch": 0.9614789821453676,
      "grad_norm": 1.508164882659912,
      "learning_rate": 9.055260213459664e-05,
      "loss": 1.047,
      "step": 6839
    },
    {
      "epoch": 0.9616195698017714,
      "grad_norm": 1.6084500551223755,
      "learning_rate": 9.032119814241454e-05,
      "loss": 1.2206,
      "step": 6840
    },
    {
      "epoch": 0.9617601574581752,
      "grad_norm": 1.361277461051941,
      "learning_rate": 9.008984645645046e-05,
      "loss": 1.1275,
      "step": 6841
    },
    {
      "epoch": 0.9619007451145789,
      "grad_norm": 1.4482784271240234,
      "learning_rate": 8.985854832697609e-05,
      "loss": 1.028,
      "step": 6842
    },
    {
      "epoch": 0.9620413327709827,
      "grad_norm": 1.5694421529769897,
      "learning_rate": 8.962730500397366e-05,
      "loss": 1.2192,
      "step": 6843
    },
    {
      "epoch": 0.9621819204273865,
      "grad_norm": 1.5783997774124146,
      "learning_rate": 8.93961177371294e-05,
      "loss": 1.0761,
      "step": 6844
    },
    {
      "epoch": 0.9623225080837903,
      "grad_norm": 1.3813986778259277,
      "learning_rate": 8.916498777582587e-05,
      "loss": 1.0865,
      "step": 6845
    },
    {
      "epoch": 0.9624630957401941,
      "grad_norm": 1.3125511407852173,
      "learning_rate": 8.893391636913721e-05,
      "loss": 1.1816,
      "step": 6846
    },
    {
      "epoch": 0.9626036833965977,
      "grad_norm": 1.650430679321289,
      "learning_rate": 8.87029047658199e-05,
      "loss": 1.032,
      "step": 6847
    },
    {
      "epoch": 0.9627442710530015,
      "grad_norm": 1.5131962299346924,
      "learning_rate": 8.847195421430786e-05,
      "loss": 1.2179,
      "step": 6848
    },
    {
      "epoch": 0.9628848587094053,
      "grad_norm": 1.5433132648468018,
      "learning_rate": 8.824106596270497e-05,
      "loss": 1.2124,
      "step": 6849
    },
    {
      "epoch": 0.9630254463658091,
      "grad_norm": 1.490655541419983,
      "learning_rate": 8.801024125877841e-05,
      "loss": 1.0916,
      "step": 6850
    },
    {
      "epoch": 0.9631660340222129,
      "grad_norm": 1.5451295375823975,
      "learning_rate": 8.77794813499519e-05,
      "loss": 1.0943,
      "step": 6851
    },
    {
      "epoch": 0.9633066216786166,
      "grad_norm": 1.7356069087982178,
      "learning_rate": 8.754878748329904e-05,
      "loss": 1.0917,
      "step": 6852
    },
    {
      "epoch": 0.9634472093350204,
      "grad_norm": 1.3452706336975098,
      "learning_rate": 8.731816090553653e-05,
      "loss": 1.1508,
      "step": 6853
    },
    {
      "epoch": 0.9635877969914242,
      "grad_norm": 1.28617525100708,
      "learning_rate": 8.708760286301754e-05,
      "loss": 0.991,
      "step": 6854
    },
    {
      "epoch": 0.963728384647828,
      "grad_norm": 1.8137849569320679,
      "learning_rate": 8.685711460172439e-05,
      "loss": 0.9669,
      "step": 6855
    },
    {
      "epoch": 0.9638689723042317,
      "grad_norm": 1.9882409572601318,
      "learning_rate": 8.66266973672626e-05,
      "loss": 1.0583,
      "step": 6856
    },
    {
      "epoch": 0.9640095599606354,
      "grad_norm": 1.5536166429519653,
      "learning_rate": 8.639635240485446e-05,
      "loss": 1.1049,
      "step": 6857
    },
    {
      "epoch": 0.9641501476170392,
      "grad_norm": 1.3959935903549194,
      "learning_rate": 8.616608095933051e-05,
      "loss": 1.1548,
      "step": 6858
    },
    {
      "epoch": 0.964290735273443,
      "grad_norm": 1.6206402778625488,
      "learning_rate": 8.593588427512477e-05,
      "loss": 1.1107,
      "step": 6859
    },
    {
      "epoch": 0.9644313229298468,
      "grad_norm": 1.468593955039978,
      "learning_rate": 8.570576359626706e-05,
      "loss": 1.0581,
      "step": 6860
    },
    {
      "epoch": 0.9645719105862506,
      "grad_norm": 1.4406644105911255,
      "learning_rate": 8.547572016637642e-05,
      "loss": 1.0414,
      "step": 6861
    },
    {
      "epoch": 0.9647124982426543,
      "grad_norm": 1.5465401411056519,
      "learning_rate": 8.524575522865442e-05,
      "loss": 1.0405,
      "step": 6862
    },
    {
      "epoch": 0.964853085899058,
      "grad_norm": 1.4239354133605957,
      "learning_rate": 8.501587002587865e-05,
      "loss": 1.2309,
      "step": 6863
    },
    {
      "epoch": 0.9649936735554618,
      "grad_norm": 1.5161837339401245,
      "learning_rate": 8.478606580039506e-05,
      "loss": 1.1687,
      "step": 6864
    },
    {
      "epoch": 0.9651342612118656,
      "grad_norm": 1.3569387197494507,
      "learning_rate": 8.455634379411326e-05,
      "loss": 0.9929,
      "step": 6865
    },
    {
      "epoch": 0.9652748488682694,
      "grad_norm": 1.4249228239059448,
      "learning_rate": 8.432670524849723e-05,
      "loss": 1.1258,
      "step": 6866
    },
    {
      "epoch": 0.9654154365246731,
      "grad_norm": 1.3417073488235474,
      "learning_rate": 8.409715140456067e-05,
      "loss": 1.0769,
      "step": 6867
    },
    {
      "epoch": 0.9655560241810769,
      "grad_norm": 1.542946696281433,
      "learning_rate": 8.386768350285933e-05,
      "loss": 1.0551,
      "step": 6868
    },
    {
      "epoch": 0.9656966118374807,
      "grad_norm": 1.3178001642227173,
      "learning_rate": 8.363830278348455e-05,
      "loss": 1.106,
      "step": 6869
    },
    {
      "epoch": 0.9658371994938845,
      "grad_norm": 1.2758049964904785,
      "learning_rate": 8.340901048605647e-05,
      "loss": 1.0802,
      "step": 6870
    },
    {
      "epoch": 0.9659777871502883,
      "grad_norm": 1.503786325454712,
      "learning_rate": 8.317980784971739e-05,
      "loss": 1.0979,
      "step": 6871
    },
    {
      "epoch": 0.9661183748066919,
      "grad_norm": 1.583203673362732,
      "learning_rate": 8.295069611312512e-05,
      "loss": 1.0826,
      "step": 6872
    },
    {
      "epoch": 0.9662589624630957,
      "grad_norm": 1.5261930227279663,
      "learning_rate": 8.27216765144463e-05,
      "loss": 1.2401,
      "step": 6873
    },
    {
      "epoch": 0.9663995501194995,
      "grad_norm": 1.4802663326263428,
      "learning_rate": 8.249275029134921e-05,
      "loss": 1.1166,
      "step": 6874
    },
    {
      "epoch": 0.9665401377759033,
      "grad_norm": 1.9045013189315796,
      "learning_rate": 8.22639186809978e-05,
      "loss": 1.1158,
      "step": 6875
    },
    {
      "epoch": 0.9666807254323071,
      "grad_norm": 1.5785642862319946,
      "learning_rate": 8.203518292004528e-05,
      "loss": 0.9579,
      "step": 6876
    },
    {
      "epoch": 0.9668213130887108,
      "grad_norm": 1.6761881113052368,
      "learning_rate": 8.180654424462579e-05,
      "loss": 1.1258,
      "step": 6877
    },
    {
      "epoch": 0.9669619007451146,
      "grad_norm": 1.5290614366531372,
      "learning_rate": 8.15780038903495e-05,
      "loss": 1.107,
      "step": 6878
    },
    {
      "epoch": 0.9671024884015184,
      "grad_norm": 1.549878716468811,
      "learning_rate": 8.134956309229509e-05,
      "loss": 1.0793,
      "step": 6879
    },
    {
      "epoch": 0.9672430760579221,
      "grad_norm": 1.4726579189300537,
      "learning_rate": 8.112122308500316e-05,
      "loss": 1.0752,
      "step": 6880
    },
    {
      "epoch": 0.9673836637143259,
      "grad_norm": 1.5350258350372314,
      "learning_rate": 8.089298510246986e-05,
      "loss": 1.0799,
      "step": 6881
    },
    {
      "epoch": 0.9675242513707296,
      "grad_norm": 1.4702682495117188,
      "learning_rate": 8.06648503781394e-05,
      "loss": 1.0514,
      "step": 6882
    },
    {
      "epoch": 0.9676648390271334,
      "grad_norm": 1.3353904485702515,
      "learning_rate": 8.043682014489835e-05,
      "loss": 1.1403,
      "step": 6883
    },
    {
      "epoch": 0.9678054266835372,
      "grad_norm": 1.8351938724517822,
      "learning_rate": 8.020889563506905e-05,
      "loss": 1.0527,
      "step": 6884
    },
    {
      "epoch": 0.967946014339941,
      "grad_norm": 1.4033453464508057,
      "learning_rate": 7.998107808040154e-05,
      "loss": 1.012,
      "step": 6885
    },
    {
      "epoch": 0.9680866019963448,
      "grad_norm": 1.4938366413116455,
      "learning_rate": 7.975336871206847e-05,
      "loss": 1.1847,
      "step": 6886
    },
    {
      "epoch": 0.9682271896527485,
      "grad_norm": 1.6872587203979492,
      "learning_rate": 7.952576876065767e-05,
      "loss": 1.1654,
      "step": 6887
    },
    {
      "epoch": 0.9683677773091522,
      "grad_norm": 1.4420396089553833,
      "learning_rate": 7.929827945616566e-05,
      "loss": 1.1214,
      "step": 6888
    },
    {
      "epoch": 0.968508364965556,
      "grad_norm": 1.7147608995437622,
      "learning_rate": 7.907090202799121e-05,
      "loss": 1.1853,
      "step": 6889
    },
    {
      "epoch": 0.9686489526219598,
      "grad_norm": 1.430894374847412,
      "learning_rate": 7.884363770492777e-05,
      "loss": 1.1445,
      "step": 6890
    },
    {
      "epoch": 0.9687895402783636,
      "grad_norm": 1.463761329650879,
      "learning_rate": 7.861648771515855e-05,
      "loss": 1.1388,
      "step": 6891
    },
    {
      "epoch": 0.9689301279347673,
      "grad_norm": 1.5189073085784912,
      "learning_rate": 7.838945328624843e-05,
      "loss": 1.109,
      "step": 6892
    },
    {
      "epoch": 0.9690707155911711,
      "grad_norm": 1.690994381904602,
      "learning_rate": 7.816253564513736e-05,
      "loss": 1.1457,
      "step": 6893
    },
    {
      "epoch": 0.9692113032475749,
      "grad_norm": 1.5181734561920166,
      "learning_rate": 7.793573601813456e-05,
      "loss": 1.1233,
      "step": 6894
    },
    {
      "epoch": 0.9693518909039787,
      "grad_norm": 1.3520088195800781,
      "learning_rate": 7.770905563091194e-05,
      "loss": 1.0747,
      "step": 6895
    },
    {
      "epoch": 0.9694924785603825,
      "grad_norm": 1.5394023656845093,
      "learning_rate": 7.748249570849601e-05,
      "loss": 1.2377,
      "step": 6896
    },
    {
      "epoch": 0.9696330662167861,
      "grad_norm": 1.2523149251937866,
      "learning_rate": 7.72560574752629e-05,
      "loss": 1.0808,
      "step": 6897
    },
    {
      "epoch": 0.9697736538731899,
      "grad_norm": 1.2592214345932007,
      "learning_rate": 7.702974215493104e-05,
      "loss": 0.9826,
      "step": 6898
    },
    {
      "epoch": 0.9699142415295937,
      "grad_norm": 1.569685935974121,
      "learning_rate": 7.680355097055452e-05,
      "loss": 1.0999,
      "step": 6899
    },
    {
      "epoch": 0.9700548291859975,
      "grad_norm": 1.5708723068237305,
      "learning_rate": 7.657748514451677e-05,
      "loss": 1.0191,
      "step": 6900
    },
    {
      "epoch": 0.9701954168424013,
      "grad_norm": 1.7383564710617065,
      "learning_rate": 7.635154589852325e-05,
      "loss": 1.0996,
      "step": 6901
    },
    {
      "epoch": 0.970336004498805,
      "grad_norm": 1.475844144821167,
      "learning_rate": 7.61257344535957e-05,
      "loss": 0.9886,
      "step": 6902
    },
    {
      "epoch": 0.9704765921552088,
      "grad_norm": 1.3312478065490723,
      "learning_rate": 7.590005203006571e-05,
      "loss": 1.3043,
      "step": 6903
    },
    {
      "epoch": 0.9706171798116126,
      "grad_norm": 1.543102741241455,
      "learning_rate": 7.567449984756658e-05,
      "loss": 1.0945,
      "step": 6904
    },
    {
      "epoch": 0.9707577674680163,
      "grad_norm": 1.597222924232483,
      "learning_rate": 7.544907912502847e-05,
      "loss": 1.0692,
      "step": 6905
    },
    {
      "epoch": 0.97089835512442,
      "grad_norm": 1.6704896688461304,
      "learning_rate": 7.522379108067088e-05,
      "loss": 1.0702,
      "step": 6906
    },
    {
      "epoch": 0.9710389427808238,
      "grad_norm": 1.3987748622894287,
      "learning_rate": 7.499863693199634e-05,
      "loss": 1.2361,
      "step": 6907
    },
    {
      "epoch": 0.9711795304372276,
      "grad_norm": 1.2784814834594727,
      "learning_rate": 7.477361789578392e-05,
      "loss": 1.1657,
      "step": 6908
    },
    {
      "epoch": 0.9713201180936314,
      "grad_norm": 1.3739652633666992,
      "learning_rate": 7.454873518808204e-05,
      "loss": 1.1949,
      "step": 6909
    },
    {
      "epoch": 0.9714607057500352,
      "grad_norm": 1.4236023426055908,
      "learning_rate": 7.432399002420266e-05,
      "loss": 0.9165,
      "step": 6910
    },
    {
      "epoch": 0.9716012934064389,
      "grad_norm": 1.6174434423446655,
      "learning_rate": 7.409938361871498e-05,
      "loss": 1.052,
      "step": 6911
    },
    {
      "epoch": 0.9717418810628426,
      "grad_norm": 1.2940337657928467,
      "learning_rate": 7.387491718543729e-05,
      "loss": 1.0827,
      "step": 6912
    },
    {
      "epoch": 0.9718824687192464,
      "grad_norm": 1.5632166862487793,
      "learning_rate": 7.365059193743193e-05,
      "loss": 1.1002,
      "step": 6913
    },
    {
      "epoch": 0.9720230563756502,
      "grad_norm": 1.4583629369735718,
      "learning_rate": 7.34264090869988e-05,
      "loss": 1.1322,
      "step": 6914
    },
    {
      "epoch": 0.972163644032054,
      "grad_norm": 1.5475852489471436,
      "learning_rate": 7.320236984566718e-05,
      "loss": 1.0947,
      "step": 6915
    },
    {
      "epoch": 0.9723042316884577,
      "grad_norm": 1.50823175907135,
      "learning_rate": 7.297847542419116e-05,
      "loss": 1.1484,
      "step": 6916
    },
    {
      "epoch": 0.9724448193448615,
      "grad_norm": 1.4926129579544067,
      "learning_rate": 7.27547270325413e-05,
      "loss": 1.1683,
      "step": 6917
    },
    {
      "epoch": 0.9725854070012653,
      "grad_norm": 1.6797758340835571,
      "learning_rate": 7.253112587990002e-05,
      "loss": 1.1233,
      "step": 6918
    },
    {
      "epoch": 0.9727259946576691,
      "grad_norm": 1.8521652221679688,
      "learning_rate": 7.230767317465355e-05,
      "loss": 1.1393,
      "step": 6919
    },
    {
      "epoch": 0.9728665823140729,
      "grad_norm": 1.5792642831802368,
      "learning_rate": 7.208437012438551e-05,
      "loss": 0.9893,
      "step": 6920
    },
    {
      "epoch": 0.9730071699704765,
      "grad_norm": 1.4197769165039062,
      "learning_rate": 7.186121793587109e-05,
      "loss": 0.9747,
      "step": 6921
    },
    {
      "epoch": 0.9731477576268803,
      "grad_norm": 1.4667415618896484,
      "learning_rate": 7.163821781507069e-05,
      "loss": 1.1654,
      "step": 6922
    },
    {
      "epoch": 0.9732883452832841,
      "grad_norm": 1.4208130836486816,
      "learning_rate": 7.141537096712194e-05,
      "loss": 1.14,
      "step": 6923
    },
    {
      "epoch": 0.9734289329396879,
      "grad_norm": 1.6065688133239746,
      "learning_rate": 7.119267859633495e-05,
      "loss": 1.1382,
      "step": 6924
    },
    {
      "epoch": 0.9735695205960917,
      "grad_norm": 1.6730635166168213,
      "learning_rate": 7.097014190618418e-05,
      "loss": 1.2099,
      "step": 6925
    },
    {
      "epoch": 0.9737101082524954,
      "grad_norm": 1.5132180452346802,
      "learning_rate": 7.074776209930378e-05,
      "loss": 1.0678,
      "step": 6926
    },
    {
      "epoch": 0.9738506959088992,
      "grad_norm": 1.6182069778442383,
      "learning_rate": 7.052554037747953e-05,
      "loss": 1.0788,
      "step": 6927
    },
    {
      "epoch": 0.973991283565303,
      "grad_norm": 1.7683730125427246,
      "learning_rate": 7.03034779416426e-05,
      "loss": 1.1522,
      "step": 6928
    },
    {
      "epoch": 0.9741318712217067,
      "grad_norm": 1.845323920249939,
      "learning_rate": 7.008157599186363e-05,
      "loss": 1.1663,
      "step": 6929
    },
    {
      "epoch": 0.9742724588781105,
      "grad_norm": 1.4028069972991943,
      "learning_rate": 6.98598357273465e-05,
      "loss": 1.0545,
      "step": 6930
    },
    {
      "epoch": 0.9744130465345142,
      "grad_norm": 1.4146915674209595,
      "learning_rate": 6.963825834642028e-05,
      "loss": 0.9758,
      "step": 6931
    },
    {
      "epoch": 0.974553634190918,
      "grad_norm": 1.4085866212844849,
      "learning_rate": 6.94168450465345e-05,
      "loss": 1.0022,
      "step": 6932
    },
    {
      "epoch": 0.9746942218473218,
      "grad_norm": 1.5087003707885742,
      "learning_rate": 6.919559702425174e-05,
      "loss": 1.0615,
      "step": 6933
    },
    {
      "epoch": 0.9748348095037256,
      "grad_norm": 1.5092992782592773,
      "learning_rate": 6.89745154752414e-05,
      "loss": 1.1164,
      "step": 6934
    },
    {
      "epoch": 0.9749753971601294,
      "grad_norm": 2.005453586578369,
      "learning_rate": 6.875360159427342e-05,
      "loss": 1.178,
      "step": 6935
    },
    {
      "epoch": 0.975115984816533,
      "grad_norm": 1.511159062385559,
      "learning_rate": 6.853285657521092e-05,
      "loss": 1.091,
      "step": 6936
    },
    {
      "epoch": 0.9752565724729368,
      "grad_norm": 1.3348125219345093,
      "learning_rate": 6.83122816110055e-05,
      "loss": 1.1909,
      "step": 6937
    },
    {
      "epoch": 0.9753971601293406,
      "grad_norm": 1.4853235483169556,
      "learning_rate": 6.809187789368923e-05,
      "loss": 1.0402,
      "step": 6938
    },
    {
      "epoch": 0.9755377477857444,
      "grad_norm": 1.439894437789917,
      "learning_rate": 6.787164661436835e-05,
      "loss": 1.0159,
      "step": 6939
    },
    {
      "epoch": 0.9756783354421482,
      "grad_norm": 1.5562024116516113,
      "learning_rate": 6.76515889632176e-05,
      "loss": 1.217,
      "step": 6940
    },
    {
      "epoch": 0.9758189230985519,
      "grad_norm": 1.2688101530075073,
      "learning_rate": 6.74317061294739e-05,
      "loss": 1.0886,
      "step": 6941
    },
    {
      "epoch": 0.9759595107549557,
      "grad_norm": 1.583418369293213,
      "learning_rate": 6.721199930142838e-05,
      "loss": 0.8656,
      "step": 6942
    },
    {
      "epoch": 0.9761000984113595,
      "grad_norm": 1.711207628250122,
      "learning_rate": 6.699246966642188e-05,
      "loss": 1.1965,
      "step": 6943
    },
    {
      "epoch": 0.9762406860677633,
      "grad_norm": 1.5033369064331055,
      "learning_rate": 6.677311841083675e-05,
      "loss": 1.1361,
      "step": 6944
    },
    {
      "epoch": 0.976381273724167,
      "grad_norm": 1.5265699625015259,
      "learning_rate": 6.655394672009243e-05,
      "loss": 1.1068,
      "step": 6945
    },
    {
      "epoch": 0.9765218613805707,
      "grad_norm": 1.5450338125228882,
      "learning_rate": 6.633495577863739e-05,
      "loss": 1.2234,
      "step": 6946
    },
    {
      "epoch": 0.9766624490369745,
      "grad_norm": 1.4601378440856934,
      "learning_rate": 6.611614676994298e-05,
      "loss": 1.1948,
      "step": 6947
    },
    {
      "epoch": 0.9768030366933783,
      "grad_norm": 1.3029245138168335,
      "learning_rate": 6.589752087649766e-05,
      "loss": 1.1401,
      "step": 6948
    },
    {
      "epoch": 0.9769436243497821,
      "grad_norm": 1.35009765625,
      "learning_rate": 6.567907927980086e-05,
      "loss": 1.242,
      "step": 6949
    },
    {
      "epoch": 0.9770842120061859,
      "grad_norm": 1.4422743320465088,
      "learning_rate": 6.546082316035498e-05,
      "loss": 1.0536,
      "step": 6950
    },
    {
      "epoch": 0.9772247996625896,
      "grad_norm": 1.6354024410247803,
      "learning_rate": 6.524275369766073e-05,
      "loss": 0.9131,
      "step": 6951
    },
    {
      "epoch": 0.9773653873189934,
      "grad_norm": 1.5509005784988403,
      "learning_rate": 6.502487207020993e-05,
      "loss": 0.9587,
      "step": 6952
    },
    {
      "epoch": 0.9775059749753972,
      "grad_norm": 1.7896506786346436,
      "learning_rate": 6.480717945547934e-05,
      "loss": 1.0052,
      "step": 6953
    },
    {
      "epoch": 0.9776465626318009,
      "grad_norm": 1.2442312240600586,
      "learning_rate": 6.458967702992434e-05,
      "loss": 1.0595,
      "step": 6954
    },
    {
      "epoch": 0.9777871502882047,
      "grad_norm": 1.4085705280303955,
      "learning_rate": 6.437236596897191e-05,
      "loss": 1.133,
      "step": 6955
    },
    {
      "epoch": 0.9779277379446084,
      "grad_norm": 1.390135645866394,
      "learning_rate": 6.41552474470158e-05,
      "loss": 0.8678,
      "step": 6956
    },
    {
      "epoch": 0.9780683256010122,
      "grad_norm": 1.7850888967514038,
      "learning_rate": 6.393832263740877e-05,
      "loss": 0.9971,
      "step": 6957
    },
    {
      "epoch": 0.978208913257416,
      "grad_norm": 1.6166343688964844,
      "learning_rate": 6.372159271245634e-05,
      "loss": 1.0052,
      "step": 6958
    },
    {
      "epoch": 0.9783495009138198,
      "grad_norm": 1.3940684795379639,
      "learning_rate": 6.350505884341141e-05,
      "loss": 1.0795,
      "step": 6959
    },
    {
      "epoch": 0.9784900885702236,
      "grad_norm": 1.2823158502578735,
      "learning_rate": 6.328872220046711e-05,
      "loss": 1.1859,
      "step": 6960
    },
    {
      "epoch": 0.9786306762266272,
      "grad_norm": 1.372239351272583,
      "learning_rate": 6.307258395275077e-05,
      "loss": 1.0756,
      "step": 6961
    },
    {
      "epoch": 0.978771263883031,
      "grad_norm": 1.4542557001113892,
      "learning_rate": 6.285664526831771e-05,
      "loss": 1.1669,
      "step": 6962
    },
    {
      "epoch": 0.9789118515394348,
      "grad_norm": 1.4421824216842651,
      "learning_rate": 6.264090731414407e-05,
      "loss": 1.1183,
      "step": 6963
    },
    {
      "epoch": 0.9790524391958386,
      "grad_norm": 1.5484731197357178,
      "learning_rate": 6.242537125612224e-05,
      "loss": 1.0566,
      "step": 6964
    },
    {
      "epoch": 0.9791930268522424,
      "grad_norm": 1.6086316108703613,
      "learning_rate": 6.2210038259053e-05,
      "loss": 1.0932,
      "step": 6965
    },
    {
      "epoch": 0.9793336145086461,
      "grad_norm": 1.320766806602478,
      "learning_rate": 6.199490948663946e-05,
      "loss": 1.1128,
      "step": 6966
    },
    {
      "epoch": 0.9794742021650499,
      "grad_norm": 1.7345912456512451,
      "learning_rate": 6.177998610148148e-05,
      "loss": 1.0801,
      "step": 6967
    },
    {
      "epoch": 0.9796147898214537,
      "grad_norm": 1.3780122995376587,
      "learning_rate": 6.156526926506884e-05,
      "loss": 1.0595,
      "step": 6968
    },
    {
      "epoch": 0.9797553774778575,
      "grad_norm": 1.5253669023513794,
      "learning_rate": 6.135076013777513e-05,
      "loss": 0.9701,
      "step": 6969
    },
    {
      "epoch": 0.9798959651342612,
      "grad_norm": 1.3239765167236328,
      "learning_rate": 6.113645987885135e-05,
      "loss": 1.0924,
      "step": 6970
    },
    {
      "epoch": 0.9800365527906649,
      "grad_norm": 1.4777753353118896,
      "learning_rate": 6.092236964641982e-05,
      "loss": 1.3117,
      "step": 6971
    },
    {
      "epoch": 0.9801771404470687,
      "grad_norm": 1.4878495931625366,
      "learning_rate": 6.070849059746777e-05,
      "loss": 1.1314,
      "step": 6972
    },
    {
      "epoch": 0.9803177281034725,
      "grad_norm": 1.6350319385528564,
      "learning_rate": 6.049482388784133e-05,
      "loss": 0.9398,
      "step": 6973
    },
    {
      "epoch": 0.9804583157598763,
      "grad_norm": 1.5433138608932495,
      "learning_rate": 6.028137067223847e-05,
      "loss": 1.217,
      "step": 6974
    },
    {
      "epoch": 0.9805989034162801,
      "grad_norm": 1.480332612991333,
      "learning_rate": 6.0068132104204374e-05,
      "loss": 1.1434,
      "step": 6975
    },
    {
      "epoch": 0.9807394910726838,
      "grad_norm": 1.3822287321090698,
      "learning_rate": 5.985510933612373e-05,
      "loss": 1.0643,
      "step": 6976
    },
    {
      "epoch": 0.9808800787290876,
      "grad_norm": 1.5032798051834106,
      "learning_rate": 5.964230351921464e-05,
      "loss": 1.316,
      "step": 6977
    },
    {
      "epoch": 0.9810206663854913,
      "grad_norm": 1.3768588304519653,
      "learning_rate": 5.94297158035233e-05,
      "loss": 1.2898,
      "step": 6978
    },
    {
      "epoch": 0.9811612540418951,
      "grad_norm": 1.3427734375,
      "learning_rate": 5.9217347337917054e-05,
      "loss": 1.0574,
      "step": 6979
    },
    {
      "epoch": 0.9813018416982989,
      "grad_norm": 1.361555814743042,
      "learning_rate": 5.900519927007837e-05,
      "loss": 1.1317,
      "step": 6980
    },
    {
      "epoch": 0.9814424293547026,
      "grad_norm": 1.3869307041168213,
      "learning_rate": 5.879327274649882e-05,
      "loss": 1.1709,
      "step": 6981
    },
    {
      "epoch": 0.9815830170111064,
      "grad_norm": 1.5682485103607178,
      "learning_rate": 5.858156891247204e-05,
      "loss": 1.1076,
      "step": 6982
    },
    {
      "epoch": 0.9817236046675102,
      "grad_norm": 1.3626019954681396,
      "learning_rate": 5.83700889120892e-05,
      "loss": 1.0532,
      "step": 6983
    },
    {
      "epoch": 0.981864192323914,
      "grad_norm": 1.5095198154449463,
      "learning_rate": 5.815883388823129e-05,
      "loss": 1.0344,
      "step": 6984
    },
    {
      "epoch": 0.9820047799803178,
      "grad_norm": 1.5640226602554321,
      "learning_rate": 5.7947804982563245e-05,
      "loss": 0.9981,
      "step": 6985
    },
    {
      "epoch": 0.9821453676367214,
      "grad_norm": 1.6817700862884521,
      "learning_rate": 5.773700333552841e-05,
      "loss": 1.33,
      "step": 6986
    },
    {
      "epoch": 0.9822859552931252,
      "grad_norm": 1.4244813919067383,
      "learning_rate": 5.7526430086341864e-05,
      "loss": 1.1397,
      "step": 6987
    },
    {
      "epoch": 0.982426542949529,
      "grad_norm": 1.503833293914795,
      "learning_rate": 5.731608637298438e-05,
      "loss": 1.1764,
      "step": 6988
    },
    {
      "epoch": 0.9825671306059328,
      "grad_norm": 1.4407190084457397,
      "learning_rate": 5.710597333219623e-05,
      "loss": 1.1221,
      "step": 6989
    },
    {
      "epoch": 0.9827077182623366,
      "grad_norm": 1.1421538591384888,
      "learning_rate": 5.6896092099471135e-05,
      "loss": 1.1632,
      "step": 6990
    },
    {
      "epoch": 0.9828483059187403,
      "grad_norm": 1.7203139066696167,
      "learning_rate": 5.6686443809050036e-05,
      "loss": 0.9909,
      "step": 6991
    },
    {
      "epoch": 0.9829888935751441,
      "grad_norm": 1.6034400463104248,
      "learning_rate": 5.647702959391521e-05,
      "loss": 1.0488,
      "step": 6992
    },
    {
      "epoch": 0.9831294812315479,
      "grad_norm": 1.4659302234649658,
      "learning_rate": 5.626785058578328e-05,
      "loss": 1.0268,
      "step": 6993
    },
    {
      "epoch": 0.9832700688879517,
      "grad_norm": 1.6702873706817627,
      "learning_rate": 5.6058907915100846e-05,
      "loss": 1.0084,
      "step": 6994
    },
    {
      "epoch": 0.9834106565443554,
      "grad_norm": 1.5733299255371094,
      "learning_rate": 5.5850202711036246e-05,
      "loss": 1.0379,
      "step": 6995
    },
    {
      "epoch": 0.9835512442007591,
      "grad_norm": 1.4653249979019165,
      "learning_rate": 5.5641736101475094e-05,
      "loss": 0.9918,
      "step": 6996
    },
    {
      "epoch": 0.9836918318571629,
      "grad_norm": 2.0999574661254883,
      "learning_rate": 5.5433509213013426e-05,
      "loss": 1.0394,
      "step": 6997
    },
    {
      "epoch": 0.9838324195135667,
      "grad_norm": 1.4086732864379883,
      "learning_rate": 5.52255231709518e-05,
      "loss": 1.1773,
      "step": 6998
    },
    {
      "epoch": 0.9839730071699705,
      "grad_norm": 1.6002137660980225,
      "learning_rate": 5.501777909928918e-05,
      "loss": 0.9906,
      "step": 6999
    },
    {
      "epoch": 0.9841135948263743,
      "grad_norm": 1.454197883605957,
      "learning_rate": 5.481027812071703e-05,
      "loss": 1.0777,
      "step": 7000
    },
    {
      "epoch": 0.9841135948263743,
      "eval_loss": 1.1470837593078613,
      "eval_runtime": 771.2764,
      "eval_samples_per_second": 16.396,
      "eval_steps_per_second": 8.198,
      "step": 7000
    },
    {
      "epoch": 0.984254182482778,
      "grad_norm": 1.8197898864746094,
      "learning_rate": 5.460302135661246e-05,
      "loss": 1.1249,
      "step": 7001
    },
    {
      "epoch": 0.9843947701391818,
      "grad_norm": 1.3743577003479004,
      "learning_rate": 5.4396009927033885e-05,
      "loss": 0.9913,
      "step": 7002
    },
    {
      "epoch": 0.9845353577955855,
      "grad_norm": 1.4603700637817383,
      "learning_rate": 5.418924495071282e-05,
      "loss": 1.1552,
      "step": 7003
    },
    {
      "epoch": 0.9846759454519893,
      "grad_norm": 1.381390929222107,
      "learning_rate": 5.3982727545049474e-05,
      "loss": 1.1372,
      "step": 7004
    },
    {
      "epoch": 0.9848165331083931,
      "grad_norm": 1.5838944911956787,
      "learning_rate": 5.3776458826106e-05,
      "loss": 0.9288,
      "step": 7005
    },
    {
      "epoch": 0.9849571207647968,
      "grad_norm": 1.3668098449707031,
      "learning_rate": 5.3570439908600626e-05,
      "loss": 1.209,
      "step": 7006
    },
    {
      "epoch": 0.9850977084212006,
      "grad_norm": 1.4661860466003418,
      "learning_rate": 5.3364671905901556e-05,
      "loss": 1.1082,
      "step": 7007
    },
    {
      "epoch": 0.9852382960776044,
      "grad_norm": 1.3951280117034912,
      "learning_rate": 5.3159155930021e-05,
      "loss": 1.0508,
      "step": 7008
    },
    {
      "epoch": 0.9853788837340082,
      "grad_norm": 2.0603625774383545,
      "learning_rate": 5.295389309160921e-05,
      "loss": 1.2133,
      "step": 7009
    },
    {
      "epoch": 0.985519471390412,
      "grad_norm": 1.4380947351455688,
      "learning_rate": 5.274888449994855e-05,
      "loss": 0.9638,
      "step": 7010
    },
    {
      "epoch": 0.9856600590468156,
      "grad_norm": 1.443787932395935,
      "learning_rate": 5.2544131262946906e-05,
      "loss": 1.0608,
      "step": 7011
    },
    {
      "epoch": 0.9858006467032194,
      "grad_norm": 1.6421949863433838,
      "learning_rate": 5.2339634487132526e-05,
      "loss": 0.9033,
      "step": 7012
    },
    {
      "epoch": 0.9859412343596232,
      "grad_norm": 1.7539867162704468,
      "learning_rate": 5.2135395277648146e-05,
      "loss": 1.2432,
      "step": 7013
    },
    {
      "epoch": 0.986081822016027,
      "grad_norm": 1.5662457942962646,
      "learning_rate": 5.193141473824359e-05,
      "loss": 1.0036,
      "step": 7014
    },
    {
      "epoch": 0.9862224096724308,
      "grad_norm": 1.3705506324768066,
      "learning_rate": 5.172769397127142e-05,
      "loss": 1.0282,
      "step": 7015
    },
    {
      "epoch": 0.9863629973288345,
      "grad_norm": 1.5273228883743286,
      "learning_rate": 5.152423407768021e-05,
      "loss": 0.9982,
      "step": 7016
    },
    {
      "epoch": 0.9865035849852383,
      "grad_norm": 1.3906829357147217,
      "learning_rate": 5.1321036157008694e-05,
      "loss": 1.1737,
      "step": 7017
    },
    {
      "epoch": 0.9866441726416421,
      "grad_norm": 1.7082486152648926,
      "learning_rate": 5.111810130737984e-05,
      "loss": 1.3203,
      "step": 7018
    },
    {
      "epoch": 0.9867847602980458,
      "grad_norm": 1.5947237014770508,
      "learning_rate": 5.0915430625495084e-05,
      "loss": 0.9058,
      "step": 7019
    },
    {
      "epoch": 0.9869253479544496,
      "grad_norm": 1.5866906642913818,
      "learning_rate": 5.0713025206627666e-05,
      "loss": 0.9016,
      "step": 7020
    },
    {
      "epoch": 0.9870659356108533,
      "grad_norm": 1.265514850616455,
      "learning_rate": 5.051088614461832e-05,
      "loss": 1.182,
      "step": 7021
    },
    {
      "epoch": 0.9872065232672571,
      "grad_norm": 1.576364517211914,
      "learning_rate": 5.030901453186727e-05,
      "loss": 0.9591,
      "step": 7022
    },
    {
      "epoch": 0.9873471109236609,
      "grad_norm": 2.1358134746551514,
      "learning_rate": 5.0107411459330034e-05,
      "loss": 1.1584,
      "step": 7023
    },
    {
      "epoch": 0.9874876985800647,
      "grad_norm": 1.5574533939361572,
      "learning_rate": 4.9906078016510705e-05,
      "loss": 1.0329,
      "step": 7024
    },
    {
      "epoch": 0.9876282862364685,
      "grad_norm": 1.5096094608306885,
      "learning_rate": 4.9705015291456224e-05,
      "loss": 1.0142,
      "step": 7025
    },
    {
      "epoch": 0.9877688738928722,
      "grad_norm": 1.996420979499817,
      "learning_rate": 4.950422437075056e-05,
      "loss": 1.0764,
      "step": 7026
    },
    {
      "epoch": 0.987909461549276,
      "grad_norm": 1.3893033266067505,
      "learning_rate": 4.930370633950878e-05,
      "loss": 1.227,
      "step": 7027
    },
    {
      "epoch": 0.9880500492056797,
      "grad_norm": 1.4948948621749878,
      "learning_rate": 4.910346228137117e-05,
      "loss": 1.0722,
      "step": 7028
    },
    {
      "epoch": 0.9881906368620835,
      "grad_norm": 1.7939300537109375,
      "learning_rate": 4.89034932784976e-05,
      "loss": 1.1695,
      "step": 7029
    },
    {
      "epoch": 0.9883312245184873,
      "grad_norm": 1.5397518873214722,
      "learning_rate": 4.870380041156103e-05,
      "loss": 1.2103,
      "step": 7030
    },
    {
      "epoch": 0.988471812174891,
      "grad_norm": 1.4981374740600586,
      "learning_rate": 4.850438475974239e-05,
      "loss": 1.0819,
      "step": 7031
    },
    {
      "epoch": 0.9886123998312948,
      "grad_norm": 1.5639245510101318,
      "learning_rate": 4.8305247400724994e-05,
      "loss": 0.9436,
      "step": 7032
    },
    {
      "epoch": 0.9887529874876986,
      "grad_norm": 1.394305944442749,
      "learning_rate": 4.810638941068724e-05,
      "loss": 1.0881,
      "step": 7033
    },
    {
      "epoch": 0.9888935751441024,
      "grad_norm": 1.553920030593872,
      "learning_rate": 4.790781186429839e-05,
      "loss": 1.0205,
      "step": 7034
    },
    {
      "epoch": 0.9890341628005062,
      "grad_norm": 1.5767360925674438,
      "learning_rate": 4.7709515834711995e-05,
      "loss": 1.1096,
      "step": 7035
    },
    {
      "epoch": 0.9891747504569098,
      "grad_norm": 1.4725526571273804,
      "learning_rate": 4.751150239356021e-05,
      "loss": 1.2149,
      "step": 7036
    },
    {
      "epoch": 0.9893153381133136,
      "grad_norm": 1.4838228225708008,
      "learning_rate": 4.731377261094818e-05,
      "loss": 1.1533,
      "step": 7037
    },
    {
      "epoch": 0.9894559257697174,
      "grad_norm": 1.5752273797988892,
      "learning_rate": 4.711632755544766e-05,
      "loss": 1.0205,
      "step": 7038
    },
    {
      "epoch": 0.9895965134261212,
      "grad_norm": 1.613810658454895,
      "learning_rate": 4.691916829409196e-05,
      "loss": 1.2338,
      "step": 7039
    },
    {
      "epoch": 0.989737101082525,
      "grad_norm": 1.424438714981079,
      "learning_rate": 4.6722295892370384e-05,
      "loss": 1.0513,
      "step": 7040
    },
    {
      "epoch": 0.9898776887389287,
      "grad_norm": 1.416923999786377,
      "learning_rate": 4.652571141422105e-05,
      "loss": 1.1061,
      "step": 7041
    },
    {
      "epoch": 0.9900182763953325,
      "grad_norm": 1.305484652519226,
      "learning_rate": 4.632941592202671e-05,
      "loss": 1.0197,
      "step": 7042
    },
    {
      "epoch": 0.9901588640517363,
      "grad_norm": 1.754050374031067,
      "learning_rate": 4.613341047660821e-05,
      "loss": 1.2079,
      "step": 7043
    },
    {
      "epoch": 0.99029945170814,
      "grad_norm": 1.4438799619674683,
      "learning_rate": 4.593769613721891e-05,
      "loss": 1.161,
      "step": 7044
    },
    {
      "epoch": 0.9904400393645438,
      "grad_norm": 1.4338399171829224,
      "learning_rate": 4.5742273961539174e-05,
      "loss": 1.1091,
      "step": 7045
    },
    {
      "epoch": 0.9905806270209475,
      "grad_norm": 1.5512346029281616,
      "learning_rate": 4.5547145005669814e-05,
      "loss": 1.2938,
      "step": 7046
    },
    {
      "epoch": 0.9907212146773513,
      "grad_norm": 1.3071600198745728,
      "learning_rate": 4.535231032412791e-05,
      "loss": 1.0814,
      "step": 7047
    },
    {
      "epoch": 0.9908618023337551,
      "grad_norm": 1.5164111852645874,
      "learning_rate": 4.515777096983981e-05,
      "loss": 1.02,
      "step": 7048
    },
    {
      "epoch": 0.9910023899901589,
      "grad_norm": 1.4089860916137695,
      "learning_rate": 4.4963527994135524e-05,
      "loss": 0.976,
      "step": 7049
    },
    {
      "epoch": 0.9911429776465627,
      "grad_norm": 1.2942383289337158,
      "learning_rate": 4.4769582446743706e-05,
      "loss": 1.0386,
      "step": 7050
    },
    {
      "epoch": 0.9912835653029664,
      "grad_norm": 1.5589377880096436,
      "learning_rate": 4.457593537578606e-05,
      "loss": 1.1914,
      "step": 7051
    },
    {
      "epoch": 0.9914241529593701,
      "grad_norm": 1.6001297235488892,
      "learning_rate": 4.438258782777037e-05,
      "loss": 1.1662,
      "step": 7052
    },
    {
      "epoch": 0.9915647406157739,
      "grad_norm": 1.3034141063690186,
      "learning_rate": 4.4189540847586375e-05,
      "loss": 1.1688,
      "step": 7053
    },
    {
      "epoch": 0.9917053282721777,
      "grad_norm": 1.5921030044555664,
      "learning_rate": 4.3996795478499264e-05,
      "loss": 1.0191,
      "step": 7054
    },
    {
      "epoch": 0.9918459159285815,
      "grad_norm": 1.756485939025879,
      "learning_rate": 4.380435276214433e-05,
      "loss": 1.0218,
      "step": 7055
    },
    {
      "epoch": 0.9919865035849852,
      "grad_norm": 1.5053188800811768,
      "learning_rate": 4.361221373852137e-05,
      "loss": 1.1033,
      "step": 7056
    },
    {
      "epoch": 0.992127091241389,
      "grad_norm": 1.3770291805267334,
      "learning_rate": 4.342037944598855e-05,
      "loss": 1.0579,
      "step": 7057
    },
    {
      "epoch": 0.9922676788977928,
      "grad_norm": 1.399750828742981,
      "learning_rate": 4.322885092125746e-05,
      "loss": 1.1515,
      "step": 7058
    },
    {
      "epoch": 0.9924082665541966,
      "grad_norm": 1.4204837083816528,
      "learning_rate": 4.303762919938775e-05,
      "loss": 1.0683,
      "step": 7059
    },
    {
      "epoch": 0.9925488542106004,
      "grad_norm": 1.725449800491333,
      "learning_rate": 4.284671531378014e-05,
      "loss": 0.9091,
      "step": 7060
    },
    {
      "epoch": 0.992689441867004,
      "grad_norm": 1.518404245376587,
      "learning_rate": 4.2656110296172314e-05,
      "loss": 1.1988,
      "step": 7061
    },
    {
      "epoch": 0.9928300295234078,
      "grad_norm": 1.3507180213928223,
      "learning_rate": 4.246581517663265e-05,
      "loss": 1.1464,
      "step": 7062
    },
    {
      "epoch": 0.9929706171798116,
      "grad_norm": 1.5258634090423584,
      "learning_rate": 4.227583098355475e-05,
      "loss": 1.0516,
      "step": 7063
    },
    {
      "epoch": 0.9931112048362154,
      "grad_norm": 1.2326143980026245,
      "learning_rate": 4.2086158743652084e-05,
      "loss": 1.1623,
      "step": 7064
    },
    {
      "epoch": 0.9932517924926192,
      "grad_norm": 1.6842622756958008,
      "learning_rate": 4.1896799481951785e-05,
      "loss": 1.0989,
      "step": 7065
    },
    {
      "epoch": 0.9933923801490229,
      "grad_norm": 1.5034457445144653,
      "learning_rate": 4.170775422178991e-05,
      "loss": 1.0513,
      "step": 7066
    },
    {
      "epoch": 0.9935329678054267,
      "grad_norm": 1.8134254217147827,
      "learning_rate": 4.1519023984805997e-05,
      "loss": 0.9055,
      "step": 7067
    },
    {
      "epoch": 0.9936735554618304,
      "grad_norm": 1.3622709512710571,
      "learning_rate": 4.1330609790936305e-05,
      "loss": 0.9586,
      "step": 7068
    },
    {
      "epoch": 0.9938141431182342,
      "grad_norm": 1.7575784921646118,
      "learning_rate": 4.114251265840952e-05,
      "loss": 0.9406,
      "step": 7069
    },
    {
      "epoch": 0.993954730774638,
      "grad_norm": 1.5277637243270874,
      "learning_rate": 4.095473360374133e-05,
      "loss": 0.9859,
      "step": 7070
    },
    {
      "epoch": 0.9940953184310417,
      "grad_norm": 1.3744220733642578,
      "learning_rate": 4.0767273641727575e-05,
      "loss": 1.1131,
      "step": 7071
    },
    {
      "epoch": 0.9942359060874455,
      "grad_norm": 1.3571871519088745,
      "learning_rate": 4.058013378544041e-05,
      "loss": 0.8911,
      "step": 7072
    },
    {
      "epoch": 0.9943764937438493,
      "grad_norm": 1.3261908292770386,
      "learning_rate": 4.039331504622131e-05,
      "loss": 0.9257,
      "step": 7073
    },
    {
      "epoch": 0.9945170814002531,
      "grad_norm": 1.3632086515426636,
      "learning_rate": 4.0206818433677276e-05,
      "loss": 0.889,
      "step": 7074
    },
    {
      "epoch": 0.9946576690566569,
      "grad_norm": 1.4098873138427734,
      "learning_rate": 4.002064495567398e-05,
      "loss": 1.0629,
      "step": 7075
    },
    {
      "epoch": 0.9947982567130605,
      "grad_norm": 1.4357661008834839,
      "learning_rate": 3.9834795618330575e-05,
      "loss": 1.1157,
      "step": 7076
    },
    {
      "epoch": 0.9949388443694643,
      "grad_norm": 1.54239821434021,
      "learning_rate": 3.964927142601478e-05,
      "loss": 1.0632,
      "step": 7077
    },
    {
      "epoch": 0.9950794320258681,
      "grad_norm": 1.6670222282409668,
      "learning_rate": 3.946407338133754e-05,
      "loss": 0.987,
      "step": 7078
    },
    {
      "epoch": 0.9952200196822719,
      "grad_norm": 1.4960718154907227,
      "learning_rate": 3.9279202485146435e-05,
      "loss": 1.1749,
      "step": 7079
    },
    {
      "epoch": 0.9953606073386757,
      "grad_norm": 1.7500368356704712,
      "learning_rate": 3.9094659736521736e-05,
      "loss": 1.0985,
      "step": 7080
    },
    {
      "epoch": 0.9955011949950794,
      "grad_norm": 1.6385846138000488,
      "learning_rate": 3.891044613276966e-05,
      "loss": 0.9837,
      "step": 7081
    },
    {
      "epoch": 0.9956417826514832,
      "grad_norm": 1.868635892868042,
      "learning_rate": 3.872656266941848e-05,
      "loss": 1.0697,
      "step": 7082
    },
    {
      "epoch": 0.995782370307887,
      "grad_norm": 1.5955666303634644,
      "learning_rate": 3.854301034021193e-05,
      "loss": 0.8776,
      "step": 7083
    },
    {
      "epoch": 0.9959229579642908,
      "grad_norm": 1.5362552404403687,
      "learning_rate": 3.83597901371039e-05,
      "loss": 1.0975,
      "step": 7084
    },
    {
      "epoch": 0.9960635456206945,
      "grad_norm": 1.5052447319030762,
      "learning_rate": 3.8176903050253745e-05,
      "loss": 1.2186,
      "step": 7085
    },
    {
      "epoch": 0.9962041332770982,
      "grad_norm": 1.4731197357177734,
      "learning_rate": 3.7994350068020954e-05,
      "loss": 1.2456,
      "step": 7086
    },
    {
      "epoch": 0.996344720933502,
      "grad_norm": 1.5566811561584473,
      "learning_rate": 3.781213217695866e-05,
      "loss": 1.1057,
      "step": 7087
    },
    {
      "epoch": 0.9964853085899058,
      "grad_norm": 1.3439768552780151,
      "learning_rate": 3.763025036180946e-05,
      "loss": 1.1664,
      "step": 7088
    },
    {
      "epoch": 0.9966258962463096,
      "grad_norm": 1.2813482284545898,
      "learning_rate": 3.744870560550019e-05,
      "loss": 1.1708,
      "step": 7089
    },
    {
      "epoch": 0.9967664839027134,
      "grad_norm": 1.4667906761169434,
      "learning_rate": 3.7267498889135285e-05,
      "loss": 1.0428,
      "step": 7090
    },
    {
      "epoch": 0.9969070715591171,
      "grad_norm": 1.3082643747329712,
      "learning_rate": 3.708663119199307e-05,
      "loss": 1.0578,
      "step": 7091
    },
    {
      "epoch": 0.9970476592155209,
      "grad_norm": 1.7264735698699951,
      "learning_rate": 3.690610349151902e-05,
      "loss": 1.034,
      "step": 7092
    },
    {
      "epoch": 0.9971882468719246,
      "grad_norm": 1.7701631784439087,
      "learning_rate": 3.672591676332204e-05,
      "loss": 1.1178,
      "step": 7093
    },
    {
      "epoch": 0.9973288345283284,
      "grad_norm": 1.5156649351119995,
      "learning_rate": 3.654607198116794e-05,
      "loss": 0.985,
      "step": 7094
    },
    {
      "epoch": 0.9974694221847322,
      "grad_norm": 1.6105211973190308,
      "learning_rate": 3.63665701169743e-05,
      "loss": 1.2828,
      "step": 7095
    },
    {
      "epoch": 0.9976100098411359,
      "grad_norm": 1.55417799949646,
      "learning_rate": 3.6187412140805775e-05,
      "loss": 1.1582,
      "step": 7096
    },
    {
      "epoch": 0.9977505974975397,
      "grad_norm": 1.3564273118972778,
      "learning_rate": 3.600859902086902e-05,
      "loss": 1.1395,
      "step": 7097
    },
    {
      "epoch": 0.9978911851539435,
      "grad_norm": 1.639605164527893,
      "learning_rate": 3.583013172350615e-05,
      "loss": 1.103,
      "step": 7098
    },
    {
      "epoch": 0.9980317728103473,
      "grad_norm": 1.4231047630310059,
      "learning_rate": 3.5652011213191085e-05,
      "loss": 0.9508,
      "step": 7099
    },
    {
      "epoch": 0.9981723604667511,
      "grad_norm": 1.5504980087280273,
      "learning_rate": 3.547423845252298e-05,
      "loss": 1.0799,
      "step": 7100
    },
    {
      "epoch": 0.9983129481231547,
      "grad_norm": 1.3950064182281494,
      "learning_rate": 3.529681440222247e-05,
      "loss": 1.1981,
      "step": 7101
    },
    {
      "epoch": 0.9984535357795585,
      "grad_norm": 1.3455109596252441,
      "learning_rate": 3.511974002112528e-05,
      "loss": 1.0674,
      "step": 7102
    },
    {
      "epoch": 0.9985941234359623,
      "grad_norm": 1.7137280702590942,
      "learning_rate": 3.494301626617719e-05,
      "loss": 1.2208,
      "step": 7103
    },
    {
      "epoch": 0.9987347110923661,
      "grad_norm": 1.3555837869644165,
      "learning_rate": 3.476664409242939e-05,
      "loss": 1.1406,
      "step": 7104
    },
    {
      "epoch": 0.9988752987487699,
      "grad_norm": 1.2667356729507446,
      "learning_rate": 3.459062445303346e-05,
      "loss": 0.9894,
      "step": 7105
    },
    {
      "epoch": 0.9990158864051736,
      "grad_norm": 1.5275588035583496,
      "learning_rate": 3.441495829923501e-05,
      "loss": 1.2042,
      "step": 7106
    },
    {
      "epoch": 0.9991564740615774,
      "grad_norm": 1.5426528453826904,
      "learning_rate": 3.423964658036989e-05,
      "loss": 1.0572,
      "step": 7107
    },
    {
      "epoch": 0.9992970617179812,
      "grad_norm": 1.4342981576919556,
      "learning_rate": 3.406469024385833e-05,
      "loss": 1.1874,
      "step": 7108
    },
    {
      "epoch": 0.999437649374385,
      "grad_norm": 1.4254449605941772,
      "learning_rate": 3.3890090235200045e-05,
      "loss": 1.1984,
      "step": 7109
    },
    {
      "epoch": 0.9995782370307887,
      "grad_norm": 1.451080083847046,
      "learning_rate": 3.371584749796922e-05,
      "loss": 1.0495,
      "step": 7110
    },
    {
      "epoch": 0.9997188246871924,
      "grad_norm": 1.4365315437316895,
      "learning_rate": 3.35419629738087e-05,
      "loss": 0.9821,
      "step": 7111
    },
    {
      "epoch": 0.9998594123435962,
      "grad_norm": 2.3914244174957275,
      "learning_rate": 3.3368437602426304e-05,
      "loss": 1.0643,
      "step": 7112
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.5359708070755005,
      "learning_rate": 3.319527232158852e-05,
      "loss": 0.9348,
      "step": 7113
    },
    {
      "epoch": 1.0001405876564038,
      "grad_norm": 1.3420566320419312,
      "learning_rate": 3.302246806711553e-05,
      "loss": 1.1932,
      "step": 7114
    },
    {
      "epoch": 1.0002811753128076,
      "grad_norm": 1.597287654876709,
      "learning_rate": 3.28500257728769e-05,
      "loss": 1.0109,
      "step": 7115
    },
    {
      "epoch": 1.0004217629692114,
      "grad_norm": 1.269960641860962,
      "learning_rate": 3.267794637078594e-05,
      "loss": 1.2056,
      "step": 7116
    },
    {
      "epoch": 1.0005623506256152,
      "grad_norm": 1.3499561548233032,
      "learning_rate": 3.250623079079479e-05,
      "loss": 1.0944,
      "step": 7117
    },
    {
      "epoch": 1.0007029382820187,
      "grad_norm": 1.3291095495224,
      "learning_rate": 3.2334879960889595e-05,
      "loss": 0.9819,
      "step": 7118
    },
    {
      "epoch": 1.0008435259384225,
      "grad_norm": 1.573964238166809,
      "learning_rate": 3.216389480708477e-05,
      "loss": 1.0504,
      "step": 7119
    },
    {
      "epoch": 1.0009841135948263,
      "grad_norm": 1.5405292510986328,
      "learning_rate": 3.199327625341933e-05,
      "loss": 1.1106,
      "step": 7120
    },
    {
      "epoch": 1.00112470125123,
      "grad_norm": 1.3118336200714111,
      "learning_rate": 3.18230252219507e-05,
      "loss": 0.9191,
      "step": 7121
    },
    {
      "epoch": 1.0012652889076339,
      "grad_norm": 1.4346309900283813,
      "learning_rate": 3.165314263274991e-05,
      "loss": 1.1476,
      "step": 7122
    },
    {
      "epoch": 1.0014058765640377,
      "grad_norm": 1.4038722515106201,
      "learning_rate": 3.148362940389723e-05,
      "loss": 1.0508,
      "step": 7123
    },
    {
      "epoch": 1.0015464642204415,
      "grad_norm": 1.377076268196106,
      "learning_rate": 3.13144864514767e-05,
      "loss": 0.9729,
      "step": 7124
    },
    {
      "epoch": 1.0016870518768453,
      "grad_norm": 1.3491467237472534,
      "learning_rate": 3.1145714689571305e-05,
      "loss": 1.0587,
      "step": 7125
    },
    {
      "epoch": 1.001827639533249,
      "grad_norm": 1.3749178647994995,
      "learning_rate": 3.097731503025805e-05,
      "loss": 1.1039,
      "step": 7126
    },
    {
      "epoch": 1.0019682271896528,
      "grad_norm": 1.5089621543884277,
      "learning_rate": 3.080928838360301e-05,
      "loss": 1.1397,
      "step": 7127
    },
    {
      "epoch": 1.0021088148460564,
      "grad_norm": 1.399984359741211,
      "learning_rate": 3.064163565765644e-05,
      "loss": 0.9735,
      "step": 7128
    },
    {
      "epoch": 1.0022494025024602,
      "grad_norm": 1.8229072093963623,
      "learning_rate": 3.0474357758447968e-05,
      "loss": 0.992,
      "step": 7129
    },
    {
      "epoch": 1.002389990158864,
      "grad_norm": 1.5823179483413696,
      "learning_rate": 3.030745558998108e-05,
      "loss": 1.0032,
      "step": 7130
    },
    {
      "epoch": 1.0025305778152678,
      "grad_norm": 1.5762693881988525,
      "learning_rate": 3.014093005422949e-05,
      "loss": 0.9516,
      "step": 7131
    },
    {
      "epoch": 1.0026711654716716,
      "grad_norm": 1.3645492792129517,
      "learning_rate": 2.9974782051131134e-05,
      "loss": 1.015,
      "step": 7132
    },
    {
      "epoch": 1.0028117531280754,
      "grad_norm": 1.531137466430664,
      "learning_rate": 2.9809012478583432e-05,
      "loss": 1.0399,
      "step": 7133
    },
    {
      "epoch": 1.0029523407844791,
      "grad_norm": 1.6920922994613647,
      "learning_rate": 2.9643622232439074e-05,
      "loss": 1.0722,
      "step": 7134
    },
    {
      "epoch": 1.003092928440883,
      "grad_norm": 1.3649088144302368,
      "learning_rate": 2.9478612206500665e-05,
      "loss": 1.3356,
      "step": 7135
    },
    {
      "epoch": 1.0032335160972867,
      "grad_norm": 1.5856913328170776,
      "learning_rate": 2.9313983292516012e-05,
      "loss": 0.985,
      "step": 7136
    },
    {
      "epoch": 1.0033741037536905,
      "grad_norm": 1.4418002367019653,
      "learning_rate": 2.9149736380173444e-05,
      "loss": 1.1342,
      "step": 7137
    },
    {
      "epoch": 1.003514691410094,
      "grad_norm": 1.4687809944152832,
      "learning_rate": 2.8985872357096378e-05,
      "loss": 1.1065,
      "step": 7138
    },
    {
      "epoch": 1.0036552790664979,
      "grad_norm": 1.603753924369812,
      "learning_rate": 2.8822392108839768e-05,
      "loss": 0.9771,
      "step": 7139
    },
    {
      "epoch": 1.0037958667229017,
      "grad_norm": 1.4927492141723633,
      "learning_rate": 2.8659296518884216e-05,
      "loss": 1.0867,
      "step": 7140
    },
    {
      "epoch": 1.0039364543793055,
      "grad_norm": 1.3703263998031616,
      "learning_rate": 2.849658646863126e-05,
      "loss": 1.1518,
      "step": 7141
    },
    {
      "epoch": 1.0040770420357092,
      "grad_norm": 1.371498703956604,
      "learning_rate": 2.8334262837399316e-05,
      "loss": 0.9477,
      "step": 7142
    },
    {
      "epoch": 1.004217629692113,
      "grad_norm": 1.556583285331726,
      "learning_rate": 2.8172326502418368e-05,
      "loss": 1.2042,
      "step": 7143
    },
    {
      "epoch": 1.0043582173485168,
      "grad_norm": 1.5986324548721313,
      "learning_rate": 2.8010778338825372e-05,
      "loss": 0.9466,
      "step": 7144
    },
    {
      "epoch": 1.0044988050049206,
      "grad_norm": 1.5237778425216675,
      "learning_rate": 2.784961921965953e-05,
      "loss": 0.918,
      "step": 7145
    },
    {
      "epoch": 1.0046393926613244,
      "grad_norm": 1.6682713031768799,
      "learning_rate": 2.7688850015857594e-05,
      "loss": 1.1109,
      "step": 7146
    },
    {
      "epoch": 1.0047799803177282,
      "grad_norm": 1.494329810142517,
      "learning_rate": 2.7528471596249083e-05,
      "loss": 1.0312,
      "step": 7147
    },
    {
      "epoch": 1.0049205679741318,
      "grad_norm": 1.5172903537750244,
      "learning_rate": 2.73684848275518e-05,
      "loss": 1.0511,
      "step": 7148
    },
    {
      "epoch": 1.0050611556305356,
      "grad_norm": 1.4277836084365845,
      "learning_rate": 2.7208890574366484e-05,
      "loss": 1.0488,
      "step": 7149
    },
    {
      "epoch": 1.0052017432869393,
      "grad_norm": 1.664438009262085,
      "learning_rate": 2.7049689699173496e-05,
      "loss": 0.9892,
      "step": 7150
    },
    {
      "epoch": 1.0053423309433431,
      "grad_norm": 1.4506053924560547,
      "learning_rate": 2.6890883062326457e-05,
      "loss": 1.136,
      "step": 7151
    },
    {
      "epoch": 1.005482918599747,
      "grad_norm": 1.729062795639038,
      "learning_rate": 2.673247152204891e-05,
      "loss": 1.0092,
      "step": 7152
    },
    {
      "epoch": 1.0056235062561507,
      "grad_norm": 1.6269538402557373,
      "learning_rate": 2.657445593442912e-05,
      "loss": 1.0438,
      "step": 7153
    },
    {
      "epoch": 1.0057640939125545,
      "grad_norm": 1.4367226362228394,
      "learning_rate": 2.6416837153415518e-05,
      "loss": 1.1125,
      "step": 7154
    },
    {
      "epoch": 1.0059046815689583,
      "grad_norm": 1.5254462957382202,
      "learning_rate": 2.625961603081213e-05,
      "loss": 1.0288,
      "step": 7155
    },
    {
      "epoch": 1.006045269225362,
      "grad_norm": 1.5985896587371826,
      "learning_rate": 2.6102793416274075e-05,
      "loss": 1.0204,
      "step": 7156
    },
    {
      "epoch": 1.0061858568817659,
      "grad_norm": 1.5975406169891357,
      "learning_rate": 2.5946370157302357e-05,
      "loss": 1.0672,
      "step": 7157
    },
    {
      "epoch": 1.0063264445381694,
      "grad_norm": 1.7659188508987427,
      "learning_rate": 2.579034709924063e-05,
      "loss": 0.9647,
      "step": 7158
    },
    {
      "epoch": 1.0064670321945732,
      "grad_norm": 1.5538651943206787,
      "learning_rate": 2.5634725085268952e-05,
      "loss": 0.9579,
      "step": 7159
    },
    {
      "epoch": 1.006607619850977,
      "grad_norm": 1.5662903785705566,
      "learning_rate": 2.5479504956400513e-05,
      "loss": 0.9807,
      "step": 7160
    },
    {
      "epoch": 1.0067482075073808,
      "grad_norm": 1.5109504461288452,
      "learning_rate": 2.5324687551476534e-05,
      "loss": 1.1528,
      "step": 7161
    },
    {
      "epoch": 1.0068887951637846,
      "grad_norm": 1.3583396673202515,
      "learning_rate": 2.517027370716183e-05,
      "loss": 1.0596,
      "step": 7162
    },
    {
      "epoch": 1.0070293828201884,
      "grad_norm": 1.3149935007095337,
      "learning_rate": 2.501626425794028e-05,
      "loss": 1.3209,
      "step": 7163
    },
    {
      "epoch": 1.0071699704765922,
      "grad_norm": 1.4422473907470703,
      "learning_rate": 2.486266003611034e-05,
      "loss": 1.1718,
      "step": 7164
    },
    {
      "epoch": 1.007310558132996,
      "grad_norm": 1.6137259006500244,
      "learning_rate": 2.4709461871780547e-05,
      "loss": 1.1048,
      "step": 7165
    },
    {
      "epoch": 1.0074511457893998,
      "grad_norm": 1.5410677194595337,
      "learning_rate": 2.4556670592865007e-05,
      "loss": 0.8588,
      "step": 7166
    },
    {
      "epoch": 1.0075917334458036,
      "grad_norm": 1.6106072664260864,
      "learning_rate": 2.440428702507903e-05,
      "loss": 1.1315,
      "step": 7167
    },
    {
      "epoch": 1.0077323211022071,
      "grad_norm": 1.6490023136138916,
      "learning_rate": 2.425231199193414e-05,
      "loss": 1.0868,
      "step": 7168
    },
    {
      "epoch": 1.007872908758611,
      "grad_norm": 1.3732061386108398,
      "learning_rate": 2.4100746314734858e-05,
      "loss": 1.069,
      "step": 7169
    },
    {
      "epoch": 1.0080134964150147,
      "grad_norm": 1.523517370223999,
      "learning_rate": 2.3949590812572643e-05,
      "loss": 1.1345,
      "step": 7170
    },
    {
      "epoch": 1.0081540840714185,
      "grad_norm": 1.5410020351409912,
      "learning_rate": 2.379884630232275e-05,
      "loss": 0.9583,
      "step": 7171
    },
    {
      "epoch": 1.0082946717278223,
      "grad_norm": 1.306692361831665,
      "learning_rate": 2.3648513598639266e-05,
      "loss": 1.0899,
      "step": 7172
    },
    {
      "epoch": 1.008435259384226,
      "grad_norm": 1.4949569702148438,
      "learning_rate": 2.349859351395073e-05,
      "loss": 0.9494,
      "step": 7173
    },
    {
      "epoch": 1.0085758470406299,
      "grad_norm": 1.3254300355911255,
      "learning_rate": 2.334908685845585e-05,
      "loss": 1.0903,
      "step": 7174
    },
    {
      "epoch": 1.0087164346970336,
      "grad_norm": 1.5935840606689453,
      "learning_rate": 2.319999444011919e-05,
      "loss": 1.0589,
      "step": 7175
    },
    {
      "epoch": 1.0088570223534374,
      "grad_norm": 1.4763513803482056,
      "learning_rate": 2.30513170646662e-05,
      "loss": 0.9974,
      "step": 7176
    },
    {
      "epoch": 1.0089976100098412,
      "grad_norm": 1.5197933912277222,
      "learning_rate": 2.2903055535580208e-05,
      "loss": 0.8442,
      "step": 7177
    },
    {
      "epoch": 1.0091381976662448,
      "grad_norm": 1.4842288494110107,
      "learning_rate": 2.275521065409637e-05,
      "loss": 1.0965,
      "step": 7178
    },
    {
      "epoch": 1.0092787853226486,
      "grad_norm": 1.6083124876022339,
      "learning_rate": 2.2607783219198687e-05,
      "loss": 1.2891,
      "step": 7179
    },
    {
      "epoch": 1.0094193729790524,
      "grad_norm": 1.4966883659362793,
      "learning_rate": 2.2460774027615072e-05,
      "loss": 1.0489,
      "step": 7180
    },
    {
      "epoch": 1.0095599606354562,
      "grad_norm": 1.3921725749969482,
      "learning_rate": 2.2314183873813145e-05,
      "loss": 1.1536,
      "step": 7181
    },
    {
      "epoch": 1.00970054829186,
      "grad_norm": 1.5813889503479004,
      "learning_rate": 2.216801354999598e-05,
      "loss": 1.0689,
      "step": 7182
    },
    {
      "epoch": 1.0098411359482637,
      "grad_norm": 1.8177207708358765,
      "learning_rate": 2.2022263846097812e-05,
      "loss": 1.1084,
      "step": 7183
    },
    {
      "epoch": 1.0099817236046675,
      "grad_norm": 1.257386565208435,
      "learning_rate": 2.187693554977974e-05,
      "loss": 1.1432,
      "step": 7184
    },
    {
      "epoch": 1.0101223112610713,
      "grad_norm": 1.3818007707595825,
      "learning_rate": 2.1732029446425605e-05,
      "loss": 0.9816,
      "step": 7185
    },
    {
      "epoch": 1.0102628989174751,
      "grad_norm": 1.5492340326309204,
      "learning_rate": 2.158754631913732e-05,
      "loss": 0.9701,
      "step": 7186
    },
    {
      "epoch": 1.010403486573879,
      "grad_norm": 1.4685629606246948,
      "learning_rate": 2.144348694873113e-05,
      "loss": 1.0169,
      "step": 7187
    },
    {
      "epoch": 1.0105440742302825,
      "grad_norm": 1.3837218284606934,
      "learning_rate": 2.129985211373359e-05,
      "loss": 0.9737,
      "step": 7188
    },
    {
      "epoch": 1.0106846618866863,
      "grad_norm": 1.500636100769043,
      "learning_rate": 2.1156642590376274e-05,
      "loss": 0.979,
      "step": 7189
    },
    {
      "epoch": 1.01082524954309,
      "grad_norm": 1.3121509552001953,
      "learning_rate": 2.1013859152592775e-05,
      "loss": 1.0525,
      "step": 7190
    },
    {
      "epoch": 1.0109658371994938,
      "grad_norm": 1.4288678169250488,
      "learning_rate": 2.0871502572013855e-05,
      "loss": 1.0247,
      "step": 7191
    },
    {
      "epoch": 1.0111064248558976,
      "grad_norm": 1.4085490703582764,
      "learning_rate": 2.0729573617963527e-05,
      "loss": 0.9604,
      "step": 7192
    },
    {
      "epoch": 1.0112470125123014,
      "grad_norm": 1.4107152223587036,
      "learning_rate": 2.058807305745488e-05,
      "loss": 0.962,
      "step": 7193
    },
    {
      "epoch": 1.0113876001687052,
      "grad_norm": 1.7004605531692505,
      "learning_rate": 2.044700165518556e-05,
      "loss": 1.0036,
      "step": 7194
    },
    {
      "epoch": 1.011528187825109,
      "grad_norm": 1.505251169204712,
      "learning_rate": 2.03063601735342e-05,
      "loss": 1.1326,
      "step": 7195
    },
    {
      "epoch": 1.0116687754815128,
      "grad_norm": 1.4610600471496582,
      "learning_rate": 2.016614937255633e-05,
      "loss": 1.0987,
      "step": 7196
    },
    {
      "epoch": 1.0118093631379166,
      "grad_norm": 2.0142178535461426,
      "learning_rate": 2.00263700099794e-05,
      "loss": 1.1393,
      "step": 7197
    },
    {
      "epoch": 1.0119499507943202,
      "grad_norm": 1.7644439935684204,
      "learning_rate": 1.9887022841199677e-05,
      "loss": 0.7873,
      "step": 7198
    },
    {
      "epoch": 1.012090538450724,
      "grad_norm": 1.6255332231521606,
      "learning_rate": 1.974810861927765e-05,
      "loss": 1.0,
      "step": 7199
    },
    {
      "epoch": 1.0122311261071277,
      "grad_norm": 1.768632411956787,
      "learning_rate": 1.9609628094934107e-05,
      "loss": 1.1506,
      "step": 7200
    },
    {
      "epoch": 1.0123717137635315,
      "grad_norm": 1.403295874595642,
      "learning_rate": 1.947158201654612e-05,
      "loss": 1.076,
      "step": 7201
    },
    {
      "epoch": 1.0125123014199353,
      "grad_norm": 1.8181434869766235,
      "learning_rate": 1.9333971130142515e-05,
      "loss": 1.0677,
      "step": 7202
    },
    {
      "epoch": 1.012652889076339,
      "grad_norm": 1.4645088911056519,
      "learning_rate": 1.91967961794009e-05,
      "loss": 1.116,
      "step": 7203
    },
    {
      "epoch": 1.012793476732743,
      "grad_norm": 1.407679557800293,
      "learning_rate": 1.906005790564266e-05,
      "loss": 1.1066,
      "step": 7204
    },
    {
      "epoch": 1.0129340643891467,
      "grad_norm": 1.5630123615264893,
      "learning_rate": 1.8923757047829093e-05,
      "loss": 1.2118,
      "step": 7205
    },
    {
      "epoch": 1.0130746520455505,
      "grad_norm": 1.7162995338439941,
      "learning_rate": 1.878789434255781e-05,
      "loss": 0.9742,
      "step": 7206
    },
    {
      "epoch": 1.0132152397019543,
      "grad_norm": 1.680738091468811,
      "learning_rate": 1.8652470524058884e-05,
      "loss": 1.0014,
      "step": 7207
    },
    {
      "epoch": 1.0133558273583578,
      "grad_norm": 1.5014522075653076,
      "learning_rate": 1.8517486324189916e-05,
      "loss": 1.116,
      "step": 7208
    },
    {
      "epoch": 1.0134964150147616,
      "grad_norm": 1.4506645202636719,
      "learning_rate": 1.838294247243314e-05,
      "loss": 0.9782,
      "step": 7209
    },
    {
      "epoch": 1.0136370026711654,
      "grad_norm": 1.324790358543396,
      "learning_rate": 1.824883969589094e-05,
      "loss": 1.0642,
      "step": 7210
    },
    {
      "epoch": 1.0137775903275692,
      "grad_norm": 2.066535472869873,
      "learning_rate": 1.8115178719282078e-05,
      "loss": 1.0252,
      "step": 7211
    },
    {
      "epoch": 1.013918177983973,
      "grad_norm": 1.3600592613220215,
      "learning_rate": 1.7981960264937814e-05,
      "loss": 0.9262,
      "step": 7212
    },
    {
      "epoch": 1.0140587656403768,
      "grad_norm": 1.3873893022537231,
      "learning_rate": 1.784918505279761e-05,
      "loss": 1.1005,
      "step": 7213
    },
    {
      "epoch": 1.0141993532967806,
      "grad_norm": 1.3841638565063477,
      "learning_rate": 1.7716853800405787e-05,
      "loss": 1.0462,
      "step": 7214
    },
    {
      "epoch": 1.0143399409531844,
      "grad_norm": 1.5362440347671509,
      "learning_rate": 1.75849672229077e-05,
      "loss": 0.9708,
      "step": 7215
    },
    {
      "epoch": 1.0144805286095882,
      "grad_norm": 1.615399718284607,
      "learning_rate": 1.7453526033044998e-05,
      "loss": 1.1249,
      "step": 7216
    },
    {
      "epoch": 1.014621116265992,
      "grad_norm": 1.6218479871749878,
      "learning_rate": 1.732253094115277e-05,
      "loss": 1.0535,
      "step": 7217
    },
    {
      "epoch": 1.0147617039223955,
      "grad_norm": 1.4778813123703003,
      "learning_rate": 1.7191982655155182e-05,
      "loss": 0.9882,
      "step": 7218
    },
    {
      "epoch": 1.0149022915787993,
      "grad_norm": 1.459153175354004,
      "learning_rate": 1.7061881880561792e-05,
      "loss": 0.8351,
      "step": 7219
    },
    {
      "epoch": 1.015042879235203,
      "grad_norm": 1.3794708251953125,
      "learning_rate": 1.693222932046379e-05,
      "loss": 1.1483,
      "step": 7220
    },
    {
      "epoch": 1.0151834668916069,
      "grad_norm": 1.4329463243484497,
      "learning_rate": 1.6803025675529748e-05,
      "loss": 0.9041,
      "step": 7221
    },
    {
      "epoch": 1.0153240545480107,
      "grad_norm": 1.3548088073730469,
      "learning_rate": 1.6674271644002793e-05,
      "loss": 1.1566,
      "step": 7222
    },
    {
      "epoch": 1.0154646422044145,
      "grad_norm": 1.7825151681900024,
      "learning_rate": 1.6545967921695947e-05,
      "loss": 1.0495,
      "step": 7223
    },
    {
      "epoch": 1.0156052298608182,
      "grad_norm": 1.4358363151550293,
      "learning_rate": 1.6418115201988504e-05,
      "loss": 0.9508,
      "step": 7224
    },
    {
      "epoch": 1.015745817517222,
      "grad_norm": 1.3745015859603882,
      "learning_rate": 1.6290714175822598e-05,
      "loss": 1.0892,
      "step": 7225
    },
    {
      "epoch": 1.0158864051736258,
      "grad_norm": 1.45799720287323,
      "learning_rate": 1.6163765531699647e-05,
      "loss": 0.9421,
      "step": 7226
    },
    {
      "epoch": 1.0160269928300296,
      "grad_norm": 1.5777772665023804,
      "learning_rate": 1.6037269955675692e-05,
      "loss": 1.0516,
      "step": 7227
    },
    {
      "epoch": 1.0161675804864332,
      "grad_norm": 1.4504019021987915,
      "learning_rate": 1.5911228131358823e-05,
      "loss": 0.9868,
      "step": 7228
    },
    {
      "epoch": 1.016308168142837,
      "grad_norm": 2.021071195602417,
      "learning_rate": 1.578564073990445e-05,
      "loss": 1.0651,
      "step": 7229
    },
    {
      "epoch": 1.0164487557992408,
      "grad_norm": 1.2744444608688354,
      "learning_rate": 1.5660508460012723e-05,
      "loss": 1.1187,
      "step": 7230
    },
    {
      "epoch": 1.0165893434556446,
      "grad_norm": 1.7761133909225464,
      "learning_rate": 1.5535831967923996e-05,
      "loss": 0.9226,
      "step": 7231
    },
    {
      "epoch": 1.0167299311120483,
      "grad_norm": 1.6068034172058105,
      "learning_rate": 1.541161193741524e-05,
      "loss": 1.1031,
      "step": 7232
    },
    {
      "epoch": 1.0168705187684521,
      "grad_norm": 1.699084758758545,
      "learning_rate": 1.528784903979682e-05,
      "loss": 1.2531,
      "step": 7233
    },
    {
      "epoch": 1.017011106424856,
      "grad_norm": 1.421723484992981,
      "learning_rate": 1.516454394390896e-05,
      "loss": 1.226,
      "step": 7234
    },
    {
      "epoch": 1.0171516940812597,
      "grad_norm": 1.48567533493042,
      "learning_rate": 1.5041697316117231e-05,
      "loss": 1.1472,
      "step": 7235
    },
    {
      "epoch": 1.0172922817376635,
      "grad_norm": 1.5358043909072876,
      "learning_rate": 1.4919309820310046e-05,
      "loss": 1.1655,
      "step": 7236
    },
    {
      "epoch": 1.0174328693940673,
      "grad_norm": 1.6314585208892822,
      "learning_rate": 1.4797382117894087e-05,
      "loss": 1.0491,
      "step": 7237
    },
    {
      "epoch": 1.0175734570504709,
      "grad_norm": 1.4880248308181763,
      "learning_rate": 1.4675914867791807e-05,
      "loss": 0.9249,
      "step": 7238
    },
    {
      "epoch": 1.0177140447068747,
      "grad_norm": 1.290797472000122,
      "learning_rate": 1.4554908726436977e-05,
      "loss": 1.1422,
      "step": 7239
    },
    {
      "epoch": 1.0178546323632784,
      "grad_norm": 1.3299438953399658,
      "learning_rate": 1.4434364347771257e-05,
      "loss": 0.9415,
      "step": 7240
    },
    {
      "epoch": 1.0179952200196822,
      "grad_norm": 1.6577279567718506,
      "learning_rate": 1.4314282383241052e-05,
      "loss": 1.0884,
      "step": 7241
    },
    {
      "epoch": 1.018135807676086,
      "grad_norm": 1.4665013551712036,
      "learning_rate": 1.4194663481794035e-05,
      "loss": 1.1098,
      "step": 7242
    },
    {
      "epoch": 1.0182763953324898,
      "grad_norm": 1.8941113948822021,
      "learning_rate": 1.407550828987486e-05,
      "loss": 1.1212,
      "step": 7243
    },
    {
      "epoch": 1.0184169829888936,
      "grad_norm": 1.3331152200698853,
      "learning_rate": 1.3956817451422489e-05,
      "loss": 1.0709,
      "step": 7244
    },
    {
      "epoch": 1.0185575706452974,
      "grad_norm": 1.3274990320205688,
      "learning_rate": 1.3838591607866658e-05,
      "loss": 1.1874,
      "step": 7245
    },
    {
      "epoch": 1.0186981583017012,
      "grad_norm": 1.4853616952896118,
      "learning_rate": 1.3720831398123646e-05,
      "loss": 0.9209,
      "step": 7246
    },
    {
      "epoch": 1.018838745958105,
      "grad_norm": 1.6424025297164917,
      "learning_rate": 1.360353745859383e-05,
      "loss": 1.0608,
      "step": 7247
    },
    {
      "epoch": 1.0189793336145085,
      "grad_norm": 1.2963334321975708,
      "learning_rate": 1.3486710423157289e-05,
      "loss": 1.1024,
      "step": 7248
    },
    {
      "epoch": 1.0191199212709123,
      "grad_norm": 1.2700539827346802,
      "learning_rate": 1.3370350923171404e-05,
      "loss": 0.869,
      "step": 7249
    },
    {
      "epoch": 1.0192605089273161,
      "grad_norm": 1.6258535385131836,
      "learning_rate": 1.3254459587466627e-05,
      "loss": 1.0479,
      "step": 7250
    },
    {
      "epoch": 1.01940109658372,
      "grad_norm": 1.4653228521347046,
      "learning_rate": 1.3139037042343172e-05,
      "loss": 0.9455,
      "step": 7251
    },
    {
      "epoch": 1.0195416842401237,
      "grad_norm": 1.4495126008987427,
      "learning_rate": 1.302408391156802e-05,
      "loss": 1.1203,
      "step": 7252
    },
    {
      "epoch": 1.0196822718965275,
      "grad_norm": 1.4283028841018677,
      "learning_rate": 1.290960081637157e-05,
      "loss": 0.9379,
      "step": 7253
    },
    {
      "epoch": 1.0198228595529313,
      "grad_norm": 1.3744627237319946,
      "learning_rate": 1.2795588375443546e-05,
      "loss": 0.9157,
      "step": 7254
    },
    {
      "epoch": 1.019963447209335,
      "grad_norm": 1.5965403318405151,
      "learning_rate": 1.268204720493058e-05,
      "loss": 0.9915,
      "step": 7255
    },
    {
      "epoch": 1.0201040348657389,
      "grad_norm": 1.683557152748108,
      "learning_rate": 1.2568977918432056e-05,
      "loss": 0.8233,
      "step": 7256
    },
    {
      "epoch": 1.0202446225221427,
      "grad_norm": 1.3486860990524292,
      "learning_rate": 1.2456381126997696e-05,
      "loss": 1.131,
      "step": 7257
    },
    {
      "epoch": 1.0203852101785462,
      "grad_norm": 1.6343109607696533,
      "learning_rate": 1.2344257439123508e-05,
      "loss": 1.0602,
      "step": 7258
    },
    {
      "epoch": 1.02052579783495,
      "grad_norm": 1.5759952068328857,
      "learning_rate": 1.223260746074859e-05,
      "loss": 1.2356,
      "step": 7259
    },
    {
      "epoch": 1.0206663854913538,
      "grad_norm": 1.4166204929351807,
      "learning_rate": 1.212143179525218e-05,
      "loss": 1.1323,
      "step": 7260
    },
    {
      "epoch": 1.0208069731477576,
      "grad_norm": 1.5639125108718872,
      "learning_rate": 1.2010731043450496e-05,
      "loss": 1.1877,
      "step": 7261
    },
    {
      "epoch": 1.0209475608041614,
      "grad_norm": 1.462023138999939,
      "learning_rate": 1.19005058035927e-05,
      "loss": 0.9564,
      "step": 7262
    },
    {
      "epoch": 1.0210881484605652,
      "grad_norm": 1.435020923614502,
      "learning_rate": 1.1790756671358538e-05,
      "loss": 1.0198,
      "step": 7263
    },
    {
      "epoch": 1.021228736116969,
      "grad_norm": 1.6229645013809204,
      "learning_rate": 1.16814842398547e-05,
      "loss": 0.955,
      "step": 7264
    },
    {
      "epoch": 1.0213693237733728,
      "grad_norm": 1.5336557626724243,
      "learning_rate": 1.1572689099611678e-05,
      "loss": 1.0227,
      "step": 7265
    },
    {
      "epoch": 1.0215099114297765,
      "grad_norm": 1.3408896923065186,
      "learning_rate": 1.1464371838580657e-05,
      "loss": 0.9782,
      "step": 7266
    },
    {
      "epoch": 1.0216504990861803,
      "grad_norm": 1.5095473527908325,
      "learning_rate": 1.1356533042129958e-05,
      "loss": 1.0988,
      "step": 7267
    },
    {
      "epoch": 1.021791086742584,
      "grad_norm": 1.7253886461257935,
      "learning_rate": 1.124917329304266e-05,
      "loss": 1.1919,
      "step": 7268
    },
    {
      "epoch": 1.0219316743989877,
      "grad_norm": 1.5368906259536743,
      "learning_rate": 1.114229317151273e-05,
      "loss": 1.0883,
      "step": 7269
    },
    {
      "epoch": 1.0220722620553915,
      "grad_norm": 1.4344271421432495,
      "learning_rate": 1.103589325514196e-05,
      "loss": 1.0079,
      "step": 7270
    },
    {
      "epoch": 1.0222128497117953,
      "grad_norm": 1.5866864919662476,
      "learning_rate": 1.092997411893728e-05,
      "loss": 1.0185,
      "step": 7271
    },
    {
      "epoch": 1.022353437368199,
      "grad_norm": 1.3767361640930176,
      "learning_rate": 1.0824536335307279e-05,
      "loss": 1.0952,
      "step": 7272
    },
    {
      "epoch": 1.0224940250246028,
      "grad_norm": 1.8260283470153809,
      "learning_rate": 1.0719580474059243e-05,
      "loss": 0.9678,
      "step": 7273
    },
    {
      "epoch": 1.0226346126810066,
      "grad_norm": 1.3202228546142578,
      "learning_rate": 1.0615107102396104e-05,
      "loss": 1.1173,
      "step": 7274
    },
    {
      "epoch": 1.0227752003374104,
      "grad_norm": 1.445241093635559,
      "learning_rate": 1.0511116784913022e-05,
      "loss": 1.0019,
      "step": 7275
    },
    {
      "epoch": 1.0229157879938142,
      "grad_norm": 1.433897852897644,
      "learning_rate": 1.040761008359512e-05,
      "loss": 0.9616,
      "step": 7276
    },
    {
      "epoch": 1.023056375650218,
      "grad_norm": 1.577541708946228,
      "learning_rate": 1.0304587557813694e-05,
      "loss": 1.03,
      "step": 7277
    },
    {
      "epoch": 1.0231969633066216,
      "grad_norm": 1.4882720708847046,
      "learning_rate": 1.0202049764323318e-05,
      "loss": 1.0899,
      "step": 7278
    },
    {
      "epoch": 1.0233375509630254,
      "grad_norm": 1.6175670623779297,
      "learning_rate": 1.0099997257259209e-05,
      "loss": 0.9631,
      "step": 7279
    },
    {
      "epoch": 1.0234781386194292,
      "grad_norm": 1.5523431301116943,
      "learning_rate": 9.998430588133911e-06,
      "loss": 0.9702,
      "step": 7280
    },
    {
      "epoch": 1.023618726275833,
      "grad_norm": 1.8178857564926147,
      "learning_rate": 9.897350305834418e-06,
      "loss": 1.0406,
      "step": 7281
    },
    {
      "epoch": 1.0237593139322367,
      "grad_norm": 1.3142282962799072,
      "learning_rate": 9.796756956619168e-06,
      "loss": 1.0544,
      "step": 7282
    },
    {
      "epoch": 1.0238999015886405,
      "grad_norm": 1.466064214706421,
      "learning_rate": 9.696651084115116e-06,
      "loss": 0.8843,
      "step": 7283
    },
    {
      "epoch": 1.0240404892450443,
      "grad_norm": 1.878197193145752,
      "learning_rate": 9.597033229314823e-06,
      "loss": 0.9256,
      "step": 7284
    },
    {
      "epoch": 1.024181076901448,
      "grad_norm": 1.3471401929855347,
      "learning_rate": 9.497903930573548e-06,
      "loss": 0.8889,
      "step": 7285
    },
    {
      "epoch": 1.024321664557852,
      "grad_norm": 1.489101529121399,
      "learning_rate": 9.399263723605988e-06,
      "loss": 1.1121,
      "step": 7286
    },
    {
      "epoch": 1.0244622522142557,
      "grad_norm": 1.5779095888137817,
      "learning_rate": 9.301113141484174e-06,
      "loss": 0.9978,
      "step": 7287
    },
    {
      "epoch": 1.0246028398706593,
      "grad_norm": 1.7173233032226562,
      "learning_rate": 9.203452714633832e-06,
      "loss": 1.3257,
      "step": 7288
    },
    {
      "epoch": 1.024743427527063,
      "grad_norm": 1.5250251293182373,
      "learning_rate": 9.106282970831692e-06,
      "loss": 1.1331,
      "step": 7289
    },
    {
      "epoch": 1.0248840151834668,
      "grad_norm": 1.3517266511917114,
      "learning_rate": 9.009604435202901e-06,
      "loss": 1.0637,
      "step": 7290
    },
    {
      "epoch": 1.0250246028398706,
      "grad_norm": 1.5456668138504028,
      "learning_rate": 8.913417630218002e-06,
      "loss": 0.8971,
      "step": 7291
    },
    {
      "epoch": 1.0251651904962744,
      "grad_norm": 1.4483518600463867,
      "learning_rate": 8.817723075690064e-06,
      "loss": 0.8366,
      "step": 7292
    },
    {
      "epoch": 1.0253057781526782,
      "grad_norm": 1.5994899272918701,
      "learning_rate": 8.722521288772067e-06,
      "loss": 1.0183,
      "step": 7293
    },
    {
      "epoch": 1.025446365809082,
      "grad_norm": 1.6329482793807983,
      "learning_rate": 8.627812783953626e-06,
      "loss": 0.9254,
      "step": 7294
    },
    {
      "epoch": 1.0255869534654858,
      "grad_norm": 1.572769284248352,
      "learning_rate": 8.533598073059047e-06,
      "loss": 1.0849,
      "step": 7295
    },
    {
      "epoch": 1.0257275411218896,
      "grad_norm": 1.5653319358825684,
      "learning_rate": 8.439877665243823e-06,
      "loss": 1.131,
      "step": 7296
    },
    {
      "epoch": 1.0258681287782934,
      "grad_norm": 1.28908109664917,
      "learning_rate": 8.346652066991978e-06,
      "loss": 1.1773,
      "step": 7297
    },
    {
      "epoch": 1.026008716434697,
      "grad_norm": 1.375524878501892,
      "learning_rate": 8.25392178211375e-06,
      "loss": 1.0515,
      "step": 7298
    },
    {
      "epoch": 1.0261493040911007,
      "grad_norm": 1.7017937898635864,
      "learning_rate": 8.161687311742471e-06,
      "loss": 1.2185,
      "step": 7299
    },
    {
      "epoch": 1.0262898917475045,
      "grad_norm": 1.4430265426635742,
      "learning_rate": 8.069949154332045e-06,
      "loss": 0.9711,
      "step": 7300
    },
    {
      "epoch": 1.0264304794039083,
      "grad_norm": 1.3410766124725342,
      "learning_rate": 7.97870780565415e-06,
      "loss": 1.1916,
      "step": 7301
    },
    {
      "epoch": 1.026571067060312,
      "grad_norm": 1.9308757781982422,
      "learning_rate": 7.887963758795647e-06,
      "loss": 1.127,
      "step": 7302
    },
    {
      "epoch": 1.0267116547167159,
      "grad_norm": 1.5695571899414062,
      "learning_rate": 7.797717504155855e-06,
      "loss": 1.0913,
      "step": 7303
    },
    {
      "epoch": 1.0268522423731197,
      "grad_norm": 1.3744035959243774,
      "learning_rate": 7.707969529443993e-06,
      "loss": 1.0745,
      "step": 7304
    },
    {
      "epoch": 1.0269928300295235,
      "grad_norm": 1.5892186164855957,
      "learning_rate": 7.61872031967622e-06,
      "loss": 1.0108,
      "step": 7305
    },
    {
      "epoch": 1.0271334176859273,
      "grad_norm": 1.5681127309799194,
      "learning_rate": 7.52997035717371e-06,
      "loss": 1.2911,
      "step": 7306
    },
    {
      "epoch": 1.027274005342331,
      "grad_norm": 1.390880823135376,
      "learning_rate": 7.441720121559159e-06,
      "loss": 1.0517,
      "step": 7307
    },
    {
      "epoch": 1.0274145929987346,
      "grad_norm": 1.816503882408142,
      "learning_rate": 7.3539700897548915e-06,
      "loss": 1.1135,
      "step": 7308
    },
    {
      "epoch": 1.0275551806551384,
      "grad_norm": 1.542775273323059,
      "learning_rate": 7.266720735979959e-06,
      "loss": 1.1368,
      "step": 7309
    },
    {
      "epoch": 1.0276957683115422,
      "grad_norm": 1.5376708507537842,
      "learning_rate": 7.1799725317476585e-06,
      "loss": 1.0395,
      "step": 7310
    },
    {
      "epoch": 1.027836355967946,
      "grad_norm": 1.8192800283432007,
      "learning_rate": 7.093725945862972e-06,
      "loss": 0.9325,
      "step": 7311
    },
    {
      "epoch": 1.0279769436243498,
      "grad_norm": 1.6886401176452637,
      "learning_rate": 7.0079814444200975e-06,
      "loss": 1.0644,
      "step": 7312
    },
    {
      "epoch": 1.0281175312807536,
      "grad_norm": 1.538917064666748,
      "learning_rate": 6.92273949079959e-06,
      "loss": 1.0107,
      "step": 7313
    },
    {
      "epoch": 1.0282581189371574,
      "grad_norm": 1.4504956007003784,
      "learning_rate": 6.838000545666601e-06,
      "loss": 1.0025,
      "step": 7314
    },
    {
      "epoch": 1.0283987065935611,
      "grad_norm": 1.2843478918075562,
      "learning_rate": 6.7537650669674765e-06,
      "loss": 1.1566,
      "step": 7315
    },
    {
      "epoch": 1.028539294249965,
      "grad_norm": 1.5761348009109497,
      "learning_rate": 6.670033509927975e-06,
      "loss": 1.2232,
      "step": 7316
    },
    {
      "epoch": 1.0286798819063687,
      "grad_norm": 1.1856194734573364,
      "learning_rate": 6.586806327050498e-06,
      "loss": 1.0516,
      "step": 7317
    },
    {
      "epoch": 1.0288204695627723,
      "grad_norm": 1.3811259269714355,
      "learning_rate": 6.504083968111707e-06,
      "loss": 1.1098,
      "step": 7318
    },
    {
      "epoch": 1.028961057219176,
      "grad_norm": 1.7371106147766113,
      "learning_rate": 6.421866880160077e-06,
      "loss": 0.8946,
      "step": 7319
    },
    {
      "epoch": 1.0291016448755799,
      "grad_norm": 1.7434462308883667,
      "learning_rate": 6.340155507513534e-06,
      "loss": 1.0253,
      "step": 7320
    },
    {
      "epoch": 1.0292422325319837,
      "grad_norm": 1.4227527379989624,
      "learning_rate": 6.258950291756971e-06,
      "loss": 0.9328,
      "step": 7321
    },
    {
      "epoch": 1.0293828201883874,
      "grad_norm": 1.5113885402679443,
      "learning_rate": 6.178251671739932e-06,
      "loss": 1.1261,
      "step": 7322
    },
    {
      "epoch": 1.0295234078447912,
      "grad_norm": 1.529666781425476,
      "learning_rate": 6.098060083574264e-06,
      "loss": 1.0405,
      "step": 7323
    },
    {
      "epoch": 1.029663995501195,
      "grad_norm": 1.387815237045288,
      "learning_rate": 6.018375960631484e-06,
      "loss": 0.9626,
      "step": 7324
    },
    {
      "epoch": 1.0298045831575988,
      "grad_norm": 1.4615569114685059,
      "learning_rate": 5.939199733541056e-06,
      "loss": 1.0175,
      "step": 7325
    },
    {
      "epoch": 1.0299451708140026,
      "grad_norm": 1.5500293970108032,
      "learning_rate": 5.860531830187299e-06,
      "loss": 0.9669,
      "step": 7326
    },
    {
      "epoch": 1.0300857584704064,
      "grad_norm": 1.4499295949935913,
      "learning_rate": 5.782372675707626e-06,
      "loss": 1.1545,
      "step": 7327
    },
    {
      "epoch": 1.03022634612681,
      "grad_norm": 1.7740131616592407,
      "learning_rate": 5.704722692490061e-06,
      "loss": 1.0551,
      "step": 7328
    },
    {
      "epoch": 1.0303669337832138,
      "grad_norm": 1.4130096435546875,
      "learning_rate": 5.62758230017093e-06,
      "loss": 1.1935,
      "step": 7329
    },
    {
      "epoch": 1.0305075214396175,
      "grad_norm": 1.644066333770752,
      "learning_rate": 5.5509519156326405e-06,
      "loss": 0.9971,
      "step": 7330
    },
    {
      "epoch": 1.0306481090960213,
      "grad_norm": 1.6475977897644043,
      "learning_rate": 5.4748319530014584e-06,
      "loss": 1.0471,
      "step": 7331
    },
    {
      "epoch": 1.0307886967524251,
      "grad_norm": 1.8083319664001465,
      "learning_rate": 5.399222823645023e-06,
      "loss": 1.0407,
      "step": 7332
    },
    {
      "epoch": 1.030929284408829,
      "grad_norm": 1.4511609077453613,
      "learning_rate": 5.3241249361706935e-06,
      "loss": 0.9441,
      "step": 7333
    },
    {
      "epoch": 1.0310698720652327,
      "grad_norm": 1.4462833404541016,
      "learning_rate": 5.249538696422596e-06,
      "loss": 0.9439,
      "step": 7334
    },
    {
      "epoch": 1.0312104597216365,
      "grad_norm": 1.5828665494918823,
      "learning_rate": 5.175464507480021e-06,
      "loss": 1.0505,
      "step": 7335
    },
    {
      "epoch": 1.0313510473780403,
      "grad_norm": 1.274104356765747,
      "learning_rate": 5.101902769654998e-06,
      "loss": 1.1646,
      "step": 7336
    },
    {
      "epoch": 1.031491635034444,
      "grad_norm": 1.4121143817901611,
      "learning_rate": 5.028853880490136e-06,
      "loss": 0.9409,
      "step": 7337
    },
    {
      "epoch": 1.0316322226908476,
      "grad_norm": 1.415476679801941,
      "learning_rate": 4.95631823475654e-06,
      "loss": 0.9444,
      "step": 7338
    },
    {
      "epoch": 1.0317728103472514,
      "grad_norm": 1.5451878309249878,
      "learning_rate": 4.884296224451612e-06,
      "loss": 1.1847,
      "step": 7339
    },
    {
      "epoch": 1.0319133980036552,
      "grad_norm": 1.5770334005355835,
      "learning_rate": 4.812788238796972e-06,
      "loss": 1.0382,
      "step": 7340
    },
    {
      "epoch": 1.032053985660059,
      "grad_norm": 1.3503503799438477,
      "learning_rate": 4.741794664236421e-06,
      "loss": 0.8386,
      "step": 7341
    },
    {
      "epoch": 1.0321945733164628,
      "grad_norm": 1.8396228551864624,
      "learning_rate": 4.67131588443358e-06,
      "loss": 1.008,
      "step": 7342
    },
    {
      "epoch": 1.0323351609728666,
      "grad_norm": 1.6986032724380493,
      "learning_rate": 4.601352280270144e-06,
      "loss": 0.9977,
      "step": 7343
    },
    {
      "epoch": 1.0324757486292704,
      "grad_norm": 1.4830639362335205,
      "learning_rate": 4.531904229843864e-06,
      "loss": 1.0261,
      "step": 7344
    },
    {
      "epoch": 1.0326163362856742,
      "grad_norm": 1.640820026397705,
      "learning_rate": 4.4629721084659904e-06,
      "loss": 0.9801,
      "step": 7345
    },
    {
      "epoch": 1.032756923942078,
      "grad_norm": 1.3836439847946167,
      "learning_rate": 4.3945562886598015e-06,
      "loss": 0.9295,
      "step": 7346
    },
    {
      "epoch": 1.0328975115984818,
      "grad_norm": 1.382209300994873,
      "learning_rate": 4.326657140158341e-06,
      "loss": 1.1024,
      "step": 7347
    },
    {
      "epoch": 1.0330380992548853,
      "grad_norm": 1.6988322734832764,
      "learning_rate": 4.259275029902454e-06,
      "loss": 1.0463,
      "step": 7348
    },
    {
      "epoch": 1.0331786869112891,
      "grad_norm": 1.341835856437683,
      "learning_rate": 4.192410322038853e-06,
      "loss": 1.0303,
      "step": 7349
    },
    {
      "epoch": 1.033319274567693,
      "grad_norm": 1.4652339220046997,
      "learning_rate": 4.126063377917922e-06,
      "loss": 1.1759,
      "step": 7350
    },
    {
      "epoch": 1.0334598622240967,
      "grad_norm": 1.3527277708053589,
      "learning_rate": 4.06023455609209e-06,
      "loss": 1.2372,
      "step": 7351
    },
    {
      "epoch": 1.0336004498805005,
      "grad_norm": 1.5057971477508545,
      "learning_rate": 3.994924212313877e-06,
      "loss": 1.0617,
      "step": 7352
    },
    {
      "epoch": 1.0337410375369043,
      "grad_norm": 1.3917059898376465,
      "learning_rate": 3.930132699533528e-06,
      "loss": 1.0283,
      "step": 7353
    },
    {
      "epoch": 1.033881625193308,
      "grad_norm": 1.2661685943603516,
      "learning_rate": 3.8658603678976555e-06,
      "loss": 0.9592,
      "step": 7354
    },
    {
      "epoch": 1.0340222128497119,
      "grad_norm": 1.4580186605453491,
      "learning_rate": 3.802107564747026e-06,
      "loss": 1.0403,
      "step": 7355
    },
    {
      "epoch": 1.0341628005061156,
      "grad_norm": 1.604075312614441,
      "learning_rate": 3.7388746346147863e-06,
      "loss": 1.1236,
      "step": 7356
    },
    {
      "epoch": 1.0343033881625194,
      "grad_norm": 1.5621041059494019,
      "learning_rate": 3.67616191922463e-06,
      "loss": 0.9988,
      "step": 7357
    },
    {
      "epoch": 1.034443975818923,
      "grad_norm": 1.4339983463287354,
      "learning_rate": 3.613969757488711e-06,
      "loss": 1.0961,
      "step": 7358
    },
    {
      "epoch": 1.0345845634753268,
      "grad_norm": 1.4659901857376099,
      "learning_rate": 3.552298485506278e-06,
      "loss": 1.0885,
      "step": 7359
    },
    {
      "epoch": 1.0347251511317306,
      "grad_norm": 1.699198603630066,
      "learning_rate": 3.4911484365614756e-06,
      "loss": 0.9589,
      "step": 7360
    },
    {
      "epoch": 1.0348657387881344,
      "grad_norm": 1.516530990600586,
      "learning_rate": 3.4305199411215126e-06,
      "loss": 0.9974,
      "step": 7361
    },
    {
      "epoch": 1.0350063264445382,
      "grad_norm": 1.6149194240570068,
      "learning_rate": 3.3704133268351645e-06,
      "loss": 1.0901,
      "step": 7362
    },
    {
      "epoch": 1.035146914100942,
      "grad_norm": 1.422570824623108,
      "learning_rate": 3.3108289185309395e-06,
      "loss": 1.0441,
      "step": 7363
    },
    {
      "epoch": 1.0352875017573457,
      "grad_norm": 1.6459753513336182,
      "learning_rate": 3.2517670382150032e-06,
      "loss": 0.9776,
      "step": 7364
    },
    {
      "epoch": 1.0354280894137495,
      "grad_norm": 1.5483568906784058,
      "learning_rate": 3.193228005069837e-06,
      "loss": 0.9789,
      "step": 7365
    },
    {
      "epoch": 1.0355686770701533,
      "grad_norm": 1.5683586597442627,
      "learning_rate": 3.1352121354523035e-06,
      "loss": 1.0192,
      "step": 7366
    },
    {
      "epoch": 1.035709264726557,
      "grad_norm": 1.6272835731506348,
      "learning_rate": 3.0777197428919825e-06,
      "loss": 0.9916,
      "step": 7367
    },
    {
      "epoch": 1.0358498523829607,
      "grad_norm": 1.5113105773925781,
      "learning_rate": 3.0207511380895504e-06,
      "loss": 1.0993,
      "step": 7368
    },
    {
      "epoch": 1.0359904400393645,
      "grad_norm": 1.5294599533081055,
      "learning_rate": 2.964306628914848e-06,
      "loss": 0.9523,
      "step": 7369
    },
    {
      "epoch": 1.0361310276957683,
      "grad_norm": 1.360330581665039,
      "learning_rate": 2.908386520405515e-06,
      "loss": 1.0931,
      "step": 7370
    },
    {
      "epoch": 1.036271615352172,
      "grad_norm": 1.2664473056793213,
      "learning_rate": 2.8529911147653466e-06,
      "loss": 1.0941,
      "step": 7371
    },
    {
      "epoch": 1.0364122030085758,
      "grad_norm": 1.3432201147079468,
      "learning_rate": 2.7981207113622732e-06,
      "loss": 1.1324,
      "step": 7372
    },
    {
      "epoch": 1.0365527906649796,
      "grad_norm": 1.5031368732452393,
      "learning_rate": 2.7437756067271834e-06,
      "loss": 0.9085,
      "step": 7373
    },
    {
      "epoch": 1.0366933783213834,
      "grad_norm": 1.7502188682556152,
      "learning_rate": 2.689956094552104e-06,
      "loss": 1.079,
      "step": 7374
    },
    {
      "epoch": 1.0368339659777872,
      "grad_norm": 1.5532528162002563,
      "learning_rate": 2.636662465688644e-06,
      "loss": 1.1044,
      "step": 7375
    },
    {
      "epoch": 1.036974553634191,
      "grad_norm": 1.4202972650527954,
      "learning_rate": 2.5838950081464753e-06,
      "loss": 1.1916,
      "step": 7376
    },
    {
      "epoch": 1.0371151412905948,
      "grad_norm": 1.2843525409698486,
      "learning_rate": 2.5316540070915774e-06,
      "loss": 1.0023,
      "step": 7377
    },
    {
      "epoch": 1.0372557289469984,
      "grad_norm": 1.330298900604248,
      "learning_rate": 2.4799397448450944e-06,
      "loss": 1.0228,
      "step": 7378
    },
    {
      "epoch": 1.0373963166034021,
      "grad_norm": 1.5532774925231934,
      "learning_rate": 2.428752500881426e-06,
      "loss": 1.0088,
      "step": 7379
    },
    {
      "epoch": 1.037536904259806,
      "grad_norm": 1.4063657522201538,
      "learning_rate": 2.378092551826794e-06,
      "loss": 1.0501,
      "step": 7380
    },
    {
      "epoch": 1.0376774919162097,
      "grad_norm": 1.4077318906784058,
      "learning_rate": 2.3279601714578993e-06,
      "loss": 1.0724,
      "step": 7381
    },
    {
      "epoch": 1.0378180795726135,
      "grad_norm": 1.3763009309768677,
      "learning_rate": 2.278355630700457e-06,
      "loss": 1.0208,
      "step": 7382
    },
    {
      "epoch": 1.0379586672290173,
      "grad_norm": 1.6546697616577148,
      "learning_rate": 2.2292791976273984e-06,
      "loss": 1.0325,
      "step": 7383
    },
    {
      "epoch": 1.038099254885421,
      "grad_norm": 1.4320520162582397,
      "learning_rate": 2.1807311374578365e-06,
      "loss": 1.0778,
      "step": 7384
    },
    {
      "epoch": 1.0382398425418249,
      "grad_norm": 1.5389633178710938,
      "learning_rate": 2.1327117125552907e-06,
      "loss": 1.0948,
      "step": 7385
    },
    {
      "epoch": 1.0383804301982287,
      "grad_norm": 1.8929367065429688,
      "learning_rate": 2.085221182426633e-06,
      "loss": 0.9378,
      "step": 7386
    },
    {
      "epoch": 1.0385210178546325,
      "grad_norm": 1.5684199333190918,
      "learning_rate": 2.038259803720344e-06,
      "loss": 0.9484,
      "step": 7387
    },
    {
      "epoch": 1.038661605511036,
      "grad_norm": 1.3743641376495361,
      "learning_rate": 1.9918278302252346e-06,
      "loss": 1.1932,
      "step": 7388
    },
    {
      "epoch": 1.0388021931674398,
      "grad_norm": 1.368688941001892,
      "learning_rate": 1.945925512869151e-06,
      "loss": 0.9626,
      "step": 7389
    },
    {
      "epoch": 1.0389427808238436,
      "grad_norm": 1.7270339727401733,
      "learning_rate": 1.9005530997176612e-06,
      "loss": 1.012,
      "step": 7390
    },
    {
      "epoch": 1.0390833684802474,
      "grad_norm": 1.357089638710022,
      "learning_rate": 1.8557108359724572e-06,
      "loss": 1.0802,
      "step": 7391
    },
    {
      "epoch": 1.0392239561366512,
      "grad_norm": 1.68410325050354,
      "learning_rate": 1.8113989639703233e-06,
      "loss": 0.996,
      "step": 7392
    },
    {
      "epoch": 1.039364543793055,
      "grad_norm": 1.4111616611480713,
      "learning_rate": 1.7676177231815361e-06,
      "loss": 0.9108,
      "step": 7393
    },
    {
      "epoch": 1.0395051314494588,
      "grad_norm": 1.4396833181381226,
      "learning_rate": 1.7243673502089775e-06,
      "loss": 1.0703,
      "step": 7394
    },
    {
      "epoch": 1.0396457191058626,
      "grad_norm": 1.816718578338623,
      "learning_rate": 1.6816480787864464e-06,
      "loss": 0.8724,
      "step": 7395
    },
    {
      "epoch": 1.0397863067622664,
      "grad_norm": 1.956730842590332,
      "learning_rate": 1.6394601397775488e-06,
      "loss": 1.045,
      "step": 7396
    },
    {
      "epoch": 1.0399268944186701,
      "grad_norm": 1.4896612167358398,
      "learning_rate": 1.597803761174488e-06,
      "loss": 0.9701,
      "step": 7397
    },
    {
      "epoch": 1.0400674820750737,
      "grad_norm": 1.377267837524414,
      "learning_rate": 1.5566791680969417e-06,
      "loss": 1.0179,
      "step": 7398
    },
    {
      "epoch": 1.0402080697314775,
      "grad_norm": 1.7097358703613281,
      "learning_rate": 1.516086582790477e-06,
      "loss": 1.096,
      "step": 7399
    },
    {
      "epoch": 1.0403486573878813,
      "grad_norm": 1.7153594493865967,
      "learning_rate": 1.476026224625715e-06,
      "loss": 0.9676,
      "step": 7400
    },
    {
      "epoch": 1.040489245044285,
      "grad_norm": 1.674796462059021,
      "learning_rate": 1.4364983100970786e-06,
      "loss": 0.9914,
      "step": 7401
    },
    {
      "epoch": 1.0406298327006889,
      "grad_norm": 1.5090444087982178,
      "learning_rate": 1.3975030528213918e-06,
      "loss": 1.01,
      "step": 7402
    },
    {
      "epoch": 1.0407704203570927,
      "grad_norm": 1.565069317817688,
      "learning_rate": 1.3590406635370257e-06,
      "loss": 1.0363,
      "step": 7403
    },
    {
      "epoch": 1.0409110080134965,
      "grad_norm": 1.784616231918335,
      "learning_rate": 1.3211113501024885e-06,
      "loss": 1.0202,
      "step": 7404
    },
    {
      "epoch": 1.0410515956699002,
      "grad_norm": 1.7871835231781006,
      "learning_rate": 1.2837153174956041e-06,
      "loss": 1.1047,
      "step": 7405
    },
    {
      "epoch": 1.041192183326304,
      "grad_norm": 1.5277364253997803,
      "learning_rate": 1.2468527678121567e-06,
      "loss": 1.2073,
      "step": 7406
    },
    {
      "epoch": 1.0413327709827078,
      "grad_norm": 1.5287423133850098,
      "learning_rate": 1.210523900264826e-06,
      "loss": 1.091,
      "step": 7407
    },
    {
      "epoch": 1.0414733586391114,
      "grad_norm": 1.4245407581329346,
      "learning_rate": 1.1747289111822102e-06,
      "loss": 1.0364,
      "step": 7408
    },
    {
      "epoch": 1.0416139462955152,
      "grad_norm": 1.4376643896102905,
      "learning_rate": 1.1394679940078145e-06,
      "loss": 0.9161,
      "step": 7409
    },
    {
      "epoch": 1.041754533951919,
      "grad_norm": 1.700852394104004,
      "learning_rate": 1.1047413392987315e-06,
      "loss": 0.9123,
      "step": 7410
    },
    {
      "epoch": 1.0418951216083228,
      "grad_norm": 1.9847846031188965,
      "learning_rate": 1.0705491347249519e-06,
      "loss": 1.0002,
      "step": 7411
    },
    {
      "epoch": 1.0420357092647266,
      "grad_norm": 1.4610950946807861,
      "learning_rate": 1.0368915650680212e-06,
      "loss": 0.9523,
      "step": 7412
    },
    {
      "epoch": 1.0421762969211303,
      "grad_norm": 1.5872931480407715,
      "learning_rate": 1.0037688122203958e-06,
      "loss": 0.999,
      "step": 7413
    },
    {
      "epoch": 1.0423168845775341,
      "grad_norm": 1.535187840461731,
      "learning_rate": 9.711810551841783e-07,
      "loss": 0.9944,
      "step": 7414
    },
    {
      "epoch": 1.042457472233938,
      "grad_norm": 1.6387301683425903,
      "learning_rate": 9.391284700701941e-07,
      "loss": 1.1221,
      "step": 7415
    },
    {
      "epoch": 1.0425980598903417,
      "grad_norm": 1.8672726154327393,
      "learning_rate": 9.076112300971607e-07,
      "loss": 1.0116,
      "step": 7416
    },
    {
      "epoch": 1.0427386475467455,
      "grad_norm": 1.4490668773651123,
      "learning_rate": 8.766295055907315e-07,
      "loss": 1.0497,
      "step": 7417
    },
    {
      "epoch": 1.042879235203149,
      "grad_norm": 1.4482072591781616,
      "learning_rate": 8.461834639823863e-07,
      "loss": 1.1171,
      "step": 7418
    },
    {
      "epoch": 1.0430198228595529,
      "grad_norm": 1.4973803758621216,
      "learning_rate": 8.16273269808776e-07,
      "loss": 1.0689,
      "step": 7419
    },
    {
      "epoch": 1.0431604105159566,
      "grad_norm": 1.6939635276794434,
      "learning_rate": 7.868990847106683e-07,
      "loss": 1.0822,
      "step": 7420
    },
    {
      "epoch": 1.0433009981723604,
      "grad_norm": 1.6266913414001465,
      "learning_rate": 7.580610674321587e-07,
      "loss": 1.0916,
      "step": 7421
    },
    {
      "epoch": 1.0434415858287642,
      "grad_norm": 1.3450113534927368,
      "learning_rate": 7.297593738197938e-07,
      "loss": 1.0565,
      "step": 7422
    },
    {
      "epoch": 1.043582173485168,
      "grad_norm": 1.787305474281311,
      "learning_rate": 7.019941568216171e-07,
      "loss": 0.9877,
      "step": 7423
    },
    {
      "epoch": 1.0437227611415718,
      "grad_norm": 1.5477948188781738,
      "learning_rate": 6.747655664866015e-07,
      "loss": 1.0593,
      "step": 7424
    },
    {
      "epoch": 1.0438633487979756,
      "grad_norm": 1.3710073232650757,
      "learning_rate": 6.480737499635847e-07,
      "loss": 1.0522,
      "step": 7425
    },
    {
      "epoch": 1.0440039364543794,
      "grad_norm": 1.4484994411468506,
      "learning_rate": 6.219188515005469e-07,
      "loss": 1.0035,
      "step": 7426
    },
    {
      "epoch": 1.0441445241107832,
      "grad_norm": 1.4611567258834839,
      "learning_rate": 5.963010124439116e-07,
      "loss": 0.9474,
      "step": 7427
    },
    {
      "epoch": 1.0442851117671867,
      "grad_norm": 1.5895005464553833,
      "learning_rate": 5.712203712377018e-07,
      "loss": 0.8511,
      "step": 7428
    },
    {
      "epoch": 1.0444256994235905,
      "grad_norm": 1.3179590702056885,
      "learning_rate": 5.46677063422818e-07,
      "loss": 1.0514,
      "step": 7429
    },
    {
      "epoch": 1.0445662870799943,
      "grad_norm": 1.781865119934082,
      "learning_rate": 5.226712216363172e-07,
      "loss": 0.982,
      "step": 7430
    },
    {
      "epoch": 1.0447068747363981,
      "grad_norm": 1.7055500745773315,
      "learning_rate": 4.992029756105909e-07,
      "loss": 1.0635,
      "step": 7431
    },
    {
      "epoch": 1.044847462392802,
      "grad_norm": 1.402642011642456,
      "learning_rate": 4.7627245217291004e-07,
      "loss": 1.1124,
      "step": 7432
    },
    {
      "epoch": 1.0449880500492057,
      "grad_norm": 1.835249900817871,
      "learning_rate": 4.53879775244459e-07,
      "loss": 1.1382,
      "step": 7433
    },
    {
      "epoch": 1.0451286377056095,
      "grad_norm": 1.7065354585647583,
      "learning_rate": 4.3202506583982504e-07,
      "loss": 0.9466,
      "step": 7434
    },
    {
      "epoch": 1.0452692253620133,
      "grad_norm": 1.2742799520492554,
      "learning_rate": 4.107084420663099e-07,
      "loss": 1.0804,
      "step": 7435
    },
    {
      "epoch": 1.045409813018417,
      "grad_norm": 1.5067863464355469,
      "learning_rate": 3.8993001912330795e-07,
      "loss": 1.0044,
      "step": 7436
    },
    {
      "epoch": 1.0455504006748209,
      "grad_norm": 1.4375189542770386,
      "learning_rate": 3.6968990930164035e-07,
      "loss": 0.9066,
      "step": 7437
    },
    {
      "epoch": 1.0456909883312244,
      "grad_norm": 1.5230399370193481,
      "learning_rate": 3.4998822198298865e-07,
      "loss": 1.0881,
      "step": 7438
    },
    {
      "epoch": 1.0458315759876282,
      "grad_norm": 1.6760952472686768,
      "learning_rate": 3.30825063639284e-07,
      "loss": 1.0949,
      "step": 7439
    },
    {
      "epoch": 1.045972163644032,
      "grad_norm": 1.4915120601654053,
      "learning_rate": 3.122005378321524e-07,
      "loss": 0.9557,
      "step": 7440
    },
    {
      "epoch": 1.0461127513004358,
      "grad_norm": 1.401726484298706,
      "learning_rate": 2.9411474521232605e-07,
      "loss": 1.036,
      "step": 7441
    },
    {
      "epoch": 1.0462533389568396,
      "grad_norm": 1.5590780973434448,
      "learning_rate": 2.765677835190772e-07,
      "loss": 1.0347,
      "step": 7442
    },
    {
      "epoch": 1.0463939266132434,
      "grad_norm": 1.6937497854232788,
      "learning_rate": 2.5955974757981836e-07,
      "loss": 0.9473,
      "step": 7443
    },
    {
      "epoch": 1.0465345142696472,
      "grad_norm": 1.486203908920288,
      "learning_rate": 2.4309072930942536e-07,
      "loss": 1.0475,
      "step": 7444
    },
    {
      "epoch": 1.046675101926051,
      "grad_norm": 1.490031361579895,
      "learning_rate": 2.2716081770981502e-07,
      "loss": 1.0657,
      "step": 7445
    },
    {
      "epoch": 1.0468156895824547,
      "grad_norm": 1.538040041923523,
      "learning_rate": 2.117700988694793e-07,
      "loss": 0.9285,
      "step": 7446
    },
    {
      "epoch": 1.0469562772388585,
      "grad_norm": 1.4191606044769287,
      "learning_rate": 1.9691865596299653e-07,
      "loss": 1.2666,
      "step": 7447
    },
    {
      "epoch": 1.047096864895262,
      "grad_norm": 1.401591181755066,
      "learning_rate": 1.826065692506096e-07,
      "loss": 1.155,
      "step": 7448
    },
    {
      "epoch": 1.047237452551666,
      "grad_norm": 1.326306700706482,
      "learning_rate": 1.6883391607774857e-07,
      "loss": 1.1122,
      "step": 7449
    },
    {
      "epoch": 1.0473780402080697,
      "grad_norm": 1.423309564590454,
      "learning_rate": 1.556007708746199e-07,
      "loss": 1.1154,
      "step": 7450
    },
    {
      "epoch": 1.0475186278644735,
      "grad_norm": 1.9978773593902588,
      "learning_rate": 1.429072051558511e-07,
      "loss": 1.0549,
      "step": 7451
    },
    {
      "epoch": 1.0476592155208773,
      "grad_norm": 1.4259705543518066,
      "learning_rate": 1.3075328752006898e-07,
      "loss": 1.0509,
      "step": 7452
    },
    {
      "epoch": 1.047799803177281,
      "grad_norm": 1.561458706855774,
      "learning_rate": 1.1913908364949988e-07,
      "loss": 0.9862,
      "step": 7453
    },
    {
      "epoch": 1.0479403908336848,
      "grad_norm": 1.5968376398086548,
      "learning_rate": 1.0806465630966989e-07,
      "loss": 1.0862,
      "step": 7454
    },
    {
      "epoch": 1.0480809784900886,
      "grad_norm": 1.5060172080993652,
      "learning_rate": 9.753006534906073e-08,
      "loss": 1.0206,
      "step": 7455
    },
    {
      "epoch": 1.0482215661464924,
      "grad_norm": 1.7504246234893799,
      "learning_rate": 8.753536769874337e-08,
      "loss": 1.044,
      "step": 7456
    },
    {
      "epoch": 1.0483621538028962,
      "grad_norm": 1.5773710012435913,
      "learning_rate": 7.808061737208938e-08,
      "loss": 1.0635,
      "step": 7457
    },
    {
      "epoch": 1.0485027414592998,
      "grad_norm": 1.5233796834945679,
      "learning_rate": 6.916586546450444e-08,
      "loss": 1.2576,
      "step": 7458
    },
    {
      "epoch": 1.0486433291157036,
      "grad_norm": 1.6016193628311157,
      "learning_rate": 6.079116015311747e-08,
      "loss": 0.8984,
      "step": 7459
    },
    {
      "epoch": 1.0487839167721074,
      "grad_norm": 1.4310550689697266,
      "learning_rate": 5.295654669655869e-08,
      "loss": 0.8607,
      "step": 7460
    },
    {
      "epoch": 1.0489245044285112,
      "grad_norm": 1.3714909553527832,
      "learning_rate": 4.566206743465973e-08,
      "loss": 1.0885,
      "step": 7461
    },
    {
      "epoch": 1.049065092084915,
      "grad_norm": 1.483629822731018,
      "learning_rate": 3.8907761788298247e-08,
      "loss": 1.1072,
      "step": 7462
    },
    {
      "epoch": 1.0492056797413187,
      "grad_norm": 1.5710159540176392,
      "learning_rate": 3.26936662590871e-08,
      "loss": 0.9771,
      "step": 7463
    },
    {
      "epoch": 1.0493462673977225,
      "grad_norm": 1.3411877155303955,
      "learning_rate": 2.7019814429274372e-08,
      "loss": 0.9996,
      "step": 7464
    },
    {
      "epoch": 1.0494868550541263,
      "grad_norm": 1.4881250858306885,
      "learning_rate": 2.1886236961521365e-08,
      "loss": 1.1029,
      "step": 7465
    },
    {
      "epoch": 1.04962744271053,
      "grad_norm": 1.8134263753890991,
      "learning_rate": 1.7292961598724956e-08,
      "loss": 1.0964,
      "step": 7466
    },
    {
      "epoch": 1.0497680303669337,
      "grad_norm": 1.6593881845474243,
      "learning_rate": 1.3240013163884346e-08,
      "loss": 0.8947,
      "step": 7467
    },
    {
      "epoch": 1.0499086180233375,
      "grad_norm": 1.267920732498169,
      "learning_rate": 9.727413559945664e-09,
      "loss": 1.0521,
      "step": 7468
    },
    {
      "epoch": 1.0500492056797412,
      "grad_norm": 1.6211906671524048,
      "learning_rate": 6.755181769724228e-09,
      "loss": 1.0345,
      "step": 7469
    },
    {
      "epoch": 1.050189793336145,
      "grad_norm": 1.3643829822540283,
      "learning_rate": 4.323333855760226e-09,
      "loss": 0.9688,
      "step": 7470
    },
    {
      "epoch": 1.0503303809925488,
      "grad_norm": 1.466988444328308,
      "learning_rate": 2.4318829602631986e-09,
      "loss": 0.9335,
      "step": 7471
    },
    {
      "epoch": 1.0504709686489526,
      "grad_norm": 1.3193769454956055,
      "learning_rate": 1.0808393050121267e-09,
      "loss": 1.0899,
      "step": 7472
    },
    {
      "epoch": 1.0506115563053564,
      "grad_norm": 1.464869737625122,
      "learning_rate": 2.702101913221178e-10,
      "loss": 1.0823,
      "step": 7473
    },
    {
      "epoch": 1.0507521439617602,
      "grad_norm": 1.4806318283081055,
      "learning_rate": 0.0,
      "loss": 0.9492,
      "step": 7474
    },
    {
      "epoch": 1.050892731618164,
      "grad_norm": 1.7873996496200562,
      "learning_rate": 2.702101913221178e-10,
      "loss": 1.2153,
      "step": 7475
    },
    {
      "epoch": 1.0510333192745678,
      "grad_norm": 1.7119433879852295,
      "learning_rate": 1.0808393050121267e-09,
      "loss": 0.9788,
      "step": 7476
    },
    {
      "epoch": 1.0511739069309716,
      "grad_norm": 1.6421407461166382,
      "learning_rate": 2.4318829602631986e-09,
      "loss": 0.8924,
      "step": 7477
    },
    {
      "epoch": 1.0513144945873751,
      "grad_norm": 1.535558819770813,
      "learning_rate": 4.323333855760226e-09,
      "loss": 1.0256,
      "step": 7478
    },
    {
      "epoch": 1.051455082243779,
      "grad_norm": 1.5903263092041016,
      "learning_rate": 6.755181769724228e-09,
      "loss": 0.972,
      "step": 7479
    },
    {
      "epoch": 1.0515956699001827,
      "grad_norm": 1.2777702808380127,
      "learning_rate": 9.727413559945664e-09,
      "loss": 1.1756,
      "step": 7480
    },
    {
      "epoch": 1.0517362575565865,
      "grad_norm": 1.5013158321380615,
      "learning_rate": 1.3240013163884346e-08,
      "loss": 0.9112,
      "step": 7481
    },
    {
      "epoch": 1.0518768452129903,
      "grad_norm": 1.5822244882583618,
      "learning_rate": 1.7292961598736057e-08,
      "loss": 1.0671,
      "step": 7482
    },
    {
      "epoch": 1.052017432869394,
      "grad_norm": 1.6155567169189453,
      "learning_rate": 2.1886236961521365e-08,
      "loss": 1.0956,
      "step": 7483
    },
    {
      "epoch": 1.0521580205257979,
      "grad_norm": 1.374367117881775,
      "learning_rate": 2.7019814429274372e-08,
      "loss": 1.162,
      "step": 7484
    },
    {
      "epoch": 1.0522986081822017,
      "grad_norm": 1.5334498882293701,
      "learning_rate": 3.269366625907599e-08,
      "loss": 1.0906,
      "step": 7485
    },
    {
      "epoch": 1.0524391958386055,
      "grad_norm": 1.3204318284988403,
      "learning_rate": 3.890776178828714e-08,
      "loss": 1.0025,
      "step": 7486
    },
    {
      "epoch": 1.052579783495009,
      "grad_norm": 1.5051853656768799,
      "learning_rate": 4.566206743465973e-08,
      "loss": 1.2161,
      "step": 7487
    },
    {
      "epoch": 1.0527203711514128,
      "grad_norm": 1.6620244979858398,
      "learning_rate": 5.295654669655869e-08,
      "loss": 0.9906,
      "step": 7488
    },
    {
      "epoch": 1.0528609588078166,
      "grad_norm": 1.8043627738952637,
      "learning_rate": 6.079116015311747e-08,
      "loss": 0.9619,
      "step": 7489
    },
    {
      "epoch": 1.0530015464642204,
      "grad_norm": 1.8138917684555054,
      "learning_rate": 6.916586546449333e-08,
      "loss": 1.0194,
      "step": 7490
    },
    {
      "epoch": 1.0531421341206242,
      "grad_norm": 1.6782792806625366,
      "learning_rate": 7.808061737207828e-08,
      "loss": 1.1349,
      "step": 7491
    },
    {
      "epoch": 1.053282721777028,
      "grad_norm": 1.4648350477218628,
      "learning_rate": 8.753536769874337e-08,
      "loss": 1.0809,
      "step": 7492
    },
    {
      "epoch": 1.0534233094334318,
      "grad_norm": 1.4836149215698242,
      "learning_rate": 9.753006534904962e-08,
      "loss": 1.1695,
      "step": 7493
    },
    {
      "epoch": 1.0535638970898356,
      "grad_norm": 1.7466017007827759,
      "learning_rate": 1.0806465630965879e-07,
      "loss": 0.9852,
      "step": 7494
    },
    {
      "epoch": 1.0537044847462393,
      "grad_norm": 1.332489013671875,
      "learning_rate": 1.1913908364948879e-07,
      "loss": 1.2166,
      "step": 7495
    },
    {
      "epoch": 1.0538450724026431,
      "grad_norm": 1.6642223596572876,
      "learning_rate": 1.307532875200579e-07,
      "loss": 0.9873,
      "step": 7496
    },
    {
      "epoch": 1.053985660059047,
      "grad_norm": 1.9976531267166138,
      "learning_rate": 1.429072051558511e-07,
      "loss": 1.1523,
      "step": 7497
    },
    {
      "epoch": 1.0541262477154505,
      "grad_norm": 1.533713459968567,
      "learning_rate": 1.556007708746199e-07,
      "loss": 0.9338,
      "step": 7498
    },
    {
      "epoch": 1.0542668353718543,
      "grad_norm": 1.2866524457931519,
      "learning_rate": 1.6883391607773746e-07,
      "loss": 1.0839,
      "step": 7499
    },
    {
      "epoch": 1.054407423028258,
      "grad_norm": 1.5042338371276855,
      "learning_rate": 1.826065692506096e-07,
      "loss": 0.9863,
      "step": 7500
    },
    {
      "epoch": 1.054407423028258,
      "eval_loss": 1.1404186487197876,
      "eval_runtime": 771.6905,
      "eval_samples_per_second": 16.387,
      "eval_steps_per_second": 8.194,
      "step": 7500
    },
    {
      "epoch": 1.0545480106846619,
      "grad_norm": 1.2974368333816528,
      "learning_rate": 1.9691865596299653e-07,
      "loss": 1.0749,
      "step": 7501
    },
    {
      "epoch": 1.0546885983410657,
      "grad_norm": 1.6719281673431396,
      "learning_rate": 2.1177009886946818e-07,
      "loss": 1.0733,
      "step": 7502
    },
    {
      "epoch": 1.0548291859974694,
      "grad_norm": 1.4457497596740723,
      "learning_rate": 2.2716081770981502e-07,
      "loss": 1.0203,
      "step": 7503
    },
    {
      "epoch": 1.0549697736538732,
      "grad_norm": 1.5708450078964233,
      "learning_rate": 2.430907293094031e-07,
      "loss": 0.9572,
      "step": 7504
    },
    {
      "epoch": 1.055110361310277,
      "grad_norm": 1.9069745540618896,
      "learning_rate": 2.595597475798073e-07,
      "loss": 1.1248,
      "step": 7505
    },
    {
      "epoch": 1.0552509489666808,
      "grad_norm": 1.483353614807129,
      "learning_rate": 2.765677835190772e-07,
      "loss": 1.0511,
      "step": 7506
    },
    {
      "epoch": 1.0553915366230844,
      "grad_norm": 1.3298616409301758,
      "learning_rate": 2.94114745212315e-07,
      "loss": 1.1297,
      "step": 7507
    },
    {
      "epoch": 1.0555321242794882,
      "grad_norm": 1.4353033304214478,
      "learning_rate": 3.122005378321524e-07,
      "loss": 0.9861,
      "step": 7508
    },
    {
      "epoch": 1.055672711935892,
      "grad_norm": 1.5081331729888916,
      "learning_rate": 3.308250636392618e-07,
      "loss": 1.0269,
      "step": 7509
    },
    {
      "epoch": 1.0558132995922958,
      "grad_norm": 1.5431466102600098,
      "learning_rate": 3.4998822198295535e-07,
      "loss": 1.0362,
      "step": 7510
    },
    {
      "epoch": 1.0559538872486995,
      "grad_norm": 1.5416865348815918,
      "learning_rate": 3.696899093016293e-07,
      "loss": 0.9684,
      "step": 7511
    },
    {
      "epoch": 1.0560944749051033,
      "grad_norm": 1.576179027557373,
      "learning_rate": 3.8993001912329683e-07,
      "loss": 0.9967,
      "step": 7512
    },
    {
      "epoch": 1.0562350625615071,
      "grad_norm": 1.7365208864212036,
      "learning_rate": 4.1070844206629877e-07,
      "loss": 0.9319,
      "step": 7513
    },
    {
      "epoch": 1.056375650217911,
      "grad_norm": 1.5179431438446045,
      "learning_rate": 4.3202506583981393e-07,
      "loss": 0.9037,
      "step": 7514
    },
    {
      "epoch": 1.0565162378743147,
      "grad_norm": 1.5765818357467651,
      "learning_rate": 4.5387977524443684e-07,
      "loss": 0.9666,
      "step": 7515
    },
    {
      "epoch": 1.0566568255307185,
      "grad_norm": 1.3905575275421143,
      "learning_rate": 4.7627245217288786e-07,
      "loss": 1.1203,
      "step": 7516
    },
    {
      "epoch": 1.0567974131871223,
      "grad_norm": 1.4948029518127441,
      "learning_rate": 4.992029756105798e-07,
      "loss": 1.0026,
      "step": 7517
    },
    {
      "epoch": 1.0569380008435258,
      "grad_norm": 1.3580808639526367,
      "learning_rate": 5.226712216362951e-07,
      "loss": 0.9722,
      "step": 7518
    },
    {
      "epoch": 1.0570785884999296,
      "grad_norm": 1.254178524017334,
      "learning_rate": 5.46677063422818e-07,
      "loss": 1.1195,
      "step": 7519
    },
    {
      "epoch": 1.0572191761563334,
      "grad_norm": 1.424198865890503,
      "learning_rate": 5.712203712376685e-07,
      "loss": 1.096,
      "step": 7520
    },
    {
      "epoch": 1.0573597638127372,
      "grad_norm": 1.4470114707946777,
      "learning_rate": 5.963010124438784e-07,
      "loss": 1.0065,
      "step": 7521
    },
    {
      "epoch": 1.057500351469141,
      "grad_norm": 1.2995045185089111,
      "learning_rate": 6.219188515005358e-07,
      "loss": 1.0214,
      "step": 7522
    },
    {
      "epoch": 1.0576409391255448,
      "grad_norm": 1.494919776916504,
      "learning_rate": 6.480737499635625e-07,
      "loss": 1.1944,
      "step": 7523
    },
    {
      "epoch": 1.0577815267819486,
      "grad_norm": 1.2921452522277832,
      "learning_rate": 6.747655664866015e-07,
      "loss": 1.0667,
      "step": 7524
    },
    {
      "epoch": 1.0579221144383524,
      "grad_norm": 1.4504281282424927,
      "learning_rate": 7.019941568216282e-07,
      "loss": 1.2005,
      "step": 7525
    },
    {
      "epoch": 1.0580627020947562,
      "grad_norm": 1.4284292459487915,
      "learning_rate": 7.297593738197606e-07,
      "loss": 0.9898,
      "step": 7526
    },
    {
      "epoch": 1.0582032897511597,
      "grad_norm": 1.3166258335113525,
      "learning_rate": 7.580610674321475e-07,
      "loss": 0.9969,
      "step": 7527
    },
    {
      "epoch": 1.0583438774075635,
      "grad_norm": 1.429358720779419,
      "learning_rate": 7.868990847106573e-07,
      "loss": 1.0495,
      "step": 7528
    },
    {
      "epoch": 1.0584844650639673,
      "grad_norm": 1.4984984397888184,
      "learning_rate": 8.16273269808765e-07,
      "loss": 1.0634,
      "step": 7529
    },
    {
      "epoch": 1.058625052720371,
      "grad_norm": 1.5670768022537231,
      "learning_rate": 8.461834639823974e-07,
      "loss": 1.0432,
      "step": 7530
    },
    {
      "epoch": 1.058765640376775,
      "grad_norm": 1.8118327856063843,
      "learning_rate": 8.766295055906981e-07,
      "loss": 1.024,
      "step": 7531
    },
    {
      "epoch": 1.0589062280331787,
      "grad_norm": 1.550458550453186,
      "learning_rate": 9.076112300971496e-07,
      "loss": 1.0865,
      "step": 7532
    },
    {
      "epoch": 1.0590468156895825,
      "grad_norm": 1.3819161653518677,
      "learning_rate": 9.39128470070183e-07,
      "loss": 1.0977,
      "step": 7533
    },
    {
      "epoch": 1.0591874033459863,
      "grad_norm": 1.540151596069336,
      "learning_rate": 9.71181055184156e-07,
      "loss": 1.0664,
      "step": 7534
    },
    {
      "epoch": 1.05932799100239,
      "grad_norm": 1.5149235725402832,
      "learning_rate": 1.0037688122203958e-06,
      "loss": 1.1502,
      "step": 7535
    },
    {
      "epoch": 1.0594685786587938,
      "grad_norm": 1.4674099683761597,
      "learning_rate": 1.0368915650679767e-06,
      "loss": 1.0071,
      "step": 7536
    },
    {
      "epoch": 1.0596091663151976,
      "grad_norm": 1.592854619026184,
      "learning_rate": 1.0705491347249074e-06,
      "loss": 1.0613,
      "step": 7537
    },
    {
      "epoch": 1.0597497539716012,
      "grad_norm": 1.4145671129226685,
      "learning_rate": 1.1047413392987095e-06,
      "loss": 0.9335,
      "step": 7538
    },
    {
      "epoch": 1.059890341628005,
      "grad_norm": 1.6950422525405884,
      "learning_rate": 1.1394679940077923e-06,
      "loss": 1.2074,
      "step": 7539
    },
    {
      "epoch": 1.0600309292844088,
      "grad_norm": 1.486403226852417,
      "learning_rate": 1.1747289111822212e-06,
      "loss": 1.1004,
      "step": 7540
    },
    {
      "epoch": 1.0601715169408126,
      "grad_norm": 1.4569374322891235,
      "learning_rate": 1.2105239002648371e-06,
      "loss": 1.0257,
      "step": 7541
    },
    {
      "epoch": 1.0603121045972164,
      "grad_norm": 1.5236237049102783,
      "learning_rate": 1.2468527678121123e-06,
      "loss": 0.9773,
      "step": 7542
    },
    {
      "epoch": 1.0604526922536202,
      "grad_norm": 1.706689476966858,
      "learning_rate": 1.2837153174955819e-06,
      "loss": 1.0951,
      "step": 7543
    },
    {
      "epoch": 1.060593279910024,
      "grad_norm": 1.4382357597351074,
      "learning_rate": 1.3211113501024662e-06,
      "loss": 1.1478,
      "step": 7544
    },
    {
      "epoch": 1.0607338675664277,
      "grad_norm": 1.5257270336151123,
      "learning_rate": 1.3590406635370035e-06,
      "loss": 1.206,
      "step": 7545
    },
    {
      "epoch": 1.0608744552228315,
      "grad_norm": 1.4588719606399536,
      "learning_rate": 1.3975030528214028e-06,
      "loss": 0.901,
      "step": 7546
    },
    {
      "epoch": 1.061015042879235,
      "grad_norm": 1.6819897890090942,
      "learning_rate": 1.4364983100970231e-06,
      "loss": 0.9701,
      "step": 7547
    },
    {
      "epoch": 1.0611556305356389,
      "grad_norm": 1.3491835594177246,
      "learning_rate": 1.4760262246256928e-06,
      "loss": 0.9524,
      "step": 7548
    },
    {
      "epoch": 1.0612962181920427,
      "grad_norm": 1.4729050397872925,
      "learning_rate": 1.5160865827904548e-06,
      "loss": 1.0575,
      "step": 7549
    },
    {
      "epoch": 1.0614368058484465,
      "grad_norm": 1.4277278184890747,
      "learning_rate": 1.5566791680969195e-06,
      "loss": 0.9526,
      "step": 7550
    },
    {
      "epoch": 1.0615773935048503,
      "grad_norm": 1.425094723701477,
      "learning_rate": 1.597803761174499e-06,
      "loss": 1.0199,
      "step": 7551
    },
    {
      "epoch": 1.061717981161254,
      "grad_norm": 1.413521647453308,
      "learning_rate": 1.6394601397775268e-06,
      "loss": 1.0862,
      "step": 7552
    },
    {
      "epoch": 1.0618585688176578,
      "grad_norm": 1.4185785055160522,
      "learning_rate": 1.681648078786391e-06,
      "loss": 1.1517,
      "step": 7553
    },
    {
      "epoch": 1.0619991564740616,
      "grad_norm": 1.8018213510513306,
      "learning_rate": 1.7243673502089553e-06,
      "loss": 1.0697,
      "step": 7554
    },
    {
      "epoch": 1.0621397441304654,
      "grad_norm": 1.6009984016418457,
      "learning_rate": 1.767617723181514e-06,
      "loss": 1.041,
      "step": 7555
    },
    {
      "epoch": 1.0622803317868692,
      "grad_norm": 1.4294462203979492,
      "learning_rate": 1.8113989639702899e-06,
      "loss": 1.0573,
      "step": 7556
    },
    {
      "epoch": 1.062420919443273,
      "grad_norm": 1.774080753326416,
      "learning_rate": 1.8557108359724685e-06,
      "loss": 1.136,
      "step": 7557
    },
    {
      "epoch": 1.0625615070996766,
      "grad_norm": 1.3736109733581543,
      "learning_rate": 1.9005530997176058e-06,
      "loss": 0.9643,
      "step": 7558
    },
    {
      "epoch": 1.0627020947560804,
      "grad_norm": 1.544493317604065,
      "learning_rate": 1.9459255128691177e-06,
      "loss": 0.9906,
      "step": 7559
    },
    {
      "epoch": 1.0628426824124841,
      "grad_norm": 1.3348678350448608,
      "learning_rate": 1.9918278302252126e-06,
      "loss": 1.2223,
      "step": 7560
    },
    {
      "epoch": 1.062983270068888,
      "grad_norm": 1.7784899473190308,
      "learning_rate": 2.0382598037203215e-06,
      "loss": 1.0488,
      "step": 7561
    },
    {
      "epoch": 1.0631238577252917,
      "grad_norm": 1.566510558128357,
      "learning_rate": 2.0852211824266445e-06,
      "loss": 1.1453,
      "step": 7562
    },
    {
      "epoch": 1.0632644453816955,
      "grad_norm": 1.61956787109375,
      "learning_rate": 2.1327117125552352e-06,
      "loss": 1.058,
      "step": 7563
    },
    {
      "epoch": 1.0634050330380993,
      "grad_norm": 1.7936275005340576,
      "learning_rate": 2.180731137457781e-06,
      "loss": 1.0814,
      "step": 7564
    },
    {
      "epoch": 1.063545620694503,
      "grad_norm": 1.4486342668533325,
      "learning_rate": 2.2292791976273653e-06,
      "loss": 1.1498,
      "step": 7565
    },
    {
      "epoch": 1.0636862083509069,
      "grad_norm": 1.7238483428955078,
      "learning_rate": 2.278355630700424e-06,
      "loss": 0.9045,
      "step": 7566
    },
    {
      "epoch": 1.0638267960073104,
      "grad_norm": 1.6770517826080322,
      "learning_rate": 2.3279601714579103e-06,
      "loss": 1.1191,
      "step": 7567
    },
    {
      "epoch": 1.0639673836637142,
      "grad_norm": 1.6059991121292114,
      "learning_rate": 2.378092551826805e-06,
      "loss": 1.0752,
      "step": 7568
    },
    {
      "epoch": 1.064107971320118,
      "grad_norm": 1.4534837007522583,
      "learning_rate": 2.4287525008813594e-06,
      "loss": 1.0446,
      "step": 7569
    },
    {
      "epoch": 1.0642485589765218,
      "grad_norm": 1.582134485244751,
      "learning_rate": 2.4799397448450614e-06,
      "loss": 1.0361,
      "step": 7570
    },
    {
      "epoch": 1.0643891466329256,
      "grad_norm": 1.5538090467453003,
      "learning_rate": 2.531654007091555e-06,
      "loss": 1.0726,
      "step": 7571
    },
    {
      "epoch": 1.0645297342893294,
      "grad_norm": 1.3152004480361938,
      "learning_rate": 2.583895008146453e-06,
      "loss": 1.165,
      "step": 7572
    },
    {
      "epoch": 1.0646703219457332,
      "grad_norm": 2.4225685596466064,
      "learning_rate": 2.6366624656886554e-06,
      "loss": 1.0993,
      "step": 7573
    },
    {
      "epoch": 1.064810909602137,
      "grad_norm": 1.7924308776855469,
      "learning_rate": 2.6899560945520375e-06,
      "loss": 0.9629,
      "step": 7574
    },
    {
      "epoch": 1.0649514972585408,
      "grad_norm": 1.3300851583480835,
      "learning_rate": 2.7437756067271504e-06,
      "loss": 1.2103,
      "step": 7575
    },
    {
      "epoch": 1.0650920849149446,
      "grad_norm": 1.765349268913269,
      "learning_rate": 2.7981207113622398e-06,
      "loss": 1.0278,
      "step": 7576
    },
    {
      "epoch": 1.0652326725713483,
      "grad_norm": 1.3900808095932007,
      "learning_rate": 2.852991114765313e-06,
      "loss": 1.188,
      "step": 7577
    },
    {
      "epoch": 1.065373260227752,
      "grad_norm": 1.497376561164856,
      "learning_rate": 2.908386520405526e-06,
      "loss": 1.0335,
      "step": 7578
    },
    {
      "epoch": 1.0655138478841557,
      "grad_norm": 1.7596478462219238,
      "learning_rate": 2.9643066289147702e-06,
      "loss": 0.9678,
      "step": 7579
    },
    {
      "epoch": 1.0656544355405595,
      "grad_norm": 1.4742902517318726,
      "learning_rate": 3.0207511380894725e-06,
      "loss": 1.0469,
      "step": 7580
    },
    {
      "epoch": 1.0657950231969633,
      "grad_norm": 1.280154824256897,
      "learning_rate": 3.07771974289196e-06,
      "loss": 1.0835,
      "step": 7581
    },
    {
      "epoch": 1.065935610853367,
      "grad_norm": 1.5031871795654297,
      "learning_rate": 3.13521213545227e-06,
      "loss": 1.2168,
      "step": 7582
    },
    {
      "epoch": 1.0660761985097709,
      "grad_norm": 1.3833279609680176,
      "learning_rate": 3.193228005069848e-06,
      "loss": 1.1682,
      "step": 7583
    },
    {
      "epoch": 1.0662167861661747,
      "grad_norm": 1.4870816469192505,
      "learning_rate": 3.2517670382150147e-06,
      "loss": 0.999,
      "step": 7584
    },
    {
      "epoch": 1.0663573738225784,
      "grad_norm": 1.7917933464050293,
      "learning_rate": 3.3108289185308616e-06,
      "loss": 1.1514,
      "step": 7585
    },
    {
      "epoch": 1.0664979614789822,
      "grad_norm": 1.4415546655654907,
      "learning_rate": 3.370413326835131e-06,
      "loss": 0.9552,
      "step": 7586
    },
    {
      "epoch": 1.0666385491353858,
      "grad_norm": 1.6579047441482544,
      "learning_rate": 3.4305199411214796e-06,
      "loss": 1.0642,
      "step": 7587
    },
    {
      "epoch": 1.0667791367917896,
      "grad_norm": 1.4309881925582886,
      "learning_rate": 3.491148436561442e-06,
      "loss": 1.3123,
      "step": 7588
    },
    {
      "epoch": 1.0669197244481934,
      "grad_norm": 1.5368642807006836,
      "learning_rate": 3.5522984855063e-06,
      "loss": 1.0017,
      "step": 7589
    },
    {
      "epoch": 1.0670603121045972,
      "grad_norm": 1.4481183290481567,
      "learning_rate": 3.613969757488678e-06,
      "loss": 0.9283,
      "step": 7590
    },
    {
      "epoch": 1.067200899761001,
      "grad_norm": 1.7736029624938965,
      "learning_rate": 3.6761619192245523e-06,
      "loss": 1.0302,
      "step": 7591
    },
    {
      "epoch": 1.0673414874174048,
      "grad_norm": 1.8456751108169556,
      "learning_rate": 3.738874634614753e-06,
      "loss": 1.1016,
      "step": 7592
    },
    {
      "epoch": 1.0674820750738085,
      "grad_norm": 1.4660898447036743,
      "learning_rate": 3.8021075647469927e-06,
      "loss": 1.0502,
      "step": 7593
    },
    {
      "epoch": 1.0676226627302123,
      "grad_norm": 1.299400806427002,
      "learning_rate": 3.8658603678976665e-06,
      "loss": 1.3496,
      "step": 7594
    },
    {
      "epoch": 1.0677632503866161,
      "grad_norm": 1.6917648315429688,
      "learning_rate": 3.930132699533551e-06,
      "loss": 1.1829,
      "step": 7595
    },
    {
      "epoch": 1.06790383804302,
      "grad_norm": 1.5883735418319702,
      "learning_rate": 3.994924212313788e-06,
      "loss": 0.9924,
      "step": 7596
    },
    {
      "epoch": 1.0680444256994237,
      "grad_norm": 1.5358706712722778,
      "learning_rate": 4.060234556092057e-06,
      "loss": 0.992,
      "step": 7597
    },
    {
      "epoch": 1.0681850133558273,
      "grad_norm": 1.4552007913589478,
      "learning_rate": 4.126063377917888e-06,
      "loss": 1.228,
      "step": 7598
    },
    {
      "epoch": 1.068325601012231,
      "grad_norm": 1.4546085596084595,
      "learning_rate": 4.192410322038809e-06,
      "loss": 1.0248,
      "step": 7599
    },
    {
      "epoch": 1.0684661886686349,
      "grad_norm": 2.0269877910614014,
      "learning_rate": 4.259275029902532e-06,
      "loss": 1.0181,
      "step": 7600
    },
    {
      "epoch": 1.0686067763250386,
      "grad_norm": 1.4507973194122314,
      "learning_rate": 4.3266571401583075e-06,
      "loss": 0.9566,
      "step": 7601
    },
    {
      "epoch": 1.0687473639814424,
      "grad_norm": 1.5106302499771118,
      "learning_rate": 4.394556288659768e-06,
      "loss": 0.9271,
      "step": 7602
    },
    {
      "epoch": 1.0688879516378462,
      "grad_norm": 1.9653428792953491,
      "learning_rate": 4.4629721084659455e-06,
      "loss": 0.9206,
      "step": 7603
    },
    {
      "epoch": 1.06902853929425,
      "grad_norm": 1.5793572664260864,
      "learning_rate": 4.531904229843831e-06,
      "loss": 1.2006,
      "step": 7604
    },
    {
      "epoch": 1.0691691269506538,
      "grad_norm": 1.83607816696167,
      "learning_rate": 4.601352280270166e-06,
      "loss": 0.9449,
      "step": 7605
    },
    {
      "epoch": 1.0693097146070576,
      "grad_norm": 1.3722200393676758,
      "learning_rate": 4.671315884433436e-06,
      "loss": 0.9071,
      "step": 7606
    },
    {
      "epoch": 1.0694503022634612,
      "grad_norm": 1.4874130487442017,
      "learning_rate": 4.741794664236277e-06,
      "loss": 0.8247,
      "step": 7607
    },
    {
      "epoch": 1.069590889919865,
      "grad_norm": 1.3872942924499512,
      "learning_rate": 4.812788238796995e-06,
      "loss": 0.9046,
      "step": 7608
    },
    {
      "epoch": 1.0697314775762687,
      "grad_norm": 1.3905807733535767,
      "learning_rate": 4.884296224451623e-06,
      "loss": 0.9551,
      "step": 7609
    },
    {
      "epoch": 1.0698720652326725,
      "grad_norm": 1.5343624353408813,
      "learning_rate": 4.956318234756551e-06,
      "loss": 1.004,
      "step": 7610
    },
    {
      "epoch": 1.0700126528890763,
      "grad_norm": 1.3348400592803955,
      "learning_rate": 5.028853880490158e-06,
      "loss": 1.0871,
      "step": 7611
    },
    {
      "epoch": 1.0701532405454801,
      "grad_norm": 1.5138661861419678,
      "learning_rate": 5.101902769654898e-06,
      "loss": 0.8787,
      "step": 7612
    },
    {
      "epoch": 1.070293828201884,
      "grad_norm": 1.6948344707489014,
      "learning_rate": 5.175464507479921e-06,
      "loss": 1.0106,
      "step": 7613
    },
    {
      "epoch": 1.0704344158582877,
      "grad_norm": 1.3809024095535278,
      "learning_rate": 5.249538696422496e-06,
      "loss": 1.002,
      "step": 7614
    },
    {
      "epoch": 1.0705750035146915,
      "grad_norm": 1.4856010675430298,
      "learning_rate": 5.3241249361707045e-06,
      "loss": 1.0372,
      "step": 7615
    },
    {
      "epoch": 1.0707155911710953,
      "grad_norm": 1.692852258682251,
      "learning_rate": 5.3992228236451005e-06,
      "loss": 1.0873,
      "step": 7616
    },
    {
      "epoch": 1.070856178827499,
      "grad_norm": 1.7191356420516968,
      "learning_rate": 5.4748319530013695e-06,
      "loss": 1.0444,
      "step": 7617
    },
    {
      "epoch": 1.0709967664839026,
      "grad_norm": 1.7170023918151855,
      "learning_rate": 5.5509519156325964e-06,
      "loss": 0.9127,
      "step": 7618
    },
    {
      "epoch": 1.0711373541403064,
      "grad_norm": 1.3879516124725342,
      "learning_rate": 5.627582300170886e-06,
      "loss": 0.9728,
      "step": 7619
    },
    {
      "epoch": 1.0712779417967102,
      "grad_norm": 1.484866976737976,
      "learning_rate": 5.7047226924900165e-06,
      "loss": 0.9382,
      "step": 7620
    },
    {
      "epoch": 1.071418529453114,
      "grad_norm": 1.4223469495773315,
      "learning_rate": 5.782372675707582e-06,
      "loss": 1.1235,
      "step": 7621
    },
    {
      "epoch": 1.0715591171095178,
      "grad_norm": 1.5206254720687866,
      "learning_rate": 5.860531830187133e-06,
      "loss": 1.0274,
      "step": 7622
    },
    {
      "epoch": 1.0716997047659216,
      "grad_norm": 1.458348274230957,
      "learning_rate": 5.939199733541023e-06,
      "loss": 0.9888,
      "step": 7623
    },
    {
      "epoch": 1.0718402924223254,
      "grad_norm": 1.6202799081802368,
      "learning_rate": 6.018375960631495e-06,
      "loss": 1.1183,
      "step": 7624
    },
    {
      "epoch": 1.0719808800787292,
      "grad_norm": 1.7359994649887085,
      "learning_rate": 6.09806008357422e-06,
      "loss": 0.9802,
      "step": 7625
    },
    {
      "epoch": 1.072121467735133,
      "grad_norm": 1.4082937240600586,
      "learning_rate": 6.178251671739943e-06,
      "loss": 1.1181,
      "step": 7626
    },
    {
      "epoch": 1.0722620553915365,
      "grad_norm": 1.439765453338623,
      "learning_rate": 6.258950291756982e-06,
      "loss": 0.8491,
      "step": 7627
    },
    {
      "epoch": 1.0724026430479403,
      "grad_norm": 1.5195555686950684,
      "learning_rate": 6.3401555075134235e-06,
      "loss": 1.2427,
      "step": 7628
    },
    {
      "epoch": 1.072543230704344,
      "grad_norm": 1.510003924369812,
      "learning_rate": 6.421866880159977e-06,
      "loss": 1.0141,
      "step": 7629
    },
    {
      "epoch": 1.0726838183607479,
      "grad_norm": 1.5310614109039307,
      "learning_rate": 6.504083968111729e-06,
      "loss": 1.1886,
      "step": 7630
    },
    {
      "epoch": 1.0728244060171517,
      "grad_norm": 1.775181770324707,
      "learning_rate": 6.586806327050521e-06,
      "loss": 1.0025,
      "step": 7631
    },
    {
      "epoch": 1.0729649936735555,
      "grad_norm": 1.4907737970352173,
      "learning_rate": 6.6700335099279974e-06,
      "loss": 0.8439,
      "step": 7632
    },
    {
      "epoch": 1.0731055813299593,
      "grad_norm": 1.3737210035324097,
      "learning_rate": 6.753765066967377e-06,
      "loss": 0.9412,
      "step": 7633
    },
    {
      "epoch": 1.073246168986363,
      "grad_norm": 1.5563627481460571,
      "learning_rate": 6.83800054566649e-06,
      "loss": 1.0019,
      "step": 7634
    },
    {
      "epoch": 1.0733867566427668,
      "grad_norm": 1.5916659832000732,
      "learning_rate": 6.9227394907995454e-06,
      "loss": 0.8518,
      "step": 7635
    },
    {
      "epoch": 1.0735273442991706,
      "grad_norm": 1.568564772605896,
      "learning_rate": 7.0079814444199866e-06,
      "loss": 1.0063,
      "step": 7636
    },
    {
      "epoch": 1.0736679319555744,
      "grad_norm": 1.556060552597046,
      "learning_rate": 7.093725945862928e-06,
      "loss": 0.9542,
      "step": 7637
    },
    {
      "epoch": 1.073808519611978,
      "grad_norm": 1.515141248703003,
      "learning_rate": 7.179972531747747e-06,
      "loss": 0.924,
      "step": 7638
    },
    {
      "epoch": 1.0739491072683818,
      "grad_norm": 1.4444221258163452,
      "learning_rate": 7.266720735979915e-06,
      "loss": 1.0952,
      "step": 7639
    },
    {
      "epoch": 1.0740896949247856,
      "grad_norm": 1.8240028619766235,
      "learning_rate": 7.3539700897548356e-06,
      "loss": 1.023,
      "step": 7640
    },
    {
      "epoch": 1.0742302825811894,
      "grad_norm": 1.3686672449111938,
      "learning_rate": 7.4417201215591145e-06,
      "loss": 1.0353,
      "step": 7641
    },
    {
      "epoch": 1.0743708702375931,
      "grad_norm": 1.5114552974700928,
      "learning_rate": 7.529970357173665e-06,
      "loss": 0.8997,
      "step": 7642
    },
    {
      "epoch": 1.074511457893997,
      "grad_norm": 1.2720465660095215,
      "learning_rate": 7.6187203196762424e-06,
      "loss": 1.0842,
      "step": 7643
    },
    {
      "epoch": 1.0746520455504007,
      "grad_norm": 1.46778404712677,
      "learning_rate": 7.707969529443815e-06,
      "loss": 1.0065,
      "step": 7644
    },
    {
      "epoch": 1.0747926332068045,
      "grad_norm": 1.82071852684021,
      "learning_rate": 7.797717504155743e-06,
      "loss": 1.0178,
      "step": 7645
    },
    {
      "epoch": 1.0749332208632083,
      "grad_norm": 1.5554766654968262,
      "learning_rate": 7.887963758795669e-06,
      "loss": 1.0555,
      "step": 7646
    },
    {
      "epoch": 1.0750738085196119,
      "grad_norm": 1.6054444313049316,
      "learning_rate": 7.978707805654173e-06,
      "loss": 1.1203,
      "step": 7647
    },
    {
      "epoch": 1.0752143961760157,
      "grad_norm": 1.670786738395691,
      "learning_rate": 8.069949154332069e-06,
      "loss": 0.972,
      "step": 7648
    },
    {
      "epoch": 1.0753549838324195,
      "grad_norm": 1.4161821603775024,
      "learning_rate": 8.161687311742361e-06,
      "loss": 1.2104,
      "step": 7649
    },
    {
      "epoch": 1.0754955714888232,
      "grad_norm": 1.5039936304092407,
      "learning_rate": 8.253921782113626e-06,
      "loss": 1.134,
      "step": 7650
    },
    {
      "epoch": 1.075636159145227,
      "grad_norm": 1.4248892068862915,
      "learning_rate": 8.346652066991868e-06,
      "loss": 1.1418,
      "step": 7651
    },
    {
      "epoch": 1.0757767468016308,
      "grad_norm": 1.5301259756088257,
      "learning_rate": 8.4398776652437e-06,
      "loss": 1.0613,
      "step": 7652
    },
    {
      "epoch": 1.0759173344580346,
      "grad_norm": 1.5535019636154175,
      "learning_rate": 8.533598073059147e-06,
      "loss": 1.1629,
      "step": 7653
    },
    {
      "epoch": 1.0760579221144384,
      "grad_norm": 1.6467928886413574,
      "learning_rate": 8.627812783953726e-06,
      "loss": 1.0529,
      "step": 7654
    },
    {
      "epoch": 1.0761985097708422,
      "grad_norm": 1.3641315698623657,
      "learning_rate": 8.722521288771945e-06,
      "loss": 1.0387,
      "step": 7655
    },
    {
      "epoch": 1.076339097427246,
      "grad_norm": 1.46486234664917,
      "learning_rate": 8.81772307569002e-06,
      "loss": 1.1059,
      "step": 7656
    },
    {
      "epoch": 1.0764796850836498,
      "grad_norm": 1.8572417497634888,
      "learning_rate": 8.913417630217946e-06,
      "loss": 1.0231,
      "step": 7657
    },
    {
      "epoch": 1.0766202727400533,
      "grad_norm": 1.571033000946045,
      "learning_rate": 9.009604435202857e-06,
      "loss": 1.1153,
      "step": 7658
    },
    {
      "epoch": 1.0767608603964571,
      "grad_norm": 1.2974190711975098,
      "learning_rate": 9.106282970831636e-06,
      "loss": 1.0674,
      "step": 7659
    },
    {
      "epoch": 1.076901448052861,
      "grad_norm": 1.5318596363067627,
      "learning_rate": 9.203452714633632e-06,
      "loss": 1.2278,
      "step": 7660
    },
    {
      "epoch": 1.0770420357092647,
      "grad_norm": 1.4541805982589722,
      "learning_rate": 9.301113141484196e-06,
      "loss": 1.1907,
      "step": 7661
    },
    {
      "epoch": 1.0771826233656685,
      "grad_norm": 1.4727864265441895,
      "learning_rate": 9.39926372360601e-06,
      "loss": 1.0804,
      "step": 7662
    },
    {
      "epoch": 1.0773232110220723,
      "grad_norm": 1.6566400527954102,
      "learning_rate": 9.497903930573494e-06,
      "loss": 0.9891,
      "step": 7663
    },
    {
      "epoch": 1.077463798678476,
      "grad_norm": 1.7612003087997437,
      "learning_rate": 9.597033229314845e-06,
      "loss": 1.1237,
      "step": 7664
    },
    {
      "epoch": 1.0776043863348799,
      "grad_norm": 1.4051235914230347,
      "learning_rate": 9.696651084114994e-06,
      "loss": 1.2362,
      "step": 7665
    },
    {
      "epoch": 1.0777449739912837,
      "grad_norm": 1.5281331539154053,
      "learning_rate": 9.796756956619035e-06,
      "loss": 1.1692,
      "step": 7666
    },
    {
      "epoch": 1.0778855616476872,
      "grad_norm": 1.5674617290496826,
      "learning_rate": 9.897350305834286e-06,
      "loss": 1.0567,
      "step": 7667
    },
    {
      "epoch": 1.078026149304091,
      "grad_norm": 1.777355432510376,
      "learning_rate": 9.998430588133777e-06,
      "loss": 1.2109,
      "step": 7668
    },
    {
      "epoch": 1.0781667369604948,
      "grad_norm": 1.6983052492141724,
      "learning_rate": 1.009999725725923e-05,
      "loss": 1.0156,
      "step": 7669
    },
    {
      "epoch": 1.0783073246168986,
      "grad_norm": 1.6042579412460327,
      "learning_rate": 1.020204976432334e-05,
      "loss": 1.0764,
      "step": 7670
    },
    {
      "epoch": 1.0784479122733024,
      "grad_norm": 1.5201841592788696,
      "learning_rate": 1.0304587557813562e-05,
      "loss": 1.0873,
      "step": 7671
    },
    {
      "epoch": 1.0785884999297062,
      "grad_norm": 1.6250780820846558,
      "learning_rate": 1.0407610083595065e-05,
      "loss": 1.0437,
      "step": 7672
    },
    {
      "epoch": 1.07872908758611,
      "grad_norm": 1.594515323638916,
      "learning_rate": 1.0511116784912966e-05,
      "loss": 1.1352,
      "step": 7673
    },
    {
      "epoch": 1.0788696752425138,
      "grad_norm": 1.4831093549728394,
      "learning_rate": 1.0615107102395972e-05,
      "loss": 1.1166,
      "step": 7674
    },
    {
      "epoch": 1.0790102628989175,
      "grad_norm": 1.5498623847961426,
      "learning_rate": 1.0719580474059187e-05,
      "loss": 1.0897,
      "step": 7675
    },
    {
      "epoch": 1.0791508505553213,
      "grad_norm": 1.7373263835906982,
      "learning_rate": 1.0824536335307223e-05,
      "loss": 0.9588,
      "step": 7676
    },
    {
      "epoch": 1.079291438211725,
      "grad_norm": 1.4593071937561035,
      "learning_rate": 1.0929974118937225e-05,
      "loss": 1.1108,
      "step": 7677
    },
    {
      "epoch": 1.0794320258681287,
      "grad_norm": 1.3457814455032349,
      "learning_rate": 1.1035893255141905e-05,
      "loss": 1.0002,
      "step": 7678
    },
    {
      "epoch": 1.0795726135245325,
      "grad_norm": 1.5281860828399658,
      "learning_rate": 1.1142293171512675e-05,
      "loss": 0.9076,
      "step": 7679
    },
    {
      "epoch": 1.0797132011809363,
      "grad_norm": 1.4744161367416382,
      "learning_rate": 1.1249173293042681e-05,
      "loss": 1.1197,
      "step": 7680
    },
    {
      "epoch": 1.07985378883734,
      "grad_norm": 1.5899327993392944,
      "learning_rate": 1.135653304212998e-05,
      "loss": 1.0208,
      "step": 7681
    },
    {
      "epoch": 1.0799943764937439,
      "grad_norm": 1.7623028755187988,
      "learning_rate": 1.1464371838580435e-05,
      "loss": 1.0879,
      "step": 7682
    },
    {
      "epoch": 1.0801349641501476,
      "grad_norm": 1.3190147876739502,
      "learning_rate": 1.1572689099611534e-05,
      "loss": 1.1516,
      "step": 7683
    },
    {
      "epoch": 1.0802755518065514,
      "grad_norm": 1.3950145244598389,
      "learning_rate": 1.1681484239854722e-05,
      "loss": 1.0804,
      "step": 7684
    },
    {
      "epoch": 1.0804161394629552,
      "grad_norm": 1.561371922492981,
      "learning_rate": 1.179075667135856e-05,
      "loss": 1.0876,
      "step": 7685
    },
    {
      "epoch": 1.080556727119359,
      "grad_norm": 1.4008588790893555,
      "learning_rate": 1.1900505803592721e-05,
      "loss": 0.9894,
      "step": 7686
    },
    {
      "epoch": 1.0806973147757626,
      "grad_norm": 1.3317615985870361,
      "learning_rate": 1.2010731043450352e-05,
      "loss": 1.1801,
      "step": 7687
    },
    {
      "epoch": 1.0808379024321664,
      "grad_norm": 1.5143578052520752,
      "learning_rate": 1.2121431795252124e-05,
      "loss": 1.1757,
      "step": 7688
    },
    {
      "epoch": 1.0809784900885702,
      "grad_norm": 1.612565040588379,
      "learning_rate": 1.2232607460748447e-05,
      "loss": 0.9203,
      "step": 7689
    },
    {
      "epoch": 1.081119077744974,
      "grad_norm": 1.3897243738174438,
      "learning_rate": 1.2344257439123363e-05,
      "loss": 1.0998,
      "step": 7690
    },
    {
      "epoch": 1.0812596654013777,
      "grad_norm": 1.410320520401001,
      "learning_rate": 1.2456381126997808e-05,
      "loss": 1.1078,
      "step": 7691
    },
    {
      "epoch": 1.0814002530577815,
      "grad_norm": 1.5895731449127197,
      "learning_rate": 1.256897791843199e-05,
      "loss": 1.036,
      "step": 7692
    },
    {
      "epoch": 1.0815408407141853,
      "grad_norm": 1.775536298751831,
      "learning_rate": 1.2682047204930436e-05,
      "loss": 1.0958,
      "step": 7693
    },
    {
      "epoch": 1.0816814283705891,
      "grad_norm": 1.518997073173523,
      "learning_rate": 1.2795588375443478e-05,
      "loss": 0.983,
      "step": 7694
    },
    {
      "epoch": 1.081822016026993,
      "grad_norm": 1.3921091556549072,
      "learning_rate": 1.2909600816371504e-05,
      "loss": 1.0256,
      "step": 7695
    },
    {
      "epoch": 1.0819626036833967,
      "grad_norm": 2.28194522857666,
      "learning_rate": 1.3024083911568042e-05,
      "loss": 0.922,
      "step": 7696
    },
    {
      "epoch": 1.0821031913398003,
      "grad_norm": 1.5586000680923462,
      "learning_rate": 1.3139037042343116e-05,
      "loss": 1.0506,
      "step": 7697
    },
    {
      "epoch": 1.082243778996204,
      "grad_norm": 1.2755956649780273,
      "learning_rate": 1.3254459587466383e-05,
      "loss": 1.0806,
      "step": 7698
    },
    {
      "epoch": 1.0823843666526078,
      "grad_norm": 1.8547762632369995,
      "learning_rate": 1.3370350923171426e-05,
      "loss": 1.0096,
      "step": 7699
    },
    {
      "epoch": 1.0825249543090116,
      "grad_norm": 1.7242828607559204,
      "learning_rate": 1.3486710423157323e-05,
      "loss": 0.895,
      "step": 7700
    },
    {
      "epoch": 1.0826655419654154,
      "grad_norm": 1.3552380800247192,
      "learning_rate": 1.3603537458593774e-05,
      "loss": 0.9061,
      "step": 7701
    },
    {
      "epoch": 1.0828061296218192,
      "grad_norm": 1.6830592155456543,
      "learning_rate": 1.3720831398123678e-05,
      "loss": 1.1384,
      "step": 7702
    },
    {
      "epoch": 1.082946717278223,
      "grad_norm": 1.469437599182129,
      "learning_rate": 1.3838591607866502e-05,
      "loss": 1.0796,
      "step": 7703
    },
    {
      "epoch": 1.0830873049346268,
      "grad_norm": 1.4056177139282227,
      "learning_rate": 1.3956817451422421e-05,
      "loss": 0.9391,
      "step": 7704
    },
    {
      "epoch": 1.0832278925910306,
      "grad_norm": 1.5958421230316162,
      "learning_rate": 1.4075508289874717e-05,
      "loss": 1.0319,
      "step": 7705
    },
    {
      "epoch": 1.0833684802474344,
      "grad_norm": 1.641786813735962,
      "learning_rate": 1.4194663481793879e-05,
      "loss": 1.0719,
      "step": 7706
    },
    {
      "epoch": 1.083509067903838,
      "grad_norm": 1.2988569736480713,
      "learning_rate": 1.4314282383241174e-05,
      "loss": 1.1158,
      "step": 7707
    },
    {
      "epoch": 1.0836496555602417,
      "grad_norm": 1.5496385097503662,
      "learning_rate": 1.443436434777129e-05,
      "loss": 1.0718,
      "step": 7708
    },
    {
      "epoch": 1.0837902432166455,
      "grad_norm": 1.5578267574310303,
      "learning_rate": 1.4554908726436822e-05,
      "loss": 1.1021,
      "step": 7709
    },
    {
      "epoch": 1.0839308308730493,
      "grad_norm": 1.6231746673583984,
      "learning_rate": 1.467591486779175e-05,
      "loss": 1.0096,
      "step": 7710
    },
    {
      "epoch": 1.084071418529453,
      "grad_norm": 1.4935288429260254,
      "learning_rate": 1.479738211789402e-05,
      "loss": 0.904,
      "step": 7711
    },
    {
      "epoch": 1.084212006185857,
      "grad_norm": 1.4438711404800415,
      "learning_rate": 1.4919309820309879e-05,
      "loss": 0.8111,
      "step": 7712
    },
    {
      "epoch": 1.0843525938422607,
      "grad_norm": 1.3264319896697998,
      "learning_rate": 1.5041697316117165e-05,
      "loss": 1.1638,
      "step": 7713
    },
    {
      "epoch": 1.0844931814986645,
      "grad_norm": 1.5287920236587524,
      "learning_rate": 1.5164543943908893e-05,
      "loss": 0.9739,
      "step": 7714
    },
    {
      "epoch": 1.0846337691550683,
      "grad_norm": 1.4461350440979004,
      "learning_rate": 1.5287849039796854e-05,
      "loss": 0.9777,
      "step": 7715
    },
    {
      "epoch": 1.084774356811472,
      "grad_norm": 1.6047371625900269,
      "learning_rate": 1.541161193741517e-05,
      "loss": 1.0809,
      "step": 7716
    },
    {
      "epoch": 1.0849149444678756,
      "grad_norm": 1.4332935810089111,
      "learning_rate": 1.553583196792393e-05,
      "loss": 1.0656,
      "step": 7717
    },
    {
      "epoch": 1.0850555321242794,
      "grad_norm": 1.4226932525634766,
      "learning_rate": 1.5660508460012757e-05,
      "loss": 1.2675,
      "step": 7718
    },
    {
      "epoch": 1.0851961197806832,
      "grad_norm": 1.69802725315094,
      "learning_rate": 1.5785640739904296e-05,
      "loss": 0.9255,
      "step": 7719
    },
    {
      "epoch": 1.085336707437087,
      "grad_norm": 1.3761496543884277,
      "learning_rate": 1.591122813135857e-05,
      "loss": 1.0177,
      "step": 7720
    },
    {
      "epoch": 1.0854772950934908,
      "grad_norm": 1.56014883518219,
      "learning_rate": 1.603726995567554e-05,
      "loss": 1.0084,
      "step": 7721
    },
    {
      "epoch": 1.0856178827498946,
      "grad_norm": 1.583262324333191,
      "learning_rate": 1.616376553169967e-05,
      "loss": 1.0642,
      "step": 7722
    },
    {
      "epoch": 1.0857584704062984,
      "grad_norm": 1.474304437637329,
      "learning_rate": 1.629071417582273e-05,
      "loss": 0.8791,
      "step": 7723
    },
    {
      "epoch": 1.0858990580627021,
      "grad_norm": 1.4812438488006592,
      "learning_rate": 1.6418115201988537e-05,
      "loss": 1.0724,
      "step": 7724
    },
    {
      "epoch": 1.086039645719106,
      "grad_norm": 1.4800291061401367,
      "learning_rate": 1.654596792169579e-05,
      "loss": 1.053,
      "step": 7725
    },
    {
      "epoch": 1.0861802333755097,
      "grad_norm": 1.7456525564193726,
      "learning_rate": 1.6674271644002726e-05,
      "loss": 1.1179,
      "step": 7726
    },
    {
      "epoch": 1.0863208210319133,
      "grad_norm": 1.429742693901062,
      "learning_rate": 1.6803025675529684e-05,
      "loss": 1.1112,
      "step": 7727
    },
    {
      "epoch": 1.086461408688317,
      "grad_norm": 1.9685254096984863,
      "learning_rate": 1.6932229320463623e-05,
      "loss": 1.1208,
      "step": 7728
    },
    {
      "epoch": 1.0866019963447209,
      "grad_norm": 1.6772583723068237,
      "learning_rate": 1.7061881880561914e-05,
      "loss": 1.2268,
      "step": 7729
    },
    {
      "epoch": 1.0867425840011247,
      "grad_norm": 1.5441615581512451,
      "learning_rate": 1.7191982655155114e-05,
      "loss": 1.017,
      "step": 7730
    },
    {
      "epoch": 1.0868831716575285,
      "grad_norm": 1.7092124223709106,
      "learning_rate": 1.7322530941152705e-05,
      "loss": 1.0195,
      "step": 7731
    },
    {
      "epoch": 1.0870237593139322,
      "grad_norm": 1.3342162370681763,
      "learning_rate": 1.745352603304493e-05,
      "loss": 1.1827,
      "step": 7732
    },
    {
      "epoch": 1.087164346970336,
      "grad_norm": 1.4916080236434937,
      "learning_rate": 1.7584967222907635e-05,
      "loss": 1.1515,
      "step": 7733
    },
    {
      "epoch": 1.0873049346267398,
      "grad_norm": 1.706263780593872,
      "learning_rate": 1.7716853800405807e-05,
      "loss": 0.9938,
      "step": 7734
    },
    {
      "epoch": 1.0874455222831436,
      "grad_norm": 1.3456329107284546,
      "learning_rate": 1.7849185052797336e-05,
      "loss": 0.9753,
      "step": 7735
    },
    {
      "epoch": 1.0875861099395474,
      "grad_norm": 1.6018050909042358,
      "learning_rate": 1.7981960264937536e-05,
      "loss": 1.1649,
      "step": 7736
    },
    {
      "epoch": 1.087726697595951,
      "grad_norm": 1.5431888103485107,
      "learning_rate": 1.8115178719282112e-05,
      "loss": 0.9338,
      "step": 7737
    },
    {
      "epoch": 1.0878672852523548,
      "grad_norm": 1.3936315774917603,
      "learning_rate": 1.8248839695890974e-05,
      "loss": 1.0096,
      "step": 7738
    },
    {
      "epoch": 1.0880078729087586,
      "grad_norm": 1.3770583868026733,
      "learning_rate": 1.8382942472433163e-05,
      "loss": 0.9445,
      "step": 7739
    },
    {
      "epoch": 1.0881484605651623,
      "grad_norm": 1.3043593168258667,
      "learning_rate": 1.8517486324189946e-05,
      "loss": 1.2281,
      "step": 7740
    },
    {
      "epoch": 1.0882890482215661,
      "grad_norm": 1.2850086688995361,
      "learning_rate": 1.8652470524058708e-05,
      "loss": 1.0615,
      "step": 7741
    },
    {
      "epoch": 1.08842963587797,
      "grad_norm": 1.5812381505966187,
      "learning_rate": 1.8787894342557743e-05,
      "loss": 1.0797,
      "step": 7742
    },
    {
      "epoch": 1.0885702235343737,
      "grad_norm": 1.6155269145965576,
      "learning_rate": 1.8923757047828917e-05,
      "loss": 1.0886,
      "step": 7743
    },
    {
      "epoch": 1.0887108111907775,
      "grad_norm": 1.395843267440796,
      "learning_rate": 1.906005790564248e-05,
      "loss": 1.2231,
      "step": 7744
    },
    {
      "epoch": 1.0888513988471813,
      "grad_norm": 1.2612642049789429,
      "learning_rate": 1.9196796179401034e-05,
      "loss": 1.0217,
      "step": 7745
    },
    {
      "epoch": 1.088991986503585,
      "grad_norm": 1.4123172760009766,
      "learning_rate": 1.9333971130142447e-05,
      "loss": 1.2602,
      "step": 7746
    },
    {
      "epoch": 1.0891325741599887,
      "grad_norm": 1.5884568691253662,
      "learning_rate": 1.947158201654594e-05,
      "loss": 0.879,
      "step": 7747
    },
    {
      "epoch": 1.0892731618163924,
      "grad_norm": 1.3134346008300781,
      "learning_rate": 1.960962809493403e-05,
      "loss": 1.2482,
      "step": 7748
    },
    {
      "epoch": 1.0894137494727962,
      "grad_norm": 1.3274883031845093,
      "learning_rate": 1.9748108619277582e-05,
      "loss": 0.9726,
      "step": 7749
    },
    {
      "epoch": 1.0895543371292,
      "grad_norm": 1.4656354188919067,
      "learning_rate": 1.98870228411996e-05,
      "loss": 1.0338,
      "step": 7750
    },
    {
      "epoch": 1.0896949247856038,
      "grad_norm": 1.459166169166565,
      "learning_rate": 2.0026370009979335e-05,
      "loss": 0.9585,
      "step": 7751
    },
    {
      "epoch": 1.0898355124420076,
      "grad_norm": 1.9358081817626953,
      "learning_rate": 2.0166149372556265e-05,
      "loss": 0.9365,
      "step": 7752
    },
    {
      "epoch": 1.0899761000984114,
      "grad_norm": 1.4240763187408447,
      "learning_rate": 2.0306360173534235e-05,
      "loss": 0.9784,
      "step": 7753
    },
    {
      "epoch": 1.0901166877548152,
      "grad_norm": 1.6133391857147217,
      "learning_rate": 2.0447001655185482e-05,
      "loss": 0.9446,
      "step": 7754
    },
    {
      "epoch": 1.090257275411219,
      "grad_norm": 1.6446492671966553,
      "learning_rate": 2.0588073057454804e-05,
      "loss": 1.1773,
      "step": 7755
    },
    {
      "epoch": 1.0903978630676228,
      "grad_norm": 1.4118220806121826,
      "learning_rate": 2.072957361796356e-05,
      "loss": 1.2866,
      "step": 7756
    },
    {
      "epoch": 1.0905384507240263,
      "grad_norm": 1.5523968935012817,
      "learning_rate": 2.087150257201368e-05,
      "loss": 1.059,
      "step": 7757
    },
    {
      "epoch": 1.0906790383804301,
      "grad_norm": 1.7813712358474731,
      "learning_rate": 2.1013859152592586e-05,
      "loss": 1.1119,
      "step": 7758
    },
    {
      "epoch": 1.090819626036834,
      "grad_norm": 1.4947700500488281,
      "learning_rate": 2.1156642590376098e-05,
      "loss": 1.2178,
      "step": 7759
    },
    {
      "epoch": 1.0909602136932377,
      "grad_norm": 1.3963366746902466,
      "learning_rate": 2.129985211373362e-05,
      "loss": 1.1506,
      "step": 7760
    },
    {
      "epoch": 1.0911008013496415,
      "grad_norm": 1.5226809978485107,
      "learning_rate": 2.1443486948731274e-05,
      "loss": 0.9706,
      "step": 7761
    },
    {
      "epoch": 1.0912413890060453,
      "grad_norm": 1.4119855165481567,
      "learning_rate": 2.158754631913713e-05,
      "loss": 0.9671,
      "step": 7762
    },
    {
      "epoch": 1.091381976662449,
      "grad_norm": 1.6501706838607788,
      "learning_rate": 2.1732029446425415e-05,
      "loss": 1.1414,
      "step": 7763
    },
    {
      "epoch": 1.0915225643188529,
      "grad_norm": 1.4566634893417358,
      "learning_rate": 2.1876935549779663e-05,
      "loss": 1.0151,
      "step": 7764
    },
    {
      "epoch": 1.0916631519752567,
      "grad_norm": 1.4509358406066895,
      "learning_rate": 2.2022263846097734e-05,
      "loss": 0.9559,
      "step": 7765
    },
    {
      "epoch": 1.0918037396316604,
      "grad_norm": 1.4286967515945435,
      "learning_rate": 2.21680135499959e-05,
      "loss": 0.974,
      "step": 7766
    },
    {
      "epoch": 1.091944327288064,
      "grad_norm": 1.647512435913086,
      "learning_rate": 2.2314183873813067e-05,
      "loss": 1.0148,
      "step": 7767
    },
    {
      "epoch": 1.0920849149444678,
      "grad_norm": 1.4301561117172241,
      "learning_rate": 2.2460774027614994e-05,
      "loss": 1.0539,
      "step": 7768
    },
    {
      "epoch": 1.0922255026008716,
      "grad_norm": 1.4007726907730103,
      "learning_rate": 2.260778321919861e-05,
      "loss": 1.0406,
      "step": 7769
    },
    {
      "epoch": 1.0923660902572754,
      "grad_norm": 1.310465931892395,
      "learning_rate": 2.2755210654096293e-05,
      "loss": 0.9813,
      "step": 7770
    },
    {
      "epoch": 1.0925066779136792,
      "grad_norm": 1.7978153228759766,
      "learning_rate": 2.290305553558013e-05,
      "loss": 1.0295,
      "step": 7771
    },
    {
      "epoch": 1.092647265570083,
      "grad_norm": 1.4234975576400757,
      "learning_rate": 2.3051317064666243e-05,
      "loss": 0.834,
      "step": 7772
    },
    {
      "epoch": 1.0927878532264867,
      "grad_norm": 1.5047249794006348,
      "learning_rate": 2.3199994440118877e-05,
      "loss": 1.1925,
      "step": 7773
    },
    {
      "epoch": 1.0929284408828905,
      "grad_norm": 1.6484166383743286,
      "learning_rate": 2.334908685845566e-05,
      "loss": 1.0142,
      "step": 7774
    },
    {
      "epoch": 1.0930690285392943,
      "grad_norm": 1.5253608226776123,
      "learning_rate": 2.3498593513950763e-05,
      "loss": 0.9746,
      "step": 7775
    },
    {
      "epoch": 1.0932096161956981,
      "grad_norm": 1.4417623281478882,
      "learning_rate": 2.36485135986393e-05,
      "loss": 1.1017,
      "step": 7776
    },
    {
      "epoch": 1.0933502038521017,
      "grad_norm": 1.4259668588638306,
      "learning_rate": 2.37988463023228e-05,
      "loss": 0.9365,
      "step": 7777
    },
    {
      "epoch": 1.0934907915085055,
      "grad_norm": 1.659018874168396,
      "learning_rate": 2.3949590812572443e-05,
      "loss": 1.0449,
      "step": 7778
    },
    {
      "epoch": 1.0936313791649093,
      "grad_norm": 1.9021551609039307,
      "learning_rate": 2.410074631473466e-05,
      "loss": 1.0627,
      "step": 7779
    },
    {
      "epoch": 1.093771966821313,
      "grad_norm": 1.5574105978012085,
      "learning_rate": 2.4252311991934062e-05,
      "loss": 1.2079,
      "step": 7780
    },
    {
      "epoch": 1.0939125544777168,
      "grad_norm": 1.5512738227844238,
      "learning_rate": 2.440428702507883e-05,
      "loss": 1.0781,
      "step": 7781
    },
    {
      "epoch": 1.0940531421341206,
      "grad_norm": 1.947492003440857,
      "learning_rate": 2.455667059286493e-05,
      "loss": 1.0202,
      "step": 7782
    },
    {
      "epoch": 1.0941937297905244,
      "grad_norm": 1.4615358114242554,
      "learning_rate": 2.4709461871780703e-05,
      "loss": 0.9689,
      "step": 7783
    },
    {
      "epoch": 1.0943343174469282,
      "grad_norm": 1.552018642425537,
      "learning_rate": 2.4862660036110264e-05,
      "loss": 0.9943,
      "step": 7784
    },
    {
      "epoch": 1.094474905103332,
      "grad_norm": 1.7110774517059326,
      "learning_rate": 2.5016264257940202e-05,
      "loss": 1.0437,
      "step": 7785
    },
    {
      "epoch": 1.0946154927597358,
      "grad_norm": 1.5972321033477783,
      "learning_rate": 2.517027370716174e-05,
      "loss": 1.0189,
      "step": 7786
    },
    {
      "epoch": 1.0947560804161394,
      "grad_norm": 1.6838866472244263,
      "learning_rate": 2.5324687551476446e-05,
      "loss": 0.9457,
      "step": 7787
    },
    {
      "epoch": 1.0948966680725432,
      "grad_norm": 1.5528881549835205,
      "learning_rate": 2.5479504956400425e-05,
      "loss": 1.001,
      "step": 7788
    },
    {
      "epoch": 1.095037255728947,
      "grad_norm": 1.5242118835449219,
      "learning_rate": 2.563472508526863e-05,
      "loss": 1.1316,
      "step": 7789
    },
    {
      "epoch": 1.0951778433853507,
      "grad_norm": 1.4344483613967896,
      "learning_rate": 2.5790347099240553e-05,
      "loss": 1.067,
      "step": 7790
    },
    {
      "epoch": 1.0953184310417545,
      "grad_norm": 1.6613954305648804,
      "learning_rate": 2.594637015730239e-05,
      "loss": 1.0513,
      "step": 7791
    },
    {
      "epoch": 1.0954590186981583,
      "grad_norm": 1.38158118724823,
      "learning_rate": 2.6102793416273997e-05,
      "loss": 1.0991,
      "step": 7792
    },
    {
      "epoch": 1.095599606354562,
      "grad_norm": 1.4413321018218994,
      "learning_rate": 2.6259616030812174e-05,
      "loss": 1.0271,
      "step": 7793
    },
    {
      "epoch": 1.095740194010966,
      "grad_norm": 2.8124148845672607,
      "learning_rate": 2.641683715341555e-05,
      "loss": 0.9621,
      "step": 7794
    },
    {
      "epoch": 1.0958807816673697,
      "grad_norm": 1.4814436435699463,
      "learning_rate": 2.657445593442891e-05,
      "loss": 1.0372,
      "step": 7795
    },
    {
      "epoch": 1.0960213693237735,
      "grad_norm": 1.5962332487106323,
      "learning_rate": 2.673247152204871e-05,
      "loss": 0.9404,
      "step": 7796
    },
    {
      "epoch": 1.096161956980177,
      "grad_norm": 1.487452745437622,
      "learning_rate": 2.6890883062326243e-05,
      "loss": 1.125,
      "step": 7797
    },
    {
      "epoch": 1.0963025446365808,
      "grad_norm": 1.6293123960494995,
      "learning_rate": 2.704968969917353e-05,
      "loss": 0.9483,
      "step": 7798
    },
    {
      "epoch": 1.0964431322929846,
      "grad_norm": 1.8828271627426147,
      "learning_rate": 2.720889057436664e-05,
      "loss": 1.1857,
      "step": 7799
    },
    {
      "epoch": 1.0965837199493884,
      "grad_norm": 1.7038354873657227,
      "learning_rate": 2.736848482755159e-05,
      "loss": 1.0669,
      "step": 7800
    },
    {
      "epoch": 1.0967243076057922,
      "grad_norm": 1.628753900527954,
      "learning_rate": 2.7528471596248995e-05,
      "loss": 1.0681,
      "step": 7801
    },
    {
      "epoch": 1.096864895262196,
      "grad_norm": 1.666140079498291,
      "learning_rate": 2.7688850015857502e-05,
      "loss": 1.0575,
      "step": 7802
    },
    {
      "epoch": 1.0970054829185998,
      "grad_norm": 1.3949599266052246,
      "learning_rate": 2.7849619219659452e-05,
      "loss": 1.0799,
      "step": 7803
    },
    {
      "epoch": 1.0971460705750036,
      "grad_norm": 1.518371820449829,
      "learning_rate": 2.8010778338825284e-05,
      "loss": 1.1259,
      "step": 7804
    },
    {
      "epoch": 1.0972866582314074,
      "grad_norm": 1.7984519004821777,
      "learning_rate": 2.817232650241828e-05,
      "loss": 0.8885,
      "step": 7805
    },
    {
      "epoch": 1.0974272458878112,
      "grad_norm": 1.731583833694458,
      "learning_rate": 2.833426283739923e-05,
      "loss": 0.9537,
      "step": 7806
    },
    {
      "epoch": 1.0975678335442147,
      "grad_norm": 1.655996322631836,
      "learning_rate": 2.849658646863117e-05,
      "loss": 1.0163,
      "step": 7807
    },
    {
      "epoch": 1.0977084212006185,
      "grad_norm": 1.6298633813858032,
      "learning_rate": 2.8659296518884128e-05,
      "loss": 1.2297,
      "step": 7808
    },
    {
      "epoch": 1.0978490088570223,
      "grad_norm": 1.4870564937591553,
      "learning_rate": 2.8822392108839812e-05,
      "loss": 1.1579,
      "step": 7809
    },
    {
      "epoch": 1.097989596513426,
      "grad_norm": 1.3555018901824951,
      "learning_rate": 2.8985872357096412e-05,
      "loss": 0.8493,
      "step": 7810
    },
    {
      "epoch": 1.0981301841698299,
      "grad_norm": 1.4079304933547974,
      "learning_rate": 2.9149736380173098e-05,
      "loss": 1.0635,
      "step": 7811
    },
    {
      "epoch": 1.0982707718262337,
      "grad_norm": 1.644815444946289,
      "learning_rate": 2.931398329251579e-05,
      "loss": 0.8592,
      "step": 7812
    },
    {
      "epoch": 1.0984113594826375,
      "grad_norm": 1.5343973636627197,
      "learning_rate": 2.94786122065007e-05,
      "loss": 1.0987,
      "step": 7813
    },
    {
      "epoch": 1.0985519471390413,
      "grad_norm": 1.457076907157898,
      "learning_rate": 2.9643622232439118e-05,
      "loss": 0.9865,
      "step": 7814
    },
    {
      "epoch": 1.098692534795445,
      "grad_norm": 1.8411927223205566,
      "learning_rate": 2.9809012478583476e-05,
      "loss": 0.9596,
      "step": 7815
    },
    {
      "epoch": 1.0988331224518488,
      "grad_norm": 1.539038062095642,
      "learning_rate": 2.9974782051130924e-05,
      "loss": 1.0187,
      "step": 7816
    },
    {
      "epoch": 1.0989737101082524,
      "grad_norm": 1.337794303894043,
      "learning_rate": 3.01409300542294e-05,
      "loss": 0.9787,
      "step": 7817
    },
    {
      "epoch": 1.0991142977646562,
      "grad_norm": 1.6686553955078125,
      "learning_rate": 3.0307455589980993e-05,
      "loss": 0.8199,
      "step": 7818
    },
    {
      "epoch": 1.09925488542106,
      "grad_norm": 1.4642328023910522,
      "learning_rate": 3.0474357758447758e-05,
      "loss": 1.0986,
      "step": 7819
    },
    {
      "epoch": 1.0993954730774638,
      "grad_norm": 1.459033489227295,
      "learning_rate": 3.0641635657656354e-05,
      "loss": 0.9713,
      "step": 7820
    },
    {
      "epoch": 1.0995360607338676,
      "grad_norm": 1.5468670129776,
      "learning_rate": 3.0809288383602916e-05,
      "loss": 1.0657,
      "step": 7821
    },
    {
      "epoch": 1.0996766483902713,
      "grad_norm": 1.4887806177139282,
      "learning_rate": 3.097731503025796e-05,
      "loss": 1.0604,
      "step": 7822
    },
    {
      "epoch": 1.0998172360466751,
      "grad_norm": 1.408673882484436,
      "learning_rate": 3.114571468957122e-05,
      "loss": 1.0361,
      "step": 7823
    },
    {
      "epoch": 1.099957823703079,
      "grad_norm": 1.99740469455719,
      "learning_rate": 3.131448645147661e-05,
      "loss": 1.0075,
      "step": 7824
    },
    {
      "epoch": 1.1000984113594827,
      "grad_norm": 1.6392226219177246,
      "learning_rate": 3.148362940389714e-05,
      "loss": 0.9843,
      "step": 7825
    },
    {
      "epoch": 1.1002389990158865,
      "grad_norm": 1.4734517335891724,
      "learning_rate": 3.165314263274982e-05,
      "loss": 1.0071,
      "step": 7826
    },
    {
      "epoch": 1.10037958667229,
      "grad_norm": 1.4516432285308838,
      "learning_rate": 3.182302522195035e-05,
      "loss": 0.9085,
      "step": 7827
    },
    {
      "epoch": 1.1005201743286939,
      "grad_norm": 1.3409602642059326,
      "learning_rate": 3.199327625341937e-05,
      "loss": 1.0401,
      "step": 7828
    },
    {
      "epoch": 1.1006607619850977,
      "grad_norm": 1.5317832231521606,
      "learning_rate": 3.2163894807084816e-05,
      "loss": 1.0554,
      "step": 7829
    },
    {
      "epoch": 1.1008013496415014,
      "grad_norm": 1.6291999816894531,
      "learning_rate": 3.233487996088951e-05,
      "loss": 1.0181,
      "step": 7830
    },
    {
      "epoch": 1.1009419372979052,
      "grad_norm": 1.4603968858718872,
      "learning_rate": 3.250623079079483e-05,
      "loss": 1.0431,
      "step": 7831
    },
    {
      "epoch": 1.101082524954309,
      "grad_norm": 1.3450168371200562,
      "learning_rate": 3.267794637078572e-05,
      "loss": 1.096,
      "step": 7832
    },
    {
      "epoch": 1.1012231126107128,
      "grad_norm": 1.4562668800354004,
      "learning_rate": 3.285002577287668e-05,
      "loss": 1.0953,
      "step": 7833
    },
    {
      "epoch": 1.1013637002671166,
      "grad_norm": 1.4175511598587036,
      "learning_rate": 3.302246806711531e-05,
      "loss": 1.1193,
      "step": 7834
    },
    {
      "epoch": 1.1015042879235204,
      "grad_norm": 1.4504954814910889,
      "learning_rate": 3.31952723215883e-05,
      "loss": 1.1495,
      "step": 7835
    },
    {
      "epoch": 1.101644875579924,
      "grad_norm": 1.7099275588989258,
      "learning_rate": 3.3368437602426487e-05,
      "loss": 0.9342,
      "step": 7836
    },
    {
      "epoch": 1.1017854632363278,
      "grad_norm": 1.556854009628296,
      "learning_rate": 3.354196297380887e-05,
      "loss": 1.1444,
      "step": 7837
    },
    {
      "epoch": 1.1019260508927315,
      "grad_norm": 1.6294986009597778,
      "learning_rate": 3.371584749796899e-05,
      "loss": 1.108,
      "step": 7838
    },
    {
      "epoch": 1.1020666385491353,
      "grad_norm": 1.4379794597625732,
      "learning_rate": 3.389009023519996e-05,
      "loss": 1.0241,
      "step": 7839
    },
    {
      "epoch": 1.1022072262055391,
      "grad_norm": 1.3269388675689697,
      "learning_rate": 3.406469024385823e-05,
      "loss": 0.9789,
      "step": 7840
    },
    {
      "epoch": 1.102347813861943,
      "grad_norm": 1.523784875869751,
      "learning_rate": 3.4239646580369787e-05,
      "loss": 0.9252,
      "step": 7841
    },
    {
      "epoch": 1.1024884015183467,
      "grad_norm": 1.5624134540557861,
      "learning_rate": 3.441495829923492e-05,
      "loss": 0.9403,
      "step": 7842
    },
    {
      "epoch": 1.1026289891747505,
      "grad_norm": 1.3606758117675781,
      "learning_rate": 3.459062445303337e-05,
      "loss": 1.0133,
      "step": 7843
    },
    {
      "epoch": 1.1027695768311543,
      "grad_norm": 1.5273842811584473,
      "learning_rate": 3.476664409242942e-05,
      "loss": 1.0364,
      "step": 7844
    },
    {
      "epoch": 1.102910164487558,
      "grad_norm": 1.6799002885818481,
      "learning_rate": 3.4943016266177086e-05,
      "loss": 0.953,
      "step": 7845
    },
    {
      "epoch": 1.1030507521439619,
      "grad_norm": 1.5040037631988525,
      "learning_rate": 3.511974002112518e-05,
      "loss": 1.0225,
      "step": 7846
    },
    {
      "epoch": 1.1031913398003654,
      "grad_norm": 1.5151273012161255,
      "learning_rate": 3.529681440222251e-05,
      "loss": 1.1004,
      "step": 7847
    },
    {
      "epoch": 1.1033319274567692,
      "grad_norm": 1.6755423545837402,
      "learning_rate": 3.5474238452522755e-05,
      "loss": 1.0557,
      "step": 7848
    },
    {
      "epoch": 1.103472515113173,
      "grad_norm": 1.453334093093872,
      "learning_rate": 3.565201121319072e-05,
      "loss": 1.1142,
      "step": 7849
    },
    {
      "epoch": 1.1036131027695768,
      "grad_norm": 1.3066785335540771,
      "learning_rate": 3.583013172350591e-05,
      "loss": 1.2327,
      "step": 7850
    },
    {
      "epoch": 1.1037536904259806,
      "grad_norm": 1.7947720289230347,
      "learning_rate": 3.600859902086906e-05,
      "loss": 1.1346,
      "step": 7851
    },
    {
      "epoch": 1.1038942780823844,
      "grad_norm": 1.539158582687378,
      "learning_rate": 3.618741214080595e-05,
      "loss": 0.943,
      "step": 7852
    },
    {
      "epoch": 1.1040348657387882,
      "grad_norm": 1.4725549221038818,
      "learning_rate": 3.636657011697434e-05,
      "loss": 1.0851,
      "step": 7853
    },
    {
      "epoch": 1.104175453395192,
      "grad_norm": 1.4900853633880615,
      "learning_rate": 3.654607198116771e-05,
      "loss": 0.9863,
      "step": 7854
    },
    {
      "epoch": 1.1043160410515958,
      "grad_norm": 1.400316596031189,
      "learning_rate": 3.6725916763321945e-05,
      "loss": 1.0725,
      "step": 7855
    },
    {
      "epoch": 1.1044566287079993,
      "grad_norm": 1.5137991905212402,
      "learning_rate": 3.6906103491518925e-05,
      "loss": 1.0123,
      "step": 7856
    },
    {
      "epoch": 1.1045972163644031,
      "grad_norm": 1.483134388923645,
      "learning_rate": 3.7086631191992835e-05,
      "loss": 1.0199,
      "step": 7857
    },
    {
      "epoch": 1.104737804020807,
      "grad_norm": 1.600327730178833,
      "learning_rate": 3.726749888913519e-05,
      "loss": 1.1533,
      "step": 7858
    },
    {
      "epoch": 1.1048783916772107,
      "grad_norm": 1.3715656995773315,
      "learning_rate": 3.744870560550009e-05,
      "loss": 1.3126,
      "step": 7859
    },
    {
      "epoch": 1.1050189793336145,
      "grad_norm": 1.4890981912612915,
      "learning_rate": 3.763025036180951e-05,
      "loss": 1.0227,
      "step": 7860
    },
    {
      "epoch": 1.1051595669900183,
      "grad_norm": 1.620696783065796,
      "learning_rate": 3.7812132176958557e-05,
      "loss": 1.0354,
      "step": 7861
    },
    {
      "epoch": 1.105300154646422,
      "grad_norm": 1.355808973312378,
      "learning_rate": 3.7994350068020866e-05,
      "loss": 1.2721,
      "step": 7862
    },
    {
      "epoch": 1.1054407423028259,
      "grad_norm": 1.6032540798187256,
      "learning_rate": 3.8176903050253786e-05,
      "loss": 0.9166,
      "step": 7863
    },
    {
      "epoch": 1.1055813299592296,
      "grad_norm": 1.5114414691925049,
      "learning_rate": 3.83597901371038e-05,
      "loss": 0.9524,
      "step": 7864
    },
    {
      "epoch": 1.1057219176156334,
      "grad_norm": 1.6334044933319092,
      "learning_rate": 3.854301034021155e-05,
      "loss": 0.9988,
      "step": 7865
    },
    {
      "epoch": 1.1058625052720372,
      "grad_norm": 1.4789531230926514,
      "learning_rate": 3.872656266941852e-05,
      "loss": 1.0319,
      "step": 7866
    },
    {
      "epoch": 1.1060030929284408,
      "grad_norm": 1.6051043272018433,
      "learning_rate": 3.8910446132769696e-05,
      "loss": 1.0598,
      "step": 7867
    },
    {
      "epoch": 1.1061436805848446,
      "grad_norm": 1.4525437355041504,
      "learning_rate": 3.909465973652164e-05,
      "loss": 1.1452,
      "step": 7868
    },
    {
      "epoch": 1.1062842682412484,
      "grad_norm": 1.5192784070968628,
      "learning_rate": 3.927920248514648e-05,
      "loss": 1.2197,
      "step": 7869
    },
    {
      "epoch": 1.1064248558976522,
      "grad_norm": 1.3208376169204712,
      "learning_rate": 3.946407338133731e-05,
      "loss": 1.0808,
      "step": 7870
    },
    {
      "epoch": 1.106565443554056,
      "grad_norm": 1.5668574571609497,
      "learning_rate": 3.964927142601468e-05,
      "loss": 1.1138,
      "step": 7871
    },
    {
      "epoch": 1.1067060312104597,
      "grad_norm": 1.2960551977157593,
      "learning_rate": 3.9834795618330345e-05,
      "loss": 1.0412,
      "step": 7872
    },
    {
      "epoch": 1.1068466188668635,
      "grad_norm": 1.2784868478775024,
      "learning_rate": 4.002064495567373e-05,
      "loss": 0.9497,
      "step": 7873
    },
    {
      "epoch": 1.1069872065232673,
      "grad_norm": 1.5779016017913818,
      "learning_rate": 4.020681843367745e-05,
      "loss": 1.1199,
      "step": 7874
    },
    {
      "epoch": 1.107127794179671,
      "grad_norm": 1.5163078308105469,
      "learning_rate": 4.039331504622121e-05,
      "loss": 0.8169,
      "step": 7875
    },
    {
      "epoch": 1.1072683818360747,
      "grad_norm": 1.3817194700241089,
      "learning_rate": 4.0580133785440165e-05,
      "loss": 0.9736,
      "step": 7876
    },
    {
      "epoch": 1.1074089694924785,
      "grad_norm": 1.6865366697311401,
      "learning_rate": 4.076727364172748e-05,
      "loss": 1.1877,
      "step": 7877
    },
    {
      "epoch": 1.1075495571488823,
      "grad_norm": 1.4314329624176025,
      "learning_rate": 4.0954733603741225e-05,
      "loss": 0.9843,
      "step": 7878
    },
    {
      "epoch": 1.107690144805286,
      "grad_norm": 1.317398190498352,
      "learning_rate": 4.114251265840956e-05,
      "loss": 1.0966,
      "step": 7879
    },
    {
      "epoch": 1.1078307324616898,
      "grad_norm": 1.6348400115966797,
      "learning_rate": 4.13306097909362e-05,
      "loss": 1.1201,
      "step": 7880
    },
    {
      "epoch": 1.1079713201180936,
      "grad_norm": 1.5259648561477661,
      "learning_rate": 4.1519023984805604e-05,
      "loss": 1.0963,
      "step": 7881
    },
    {
      "epoch": 1.1081119077744974,
      "grad_norm": 1.6296329498291016,
      "learning_rate": 4.1707754221789954e-05,
      "loss": 0.9808,
      "step": 7882
    },
    {
      "epoch": 1.1082524954309012,
      "grad_norm": 1.4138864278793335,
      "learning_rate": 4.189679948195169e-05,
      "loss": 0.9863,
      "step": 7883
    },
    {
      "epoch": 1.108393083087305,
      "grad_norm": 1.3486350774765015,
      "learning_rate": 4.208615874365198e-05,
      "loss": 1.1282,
      "step": 7884
    },
    {
      "epoch": 1.1085336707437088,
      "grad_norm": 1.473254919052124,
      "learning_rate": 4.22758309835548e-05,
      "loss": 1.0829,
      "step": 7885
    },
    {
      "epoch": 1.1086742584001126,
      "grad_norm": 1.4412426948547363,
      "learning_rate": 4.246581517663241e-05,
      "loss": 1.1438,
      "step": 7886
    },
    {
      "epoch": 1.1088148460565161,
      "grad_norm": 1.4558470249176025,
      "learning_rate": 4.265611029617207e-05,
      "loss": 1.0954,
      "step": 7887
    },
    {
      "epoch": 1.10895543371292,
      "grad_norm": 1.5786532163619995,
      "learning_rate": 4.28467153137799e-05,
      "loss": 1.0917,
      "step": 7888
    },
    {
      "epoch": 1.1090960213693237,
      "grad_norm": 1.5764168500900269,
      "learning_rate": 4.30376291993878e-05,
      "loss": 1.0268,
      "step": 7889
    },
    {
      "epoch": 1.1092366090257275,
      "grad_norm": 1.5024412870407104,
      "learning_rate": 4.322885092125766e-05,
      "loss": 0.9823,
      "step": 7890
    },
    {
      "epoch": 1.1093771966821313,
      "grad_norm": 1.41814386844635,
      "learning_rate": 4.342037944598829e-05,
      "loss": 0.9697,
      "step": 7891
    },
    {
      "epoch": 1.109517784338535,
      "grad_norm": 1.4785349369049072,
      "learning_rate": 4.3612213738521126e-05,
      "loss": 1.1834,
      "step": 7892
    },
    {
      "epoch": 1.1096583719949389,
      "grad_norm": 1.602854609489441,
      "learning_rate": 4.380435276214423e-05,
      "loss": 0.8386,
      "step": 7893
    },
    {
      "epoch": 1.1097989596513427,
      "grad_norm": 1.829559564590454,
      "learning_rate": 4.399679547849916e-05,
      "loss": 1.0878,
      "step": 7894
    },
    {
      "epoch": 1.1099395473077465,
      "grad_norm": 1.359503984451294,
      "learning_rate": 4.418954084758627e-05,
      "loss": 1.1782,
      "step": 7895
    },
    {
      "epoch": 1.11008013496415,
      "grad_norm": 1.370397925376892,
      "learning_rate": 4.4382587827770275e-05,
      "loss": 1.1248,
      "step": 7896
    },
    {
      "epoch": 1.1102207226205538,
      "grad_norm": 1.4744514226913452,
      "learning_rate": 4.457593537578596e-05,
      "loss": 0.8539,
      "step": 7897
    },
    {
      "epoch": 1.1103613102769576,
      "grad_norm": 1.8199747800827026,
      "learning_rate": 4.476958244674375e-05,
      "loss": 1.3176,
      "step": 7898
    },
    {
      "epoch": 1.1105018979333614,
      "grad_norm": 1.4221042394638062,
      "learning_rate": 4.496352799413542e-05,
      "loss": 1.0566,
      "step": 7899
    },
    {
      "epoch": 1.1106424855897652,
      "grad_norm": 1.36775541305542,
      "learning_rate": 4.515777096983971e-05,
      "loss": 1.0123,
      "step": 7900
    },
    {
      "epoch": 1.110783073246169,
      "grad_norm": 1.651718258857727,
      "learning_rate": 4.5352310324127955e-05,
      "loss": 1.0139,
      "step": 7901
    },
    {
      "epoch": 1.1109236609025728,
      "grad_norm": 1.709584355354309,
      "learning_rate": 4.554714500566956e-05,
      "loss": 1.1801,
      "step": 7902
    },
    {
      "epoch": 1.1110642485589766,
      "grad_norm": 1.6195406913757324,
      "learning_rate": 4.5742273961538775e-05,
      "loss": 0.9931,
      "step": 7903
    },
    {
      "epoch": 1.1112048362153804,
      "grad_norm": 1.7617369890213013,
      "learning_rate": 4.5937696137218966e-05,
      "loss": 0.866,
      "step": 7904
    },
    {
      "epoch": 1.1113454238717841,
      "grad_norm": 1.3472628593444824,
      "learning_rate": 4.613341047660825e-05,
      "loss": 1.2694,
      "step": 7905
    },
    {
      "epoch": 1.111486011528188,
      "grad_norm": 1.708240270614624,
      "learning_rate": 4.6329415922026756e-05,
      "loss": 1.0319,
      "step": 7906
    },
    {
      "epoch": 1.1116265991845915,
      "grad_norm": 1.5368905067443848,
      "learning_rate": 4.65257114142211e-05,
      "loss": 0.9435,
      "step": 7907
    },
    {
      "epoch": 1.1117671868409953,
      "grad_norm": 1.2923450469970703,
      "learning_rate": 4.6722295892370126e-05,
      "loss": 1.2371,
      "step": 7908
    },
    {
      "epoch": 1.111907774497399,
      "grad_norm": 1.2553902864456177,
      "learning_rate": 4.691916829409187e-05,
      "loss": 0.9351,
      "step": 7909
    },
    {
      "epoch": 1.1120483621538029,
      "grad_norm": 1.4772344827651978,
      "learning_rate": 4.711632755544741e-05,
      "loss": 1.1483,
      "step": 7910
    },
    {
      "epoch": 1.1121889498102067,
      "grad_norm": 1.5147767066955566,
      "learning_rate": 4.731377261094793e-05,
      "loss": 0.9702,
      "step": 7911
    },
    {
      "epoch": 1.1123295374666105,
      "grad_norm": 1.5177935361862183,
      "learning_rate": 4.7511502393560414e-05,
      "loss": 0.9033,
      "step": 7912
    },
    {
      "epoch": 1.1124701251230142,
      "grad_norm": 1.337714672088623,
      "learning_rate": 4.770951583471189e-05,
      "loss": 1.2071,
      "step": 7913
    },
    {
      "epoch": 1.112610712779418,
      "grad_norm": 1.4255610704421997,
      "learning_rate": 4.790781186429829e-05,
      "loss": 0.878,
      "step": 7914
    },
    {
      "epoch": 1.1127513004358218,
      "grad_norm": 1.8556663990020752,
      "learning_rate": 4.810638941068714e-05,
      "loss": 1.21,
      "step": 7915
    },
    {
      "epoch": 1.1128918880922254,
      "grad_norm": 1.4839733839035034,
      "learning_rate": 4.8305247400724885e-05,
      "loss": 1.1605,
      "step": 7916
    },
    {
      "epoch": 1.1130324757486292,
      "grad_norm": 1.578125238418579,
      "learning_rate": 4.850438475974244e-05,
      "loss": 1.0195,
      "step": 7917
    },
    {
      "epoch": 1.113173063405033,
      "grad_norm": 1.57439386844635,
      "learning_rate": 4.8703800411560617e-05,
      "loss": 1.0589,
      "step": 7918
    },
    {
      "epoch": 1.1133136510614368,
      "grad_norm": 1.5192112922668457,
      "learning_rate": 4.8903493278497194e-05,
      "loss": 0.9895,
      "step": 7919
    },
    {
      "epoch": 1.1134542387178405,
      "grad_norm": 1.5751876831054688,
      "learning_rate": 4.910346228137122e-05,
      "loss": 1.0719,
      "step": 7920
    },
    {
      "epoch": 1.1135948263742443,
      "grad_norm": 1.518311619758606,
      "learning_rate": 4.930370633950883e-05,
      "loss": 1.0809,
      "step": 7921
    },
    {
      "epoch": 1.1137354140306481,
      "grad_norm": 1.5506244897842407,
      "learning_rate": 4.9504224370750616e-05,
      "loss": 1.0798,
      "step": 7922
    },
    {
      "epoch": 1.113876001687052,
      "grad_norm": 1.3660707473754883,
      "learning_rate": 4.970501529145628e-05,
      "loss": 0.9848,
      "step": 7923
    },
    {
      "epoch": 1.1140165893434557,
      "grad_norm": 1.5942387580871582,
      "learning_rate": 4.990607801651044e-05,
      "loss": 1.062,
      "step": 7924
    },
    {
      "epoch": 1.1141571769998595,
      "grad_norm": 1.6674541234970093,
      "learning_rate": 5.0107411459329776e-05,
      "loss": 0.9636,
      "step": 7925
    },
    {
      "epoch": 1.1142977646562633,
      "grad_norm": 1.8387149572372437,
      "learning_rate": 5.0309014531867004e-05,
      "loss": 1.1829,
      "step": 7926
    },
    {
      "epoch": 1.1144383523126669,
      "grad_norm": 1.5636913776397705,
      "learning_rate": 5.0510886144618366e-05,
      "loss": 1.2018,
      "step": 7927
    },
    {
      "epoch": 1.1145789399690706,
      "grad_norm": 1.4404199123382568,
      "learning_rate": 5.071302520662786e-05,
      "loss": 1.2407,
      "step": 7928
    },
    {
      "epoch": 1.1147195276254744,
      "grad_norm": 1.4563047885894775,
      "learning_rate": 5.091543062549482e-05,
      "loss": 1.095,
      "step": 7929
    },
    {
      "epoch": 1.1148601152818782,
      "grad_norm": 1.6091216802597046,
      "learning_rate": 5.111810130737973e-05,
      "loss": 1.1718,
      "step": 7930
    },
    {
      "epoch": 1.115000702938282,
      "grad_norm": 1.328715205192566,
      "learning_rate": 5.1321036157008586e-05,
      "loss": 1.0043,
      "step": 7931
    },
    {
      "epoch": 1.1151412905946858,
      "grad_norm": 1.6586750745773315,
      "learning_rate": 5.15242340776801e-05,
      "loss": 1.0744,
      "step": 7932
    },
    {
      "epoch": 1.1152818782510896,
      "grad_norm": 1.3809967041015625,
      "learning_rate": 5.172769397127132e-05,
      "loss": 1.1155,
      "step": 7933
    },
    {
      "epoch": 1.1154224659074934,
      "grad_norm": 1.5830093622207642,
      "learning_rate": 5.193141473824317e-05,
      "loss": 1.0119,
      "step": 7934
    },
    {
      "epoch": 1.1155630535638972,
      "grad_norm": 1.6008999347686768,
      "learning_rate": 5.213539527764804e-05,
      "loss": 1.0278,
      "step": 7935
    },
    {
      "epoch": 1.1157036412203007,
      "grad_norm": 1.6205440759658813,
      "learning_rate": 5.233963448713258e-05,
      "loss": 0.9428,
      "step": 7936
    },
    {
      "epoch": 1.1158442288767045,
      "grad_norm": 1.3415147066116333,
      "learning_rate": 5.254413126294679e-05,
      "loss": 0.9587,
      "step": 7937
    },
    {
      "epoch": 1.1159848165331083,
      "grad_norm": 1.4445840120315552,
      "learning_rate": 5.274888449994844e-05,
      "loss": 1.1761,
      "step": 7938
    },
    {
      "epoch": 1.1161254041895121,
      "grad_norm": 1.5387409925460815,
      "learning_rate": 5.295389309160925e-05,
      "loss": 1.0154,
      "step": 7939
    },
    {
      "epoch": 1.116265991845916,
      "grad_norm": 1.7489383220672607,
      "learning_rate": 5.3159155930020744e-05,
      "loss": 1.0725,
      "step": 7940
    },
    {
      "epoch": 1.1164065795023197,
      "grad_norm": 1.4648561477661133,
      "learning_rate": 5.3364671905901285e-05,
      "loss": 0.9453,
      "step": 7941
    },
    {
      "epoch": 1.1165471671587235,
      "grad_norm": 1.48971688747406,
      "learning_rate": 5.357043990860068e-05,
      "loss": 0.9265,
      "step": 7942
    },
    {
      "epoch": 1.1166877548151273,
      "grad_norm": 1.5295847654342651,
      "learning_rate": 5.377645882610606e-05,
      "loss": 1.0381,
      "step": 7943
    },
    {
      "epoch": 1.116828342471531,
      "grad_norm": 1.4630861282348633,
      "learning_rate": 5.3982727545049514e-05,
      "loss": 1.2894,
      "step": 7944
    },
    {
      "epoch": 1.1169689301279349,
      "grad_norm": 1.5402311086654663,
      "learning_rate": 5.418924495071256e-05,
      "loss": 1.1568,
      "step": 7945
    },
    {
      "epoch": 1.1171095177843386,
      "grad_norm": 1.630549669265747,
      "learning_rate": 5.439600992703362e-05,
      "loss": 0.9492,
      "step": 7946
    },
    {
      "epoch": 1.1172501054407422,
      "grad_norm": 1.590765118598938,
      "learning_rate": 5.460302135661235e-05,
      "loss": 1.045,
      "step": 7947
    },
    {
      "epoch": 1.117390693097146,
      "grad_norm": 1.5419087409973145,
      "learning_rate": 5.481027812071676e-05,
      "loss": 1.1407,
      "step": 7948
    },
    {
      "epoch": 1.1175312807535498,
      "grad_norm": 1.4255270957946777,
      "learning_rate": 5.501777909928907e-05,
      "loss": 1.0504,
      "step": 7949
    },
    {
      "epoch": 1.1176718684099536,
      "grad_norm": 1.401212453842163,
      "learning_rate": 5.522552317095201e-05,
      "loss": 0.907,
      "step": 7950
    },
    {
      "epoch": 1.1178124560663574,
      "grad_norm": 1.408838152885437,
      "learning_rate": 5.543350921301331e-05,
      "loss": 1.1341,
      "step": 7951
    },
    {
      "epoch": 1.1179530437227612,
      "grad_norm": 1.8662331104278564,
      "learning_rate": 5.5641736101474986e-05,
      "loss": 0.9859,
      "step": 7952
    },
    {
      "epoch": 1.118093631379165,
      "grad_norm": 1.5770279169082642,
      "learning_rate": 5.585020271103614e-05,
      "loss": 0.8627,
      "step": 7953
    },
    {
      "epoch": 1.1182342190355687,
      "grad_norm": 1.4958443641662598,
      "learning_rate": 5.605890791510073e-05,
      "loss": 1.2544,
      "step": 7954
    },
    {
      "epoch": 1.1183748066919725,
      "grad_norm": 1.5972412824630737,
      "learning_rate": 5.626785058578332e-05,
      "loss": 0.9813,
      "step": 7955
    },
    {
      "epoch": 1.118515394348376,
      "grad_norm": 1.5777820348739624,
      "learning_rate": 5.647702959391479e-05,
      "loss": 0.992,
      "step": 7956
    },
    {
      "epoch": 1.11865598200478,
      "grad_norm": 1.3535972833633423,
      "learning_rate": 5.668644380904977e-05,
      "loss": 0.9923,
      "step": 7957
    },
    {
      "epoch": 1.1187965696611837,
      "grad_norm": 1.7189544439315796,
      "learning_rate": 5.6896092099471176e-05,
      "loss": 1.0802,
      "step": 7958
    },
    {
      "epoch": 1.1189371573175875,
      "grad_norm": 1.3111454248428345,
      "learning_rate": 5.710597333219627e-05,
      "loss": 1.0394,
      "step": 7959
    },
    {
      "epoch": 1.1190777449739913,
      "grad_norm": 1.6484637260437012,
      "learning_rate": 5.7316086372984426e-05,
      "loss": 0.9136,
      "step": 7960
    },
    {
      "epoch": 1.119218332630395,
      "grad_norm": 1.4452550411224365,
      "learning_rate": 5.75264300863416e-05,
      "loss": 1.1732,
      "step": 7961
    },
    {
      "epoch": 1.1193589202867988,
      "grad_norm": 1.6288236379623413,
      "learning_rate": 5.773700333552814e-05,
      "loss": 1.0822,
      "step": 7962
    },
    {
      "epoch": 1.1194995079432026,
      "grad_norm": 1.4663652181625366,
      "learning_rate": 5.794780498256297e-05,
      "loss": 1.0446,
      "step": 7963
    },
    {
      "epoch": 1.1196400955996064,
      "grad_norm": 1.626695156097412,
      "learning_rate": 5.8158833888231024e-05,
      "loss": 1.0909,
      "step": 7964
    },
    {
      "epoch": 1.1197806832560102,
      "grad_norm": 1.5957512855529785,
      "learning_rate": 5.837008891208941e-05,
      "loss": 0.9785,
      "step": 7965
    },
    {
      "epoch": 1.119921270912414,
      "grad_norm": 1.7162519693374634,
      "learning_rate": 5.8581568912472254e-05,
      "loss": 0.9633,
      "step": 7966
    },
    {
      "epoch": 1.1200618585688176,
      "grad_norm": 1.4954910278320312,
      "learning_rate": 5.8793272746498554e-05,
      "loss": 1.2382,
      "step": 7967
    },
    {
      "epoch": 1.1202024462252214,
      "grad_norm": 1.5099384784698486,
      "learning_rate": 5.900519927007826e-05,
      "loss": 0.9968,
      "step": 7968
    },
    {
      "epoch": 1.1203430338816251,
      "grad_norm": 1.430387258529663,
      "learning_rate": 5.921734733791694e-05,
      "loss": 1.1743,
      "step": 7969
    },
    {
      "epoch": 1.120483621538029,
      "grad_norm": 1.425026774406433,
      "learning_rate": 5.942971580352319e-05,
      "loss": 1.24,
      "step": 7970
    },
    {
      "epoch": 1.1206242091944327,
      "grad_norm": 1.507201910018921,
      "learning_rate": 5.964230351921452e-05,
      "loss": 1.031,
      "step": 7971
    },
    {
      "epoch": 1.1207647968508365,
      "grad_norm": 1.455224633216858,
      "learning_rate": 5.985510933612328e-05,
      "loss": 1.1249,
      "step": 7972
    },
    {
      "epoch": 1.1209053845072403,
      "grad_norm": 1.7587783336639404,
      "learning_rate": 6.0068132104204435e-05,
      "loss": 1.1507,
      "step": 7973
    },
    {
      "epoch": 1.121045972163644,
      "grad_norm": 1.5784313678741455,
      "learning_rate": 6.028137067223852e-05,
      "loss": 0.8126,
      "step": 7974
    },
    {
      "epoch": 1.121186559820048,
      "grad_norm": 1.5240775346755981,
      "learning_rate": 6.0494823887841225e-05,
      "loss": 0.9116,
      "step": 7975
    },
    {
      "epoch": 1.1213271474764515,
      "grad_norm": 1.269802212715149,
      "learning_rate": 6.070849059746782e-05,
      "loss": 1.1228,
      "step": 7976
    },
    {
      "epoch": 1.1214677351328552,
      "grad_norm": 1.5440120697021484,
      "learning_rate": 6.0922369646419875e-05,
      "loss": 1.1362,
      "step": 7977
    },
    {
      "epoch": 1.121608322789259,
      "grad_norm": 1.6962108612060547,
      "learning_rate": 6.113645987885108e-05,
      "loss": 0.9861,
      "step": 7978
    },
    {
      "epoch": 1.1217489104456628,
      "grad_norm": 1.3980650901794434,
      "learning_rate": 6.135076013777485e-05,
      "loss": 1.0126,
      "step": 7979
    },
    {
      "epoch": 1.1218894981020666,
      "grad_norm": 1.4710735082626343,
      "learning_rate": 6.156526926506859e-05,
      "loss": 1.069,
      "step": 7980
    },
    {
      "epoch": 1.1220300857584704,
      "grad_norm": 1.5446945428848267,
      "learning_rate": 6.177998610148152e-05,
      "loss": 0.9332,
      "step": 7981
    },
    {
      "epoch": 1.1221706734148742,
      "grad_norm": 1.5846105813980103,
      "learning_rate": 6.199490948663951e-05,
      "loss": 1.0901,
      "step": 7982
    },
    {
      "epoch": 1.122311261071278,
      "grad_norm": 1.293689489364624,
      "learning_rate": 6.221003825905273e-05,
      "loss": 1.0442,
      "step": 7983
    },
    {
      "epoch": 1.1224518487276818,
      "grad_norm": 1.4767522811889648,
      "learning_rate": 6.242537125612213e-05,
      "loss": 1.059,
      "step": 7984
    },
    {
      "epoch": 1.1225924363840856,
      "grad_norm": 1.4490946531295776,
      "learning_rate": 6.264090731414397e-05,
      "loss": 1.0175,
      "step": 7985
    },
    {
      "epoch": 1.1227330240404894,
      "grad_norm": 1.500760793685913,
      "learning_rate": 6.285664526831743e-05,
      "loss": 0.8364,
      "step": 7986
    },
    {
      "epoch": 1.122873611696893,
      "grad_norm": 1.3824079036712646,
      "learning_rate": 6.307258395275067e-05,
      "loss": 1.0863,
      "step": 7987
    },
    {
      "epoch": 1.1230141993532967,
      "grad_norm": 1.859919786453247,
      "learning_rate": 6.3288722200467e-05,
      "loss": 0.8538,
      "step": 7988
    },
    {
      "epoch": 1.1231547870097005,
      "grad_norm": 1.7978018522262573,
      "learning_rate": 6.350505884341129e-05,
      "loss": 1.1617,
      "step": 7989
    },
    {
      "epoch": 1.1232953746661043,
      "grad_norm": 1.5698349475860596,
      "learning_rate": 6.372159271245623e-05,
      "loss": 1.0636,
      "step": 7990
    },
    {
      "epoch": 1.123435962322508,
      "grad_norm": 1.6500861644744873,
      "learning_rate": 6.393832263740865e-05,
      "loss": 1.09,
      "step": 7991
    },
    {
      "epoch": 1.1235765499789119,
      "grad_norm": 1.5823208093643188,
      "learning_rate": 6.415524744701587e-05,
      "loss": 1.1573,
      "step": 7992
    },
    {
      "epoch": 1.1237171376353157,
      "grad_norm": 1.3578847646713257,
      "learning_rate": 6.437236596897196e-05,
      "loss": 1.1355,
      "step": 7993
    },
    {
      "epoch": 1.1238577252917195,
      "grad_norm": 1.699781894683838,
      "learning_rate": 6.45896770299239e-05,
      "loss": 1.1667,
      "step": 7994
    },
    {
      "epoch": 1.1239983129481232,
      "grad_norm": 1.4669208526611328,
      "learning_rate": 6.480717945547905e-05,
      "loss": 1.0412,
      "step": 7995
    },
    {
      "epoch": 1.1241389006045268,
      "grad_norm": 1.7962921857833862,
      "learning_rate": 6.502487207020998e-05,
      "loss": 0.9855,
      "step": 7996
    },
    {
      "epoch": 1.1242794882609306,
      "grad_norm": 1.4989782571792603,
      "learning_rate": 6.524275369766077e-05,
      "loss": 1.0891,
      "step": 7997
    },
    {
      "epoch": 1.1244200759173344,
      "grad_norm": 1.4818774461746216,
      "learning_rate": 6.546082316035502e-05,
      "loss": 1.0732,
      "step": 7998
    },
    {
      "epoch": 1.1245606635737382,
      "grad_norm": 1.6167851686477661,
      "learning_rate": 6.567907927980059e-05,
      "loss": 0.9603,
      "step": 7999
    },
    {
      "epoch": 1.124701251230142,
      "grad_norm": 1.5754917860031128,
      "learning_rate": 6.589752087649754e-05,
      "loss": 1.1493,
      "step": 8000
    },
    {
      "epoch": 1.124701251230142,
      "eval_loss": 1.1398881673812866,
      "eval_runtime": 771.9541,
      "eval_samples_per_second": 16.382,
      "eval_steps_per_second": 8.191,
      "step": 8000
    },
    {
      "epoch": 1.1248418388865458,
      "grad_norm": 1.2928415536880493,
      "learning_rate": 6.611614676994269e-05,
      "loss": 1.2035,
      "step": 8001
    },
    {
      "epoch": 1.1249824265429496,
      "grad_norm": 1.4120444059371948,
      "learning_rate": 6.633495577863711e-05,
      "loss": 1.0151,
      "step": 8002
    },
    {
      "epoch": 1.1251230141993533,
      "grad_norm": 1.4072179794311523,
      "learning_rate": 6.655394672009266e-05,
      "loss": 1.0279,
      "step": 8003
    },
    {
      "epoch": 1.1252636018557571,
      "grad_norm": 1.440866470336914,
      "learning_rate": 6.677311841083664e-05,
      "loss": 1.1571,
      "step": 8004
    },
    {
      "epoch": 1.125404189512161,
      "grad_norm": 1.3435908555984497,
      "learning_rate": 6.69924696664216e-05,
      "loss": 1.0861,
      "step": 8005
    },
    {
      "epoch": 1.1255447771685647,
      "grad_norm": 1.6741411685943604,
      "learning_rate": 6.721199930142827e-05,
      "loss": 1.0028,
      "step": 8006
    },
    {
      "epoch": 1.1256853648249683,
      "grad_norm": 1.367733359336853,
      "learning_rate": 6.743170612947378e-05,
      "loss": 0.9163,
      "step": 8007
    },
    {
      "epoch": 1.125825952481372,
      "grad_norm": 1.4016305208206177,
      "learning_rate": 6.765158896321767e-05,
      "loss": 0.7915,
      "step": 8008
    },
    {
      "epoch": 1.1259665401377759,
      "grad_norm": 1.424259901046753,
      "learning_rate": 6.787164661436822e-05,
      "loss": 1.084,
      "step": 8009
    },
    {
      "epoch": 1.1261071277941797,
      "grad_norm": 1.5766141414642334,
      "learning_rate": 6.809187789368876e-05,
      "loss": 1.037,
      "step": 8010
    },
    {
      "epoch": 1.1262477154505834,
      "grad_norm": 1.395946979522705,
      "learning_rate": 6.831228161100556e-05,
      "loss": 1.0191,
      "step": 8011
    },
    {
      "epoch": 1.1263883031069872,
      "grad_norm": 1.4997957944869995,
      "learning_rate": 6.853285657521098e-05,
      "loss": 1.0867,
      "step": 8012
    },
    {
      "epoch": 1.126528890763391,
      "grad_norm": 1.478792667388916,
      "learning_rate": 6.87536015942733e-05,
      "loss": 1.0163,
      "step": 8013
    },
    {
      "epoch": 1.1266694784197948,
      "grad_norm": 1.7269972562789917,
      "learning_rate": 6.897451547524145e-05,
      "loss": 0.7605,
      "step": 8014
    },
    {
      "epoch": 1.1268100660761986,
      "grad_norm": 1.7424346208572388,
      "learning_rate": 6.919559702425145e-05,
      "loss": 1.0734,
      "step": 8015
    },
    {
      "epoch": 1.1269506537326022,
      "grad_norm": 1.6768043041229248,
      "learning_rate": 6.94168450465342e-05,
      "loss": 1.1146,
      "step": 8016
    },
    {
      "epoch": 1.127091241389006,
      "grad_norm": 1.6775643825531006,
      "learning_rate": 6.963825834642e-05,
      "loss": 0.9455,
      "step": 8017
    },
    {
      "epoch": 1.1272318290454098,
      "grad_norm": 1.8776803016662598,
      "learning_rate": 6.98598357273462e-05,
      "loss": 1.0601,
      "step": 8018
    },
    {
      "epoch": 1.1273724167018135,
      "grad_norm": 1.5352288484573364,
      "learning_rate": 7.008157599186385e-05,
      "loss": 1.0118,
      "step": 8019
    },
    {
      "epoch": 1.1275130043582173,
      "grad_norm": 1.8858658075332642,
      "learning_rate": 7.030347794164265e-05,
      "loss": 1.1652,
      "step": 8020
    },
    {
      "epoch": 1.1276535920146211,
      "grad_norm": 1.5170319080352783,
      "learning_rate": 7.052554037747925e-05,
      "loss": 1.0156,
      "step": 8021
    },
    {
      "epoch": 1.127794179671025,
      "grad_norm": 1.4351270198822021,
      "learning_rate": 7.074776209930366e-05,
      "loss": 1.061,
      "step": 8022
    },
    {
      "epoch": 1.1279347673274287,
      "grad_norm": 1.4805431365966797,
      "learning_rate": 7.097014190618408e-05,
      "loss": 1.0028,
      "step": 8023
    },
    {
      "epoch": 1.1280753549838325,
      "grad_norm": 1.8345324993133545,
      "learning_rate": 7.119267859633466e-05,
      "loss": 1.0706,
      "step": 8024
    },
    {
      "epoch": 1.1282159426402363,
      "grad_norm": 1.4479743242263794,
      "learning_rate": 7.141537096712181e-05,
      "loss": 1.0576,
      "step": 8025
    },
    {
      "epoch": 1.12835653029664,
      "grad_norm": 1.4862560033798218,
      "learning_rate": 7.163821781507057e-05,
      "loss": 1.0393,
      "step": 8026
    },
    {
      "epoch": 1.1284971179530436,
      "grad_norm": 1.5192781686782837,
      "learning_rate": 7.186121793587113e-05,
      "loss": 1.1668,
      "step": 8027
    },
    {
      "epoch": 1.1286377056094474,
      "grad_norm": 1.540913462638855,
      "learning_rate": 7.208437012438539e-05,
      "loss": 1.0355,
      "step": 8028
    },
    {
      "epoch": 1.1287782932658512,
      "grad_norm": 1.3938934803009033,
      "learning_rate": 7.230767317465343e-05,
      "loss": 1.0265,
      "step": 8029
    },
    {
      "epoch": 1.128918880922255,
      "grad_norm": 1.5552793741226196,
      "learning_rate": 7.253112587990008e-05,
      "loss": 0.949,
      "step": 8030
    },
    {
      "epoch": 1.1290594685786588,
      "grad_norm": 1.3517965078353882,
      "learning_rate": 7.2754727032541e-05,
      "loss": 1.1346,
      "step": 8031
    },
    {
      "epoch": 1.1292000562350626,
      "grad_norm": 1.3898532390594482,
      "learning_rate": 7.297847542419069e-05,
      "loss": 1.1552,
      "step": 8032
    },
    {
      "epoch": 1.1293406438914664,
      "grad_norm": 1.5655263662338257,
      "learning_rate": 7.320236984566689e-05,
      "loss": 1.126,
      "step": 8033
    },
    {
      "epoch": 1.1294812315478702,
      "grad_norm": 1.5723017454147339,
      "learning_rate": 7.342640908699885e-05,
      "loss": 1.1324,
      "step": 8034
    },
    {
      "epoch": 1.129621819204274,
      "grad_norm": 1.703905701637268,
      "learning_rate": 7.365059193743215e-05,
      "loss": 1.008,
      "step": 8035
    },
    {
      "epoch": 1.1297624068606775,
      "grad_norm": 1.5481170415878296,
      "learning_rate": 7.387491718543734e-05,
      "loss": 1.0814,
      "step": 8036
    },
    {
      "epoch": 1.1299029945170813,
      "grad_norm": 1.4290508031845093,
      "learning_rate": 7.409938361871469e-05,
      "loss": 1.0678,
      "step": 8037
    },
    {
      "epoch": 1.130043582173485,
      "grad_norm": 1.4125958681106567,
      "learning_rate": 7.432399002420254e-05,
      "loss": 1.1962,
      "step": 8038
    },
    {
      "epoch": 1.130184169829889,
      "grad_norm": 1.4589862823486328,
      "learning_rate": 7.454873518808174e-05,
      "loss": 0.8538,
      "step": 8039
    },
    {
      "epoch": 1.1303247574862927,
      "grad_norm": 1.290863275527954,
      "learning_rate": 7.477361789578364e-05,
      "loss": 1.0604,
      "step": 8040
    },
    {
      "epoch": 1.1304653451426965,
      "grad_norm": 1.6728789806365967,
      "learning_rate": 7.499863693199655e-05,
      "loss": 1.2086,
      "step": 8041
    },
    {
      "epoch": 1.1306059327991003,
      "grad_norm": 1.5454550981521606,
      "learning_rate": 7.522379108067076e-05,
      "loss": 1.0184,
      "step": 8042
    },
    {
      "epoch": 1.130746520455504,
      "grad_norm": 1.338526964187622,
      "learning_rate": 7.544907912502835e-05,
      "loss": 1.0869,
      "step": 8043
    },
    {
      "epoch": 1.1308871081119078,
      "grad_norm": 1.5175895690917969,
      "learning_rate": 7.567449984756648e-05,
      "loss": 0.9275,
      "step": 8044
    },
    {
      "epoch": 1.1310276957683116,
      "grad_norm": 1.4850431680679321,
      "learning_rate": 7.590005203006559e-05,
      "loss": 1.0281,
      "step": 8045
    },
    {
      "epoch": 1.1311682834247154,
      "grad_norm": 1.5337626934051514,
      "learning_rate": 7.612573445359574e-05,
      "loss": 1.1154,
      "step": 8046
    },
    {
      "epoch": 1.131308871081119,
      "grad_norm": 1.4920343160629272,
      "learning_rate": 7.635154589852278e-05,
      "loss": 0.8925,
      "step": 8047
    },
    {
      "epoch": 1.1314494587375228,
      "grad_norm": 1.5975598096847534,
      "learning_rate": 7.65774851445163e-05,
      "loss": 0.9867,
      "step": 8048
    },
    {
      "epoch": 1.1315900463939266,
      "grad_norm": 1.7170212268829346,
      "learning_rate": 7.680355097055457e-05,
      "loss": 1.0195,
      "step": 8049
    },
    {
      "epoch": 1.1317306340503304,
      "grad_norm": 1.4739601612091064,
      "learning_rate": 7.702974215493111e-05,
      "loss": 0.8645,
      "step": 8050
    },
    {
      "epoch": 1.1318712217067342,
      "grad_norm": 1.5299092531204224,
      "learning_rate": 7.725605747526297e-05,
      "loss": 1.166,
      "step": 8051
    },
    {
      "epoch": 1.132011809363138,
      "grad_norm": 1.8312615156173706,
      "learning_rate": 7.748249570849606e-05,
      "loss": 0.9916,
      "step": 8052
    },
    {
      "epoch": 1.1321523970195417,
      "grad_norm": 1.5293668508529663,
      "learning_rate": 7.770905563091166e-05,
      "loss": 0.9705,
      "step": 8053
    },
    {
      "epoch": 1.1322929846759455,
      "grad_norm": 1.4305615425109863,
      "learning_rate": 7.793573601813444e-05,
      "loss": 0.9938,
      "step": 8054
    },
    {
      "epoch": 1.1324335723323493,
      "grad_norm": 1.5134992599487305,
      "learning_rate": 7.816253564513708e-05,
      "loss": 1.1163,
      "step": 8055
    },
    {
      "epoch": 1.1325741599887529,
      "grad_norm": 1.8135989904403687,
      "learning_rate": 7.838945328624813e-05,
      "loss": 1.0371,
      "step": 8056
    },
    {
      "epoch": 1.1327147476451567,
      "grad_norm": 1.6931174993515015,
      "learning_rate": 7.861648771515878e-05,
      "loss": 0.9288,
      "step": 8057
    },
    {
      "epoch": 1.1328553353015605,
      "grad_norm": 1.3343701362609863,
      "learning_rate": 7.884363770492765e-05,
      "loss": 1.1353,
      "step": 8058
    },
    {
      "epoch": 1.1329959229579643,
      "grad_norm": 1.5206429958343506,
      "learning_rate": 7.907090202799092e-05,
      "loss": 1.0524,
      "step": 8059
    },
    {
      "epoch": 1.133136510614368,
      "grad_norm": 1.4363166093826294,
      "learning_rate": 7.929827945616555e-05,
      "loss": 1.0702,
      "step": 8060
    },
    {
      "epoch": 1.1332770982707718,
      "grad_norm": 1.451819896697998,
      "learning_rate": 7.952576876065753e-05,
      "loss": 1.0984,
      "step": 8061
    },
    {
      "epoch": 1.1334176859271756,
      "grad_norm": 1.2559882402420044,
      "learning_rate": 7.975336871206834e-05,
      "loss": 0.994,
      "step": 8062
    },
    {
      "epoch": 1.1335582735835794,
      "grad_norm": 1.5204089879989624,
      "learning_rate": 7.998107808040143e-05,
      "loss": 1.0196,
      "step": 8063
    },
    {
      "epoch": 1.1336988612399832,
      "grad_norm": 1.4340629577636719,
      "learning_rate": 8.020889563506895e-05,
      "loss": 1.1254,
      "step": 8064
    },
    {
      "epoch": 1.133839448896387,
      "grad_norm": 1.4375121593475342,
      "learning_rate": 8.043682014489839e-05,
      "loss": 0.9375,
      "step": 8065
    },
    {
      "epoch": 1.1339800365527908,
      "grad_norm": 1.5756468772888184,
      "learning_rate": 8.066485037813928e-05,
      "loss": 1.0962,
      "step": 8066
    },
    {
      "epoch": 1.1341206242091944,
      "grad_norm": 1.3794435262680054,
      "learning_rate": 8.089298510246974e-05,
      "loss": 1.2166,
      "step": 8067
    },
    {
      "epoch": 1.1342612118655981,
      "grad_norm": 1.6047816276550293,
      "learning_rate": 8.112122308500321e-05,
      "loss": 1.0119,
      "step": 8068
    },
    {
      "epoch": 1.134401799522002,
      "grad_norm": 1.4898228645324707,
      "learning_rate": 8.13495630922948e-05,
      "loss": 0.9346,
      "step": 8069
    },
    {
      "epoch": 1.1345423871784057,
      "grad_norm": 1.4127479791641235,
      "learning_rate": 8.157800389034921e-05,
      "loss": 0.9285,
      "step": 8070
    },
    {
      "epoch": 1.1346829748348095,
      "grad_norm": 1.939805507659912,
      "learning_rate": 8.180654424462549e-05,
      "loss": 1.0632,
      "step": 8071
    },
    {
      "epoch": 1.1348235624912133,
      "grad_norm": 1.657138705253601,
      "learning_rate": 8.203518292004535e-05,
      "loss": 1.1197,
      "step": 8072
    },
    {
      "epoch": 1.134964150147617,
      "grad_norm": 1.4156373739242554,
      "learning_rate": 8.226391868099804e-05,
      "loss": 1.0846,
      "step": 8073
    },
    {
      "epoch": 1.1351047378040209,
      "grad_norm": 1.6802610158920288,
      "learning_rate": 8.24927502913489e-05,
      "loss": 1.0324,
      "step": 8074
    },
    {
      "epoch": 1.1352453254604247,
      "grad_norm": 1.6702146530151367,
      "learning_rate": 8.2721676514446e-05,
      "loss": 1.0494,
      "step": 8075
    },
    {
      "epoch": 1.1353859131168282,
      "grad_norm": 1.4069123268127441,
      "learning_rate": 8.2950696113125e-05,
      "loss": 1.1442,
      "step": 8076
    },
    {
      "epoch": 1.135526500773232,
      "grad_norm": 1.54850172996521,
      "learning_rate": 8.317980784971728e-05,
      "loss": 1.0937,
      "step": 8077
    },
    {
      "epoch": 1.1356670884296358,
      "grad_norm": 1.5763064622879028,
      "learning_rate": 8.340901048605635e-05,
      "loss": 1.0497,
      "step": 8078
    },
    {
      "epoch": 1.1358076760860396,
      "grad_norm": 1.6727176904678345,
      "learning_rate": 8.363830278348478e-05,
      "loss": 1.0954,
      "step": 8079
    },
    {
      "epoch": 1.1359482637424434,
      "grad_norm": 1.5958960056304932,
      "learning_rate": 8.386768350285921e-05,
      "loss": 0.9607,
      "step": 8080
    },
    {
      "epoch": 1.1360888513988472,
      "grad_norm": 1.4031168222427368,
      "learning_rate": 8.409715140456055e-05,
      "loss": 1.0892,
      "step": 8081
    },
    {
      "epoch": 1.136229439055251,
      "grad_norm": 1.4835937023162842,
      "learning_rate": 8.432670524849711e-05,
      "loss": 1.2137,
      "step": 8082
    },
    {
      "epoch": 1.1363700267116548,
      "grad_norm": 1.6977176666259766,
      "learning_rate": 8.455634379411314e-05,
      "loss": 1.1355,
      "step": 8083
    },
    {
      "epoch": 1.1365106143680586,
      "grad_norm": 1.5869245529174805,
      "learning_rate": 8.478606580039512e-05,
      "loss": 1.1024,
      "step": 8084
    },
    {
      "epoch": 1.1366512020244623,
      "grad_norm": 1.5953090190887451,
      "learning_rate": 8.501587002587818e-05,
      "loss": 1.1033,
      "step": 8085
    },
    {
      "epoch": 1.1367917896808661,
      "grad_norm": 1.6837549209594727,
      "learning_rate": 8.524575522865412e-05,
      "loss": 1.1436,
      "step": 8086
    },
    {
      "epoch": 1.1369323773372697,
      "grad_norm": 1.448849081993103,
      "learning_rate": 8.547572016637648e-05,
      "loss": 1.0564,
      "step": 8087
    },
    {
      "epoch": 1.1370729649936735,
      "grad_norm": 1.5836659669876099,
      "learning_rate": 8.570576359626712e-05,
      "loss": 0.9394,
      "step": 8088
    },
    {
      "epoch": 1.1372135526500773,
      "grad_norm": 1.4439525604248047,
      "learning_rate": 8.593588427512484e-05,
      "loss": 1.005,
      "step": 8089
    },
    {
      "epoch": 1.137354140306481,
      "grad_norm": 1.3903321027755737,
      "learning_rate": 8.616608095933021e-05,
      "loss": 1.0341,
      "step": 8090
    },
    {
      "epoch": 1.1374947279628849,
      "grad_norm": 1.4931284189224243,
      "learning_rate": 8.639635240485416e-05,
      "loss": 1.0775,
      "step": 8091
    },
    {
      "epoch": 1.1376353156192887,
      "grad_norm": 1.5026417970657349,
      "learning_rate": 8.662669736726248e-05,
      "loss": 1.0235,
      "step": 8092
    },
    {
      "epoch": 1.1377759032756924,
      "grad_norm": 1.5695489645004272,
      "learning_rate": 8.685711460172408e-05,
      "loss": 0.9293,
      "step": 8093
    },
    {
      "epoch": 1.1379164909320962,
      "grad_norm": 1.5925483703613281,
      "learning_rate": 8.708760286301724e-05,
      "loss": 1.245,
      "step": 8094
    },
    {
      "epoch": 1.1380570785885,
      "grad_norm": 1.7185925245285034,
      "learning_rate": 8.731816090553674e-05,
      "loss": 1.1509,
      "step": 8095
    },
    {
      "epoch": 1.1381976662449036,
      "grad_norm": 1.5592790842056274,
      "learning_rate": 8.754878748329892e-05,
      "loss": 1.1224,
      "step": 8096
    },
    {
      "epoch": 1.1383382539013074,
      "grad_norm": 2.231571912765503,
      "learning_rate": 8.777948134995177e-05,
      "loss": 0.9838,
      "step": 8097
    },
    {
      "epoch": 1.1384788415577112,
      "grad_norm": 1.4910058975219727,
      "learning_rate": 8.801024125877827e-05,
      "loss": 0.9576,
      "step": 8098
    },
    {
      "epoch": 1.138619429214115,
      "grad_norm": 1.6886614561080933,
      "learning_rate": 8.824106596270485e-05,
      "loss": 0.9756,
      "step": 8099
    },
    {
      "epoch": 1.1387600168705188,
      "grad_norm": 1.4869235754013062,
      "learning_rate": 8.847195421430773e-05,
      "loss": 1.1464,
      "step": 8100
    },
    {
      "epoch": 1.1389006045269225,
      "grad_norm": 1.7200859785079956,
      "learning_rate": 8.870290476581941e-05,
      "loss": 1.1529,
      "step": 8101
    },
    {
      "epoch": 1.1390411921833263,
      "grad_norm": 1.4104657173156738,
      "learning_rate": 8.893391636913709e-05,
      "loss": 1.0409,
      "step": 8102
    },
    {
      "epoch": 1.1391817798397301,
      "grad_norm": 1.5294945240020752,
      "learning_rate": 8.916498777582591e-05,
      "loss": 1.081,
      "step": 8103
    },
    {
      "epoch": 1.139322367496134,
      "grad_norm": 2.1808817386627197,
      "learning_rate": 8.939611773712927e-05,
      "loss": 1.0447,
      "step": 8104
    },
    {
      "epoch": 1.1394629551525377,
      "grad_norm": 1.4282207489013672,
      "learning_rate": 8.962730500397371e-05,
      "loss": 1.1792,
      "step": 8105
    },
    {
      "epoch": 1.1396035428089415,
      "grad_norm": 1.372712254524231,
      "learning_rate": 8.985854832697614e-05,
      "loss": 1.293,
      "step": 8106
    },
    {
      "epoch": 1.139744130465345,
      "grad_norm": 1.7007818222045898,
      "learning_rate": 9.008984645645016e-05,
      "loss": 1.0067,
      "step": 8107
    },
    {
      "epoch": 1.1398847181217489,
      "grad_norm": 1.4119077920913696,
      "learning_rate": 9.032119814241424e-05,
      "loss": 1.0932,
      "step": 8108
    },
    {
      "epoch": 1.1400253057781526,
      "grad_norm": 1.8872010707855225,
      "learning_rate": 9.055260213459634e-05,
      "loss": 1.057,
      "step": 8109
    },
    {
      "epoch": 1.1401658934345564,
      "grad_norm": 1.4098925590515137,
      "learning_rate": 9.07840571824425e-05,
      "loss": 1.0345,
      "step": 8110
    },
    {
      "epoch": 1.1403064810909602,
      "grad_norm": 1.6154896020889282,
      "learning_rate": 9.101556203512173e-05,
      "loss": 0.9379,
      "step": 8111
    },
    {
      "epoch": 1.140447068747364,
      "grad_norm": 1.8330755233764648,
      "learning_rate": 9.124711544153463e-05,
      "loss": 0.8519,
      "step": 8112
    },
    {
      "epoch": 1.1405876564037678,
      "grad_norm": 1.7510950565338135,
      "learning_rate": 9.147871615032012e-05,
      "loss": 1.0261,
      "step": 8113
    },
    {
      "epoch": 1.1407282440601716,
      "grad_norm": 1.4486825466156006,
      "learning_rate": 9.17103629098604e-05,
      "loss": 1.2231,
      "step": 8114
    },
    {
      "epoch": 1.1408688317165754,
      "grad_norm": 1.3956410884857178,
      "learning_rate": 9.194205446828915e-05,
      "loss": 1.16,
      "step": 8115
    },
    {
      "epoch": 1.141009419372979,
      "grad_norm": 1.8485442399978638,
      "learning_rate": 9.217378957349799e-05,
      "loss": 1.1682,
      "step": 8116
    },
    {
      "epoch": 1.1411500070293827,
      "grad_norm": 1.568904161453247,
      "learning_rate": 9.240556697314315e-05,
      "loss": 1.0302,
      "step": 8117
    },
    {
      "epoch": 1.1412905946857865,
      "grad_norm": 1.3443504571914673,
      "learning_rate": 9.263738541465233e-05,
      "loss": 1.043,
      "step": 8118
    },
    {
      "epoch": 1.1414311823421903,
      "grad_norm": 1.8113776445388794,
      "learning_rate": 9.286924364523144e-05,
      "loss": 1.0703,
      "step": 8119
    },
    {
      "epoch": 1.141571769998594,
      "grad_norm": 1.5262572765350342,
      "learning_rate": 9.310114041187131e-05,
      "loss": 1.0363,
      "step": 8120
    },
    {
      "epoch": 1.141712357654998,
      "grad_norm": 1.4574187994003296,
      "learning_rate": 9.333307446135457e-05,
      "loss": 1.0402,
      "step": 8121
    },
    {
      "epoch": 1.1418529453114017,
      "grad_norm": 1.4722541570663452,
      "learning_rate": 9.356504454026235e-05,
      "loss": 1.09,
      "step": 8122
    },
    {
      "epoch": 1.1419935329678055,
      "grad_norm": 1.620560646057129,
      "learning_rate": 9.379704939498068e-05,
      "loss": 0.9439,
      "step": 8123
    },
    {
      "epoch": 1.1421341206242093,
      "grad_norm": 1.4312089681625366,
      "learning_rate": 9.402908777170876e-05,
      "loss": 1.0077,
      "step": 8124
    },
    {
      "epoch": 1.1422747082806128,
      "grad_norm": 2.208932876586914,
      "learning_rate": 9.426115841646392e-05,
      "loss": 1.037,
      "step": 8125
    },
    {
      "epoch": 1.1424152959370169,
      "grad_norm": 1.561514139175415,
      "learning_rate": 9.449326007508836e-05,
      "loss": 1.0566,
      "step": 8126
    },
    {
      "epoch": 1.1425558835934204,
      "grad_norm": 1.4266459941864014,
      "learning_rate": 9.472539149325776e-05,
      "loss": 0.7757,
      "step": 8127
    },
    {
      "epoch": 1.1426964712498242,
      "grad_norm": 1.591823935508728,
      "learning_rate": 9.495755141648629e-05,
      "loss": 1.1939,
      "step": 8128
    },
    {
      "epoch": 1.142837058906228,
      "grad_norm": 1.519750714302063,
      "learning_rate": 9.51897385901351e-05,
      "loss": 1.1132,
      "step": 8129
    },
    {
      "epoch": 1.1429776465626318,
      "grad_norm": 1.5695440769195557,
      "learning_rate": 9.542195175941704e-05,
      "loss": 1.1057,
      "step": 8130
    },
    {
      "epoch": 1.1431182342190356,
      "grad_norm": 2.0016751289367676,
      "learning_rate": 9.565418966940479e-05,
      "loss": 1.0059,
      "step": 8131
    },
    {
      "epoch": 1.1432588218754394,
      "grad_norm": 1.4541233777999878,
      "learning_rate": 9.588645106503735e-05,
      "loss": 1.086,
      "step": 8132
    },
    {
      "epoch": 1.1433994095318432,
      "grad_norm": 1.4928820133209229,
      "learning_rate": 9.611873469112718e-05,
      "loss": 1.2189,
      "step": 8133
    },
    {
      "epoch": 1.143539997188247,
      "grad_norm": 1.4646990299224854,
      "learning_rate": 9.635103929236512e-05,
      "loss": 1.1112,
      "step": 8134
    },
    {
      "epoch": 1.1436805848446507,
      "grad_norm": 1.7736905813217163,
      "learning_rate": 9.658336361333083e-05,
      "loss": 0.9291,
      "step": 8135
    },
    {
      "epoch": 1.1438211725010543,
      "grad_norm": 1.59364914894104,
      "learning_rate": 9.681570639849599e-05,
      "loss": 1.0028,
      "step": 8136
    },
    {
      "epoch": 1.143961760157458,
      "grad_norm": 1.8341081142425537,
      "learning_rate": 9.70480663922328e-05,
      "loss": 1.0956,
      "step": 8137
    },
    {
      "epoch": 1.1441023478138619,
      "grad_norm": 1.645006537437439,
      "learning_rate": 9.728044233882051e-05,
      "loss": 1.0742,
      "step": 8138
    },
    {
      "epoch": 1.1442429354702657,
      "grad_norm": 1.6721842288970947,
      "learning_rate": 9.751283298245178e-05,
      "loss": 1.0784,
      "step": 8139
    },
    {
      "epoch": 1.1443835231266695,
      "grad_norm": 1.7532727718353271,
      "learning_rate": 9.774523706724129e-05,
      "loss": 1.2384,
      "step": 8140
    },
    {
      "epoch": 1.1445241107830733,
      "grad_norm": 1.5918586254119873,
      "learning_rate": 9.797765333722891e-05,
      "loss": 0.965,
      "step": 8141
    },
    {
      "epoch": 1.144664698439477,
      "grad_norm": 1.4208433628082275,
      "learning_rate": 9.82100805363901e-05,
      "loss": 1.0244,
      "step": 8142
    },
    {
      "epoch": 1.1448052860958808,
      "grad_norm": 1.5741428136825562,
      "learning_rate": 9.84425174086409e-05,
      "loss": 1.044,
      "step": 8143
    },
    {
      "epoch": 1.1449458737522846,
      "grad_norm": 1.50959312915802,
      "learning_rate": 9.867496269784472e-05,
      "loss": 1.1458,
      "step": 8144
    },
    {
      "epoch": 1.1450864614086882,
      "grad_norm": 1.9365026950836182,
      "learning_rate": 9.890741514782056e-05,
      "loss": 1.0406,
      "step": 8145
    },
    {
      "epoch": 1.1452270490650922,
      "grad_norm": 1.2997233867645264,
      "learning_rate": 9.913987350234763e-05,
      "loss": 1.2364,
      "step": 8146
    },
    {
      "epoch": 1.1453676367214958,
      "grad_norm": 1.365930199623108,
      "learning_rate": 9.937233650517363e-05,
      "loss": 1.0935,
      "step": 8147
    },
    {
      "epoch": 1.1455082243778996,
      "grad_norm": 1.5654751062393188,
      "learning_rate": 9.960480290002143e-05,
      "loss": 0.9849,
      "step": 8148
    },
    {
      "epoch": 1.1456488120343034,
      "grad_norm": 1.6099398136138916,
      "learning_rate": 9.983727143059455e-05,
      "loss": 1.0914,
      "step": 8149
    },
    {
      "epoch": 1.1457893996907071,
      "grad_norm": 1.402510404586792,
      "learning_rate": 0.00010006974084058569,
      "loss": 1.099,
      "step": 8150
    },
    {
      "epoch": 1.145929987347111,
      "grad_norm": 1.3939307928085327,
      "learning_rate": 0.00010030220987368346,
      "loss": 0.905,
      "step": 8151
    },
    {
      "epoch": 1.1460705750035147,
      "grad_norm": 1.608098030090332,
      "learning_rate": 0.00010053467727357748,
      "loss": 0.9504,
      "step": 8152
    },
    {
      "epoch": 1.1462111626599185,
      "grad_norm": 1.614644169807434,
      "learning_rate": 0.00010076714178396652,
      "loss": 0.9555,
      "step": 8153
    },
    {
      "epoch": 1.1463517503163223,
      "grad_norm": 1.511731505393982,
      "learning_rate": 0.000100999602148565,
      "loss": 1.2949,
      "step": 8154
    },
    {
      "epoch": 1.146492337972726,
      "grad_norm": 1.419305682182312,
      "learning_rate": 0.00010123205711110973,
      "loss": 1.1461,
      "step": 8155
    },
    {
      "epoch": 1.1466329256291297,
      "grad_norm": 1.5146242380142212,
      "learning_rate": 0.00010146450541536669,
      "loss": 1.1469,
      "step": 8156
    },
    {
      "epoch": 1.1467735132855335,
      "grad_norm": 1.518727421760559,
      "learning_rate": 0.00010169694580513789,
      "loss": 1.0165,
      "step": 8157
    },
    {
      "epoch": 1.1469141009419372,
      "grad_norm": 1.3423038721084595,
      "learning_rate": 0.00010192937702426808,
      "loss": 0.9795,
      "step": 8158
    },
    {
      "epoch": 1.147054688598341,
      "grad_norm": 1.461344838142395,
      "learning_rate": 0.00010216179781665158,
      "loss": 1.1341,
      "step": 8159
    },
    {
      "epoch": 1.1471952762547448,
      "grad_norm": 1.702987790107727,
      "learning_rate": 0.00010239420692623869,
      "loss": 0.9776,
      "step": 8160
    },
    {
      "epoch": 1.1473358639111486,
      "grad_norm": 1.744562029838562,
      "learning_rate": 0.0001026266030970439,
      "loss": 1.0873,
      "step": 8161
    },
    {
      "epoch": 1.1474764515675524,
      "grad_norm": 1.6018376350402832,
      "learning_rate": 0.00010285898507315061,
      "loss": 1.1376,
      "step": 8162
    },
    {
      "epoch": 1.1476170392239562,
      "grad_norm": 1.300618052482605,
      "learning_rate": 0.00010309135159871963,
      "loss": 1.0618,
      "step": 8163
    },
    {
      "epoch": 1.14775762688036,
      "grad_norm": 1.545748233795166,
      "learning_rate": 0.00010332370141799412,
      "loss": 1.143,
      "step": 8164
    },
    {
      "epoch": 1.1478982145367636,
      "grad_norm": 1.4827675819396973,
      "learning_rate": 0.00010355603327530872,
      "loss": 1.0926,
      "step": 8165
    },
    {
      "epoch": 1.1480388021931676,
      "grad_norm": 1.5785294771194458,
      "learning_rate": 0.00010378834591509431,
      "loss": 0.9517,
      "step": 8166
    },
    {
      "epoch": 1.1481793898495711,
      "grad_norm": 1.474131464958191,
      "learning_rate": 0.00010402063808188678,
      "loss": 0.9602,
      "step": 8167
    },
    {
      "epoch": 1.148319977505975,
      "grad_norm": 1.5092469453811646,
      "learning_rate": 0.00010425290852033153,
      "loss": 1.0296,
      "step": 8168
    },
    {
      "epoch": 1.1484605651623787,
      "grad_norm": 1.681155800819397,
      "learning_rate": 0.0001044851559751918,
      "loss": 1.1493,
      "step": 8169
    },
    {
      "epoch": 1.1486011528187825,
      "grad_norm": 1.9253419637680054,
      "learning_rate": 0.00010471737919135499,
      "loss": 0.9837,
      "step": 8170
    },
    {
      "epoch": 1.1487417404751863,
      "grad_norm": 1.6649043560028076,
      "learning_rate": 0.00010494957691383947,
      "loss": 0.9443,
      "step": 8171
    },
    {
      "epoch": 1.14888232813159,
      "grad_norm": 1.6244556903839111,
      "learning_rate": 0.00010518174788780149,
      "loss": 0.9826,
      "step": 8172
    },
    {
      "epoch": 1.1490229157879939,
      "grad_norm": 1.5608835220336914,
      "learning_rate": 0.00010541389085854172,
      "loss": 1.1491,
      "step": 8173
    },
    {
      "epoch": 1.1491635034443977,
      "grad_norm": 1.7213412523269653,
      "learning_rate": 0.00010564600457151226,
      "loss": 0.9341,
      "step": 8174
    },
    {
      "epoch": 1.1493040911008015,
      "grad_norm": 2.313764810562134,
      "learning_rate": 0.00010587808777232328,
      "loss": 1.125,
      "step": 8175
    },
    {
      "epoch": 1.149444678757205,
      "grad_norm": 1.4285316467285156,
      "learning_rate": 0.0001061101392067499,
      "loss": 1.0681,
      "step": 8176
    },
    {
      "epoch": 1.1495852664136088,
      "grad_norm": 1.47178316116333,
      "learning_rate": 0.00010634215762073848,
      "loss": 0.9937,
      "step": 8177
    },
    {
      "epoch": 1.1497258540700126,
      "grad_norm": 1.4353749752044678,
      "learning_rate": 0.00010657414176041527,
      "loss": 1.0653,
      "step": 8178
    },
    {
      "epoch": 1.1498664417264164,
      "grad_norm": 1.6498918533325195,
      "learning_rate": 0.00010680609037208966,
      "loss": 0.9499,
      "step": 8179
    },
    {
      "epoch": 1.1500070293828202,
      "grad_norm": 1.4358419179916382,
      "learning_rate": 0.00010703800220226441,
      "loss": 1.0574,
      "step": 8180
    },
    {
      "epoch": 1.150147617039224,
      "grad_norm": 1.703255534172058,
      "learning_rate": 0.00010726987599764074,
      "loss": 1.046,
      "step": 8181
    },
    {
      "epoch": 1.1502882046956278,
      "grad_norm": 1.6140501499176025,
      "learning_rate": 0.00010750171050512503,
      "loss": 1.119,
      "step": 8182
    },
    {
      "epoch": 1.1504287923520315,
      "grad_norm": 1.5477116107940674,
      "learning_rate": 0.00010773350447183703,
      "loss": 1.0203,
      "step": 8183
    },
    {
      "epoch": 1.1505693800084353,
      "grad_norm": 1.7722476720809937,
      "learning_rate": 0.00010796525664511461,
      "loss": 0.9643,
      "step": 8184
    },
    {
      "epoch": 1.150709967664839,
      "grad_norm": 1.8266323804855347,
      "learning_rate": 0.00010819696577252175,
      "loss": 1.0664,
      "step": 8185
    },
    {
      "epoch": 1.150850555321243,
      "grad_norm": 1.3453068733215332,
      "learning_rate": 0.00010842863060185541,
      "loss": 1.2161,
      "step": 8186
    },
    {
      "epoch": 1.1509911429776465,
      "grad_norm": 1.7074412107467651,
      "learning_rate": 0.00010866024988115065,
      "loss": 0.9951,
      "step": 8187
    },
    {
      "epoch": 1.1511317306340503,
      "grad_norm": 1.7408623695373535,
      "learning_rate": 0.00010889182235869069,
      "loss": 1.0416,
      "step": 8188
    },
    {
      "epoch": 1.151272318290454,
      "grad_norm": 1.749251365661621,
      "learning_rate": 0.0001091233467830103,
      "loss": 0.8819,
      "step": 8189
    },
    {
      "epoch": 1.1514129059468579,
      "grad_norm": 1.4665287733078003,
      "learning_rate": 0.00010935482190290433,
      "loss": 0.9449,
      "step": 8190
    },
    {
      "epoch": 1.1515534936032616,
      "grad_norm": 1.404467225074768,
      "learning_rate": 0.00010958624646743403,
      "loss": 1.0134,
      "step": 8191
    },
    {
      "epoch": 1.1516940812596654,
      "grad_norm": 1.614733338356018,
      "learning_rate": 0.0001098176192259339,
      "loss": 1.0972,
      "step": 8192
    },
    {
      "epoch": 1.1518346689160692,
      "grad_norm": 1.5031224489212036,
      "learning_rate": 0.00011004893892801802,
      "loss": 1.0694,
      "step": 8193
    },
    {
      "epoch": 1.151975256572473,
      "grad_norm": 1.551346778869629,
      "learning_rate": 0.00011028020432358866,
      "loss": 1.0022,
      "step": 8194
    },
    {
      "epoch": 1.1521158442288768,
      "grad_norm": 1.607089877128601,
      "learning_rate": 0.00011051141416283939,
      "loss": 0.9662,
      "step": 8195
    },
    {
      "epoch": 1.1522564318852804,
      "grad_norm": 1.5204442739486694,
      "learning_rate": 0.00011074256719626548,
      "loss": 1.0417,
      "step": 8196
    },
    {
      "epoch": 1.1523970195416842,
      "grad_norm": 1.4435614347457886,
      "learning_rate": 0.00011097366217466883,
      "loss": 1.1352,
      "step": 8197
    },
    {
      "epoch": 1.152537607198088,
      "grad_norm": 1.5708372592926025,
      "learning_rate": 0.0001112046978491647,
      "loss": 1.0576,
      "step": 8198
    },
    {
      "epoch": 1.1526781948544917,
      "grad_norm": 1.5790884494781494,
      "learning_rate": 0.00011143567297118993,
      "loss": 0.9075,
      "step": 8199
    },
    {
      "epoch": 1.1528187825108955,
      "grad_norm": 1.4454033374786377,
      "learning_rate": 0.00011166658629250754,
      "loss": 1.0679,
      "step": 8200
    },
    {
      "epoch": 1.1529593701672993,
      "grad_norm": 1.551030158996582,
      "learning_rate": 0.00011189743656521523,
      "loss": 1.2193,
      "step": 8201
    },
    {
      "epoch": 1.1530999578237031,
      "grad_norm": 1.3331315517425537,
      "learning_rate": 0.00011212822254175035,
      "loss": 0.9727,
      "step": 8202
    },
    {
      "epoch": 1.153240545480107,
      "grad_norm": 1.7583616971969604,
      "learning_rate": 0.00011235894297489844,
      "loss": 1.1461,
      "step": 8203
    },
    {
      "epoch": 1.1533811331365107,
      "grad_norm": 1.6058393716812134,
      "learning_rate": 0.00011258959661779998,
      "loss": 0.9817,
      "step": 8204
    },
    {
      "epoch": 1.1535217207929143,
      "grad_norm": 1.4275188446044922,
      "learning_rate": 0.00011282018222395529,
      "loss": 0.893,
      "step": 8205
    },
    {
      "epoch": 1.1536623084493183,
      "grad_norm": 1.4375951290130615,
      "learning_rate": 0.0001130506985472328,
      "loss": 0.9827,
      "step": 8206
    },
    {
      "epoch": 1.1538028961057218,
      "grad_norm": 1.643064260482788,
      "learning_rate": 0.00011328114434187527,
      "loss": 1.0765,
      "step": 8207
    },
    {
      "epoch": 1.1539434837621256,
      "grad_norm": 1.4135535955429077,
      "learning_rate": 0.00011351151836250666,
      "loss": 1.218,
      "step": 8208
    },
    {
      "epoch": 1.1540840714185294,
      "grad_norm": 1.3907957077026367,
      "learning_rate": 0.00011374181936413881,
      "loss": 1.1454,
      "step": 8209
    },
    {
      "epoch": 1.1542246590749332,
      "grad_norm": 1.4862066507339478,
      "learning_rate": 0.00011397204610217817,
      "loss": 1.1356,
      "step": 8210
    },
    {
      "epoch": 1.154365246731337,
      "grad_norm": 1.3993346691131592,
      "learning_rate": 0.00011420219733243254,
      "loss": 1.0312,
      "step": 8211
    },
    {
      "epoch": 1.1545058343877408,
      "grad_norm": 1.4550132751464844,
      "learning_rate": 0.00011443227181111773,
      "loss": 0.9597,
      "step": 8212
    },
    {
      "epoch": 1.1546464220441446,
      "grad_norm": 1.7249163389205933,
      "learning_rate": 0.0001146622682948644,
      "loss": 0.893,
      "step": 8213
    },
    {
      "epoch": 1.1547870097005484,
      "grad_norm": 1.5152353048324585,
      "learning_rate": 0.00011489218554072425,
      "loss": 1.0215,
      "step": 8214
    },
    {
      "epoch": 1.1549275973569522,
      "grad_norm": 1.7051892280578613,
      "learning_rate": 0.00011512202230617842,
      "loss": 1.1058,
      "step": 8215
    },
    {
      "epoch": 1.1550681850133557,
      "grad_norm": 1.5688424110412598,
      "learning_rate": 0.00011535177734914209,
      "loss": 1.2105,
      "step": 8216
    },
    {
      "epoch": 1.1552087726697595,
      "grad_norm": 1.5014439821243286,
      "learning_rate": 0.00011558144942797153,
      "loss": 1.0736,
      "step": 8217
    },
    {
      "epoch": 1.1553493603261633,
      "grad_norm": 1.657265305519104,
      "learning_rate": 0.00011581103730147236,
      "loss": 0.9302,
      "step": 8218
    },
    {
      "epoch": 1.155489947982567,
      "grad_norm": 1.6987712383270264,
      "learning_rate": 0.0001160405397289049,
      "loss": 1.1205,
      "step": 8219
    },
    {
      "epoch": 1.155630535638971,
      "grad_norm": 1.8139792680740356,
      "learning_rate": 0.00011626995546999091,
      "loss": 1.0632,
      "step": 8220
    },
    {
      "epoch": 1.1557711232953747,
      "grad_norm": 1.5610498189926147,
      "learning_rate": 0.00011649928328492165,
      "loss": 0.9798,
      "step": 8221
    },
    {
      "epoch": 1.1559117109517785,
      "grad_norm": 1.4376550912857056,
      "learning_rate": 0.00011672852193436255,
      "loss": 1.1853,
      "step": 8222
    },
    {
      "epoch": 1.1560522986081823,
      "grad_norm": 1.7605502605438232,
      "learning_rate": 0.00011695767017946117,
      "loss": 1.1228,
      "step": 8223
    },
    {
      "epoch": 1.156192886264586,
      "grad_norm": 1.4173846244812012,
      "learning_rate": 0.00011718672678185411,
      "loss": 0.9022,
      "step": 8224
    },
    {
      "epoch": 1.1563334739209896,
      "grad_norm": 1.3553671836853027,
      "learning_rate": 0.0001174156905036717,
      "loss": 1.073,
      "step": 8225
    },
    {
      "epoch": 1.1564740615773934,
      "grad_norm": 1.406603455543518,
      "learning_rate": 0.00011764456010754837,
      "loss": 1.1337,
      "step": 8226
    },
    {
      "epoch": 1.1566146492337972,
      "grad_norm": 1.4768558740615845,
      "learning_rate": 0.0001178733343566258,
      "loss": 1.1655,
      "step": 8227
    },
    {
      "epoch": 1.156755236890201,
      "grad_norm": 1.411938190460205,
      "learning_rate": 0.0001181020120145613,
      "loss": 1.0948,
      "step": 8228
    },
    {
      "epoch": 1.1568958245466048,
      "grad_norm": 1.73198664188385,
      "learning_rate": 0.00011833059184553421,
      "loss": 1.0349,
      "step": 8229
    },
    {
      "epoch": 1.1570364122030086,
      "grad_norm": 1.6730529069900513,
      "learning_rate": 0.00011855907261425219,
      "loss": 0.9374,
      "step": 8230
    },
    {
      "epoch": 1.1571769998594124,
      "grad_norm": 1.9239482879638672,
      "learning_rate": 0.00011878745308595927,
      "loss": 0.9469,
      "step": 8231
    },
    {
      "epoch": 1.1573175875158161,
      "grad_norm": 1.4010392427444458,
      "learning_rate": 0.00011901573202644085,
      "loss": 1.1227,
      "step": 8232
    },
    {
      "epoch": 1.15745817517222,
      "grad_norm": 1.7060394287109375,
      "learning_rate": 0.00011924390820203029,
      "loss": 0.993,
      "step": 8233
    },
    {
      "epoch": 1.1575987628286237,
      "grad_norm": 1.8673629760742188,
      "learning_rate": 0.0001194719803796174,
      "loss": 1.0278,
      "step": 8234
    },
    {
      "epoch": 1.1577393504850275,
      "grad_norm": 1.7599153518676758,
      "learning_rate": 0.00011969994732665364,
      "loss": 0.9686,
      "step": 8235
    },
    {
      "epoch": 1.157879938141431,
      "grad_norm": 1.5975189208984375,
      "learning_rate": 0.0001199278078111588,
      "loss": 1.1361,
      "step": 8236
    },
    {
      "epoch": 1.1580205257978349,
      "grad_norm": 1.6865031719207764,
      "learning_rate": 0.0001201555606017291,
      "loss": 1.1354,
      "step": 8237
    },
    {
      "epoch": 1.1581611134542387,
      "grad_norm": 1.9023773670196533,
      "learning_rate": 0.00012038320446754166,
      "loss": 1.1243,
      "step": 8238
    },
    {
      "epoch": 1.1583017011106425,
      "grad_norm": 1.3433409929275513,
      "learning_rate": 0.000120610738178363,
      "loss": 1.0668,
      "step": 8239
    },
    {
      "epoch": 1.1584422887670462,
      "grad_norm": 1.7242947816848755,
      "learning_rate": 0.00012083816050455383,
      "loss": 1.1096,
      "step": 8240
    },
    {
      "epoch": 1.15858287642345,
      "grad_norm": 1.6822878122329712,
      "learning_rate": 0.00012106547021707762,
      "loss": 1.2539,
      "step": 8241
    },
    {
      "epoch": 1.1587234640798538,
      "grad_norm": 1.682541012763977,
      "learning_rate": 0.00012129266608750697,
      "loss": 1.0683,
      "step": 8242
    },
    {
      "epoch": 1.1588640517362576,
      "grad_norm": 1.7409876585006714,
      "learning_rate": 0.0001215197468880288,
      "loss": 1.1135,
      "step": 8243
    },
    {
      "epoch": 1.1590046393926614,
      "grad_norm": 1.8751074075698853,
      "learning_rate": 0.00012174671139145214,
      "loss": 1.0439,
      "step": 8244
    },
    {
      "epoch": 1.159145227049065,
      "grad_norm": 1.410535454750061,
      "learning_rate": 0.00012197355837121459,
      "loss": 0.9818,
      "step": 8245
    },
    {
      "epoch": 1.1592858147054688,
      "grad_norm": 1.6034044027328491,
      "learning_rate": 0.00012220028660138844,
      "loss": 0.9982,
      "step": 8246
    },
    {
      "epoch": 1.1594264023618726,
      "grad_norm": 1.213447093963623,
      "learning_rate": 0.00012242689485668918,
      "loss": 1.199,
      "step": 8247
    },
    {
      "epoch": 1.1595669900182763,
      "grad_norm": 1.5562199354171753,
      "learning_rate": 0.0001226533819124786,
      "loss": 1.0849,
      "step": 8248
    },
    {
      "epoch": 1.1597075776746801,
      "grad_norm": 1.5100189447402954,
      "learning_rate": 0.0001228797465447748,
      "loss": 1.0716,
      "step": 8249
    },
    {
      "epoch": 1.159848165331084,
      "grad_norm": 1.3887306451797485,
      "learning_rate": 0.00012310598753025717,
      "loss": 0.9908,
      "step": 8250
    },
    {
      "epoch": 1.1599887529874877,
      "grad_norm": 1.3725030422210693,
      "learning_rate": 0.00012333210364627332,
      "loss": 1.1234,
      "step": 8251
    },
    {
      "epoch": 1.1601293406438915,
      "grad_norm": 1.29063880443573,
      "learning_rate": 0.00012355809367084532,
      "loss": 1.1604,
      "step": 8252
    },
    {
      "epoch": 1.1602699283002953,
      "grad_norm": 1.6574503183364868,
      "learning_rate": 0.00012378395638267773,
      "loss": 1.0493,
      "step": 8253
    },
    {
      "epoch": 1.160410515956699,
      "grad_norm": 1.5438109636306763,
      "learning_rate": 0.00012400969056116236,
      "loss": 0.9398,
      "step": 8254
    },
    {
      "epoch": 1.1605511036131029,
      "grad_norm": 1.4350534677505493,
      "learning_rate": 0.00012423529498638506,
      "loss": 1.0953,
      "step": 8255
    },
    {
      "epoch": 1.1606916912695064,
      "grad_norm": 1.655397891998291,
      "learning_rate": 0.00012446076843913387,
      "loss": 0.9106,
      "step": 8256
    },
    {
      "epoch": 1.1608322789259102,
      "grad_norm": 1.601095199584961,
      "learning_rate": 0.0001246861097009039,
      "loss": 0.9751,
      "step": 8257
    },
    {
      "epoch": 1.160972866582314,
      "grad_norm": 1.5275670289993286,
      "learning_rate": 0.0001249113175539058,
      "loss": 1.178,
      "step": 8258
    },
    {
      "epoch": 1.1611134542387178,
      "grad_norm": 1.4212321043014526,
      "learning_rate": 0.00012513639078107005,
      "loss": 1.183,
      "step": 8259
    },
    {
      "epoch": 1.1612540418951216,
      "grad_norm": 1.4805241823196411,
      "learning_rate": 0.00012536132816605506,
      "loss": 1.0818,
      "step": 8260
    },
    {
      "epoch": 1.1613946295515254,
      "grad_norm": 1.5284541845321655,
      "learning_rate": 0.00012558612849325335,
      "loss": 1.0766,
      "step": 8261
    },
    {
      "epoch": 1.1615352172079292,
      "grad_norm": 1.3925724029541016,
      "learning_rate": 0.0001258107905477985,
      "loss": 1.2172,
      "step": 8262
    },
    {
      "epoch": 1.161675804864333,
      "grad_norm": 1.4143240451812744,
      "learning_rate": 0.0001260353131155699,
      "loss": 0.9649,
      "step": 8263
    },
    {
      "epoch": 1.1618163925207368,
      "grad_norm": 1.6125093698501587,
      "learning_rate": 0.0001262596949832029,
      "loss": 1.0823,
      "step": 8264
    },
    {
      "epoch": 1.1619569801771403,
      "grad_norm": 1.6558151245117188,
      "learning_rate": 0.00012648393493809175,
      "loss": 1.0806,
      "step": 8265
    },
    {
      "epoch": 1.1620975678335441,
      "grad_norm": 1.4713127613067627,
      "learning_rate": 0.0001267080317683981,
      "loss": 1.0859,
      "step": 8266
    },
    {
      "epoch": 1.162238155489948,
      "grad_norm": 1.518877625465393,
      "learning_rate": 0.00012693198426305696,
      "loss": 0.8892,
      "step": 8267
    },
    {
      "epoch": 1.1623787431463517,
      "grad_norm": 1.5974862575531006,
      "learning_rate": 0.0001271557912117831,
      "loss": 0.9795,
      "step": 8268
    },
    {
      "epoch": 1.1625193308027555,
      "grad_norm": 1.6890313625335693,
      "learning_rate": 0.0001273794514050788,
      "loss": 1.1044,
      "step": 8269
    },
    {
      "epoch": 1.1626599184591593,
      "grad_norm": 1.608190894126892,
      "learning_rate": 0.00012760296363423877,
      "loss": 1.1662,
      "step": 8270
    },
    {
      "epoch": 1.162800506115563,
      "grad_norm": 1.5644993782043457,
      "learning_rate": 0.0001278263266913567,
      "loss": 1.0897,
      "step": 8271
    },
    {
      "epoch": 1.1629410937719669,
      "grad_norm": 1.442016363143921,
      "learning_rate": 0.0001280495393693335,
      "loss": 0.9262,
      "step": 8272
    },
    {
      "epoch": 1.1630816814283707,
      "grad_norm": 1.602025032043457,
      "learning_rate": 0.00012827260046188193,
      "loss": 1.0356,
      "step": 8273
    },
    {
      "epoch": 1.1632222690847744,
      "grad_norm": 1.5461821556091309,
      "learning_rate": 0.0001284955087635351,
      "loss": 0.9475,
      "step": 8274
    },
    {
      "epoch": 1.1633628567411782,
      "grad_norm": 1.348976969718933,
      "learning_rate": 0.00012871826306965078,
      "loss": 1.0109,
      "step": 8275
    },
    {
      "epoch": 1.1635034443975818,
      "grad_norm": 1.7918790578842163,
      "learning_rate": 0.00012894086217641928,
      "loss": 1.1363,
      "step": 8276
    },
    {
      "epoch": 1.1636440320539856,
      "grad_norm": 2.4025912284851074,
      "learning_rate": 0.00012916330488087004,
      "loss": 1.0529,
      "step": 8277
    },
    {
      "epoch": 1.1637846197103894,
      "grad_norm": 1.4294391870498657,
      "learning_rate": 0.0001293855899808766,
      "loss": 1.1381,
      "step": 8278
    },
    {
      "epoch": 1.1639252073667932,
      "grad_norm": 1.3230043649673462,
      "learning_rate": 0.00012960771627516503,
      "loss": 1.0538,
      "step": 8279
    },
    {
      "epoch": 1.164065795023197,
      "grad_norm": 1.7201015949249268,
      "learning_rate": 0.00012982968256332022,
      "loss": 1.0905,
      "step": 8280
    },
    {
      "epoch": 1.1642063826796007,
      "grad_norm": 1.4257702827453613,
      "learning_rate": 0.00013005148764579072,
      "loss": 1.0982,
      "step": 8281
    },
    {
      "epoch": 1.1643469703360045,
      "grad_norm": 1.638521432876587,
      "learning_rate": 0.00013027313032389675,
      "loss": 1.1678,
      "step": 8282
    },
    {
      "epoch": 1.1644875579924083,
      "grad_norm": 1.6766321659088135,
      "learning_rate": 0.00013049460939983603,
      "loss": 1.0246,
      "step": 8283
    },
    {
      "epoch": 1.1646281456488121,
      "grad_norm": 1.5094326734542847,
      "learning_rate": 0.00013071592367669015,
      "loss": 1.024,
      "step": 8284
    },
    {
      "epoch": 1.1647687333052157,
      "grad_norm": 1.4342548847198486,
      "learning_rate": 0.00013093707195843267,
      "loss": 1.2201,
      "step": 8285
    },
    {
      "epoch": 1.1649093209616195,
      "grad_norm": 1.4284077882766724,
      "learning_rate": 0.00013115805304993224,
      "loss": 1.076,
      "step": 8286
    },
    {
      "epoch": 1.1650499086180233,
      "grad_norm": 1.46504545211792,
      "learning_rate": 0.00013137886575696224,
      "loss": 1.1847,
      "step": 8287
    },
    {
      "epoch": 1.165190496274427,
      "grad_norm": 1.3904975652694702,
      "learning_rate": 0.00013159950888620592,
      "loss": 1.2216,
      "step": 8288
    },
    {
      "epoch": 1.1653310839308308,
      "grad_norm": 1.7247267961502075,
      "learning_rate": 0.0001318199812452627,
      "loss": 1.1643,
      "step": 8289
    },
    {
      "epoch": 1.1654716715872346,
      "grad_norm": 1.7115734815597534,
      "learning_rate": 0.00013204028164265477,
      "loss": 1.1258,
      "step": 8290
    },
    {
      "epoch": 1.1656122592436384,
      "grad_norm": 1.431399941444397,
      "learning_rate": 0.00013226040888783452,
      "loss": 1.0922,
      "step": 8291
    },
    {
      "epoch": 1.1657528469000422,
      "grad_norm": 1.5962766408920288,
      "learning_rate": 0.00013248036179118946,
      "loss": 0.9526,
      "step": 8292
    },
    {
      "epoch": 1.165893434556446,
      "grad_norm": 1.597969889640808,
      "learning_rate": 0.00013270013916404858,
      "loss": 0.9873,
      "step": 8293
    },
    {
      "epoch": 1.1660340222128498,
      "grad_norm": 1.5542529821395874,
      "learning_rate": 0.0001329197398186905,
      "loss": 0.9597,
      "step": 8294
    },
    {
      "epoch": 1.1661746098692536,
      "grad_norm": 1.407233715057373,
      "learning_rate": 0.0001331391625683482,
      "loss": 0.995,
      "step": 8295
    },
    {
      "epoch": 1.1663151975256572,
      "grad_norm": 1.455958366394043,
      "learning_rate": 0.0001333584062272171,
      "loss": 1.0459,
      "step": 8296
    },
    {
      "epoch": 1.166455785182061,
      "grad_norm": 1.6032710075378418,
      "learning_rate": 0.0001335774696104594,
      "loss": 0.9528,
      "step": 8297
    },
    {
      "epoch": 1.1665963728384647,
      "grad_norm": 1.5809532403945923,
      "learning_rate": 0.00013379635153421194,
      "loss": 1.0375,
      "step": 8298
    },
    {
      "epoch": 1.1667369604948685,
      "grad_norm": 1.9319871664047241,
      "learning_rate": 0.00013401505081559224,
      "loss": 0.8483,
      "step": 8299
    },
    {
      "epoch": 1.1668775481512723,
      "grad_norm": 1.4978668689727783,
      "learning_rate": 0.00013423356627270474,
      "loss": 1.1208,
      "step": 8300
    },
    {
      "epoch": 1.167018135807676,
      "grad_norm": 1.626335859298706,
      "learning_rate": 0.0001344518967246474,
      "loss": 1.0645,
      "step": 8301
    },
    {
      "epoch": 1.16715872346408,
      "grad_norm": 1.51334810256958,
      "learning_rate": 0.00013467004099151794,
      "loss": 1.141,
      "step": 8302
    },
    {
      "epoch": 1.1672993111204837,
      "grad_norm": 1.9116796255111694,
      "learning_rate": 0.00013488799789442032,
      "loss": 0.9847,
      "step": 8303
    },
    {
      "epoch": 1.1674398987768875,
      "grad_norm": 1.5988918542861938,
      "learning_rate": 0.00013510576625547097,
      "loss": 1.0157,
      "step": 8304
    },
    {
      "epoch": 1.167580486433291,
      "grad_norm": 1.5027031898498535,
      "learning_rate": 0.0001353233448978053,
      "loss": 1.1074,
      "step": 8305
    },
    {
      "epoch": 1.1677210740896948,
      "grad_norm": 1.5349711179733276,
      "learning_rate": 0.00013554073264558363,
      "loss": 1.0955,
      "step": 8306
    },
    {
      "epoch": 1.1678616617460986,
      "grad_norm": 1.391595721244812,
      "learning_rate": 0.00013575792832399894,
      "loss": 1.1328,
      "step": 8307
    },
    {
      "epoch": 1.1680022494025024,
      "grad_norm": 1.4789385795593262,
      "learning_rate": 0.00013597493075928152,
      "loss": 1.165,
      "step": 8308
    },
    {
      "epoch": 1.1681428370589062,
      "grad_norm": 2.0335652828216553,
      "learning_rate": 0.00013619173877870528,
      "loss": 0.9861,
      "step": 8309
    },
    {
      "epoch": 1.16828342471531,
      "grad_norm": 1.4094469547271729,
      "learning_rate": 0.00013640835121059587,
      "loss": 1.1281,
      "step": 8310
    },
    {
      "epoch": 1.1684240123717138,
      "grad_norm": 1.6415045261383057,
      "learning_rate": 0.0001366247668843352,
      "loss": 0.9076,
      "step": 8311
    },
    {
      "epoch": 1.1685646000281176,
      "grad_norm": 1.374489426612854,
      "learning_rate": 0.00013684098463036953,
      "loss": 1.192,
      "step": 8312
    },
    {
      "epoch": 1.1687051876845214,
      "grad_norm": 1.7715016603469849,
      "learning_rate": 0.00013705700328021375,
      "loss": 0.9203,
      "step": 8313
    },
    {
      "epoch": 1.1688457753409252,
      "grad_norm": 1.5460933446884155,
      "learning_rate": 0.00013727282166645902,
      "loss": 1.1396,
      "step": 8314
    },
    {
      "epoch": 1.168986362997329,
      "grad_norm": 1.5108619928359985,
      "learning_rate": 0.0001374884386227792,
      "loss": 1.0374,
      "step": 8315
    },
    {
      "epoch": 1.1691269506537325,
      "grad_norm": 1.617040991783142,
      "learning_rate": 0.00013770385298393523,
      "loss": 1.074,
      "step": 8316
    },
    {
      "epoch": 1.1692675383101363,
      "grad_norm": 2.084979772567749,
      "learning_rate": 0.00013791906358578504,
      "loss": 1.0419,
      "step": 8317
    },
    {
      "epoch": 1.16940812596654,
      "grad_norm": 1.8501118421554565,
      "learning_rate": 0.0001381340692652863,
      "loss": 1.1081,
      "step": 8318
    },
    {
      "epoch": 1.1695487136229439,
      "grad_norm": 1.6111387014389038,
      "learning_rate": 0.00013834886886050456,
      "loss": 1.1061,
      "step": 8319
    },
    {
      "epoch": 1.1696893012793477,
      "grad_norm": 1.6907649040222168,
      "learning_rate": 0.00013856346121061895,
      "loss": 1.0147,
      "step": 8320
    },
    {
      "epoch": 1.1698298889357515,
      "grad_norm": 1.7208579778671265,
      "learning_rate": 0.00013877784515592872,
      "loss": 1.0586,
      "step": 8321
    },
    {
      "epoch": 1.1699704765921553,
      "grad_norm": 2.0716495513916016,
      "learning_rate": 0.00013899201953785902,
      "loss": 1.102,
      "step": 8322
    },
    {
      "epoch": 1.170111064248559,
      "grad_norm": 1.7437448501586914,
      "learning_rate": 0.00013920598319896877,
      "loss": 0.9189,
      "step": 8323
    },
    {
      "epoch": 1.1702516519049628,
      "grad_norm": 1.5555459260940552,
      "learning_rate": 0.00013941973498295376,
      "loss": 1.0001,
      "step": 8324
    },
    {
      "epoch": 1.1703922395613664,
      "grad_norm": 1.5828157663345337,
      "learning_rate": 0.00013963327373465613,
      "loss": 0.9662,
      "step": 8325
    },
    {
      "epoch": 1.1705328272177702,
      "grad_norm": 1.4549399614334106,
      "learning_rate": 0.00013984659830006891,
      "loss": 1.158,
      "step": 8326
    },
    {
      "epoch": 1.170673414874174,
      "grad_norm": 1.5077195167541504,
      "learning_rate": 0.00014005970752634239,
      "loss": 1.1466,
      "step": 8327
    },
    {
      "epoch": 1.1708140025305778,
      "grad_norm": 1.3636356592178345,
      "learning_rate": 0.00014027260026179149,
      "loss": 1.064,
      "step": 8328
    },
    {
      "epoch": 1.1709545901869816,
      "grad_norm": 1.8559601306915283,
      "learning_rate": 0.00014048527535590014,
      "loss": 1.0405,
      "step": 8329
    },
    {
      "epoch": 1.1710951778433853,
      "grad_norm": 1.9587866067886353,
      "learning_rate": 0.0001406977316593288,
      "loss": 1.0938,
      "step": 8330
    },
    {
      "epoch": 1.1712357654997891,
      "grad_norm": 1.584280252456665,
      "learning_rate": 0.00014090996802392067,
      "loss": 0.957,
      "step": 8331
    },
    {
      "epoch": 1.171376353156193,
      "grad_norm": 1.4761970043182373,
      "learning_rate": 0.00014112198330270642,
      "loss": 1.0926,
      "step": 8332
    },
    {
      "epoch": 1.1715169408125967,
      "grad_norm": 1.5448576211929321,
      "learning_rate": 0.0001413337763499124,
      "loss": 1.1582,
      "step": 8333
    },
    {
      "epoch": 1.1716575284690005,
      "grad_norm": 1.6564656496047974,
      "learning_rate": 0.00014154534602096633,
      "loss": 1.0382,
      "step": 8334
    },
    {
      "epoch": 1.1717981161254043,
      "grad_norm": 2.040815830230713,
      "learning_rate": 0.00014175669117250236,
      "loss": 1.0752,
      "step": 8335
    },
    {
      "epoch": 1.1719387037818079,
      "grad_norm": 1.3698742389678955,
      "learning_rate": 0.00014196781066236818,
      "loss": 1.0877,
      "step": 8336
    },
    {
      "epoch": 1.1720792914382117,
      "grad_norm": 1.589530348777771,
      "learning_rate": 0.00014217870334963103,
      "loss": 0.8981,
      "step": 8337
    },
    {
      "epoch": 1.1722198790946154,
      "grad_norm": 1.6192306280136108,
      "learning_rate": 0.00014238936809458382,
      "loss": 1.0102,
      "step": 8338
    },
    {
      "epoch": 1.1723604667510192,
      "grad_norm": 1.5522438287734985,
      "learning_rate": 0.0001425998037587514,
      "loss": 1.2047,
      "step": 8339
    },
    {
      "epoch": 1.172501054407423,
      "grad_norm": 1.4679285287857056,
      "learning_rate": 0.00014281000920489647,
      "loss": 1.0277,
      "step": 8340
    },
    {
      "epoch": 1.1726416420638268,
      "grad_norm": 2.198521614074707,
      "learning_rate": 0.00014301998329702598,
      "loss": 0.9977,
      "step": 8341
    },
    {
      "epoch": 1.1727822297202306,
      "grad_norm": 1.7914639711380005,
      "learning_rate": 0.00014322972490039718,
      "loss": 1.0123,
      "step": 8342
    },
    {
      "epoch": 1.1729228173766344,
      "grad_norm": 1.6101564168930054,
      "learning_rate": 0.0001434392328815233,
      "loss": 1.0821,
      "step": 8343
    },
    {
      "epoch": 1.1730634050330382,
      "grad_norm": 1.6712136268615723,
      "learning_rate": 0.00014364850610818118,
      "loss": 1.0562,
      "step": 8344
    },
    {
      "epoch": 1.1732039926894418,
      "grad_norm": 1.354177713394165,
      "learning_rate": 0.00014385754344941538,
      "loss": 0.9024,
      "step": 8345
    },
    {
      "epoch": 1.1733445803458455,
      "grad_norm": 1.6764134168624878,
      "learning_rate": 0.00014406634377554575,
      "loss": 0.9037,
      "step": 8346
    },
    {
      "epoch": 1.1734851680022493,
      "grad_norm": 1.454962134361267,
      "learning_rate": 0.00014427490595817215,
      "loss": 1.3634,
      "step": 8347
    },
    {
      "epoch": 1.1736257556586531,
      "grad_norm": 1.5992324352264404,
      "learning_rate": 0.00014448322887018237,
      "loss": 0.9529,
      "step": 8348
    },
    {
      "epoch": 1.173766343315057,
      "grad_norm": 1.5574374198913574,
      "learning_rate": 0.00014469131138575658,
      "loss": 1.1071,
      "step": 8349
    },
    {
      "epoch": 1.1739069309714607,
      "grad_norm": 1.429556131362915,
      "learning_rate": 0.00014489915238037512,
      "loss": 0.9763,
      "step": 8350
    },
    {
      "epoch": 1.1740475186278645,
      "grad_norm": 1.5031393766403198,
      "learning_rate": 0.00014510675073082258,
      "loss": 1.1043,
      "step": 8351
    },
    {
      "epoch": 1.1741881062842683,
      "grad_norm": 1.4614472389221191,
      "learning_rate": 0.00014531410531519513,
      "loss": 1.0847,
      "step": 8352
    },
    {
      "epoch": 1.174328693940672,
      "grad_norm": 1.8174856901168823,
      "learning_rate": 0.00014552121501290664,
      "loss": 0.8808,
      "step": 8353
    },
    {
      "epoch": 1.1744692815970759,
      "grad_norm": 1.6251832246780396,
      "learning_rate": 0.0001457280787046932,
      "loss": 1.0001,
      "step": 8354
    },
    {
      "epoch": 1.1746098692534797,
      "grad_norm": 1.3960424661636353,
      "learning_rate": 0.00014593469527262215,
      "loss": 1.1358,
      "step": 8355
    },
    {
      "epoch": 1.1747504569098832,
      "grad_norm": 1.7696563005447388,
      "learning_rate": 0.00014614106360009516,
      "loss": 1.1583,
      "step": 8356
    },
    {
      "epoch": 1.174891044566287,
      "grad_norm": 1.4395935535430908,
      "learning_rate": 0.00014634718257185568,
      "loss": 1.19,
      "step": 8357
    },
    {
      "epoch": 1.1750316322226908,
      "grad_norm": 1.432121992111206,
      "learning_rate": 0.00014655305107399484,
      "loss": 1.0812,
      "step": 8358
    },
    {
      "epoch": 1.1751722198790946,
      "grad_norm": 1.4833582639694214,
      "learning_rate": 0.0001467586679939569,
      "loss": 1.1484,
      "step": 8359
    },
    {
      "epoch": 1.1753128075354984,
      "grad_norm": 1.5628278255462646,
      "learning_rate": 0.0001469640322205468,
      "loss": 1.0355,
      "step": 8360
    },
    {
      "epoch": 1.1754533951919022,
      "grad_norm": 1.4836928844451904,
      "learning_rate": 0.0001471691426439344,
      "loss": 1.1378,
      "step": 8361
    },
    {
      "epoch": 1.175593982848306,
      "grad_norm": 1.5305787324905396,
      "learning_rate": 0.00014737399815566052,
      "loss": 1.2279,
      "step": 8362
    },
    {
      "epoch": 1.1757345705047098,
      "grad_norm": 1.3910503387451172,
      "learning_rate": 0.0001475785976486445,
      "loss": 1.2028,
      "step": 8363
    },
    {
      "epoch": 1.1758751581611135,
      "grad_norm": 1.4397517442703247,
      "learning_rate": 0.00014778294001718906,
      "loss": 1.2317,
      "step": 8364
    },
    {
      "epoch": 1.176015745817517,
      "grad_norm": 1.476773977279663,
      "learning_rate": 0.000147987024156986,
      "loss": 1.1895,
      "step": 8365
    },
    {
      "epoch": 1.176156333473921,
      "grad_norm": 1.7281008958816528,
      "learning_rate": 0.0001481908489651237,
      "loss": 1.1103,
      "step": 8366
    },
    {
      "epoch": 1.1762969211303247,
      "grad_norm": 1.4122159481048584,
      "learning_rate": 0.000148394413340091,
      "loss": 1.0065,
      "step": 8367
    },
    {
      "epoch": 1.1764375087867285,
      "grad_norm": 1.5568293333053589,
      "learning_rate": 0.0001485977161817846,
      "loss": 1.189,
      "step": 8368
    },
    {
      "epoch": 1.1765780964431323,
      "grad_norm": 1.4819916486740112,
      "learning_rate": 0.00014880075639151473,
      "loss": 1.2021,
      "step": 8369
    },
    {
      "epoch": 1.176718684099536,
      "grad_norm": 1.3836122751235962,
      "learning_rate": 0.00014900353287200982,
      "loss": 1.0132,
      "step": 8370
    },
    {
      "epoch": 1.1768592717559399,
      "grad_norm": 1.4198426008224487,
      "learning_rate": 0.00014920604452742537,
      "loss": 0.8925,
      "step": 8371
    },
    {
      "epoch": 1.1769998594123436,
      "grad_norm": 1.436508297920227,
      "learning_rate": 0.00014940829026334682,
      "loss": 1.2,
      "step": 8372
    },
    {
      "epoch": 1.1771404470687474,
      "grad_norm": 1.4929662942886353,
      "learning_rate": 0.00014961026898679695,
      "loss": 1.0764,
      "step": 8373
    },
    {
      "epoch": 1.1772810347251512,
      "grad_norm": 1.5135608911514282,
      "learning_rate": 0.0001498119796062416,
      "loss": 0.8964,
      "step": 8374
    },
    {
      "epoch": 1.177421622381555,
      "grad_norm": 1.6002633571624756,
      "learning_rate": 0.00015001342103159547,
      "loss": 1.3302,
      "step": 8375
    },
    {
      "epoch": 1.1775622100379586,
      "grad_norm": 1.6846985816955566,
      "learning_rate": 0.000150214592174228,
      "loss": 0.9935,
      "step": 8376
    },
    {
      "epoch": 1.1777027976943624,
      "grad_norm": 1.5394904613494873,
      "learning_rate": 0.0001504154919469694,
      "loss": 1.1956,
      "step": 8377
    },
    {
      "epoch": 1.1778433853507662,
      "grad_norm": 1.5030351877212524,
      "learning_rate": 0.00015061611926411628,
      "loss": 1.0903,
      "step": 8378
    },
    {
      "epoch": 1.17798397300717,
      "grad_norm": 1.7044340372085571,
      "learning_rate": 0.00015081647304143777,
      "loss": 0.9915,
      "step": 8379
    },
    {
      "epoch": 1.1781245606635737,
      "grad_norm": 1.440836787223816,
      "learning_rate": 0.00015101655219618122,
      "loss": 1.0027,
      "step": 8380
    },
    {
      "epoch": 1.1782651483199775,
      "grad_norm": 1.579360842704773,
      "learning_rate": 0.00015121635564707776,
      "loss": 0.9954,
      "step": 8381
    },
    {
      "epoch": 1.1784057359763813,
      "grad_norm": 1.4248180389404297,
      "learning_rate": 0.00015141588231434944,
      "loss": 0.9186,
      "step": 8382
    },
    {
      "epoch": 1.178546323632785,
      "grad_norm": 1.7280024290084839,
      "learning_rate": 0.0001516151311197132,
      "loss": 1.0242,
      "step": 8383
    },
    {
      "epoch": 1.178686911289189,
      "grad_norm": 1.3336045742034912,
      "learning_rate": 0.00015181410098638812,
      "loss": 1.0267,
      "step": 8384
    },
    {
      "epoch": 1.1788274989455925,
      "grad_norm": 1.6284260749816895,
      "learning_rate": 0.00015201279083909997,
      "loss": 1.1075,
      "step": 8385
    },
    {
      "epoch": 1.1789680866019963,
      "grad_norm": 1.467546820640564,
      "learning_rate": 0.00015221119960408822,
      "loss": 1.196,
      "step": 8386
    },
    {
      "epoch": 1.1791086742584,
      "grad_norm": 1.4006928205490112,
      "learning_rate": 0.0001524093262091121,
      "loss": 1.0759,
      "step": 8387
    },
    {
      "epoch": 1.1792492619148038,
      "grad_norm": 1.6223708391189575,
      "learning_rate": 0.00015260716958345473,
      "loss": 1.0664,
      "step": 8388
    },
    {
      "epoch": 1.1793898495712076,
      "grad_norm": 1.4378974437713623,
      "learning_rate": 0.00015280472865793023,
      "loss": 1.0567,
      "step": 8389
    },
    {
      "epoch": 1.1795304372276114,
      "grad_norm": 1.576569676399231,
      "learning_rate": 0.00015300200236488903,
      "loss": 1.15,
      "step": 8390
    },
    {
      "epoch": 1.1796710248840152,
      "grad_norm": 1.4117883443832397,
      "learning_rate": 0.00015319898963822416,
      "loss": 1.0996,
      "step": 8391
    },
    {
      "epoch": 1.179811612540419,
      "grad_norm": 1.6206910610198975,
      "learning_rate": 0.00015339568941337534,
      "loss": 1.0436,
      "step": 8392
    },
    {
      "epoch": 1.1799522001968228,
      "grad_norm": 1.8716422319412231,
      "learning_rate": 0.00015359210062733776,
      "loss": 1.1078,
      "step": 8393
    },
    {
      "epoch": 1.1800927878532266,
      "grad_norm": 1.4632296562194824,
      "learning_rate": 0.00015378822221866494,
      "loss": 1.1674,
      "step": 8394
    },
    {
      "epoch": 1.1802333755096304,
      "grad_norm": 1.7004334926605225,
      "learning_rate": 0.00015398405312747575,
      "loss": 1.142,
      "step": 8395
    },
    {
      "epoch": 1.180373963166034,
      "grad_norm": 1.4906501770019531,
      "learning_rate": 0.00015417959229546008,
      "loss": 1.137,
      "step": 8396
    },
    {
      "epoch": 1.1805145508224377,
      "grad_norm": 1.6326786279678345,
      "learning_rate": 0.00015437483866588415,
      "loss": 1.0327,
      "step": 8397
    },
    {
      "epoch": 1.1806551384788415,
      "grad_norm": 1.7381258010864258,
      "learning_rate": 0.0001545697911835973,
      "loss": 0.952,
      "step": 8398
    },
    {
      "epoch": 1.1807957261352453,
      "grad_norm": 1.5859946012496948,
      "learning_rate": 0.00015476444879503648,
      "loss": 1.0786,
      "step": 8399
    },
    {
      "epoch": 1.180936313791649,
      "grad_norm": 1.4847426414489746,
      "learning_rate": 0.0001549588104482316,
      "loss": 1.0852,
      "step": 8400
    },
    {
      "epoch": 1.1810769014480529,
      "grad_norm": 1.5483710765838623,
      "learning_rate": 0.000155152875092813,
      "loss": 1.0103,
      "step": 8401
    },
    {
      "epoch": 1.1812174891044567,
      "grad_norm": 1.5310429334640503,
      "learning_rate": 0.0001553466416800158,
      "loss": 1.1905,
      "step": 8402
    },
    {
      "epoch": 1.1813580767608605,
      "grad_norm": 1.7573410272598267,
      "learning_rate": 0.00015554010916268553,
      "loss": 1.0986,
      "step": 8403
    },
    {
      "epoch": 1.1814986644172643,
      "grad_norm": 1.5219781398773193,
      "learning_rate": 0.00015573327649528516,
      "loss": 1.24,
      "step": 8404
    },
    {
      "epoch": 1.1816392520736678,
      "grad_norm": 2.288516044616699,
      "learning_rate": 0.0001559261426338986,
      "loss": 1.0648,
      "step": 8405
    },
    {
      "epoch": 1.1817798397300716,
      "grad_norm": 1.5944932699203491,
      "learning_rate": 0.00015611870653623814,
      "loss": 1.1033,
      "step": 8406
    },
    {
      "epoch": 1.1819204273864754,
      "grad_norm": 1.5770611763000488,
      "learning_rate": 0.0001563109671616493,
      "loss": 1.2135,
      "step": 8407
    },
    {
      "epoch": 1.1820610150428792,
      "grad_norm": 1.5121594667434692,
      "learning_rate": 0.00015650292347111567,
      "loss": 1.1221,
      "step": 8408
    },
    {
      "epoch": 1.182201602699283,
      "grad_norm": 1.6084473133087158,
      "learning_rate": 0.00015669457442726715,
      "loss": 0.9033,
      "step": 8409
    },
    {
      "epoch": 1.1823421903556868,
      "grad_norm": 1.7716455459594727,
      "learning_rate": 0.00015688591899438255,
      "loss": 0.9707,
      "step": 8410
    },
    {
      "epoch": 1.1824827780120906,
      "grad_norm": 1.565464735031128,
      "learning_rate": 0.00015707695613839686,
      "loss": 1.195,
      "step": 8411
    },
    {
      "epoch": 1.1826233656684944,
      "grad_norm": 1.3179987668991089,
      "learning_rate": 0.0001572676848269064,
      "loss": 1.0538,
      "step": 8412
    },
    {
      "epoch": 1.1827639533248981,
      "grad_norm": 1.5615147352218628,
      "learning_rate": 0.0001574581040291742,
      "loss": 0.9928,
      "step": 8413
    },
    {
      "epoch": 1.182904540981302,
      "grad_norm": 1.824580192565918,
      "learning_rate": 0.00015764821271613687,
      "loss": 1.1024,
      "step": 8414
    },
    {
      "epoch": 1.1830451286377057,
      "grad_norm": 1.5561974048614502,
      "learning_rate": 0.00015783800986040756,
      "loss": 1.0545,
      "step": 8415
    },
    {
      "epoch": 1.1831857162941093,
      "grad_norm": 1.9472181797027588,
      "learning_rate": 0.00015802749443628403,
      "loss": 1.1868,
      "step": 8416
    },
    {
      "epoch": 1.183326303950513,
      "grad_norm": 1.8832308053970337,
      "learning_rate": 0.00015821666541975305,
      "loss": 1.0525,
      "step": 8417
    },
    {
      "epoch": 1.1834668916069169,
      "grad_norm": 1.5823482275009155,
      "learning_rate": 0.000158405521788496,
      "loss": 1.1241,
      "step": 8418
    },
    {
      "epoch": 1.1836074792633207,
      "grad_norm": 1.5126303434371948,
      "learning_rate": 0.0001585940625218944,
      "loss": 1.3014,
      "step": 8419
    },
    {
      "epoch": 1.1837480669197245,
      "grad_norm": 1.6065551042556763,
      "learning_rate": 0.00015878228660103617,
      "loss": 1.1262,
      "step": 8420
    },
    {
      "epoch": 1.1838886545761282,
      "grad_norm": 1.7704180479049683,
      "learning_rate": 0.00015897019300871977,
      "loss": 1.0385,
      "step": 8421
    },
    {
      "epoch": 1.184029242232532,
      "grad_norm": 1.7671246528625488,
      "learning_rate": 0.00015915778072946095,
      "loss": 1.1052,
      "step": 8422
    },
    {
      "epoch": 1.1841698298889358,
      "grad_norm": 1.6955941915512085,
      "learning_rate": 0.00015934504874949684,
      "loss": 1.124,
      "step": 8423
    },
    {
      "epoch": 1.1843104175453396,
      "grad_norm": 1.6737942695617676,
      "learning_rate": 0.00015953199605679296,
      "loss": 1.1428,
      "step": 8424
    },
    {
      "epoch": 1.1844510052017432,
      "grad_norm": 1.715571403503418,
      "learning_rate": 0.00015971862164104844,
      "loss": 0.9816,
      "step": 8425
    },
    {
      "epoch": 1.184591592858147,
      "grad_norm": 1.4770100116729736,
      "learning_rate": 0.00015990492449370036,
      "loss": 1.1131,
      "step": 8426
    },
    {
      "epoch": 1.1847321805145508,
      "grad_norm": 1.9986255168914795,
      "learning_rate": 0.00016009090360793012,
      "loss": 1.0665,
      "step": 8427
    },
    {
      "epoch": 1.1848727681709545,
      "grad_norm": 1.936782956123352,
      "learning_rate": 0.00016027655797866866,
      "loss": 1.2477,
      "step": 8428
    },
    {
      "epoch": 1.1850133558273583,
      "grad_norm": 1.5388565063476562,
      "learning_rate": 0.00016046188660260192,
      "loss": 1.1314,
      "step": 8429
    },
    {
      "epoch": 1.1851539434837621,
      "grad_norm": 1.5186004638671875,
      "learning_rate": 0.0001606468884781763,
      "loss": 1.2101,
      "step": 8430
    },
    {
      "epoch": 1.185294531140166,
      "grad_norm": 1.4991936683654785,
      "learning_rate": 0.00016083156260560385,
      "loss": 1.1307,
      "step": 8431
    },
    {
      "epoch": 1.1854351187965697,
      "grad_norm": 1.5871189832687378,
      "learning_rate": 0.00016101590798686804,
      "loss": 1.0916,
      "step": 8432
    },
    {
      "epoch": 1.1855757064529735,
      "grad_norm": 1.4940035343170166,
      "learning_rate": 0.0001611999236257288,
      "loss": 1.0497,
      "step": 8433
    },
    {
      "epoch": 1.1857162941093773,
      "grad_norm": 1.4818145036697388,
      "learning_rate": 0.00016138360852772815,
      "loss": 1.1464,
      "step": 8434
    },
    {
      "epoch": 1.185856881765781,
      "grad_norm": 1.3824347257614136,
      "learning_rate": 0.0001615669617001951,
      "loss": 1.0946,
      "step": 8435
    },
    {
      "epoch": 1.1859974694221846,
      "grad_norm": 1.5227911472320557,
      "learning_rate": 0.00016174998215225233,
      "loss": 0.7347,
      "step": 8436
    },
    {
      "epoch": 1.1861380570785884,
      "grad_norm": 1.537693738937378,
      "learning_rate": 0.00016193266889482,
      "loss": 0.9916,
      "step": 8437
    },
    {
      "epoch": 1.1862786447349922,
      "grad_norm": 1.5089493989944458,
      "learning_rate": 0.0001621150209406212,
      "loss": 1.2109,
      "step": 8438
    },
    {
      "epoch": 1.186419232391396,
      "grad_norm": 1.6102895736694336,
      "learning_rate": 0.00016229703730418857,
      "loss": 1.0321,
      "step": 8439
    },
    {
      "epoch": 1.1865598200477998,
      "grad_norm": 1.5701249837875366,
      "learning_rate": 0.00016247871700186828,
      "loss": 0.9627,
      "step": 8440
    },
    {
      "epoch": 1.1867004077042036,
      "grad_norm": 1.5293573141098022,
      "learning_rate": 0.00016266005905182676,
      "loss": 1.1019,
      "step": 8441
    },
    {
      "epoch": 1.1868409953606074,
      "grad_norm": 1.4847432374954224,
      "learning_rate": 0.00016284106247405435,
      "loss": 1.0302,
      "step": 8442
    },
    {
      "epoch": 1.1869815830170112,
      "grad_norm": 1.4799339771270752,
      "learning_rate": 0.00016302172629037166,
      "loss": 1.1518,
      "step": 8443
    },
    {
      "epoch": 1.187122170673415,
      "grad_norm": 1.6282657384872437,
      "learning_rate": 0.00016320204952443464,
      "loss": 1.1066,
      "step": 8444
    },
    {
      "epoch": 1.1872627583298185,
      "grad_norm": 1.498926043510437,
      "learning_rate": 0.00016338203120174,
      "loss": 1.057,
      "step": 8445
    },
    {
      "epoch": 1.1874033459862223,
      "grad_norm": 1.6166255474090576,
      "learning_rate": 0.00016356167034962927,
      "loss": 1.0172,
      "step": 8446
    },
    {
      "epoch": 1.1875439336426261,
      "grad_norm": NaN,
      "learning_rate": 0.00016356167034962927,
      "loss": 0.8515,
      "step": 8447
    },
    {
      "epoch": 1.18768452129903,
      "grad_norm": 1.9016199111938477,
      "learning_rate": 0.0001637409659972967,
      "loss": 1.0428,
      "step": 8448
    },
    {
      "epoch": 1.1878251089554337,
      "grad_norm": 1.7227895259857178,
      "learning_rate": 0.00016391991717579175,
      "loss": 1.0667,
      "step": 8449
    },
    {
      "epoch": 1.1879656966118375,
      "grad_norm": 1.557047963142395,
      "learning_rate": 0.00016409852291802586,
      "loss": 1.1429,
      "step": 8450
    },
    {
      "epoch": 1.1881062842682413,
      "grad_norm": 1.4191690683364868,
      "learning_rate": 0.00016427678225877713,
      "loss": 1.0651,
      "step": 8451
    },
    {
      "epoch": 1.188246871924645,
      "grad_norm": 1.6316401958465576,
      "learning_rate": 0.0001644546942346955,
      "loss": 1.1569,
      "step": 8452
    },
    {
      "epoch": 1.1883874595810489,
      "grad_norm": 1.5253605842590332,
      "learning_rate": 0.0001646322578843092,
      "loss": 0.9816,
      "step": 8453
    },
    {
      "epoch": 1.1885280472374526,
      "grad_norm": 1.6068934202194214,
      "learning_rate": 0.00016480947224802728,
      "loss": 1.1795,
      "step": 8454
    },
    {
      "epoch": 1.1886686348938564,
      "grad_norm": 1.8708977699279785,
      "learning_rate": 0.00016498633636814742,
      "loss": 1.2096,
      "step": 8455
    },
    {
      "epoch": 1.18880922255026,
      "grad_norm": 1.5080338716506958,
      "learning_rate": 0.00016516284928885992,
      "loss": 1.0364,
      "step": 8456
    },
    {
      "epoch": 1.1889498102066638,
      "grad_norm": 1.3319355249404907,
      "learning_rate": 0.00016533901005625268,
      "loss": 1.1583,
      "step": 8457
    },
    {
      "epoch": 1.1890903978630676,
      "grad_norm": 1.52525794506073,
      "learning_rate": 0.00016551481771831753,
      "loss": 1.088,
      "step": 8458
    },
    {
      "epoch": 1.1892309855194714,
      "grad_norm": 1.7833722829818726,
      "learning_rate": 0.00016569027132495382,
      "loss": 0.9515,
      "step": 8459
    },
    {
      "epoch": 1.1893715731758752,
      "grad_norm": 1.5026564598083496,
      "learning_rate": 0.00016586536992797444,
      "loss": 1.0946,
      "step": 8460
    },
    {
      "epoch": 1.189512160832279,
      "grad_norm": 1.8017841577529907,
      "learning_rate": 0.00016604011258111113,
      "loss": 0.8153,
      "step": 8461
    },
    {
      "epoch": 1.1896527484886827,
      "grad_norm": 1.6216527223587036,
      "learning_rate": 0.00016621449834001845,
      "loss": 0.9752,
      "step": 8462
    },
    {
      "epoch": 1.1897933361450865,
      "grad_norm": 1.2645825147628784,
      "learning_rate": 0.0001663885262622802,
      "loss": 1.1487,
      "step": 8463
    },
    {
      "epoch": 1.1899339238014903,
      "grad_norm": 1.6875666379928589,
      "learning_rate": 0.00016656219540741457,
      "loss": 1.0532,
      "step": 8464
    },
    {
      "epoch": 1.190074511457894,
      "grad_norm": 1.4815661907196045,
      "learning_rate": 0.00016673550483687784,
      "loss": 1.1668,
      "step": 8465
    },
    {
      "epoch": 1.1902150991142977,
      "grad_norm": 1.8388482332229614,
      "learning_rate": 0.0001669084536140705,
      "loss": 1.1357,
      "step": 8466
    },
    {
      "epoch": 1.1903556867707015,
      "grad_norm": 1.613916277885437,
      "learning_rate": 0.00016708104080434213,
      "loss": 1.1095,
      "step": 8467
    },
    {
      "epoch": 1.1904962744271053,
      "grad_norm": 1.4559911489486694,
      "learning_rate": 0.00016725326547499636,
      "loss": 1.1887,
      "step": 8468
    },
    {
      "epoch": 1.190636862083509,
      "grad_norm": 1.6224210262298584,
      "learning_rate": 0.00016742512669529595,
      "loss": 1.159,
      "step": 8469
    },
    {
      "epoch": 1.1907774497399128,
      "grad_norm": 1.515576720237732,
      "learning_rate": 0.00016759662353646787,
      "loss": 0.9568,
      "step": 8470
    },
    {
      "epoch": 1.1909180373963166,
      "grad_norm": 1.6033971309661865,
      "learning_rate": 0.00016776775507170826,
      "loss": 1.0217,
      "step": 8471
    },
    {
      "epoch": 1.1910586250527204,
      "grad_norm": 1.4631534814834595,
      "learning_rate": 0.00016793852037618736,
      "loss": 1.0046,
      "step": 8472
    },
    {
      "epoch": 1.1911992127091242,
      "grad_norm": 1.507102608680725,
      "learning_rate": 0.0001681089185270544,
      "loss": 1.0803,
      "step": 8473
    },
    {
      "epoch": 1.191339800365528,
      "grad_norm": 1.4317959547042847,
      "learning_rate": 0.0001682789486034436,
      "loss": 1.0359,
      "step": 8474
    },
    {
      "epoch": 1.1914803880219318,
      "grad_norm": 1.8664155006408691,
      "learning_rate": 0.00016844860968647752,
      "loss": 0.8957,
      "step": 8475
    },
    {
      "epoch": 1.1916209756783354,
      "grad_norm": 1.8088529109954834,
      "learning_rate": 0.0001686179008592733,
      "loss": 1.0404,
      "step": 8476
    },
    {
      "epoch": 1.1917615633347391,
      "grad_norm": 1.937978744506836,
      "learning_rate": 0.00016878682120694642,
      "loss": 1.0837,
      "step": 8477
    },
    {
      "epoch": 1.191902150991143,
      "grad_norm": 1.8112590312957764,
      "learning_rate": 0.0001689553698166172,
      "loss": 1.0652,
      "step": 8478
    },
    {
      "epoch": 1.1920427386475467,
      "grad_norm": 1.604241967201233,
      "learning_rate": 0.00016912354577741428,
      "loss": 1.034,
      "step": 8479
    },
    {
      "epoch": 1.1921833263039505,
      "grad_norm": 2.202749013900757,
      "learning_rate": 0.00016929134818048107,
      "loss": 0.9554,
      "step": 8480
    },
    {
      "epoch": 1.1923239139603543,
      "grad_norm": 1.5610984563827515,
      "learning_rate": 0.00016945877611897883,
      "loss": 0.9846,
      "step": 8481
    },
    {
      "epoch": 1.192464501616758,
      "grad_norm": 1.5258128643035889,
      "learning_rate": 0.00016962582868809296,
      "loss": 1.0225,
      "step": 8482
    },
    {
      "epoch": 1.192605089273162,
      "grad_norm": 1.4135370254516602,
      "learning_rate": 0.00016979250498503728,
      "loss": 1.1581,
      "step": 8483
    },
    {
      "epoch": 1.1927456769295657,
      "grad_norm": 1.608154296875,
      "learning_rate": 0.0001699588041090591,
      "loss": 1.0696,
      "step": 8484
    },
    {
      "epoch": 1.1928862645859692,
      "grad_norm": 1.5534173250198364,
      "learning_rate": 0.00017012472516144404,
      "loss": 1.1711,
      "step": 8485
    },
    {
      "epoch": 1.193026852242373,
      "grad_norm": 1.5618259906768799,
      "learning_rate": 0.00017029026724552096,
      "loss": 1.0,
      "step": 8486
    },
    {
      "epoch": 1.1931674398987768,
      "grad_norm": 1.8468737602233887,
      "learning_rate": 0.00017045542946666665,
      "loss": 1.0616,
      "step": 8487
    },
    {
      "epoch": 1.1933080275551806,
      "grad_norm": 1.6241472959518433,
      "learning_rate": 0.00017062021093231083,
      "loss": 1.1886,
      "step": 8488
    },
    {
      "epoch": 1.1934486152115844,
      "grad_norm": 1.6353161334991455,
      "learning_rate": 0.0001707846107519409,
      "loss": 0.9913,
      "step": 8489
    },
    {
      "epoch": 1.1935892028679882,
      "grad_norm": 1.8410512208938599,
      "learning_rate": 0.00017094862803710643,
      "loss": 0.985,
      "step": 8490
    },
    {
      "epoch": 1.193729790524392,
      "grad_norm": 1.5380759239196777,
      "learning_rate": 0.00017111226190142534,
      "loss": 1.176,
      "step": 8491
    },
    {
      "epoch": 1.1938703781807958,
      "grad_norm": 1.5128194093704224,
      "learning_rate": 0.00017127551146058613,
      "loss": 1.2669,
      "step": 8492
    },
    {
      "epoch": 1.1940109658371996,
      "grad_norm": 1.59511137008667,
      "learning_rate": 0.00017143837583235514,
      "loss": 1.0692,
      "step": 8493
    },
    {
      "epoch": 1.1941515534936034,
      "grad_norm": 1.6875478029251099,
      "learning_rate": 0.00017160085413658015,
      "loss": 1.1129,
      "step": 8494
    },
    {
      "epoch": 1.1942921411500071,
      "grad_norm": 2.1280720233917236,
      "learning_rate": 0.000171762945495195,
      "loss": 0.8944,
      "step": 8495
    },
    {
      "epoch": 1.1944327288064107,
      "grad_norm": 1.713546872138977,
      "learning_rate": 0.00017192464903222545,
      "loss": 1.0275,
      "step": 8496
    },
    {
      "epoch": 1.1945733164628145,
      "grad_norm": 1.850006103515625,
      "learning_rate": 0.00017208596387379238,
      "loss": 1.0275,
      "step": 8497
    },
    {
      "epoch": 1.1947139041192183,
      "grad_norm": 1.6804708242416382,
      "learning_rate": 0.00017224688914811755,
      "loss": 1.0304,
      "step": 8498
    },
    {
      "epoch": 1.194854491775622,
      "grad_norm": 1.830929160118103,
      "learning_rate": 0.00017240742398552818,
      "loss": 0.8708,
      "step": 8499
    },
    {
      "epoch": 1.1949950794320259,
      "grad_norm": 1.5622563362121582,
      "learning_rate": 0.00017256756751846055,
      "loss": 1.2023,
      "step": 8500
    },
    {
      "epoch": 1.1949950794320259,
      "eval_loss": 1.1590977907180786,
      "eval_runtime": 772.4861,
      "eval_samples_per_second": 16.371,
      "eval_steps_per_second": 8.185,
      "step": 8500
    },
    {
      "epoch": 1.1951356670884297,
      "grad_norm": 2.2594382762908936,
      "learning_rate": 0.0001727273188814671,
      "loss": 1.1568,
      "step": 8501
    },
    {
      "epoch": 1.1952762547448335,
      "grad_norm": 1.4441442489624023,
      "learning_rate": 0.00017288667721121866,
      "loss": 1.2974,
      "step": 8502
    },
    {
      "epoch": 1.1954168424012372,
      "grad_norm": 1.41779625415802,
      "learning_rate": 0.00017304564164651035,
      "loss": 1.1901,
      "step": 8503
    },
    {
      "epoch": 1.195557430057641,
      "grad_norm": 1.6078667640686035,
      "learning_rate": 0.00017320421132826597,
      "loss": 1.0945,
      "step": 8504
    },
    {
      "epoch": 1.1956980177140446,
      "grad_norm": 1.7308199405670166,
      "learning_rate": 0.0001733623853995426,
      "loss": 1.0107,
      "step": 8505
    },
    {
      "epoch": 1.1958386053704484,
      "grad_norm": 1.514508605003357,
      "learning_rate": 0.0001735201630055351,
      "loss": 1.1961,
      "step": 8506
    },
    {
      "epoch": 1.1959791930268522,
      "grad_norm": 1.639037013053894,
      "learning_rate": 0.00017367754329358187,
      "loss": 1.0773,
      "step": 8507
    },
    {
      "epoch": 1.196119780683256,
      "grad_norm": 1.6729274988174438,
      "learning_rate": 0.000173834525413167,
      "loss": 0.988,
      "step": 8508
    },
    {
      "epoch": 1.1962603683396598,
      "grad_norm": 1.6018080711364746,
      "learning_rate": 0.00017399110851592736,
      "loss": 0.9535,
      "step": 8509
    },
    {
      "epoch": 1.1964009559960636,
      "grad_norm": 1.632043480873108,
      "learning_rate": 0.000174147291755656,
      "loss": 0.9504,
      "step": 8510
    },
    {
      "epoch": 1.1965415436524673,
      "grad_norm": 1.595725655555725,
      "learning_rate": 0.00017430307428830655,
      "loss": 1.0804,
      "step": 8511
    },
    {
      "epoch": 1.1966821313088711,
      "grad_norm": 2.0992701053619385,
      "learning_rate": 0.00017445845527199895,
      "loss": 1.0574,
      "step": 8512
    },
    {
      "epoch": 1.196822718965275,
      "grad_norm": 1.545019507408142,
      "learning_rate": 0.00017461343386702247,
      "loss": 1.3108,
      "step": 8513
    },
    {
      "epoch": 1.1969633066216787,
      "grad_norm": 1.6147031784057617,
      "learning_rate": 0.00017476800923584143,
      "loss": 1.0032,
      "step": 8514
    },
    {
      "epoch": 1.1971038942780825,
      "grad_norm": 1.4844285249710083,
      "learning_rate": 0.00017492218054309855,
      "loss": 1.0783,
      "step": 8515
    },
    {
      "epoch": 1.197244481934486,
      "grad_norm": 1.6717114448547363,
      "learning_rate": 0.00017507594695562065,
      "loss": 1.1576,
      "step": 8516
    },
    {
      "epoch": 1.1973850695908899,
      "grad_norm": 1.5508973598480225,
      "learning_rate": 0.00017522930764242317,
      "loss": 1.2042,
      "step": 8517
    },
    {
      "epoch": 1.1975256572472937,
      "grad_norm": 1.7134740352630615,
      "learning_rate": 0.00017538226177471347,
      "loss": 1.0239,
      "step": 8518
    },
    {
      "epoch": 1.1976662449036974,
      "grad_norm": 1.380350112915039,
      "learning_rate": 0.00017553480852589628,
      "loss": 1.324,
      "step": 8519
    },
    {
      "epoch": 1.1978068325601012,
      "grad_norm": 1.8066476583480835,
      "learning_rate": 0.0001756869470715778,
      "loss": 1.0474,
      "step": 8520
    },
    {
      "epoch": 1.197947420216505,
      "grad_norm": 1.463763952255249,
      "learning_rate": 0.00017583867658957033,
      "loss": 0.9727,
      "step": 8521
    },
    {
      "epoch": 1.1980880078729088,
      "grad_norm": 1.5638198852539062,
      "learning_rate": 0.00017598999625989667,
      "loss": 1.0123,
      "step": 8522
    },
    {
      "epoch": 1.1982285955293126,
      "grad_norm": 1.541914463043213,
      "learning_rate": 0.00017614090526479445,
      "loss": 1.141,
      "step": 8523
    },
    {
      "epoch": 1.1983691831857164,
      "grad_norm": 1.5873366594314575,
      "learning_rate": 0.00017629140278872067,
      "loss": 1.008,
      "step": 8524
    },
    {
      "epoch": 1.19850977084212,
      "grad_norm": 1.856265664100647,
      "learning_rate": 0.00017644148801835598,
      "loss": 0.9397,
      "step": 8525
    },
    {
      "epoch": 1.1986503584985237,
      "grad_norm": 1.61131751537323,
      "learning_rate": 0.00017659116014260927,
      "loss": 0.8595,
      "step": 8526
    },
    {
      "epoch": 1.1987909461549275,
      "grad_norm": 1.6980620622634888,
      "learning_rate": 0.00017674041835262163,
      "loss": 0.9464,
      "step": 8527
    },
    {
      "epoch": 1.1989315338113313,
      "grad_norm": 1.26870858669281,
      "learning_rate": 0.00017688926184177171,
      "loss": 1.088,
      "step": 8528
    },
    {
      "epoch": 1.1990721214677351,
      "grad_norm": 1.7628540992736816,
      "learning_rate": 0.00017703768980567897,
      "loss": 1.0105,
      "step": 8529
    },
    {
      "epoch": 1.199212709124139,
      "grad_norm": 1.5770834684371948,
      "learning_rate": 0.00017718570144220795,
      "loss": 1.0464,
      "step": 8530
    },
    {
      "epoch": 1.1993532967805427,
      "grad_norm": 1.5654871463775635,
      "learning_rate": 0.0001773332959514739,
      "loss": 0.9657,
      "step": 8531
    },
    {
      "epoch": 1.1994938844369465,
      "grad_norm": 2.1690986156463623,
      "learning_rate": 0.00017748047253584592,
      "loss": 1.0459,
      "step": 8532
    },
    {
      "epoch": 1.1996344720933503,
      "grad_norm": 1.977569818496704,
      "learning_rate": 0.00017762723039995158,
      "loss": 1.0533,
      "step": 8533
    },
    {
      "epoch": 1.1997750597497538,
      "grad_norm": 1.9925044775009155,
      "learning_rate": 0.0001777735687506819,
      "loss": 1.1447,
      "step": 8534
    },
    {
      "epoch": 1.1999156474061579,
      "grad_norm": 1.6049046516418457,
      "learning_rate": 0.0001779194867971944,
      "loss": 1.0733,
      "step": 8535
    },
    {
      "epoch": 1.2000562350625614,
      "grad_norm": 1.8560612201690674,
      "learning_rate": 0.00017806498375091818,
      "loss": 1.0973,
      "step": 8536
    },
    {
      "epoch": 1.2001968227189652,
      "grad_norm": 1.726683497428894,
      "learning_rate": 0.0001782100588255583,
      "loss": 1.2877,
      "step": 8537
    },
    {
      "epoch": 1.200337410375369,
      "grad_norm": 1.6090043783187866,
      "learning_rate": 0.00017835471123709882,
      "loss": 1.0413,
      "step": 8538
    },
    {
      "epoch": 1.2004779980317728,
      "grad_norm": 1.5517290830612183,
      "learning_rate": 0.00017849894020380923,
      "loss": 1.2039,
      "step": 8539
    },
    {
      "epoch": 1.2006185856881766,
      "grad_norm": 1.687759280204773,
      "learning_rate": 0.00017864274494624666,
      "loss": 1.1053,
      "step": 8540
    },
    {
      "epoch": 1.2007591733445804,
      "grad_norm": 1.6817257404327393,
      "learning_rate": 0.00017878612468726096,
      "loss": 1.1928,
      "step": 8541
    },
    {
      "epoch": 1.2008997610009842,
      "grad_norm": 1.5893776416778564,
      "learning_rate": 0.0001789290786519987,
      "loss": 1.2846,
      "step": 8542
    },
    {
      "epoch": 1.201040348657388,
      "grad_norm": 1.7076040506362915,
      "learning_rate": 0.00017907160606790742,
      "loss": 1.0075,
      "step": 8543
    },
    {
      "epoch": 1.2011809363137917,
      "grad_norm": 1.3149642944335938,
      "learning_rate": 0.00017921370616474022,
      "loss": 1.0951,
      "step": 8544
    },
    {
      "epoch": 1.2013215239701953,
      "grad_norm": 1.7127012014389038,
      "learning_rate": 0.0001793553781745593,
      "loss": 1.1702,
      "step": 8545
    },
    {
      "epoch": 1.201462111626599,
      "grad_norm": 1.5856167078018188,
      "learning_rate": 0.00017949662133173982,
      "loss": 1.1172,
      "step": 8546
    },
    {
      "epoch": 1.201602699283003,
      "grad_norm": 1.3501601219177246,
      "learning_rate": 0.0001796374348729751,
      "loss": 1.3097,
      "step": 8547
    },
    {
      "epoch": 1.2017432869394067,
      "grad_norm": 1.510524034500122,
      "learning_rate": 0.00017977781803728016,
      "loss": 1.0461,
      "step": 8548
    },
    {
      "epoch": 1.2018838745958105,
      "grad_norm": 1.607750654220581,
      "learning_rate": 0.00017991777006599549,
      "loss": 1.0155,
      "step": 8549
    },
    {
      "epoch": 1.2020244622522143,
      "grad_norm": 1.9289458990097046,
      "learning_rate": 0.00018005729020279226,
      "loss": 0.9231,
      "step": 8550
    },
    {
      "epoch": 1.202165049908618,
      "grad_norm": 1.4911998510360718,
      "learning_rate": 0.00018019637769367495,
      "loss": 0.9499,
      "step": 8551
    },
    {
      "epoch": 1.2023056375650218,
      "grad_norm": 1.688411831855774,
      "learning_rate": 0.0001803350317869867,
      "loss": 0.9881,
      "step": 8552
    },
    {
      "epoch": 1.2024462252214256,
      "grad_norm": 1.363228440284729,
      "learning_rate": 0.00018047325173341206,
      "loss": 1.0658,
      "step": 8553
    },
    {
      "epoch": 1.2025868128778292,
      "grad_norm": 1.7816060781478882,
      "learning_rate": 0.00018061103678598223,
      "loss": 1.1121,
      "step": 8554
    },
    {
      "epoch": 1.2027274005342332,
      "grad_norm": 1.6172353029251099,
      "learning_rate": 0.00018074838620007918,
      "loss": 0.9965,
      "step": 8555
    },
    {
      "epoch": 1.2028679881906368,
      "grad_norm": 1.6548540592193604,
      "learning_rate": 0.00018088529923343847,
      "loss": 1.1127,
      "step": 8556
    },
    {
      "epoch": 1.2030085758470406,
      "grad_norm": 1.7067980766296387,
      "learning_rate": 0.00018102177514615412,
      "loss": 1.0126,
      "step": 8557
    },
    {
      "epoch": 1.2031491635034444,
      "grad_norm": 1.7749303579330444,
      "learning_rate": 0.0001811578132006825,
      "loss": 1.0905,
      "step": 8558
    },
    {
      "epoch": 1.2032897511598482,
      "grad_norm": 1.4746423959732056,
      "learning_rate": 0.00018129341266184628,
      "loss": 1.2879,
      "step": 8559
    },
    {
      "epoch": 1.203430338816252,
      "grad_norm": 1.5099223852157593,
      "learning_rate": 0.00018142857279683825,
      "loss": 1.0156,
      "step": 8560
    },
    {
      "epoch": 1.2035709264726557,
      "grad_norm": 1.5086735486984253,
      "learning_rate": 0.00018156329287522555,
      "loss": 0.9892,
      "step": 8561
    },
    {
      "epoch": 1.2037115141290595,
      "grad_norm": 1.678858995437622,
      "learning_rate": 0.0001816975721689534,
      "loss": 1.2122,
      "step": 8562
    },
    {
      "epoch": 1.2038521017854633,
      "grad_norm": 1.622898817062378,
      "learning_rate": 0.00018183140995234914,
      "loss": 1.0952,
      "step": 8563
    },
    {
      "epoch": 1.203992689441867,
      "grad_norm": 1.651643991470337,
      "learning_rate": 0.00018196480550212605,
      "loss": 1.0808,
      "step": 8564
    },
    {
      "epoch": 1.2041332770982707,
      "grad_norm": 1.5287561416625977,
      "learning_rate": 0.00018209775809738726,
      "loss": 1.0129,
      "step": 8565
    },
    {
      "epoch": 1.2042738647546745,
      "grad_norm": 1.536651372909546,
      "learning_rate": 0.00018223026701963025,
      "loss": 1.2851,
      "step": 8566
    },
    {
      "epoch": 1.2044144524110783,
      "grad_norm": 1.6455296277999878,
      "learning_rate": 0.00018236233155274973,
      "loss": 0.9373,
      "step": 8567
    },
    {
      "epoch": 1.204555040067482,
      "grad_norm": 1.6717866659164429,
      "learning_rate": 0.0001824939509830417,
      "loss": 1.1134,
      "step": 8568
    },
    {
      "epoch": 1.2046956277238858,
      "grad_norm": 1.9817569255828857,
      "learning_rate": 0.0001826251245992081,
      "loss": 1.1191,
      "step": 8569
    },
    {
      "epoch": 1.2048362153802896,
      "grad_norm": 1.5004934072494507,
      "learning_rate": 0.0001827558516923598,
      "loss": 1.1622,
      "step": 8570
    },
    {
      "epoch": 1.2049768030366934,
      "grad_norm": 1.5606452226638794,
      "learning_rate": 0.00018288613155602134,
      "loss": 1.1367,
      "step": 8571
    },
    {
      "epoch": 1.2051173906930972,
      "grad_norm": 2.105276584625244,
      "learning_rate": 0.00018301596348613354,
      "loss": 1.0488,
      "step": 8572
    },
    {
      "epoch": 1.205257978349501,
      "grad_norm": 1.6584315299987793,
      "learning_rate": 0.00018314534678105822,
      "loss": 1.0862,
      "step": 8573
    },
    {
      "epoch": 1.2053985660059046,
      "grad_norm": 1.6104896068572998,
      "learning_rate": 0.0001832742807415817,
      "loss": 1.2122,
      "step": 8574
    },
    {
      "epoch": 1.2055391536623086,
      "grad_norm": 1.6374517679214478,
      "learning_rate": 0.00018340276467091875,
      "loss": 1.0718,
      "step": 8575
    },
    {
      "epoch": 1.2056797413187121,
      "grad_norm": 1.567060947418213,
      "learning_rate": 0.0001835307978747154,
      "loss": 1.1125,
      "step": 8576
    },
    {
      "epoch": 1.205820328975116,
      "grad_norm": 1.6676924228668213,
      "learning_rate": 0.0001836583796610548,
      "loss": 0.9929,
      "step": 8577
    },
    {
      "epoch": 1.2059609166315197,
      "grad_norm": 1.651229977607727,
      "learning_rate": 0.0001837855093404587,
      "loss": 0.9645,
      "step": 8578
    },
    {
      "epoch": 1.2061015042879235,
      "grad_norm": 1.3343937397003174,
      "learning_rate": 0.00018391218622589242,
      "loss": 1.0541,
      "step": 8579
    },
    {
      "epoch": 1.2062420919443273,
      "grad_norm": 1.9368882179260254,
      "learning_rate": 0.00018403840963276827,
      "loss": 1.0408,
      "step": 8580
    },
    {
      "epoch": 1.206382679600731,
      "grad_norm": 1.643975019454956,
      "learning_rate": 0.00018416417887894904,
      "loss": 0.9392,
      "step": 8581
    },
    {
      "epoch": 1.2065232672571349,
      "grad_norm": 1.7951980829238892,
      "learning_rate": 0.00018428949328475245,
      "loss": 1.062,
      "step": 8582
    },
    {
      "epoch": 1.2066638549135387,
      "grad_norm": 2.020134925842285,
      "learning_rate": 0.0001844143521729539,
      "loss": 1.1211,
      "step": 8583
    },
    {
      "epoch": 1.2068044425699425,
      "grad_norm": 1.5939949750900269,
      "learning_rate": 0.00018453875486879016,
      "loss": 1.0486,
      "step": 8584
    },
    {
      "epoch": 1.206945030226346,
      "grad_norm": 1.957159161567688,
      "learning_rate": 0.00018466270069996386,
      "loss": 1.0295,
      "step": 8585
    },
    {
      "epoch": 1.2070856178827498,
      "grad_norm": 1.5592488050460815,
      "learning_rate": 0.00018478618899664634,
      "loss": 1.0137,
      "step": 8586
    },
    {
      "epoch": 1.2072262055391536,
      "grad_norm": 1.3485738039016724,
      "learning_rate": 0.00018490921909148194,
      "loss": 1.1629,
      "step": 8587
    },
    {
      "epoch": 1.2073667931955574,
      "grad_norm": 1.770546555519104,
      "learning_rate": 0.00018503179031959088,
      "loss": 1.2009,
      "step": 8588
    },
    {
      "epoch": 1.2075073808519612,
      "grad_norm": 1.485334038734436,
      "learning_rate": 0.0001851539020185732,
      "loss": 1.0444,
      "step": 8589
    },
    {
      "epoch": 1.207647968508365,
      "grad_norm": 2.093834161758423,
      "learning_rate": 0.00018527555352851257,
      "loss": 1.1724,
      "step": 8590
    },
    {
      "epoch": 1.2077885561647688,
      "grad_norm": 1.6926357746124268,
      "learning_rate": 0.0001853967441919791,
      "loss": 1.0387,
      "step": 8591
    },
    {
      "epoch": 1.2079291438211726,
      "grad_norm": 1.7297344207763672,
      "learning_rate": 0.00018551747335403368,
      "loss": 1.0008,
      "step": 8592
    },
    {
      "epoch": 1.2080697314775763,
      "grad_norm": 1.7824223041534424,
      "learning_rate": 0.00018563774036223175,
      "loss": 1.0716,
      "step": 8593
    },
    {
      "epoch": 1.20821031913398,
      "grad_norm": 1.6505330801010132,
      "learning_rate": 0.00018575754456662567,
      "loss": 1.1436,
      "step": 8594
    },
    {
      "epoch": 1.208350906790384,
      "grad_norm": 1.6782056093215942,
      "learning_rate": 0.00018587688531976912,
      "loss": 0.9944,
      "step": 8595
    },
    {
      "epoch": 1.2084914944467875,
      "grad_norm": 1.6014411449432373,
      "learning_rate": 0.0001859957619767203,
      "loss": 1.1479,
      "step": 8596
    },
    {
      "epoch": 1.2086320821031913,
      "grad_norm": 1.9222462177276611,
      "learning_rate": 0.00018611417389504536,
      "loss": 1.2773,
      "step": 8597
    },
    {
      "epoch": 1.208772669759595,
      "grad_norm": 1.9008170366287231,
      "learning_rate": 0.00018623212043482275,
      "loss": 1.0153,
      "step": 8598
    },
    {
      "epoch": 1.2089132574159989,
      "grad_norm": 1.730663537979126,
      "learning_rate": 0.0001863496009586447,
      "loss": 1.1518,
      "step": 8599
    },
    {
      "epoch": 1.2090538450724027,
      "grad_norm": 1.451839804649353,
      "learning_rate": 0.00018646661483162277,
      "loss": 0.9223,
      "step": 8600
    },
    {
      "epoch": 1.2091944327288064,
      "grad_norm": 1.4680167436599731,
      "learning_rate": 0.00018658316142139013,
      "loss": 1.1935,
      "step": 8601
    },
    {
      "epoch": 1.2093350203852102,
      "grad_norm": 1.6806025505065918,
      "learning_rate": 0.0001866992400981052,
      "loss": 0.9898,
      "step": 8602
    },
    {
      "epoch": 1.209475608041614,
      "grad_norm": 1.5919837951660156,
      "learning_rate": 0.00018681485023445502,
      "loss": 1.0288,
      "step": 8603
    },
    {
      "epoch": 1.2096161956980178,
      "grad_norm": 1.7048786878585815,
      "learning_rate": 0.0001869299912056592,
      "loss": 1.1307,
      "step": 8604
    },
    {
      "epoch": 1.2097567833544214,
      "grad_norm": 1.6532838344573975,
      "learning_rate": 0.00018704466238947243,
      "loss": 1.0989,
      "step": 8605
    },
    {
      "epoch": 1.2098973710108252,
      "grad_norm": 1.7739169597625732,
      "learning_rate": 0.00018715886316618798,
      "loss": 1.0783,
      "step": 8606
    },
    {
      "epoch": 1.210037958667229,
      "grad_norm": 1.4767006635665894,
      "learning_rate": 0.00018727259291864167,
      "loss": 1.044,
      "step": 8607
    },
    {
      "epoch": 1.2101785463236328,
      "grad_norm": 1.5135040283203125,
      "learning_rate": 0.00018738585103221457,
      "loss": 1.2855,
      "step": 8608
    },
    {
      "epoch": 1.2103191339800365,
      "grad_norm": 1.8175337314605713,
      "learning_rate": 0.0001874986368948372,
      "loss": 1.0874,
      "step": 8609
    },
    {
      "epoch": 1.2104597216364403,
      "grad_norm": 1.5629311800003052,
      "learning_rate": 0.00018761094989699144,
      "loss": 1.0157,
      "step": 8610
    },
    {
      "epoch": 1.2106003092928441,
      "grad_norm": 1.6283754110336304,
      "learning_rate": 0.00018772278943171504,
      "loss": 0.9504,
      "step": 8611
    },
    {
      "epoch": 1.210740896949248,
      "grad_norm": 1.80652916431427,
      "learning_rate": 0.0001878341548946043,
      "loss": 1.0725,
      "step": 8612
    },
    {
      "epoch": 1.2108814846056517,
      "grad_norm": 1.4895188808441162,
      "learning_rate": 0.00018794504568381767,
      "loss": 0.9867,
      "step": 8613
    },
    {
      "epoch": 1.2110220722620553,
      "grad_norm": 1.729108452796936,
      "learning_rate": 0.00018805546120007858,
      "loss": 0.9804,
      "step": 8614
    },
    {
      "epoch": 1.2111626599184593,
      "grad_norm": 1.4263594150543213,
      "learning_rate": 0.00018816540084667916,
      "loss": 1.1461,
      "step": 8615
    },
    {
      "epoch": 1.2113032475748629,
      "grad_norm": 1.684341549873352,
      "learning_rate": 0.00018827486402948313,
      "loss": 0.9823,
      "step": 8616
    },
    {
      "epoch": 1.2114438352312666,
      "grad_norm": 1.618813157081604,
      "learning_rate": 0.00018838385015692912,
      "loss": 1.1533,
      "step": 8617
    },
    {
      "epoch": 1.2115844228876704,
      "grad_norm": 1.6146156787872314,
      "learning_rate": 0.00018849235864003391,
      "loss": 1.1112,
      "step": 8618
    },
    {
      "epoch": 1.2117250105440742,
      "grad_norm": 1.8391087055206299,
      "learning_rate": 0.00018860038889239537,
      "loss": 0.9598,
      "step": 8619
    },
    {
      "epoch": 1.211865598200478,
      "grad_norm": 1.481011986732483,
      "learning_rate": 0.0001887079403301963,
      "loss": 1.1731,
      "step": 8620
    },
    {
      "epoch": 1.2120061858568818,
      "grad_norm": 1.6288082599639893,
      "learning_rate": 0.00018881501237220686,
      "loss": 1.0032,
      "step": 8621
    },
    {
      "epoch": 1.2121467735132856,
      "grad_norm": 1.626716136932373,
      "learning_rate": 0.00018892160443978754,
      "loss": 0.91,
      "step": 8622
    },
    {
      "epoch": 1.2122873611696894,
      "grad_norm": 1.5676583051681519,
      "learning_rate": 0.00018902771595689325,
      "loss": 1.1206,
      "step": 8623
    },
    {
      "epoch": 1.2124279488260932,
      "grad_norm": 1.6604034900665283,
      "learning_rate": 0.0001891333463500756,
      "loss": 1.0458,
      "step": 8624
    },
    {
      "epoch": 1.2125685364824967,
      "grad_norm": 1.5479459762573242,
      "learning_rate": 0.00018923849504848668,
      "loss": 1.0496,
      "step": 8625
    },
    {
      "epoch": 1.2127091241389005,
      "grad_norm": 1.5222671031951904,
      "learning_rate": 0.00018934316148388135,
      "loss": 1.2197,
      "step": 8626
    },
    {
      "epoch": 1.2128497117953043,
      "grad_norm": 1.5222067832946777,
      "learning_rate": 0.0001894473450906209,
      "loss": 1.1709,
      "step": 8627
    },
    {
      "epoch": 1.212990299451708,
      "grad_norm": 1.6599632501602173,
      "learning_rate": 0.00018955104530567597,
      "loss": 1.0407,
      "step": 8628
    },
    {
      "epoch": 1.213130887108112,
      "grad_norm": 1.8445886373519897,
      "learning_rate": 0.00018965426156862904,
      "loss": 0.955,
      "step": 8629
    },
    {
      "epoch": 1.2132714747645157,
      "grad_norm": 1.7622630596160889,
      "learning_rate": 0.00018975699332167886,
      "loss": 1.024,
      "step": 8630
    },
    {
      "epoch": 1.2134120624209195,
      "grad_norm": 1.7454650402069092,
      "learning_rate": 0.0001898592400096419,
      "loss": 1.1293,
      "step": 8631
    },
    {
      "epoch": 1.2135526500773233,
      "grad_norm": 1.837781310081482,
      "learning_rate": 0.00018996100107995627,
      "loss": 1.1083,
      "step": 8632
    },
    {
      "epoch": 1.213693237733727,
      "grad_norm": 1.7949869632720947,
      "learning_rate": 0.00019006227598268436,
      "loss": 0.8764,
      "step": 8633
    },
    {
      "epoch": 1.2138338253901306,
      "grad_norm": 1.8554000854492188,
      "learning_rate": 0.00019016306417051596,
      "loss": 0.9306,
      "step": 8634
    },
    {
      "epoch": 1.2139744130465344,
      "grad_norm": 1.5800533294677734,
      "learning_rate": 0.00019026336509877102,
      "loss": 1.0014,
      "step": 8635
    },
    {
      "epoch": 1.2141150007029382,
      "grad_norm": 1.6503411531448364,
      "learning_rate": 0.00019036317822540338,
      "loss": 1.0377,
      "step": 8636
    },
    {
      "epoch": 1.214255588359342,
      "grad_norm": 1.5622003078460693,
      "learning_rate": 0.000190462503011002,
      "loss": 1.0747,
      "step": 8637
    },
    {
      "epoch": 1.2143961760157458,
      "grad_norm": 1.5208163261413574,
      "learning_rate": 0.00019056133891879573,
      "loss": 1.0178,
      "step": 8638
    },
    {
      "epoch": 1.2145367636721496,
      "grad_norm": 1.5395855903625488,
      "learning_rate": 0.00019065968541465516,
      "loss": 1.1035,
      "step": 8639
    },
    {
      "epoch": 1.2146773513285534,
      "grad_norm": 1.4205129146575928,
      "learning_rate": 0.00019075754196709558,
      "loss": 1.1496,
      "step": 8640
    },
    {
      "epoch": 1.2148179389849572,
      "grad_norm": 1.5993361473083496,
      "learning_rate": 0.00019085490804728064,
      "loss": 0.9403,
      "step": 8641
    },
    {
      "epoch": 1.214958526641361,
      "grad_norm": 1.799572229385376,
      "learning_rate": 0.00019095178312902395,
      "loss": 1.2233,
      "step": 8642
    },
    {
      "epoch": 1.2150991142977647,
      "grad_norm": 1.3525540828704834,
      "learning_rate": 0.00019104816668879285,
      "loss": 1.1351,
      "step": 8643
    },
    {
      "epoch": 1.2152397019541685,
      "grad_norm": 1.9740850925445557,
      "learning_rate": 0.00019114405820571108,
      "loss": 1.1176,
      "step": 8644
    },
    {
      "epoch": 1.215380289610572,
      "grad_norm": 1.7909122705459595,
      "learning_rate": 0.00019123945716156108,
      "loss": 0.9375,
      "step": 8645
    },
    {
      "epoch": 1.2155208772669759,
      "grad_norm": 1.5550326108932495,
      "learning_rate": 0.00019133436304078744,
      "loss": 1.0868,
      "step": 8646
    },
    {
      "epoch": 1.2156614649233797,
      "grad_norm": 1.7029340267181396,
      "learning_rate": 0.00019142877533049967,
      "loss": 1.1179,
      "step": 8647
    },
    {
      "epoch": 1.2158020525797835,
      "grad_norm": 1.748215913772583,
      "learning_rate": 0.00019152269352047446,
      "loss": 1.2518,
      "step": 8648
    },
    {
      "epoch": 1.2159426402361873,
      "grad_norm": 1.777498722076416,
      "learning_rate": 0.0001916161171031587,
      "loss": 1.0918,
      "step": 8649
    },
    {
      "epoch": 1.216083227892591,
      "grad_norm": 1.4531430006027222,
      "learning_rate": 0.00019170904557367234,
      "loss": 1.111,
      "step": 8650
    },
    {
      "epoch": 1.2162238155489948,
      "grad_norm": 1.5074812173843384,
      "learning_rate": 0.00019180147842981095,
      "loss": 1.0724,
      "step": 8651
    },
    {
      "epoch": 1.2163644032053986,
      "grad_norm": 1.8941662311553955,
      "learning_rate": 0.00019189341517204862,
      "loss": 1.004,
      "step": 8652
    },
    {
      "epoch": 1.2165049908618024,
      "grad_norm": 1.7299690246582031,
      "learning_rate": 0.00019198485530354034,
      "loss": 1.1586,
      "step": 8653
    },
    {
      "epoch": 1.216645578518206,
      "grad_norm": 1.6564629077911377,
      "learning_rate": 0.0001920757983301251,
      "loss": 1.1596,
      "step": 8654
    },
    {
      "epoch": 1.2167861661746098,
      "grad_norm": 1.7457786798477173,
      "learning_rate": 0.00019216624376032816,
      "loss": 1.0977,
      "step": 8655
    },
    {
      "epoch": 1.2169267538310136,
      "grad_norm": 1.8006007671356201,
      "learning_rate": 0.00019225619110536393,
      "loss": 0.9698,
      "step": 8656
    },
    {
      "epoch": 1.2170673414874174,
      "grad_norm": 1.5975559949874878,
      "learning_rate": 0.00019234563987913884,
      "loss": 1.2981,
      "step": 8657
    },
    {
      "epoch": 1.2172079291438211,
      "grad_norm": 1.6241254806518555,
      "learning_rate": 0.00019243458959825337,
      "loss": 1.1173,
      "step": 8658
    },
    {
      "epoch": 1.217348516800225,
      "grad_norm": 1.4858529567718506,
      "learning_rate": 0.00019252303978200522,
      "loss": 1.1774,
      "step": 8659
    },
    {
      "epoch": 1.2174891044566287,
      "grad_norm": 1.7454603910446167,
      "learning_rate": 0.00019261098995239134,
      "loss": 0.9907,
      "step": 8660
    },
    {
      "epoch": 1.2176296921130325,
      "grad_norm": 1.683753252029419,
      "learning_rate": 0.00019269843963411117,
      "loss": 1.016,
      "step": 8661
    },
    {
      "epoch": 1.2177702797694363,
      "grad_norm": 1.646357774734497,
      "learning_rate": 0.0001927853883545687,
      "loss": 1.0765,
      "step": 8662
    },
    {
      "epoch": 1.21791086742584,
      "grad_norm": 1.846459984779358,
      "learning_rate": 0.00019287183564387558,
      "loss": 1.1488,
      "step": 8663
    },
    {
      "epoch": 1.2180514550822439,
      "grad_norm": 1.767751693725586,
      "learning_rate": 0.00019295778103485292,
      "loss": 1.13,
      "step": 8664
    },
    {
      "epoch": 1.2181920427386475,
      "grad_norm": 1.6449239253997803,
      "learning_rate": 0.0001930432240630343,
      "loss": 1.1366,
      "step": 8665
    },
    {
      "epoch": 1.2183326303950512,
      "grad_norm": 1.7824351787567139,
      "learning_rate": 0.0001931281642666683,
      "loss": 1.011,
      "step": 8666
    },
    {
      "epoch": 1.218473218051455,
      "grad_norm": 2.0674524307250977,
      "learning_rate": 0.00019321260118672037,
      "loss": 1.0904,
      "step": 8667
    },
    {
      "epoch": 1.2186138057078588,
      "grad_norm": 1.615710973739624,
      "learning_rate": 0.00019329653436687657,
      "loss": 1.0912,
      "step": 8668
    },
    {
      "epoch": 1.2187543933642626,
      "grad_norm": 1.5650159120559692,
      "learning_rate": 0.00019337996335354476,
      "loss": 0.9842,
      "step": 8669
    },
    {
      "epoch": 1.2188949810206664,
      "grad_norm": 1.5245304107666016,
      "learning_rate": 0.00019346288769585767,
      "loss": 1.12,
      "step": 8670
    },
    {
      "epoch": 1.2190355686770702,
      "grad_norm": 2.2186975479125977,
      "learning_rate": 0.00019354530694567528,
      "loss": 0.997,
      "step": 8671
    },
    {
      "epoch": 1.219176156333474,
      "grad_norm": 1.9609698057174683,
      "learning_rate": 0.00019362722065758715,
      "loss": 1.0643,
      "step": 8672
    },
    {
      "epoch": 1.2193167439898778,
      "grad_norm": 1.4841314554214478,
      "learning_rate": 0.00019370862838891478,
      "loss": 1.0536,
      "step": 8673
    },
    {
      "epoch": 1.2194573316462813,
      "grad_norm": 1.7059420347213745,
      "learning_rate": 0.00019378952969971452,
      "loss": 1.1136,
      "step": 8674
    },
    {
      "epoch": 1.2195979193026851,
      "grad_norm": 1.6914831399917603,
      "learning_rate": 0.00019386992415277888,
      "loss": 1.1803,
      "step": 8675
    },
    {
      "epoch": 1.219738506959089,
      "grad_norm": 1.701818585395813,
      "learning_rate": 0.00019394981131363995,
      "loss": 1.1659,
      "step": 8676
    },
    {
      "epoch": 1.2198790946154927,
      "grad_norm": 1.512498140335083,
      "learning_rate": 0.00019402919075057123,
      "loss": 1.0758,
      "step": 8677
    },
    {
      "epoch": 1.2200196822718965,
      "grad_norm": 1.4325774908065796,
      "learning_rate": 0.00019410806203458991,
      "loss": 1.1336,
      "step": 8678
    },
    {
      "epoch": 1.2201602699283003,
      "grad_norm": 1.555996298789978,
      "learning_rate": 0.0001941864247394598,
      "loss": 1.0553,
      "step": 8679
    },
    {
      "epoch": 1.220300857584704,
      "grad_norm": 1.6040648221969604,
      "learning_rate": 0.00019426427844169273,
      "loss": 1.2159,
      "step": 8680
    },
    {
      "epoch": 1.2204414452411079,
      "grad_norm": 1.640317678451538,
      "learning_rate": 0.00019434162272055137,
      "loss": 1.2422,
      "step": 8681
    },
    {
      "epoch": 1.2205820328975117,
      "grad_norm": 1.7110555171966553,
      "learning_rate": 0.00019441845715805167,
      "loss": 1.0378,
      "step": 8682
    },
    {
      "epoch": 1.2207226205539154,
      "grad_norm": 1.8883821964263916,
      "learning_rate": 0.00019449478133896426,
      "loss": 1.0662,
      "step": 8683
    },
    {
      "epoch": 1.2208632082103192,
      "grad_norm": 1.8679864406585693,
      "learning_rate": 0.00019457059485081807,
      "loss": 1.1162,
      "step": 8684
    },
    {
      "epoch": 1.2210037958667228,
      "grad_norm": 2.2061445713043213,
      "learning_rate": 0.00019464589728390136,
      "loss": 1.1269,
      "step": 8685
    },
    {
      "epoch": 1.2211443835231266,
      "grad_norm": 1.9168332815170288,
      "learning_rate": 0.00019472068823126436,
      "loss": 1.066,
      "step": 8686
    },
    {
      "epoch": 1.2212849711795304,
      "grad_norm": 1.4180883169174194,
      "learning_rate": 0.00019479496728872162,
      "loss": 1.1609,
      "step": 8687
    },
    {
      "epoch": 1.2214255588359342,
      "grad_norm": 1.6177150011062622,
      "learning_rate": 0.0001948687340548539,
      "loss": 1.0959,
      "step": 8688
    },
    {
      "epoch": 1.221566146492338,
      "grad_norm": 1.56106436252594,
      "learning_rate": 0.0001949419881310106,
      "loss": 0.9274,
      "step": 8689
    },
    {
      "epoch": 1.2217067341487418,
      "grad_norm": 1.521384358406067,
      "learning_rate": 0.00019501472912131178,
      "loss": 1.2226,
      "step": 8690
    },
    {
      "epoch": 1.2218473218051455,
      "grad_norm": 1.8167837858200073,
      "learning_rate": 0.00019508695663265026,
      "loss": 1.0411,
      "step": 8691
    },
    {
      "epoch": 1.2219879094615493,
      "grad_norm": 1.654869556427002,
      "learning_rate": 0.00019515867027469387,
      "loss": 1.029,
      "step": 8692
    },
    {
      "epoch": 1.2221284971179531,
      "grad_norm": 1.5811618566513062,
      "learning_rate": 0.00019522986965988745,
      "loss": 0.9819,
      "step": 8693
    },
    {
      "epoch": 1.2222690847743567,
      "grad_norm": 1.826998233795166,
      "learning_rate": 0.00019530055440345495,
      "loss": 1.0576,
      "step": 8694
    },
    {
      "epoch": 1.2224096724307605,
      "grad_norm": 2.0599207878112793,
      "learning_rate": 0.00019537072412340178,
      "loss": 1.0441,
      "step": 8695
    },
    {
      "epoch": 1.2225502600871643,
      "grad_norm": 2.061185359954834,
      "learning_rate": 0.00019544037844051638,
      "loss": 1.0645,
      "step": 8696
    },
    {
      "epoch": 1.222690847743568,
      "grad_norm": 1.31425940990448,
      "learning_rate": 0.00019550951697837272,
      "loss": 0.9937,
      "step": 8697
    },
    {
      "epoch": 1.2228314353999719,
      "grad_norm": 1.875032901763916,
      "learning_rate": 0.00019557813936333185,
      "loss": 0.9932,
      "step": 8698
    },
    {
      "epoch": 1.2229720230563756,
      "grad_norm": 1.7341465950012207,
      "learning_rate": 0.0001956462452245444,
      "loss": 1.057,
      "step": 8699
    },
    {
      "epoch": 1.2231126107127794,
      "grad_norm": 1.443228840827942,
      "learning_rate": 0.0001957138341939526,
      "loss": 1.0281,
      "step": 8700
    },
    {
      "epoch": 1.2232531983691832,
      "grad_norm": 1.595674753189087,
      "learning_rate": 0.00019578090590629185,
      "loss": 1.1412,
      "step": 8701
    },
    {
      "epoch": 1.223393786025587,
      "grad_norm": 2.0976381301879883,
      "learning_rate": 0.0001958474599990929,
      "loss": 0.9938,
      "step": 8702
    },
    {
      "epoch": 1.2235343736819908,
      "grad_norm": 1.6353412866592407,
      "learning_rate": 0.00019591349611268387,
      "loss": 1.0776,
      "step": 8703
    },
    {
      "epoch": 1.2236749613383946,
      "grad_norm": 1.7680541276931763,
      "learning_rate": 0.00019597901389019223,
      "loss": 1.0562,
      "step": 8704
    },
    {
      "epoch": 1.2238155489947982,
      "grad_norm": 2.1196980476379395,
      "learning_rate": 0.00019604401297754632,
      "loss": 1.0777,
      "step": 8705
    },
    {
      "epoch": 1.223956136651202,
      "grad_norm": 1.4547349214553833,
      "learning_rate": 0.00019610849302347801,
      "loss": 1.1217,
      "step": 8706
    },
    {
      "epoch": 1.2240967243076057,
      "grad_norm": 1.8204677104949951,
      "learning_rate": 0.000196172453679524,
      "loss": 1.1154,
      "step": 8707
    },
    {
      "epoch": 1.2242373119640095,
      "grad_norm": 1.5248924493789673,
      "learning_rate": 0.0001962358946000278,
      "loss": 1.136,
      "step": 8708
    },
    {
      "epoch": 1.2243778996204133,
      "grad_norm": 1.854850172996521,
      "learning_rate": 0.00019629881544214176,
      "loss": 1.0743,
      "step": 8709
    },
    {
      "epoch": 1.2245184872768171,
      "grad_norm": 1.8095831871032715,
      "learning_rate": 0.00019636121586582875,
      "loss": 1.3496,
      "step": 8710
    },
    {
      "epoch": 1.224659074933221,
      "grad_norm": 1.6921563148498535,
      "learning_rate": 0.00019642309553386436,
      "loss": 1.1476,
      "step": 8711
    },
    {
      "epoch": 1.2247996625896247,
      "grad_norm": 1.7464624643325806,
      "learning_rate": 0.0001964844541118382,
      "loss": 1.0014,
      "step": 8712
    },
    {
      "epoch": 1.2249402502460285,
      "grad_norm": 1.6329705715179443,
      "learning_rate": 0.00019654529126815584,
      "loss": 0.9051,
      "step": 8713
    },
    {
      "epoch": 1.225080837902432,
      "grad_norm": 2.1389410495758057,
      "learning_rate": 0.00019660560667404098,
      "loss": 1.1435,
      "step": 8714
    },
    {
      "epoch": 1.2252214255588358,
      "grad_norm": 1.4429205656051636,
      "learning_rate": 0.00019666540000353691,
      "loss": 1.4002,
      "step": 8715
    },
    {
      "epoch": 1.2253620132152396,
      "grad_norm": 1.8572392463684082,
      "learning_rate": 0.00019672467093350814,
      "loss": 1.0831,
      "step": 8716
    },
    {
      "epoch": 1.2255026008716434,
      "grad_norm": 1.679436206817627,
      "learning_rate": 0.00019678341914364272,
      "loss": 1.0439,
      "step": 8717
    },
    {
      "epoch": 1.2256431885280472,
      "grad_norm": 1.7269903421401978,
      "learning_rate": 0.00019684164431645323,
      "loss": 1.0806,
      "step": 8718
    },
    {
      "epoch": 1.225783776184451,
      "grad_norm": 2.174917697906494,
      "learning_rate": 0.00019689934613727897,
      "loss": 1.0569,
      "step": 8719
    },
    {
      "epoch": 1.2259243638408548,
      "grad_norm": 1.7764158248901367,
      "learning_rate": 0.0001969565242942876,
      "loss": 0.9408,
      "step": 8720
    },
    {
      "epoch": 1.2260649514972586,
      "grad_norm": 1.7151402235031128,
      "learning_rate": 0.0001970131784784765,
      "loss": 1.0336,
      "step": 8721
    },
    {
      "epoch": 1.2262055391536624,
      "grad_norm": 1.4769755601882935,
      "learning_rate": 0.00019706930838367512,
      "loss": 1.1768,
      "step": 8722
    },
    {
      "epoch": 1.2263461268100662,
      "grad_norm": 1.7974481582641602,
      "learning_rate": 0.000197124913706546,
      "loss": 1.2559,
      "step": 8723
    },
    {
      "epoch": 1.22648671446647,
      "grad_norm": 1.8691551685333252,
      "learning_rate": 0.00019717999414658652,
      "loss": 1.0512,
      "step": 8724
    },
    {
      "epoch": 1.2266273021228735,
      "grad_norm": 1.604525089263916,
      "learning_rate": 0.00019723454940613088,
      "loss": 1.0146,
      "step": 8725
    },
    {
      "epoch": 1.2267678897792773,
      "grad_norm": 1.6912280321121216,
      "learning_rate": 0.00019728857919035114,
      "loss": 1.0224,
      "step": 8726
    },
    {
      "epoch": 1.226908477435681,
      "grad_norm": 1.5588629245758057,
      "learning_rate": 0.00019734208320725967,
      "loss": 1.0877,
      "step": 8727
    },
    {
      "epoch": 1.227049065092085,
      "grad_norm": 1.6240425109863281,
      "learning_rate": 0.0001973950611677096,
      "loss": 1.086,
      "step": 8728
    },
    {
      "epoch": 1.2271896527484887,
      "grad_norm": 1.4776901006698608,
      "learning_rate": 0.00019744751278539727,
      "loss": 1.2528,
      "step": 8729
    },
    {
      "epoch": 1.2273302404048925,
      "grad_norm": 1.6961791515350342,
      "learning_rate": 0.0001974994377768635,
      "loss": 0.9668,
      "step": 8730
    },
    {
      "epoch": 1.2274708280612963,
      "grad_norm": 1.855799674987793,
      "learning_rate": 0.00019755083586149499,
      "loss": 1.1107,
      "step": 8731
    },
    {
      "epoch": 1.2276114157177,
      "grad_norm": 2.115074872970581,
      "learning_rate": 0.00019760170676152598,
      "loss": 1.201,
      "step": 8732
    },
    {
      "epoch": 1.2277520033741038,
      "grad_norm": 1.7600239515304565,
      "learning_rate": 0.00019765205020203988,
      "loss": 1.1262,
      "step": 8733
    },
    {
      "epoch": 1.2278925910305074,
      "grad_norm": 1.5081596374511719,
      "learning_rate": 0.00019770186591097047,
      "loss": 1.1355,
      "step": 8734
    },
    {
      "epoch": 1.2280331786869112,
      "grad_norm": 1.4745432138442993,
      "learning_rate": 0.00019775115361910352,
      "loss": 1.1645,
      "step": 8735
    },
    {
      "epoch": 1.228173766343315,
      "grad_norm": 1.7961143255233765,
      "learning_rate": 0.0001977999130600781,
      "loss": 1.0234,
      "step": 8736
    },
    {
      "epoch": 1.2283143539997188,
      "grad_norm": 1.4548044204711914,
      "learning_rate": 0.0001978481439703882,
      "loss": 1.2845,
      "step": 8737
    },
    {
      "epoch": 1.2284549416561226,
      "grad_norm": 1.8089616298675537,
      "learning_rate": 0.00019789584608938428,
      "loss": 0.9616,
      "step": 8738
    },
    {
      "epoch": 1.2285955293125264,
      "grad_norm": 1.9264253377914429,
      "learning_rate": 0.00019794301915927439,
      "loss": 1.1143,
      "step": 8739
    },
    {
      "epoch": 1.2287361169689301,
      "grad_norm": 1.6537054777145386,
      "learning_rate": 0.00019798966292512558,
      "loss": 1.2687,
      "step": 8740
    },
    {
      "epoch": 1.228876704625334,
      "grad_norm": 1.8337377309799194,
      "learning_rate": 0.0001980357771348654,
      "loss": 1.1715,
      "step": 8741
    },
    {
      "epoch": 1.2290172922817377,
      "grad_norm": 1.413677453994751,
      "learning_rate": 0.00019808136153928336,
      "loss": 1.205,
      "step": 8742
    },
    {
      "epoch": 1.2291578799381415,
      "grad_norm": 1.5808470249176025,
      "learning_rate": 0.00019812641589203196,
      "loss": 1.0265,
      "step": 8743
    },
    {
      "epoch": 1.2292984675945453,
      "grad_norm": 1.3786814212799072,
      "learning_rate": 0.00019817093994962837,
      "loss": 1.1571,
      "step": 8744
    },
    {
      "epoch": 1.2294390552509489,
      "grad_norm": 1.5281074047088623,
      "learning_rate": 0.0001982149334714554,
      "loss": 0.9891,
      "step": 8745
    },
    {
      "epoch": 1.2295796429073527,
      "grad_norm": 1.5362448692321777,
      "learning_rate": 0.00019825839621976317,
      "loss": 1.2189,
      "step": 8746
    },
    {
      "epoch": 1.2297202305637565,
      "grad_norm": 1.638507604598999,
      "learning_rate": 0.00019830132795967015,
      "loss": 1.2695,
      "step": 8747
    },
    {
      "epoch": 1.2298608182201602,
      "grad_norm": 1.459954857826233,
      "learning_rate": 0.00019834372845916435,
      "loss": 1.0273,
      "step": 8748
    },
    {
      "epoch": 1.230001405876564,
      "grad_norm": 1.6469570398330688,
      "learning_rate": 0.00019838559748910496,
      "loss": 1.0551,
      "step": 8749
    },
    {
      "epoch": 1.2301419935329678,
      "grad_norm": 1.788034439086914,
      "learning_rate": 0.00019842693482322323,
      "loss": 1.2344,
      "step": 8750
    },
    {
      "epoch": 1.2302825811893716,
      "grad_norm": 1.746493935585022,
      "learning_rate": 0.00019846774023812364,
      "loss": 1.1637,
      "step": 8751
    },
    {
      "epoch": 1.2304231688457754,
      "grad_norm": 1.5676674842834473,
      "learning_rate": 0.0001985080135132855,
      "loss": 1.1255,
      "step": 8752
    },
    {
      "epoch": 1.2305637565021792,
      "grad_norm": 1.652653455734253,
      "learning_rate": 0.0001985477544310637,
      "loss": 1.0972,
      "step": 8753
    },
    {
      "epoch": 1.2307043441585828,
      "grad_norm": 1.871911883354187,
      "learning_rate": 0.00019858696277669043,
      "loss": 1.1219,
      "step": 8754
    },
    {
      "epoch": 1.2308449318149866,
      "grad_norm": 1.6508039236068726,
      "learning_rate": 0.00019862563833827565,
      "loss": 1.1272,
      "step": 8755
    },
    {
      "epoch": 1.2309855194713903,
      "grad_norm": 1.4211019277572632,
      "learning_rate": 0.00019866378090680883,
      "loss": 1.2203,
      "step": 8756
    },
    {
      "epoch": 1.2311261071277941,
      "grad_norm": 1.7572675943374634,
      "learning_rate": 0.00019870139027615964,
      "loss": 1.1344,
      "step": 8757
    },
    {
      "epoch": 1.231266694784198,
      "grad_norm": 1.6235005855560303,
      "learning_rate": 0.00019873846624307952,
      "loss": 1.0579,
      "step": 8758
    },
    {
      "epoch": 1.2314072824406017,
      "grad_norm": 1.8571717739105225,
      "learning_rate": 0.00019877500860720224,
      "loss": 1.0301,
      "step": 8759
    },
    {
      "epoch": 1.2315478700970055,
      "grad_norm": 2.09743070602417,
      "learning_rate": 0.00019881101717104553,
      "loss": 1.2596,
      "step": 8760
    },
    {
      "epoch": 1.2316884577534093,
      "grad_norm": 1.5222982168197632,
      "learning_rate": 0.00019884649174001172,
      "loss": 1.1806,
      "step": 8761
    },
    {
      "epoch": 1.231829045409813,
      "grad_norm": 1.5017739534378052,
      "learning_rate": 0.00019888143212238907,
      "loss": 0.9711,
      "step": 8762
    },
    {
      "epoch": 1.2319696330662169,
      "grad_norm": 1.7273513078689575,
      "learning_rate": 0.00019891583812935254,
      "loss": 1.0494,
      "step": 8763
    },
    {
      "epoch": 1.2321102207226207,
      "grad_norm": 1.6031317710876465,
      "learning_rate": 0.00019894970957496505,
      "loss": 1.078,
      "step": 8764
    },
    {
      "epoch": 1.2322508083790242,
      "grad_norm": 1.5496610403060913,
      "learning_rate": 0.0001989830462761786,
      "loss": 1.154,
      "step": 8765
    },
    {
      "epoch": 1.232391396035428,
      "grad_norm": 1.5393110513687134,
      "learning_rate": 0.0001990158480528346,
      "loss": 0.9418,
      "step": 8766
    },
    {
      "epoch": 1.2325319836918318,
      "grad_norm": 1.6079225540161133,
      "learning_rate": 0.00019904811472766572,
      "loss": 1.0157,
      "step": 8767
    },
    {
      "epoch": 1.2326725713482356,
      "grad_norm": 1.7007222175598145,
      "learning_rate": 0.00019907984612629623,
      "loss": 0.8908,
      "step": 8768
    },
    {
      "epoch": 1.2328131590046394,
      "grad_norm": 1.4779913425445557,
      "learning_rate": 0.0001991110420772431,
      "loss": 1.1208,
      "step": 8769
    },
    {
      "epoch": 1.2329537466610432,
      "grad_norm": 1.9653838872909546,
      "learning_rate": 0.00019914170241191724,
      "loss": 1.2843,
      "step": 8770
    },
    {
      "epoch": 1.233094334317447,
      "grad_norm": 1.5852138996124268,
      "learning_rate": 0.0001991718269646238,
      "loss": 1.1332,
      "step": 8771
    },
    {
      "epoch": 1.2332349219738508,
      "grad_norm": 1.5595401525497437,
      "learning_rate": 0.00019920141557256367,
      "loss": 1.0487,
      "step": 8772
    },
    {
      "epoch": 1.2333755096302546,
      "grad_norm": 1.8346537351608276,
      "learning_rate": 0.0001992304680758339,
      "loss": 1.1551,
      "step": 8773
    },
    {
      "epoch": 1.2335160972866581,
      "grad_norm": 1.6791480779647827,
      "learning_rate": 0.00019925898431742884,
      "loss": 1.2194,
      "step": 8774
    },
    {
      "epoch": 1.233656684943062,
      "grad_norm": 1.5488214492797852,
      "learning_rate": 0.0001992869641432409,
      "loss": 1.0627,
      "step": 8775
    },
    {
      "epoch": 1.2337972725994657,
      "grad_norm": 1.5211081504821777,
      "learning_rate": 0.00019931440740206146,
      "loss": 1.1009,
      "step": 8776
    },
    {
      "epoch": 1.2339378602558695,
      "grad_norm": 1.8726563453674316,
      "learning_rate": 0.00019934131394558151,
      "loss": 1.1231,
      "step": 8777
    },
    {
      "epoch": 1.2340784479122733,
      "grad_norm": 1.5682615041732788,
      "learning_rate": 0.00019936768362839265,
      "loss": 1.2333,
      "step": 8778
    },
    {
      "epoch": 1.234219035568677,
      "grad_norm": 2.156811237335205,
      "learning_rate": 0.00019939351630798765,
      "loss": 1.0382,
      "step": 8779
    },
    {
      "epoch": 1.2343596232250809,
      "grad_norm": 1.7145451307296753,
      "learning_rate": 0.0001994188118447615,
      "loss": 0.9728,
      "step": 8780
    },
    {
      "epoch": 1.2345002108814846,
      "grad_norm": 1.5767827033996582,
      "learning_rate": 0.00019944357010201203,
      "loss": 1.0677,
      "step": 8781
    },
    {
      "epoch": 1.2346407985378884,
      "grad_norm": 1.5015758275985718,
      "learning_rate": 0.00019946779094594046,
      "loss": 1.1389,
      "step": 8782
    },
    {
      "epoch": 1.2347813861942922,
      "grad_norm": 1.6663655042648315,
      "learning_rate": 0.00019949147424565248,
      "loss": 0.9764,
      "step": 8783
    },
    {
      "epoch": 1.234921973850696,
      "grad_norm": 1.721769094467163,
      "learning_rate": 0.00019951461987315864,
      "loss": 1.0708,
      "step": 8784
    },
    {
      "epoch": 1.2350625615070996,
      "grad_norm": 1.5271657705307007,
      "learning_rate": 0.00019953722770337532,
      "loss": 1.1795,
      "step": 8785
    },
    {
      "epoch": 1.2352031491635034,
      "grad_norm": 1.7732782363891602,
      "learning_rate": 0.00019955929761412516,
      "loss": 1.0741,
      "step": 8786
    },
    {
      "epoch": 1.2353437368199072,
      "grad_norm": 1.6980525255203247,
      "learning_rate": 0.0001995808294861379,
      "loss": 1.2289,
      "step": 8787
    },
    {
      "epoch": 1.235484324476311,
      "grad_norm": 1.7073559761047363,
      "learning_rate": 0.00019960182320305093,
      "loss": 1.1175,
      "step": 8788
    },
    {
      "epoch": 1.2356249121327147,
      "grad_norm": 1.6728875637054443,
      "learning_rate": 0.00019962227865140988,
      "loss": 1.2344,
      "step": 8789
    },
    {
      "epoch": 1.2357654997891185,
      "grad_norm": 1.7045178413391113,
      "learning_rate": 0.00019964219572066934,
      "loss": 1.0799,
      "step": 8790
    },
    {
      "epoch": 1.2359060874455223,
      "grad_norm": 1.6965159177780151,
      "learning_rate": 0.00019966157430319336,
      "loss": 0.9365,
      "step": 8791
    },
    {
      "epoch": 1.2360466751019261,
      "grad_norm": 1.805114507675171,
      "learning_rate": 0.00019968041429425622,
      "loss": 1.2645,
      "step": 8792
    },
    {
      "epoch": 1.23618726275833,
      "grad_norm": 1.3555117845535278,
      "learning_rate": 0.00019969871559204276,
      "loss": 1.2859,
      "step": 8793
    },
    {
      "epoch": 1.2363278504147335,
      "grad_norm": 1.5415759086608887,
      "learning_rate": 0.000199716478097649,
      "loss": 1.033,
      "step": 8794
    },
    {
      "epoch": 1.2364684380711373,
      "grad_norm": 1.6765121221542358,
      "learning_rate": 0.00019973370171508275,
      "loss": 1.105,
      "step": 8795
    },
    {
      "epoch": 1.236609025727541,
      "grad_norm": 1.6420179605484009,
      "learning_rate": 0.00019975038635126405,
      "loss": 1.0056,
      "step": 8796
    },
    {
      "epoch": 1.2367496133839448,
      "grad_norm": 1.8775173425674438,
      "learning_rate": 0.00019976653191602578,
      "loss": 1.0799,
      "step": 8797
    },
    {
      "epoch": 1.2368902010403486,
      "grad_norm": 1.7471799850463867,
      "learning_rate": 0.00019978213832211395,
      "loss": 1.1278,
      "step": 8798
    },
    {
      "epoch": 1.2370307886967524,
      "grad_norm": 1.5866464376449585,
      "learning_rate": 0.00019979720548518842,
      "loss": 1.1054,
      "step": 8799
    },
    {
      "epoch": 1.2371713763531562,
      "grad_norm": 1.5456726551055908,
      "learning_rate": 0.00019981173332382315,
      "loss": 1.0803,
      "step": 8800
    },
    {
      "epoch": 1.23731196400956,
      "grad_norm": 1.4369876384735107,
      "learning_rate": 0.0001998257217595067,
      "loss": 1.0496,
      "step": 8801
    },
    {
      "epoch": 1.2374525516659638,
      "grad_norm": 1.8853631019592285,
      "learning_rate": 0.00019983917071664275,
      "loss": 1.0258,
      "step": 8802
    },
    {
      "epoch": 1.2375931393223676,
      "grad_norm": 1.6758040189743042,
      "learning_rate": 0.00019985208012255042,
      "loss": 1.0932,
      "step": 8803
    },
    {
      "epoch": 1.2377337269787714,
      "grad_norm": 1.919793963432312,
      "learning_rate": 0.0001998644499074646,
      "loss": 0.9713,
      "step": 8804
    },
    {
      "epoch": 1.237874314635175,
      "grad_norm": 1.9803541898727417,
      "learning_rate": 0.00019987628000453644,
      "loss": 1.1296,
      "step": 8805
    },
    {
      "epoch": 1.2380149022915787,
      "grad_norm": 1.4986828565597534,
      "learning_rate": 0.00019988757034983373,
      "loss": 1.2346,
      "step": 8806
    },
    {
      "epoch": 1.2381554899479825,
      "grad_norm": 1.503238558769226,
      "learning_rate": 0.00019989832088234115,
      "loss": 1.0505,
      "step": 8807
    },
    {
      "epoch": 1.2382960776043863,
      "grad_norm": 1.6102577447891235,
      "learning_rate": 0.0001999085315439606,
      "loss": 1.2099,
      "step": 8808
    },
    {
      "epoch": 1.23843666526079,
      "grad_norm": 1.3462380170822144,
      "learning_rate": 0.00019991820227951157,
      "loss": 1.1233,
      "step": 8809
    },
    {
      "epoch": 1.238577252917194,
      "grad_norm": 1.4917097091674805,
      "learning_rate": 0.00019992733303673146,
      "loss": 1.0764,
      "step": 8810
    },
    {
      "epoch": 1.2387178405735977,
      "grad_norm": 1.4343390464782715,
      "learning_rate": 0.0001999359237662758,
      "loss": 1.179,
      "step": 8811
    },
    {
      "epoch": 1.2388584282300015,
      "grad_norm": 1.417841911315918,
      "learning_rate": 0.00019994397442171856,
      "loss": 1.1138,
      "step": 8812
    },
    {
      "epoch": 1.2389990158864053,
      "grad_norm": 1.3482872247695923,
      "learning_rate": 0.00019995148495955228,
      "loss": 1.2078,
      "step": 8813
    },
    {
      "epoch": 1.2391396035428088,
      "grad_norm": 1.8531399965286255,
      "learning_rate": 0.00019995845533918853,
      "loss": 1.041,
      "step": 8814
    },
    {
      "epoch": 1.2392801911992126,
      "grad_norm": 1.5510231256484985,
      "learning_rate": 0.00019996488552295795,
      "loss": 1.121,
      "step": 8815
    },
    {
      "epoch": 1.2394207788556164,
      "grad_norm": 1.6179907321929932,
      "learning_rate": 0.00019997077547611057,
      "loss": 1.1472,
      "step": 8816
    },
    {
      "epoch": 1.2395613665120202,
      "grad_norm": 1.6890829801559448,
      "learning_rate": 0.00019997612516681578,
      "loss": 1.0145,
      "step": 8817
    },
    {
      "epoch": 1.239701954168424,
      "grad_norm": 1.4449150562286377,
      "learning_rate": 0.0001999809345661628,
      "loss": 1.0409,
      "step": 8818
    },
    {
      "epoch": 1.2398425418248278,
      "grad_norm": 1.631192684173584,
      "learning_rate": 0.00019998520364816074,
      "loss": 0.9281,
      "step": 8819
    },
    {
      "epoch": 1.2399831294812316,
      "grad_norm": 1.6486408710479736,
      "learning_rate": 0.0001999889323897385,
      "loss": 1.1197,
      "step": 8820
    },
    {
      "epoch": 1.2401237171376354,
      "grad_norm": 1.5929811000823975,
      "learning_rate": 0.00019999212077074528,
      "loss": 1.1474,
      "step": 8821
    },
    {
      "epoch": 1.2402643047940392,
      "grad_norm": 1.4954066276550293,
      "learning_rate": 0.00019999476877395034,
      "loss": 1.0222,
      "step": 8822
    },
    {
      "epoch": 1.240404892450443,
      "grad_norm": 1.708186149597168,
      "learning_rate": 0.00019999687638504337,
      "loss": 1.0914,
      "step": 8823
    },
    {
      "epoch": 1.2405454801068467,
      "grad_norm": 1.638309121131897,
      "learning_rate": 0.00019999844359263445,
      "loss": 1.2142,
      "step": 8824
    },
    {
      "epoch": 1.2406860677632503,
      "grad_norm": 1.5593385696411133,
      "learning_rate": 0.000199999470388254,
      "loss": 1.2129,
      "step": 8825
    },
    {
      "epoch": 1.240826655419654,
      "grad_norm": 1.5418215990066528,
      "learning_rate": 0.00019999995676635304,
      "loss": 1.219,
      "step": 8826
    },
    {
      "epoch": 1.2409672430760579,
      "grad_norm": 1.6118425130844116,
      "learning_rate": 0.0001999999027243031,
      "loss": 1.1091,
      "step": 8827
    },
    {
      "epoch": 1.2411078307324617,
      "grad_norm": 2.006453037261963,
      "learning_rate": 0.0001999993082623962,
      "loss": 1.1647,
      "step": 8828
    },
    {
      "epoch": 1.2412484183888655,
      "grad_norm": 1.819624900817871,
      "learning_rate": 0.00019999817338384497,
      "loss": 0.9864,
      "step": 8829
    },
    {
      "epoch": 1.2413890060452692,
      "grad_norm": 1.7534576654434204,
      "learning_rate": 0.00019999649809478252,
      "loss": 1.0985,
      "step": 8830
    },
    {
      "epoch": 1.241529593701673,
      "grad_norm": 1.6684753894805908,
      "learning_rate": 0.00019999428240426238,
      "loss": 1.1551,
      "step": 8831
    },
    {
      "epoch": 1.2416701813580768,
      "grad_norm": 1.7312887907028198,
      "learning_rate": 0.0001999915263242587,
      "loss": 0.9697,
      "step": 8832
    },
    {
      "epoch": 1.2418107690144806,
      "grad_norm": 1.8143810033798218,
      "learning_rate": 0.00019998822986966585,
      "loss": 1.0923,
      "step": 8833
    },
    {
      "epoch": 1.2419513566708842,
      "grad_norm": 1.6657617092132568,
      "learning_rate": 0.00019998439305829856,
      "loss": 1.0387,
      "step": 8834
    },
    {
      "epoch": 1.242091944327288,
      "grad_norm": 1.6902797222137451,
      "learning_rate": 0.00019998001591089168,
      "loss": 1.0431,
      "step": 8835
    },
    {
      "epoch": 1.2422325319836918,
      "grad_norm": 1.5953820943832397,
      "learning_rate": 0.00019997509845110027,
      "loss": 1.1152,
      "step": 8836
    },
    {
      "epoch": 1.2423731196400956,
      "grad_norm": 1.5191895961761475,
      "learning_rate": 0.00019996964070549927,
      "loss": 0.9733,
      "step": 8837
    },
    {
      "epoch": 1.2425137072964993,
      "grad_norm": 1.764907717704773,
      "learning_rate": 0.00019996364270358346,
      "loss": 1.0286,
      "step": 8838
    },
    {
      "epoch": 1.2426542949529031,
      "grad_norm": 2.360064744949341,
      "learning_rate": 0.00019995710447776724,
      "loss": 0.9782,
      "step": 8839
    },
    {
      "epoch": 1.242794882609307,
      "grad_norm": 1.726930856704712,
      "learning_rate": 0.00019995002606338453,
      "loss": 0.9971,
      "step": 8840
    },
    {
      "epoch": 1.2429354702657107,
      "grad_norm": 1.6552038192749023,
      "learning_rate": 0.0001999424074986885,
      "loss": 1.0779,
      "step": 8841
    },
    {
      "epoch": 1.2430760579221145,
      "grad_norm": 1.6897141933441162,
      "learning_rate": 0.00019993424882485146,
      "loss": 1.1119,
      "step": 8842
    },
    {
      "epoch": 1.2432166455785183,
      "grad_norm": 1.7010161876678467,
      "learning_rate": 0.00019992555008596454,
      "loss": 1.1736,
      "step": 8843
    },
    {
      "epoch": 1.243357233234922,
      "grad_norm": 1.7201429605484009,
      "learning_rate": 0.00019991631132903746,
      "loss": 0.9788,
      "step": 8844
    },
    {
      "epoch": 1.2434978208913257,
      "grad_norm": 1.5267248153686523,
      "learning_rate": 0.00019990653260399844,
      "loss": 0.9739,
      "step": 8845
    },
    {
      "epoch": 1.2436384085477294,
      "grad_norm": 1.5924097299575806,
      "learning_rate": 0.0001998962139636936,
      "loss": 0.9997,
      "step": 8846
    },
    {
      "epoch": 1.2437789962041332,
      "grad_norm": 1.668634057044983,
      "learning_rate": 0.000199885355463887,
      "loss": 1.0359,
      "step": 8847
    },
    {
      "epoch": 1.243919583860537,
      "grad_norm": 1.6280121803283691,
      "learning_rate": 0.00019987395716326023,
      "loss": 1.1497,
      "step": 8848
    },
    {
      "epoch": 1.2440601715169408,
      "grad_norm": 1.6904282569885254,
      "learning_rate": 0.00019986201912341195,
      "loss": 1.1467,
      "step": 8849
    },
    {
      "epoch": 1.2442007591733446,
      "grad_norm": 1.4804261922836304,
      "learning_rate": 0.00019984954140885787,
      "loss": 1.0297,
      "step": 8850
    },
    {
      "epoch": 1.2443413468297484,
      "grad_norm": 1.748608946800232,
      "learning_rate": 0.00019983652408703004,
      "loss": 1.0984,
      "step": 8851
    },
    {
      "epoch": 1.2444819344861522,
      "grad_norm": 1.5730208158493042,
      "learning_rate": 0.0001998229672282767,
      "loss": 1.1626,
      "step": 8852
    },
    {
      "epoch": 1.244622522142556,
      "grad_norm": 1.7469900846481323,
      "learning_rate": 0.0001998088709058619,
      "loss": 1.2091,
      "step": 8853
    },
    {
      "epoch": 1.2447631097989595,
      "grad_norm": 1.9112447500228882,
      "learning_rate": 0.00019979423519596504,
      "loss": 1.0533,
      "step": 8854
    },
    {
      "epoch": 1.2449036974553633,
      "grad_norm": 1.4936583042144775,
      "learning_rate": 0.00019977906017768052,
      "loss": 1.1007,
      "step": 8855
    },
    {
      "epoch": 1.2450442851117671,
      "grad_norm": 1.8990849256515503,
      "learning_rate": 0.00019976334593301717,
      "loss": 0.9684,
      "step": 8856
    },
    {
      "epoch": 1.245184872768171,
      "grad_norm": 1.6921430826187134,
      "learning_rate": 0.00019974709254689797,
      "loss": 1.2605,
      "step": 8857
    },
    {
      "epoch": 1.2453254604245747,
      "grad_norm": 1.795911192893982,
      "learning_rate": 0.0001997303001071596,
      "loss": 1.2216,
      "step": 8858
    },
    {
      "epoch": 1.2454660480809785,
      "grad_norm": 1.6853060722351074,
      "learning_rate": 0.00019971296870455174,
      "loss": 0.9874,
      "step": 8859
    },
    {
      "epoch": 1.2456066357373823,
      "grad_norm": 1.7320584058761597,
      "learning_rate": 0.0001996950984327369,
      "loss": 0.9674,
      "step": 8860
    },
    {
      "epoch": 1.245747223393786,
      "grad_norm": 1.5115529298782349,
      "learning_rate": 0.00019967668938828962,
      "loss": 1.0995,
      "step": 8861
    },
    {
      "epoch": 1.2458878110501899,
      "grad_norm": 1.492476224899292,
      "learning_rate": 0.00019965774167069616,
      "loss": 0.9945,
      "step": 8862
    },
    {
      "epoch": 1.2460283987065937,
      "grad_norm": 1.637795090675354,
      "learning_rate": 0.00019963825538235383,
      "loss": 1.0658,
      "step": 8863
    },
    {
      "epoch": 1.2461689863629974,
      "grad_norm": 1.6301017999649048,
      "learning_rate": 0.00019961823062857045,
      "loss": 0.9344,
      "step": 8864
    },
    {
      "epoch": 1.246309574019401,
      "grad_norm": 1.536208987236023,
      "learning_rate": 0.00019959766751756402,
      "loss": 1.1018,
      "step": 8865
    },
    {
      "epoch": 1.2464501616758048,
      "grad_norm": 1.8306517601013184,
      "learning_rate": 0.00019957656616046164,
      "loss": 1.2113,
      "step": 8866
    },
    {
      "epoch": 1.2465907493322086,
      "grad_norm": 1.5177830457687378,
      "learning_rate": 0.00019955492667129944,
      "loss": 1.006,
      "step": 8867
    },
    {
      "epoch": 1.2467313369886124,
      "grad_norm": 1.7922754287719727,
      "learning_rate": 0.00019953274916702154,
      "loss": 1.0835,
      "step": 8868
    },
    {
      "epoch": 1.2468719246450162,
      "grad_norm": 1.6031750440597534,
      "learning_rate": 0.00019951003376747974,
      "loss": 1.2246,
      "step": 8869
    },
    {
      "epoch": 1.24701251230142,
      "grad_norm": 1.6049073934555054,
      "learning_rate": 0.00019948678059543271,
      "loss": 1.1434,
      "step": 8870
    },
    {
      "epoch": 1.2471530999578238,
      "grad_norm": 2.240219831466675,
      "learning_rate": 0.0001994629897765453,
      "loss": 1.0614,
      "step": 8871
    },
    {
      "epoch": 1.2472936876142275,
      "grad_norm": 1.4768660068511963,
      "learning_rate": 0.00019943866143938796,
      "loss": 1.1799,
      "step": 8872
    },
    {
      "epoch": 1.2474342752706313,
      "grad_norm": 1.6697365045547485,
      "learning_rate": 0.00019941379571543596,
      "loss": 1.1451,
      "step": 8873
    },
    {
      "epoch": 1.247574862927035,
      "grad_norm": 1.8739639520645142,
      "learning_rate": 0.00019938839273906877,
      "loss": 1.0867,
      "step": 8874
    },
    {
      "epoch": 1.2477154505834387,
      "grad_norm": 1.973827600479126,
      "learning_rate": 0.00019936245264756924,
      "loss": 1.019,
      "step": 8875
    },
    {
      "epoch": 1.2478560382398425,
      "grad_norm": 1.572178840637207,
      "learning_rate": 0.00019933597558112294,
      "loss": 1.1037,
      "step": 8876
    },
    {
      "epoch": 1.2479966258962463,
      "grad_norm": 1.5409736633300781,
      "learning_rate": 0.0001993089616828173,
      "loss": 1.1524,
      "step": 8877
    },
    {
      "epoch": 1.24813721355265,
      "grad_norm": 1.5501629114151,
      "learning_rate": 0.00019928141109864093,
      "loss": 0.9572,
      "step": 8878
    },
    {
      "epoch": 1.2482778012090538,
      "grad_norm": 2.1066555976867676,
      "learning_rate": 0.00019925332397748277,
      "loss": 1.2457,
      "step": 8879
    },
    {
      "epoch": 1.2484183888654576,
      "grad_norm": 1.5930047035217285,
      "learning_rate": 0.0001992247004711314,
      "loss": 1.0645,
      "step": 8880
    },
    {
      "epoch": 1.2485589765218614,
      "grad_norm": 1.543533444404602,
      "learning_rate": 0.00019919554073427408,
      "loss": 1.1898,
      "step": 8881
    },
    {
      "epoch": 1.2486995641782652,
      "grad_norm": 1.6574325561523438,
      "learning_rate": 0.00019916584492449602,
      "loss": 1.0811,
      "step": 8882
    },
    {
      "epoch": 1.248840151834669,
      "grad_norm": 1.6381242275238037,
      "learning_rate": 0.00019913561320227934,
      "loss": 1.1389,
      "step": 8883
    },
    {
      "epoch": 1.2489807394910728,
      "grad_norm": 1.6729787588119507,
      "learning_rate": 0.00019910484573100242,
      "loss": 1.1774,
      "step": 8884
    },
    {
      "epoch": 1.2491213271474764,
      "grad_norm": 1.5626262426376343,
      "learning_rate": 0.000199073542676939,
      "loss": 1.1956,
      "step": 8885
    },
    {
      "epoch": 1.2492619148038802,
      "grad_norm": 1.5403976440429688,
      "learning_rate": 0.00019904170420925719,
      "loss": 1.0158,
      "step": 8886
    },
    {
      "epoch": 1.249402502460284,
      "grad_norm": 1.7616695165634155,
      "learning_rate": 0.00019900933050001843,
      "loss": 1.0784,
      "step": 8887
    },
    {
      "epoch": 1.2495430901166877,
      "grad_norm": 1.885807752609253,
      "learning_rate": 0.00019897642172417705,
      "loss": 1.0671,
      "step": 8888
    },
    {
      "epoch": 1.2496836777730915,
      "grad_norm": 1.4126663208007812,
      "learning_rate": 0.0001989429780595786,
      "loss": 1.1699,
      "step": 8889
    },
    {
      "epoch": 1.2498242654294953,
      "grad_norm": 1.3916336297988892,
      "learning_rate": 0.00019890899968695954,
      "loss": 1.0844,
      "step": 8890
    },
    {
      "epoch": 1.249964853085899,
      "grad_norm": 1.536743402481079,
      "learning_rate": 0.0001988744867899459,
      "loss": 1.0783,
      "step": 8891
    },
    {
      "epoch": 1.250105440742303,
      "grad_norm": 1.4876354932785034,
      "learning_rate": 0.0001988394395550524,
      "loss": 1.2489,
      "step": 8892
    },
    {
      "epoch": 1.2502460283987067,
      "grad_norm": 1.4994319677352905,
      "learning_rate": 0.00019880385817168153,
      "loss": 1.1145,
      "step": 8893
    },
    {
      "epoch": 1.2503866160551103,
      "grad_norm": 1.4535565376281738,
      "learning_rate": 0.00019876774283212223,
      "loss": 1.2813,
      "step": 8894
    },
    {
      "epoch": 1.250527203711514,
      "grad_norm": 1.374750018119812,
      "learning_rate": 0.0001987310937315491,
      "loss": 1.2574,
      "step": 8895
    },
    {
      "epoch": 1.2506677913679178,
      "grad_norm": 1.4612637758255005,
      "learning_rate": 0.0001986939110680215,
      "loss": 1.1278,
      "step": 8896
    },
    {
      "epoch": 1.2508083790243216,
      "grad_norm": 1.6008418798446655,
      "learning_rate": 0.00019865619504248207,
      "loss": 1.2668,
      "step": 8897
    },
    {
      "epoch": 1.2509489666807254,
      "grad_norm": 1.7545514106750488,
      "learning_rate": 0.00019861794585875598,
      "loss": 1.0712,
      "step": 8898
    },
    {
      "epoch": 1.2510895543371292,
      "grad_norm": 1.733002781867981,
      "learning_rate": 0.00019857916372354944,
      "loss": 1.1355,
      "step": 8899
    },
    {
      "epoch": 1.251230141993533,
      "grad_norm": 1.8852620124816895,
      "learning_rate": 0.00019853984884644912,
      "loss": 1.0919,
      "step": 8900
    },
    {
      "epoch": 1.2513707296499368,
      "grad_norm": 1.724172592163086,
      "learning_rate": 0.0001985000014399206,
      "loss": 1.1357,
      "step": 8901
    },
    {
      "epoch": 1.2515113173063406,
      "grad_norm": 1.5770353078842163,
      "learning_rate": 0.00019845962171930734,
      "loss": 1.1677,
      "step": 8902
    },
    {
      "epoch": 1.2516519049627441,
      "grad_norm": 1.6057850122451782,
      "learning_rate": 0.0001984187099028297,
      "loss": 1.0439,
      "step": 8903
    },
    {
      "epoch": 1.2517924926191482,
      "grad_norm": 1.592936396598816,
      "learning_rate": 0.00019837726621158347,
      "loss": 1.1283,
      "step": 8904
    },
    {
      "epoch": 1.2519330802755517,
      "grad_norm": 1.954767107963562,
      "learning_rate": 0.0001983352908695387,
      "loss": 1.0423,
      "step": 8905
    },
    {
      "epoch": 1.2520736679319555,
      "grad_norm": 1.6344071626663208,
      "learning_rate": 0.00019829278410353875,
      "loss": 1.0494,
      "step": 8906
    },
    {
      "epoch": 1.2522142555883593,
      "grad_norm": 1.5126926898956299,
      "learning_rate": 0.00019824974614329884,
      "loss": 1.1505,
      "step": 8907
    },
    {
      "epoch": 1.252354843244763,
      "grad_norm": 1.5085692405700684,
      "learning_rate": 0.00019820617722140488,
      "loss": 1.2122,
      "step": 8908
    },
    {
      "epoch": 1.2524954309011669,
      "grad_norm": 1.913108229637146,
      "learning_rate": 0.0001981620775733122,
      "loss": 0.9876,
      "step": 8909
    },
    {
      "epoch": 1.2526360185575707,
      "grad_norm": 1.7954163551330566,
      "learning_rate": 0.00019811744743734427,
      "loss": 1.2543,
      "step": 8910
    },
    {
      "epoch": 1.2527766062139745,
      "grad_norm": 1.7854970693588257,
      "learning_rate": 0.00019807228705469147,
      "loss": 1.0797,
      "step": 8911
    },
    {
      "epoch": 1.2529171938703783,
      "grad_norm": 1.668029546737671,
      "learning_rate": 0.0001980265966694097,
      "loss": 1.15,
      "step": 8912
    },
    {
      "epoch": 1.253057781526782,
      "grad_norm": 1.8132100105285645,
      "learning_rate": 0.00019798037652841912,
      "loss": 1.0954,
      "step": 8913
    },
    {
      "epoch": 1.2531983691831856,
      "grad_norm": 1.7651753425598145,
      "learning_rate": 0.0001979336268815028,
      "loss": 1.1805,
      "step": 8914
    },
    {
      "epoch": 1.2533389568395894,
      "grad_norm": 1.587376594543457,
      "learning_rate": 0.00019788634798130546,
      "loss": 1.1071,
      "step": 8915
    },
    {
      "epoch": 1.2534795444959932,
      "grad_norm": 1.7518457174301147,
      "learning_rate": 0.00019783854008333172,
      "loss": 1.3864,
      "step": 8916
    },
    {
      "epoch": 1.253620132152397,
      "grad_norm": 1.690346360206604,
      "learning_rate": 0.0001977902034459452,
      "loss": 1.1177,
      "step": 8917
    },
    {
      "epoch": 1.2537607198088008,
      "grad_norm": 1.6849128007888794,
      "learning_rate": 0.0001977413383303671,
      "loss": 1.0636,
      "step": 8918
    },
    {
      "epoch": 1.2539013074652046,
      "grad_norm": 1.6790851354599,
      "learning_rate": 0.0001976919450006744,
      "loss": 1.0043,
      "step": 8919
    },
    {
      "epoch": 1.2540418951216084,
      "grad_norm": 1.6340261697769165,
      "learning_rate": 0.00019764202372379882,
      "loss": 1.0847,
      "step": 8920
    },
    {
      "epoch": 1.2541824827780121,
      "grad_norm": 1.7296501398086548,
      "learning_rate": 0.00019759157476952494,
      "loss": 0.9492,
      "step": 8921
    },
    {
      "epoch": 1.254323070434416,
      "grad_norm": 1.488664150238037,
      "learning_rate": 0.00019754059841048925,
      "loss": 1.2255,
      "step": 8922
    },
    {
      "epoch": 1.2544636580908195,
      "grad_norm": 1.5848923921585083,
      "learning_rate": 0.0001974890949221784,
      "loss": 1.1874,
      "step": 8923
    },
    {
      "epoch": 1.2546042457472235,
      "grad_norm": 1.6242353916168213,
      "learning_rate": 0.00019743706458292775,
      "loss": 1.0291,
      "step": 8924
    },
    {
      "epoch": 1.254744833403627,
      "grad_norm": 1.6627331972122192,
      "learning_rate": 0.00019738450767391985,
      "loss": 1.0448,
      "step": 8925
    },
    {
      "epoch": 1.2548854210600309,
      "grad_norm": 1.785285234451294,
      "learning_rate": 0.00019733142447918295,
      "loss": 1.091,
      "step": 8926
    },
    {
      "epoch": 1.2550260087164347,
      "grad_norm": 1.504705548286438,
      "learning_rate": 0.0001972778152855894,
      "loss": 1.0727,
      "step": 8927
    },
    {
      "epoch": 1.2551665963728385,
      "grad_norm": 1.5012692213058472,
      "learning_rate": 0.0001972236803828543,
      "loss": 1.1366,
      "step": 8928
    },
    {
      "epoch": 1.2553071840292422,
      "grad_norm": 1.6498301029205322,
      "learning_rate": 0.00019716902006353365,
      "loss": 1.0977,
      "step": 8929
    },
    {
      "epoch": 1.255447771685646,
      "grad_norm": 1.6072860956192017,
      "learning_rate": 0.000197113834623023,
      "loss": 1.0105,
      "step": 8930
    },
    {
      "epoch": 1.2555883593420498,
      "grad_norm": 1.7866731882095337,
      "learning_rate": 0.00019705812435955573,
      "loss": 1.1285,
      "step": 8931
    },
    {
      "epoch": 1.2557289469984536,
      "grad_norm": 1.7285078763961792,
      "learning_rate": 0.00019700188957420136,
      "loss": 1.022,
      "step": 8932
    },
    {
      "epoch": 1.2558695346548574,
      "grad_norm": 1.4999333620071411,
      "learning_rate": 0.0001969451305708641,
      "loss": 1.1972,
      "step": 8933
    },
    {
      "epoch": 1.256010122311261,
      "grad_norm": 1.9010742902755737,
      "learning_rate": 0.0001968878476562813,
      "loss": 1.1321,
      "step": 8934
    },
    {
      "epoch": 1.2561507099676648,
      "grad_norm": 1.5136919021606445,
      "learning_rate": 0.0001968300411400215,
      "loss": 1.0162,
      "step": 8935
    },
    {
      "epoch": 1.2562912976240685,
      "grad_norm": 1.6054003238677979,
      "learning_rate": 0.00019677171133448292,
      "loss": 0.994,
      "step": 8936
    },
    {
      "epoch": 1.2564318852804723,
      "grad_norm": 1.3840405941009521,
      "learning_rate": 0.0001967128585548916,
      "loss": 1.128,
      "step": 8937
    },
    {
      "epoch": 1.2565724729368761,
      "grad_norm": 1.60672128200531,
      "learning_rate": 0.00019665348311930002,
      "loss": 1.1543,
      "step": 8938
    },
    {
      "epoch": 1.25671306059328,
      "grad_norm": 1.5110177993774414,
      "learning_rate": 0.00019659358534858512,
      "loss": 1.1958,
      "step": 8939
    },
    {
      "epoch": 1.2568536482496837,
      "grad_norm": 1.6524702310562134,
      "learning_rate": 0.00019653316556644658,
      "loss": 1.1033,
      "step": 8940
    },
    {
      "epoch": 1.2569942359060875,
      "grad_norm": 1.417075753211975,
      "learning_rate": 0.00019647222409940548,
      "loss": 1.1047,
      "step": 8941
    },
    {
      "epoch": 1.2571348235624913,
      "grad_norm": 1.6225833892822266,
      "learning_rate": 0.00019641076127680171,
      "loss": 0.8423,
      "step": 8942
    },
    {
      "epoch": 1.2572754112188949,
      "grad_norm": 2.17529559135437,
      "learning_rate": 0.00019634877743079292,
      "loss": 1.1501,
      "step": 8943
    },
    {
      "epoch": 1.2574159988752989,
      "grad_norm": 1.636996865272522,
      "learning_rate": 0.00019628627289635246,
      "loss": 1.1832,
      "step": 8944
    },
    {
      "epoch": 1.2575565865317024,
      "grad_norm": 1.5845105648040771,
      "learning_rate": 0.0001962232480112676,
      "loss": 1.0707,
      "step": 8945
    },
    {
      "epoch": 1.2576971741881062,
      "grad_norm": 1.580304741859436,
      "learning_rate": 0.00019615970311613764,
      "loss": 1.0334,
      "step": 8946
    },
    {
      "epoch": 1.25783776184451,
      "grad_norm": 1.5445436239242554,
      "learning_rate": 0.00019609563855437222,
      "loss": 1.212,
      "step": 8947
    },
    {
      "epoch": 1.2579783495009138,
      "grad_norm": 1.802253246307373,
      "learning_rate": 0.00019603105467218908,
      "loss": 0.9988,
      "step": 8948
    },
    {
      "epoch": 1.2581189371573176,
      "grad_norm": 1.8967424631118774,
      "learning_rate": 0.0001959659518186129,
      "loss": 1.2046,
      "step": 8949
    },
    {
      "epoch": 1.2582595248137214,
      "grad_norm": 1.623604655265808,
      "learning_rate": 0.00019590033034547273,
      "loss": 1.2112,
      "step": 8950
    },
    {
      "epoch": 1.2584001124701252,
      "grad_norm": 1.8500906229019165,
      "learning_rate": 0.00019583419060740034,
      "loss": 1.1254,
      "step": 8951
    },
    {
      "epoch": 1.258540700126529,
      "grad_norm": 1.5405915975570679,
      "learning_rate": 0.00019576753296182844,
      "loss": 1.0436,
      "step": 8952
    },
    {
      "epoch": 1.2586812877829328,
      "grad_norm": 1.8705570697784424,
      "learning_rate": 0.00019570035776898839,
      "loss": 1.1099,
      "step": 8953
    },
    {
      "epoch": 1.2588218754393363,
      "grad_norm": 2.368288993835449,
      "learning_rate": 0.00019563266539190867,
      "loss": 1.1719,
      "step": 8954
    },
    {
      "epoch": 1.2589624630957401,
      "grad_norm": 1.618090271949768,
      "learning_rate": 0.00019556445619641277,
      "loss": 1.1841,
      "step": 8955
    },
    {
      "epoch": 1.259103050752144,
      "grad_norm": 1.7404502630233765,
      "learning_rate": 0.00019549573055111687,
      "loss": 1.0884,
      "step": 8956
    },
    {
      "epoch": 1.2592436384085477,
      "grad_norm": 1.6160500049591064,
      "learning_rate": 0.00019542648882742863,
      "loss": 1.2258,
      "step": 8957
    },
    {
      "epoch": 1.2593842260649515,
      "grad_norm": 1.5989691019058228,
      "learning_rate": 0.00019535673139954444,
      "loss": 1.0978,
      "step": 8958
    },
    {
      "epoch": 1.2595248137213553,
      "grad_norm": 1.6217190027236938,
      "learning_rate": 0.00019528645864444745,
      "loss": 1.1072,
      "step": 8959
    },
    {
      "epoch": 1.259665401377759,
      "grad_norm": 1.6472227573394775,
      "learning_rate": 0.00019521567094190606,
      "loss": 1.0431,
      "step": 8960
    },
    {
      "epoch": 1.2598059890341629,
      "grad_norm": 1.5562628507614136,
      "learning_rate": 0.00019514436867447142,
      "loss": 1.3084,
      "step": 8961
    },
    {
      "epoch": 1.2599465766905666,
      "grad_norm": 1.3458030223846436,
      "learning_rate": 0.00019507255222747554,
      "loss": 1.0783,
      "step": 8962
    },
    {
      "epoch": 1.2600871643469702,
      "grad_norm": 2.101451873779297,
      "learning_rate": 0.0001950002219890291,
      "loss": 1.0299,
      "step": 8963
    },
    {
      "epoch": 1.2602277520033742,
      "grad_norm": 1.8765223026275635,
      "learning_rate": 0.00019492737835001949,
      "loss": 1.2167,
      "step": 8964
    },
    {
      "epoch": 1.2603683396597778,
      "grad_norm": 1.9794631004333496,
      "learning_rate": 0.0001948540217041086,
      "loss": 1.1229,
      "step": 8965
    },
    {
      "epoch": 1.2605089273161816,
      "grad_norm": 1.861281394958496,
      "learning_rate": 0.00019478015244773063,
      "loss": 1.0827,
      "step": 8966
    },
    {
      "epoch": 1.2606495149725854,
      "grad_norm": 1.674946665763855,
      "learning_rate": 0.00019470577098009014,
      "loss": 1.0402,
      "step": 8967
    },
    {
      "epoch": 1.2607901026289892,
      "grad_norm": 1.4741647243499756,
      "learning_rate": 0.00019463087770315985,
      "loss": 1.1032,
      "step": 8968
    },
    {
      "epoch": 1.260930690285393,
      "grad_norm": 1.6023104190826416,
      "learning_rate": 0.0001945554730216781,
      "loss": 1.1802,
      "step": 8969
    },
    {
      "epoch": 1.2610712779417967,
      "grad_norm": 2.0788638591766357,
      "learning_rate": 0.00019447955734314723,
      "loss": 1.1369,
      "step": 8970
    },
    {
      "epoch": 1.2612118655982005,
      "grad_norm": 1.6226582527160645,
      "learning_rate": 0.0001944031310778309,
      "loss": 1.1898,
      "step": 8971
    },
    {
      "epoch": 1.2613524532546043,
      "grad_norm": 1.7946728467941284,
      "learning_rate": 0.0001943261946387525,
      "loss": 1.1105,
      "step": 8972
    },
    {
      "epoch": 1.2614930409110081,
      "grad_norm": 1.9407762289047241,
      "learning_rate": 0.00019424874844169211,
      "loss": 0.9315,
      "step": 8973
    },
    {
      "epoch": 1.2616336285674117,
      "grad_norm": 1.4426469802856445,
      "learning_rate": 0.0001941707929051849,
      "loss": 1.0992,
      "step": 8974
    },
    {
      "epoch": 1.2617742162238155,
      "grad_norm": 1.5269453525543213,
      "learning_rate": 0.0001940923284505182,
      "loss": 1.0158,
      "step": 8975
    },
    {
      "epoch": 1.2619148038802193,
      "grad_norm": 1.8823285102844238,
      "learning_rate": 0.00019401335550173006,
      "loss": 0.9641,
      "step": 8976
    },
    {
      "epoch": 1.262055391536623,
      "grad_norm": 1.4318100214004517,
      "learning_rate": 0.00019393387448560638,
      "loss": 1.1512,
      "step": 8977
    },
    {
      "epoch": 1.2621959791930268,
      "grad_norm": 1.8196361064910889,
      "learning_rate": 0.00019385388583167864,
      "loss": 1.1713,
      "step": 8978
    },
    {
      "epoch": 1.2623365668494306,
      "grad_norm": 1.6735478639602661,
      "learning_rate": 0.00019377338997222224,
      "loss": 1.137,
      "step": 8979
    },
    {
      "epoch": 1.2624771545058344,
      "grad_norm": 1.4413537979125977,
      "learning_rate": 0.0001936923873422528,
      "loss": 1.182,
      "step": 8980
    },
    {
      "epoch": 1.2626177421622382,
      "grad_norm": 1.9011050462722778,
      "learning_rate": 0.0001936108783795252,
      "loss": 1.005,
      "step": 8981
    },
    {
      "epoch": 1.262758329818642,
      "grad_norm": 1.6119846105575562,
      "learning_rate": 0.00019352886352453044,
      "loss": 1.0541,
      "step": 8982
    },
    {
      "epoch": 1.2628989174750456,
      "grad_norm": 1.5916539430618286,
      "learning_rate": 0.00019344634322049356,
      "loss": 1.0389,
      "step": 8983
    },
    {
      "epoch": 1.2630395051314496,
      "grad_norm": 1.6251658201217651,
      "learning_rate": 0.00019336331791337108,
      "loss": 1.1262,
      "step": 8984
    },
    {
      "epoch": 1.2631800927878531,
      "grad_norm": 1.5726834535598755,
      "learning_rate": 0.0001932797880518488,
      "loss": 1.1236,
      "step": 8985
    },
    {
      "epoch": 1.263320680444257,
      "grad_norm": 1.696287989616394,
      "learning_rate": 0.00019319575408733875,
      "loss": 1.1269,
      "step": 8986
    },
    {
      "epoch": 1.2634612681006607,
      "grad_norm": 1.7500778436660767,
      "learning_rate": 0.00019311121647397803,
      "loss": 1.0753,
      "step": 8987
    },
    {
      "epoch": 1.2636018557570645,
      "grad_norm": 1.6941149234771729,
      "learning_rate": 0.00019302617566862492,
      "loss": 1.0993,
      "step": 8988
    },
    {
      "epoch": 1.2637424434134683,
      "grad_norm": 1.761379361152649,
      "learning_rate": 0.00019294063213085732,
      "loss": 1.0841,
      "step": 8989
    },
    {
      "epoch": 1.263883031069872,
      "grad_norm": 1.739405870437622,
      "learning_rate": 0.00019285458632297005,
      "loss": 1.0585,
      "step": 8990
    },
    {
      "epoch": 1.2640236187262759,
      "grad_norm": 1.6853958368301392,
      "learning_rate": 0.00019276803870997195,
      "loss": 0.9972,
      "step": 8991
    },
    {
      "epoch": 1.2641642063826797,
      "grad_norm": 1.9064674377441406,
      "learning_rate": 0.00019268098975958408,
      "loss": 1.127,
      "step": 8992
    },
    {
      "epoch": 1.2643047940390835,
      "grad_norm": 1.9857524633407593,
      "learning_rate": 0.00019259343994223677,
      "loss": 1.0087,
      "step": 8993
    },
    {
      "epoch": 1.264445381695487,
      "grad_norm": 1.9272979497909546,
      "learning_rate": 0.00019250538973106682,
      "loss": 1.271,
      "step": 8994
    },
    {
      "epoch": 1.2645859693518908,
      "grad_norm": 1.4360568523406982,
      "learning_rate": 0.000192416839601916,
      "loss": 0.8453,
      "step": 8995
    },
    {
      "epoch": 1.2647265570082946,
      "grad_norm": 1.492724061012268,
      "learning_rate": 0.00019232779003332688,
      "loss": 1.1265,
      "step": 8996
    },
    {
      "epoch": 1.2648671446646984,
      "grad_norm": 2.179696559906006,
      "learning_rate": 0.00019223824150654158,
      "loss": 1.1366,
      "step": 8997
    },
    {
      "epoch": 1.2650077323211022,
      "grad_norm": 2.0924103260040283,
      "learning_rate": 0.00019214819450549854,
      "loss": 1.157,
      "step": 8998
    },
    {
      "epoch": 1.265148319977506,
      "grad_norm": 1.7041219472885132,
      "learning_rate": 0.00019205764951683022,
      "loss": 0.9177,
      "step": 8999
    },
    {
      "epoch": 1.2652889076339098,
      "grad_norm": 2.149168014526367,
      "learning_rate": 0.00019196660702986012,
      "loss": 1.1771,
      "step": 9000
    },
    {
      "epoch": 1.2652889076339098,
      "eval_loss": 1.1726939678192139,
      "eval_runtime": 772.0107,
      "eval_samples_per_second": 16.381,
      "eval_steps_per_second": 8.19,
      "step": 9000
    },
    {
      "epoch": 1.2654294952903136,
      "grad_norm": 1.6089705228805542,
      "learning_rate": 0.00019187506753660038,
      "loss": 1.4411,
      "step": 9001
    },
    {
      "epoch": 1.2655700829467174,
      "grad_norm": 1.725893259048462,
      "learning_rate": 0.0001917830315317491,
      "loss": 1.0062,
      "step": 9002
    },
    {
      "epoch": 1.265710670603121,
      "grad_norm": 1.785840630531311,
      "learning_rate": 0.00019169049951268764,
      "loss": 0.9766,
      "step": 9003
    },
    {
      "epoch": 1.265851258259525,
      "grad_norm": 1.7071647644042969,
      "learning_rate": 0.00019159747197947788,
      "loss": 0.9398,
      "step": 9004
    },
    {
      "epoch": 1.2659918459159285,
      "grad_norm": 1.892416000366211,
      "learning_rate": 0.00019150394943485955,
      "loss": 1.1777,
      "step": 9005
    },
    {
      "epoch": 1.2661324335723323,
      "grad_norm": 1.8232817649841309,
      "learning_rate": 0.0001914099323842477,
      "loss": 1.1477,
      "step": 9006
    },
    {
      "epoch": 1.266273021228736,
      "grad_norm": 1.4498577117919922,
      "learning_rate": 0.00019131542133572933,
      "loss": 1.0552,
      "step": 9007
    },
    {
      "epoch": 1.2664136088851399,
      "grad_norm": 1.7045670747756958,
      "learning_rate": 0.00019122041680006156,
      "loss": 1.0653,
      "step": 9008
    },
    {
      "epoch": 1.2665541965415437,
      "grad_norm": 1.5866096019744873,
      "learning_rate": 0.00019112491929066815,
      "loss": 1.1668,
      "step": 9009
    },
    {
      "epoch": 1.2666947841979475,
      "grad_norm": 1.771854281425476,
      "learning_rate": 0.00019102892932363737,
      "loss": 1.0815,
      "step": 9010
    },
    {
      "epoch": 1.2668353718543512,
      "grad_norm": 1.6402126550674438,
      "learning_rate": 0.0001909324474177186,
      "loss": 1.0476,
      "step": 9011
    },
    {
      "epoch": 1.266975959510755,
      "grad_norm": 1.614281177520752,
      "learning_rate": 0.00019083547409431937,
      "loss": 0.9813,
      "step": 9012
    },
    {
      "epoch": 1.2671165471671588,
      "grad_norm": 1.4798946380615234,
      "learning_rate": 0.00019073800987750347,
      "loss": 1.0283,
      "step": 9013
    },
    {
      "epoch": 1.2672571348235624,
      "grad_norm": 1.8680020570755005,
      "learning_rate": 0.0001906400552939874,
      "loss": 1.0729,
      "step": 9014
    },
    {
      "epoch": 1.2673977224799662,
      "grad_norm": 1.6200225353240967,
      "learning_rate": 0.0001905416108731377,
      "loss": 1.0126,
      "step": 9015
    },
    {
      "epoch": 1.26753831013637,
      "grad_norm": 1.6545116901397705,
      "learning_rate": 0.0001904426771469679,
      "loss": 1.0609,
      "step": 9016
    },
    {
      "epoch": 1.2676788977927738,
      "grad_norm": 1.4386752843856812,
      "learning_rate": 0.0001903432546501365,
      "loss": 1.1352,
      "step": 9017
    },
    {
      "epoch": 1.2678194854491776,
      "grad_norm": 1.6993952989578247,
      "learning_rate": 0.00019024334391994248,
      "loss": 1.0139,
      "step": 9018
    },
    {
      "epoch": 1.2679600731055813,
      "grad_norm": 1.6853634119033813,
      "learning_rate": 0.00019014294549632394,
      "loss": 1.061,
      "step": 9019
    },
    {
      "epoch": 1.2681006607619851,
      "grad_norm": 1.5226802825927734,
      "learning_rate": 0.00019004205992185442,
      "loss": 1.0856,
      "step": 9020
    },
    {
      "epoch": 1.268241248418389,
      "grad_norm": 1.660999059677124,
      "learning_rate": 0.00018994068774174015,
      "loss": 1.1949,
      "step": 9021
    },
    {
      "epoch": 1.2683818360747927,
      "grad_norm": 1.398498773574829,
      "learning_rate": 0.00018983882950381718,
      "loss": 0.9186,
      "step": 9022
    },
    {
      "epoch": 1.2685224237311963,
      "grad_norm": 1.6285406351089478,
      "learning_rate": 0.0001897364857585479,
      "loss": 1.05,
      "step": 9023
    },
    {
      "epoch": 1.2686630113876003,
      "grad_norm": 1.796517014503479,
      "learning_rate": 0.00018963365705901874,
      "loss": 1.0341,
      "step": 9024
    },
    {
      "epoch": 1.2688035990440039,
      "grad_norm": 1.4663856029510498,
      "learning_rate": 0.0001895303439609373,
      "loss": 1.1284,
      "step": 9025
    },
    {
      "epoch": 1.2689441867004077,
      "grad_norm": 1.7282633781433105,
      "learning_rate": 0.00018942654702262845,
      "loss": 1.1013,
      "step": 9026
    },
    {
      "epoch": 1.2690847743568114,
      "grad_norm": 1.6868442296981812,
      "learning_rate": 0.00018932226680503197,
      "loss": 1.1138,
      "step": 9027
    },
    {
      "epoch": 1.2692253620132152,
      "grad_norm": 1.6348527669906616,
      "learning_rate": 0.00018921750387169964,
      "loss": 1.0627,
      "step": 9028
    },
    {
      "epoch": 1.269365949669619,
      "grad_norm": 1.888849139213562,
      "learning_rate": 0.00018911225878879133,
      "loss": 1.1455,
      "step": 9029
    },
    {
      "epoch": 1.2695065373260228,
      "grad_norm": 1.4633069038391113,
      "learning_rate": 0.0001890065321250731,
      "loss": 1.0026,
      "step": 9030
    },
    {
      "epoch": 1.2696471249824266,
      "grad_norm": 1.6980609893798828,
      "learning_rate": 0.00018890032445191337,
      "loss": 0.9974,
      "step": 9031
    },
    {
      "epoch": 1.2697877126388304,
      "grad_norm": 1.6007238626480103,
      "learning_rate": 0.00018879363634327992,
      "loss": 0.9797,
      "step": 9032
    },
    {
      "epoch": 1.2699283002952342,
      "grad_norm": 1.9757797718048096,
      "learning_rate": 0.0001886864683757375,
      "loss": 0.9987,
      "step": 9033
    },
    {
      "epoch": 1.2700688879516377,
      "grad_norm": 1.6686592102050781,
      "learning_rate": 0.0001885788211284432,
      "loss": 1.2523,
      "step": 9034
    },
    {
      "epoch": 1.2702094756080415,
      "grad_norm": 1.565439224243164,
      "learning_rate": 0.00018847069518314476,
      "loss": 1.0747,
      "step": 9035
    },
    {
      "epoch": 1.2703500632644453,
      "grad_norm": 1.8278005123138428,
      "learning_rate": 0.00018836209112417694,
      "loss": 1.1838,
      "step": 9036
    },
    {
      "epoch": 1.2704906509208491,
      "grad_norm": 1.5585837364196777,
      "learning_rate": 0.00018825300953845815,
      "loss": 1.2137,
      "step": 9037
    },
    {
      "epoch": 1.270631238577253,
      "grad_norm": 1.8121451139450073,
      "learning_rate": 0.00018814345101548767,
      "loss": 1.1797,
      "step": 9038
    },
    {
      "epoch": 1.2707718262336567,
      "grad_norm": 1.5619386434555054,
      "learning_rate": 0.00018803341614734162,
      "loss": 1.0276,
      "step": 9039
    },
    {
      "epoch": 1.2709124138900605,
      "grad_norm": 1.6786249876022339,
      "learning_rate": 0.00018792290552867137,
      "loss": 1.0512,
      "step": 9040
    },
    {
      "epoch": 1.2710530015464643,
      "grad_norm": 1.4922959804534912,
      "learning_rate": 0.00018781191975669859,
      "loss": 1.2208,
      "step": 9041
    },
    {
      "epoch": 1.271193589202868,
      "grad_norm": 1.5441993474960327,
      "learning_rate": 0.00018770045943121305,
      "loss": 1.1266,
      "step": 9042
    },
    {
      "epoch": 1.2713341768592716,
      "grad_norm": 1.9017871618270874,
      "learning_rate": 0.00018758852515456912,
      "loss": 1.1452,
      "step": 9043
    },
    {
      "epoch": 1.2714747645156756,
      "grad_norm": 1.7107470035552979,
      "learning_rate": 0.00018747611753168255,
      "loss": 1.0155,
      "step": 9044
    },
    {
      "epoch": 1.2716153521720792,
      "grad_norm": 1.5273913145065308,
      "learning_rate": 0.00018736323717002672,
      "loss": 1.1074,
      "step": 9045
    },
    {
      "epoch": 1.271755939828483,
      "grad_norm": 1.6214001178741455,
      "learning_rate": 0.00018724988467963026,
      "loss": 1.1002,
      "step": 9046
    },
    {
      "epoch": 1.2718965274848868,
      "grad_norm": 1.8141028881072998,
      "learning_rate": 0.00018713606067307303,
      "loss": 0.9356,
      "step": 9047
    },
    {
      "epoch": 1.2720371151412906,
      "grad_norm": 1.647448182106018,
      "learning_rate": 0.0001870217657654835,
      "loss": 0.9973,
      "step": 9048
    },
    {
      "epoch": 1.2721777027976944,
      "grad_norm": 1.519750952720642,
      "learning_rate": 0.00018690700057453455,
      "loss": 1.0771,
      "step": 9049
    },
    {
      "epoch": 1.2723182904540982,
      "grad_norm": 1.6537381410598755,
      "learning_rate": 0.0001867917657204404,
      "loss": 1.0726,
      "step": 9050
    },
    {
      "epoch": 1.272458878110502,
      "grad_norm": 1.5540226697921753,
      "learning_rate": 0.00018667606182595385,
      "loss": 1.1597,
      "step": 9051
    },
    {
      "epoch": 1.2725994657669057,
      "grad_norm": 1.524308443069458,
      "learning_rate": 0.00018655988951636233,
      "loss": 1.09,
      "step": 9052
    },
    {
      "epoch": 1.2727400534233095,
      "grad_norm": 1.5936309099197388,
      "learning_rate": 0.0001864432494194847,
      "loss": 1.166,
      "step": 9053
    },
    {
      "epoch": 1.272880641079713,
      "grad_norm": 2.006094455718994,
      "learning_rate": 0.00018632614216566772,
      "loss": 0.9762,
      "step": 9054
    },
    {
      "epoch": 1.273021228736117,
      "grad_norm": 1.555999517440796,
      "learning_rate": 0.00018620856838778296,
      "loss": 1.204,
      "step": 9055
    },
    {
      "epoch": 1.2731618163925207,
      "grad_norm": 1.6402941942214966,
      "learning_rate": 0.00018609052872122308,
      "loss": 1.1859,
      "step": 9056
    },
    {
      "epoch": 1.2733024040489245,
      "grad_norm": 1.7737419605255127,
      "learning_rate": 0.00018597202380389846,
      "loss": 1.1372,
      "step": 9057
    },
    {
      "epoch": 1.2734429917053283,
      "grad_norm": 1.7040234804153442,
      "learning_rate": 0.00018585305427623377,
      "loss": 1.1581,
      "step": 9058
    },
    {
      "epoch": 1.273583579361732,
      "grad_norm": 1.3499815464019775,
      "learning_rate": 0.00018573362078116472,
      "loss": 1.1961,
      "step": 9059
    },
    {
      "epoch": 1.2737241670181358,
      "grad_norm": 1.5882632732391357,
      "learning_rate": 0.00018561372396413436,
      "loss": 1.0878,
      "step": 9060
    },
    {
      "epoch": 1.2738647546745396,
      "grad_norm": 1.3790736198425293,
      "learning_rate": 0.00018549336447308917,
      "loss": 1.3385,
      "step": 9061
    },
    {
      "epoch": 1.2740053423309434,
      "grad_norm": 1.6048802137374878,
      "learning_rate": 0.0001853725429584764,
      "loss": 1.1172,
      "step": 9062
    },
    {
      "epoch": 1.274145929987347,
      "grad_norm": 1.625138521194458,
      "learning_rate": 0.00018525126007324047,
      "loss": 1.0971,
      "step": 9063
    },
    {
      "epoch": 1.274286517643751,
      "grad_norm": 1.6536284685134888,
      "learning_rate": 0.00018512951647281866,
      "loss": 1.0084,
      "step": 9064
    },
    {
      "epoch": 1.2744271053001546,
      "grad_norm": 1.516570806503296,
      "learning_rate": 0.0001850073128151384,
      "loss": 1.1005,
      "step": 9065
    },
    {
      "epoch": 1.2745676929565584,
      "grad_norm": 1.6254937648773193,
      "learning_rate": 0.0001848846497606127,
      "loss": 1.0124,
      "step": 9066
    },
    {
      "epoch": 1.2747082806129622,
      "grad_norm": 1.8594939708709717,
      "learning_rate": 0.00018476152797213802,
      "loss": 1.1902,
      "step": 9067
    },
    {
      "epoch": 1.274848868269366,
      "grad_norm": 1.567474365234375,
      "learning_rate": 0.0001846379481150895,
      "loss": 1.3367,
      "step": 9068
    },
    {
      "epoch": 1.2749894559257697,
      "grad_norm": 1.7418768405914307,
      "learning_rate": 0.00018451391085731796,
      "loss": 1.1701,
      "step": 9069
    },
    {
      "epoch": 1.2751300435821735,
      "grad_norm": 1.6874725818634033,
      "learning_rate": 0.00018438941686914576,
      "loss": 1.2081,
      "step": 9070
    },
    {
      "epoch": 1.2752706312385773,
      "grad_norm": 1.534822940826416,
      "learning_rate": 0.0001842644668233644,
      "loss": 1.0758,
      "step": 9071
    },
    {
      "epoch": 1.275411218894981,
      "grad_norm": 1.5744359493255615,
      "learning_rate": 0.00018413906139522882,
      "loss": 0.9436,
      "step": 9072
    },
    {
      "epoch": 1.275551806551385,
      "grad_norm": 1.7470173835754395,
      "learning_rate": 0.00018401320126245566,
      "loss": 1.0717,
      "step": 9073
    },
    {
      "epoch": 1.2756923942077885,
      "grad_norm": 1.8013612031936646,
      "learning_rate": 0.0001838868871052188,
      "loss": 1.0235,
      "step": 9074
    },
    {
      "epoch": 1.2758329818641923,
      "grad_norm": 1.5690962076187134,
      "learning_rate": 0.00018376011960614567,
      "loss": 1.0176,
      "step": 9075
    },
    {
      "epoch": 1.275973569520596,
      "grad_norm": 1.6302660703659058,
      "learning_rate": 0.00018363289945031385,
      "loss": 0.9359,
      "step": 9076
    },
    {
      "epoch": 1.2761141571769998,
      "grad_norm": 1.5265065431594849,
      "learning_rate": 0.00018350522732524642,
      "loss": 1.2063,
      "step": 9077
    },
    {
      "epoch": 1.2762547448334036,
      "grad_norm": 1.5463529825210571,
      "learning_rate": 0.00018337710392091016,
      "loss": 1.1137,
      "step": 9078
    },
    {
      "epoch": 1.2763953324898074,
      "grad_norm": 1.5988909006118774,
      "learning_rate": 0.00018324852992970985,
      "loss": 1.1769,
      "step": 9079
    },
    {
      "epoch": 1.2765359201462112,
      "grad_norm": 1.7198119163513184,
      "learning_rate": 0.00018311950604648554,
      "loss": 1.0915,
      "step": 9080
    },
    {
      "epoch": 1.276676507802615,
      "grad_norm": 1.6485655307769775,
      "learning_rate": 0.00018299003296850882,
      "loss": 1.1292,
      "step": 9081
    },
    {
      "epoch": 1.2768170954590188,
      "grad_norm": 1.5975757837295532,
      "learning_rate": 0.00018286011139547815,
      "loss": 0.9751,
      "step": 9082
    },
    {
      "epoch": 1.2769576831154223,
      "grad_norm": 1.3916590213775635,
      "learning_rate": 0.00018272974202951644,
      "loss": 1.1839,
      "step": 9083
    },
    {
      "epoch": 1.2770982707718264,
      "grad_norm": 1.7058995962142944,
      "learning_rate": 0.0001825989255751663,
      "loss": 0.8599,
      "step": 9084
    },
    {
      "epoch": 1.27723885842823,
      "grad_norm": 1.645680546760559,
      "learning_rate": 0.00018246766273938633,
      "loss": 1.0449,
      "step": 9085
    },
    {
      "epoch": 1.2773794460846337,
      "grad_norm": 2.1111109256744385,
      "learning_rate": 0.00018233595423154802,
      "loss": 1.2198,
      "step": 9086
    },
    {
      "epoch": 1.2775200337410375,
      "grad_norm": 1.7031160593032837,
      "learning_rate": 0.00018220380076343096,
      "loss": 1.0062,
      "step": 9087
    },
    {
      "epoch": 1.2776606213974413,
      "grad_norm": 1.5303527116775513,
      "learning_rate": 0.0001820712030492191,
      "loss": 1.1799,
      "step": 9088
    },
    {
      "epoch": 1.277801209053845,
      "grad_norm": 1.4093537330627441,
      "learning_rate": 0.0001819381618054977,
      "loss": 1.3487,
      "step": 9089
    },
    {
      "epoch": 1.2779417967102489,
      "grad_norm": 1.805446743965149,
      "learning_rate": 0.00018180467775124871,
      "loss": 1.096,
      "step": 9090
    },
    {
      "epoch": 1.2780823843666527,
      "grad_norm": 1.5064477920532227,
      "learning_rate": 0.00018167075160784726,
      "loss": 0.9546,
      "step": 9091
    },
    {
      "epoch": 1.2782229720230565,
      "grad_norm": 1.6827195882797241,
      "learning_rate": 0.00018153638409905743,
      "loss": 1.1358,
      "step": 9092
    },
    {
      "epoch": 1.2783635596794602,
      "grad_norm": 1.504686713218689,
      "learning_rate": 0.00018140157595102863,
      "loss": 1.0844,
      "step": 9093
    },
    {
      "epoch": 1.2785041473358638,
      "grad_norm": 1.6719343662261963,
      "learning_rate": 0.00018126632789229163,
      "loss": 1.1611,
      "step": 9094
    },
    {
      "epoch": 1.2786447349922676,
      "grad_norm": 1.6609058380126953,
      "learning_rate": 0.00018113064065375446,
      "loss": 1.101,
      "step": 9095
    },
    {
      "epoch": 1.2787853226486714,
      "grad_norm": 1.7375859022140503,
      "learning_rate": 0.00018099451496869867,
      "loss": 1.0046,
      "step": 9096
    },
    {
      "epoch": 1.2789259103050752,
      "grad_norm": 1.7142894268035889,
      "learning_rate": 0.00018085795157277515,
      "loss": 1.2687,
      "step": 9097
    },
    {
      "epoch": 1.279066497961479,
      "grad_norm": 2.6622865200042725,
      "learning_rate": 0.00018072095120400054,
      "loss": 1.1469,
      "step": 9098
    },
    {
      "epoch": 1.2792070856178828,
      "grad_norm": 1.7290029525756836,
      "learning_rate": 0.00018058351460275237,
      "loss": 1.0209,
      "step": 9099
    },
    {
      "epoch": 1.2793476732742866,
      "grad_norm": 1.659568428993225,
      "learning_rate": 0.00018044564251176604,
      "loss": 1.1958,
      "step": 9100
    },
    {
      "epoch": 1.2794882609306903,
      "grad_norm": 1.7925031185150146,
      "learning_rate": 0.00018030733567613082,
      "loss": 1.1541,
      "step": 9101
    },
    {
      "epoch": 1.2796288485870941,
      "grad_norm": 1.6046271324157715,
      "learning_rate": 0.00018016859484328485,
      "loss": 1.1311,
      "step": 9102
    },
    {
      "epoch": 1.2797694362434977,
      "grad_norm": 1.698543906211853,
      "learning_rate": 0.00018002942076301209,
      "loss": 1.2108,
      "step": 9103
    },
    {
      "epoch": 1.2799100238999017,
      "grad_norm": 1.7889009714126587,
      "learning_rate": 0.0001798898141874372,
      "loss": 1.2483,
      "step": 9104
    },
    {
      "epoch": 1.2800506115563053,
      "grad_norm": 2.1553778648376465,
      "learning_rate": 0.0001797497758710228,
      "loss": 0.923,
      "step": 9105
    },
    {
      "epoch": 1.280191199212709,
      "grad_norm": 1.5189253091812134,
      "learning_rate": 0.00017960930657056446,
      "loss": 1.0792,
      "step": 9106
    },
    {
      "epoch": 1.2803317868691129,
      "grad_norm": 1.431976318359375,
      "learning_rate": 0.00017946840704518695,
      "loss": 1.0531,
      "step": 9107
    },
    {
      "epoch": 1.2804723745255167,
      "grad_norm": 1.92731511592865,
      "learning_rate": 0.00017932707805634002,
      "loss": 1.0712,
      "step": 9108
    },
    {
      "epoch": 1.2806129621819204,
      "grad_norm": 1.4501458406448364,
      "learning_rate": 0.00017918532036779433,
      "loss": 0.9236,
      "step": 9109
    },
    {
      "epoch": 1.2807535498383242,
      "grad_norm": 1.9755370616912842,
      "learning_rate": 0.00017904313474563727,
      "loss": 0.9503,
      "step": 9110
    },
    {
      "epoch": 1.280894137494728,
      "grad_norm": 1.9448091983795166,
      "learning_rate": 0.000178900521958269,
      "loss": 1.1225,
      "step": 9111
    },
    {
      "epoch": 1.2810347251511316,
      "grad_norm": 1.4311498403549194,
      "learning_rate": 0.00017875748277639805,
      "loss": 1.2119,
      "step": 9112
    },
    {
      "epoch": 1.2811753128075356,
      "grad_norm": 1.589487910270691,
      "learning_rate": 0.00017861401797303735,
      "loss": 1.325,
      "step": 9113
    },
    {
      "epoch": 1.2813159004639392,
      "grad_norm": 1.813454031944275,
      "learning_rate": 0.0001784701283235001,
      "loss": 1.2827,
      "step": 9114
    },
    {
      "epoch": 1.281456488120343,
      "grad_norm": 2.2482423782348633,
      "learning_rate": 0.00017832581460539465,
      "loss": 0.8681,
      "step": 9115
    },
    {
      "epoch": 1.2815970757767468,
      "grad_norm": 1.7379363775253296,
      "learning_rate": 0.00017818107759862243,
      "loss": 1.1031,
      "step": 9116
    },
    {
      "epoch": 1.2817376634331505,
      "grad_norm": 1.979575753211975,
      "learning_rate": 0.00017803591808537153,
      "loss": 1.0357,
      "step": 9117
    },
    {
      "epoch": 1.2818782510895543,
      "grad_norm": 2.000929832458496,
      "learning_rate": 0.00017789033685011352,
      "loss": 1.068,
      "step": 9118
    },
    {
      "epoch": 1.2820188387459581,
      "grad_norm": 1.8608142137527466,
      "learning_rate": 0.00017774433467959924,
      "loss": 1.0156,
      "step": 9119
    },
    {
      "epoch": 1.282159426402362,
      "grad_norm": 1.7372153997421265,
      "learning_rate": 0.00017759791236285385,
      "loss": 1.1121,
      "step": 9120
    },
    {
      "epoch": 1.2823000140587657,
      "grad_norm": 1.7145345211029053,
      "learning_rate": 0.00017745107069117347,
      "loss": 1.1685,
      "step": 9121
    },
    {
      "epoch": 1.2824406017151695,
      "grad_norm": 1.544333815574646,
      "learning_rate": 0.00017730381045812056,
      "loss": 1.124,
      "step": 9122
    },
    {
      "epoch": 1.282581189371573,
      "grad_norm": 1.6058942079544067,
      "learning_rate": 0.00017715613245951914,
      "loss": 1.137,
      "step": 9123
    },
    {
      "epoch": 1.282721777027977,
      "grad_norm": 2.835951328277588,
      "learning_rate": 0.0001770080374934519,
      "loss": 0.8186,
      "step": 9124
    },
    {
      "epoch": 1.2828623646843806,
      "grad_norm": 1.6947423219680786,
      "learning_rate": 0.00017685952636025353,
      "loss": 1.0684,
      "step": 9125
    },
    {
      "epoch": 1.2830029523407844,
      "grad_norm": 1.5621860027313232,
      "learning_rate": 0.00017671059986250874,
      "loss": 1.0616,
      "step": 9126
    },
    {
      "epoch": 1.2831435399971882,
      "grad_norm": 1.5204246044158936,
      "learning_rate": 0.00017656125880504664,
      "loss": 1.1591,
      "step": 9127
    },
    {
      "epoch": 1.283284127653592,
      "grad_norm": 2.1373069286346436,
      "learning_rate": 0.00017641150399493673,
      "loss": 1.1488,
      "step": 9128
    },
    {
      "epoch": 1.2834247153099958,
      "grad_norm": 1.520462989807129,
      "learning_rate": 0.00017626133624148456,
      "loss": 1.1008,
      "step": 9129
    },
    {
      "epoch": 1.2835653029663996,
      "grad_norm": 1.7723947763442993,
      "learning_rate": 0.0001761107563562273,
      "loss": 1.074,
      "step": 9130
    },
    {
      "epoch": 1.2837058906228034,
      "grad_norm": 2.0011065006256104,
      "learning_rate": 0.00017595976515292927,
      "loss": 0.9933,
      "step": 9131
    },
    {
      "epoch": 1.283846478279207,
      "grad_norm": 1.764878511428833,
      "learning_rate": 0.0001758083634475778,
      "loss": 1.0942,
      "step": 9132
    },
    {
      "epoch": 1.283987065935611,
      "grad_norm": 1.6459956169128418,
      "learning_rate": 0.00017565655205837842,
      "loss": 1.1139,
      "step": 9133
    },
    {
      "epoch": 1.2841276535920145,
      "grad_norm": 1.7624682188034058,
      "learning_rate": 0.000175504331805751,
      "loss": 1.1074,
      "step": 9134
    },
    {
      "epoch": 1.2842682412484183,
      "grad_norm": 1.6861704587936401,
      "learning_rate": 0.00017535170351232492,
      "loss": 1.0252,
      "step": 9135
    },
    {
      "epoch": 1.284408828904822,
      "grad_norm": 1.9871066808700562,
      "learning_rate": 0.0001751986680029342,
      "loss": 1.1335,
      "step": 9136
    },
    {
      "epoch": 1.284549416561226,
      "grad_norm": 1.6825982332229614,
      "learning_rate": 0.00017504522610461404,
      "loss": 1.162,
      "step": 9137
    },
    {
      "epoch": 1.2846900042176297,
      "grad_norm": 1.5723187923431396,
      "learning_rate": 0.00017489137864659565,
      "loss": 1.1228,
      "step": 9138
    },
    {
      "epoch": 1.2848305918740335,
      "grad_norm": 1.6528877019882202,
      "learning_rate": 0.0001747371264603024,
      "loss": 1.2424,
      "step": 9139
    },
    {
      "epoch": 1.2849711795304373,
      "grad_norm": 1.5057991743087769,
      "learning_rate": 0.0001745824703793444,
      "loss": 1.2874,
      "step": 9140
    },
    {
      "epoch": 1.285111767186841,
      "grad_norm": 1.583730936050415,
      "learning_rate": 0.00017442741123951483,
      "loss": 0.8169,
      "step": 9141
    },
    {
      "epoch": 1.2852523548432448,
      "grad_norm": 1.4642210006713867,
      "learning_rate": 0.0001742719498787844,
      "loss": 1.1452,
      "step": 9142
    },
    {
      "epoch": 1.2853929424996484,
      "grad_norm": 1.8202611207962036,
      "learning_rate": 0.0001741160871372982,
      "loss": 0.9566,
      "step": 9143
    },
    {
      "epoch": 1.2855335301560524,
      "grad_norm": 1.4370025396347046,
      "learning_rate": 0.00017395982385737035,
      "loss": 1.0807,
      "step": 9144
    },
    {
      "epoch": 1.285674117812456,
      "grad_norm": 1.4965769052505493,
      "learning_rate": 0.0001738031608834794,
      "loss": 1.199,
      "step": 9145
    },
    {
      "epoch": 1.2858147054688598,
      "grad_norm": 1.8778364658355713,
      "learning_rate": 0.00017364609906226402,
      "loss": 0.9772,
      "step": 9146
    },
    {
      "epoch": 1.2859552931252636,
      "grad_norm": 1.8390265703201294,
      "learning_rate": 0.00017348863924251826,
      "loss": 1.0981,
      "step": 9147
    },
    {
      "epoch": 1.2860958807816674,
      "grad_norm": 1.5857856273651123,
      "learning_rate": 0.00017333078227518711,
      "loss": 1.0537,
      "step": 9148
    },
    {
      "epoch": 1.2862364684380712,
      "grad_norm": 1.5776382684707642,
      "learning_rate": 0.00017317252901336182,
      "loss": 1.2449,
      "step": 9149
    },
    {
      "epoch": 1.286377056094475,
      "grad_norm": 1.578503131866455,
      "learning_rate": 0.00017301388031227522,
      "loss": 1.1942,
      "step": 9150
    },
    {
      "epoch": 1.2865176437508787,
      "grad_norm": 1.7997684478759766,
      "learning_rate": 0.0001728548370292975,
      "loss": 0.9581,
      "step": 9151
    },
    {
      "epoch": 1.2866582314072823,
      "grad_norm": 1.705040454864502,
      "learning_rate": 0.0001726954000239305,
      "loss": 1.1112,
      "step": 9152
    },
    {
      "epoch": 1.2867988190636863,
      "grad_norm": 1.4327958822250366,
      "learning_rate": 0.00017253557015780427,
      "loss": 1.1121,
      "step": 9153
    },
    {
      "epoch": 1.2869394067200899,
      "grad_norm": 1.736775279045105,
      "learning_rate": 0.0001723753482946725,
      "loss": 0.9847,
      "step": 9154
    },
    {
      "epoch": 1.2870799943764937,
      "grad_norm": 1.4962621927261353,
      "learning_rate": 0.00017221473530040657,
      "loss": 1.0303,
      "step": 9155
    },
    {
      "epoch": 1.2872205820328975,
      "grad_norm": 1.6864919662475586,
      "learning_rate": 0.0001720537320429918,
      "loss": 1.1292,
      "step": 9156
    },
    {
      "epoch": 1.2873611696893013,
      "grad_norm": 1.7718563079833984,
      "learning_rate": 0.00017189233939252283,
      "loss": 1.2316,
      "step": 9157
    },
    {
      "epoch": 1.287501757345705,
      "grad_norm": 1.6963517665863037,
      "learning_rate": 0.00017173055822119798,
      "loss": 1.0629,
      "step": 9158
    },
    {
      "epoch": 1.2876423450021088,
      "grad_norm": 2.238068103790283,
      "learning_rate": 0.00017156838940331594,
      "loss": 1.2094,
      "step": 9159
    },
    {
      "epoch": 1.2877829326585126,
      "grad_norm": 1.653233528137207,
      "learning_rate": 0.00017140583381527005,
      "loss": 1.1076,
      "step": 9160
    },
    {
      "epoch": 1.2879235203149164,
      "grad_norm": 1.9591785669326782,
      "learning_rate": 0.00017124289233554356,
      "loss": 1.0246,
      "step": 9161
    },
    {
      "epoch": 1.2880641079713202,
      "grad_norm": 1.797045350074768,
      "learning_rate": 0.00017107956584470622,
      "loss": 1.192,
      "step": 9162
    },
    {
      "epoch": 1.2882046956277238,
      "grad_norm": 1.6156853437423706,
      "learning_rate": 0.0001709158552254069,
      "loss": 1.0145,
      "step": 9163
    },
    {
      "epoch": 1.2883452832841278,
      "grad_norm": 1.7348980903625488,
      "learning_rate": 0.00017075176136237142,
      "loss": 0.9861,
      "step": 9164
    },
    {
      "epoch": 1.2884858709405314,
      "grad_norm": 1.591746211051941,
      "learning_rate": 0.00017058728514239647,
      "loss": 1.1211,
      "step": 9165
    },
    {
      "epoch": 1.2886264585969351,
      "grad_norm": 1.5713694095611572,
      "learning_rate": 0.00017042242745434502,
      "loss": 1.2097,
      "step": 9166
    },
    {
      "epoch": 1.288767046253339,
      "grad_norm": 1.5822826623916626,
      "learning_rate": 0.00017025718918914197,
      "loss": 1.0877,
      "step": 9167
    },
    {
      "epoch": 1.2889076339097427,
      "grad_norm": 1.747507095336914,
      "learning_rate": 0.00017009157123976798,
      "loss": 0.9036,
      "step": 9168
    },
    {
      "epoch": 1.2890482215661465,
      "grad_norm": 1.720215916633606,
      "learning_rate": 0.00016992557450125627,
      "loss": 1.1016,
      "step": 9169
    },
    {
      "epoch": 1.2891888092225503,
      "grad_norm": 1.7551281452178955,
      "learning_rate": 0.0001697591998706875,
      "loss": 1.0478,
      "step": 9170
    },
    {
      "epoch": 1.289329396878954,
      "grad_norm": 1.6406464576721191,
      "learning_rate": 0.00016959244824718388,
      "loss": 1.166,
      "step": 9171
    },
    {
      "epoch": 1.2894699845353577,
      "grad_norm": 1.5732672214508057,
      "learning_rate": 0.00016942532053190515,
      "loss": 1.1209,
      "step": 9172
    },
    {
      "epoch": 1.2896105721917617,
      "grad_norm": 1.6480966806411743,
      "learning_rate": 0.0001692578176280438,
      "loss": 1.1198,
      "step": 9173
    },
    {
      "epoch": 1.2897511598481652,
      "grad_norm": 1.5897349119186401,
      "learning_rate": 0.0001690899404408192,
      "loss": 1.0917,
      "step": 9174
    },
    {
      "epoch": 1.289891747504569,
      "grad_norm": 1.662063717842102,
      "learning_rate": 0.00016892168987747407,
      "loss": 1.1205,
      "step": 9175
    },
    {
      "epoch": 1.2900323351609728,
      "grad_norm": 1.8122758865356445,
      "learning_rate": 0.00016875306684726853,
      "loss": 1.1398,
      "step": 9176
    },
    {
      "epoch": 1.2901729228173766,
      "grad_norm": 1.5453275442123413,
      "learning_rate": 0.0001685840722614763,
      "loss": 1.0574,
      "step": 9177
    },
    {
      "epoch": 1.2903135104737804,
      "grad_norm": 1.7869575023651123,
      "learning_rate": 0.00016841470703337863,
      "loss": 1.1705,
      "step": 9178
    },
    {
      "epoch": 1.2904540981301842,
      "grad_norm": 1.7343049049377441,
      "learning_rate": 0.0001682449720782591,
      "loss": 1.1151,
      "step": 9179
    },
    {
      "epoch": 1.290594685786588,
      "grad_norm": 1.6561647653579712,
      "learning_rate": 0.0001680748683134004,
      "loss": 1.1751,
      "step": 9180
    },
    {
      "epoch": 1.2907352734429918,
      "grad_norm": 1.5915052890777588,
      "learning_rate": 0.00016790439665807785,
      "loss": 1.0743,
      "step": 9181
    },
    {
      "epoch": 1.2908758610993956,
      "grad_norm": 1.945485234260559,
      "learning_rate": 0.00016773355803355506,
      "loss": 1.1395,
      "step": 9182
    },
    {
      "epoch": 1.2910164487557991,
      "grad_norm": 1.9821782112121582,
      "learning_rate": 0.00016756235336307874,
      "loss": 1.052,
      "step": 9183
    },
    {
      "epoch": 1.2911570364122031,
      "grad_norm": 1.5785696506500244,
      "learning_rate": 0.0001673907835718739,
      "loss": 1.249,
      "step": 9184
    },
    {
      "epoch": 1.2912976240686067,
      "grad_norm": 2.1087288856506348,
      "learning_rate": 0.0001672188495871386,
      "loss": 1.0911,
      "step": 9185
    },
    {
      "epoch": 1.2914382117250105,
      "grad_norm": 1.8944344520568848,
      "learning_rate": 0.00016704655233803914,
      "loss": 1.118,
      "step": 9186
    },
    {
      "epoch": 1.2915787993814143,
      "grad_norm": 1.7069956064224243,
      "learning_rate": 0.000166873892755705,
      "loss": 1.045,
      "step": 9187
    },
    {
      "epoch": 1.291719387037818,
      "grad_norm": 2.3566722869873047,
      "learning_rate": 0.00016670087177322375,
      "loss": 0.8957,
      "step": 9188
    },
    {
      "epoch": 1.2918599746942219,
      "grad_norm": 1.4390168190002441,
      "learning_rate": 0.0001665274903256363,
      "loss": 1.1817,
      "step": 9189
    },
    {
      "epoch": 1.2920005623506257,
      "grad_norm": 1.7130619287490845,
      "learning_rate": 0.0001663537493499308,
      "loss": 1.0723,
      "step": 9190
    },
    {
      "epoch": 1.2921411500070294,
      "grad_norm": 1.8653732538223267,
      "learning_rate": 0.0001661796497850389,
      "loss": 1.0965,
      "step": 9191
    },
    {
      "epoch": 1.292281737663433,
      "grad_norm": 1.6446356773376465,
      "learning_rate": 0.00016600519257183068,
      "loss": 0.9988,
      "step": 9192
    },
    {
      "epoch": 1.292422325319837,
      "grad_norm": 1.786041498184204,
      "learning_rate": 0.00016583037865310817,
      "loss": 1.1133,
      "step": 9193
    },
    {
      "epoch": 1.2925629129762406,
      "grad_norm": 1.9096848964691162,
      "learning_rate": 0.00016565520897360173,
      "loss": 0.9829,
      "step": 9194
    },
    {
      "epoch": 1.2927035006326444,
      "grad_norm": 1.5199345350265503,
      "learning_rate": 0.0001654796844799635,
      "loss": 0.9931,
      "step": 9195
    },
    {
      "epoch": 1.2928440882890482,
      "grad_norm": 1.4840222597122192,
      "learning_rate": 0.0001653038061207638,
      "loss": 1.3227,
      "step": 9196
    },
    {
      "epoch": 1.292984675945452,
      "grad_norm": 1.8084676265716553,
      "learning_rate": 0.00016512757484648524,
      "loss": 1.1916,
      "step": 9197
    },
    {
      "epoch": 1.2931252636018558,
      "grad_norm": 1.4646297693252563,
      "learning_rate": 0.00016495099160951745,
      "loss": 0.9235,
      "step": 9198
    },
    {
      "epoch": 1.2932658512582595,
      "grad_norm": 1.5595066547393799,
      "learning_rate": 0.00016477405736415202,
      "loss": 1.0898,
      "step": 9199
    },
    {
      "epoch": 1.2934064389146633,
      "grad_norm": 1.52573561668396,
      "learning_rate": 0.00016459677306657845,
      "loss": 0.9277,
      "step": 9200
    },
    {
      "epoch": 1.2935470265710671,
      "grad_norm": 1.568974256515503,
      "learning_rate": 0.00016441913967487647,
      "loss": 1.0207,
      "step": 9201
    },
    {
      "epoch": 1.293687614227471,
      "grad_norm": 1.7386558055877686,
      "learning_rate": 0.00016424115814901336,
      "loss": 1.0281,
      "step": 9202
    },
    {
      "epoch": 1.2938282018838745,
      "grad_norm": 1.6656229496002197,
      "learning_rate": 0.00016406282945083755,
      "loss": 1.1381,
      "step": 9203
    },
    {
      "epoch": 1.2939687895402785,
      "grad_norm": 1.7156884670257568,
      "learning_rate": 0.00016388415454407368,
      "loss": 1.0982,
      "step": 9204
    },
    {
      "epoch": 1.294109377196682,
      "grad_norm": 1.7990193367004395,
      "learning_rate": 0.00016370513439431766,
      "loss": 1.1055,
      "step": 9205
    },
    {
      "epoch": 1.2942499648530859,
      "grad_norm": 1.5186575651168823,
      "learning_rate": 0.00016352576996903032,
      "loss": 1.2283,
      "step": 9206
    },
    {
      "epoch": 1.2943905525094896,
      "grad_norm": 1.934610366821289,
      "learning_rate": 0.00016334606223753357,
      "loss": 0.995,
      "step": 9207
    },
    {
      "epoch": 1.2945311401658934,
      "grad_norm": 1.3714196681976318,
      "learning_rate": 0.00016316601217100516,
      "loss": 1.3192,
      "step": 9208
    },
    {
      "epoch": 1.2946717278222972,
      "grad_norm": 1.5321170091629028,
      "learning_rate": 0.00016298562074247208,
      "loss": 0.8995,
      "step": 9209
    },
    {
      "epoch": 1.294812315478701,
      "grad_norm": 1.5128109455108643,
      "learning_rate": 0.00016280488892680666,
      "loss": 1.1065,
      "step": 9210
    },
    {
      "epoch": 1.2949529031351048,
      "grad_norm": 1.5014135837554932,
      "learning_rate": 0.00016262381770071988,
      "loss": 1.2149,
      "step": 9211
    },
    {
      "epoch": 1.2950934907915084,
      "grad_norm": 1.424100399017334,
      "learning_rate": 0.00016244240804275787,
      "loss": 1.1238,
      "step": 9212
    },
    {
      "epoch": 1.2952340784479124,
      "grad_norm": 1.5214693546295166,
      "learning_rate": 0.0001622606609332954,
      "loss": 0.9544,
      "step": 9213
    },
    {
      "epoch": 1.295374666104316,
      "grad_norm": 1.44595468044281,
      "learning_rate": 0.00016207857735453057,
      "loss": 1.0843,
      "step": 9214
    },
    {
      "epoch": 1.2955152537607197,
      "grad_norm": 1.6949697732925415,
      "learning_rate": 0.00016189615829048076,
      "loss": 1.277,
      "step": 9215
    },
    {
      "epoch": 1.2956558414171235,
      "grad_norm": 2.0125653743743896,
      "learning_rate": 0.00016171340472697577,
      "loss": 1.0848,
      "step": 9216
    },
    {
      "epoch": 1.2957964290735273,
      "grad_norm": 1.5192781686782837,
      "learning_rate": 0.00016153031765165255,
      "loss": 1.1554,
      "step": 9217
    },
    {
      "epoch": 1.2959370167299311,
      "grad_norm": 1.7308751344680786,
      "learning_rate": 0.0001613468980539512,
      "loss": 1.1633,
      "step": 9218
    },
    {
      "epoch": 1.296077604386335,
      "grad_norm": 2.031752109527588,
      "learning_rate": 0.00016116314692510866,
      "loss": 1.0645,
      "step": 9219
    },
    {
      "epoch": 1.2962181920427387,
      "grad_norm": 1.8887070417404175,
      "learning_rate": 0.0001609790652581535,
      "loss": 0.9632,
      "step": 9220
    },
    {
      "epoch": 1.2963587796991425,
      "grad_norm": 2.0545737743377686,
      "learning_rate": 0.00016079465404790076,
      "loss": 1.3075,
      "step": 9221
    },
    {
      "epoch": 1.2964993673555463,
      "grad_norm": 1.7033733129501343,
      "learning_rate": 0.0001606099142909455,
      "loss": 1.0366,
      "step": 9222
    },
    {
      "epoch": 1.2966399550119498,
      "grad_norm": 1.7212245464324951,
      "learning_rate": 0.00016042484698565974,
      "loss": 1.1331,
      "step": 9223
    },
    {
      "epoch": 1.2967805426683539,
      "grad_norm": 1.6856578588485718,
      "learning_rate": 0.00016023945313218472,
      "loss": 1.0695,
      "step": 9224
    },
    {
      "epoch": 1.2969211303247574,
      "grad_norm": 1.6377806663513184,
      "learning_rate": 0.00016005373373242655,
      "loss": 1.1402,
      "step": 9225
    },
    {
      "epoch": 1.2970617179811612,
      "grad_norm": 2.306138515472412,
      "learning_rate": 0.0001598676897900508,
      "loss": 1.0495,
      "step": 9226
    },
    {
      "epoch": 1.297202305637565,
      "grad_norm": 1.8183609247207642,
      "learning_rate": 0.00015968132231047706,
      "loss": 1.0834,
      "step": 9227
    },
    {
      "epoch": 1.2973428932939688,
      "grad_norm": 1.7150808572769165,
      "learning_rate": 0.00015949463230087265,
      "loss": 1.164,
      "step": 9228
    },
    {
      "epoch": 1.2974834809503726,
      "grad_norm": 1.473050594329834,
      "learning_rate": 0.00015930762077014842,
      "loss": 1.1383,
      "step": 9229
    },
    {
      "epoch": 1.2976240686067764,
      "grad_norm": 1.4614437818527222,
      "learning_rate": 0.0001591202887289534,
      "loss": 1.1329,
      "step": 9230
    },
    {
      "epoch": 1.2977646562631802,
      "grad_norm": 1.438124179840088,
      "learning_rate": 0.00015893263718966779,
      "loss": 1.1273,
      "step": 9231
    },
    {
      "epoch": 1.2979052439195837,
      "grad_norm": 1.4436020851135254,
      "learning_rate": 0.00015874466716639905,
      "loss": 1.2036,
      "step": 9232
    },
    {
      "epoch": 1.2980458315759877,
      "grad_norm": 1.4673064947128296,
      "learning_rate": 0.00015855637967497496,
      "loss": 1.0159,
      "step": 9233
    },
    {
      "epoch": 1.2981864192323913,
      "grad_norm": 1.6224939823150635,
      "learning_rate": 0.00015836777573293977,
      "loss": 1.0462,
      "step": 9234
    },
    {
      "epoch": 1.298327006888795,
      "grad_norm": 1.5631029605865479,
      "learning_rate": 0.00015817885635954762,
      "loss": 1.082,
      "step": 9235
    },
    {
      "epoch": 1.2984675945451989,
      "grad_norm": 1.6112377643585205,
      "learning_rate": 0.00015798962257575734,
      "loss": 1.1317,
      "step": 9236
    },
    {
      "epoch": 1.2986081822016027,
      "grad_norm": 1.7107939720153809,
      "learning_rate": 0.00015780007540422682,
      "loss": 1.0764,
      "step": 9237
    },
    {
      "epoch": 1.2987487698580065,
      "grad_norm": 1.4118462800979614,
      "learning_rate": 0.00015761021586930768,
      "loss": 1.1347,
      "step": 9238
    },
    {
      "epoch": 1.2988893575144103,
      "grad_norm": 1.6169922351837158,
      "learning_rate": 0.00015742004499703943,
      "loss": 1.0109,
      "step": 9239
    },
    {
      "epoch": 1.299029945170814,
      "grad_norm": 1.8238567113876343,
      "learning_rate": 0.00015722956381514433,
      "loss": 1.0625,
      "step": 9240
    },
    {
      "epoch": 1.2991705328272178,
      "grad_norm": 1.863434910774231,
      "learning_rate": 0.00015703877335302148,
      "loss": 1.1333,
      "step": 9241
    },
    {
      "epoch": 1.2993111204836216,
      "grad_norm": 1.434008002281189,
      "learning_rate": 0.00015684767464174142,
      "loss": 0.9899,
      "step": 9242
    },
    {
      "epoch": 1.2994517081400252,
      "grad_norm": 1.718916416168213,
      "learning_rate": 0.00015665626871404085,
      "loss": 1.0031,
      "step": 9243
    },
    {
      "epoch": 1.2995922957964292,
      "grad_norm": 1.6361037492752075,
      "learning_rate": 0.0001564645566043158,
      "loss": 1.1183,
      "step": 9244
    },
    {
      "epoch": 1.2997328834528328,
      "grad_norm": 1.3833993673324585,
      "learning_rate": 0.00015627253934861758,
      "loss": 1.0977,
      "step": 9245
    },
    {
      "epoch": 1.2998734711092366,
      "grad_norm": 1.5576844215393066,
      "learning_rate": 0.0001560802179846472,
      "loss": 1.3179,
      "step": 9246
    },
    {
      "epoch": 1.3000140587656404,
      "grad_norm": 1.9133611917495728,
      "learning_rate": 0.00015588759355174818,
      "loss": 1.1465,
      "step": 9247
    },
    {
      "epoch": 1.3001546464220441,
      "grad_norm": 1.873207688331604,
      "learning_rate": 0.00015569466709090257,
      "loss": 1.0241,
      "step": 9248
    },
    {
      "epoch": 1.300295234078448,
      "grad_norm": 1.6932258605957031,
      "learning_rate": 0.00015550143964472362,
      "loss": 1.0597,
      "step": 9249
    },
    {
      "epoch": 1.3004358217348517,
      "grad_norm": 1.6279176473617554,
      "learning_rate": 0.00015530791225745223,
      "loss": 0.994,
      "step": 9250
    },
    {
      "epoch": 1.3005764093912555,
      "grad_norm": 2.1053144931793213,
      "learning_rate": 0.00015511408597494977,
      "loss": 1.0892,
      "step": 9251
    },
    {
      "epoch": 1.300716997047659,
      "grad_norm": 1.5164824724197388,
      "learning_rate": 0.00015491996184469268,
      "loss": 1.0349,
      "step": 9252
    },
    {
      "epoch": 1.300857584704063,
      "grad_norm": 1.9736942052841187,
      "learning_rate": 0.00015472554091576794,
      "loss": 1.0529,
      "step": 9253
    },
    {
      "epoch": 1.3009981723604667,
      "grad_norm": 2.033090353012085,
      "learning_rate": 0.00015453082423886588,
      "loss": 1.2695,
      "step": 9254
    },
    {
      "epoch": 1.3011387600168705,
      "grad_norm": 1.6701654195785522,
      "learning_rate": 0.00015433581286627453,
      "loss": 1.0245,
      "step": 9255
    },
    {
      "epoch": 1.3012793476732742,
      "grad_norm": 1.4794327020645142,
      "learning_rate": 0.00015414050785187534,
      "loss": 1.1177,
      "step": 9256
    },
    {
      "epoch": 1.301419935329678,
      "grad_norm": 1.7729785442352295,
      "learning_rate": 0.0001539449102511365,
      "loss": 1.2418,
      "step": 9257
    },
    {
      "epoch": 1.3015605229860818,
      "grad_norm": 1.6586374044418335,
      "learning_rate": 0.00015374902112110726,
      "loss": 1.0559,
      "step": 9258
    },
    {
      "epoch": 1.3017011106424856,
      "grad_norm": 2.04660964012146,
      "learning_rate": 0.0001535528415204127,
      "loss": 1.1131,
      "step": 9259
    },
    {
      "epoch": 1.3018416982988894,
      "grad_norm": 1.6198102235794067,
      "learning_rate": 0.00015335637250924649,
      "loss": 1.1171,
      "step": 9260
    },
    {
      "epoch": 1.3019822859552932,
      "grad_norm": 1.5116353034973145,
      "learning_rate": 0.00015315961514936806,
      "loss": 1.1503,
      "step": 9261
    },
    {
      "epoch": 1.302122873611697,
      "grad_norm": 1.9650424718856812,
      "learning_rate": 0.00015296257050409408,
      "loss": 1.0539,
      "step": 9262
    },
    {
      "epoch": 1.3022634612681006,
      "grad_norm": 2.0103321075439453,
      "learning_rate": 0.00015276523963829393,
      "loss": 1.1553,
      "step": 9263
    },
    {
      "epoch": 1.3024040489245046,
      "grad_norm": 1.8666529655456543,
      "learning_rate": 0.00015256762361838408,
      "loss": 1.2037,
      "step": 9264
    },
    {
      "epoch": 1.3025446365809081,
      "grad_norm": 1.87700617313385,
      "learning_rate": 0.00015236972351232128,
      "loss": 0.9882,
      "step": 9265
    },
    {
      "epoch": 1.302685224237312,
      "grad_norm": 1.6920804977416992,
      "learning_rate": 0.00015217154038959824,
      "loss": 0.9183,
      "step": 9266
    },
    {
      "epoch": 1.3028258118937157,
      "grad_norm": 1.368003487586975,
      "learning_rate": 0.00015197307532123673,
      "loss": 1.0165,
      "step": 9267
    },
    {
      "epoch": 1.3029663995501195,
      "grad_norm": 1.5453999042510986,
      "learning_rate": 0.00015177432937978297,
      "loss": 1.1264,
      "step": 9268
    },
    {
      "epoch": 1.3031069872065233,
      "grad_norm": 1.3572089672088623,
      "learning_rate": 0.0001515753036393003,
      "loss": 1.1667,
      "step": 9269
    },
    {
      "epoch": 1.303247574862927,
      "grad_norm": 1.4822951555252075,
      "learning_rate": 0.00015137599917536463,
      "loss": 1.0173,
      "step": 9270
    },
    {
      "epoch": 1.3033881625193309,
      "grad_norm": 1.623424768447876,
      "learning_rate": 0.00015117641706505737,
      "loss": 1.1208,
      "step": 9271
    },
    {
      "epoch": 1.3035287501757344,
      "grad_norm": 1.664433479309082,
      "learning_rate": 0.00015097655838696114,
      "loss": 1.2693,
      "step": 9272
    },
    {
      "epoch": 1.3036693378321385,
      "grad_norm": 1.717576026916504,
      "learning_rate": 0.00015077642422115303,
      "loss": 1.025,
      "step": 9273
    },
    {
      "epoch": 1.303809925488542,
      "grad_norm": 1.4262754917144775,
      "learning_rate": 0.00015057601564919887,
      "loss": 1.0818,
      "step": 9274
    },
    {
      "epoch": 1.3039505131449458,
      "grad_norm": 1.3931013345718384,
      "learning_rate": 0.0001503753337541474,
      "loss": 1.2149,
      "step": 9275
    },
    {
      "epoch": 1.3040911008013496,
      "grad_norm": 1.7016568183898926,
      "learning_rate": 0.0001501743796205245,
      "loss": 1.0851,
      "step": 9276
    },
    {
      "epoch": 1.3042316884577534,
      "grad_norm": 1.4480117559432983,
      "learning_rate": 0.00014997315433432727,
      "loss": 1.0811,
      "step": 9277
    },
    {
      "epoch": 1.3043722761141572,
      "grad_norm": 1.853022813796997,
      "learning_rate": 0.00014977165898301818,
      "loss": 1.1043,
      "step": 9278
    },
    {
      "epoch": 1.304512863770561,
      "grad_norm": 1.5214147567749023,
      "learning_rate": 0.00014956989465551916,
      "loss": 1.0327,
      "step": 9279
    },
    {
      "epoch": 1.3046534514269648,
      "grad_norm": 1.679381012916565,
      "learning_rate": 0.00014936786244220605,
      "loss": 0.9404,
      "step": 9280
    },
    {
      "epoch": 1.3047940390833686,
      "grad_norm": 1.6286622285842896,
      "learning_rate": 0.00014916556343490158,
      "loss": 0.9901,
      "step": 9281
    },
    {
      "epoch": 1.3049346267397723,
      "grad_norm": 1.3367478847503662,
      "learning_rate": 0.00014896299872687102,
      "loss": 1.1114,
      "step": 9282
    },
    {
      "epoch": 1.305075214396176,
      "grad_norm": 1.6688858270645142,
      "learning_rate": 0.0001487601694128151,
      "loss": 1.003,
      "step": 9283
    },
    {
      "epoch": 1.30521580205258,
      "grad_norm": 1.359255075454712,
      "learning_rate": 0.00014855707658886536,
      "loss": 1.0998,
      "step": 9284
    },
    {
      "epoch": 1.3053563897089835,
      "grad_norm": 1.3492196798324585,
      "learning_rate": 0.00014835372135257652,
      "loss": 1.1655,
      "step": 9285
    },
    {
      "epoch": 1.3054969773653873,
      "grad_norm": 1.5355840921401978,
      "learning_rate": 0.00014815010480292203,
      "loss": 1.1115,
      "step": 9286
    },
    {
      "epoch": 1.305637565021791,
      "grad_norm": 1.6671420335769653,
      "learning_rate": 0.00014794622804028664,
      "loss": 1.2198,
      "step": 9287
    },
    {
      "epoch": 1.3057781526781949,
      "grad_norm": 1.6040133237838745,
      "learning_rate": 0.0001477420921664622,
      "loss": 1.0729,
      "step": 9288
    },
    {
      "epoch": 1.3059187403345986,
      "grad_norm": 1.8215745687484741,
      "learning_rate": 0.00014753769828464058,
      "loss": 1.0545,
      "step": 9289
    },
    {
      "epoch": 1.3060593279910024,
      "grad_norm": 1.6093366146087646,
      "learning_rate": 0.0001473330474994077,
      "loss": 1.0051,
      "step": 9290
    },
    {
      "epoch": 1.3061999156474062,
      "grad_norm": 1.612054705619812,
      "learning_rate": 0.00014712814091673903,
      "loss": 1.1688,
      "step": 9291
    },
    {
      "epoch": 1.3063405033038098,
      "grad_norm": 1.6206339597702026,
      "learning_rate": 0.0001469229796439906,
      "loss": 1.1474,
      "step": 9292
    },
    {
      "epoch": 1.3064810909602138,
      "grad_norm": 1.7335823774337769,
      "learning_rate": 0.000146717564789896,
      "loss": 1.1542,
      "step": 9293
    },
    {
      "epoch": 1.3066216786166174,
      "grad_norm": 2.014697313308716,
      "learning_rate": 0.000146511897464559,
      "loss": 1.0095,
      "step": 9294
    },
    {
      "epoch": 1.3067622662730212,
      "grad_norm": 1.665680170059204,
      "learning_rate": 0.00014630597877944774,
      "loss": 1.1914,
      "step": 9295
    },
    {
      "epoch": 1.306902853929425,
      "grad_norm": 1.333528757095337,
      "learning_rate": 0.0001460998098473888,
      "loss": 0.9256,
      "step": 9296
    },
    {
      "epoch": 1.3070434415858287,
      "grad_norm": 1.5499473810195923,
      "learning_rate": 0.00014589339178256138,
      "loss": 0.9446,
      "step": 9297
    },
    {
      "epoch": 1.3071840292422325,
      "grad_norm": 1.418007254600525,
      "learning_rate": 0.00014568672570048987,
      "loss": 1.1174,
      "step": 9298
    },
    {
      "epoch": 1.3073246168986363,
      "grad_norm": 1.7273433208465576,
      "learning_rate": 0.0001454798127180408,
      "loss": 1.0503,
      "step": 9299
    },
    {
      "epoch": 1.3074652045550401,
      "grad_norm": 2.070150136947632,
      "learning_rate": 0.00014527265395341386,
      "loss": 1.131,
      "step": 9300
    },
    {
      "epoch": 1.307605792211444,
      "grad_norm": 1.6466110944747925,
      "learning_rate": 0.00014506525052613716,
      "loss": 1.1127,
      "step": 9301
    },
    {
      "epoch": 1.3077463798678477,
      "grad_norm": 1.4370876550674438,
      "learning_rate": 0.00014485760355706145,
      "loss": 1.1362,
      "step": 9302
    },
    {
      "epoch": 1.3078869675242513,
      "grad_norm": 1.900474190711975,
      "learning_rate": 0.00014464971416835263,
      "loss": 1.0137,
      "step": 9303
    },
    {
      "epoch": 1.3080275551806553,
      "grad_norm": 2.0109877586364746,
      "learning_rate": 0.0001444415834834877,
      "loss": 0.9937,
      "step": 9304
    },
    {
      "epoch": 1.3081681428370588,
      "grad_norm": 1.507252812385559,
      "learning_rate": 0.0001442332126272472,
      "loss": 1.1758,
      "step": 9305
    },
    {
      "epoch": 1.3083087304934626,
      "grad_norm": 1.5242818593978882,
      "learning_rate": 0.0001440246027257095,
      "loss": 0.8998,
      "step": 9306
    },
    {
      "epoch": 1.3084493181498664,
      "grad_norm": 1.3350390195846558,
      "learning_rate": 0.0001438157549062459,
      "loss": 1.1127,
      "step": 9307
    },
    {
      "epoch": 1.3085899058062702,
      "grad_norm": 1.5556477308273315,
      "learning_rate": 0.00014360667029751168,
      "loss": 1.0519,
      "step": 9308
    },
    {
      "epoch": 1.308730493462674,
      "grad_norm": 1.810445785522461,
      "learning_rate": 0.00014339735002944295,
      "loss": 1.1712,
      "step": 9309
    },
    {
      "epoch": 1.3088710811190778,
      "grad_norm": 1.5738539695739746,
      "learning_rate": 0.00014318779523324915,
      "loss": 1.0679,
      "step": 9310
    },
    {
      "epoch": 1.3090116687754816,
      "grad_norm": 1.6122781038284302,
      "learning_rate": 0.0001429780070414071,
      "loss": 1.1413,
      "step": 9311
    },
    {
      "epoch": 1.3091522564318852,
      "grad_norm": 1.5615967512130737,
      "learning_rate": 0.00014276798658765494,
      "loss": 1.1963,
      "step": 9312
    },
    {
      "epoch": 1.3092928440882892,
      "grad_norm": 1.796274185180664,
      "learning_rate": 0.00014255773500698602,
      "loss": 0.9956,
      "step": 9313
    },
    {
      "epoch": 1.3094334317446927,
      "grad_norm": 1.701673984527588,
      "learning_rate": 0.00014234725343564275,
      "loss": 1.1133,
      "step": 9314
    },
    {
      "epoch": 1.3095740194010965,
      "grad_norm": 1.7188972234725952,
      "learning_rate": 0.0001421365430111104,
      "loss": 0.9599,
      "step": 9315
    },
    {
      "epoch": 1.3097146070575003,
      "grad_norm": 1.5111935138702393,
      "learning_rate": 0.00014192560487211108,
      "loss": 1.0278,
      "step": 9316
    },
    {
      "epoch": 1.309855194713904,
      "grad_norm": 1.9420205354690552,
      "learning_rate": 0.00014171444015859748,
      "loss": 1.056,
      "step": 9317
    },
    {
      "epoch": 1.309995782370308,
      "grad_norm": 1.5318670272827148,
      "learning_rate": 0.0001415030500117471,
      "loss": 1.1702,
      "step": 9318
    },
    {
      "epoch": 1.3101363700267117,
      "grad_norm": 1.529337763786316,
      "learning_rate": 0.0001412914355739547,
      "loss": 1.1656,
      "step": 9319
    },
    {
      "epoch": 1.3102769576831155,
      "grad_norm": 1.6522799730300903,
      "learning_rate": 0.00014107959798882822,
      "loss": 1.0036,
      "step": 9320
    },
    {
      "epoch": 1.3104175453395193,
      "grad_norm": 1.814904808998108,
      "learning_rate": 0.00014086753840118074,
      "loss": 1.0596,
      "step": 9321
    },
    {
      "epoch": 1.310558132995923,
      "grad_norm": 1.4853018522262573,
      "learning_rate": 0.00014065525795702616,
      "loss": 1.0919,
      "step": 9322
    },
    {
      "epoch": 1.3106987206523266,
      "grad_norm": 2.143780469894409,
      "learning_rate": 0.0001404427578035713,
      "loss": 0.9906,
      "step": 9323
    },
    {
      "epoch": 1.3108393083087304,
      "grad_norm": 1.632770299911499,
      "learning_rate": 0.00014023003908920968,
      "loss": 1.1489,
      "step": 9324
    },
    {
      "epoch": 1.3109798959651342,
      "grad_norm": 1.6462643146514893,
      "learning_rate": 0.00014001710296351687,
      "loss": 1.0832,
      "step": 9325
    },
    {
      "epoch": 1.311120483621538,
      "grad_norm": 1.6632139682769775,
      "learning_rate": 0.0001398039505772431,
      "loss": 1.0463,
      "step": 9326
    },
    {
      "epoch": 1.3112610712779418,
      "grad_norm": 1.7723900079727173,
      "learning_rate": 0.0001395905830823073,
      "loss": 1.0715,
      "step": 9327
    },
    {
      "epoch": 1.3114016589343456,
      "grad_norm": 1.6833618879318237,
      "learning_rate": 0.00013937700163179059,
      "loss": 0.9832,
      "step": 9328
    },
    {
      "epoch": 1.3115422465907494,
      "grad_norm": 1.4864213466644287,
      "learning_rate": 0.00013916320737993165,
      "loss": 0.9968,
      "step": 9329
    },
    {
      "epoch": 1.3116828342471532,
      "grad_norm": 1.898069977760315,
      "learning_rate": 0.0001389492014821172,
      "loss": 0.9594,
      "step": 9330
    },
    {
      "epoch": 1.311823421903557,
      "grad_norm": 1.8096487522125244,
      "learning_rate": 0.00013873498509487907,
      "loss": 1.1027,
      "step": 9331
    },
    {
      "epoch": 1.3119640095599605,
      "grad_norm": 1.9824308156967163,
      "learning_rate": 0.0001385205593758863,
      "loss": 1.2518,
      "step": 9332
    },
    {
      "epoch": 1.3121045972163645,
      "grad_norm": 1.6180247068405151,
      "learning_rate": 0.0001383059254839391,
      "loss": 1.0937,
      "step": 9333
    },
    {
      "epoch": 1.312245184872768,
      "grad_norm": 1.8716975450515747,
      "learning_rate": 0.00013809108457896318,
      "loss": 0.9357,
      "step": 9334
    },
    {
      "epoch": 1.3123857725291719,
      "grad_norm": 1.8501513004302979,
      "learning_rate": 0.00013787603782200192,
      "loss": 1.1595,
      "step": 9335
    },
    {
      "epoch": 1.3125263601855757,
      "grad_norm": 1.6387457847595215,
      "learning_rate": 0.00013766078637521178,
      "loss": 1.1051,
      "step": 9336
    },
    {
      "epoch": 1.3126669478419795,
      "grad_norm": 1.6748056411743164,
      "learning_rate": 0.00013744533140185614,
      "loss": 0.9657,
      "step": 9337
    },
    {
      "epoch": 1.3128075354983832,
      "grad_norm": 1.5307222604751587,
      "learning_rate": 0.00013722967406629727,
      "loss": 1.1005,
      "step": 9338
    },
    {
      "epoch": 1.312948123154787,
      "grad_norm": 1.8176019191741943,
      "learning_rate": 0.00013701381553399134,
      "loss": 1.109,
      "step": 9339
    },
    {
      "epoch": 1.3130887108111908,
      "grad_norm": 1.571407437324524,
      "learning_rate": 0.0001367977569714822,
      "loss": 1.2099,
      "step": 9340
    },
    {
      "epoch": 1.3132292984675946,
      "grad_norm": 1.7434685230255127,
      "learning_rate": 0.0001365814995463937,
      "loss": 0.9934,
      "step": 9341
    },
    {
      "epoch": 1.3133698861239984,
      "grad_norm": 2.414738893508911,
      "learning_rate": 0.00013636504442742542,
      "loss": 1.0238,
      "step": 9342
    },
    {
      "epoch": 1.313510473780402,
      "grad_norm": 1.737977147102356,
      "learning_rate": 0.00013614839278434487,
      "loss": 1.0906,
      "step": 9343
    },
    {
      "epoch": 1.3136510614368058,
      "grad_norm": 1.5664597749710083,
      "learning_rate": 0.00013593154578798138,
      "loss": 1.0334,
      "step": 9344
    },
    {
      "epoch": 1.3137916490932096,
      "grad_norm": 1.6649338006973267,
      "learning_rate": 0.0001357145046102213,
      "loss": 1.188,
      "step": 9345
    },
    {
      "epoch": 1.3139322367496133,
      "grad_norm": 1.6974161863327026,
      "learning_rate": 0.00013549727042399843,
      "loss": 1.0714,
      "step": 9346
    },
    {
      "epoch": 1.3140728244060171,
      "grad_norm": 1.7419668436050415,
      "learning_rate": 0.00013527984440329092,
      "loss": 1.0129,
      "step": 9347
    },
    {
      "epoch": 1.314213412062421,
      "grad_norm": 1.7970404624938965,
      "learning_rate": 0.0001350622277231133,
      "loss": 0.9326,
      "step": 9348
    },
    {
      "epoch": 1.3143539997188247,
      "grad_norm": 1.5491509437561035,
      "learning_rate": 0.00013484442155951043,
      "loss": 1.0499,
      "step": 9349
    },
    {
      "epoch": 1.3144945873752285,
      "grad_norm": 1.3485517501831055,
      "learning_rate": 0.00013462642708955154,
      "loss": 1.0863,
      "step": 9350
    },
    {
      "epoch": 1.3146351750316323,
      "grad_norm": 1.5245225429534912,
      "learning_rate": 0.00013440824549132226,
      "loss": 1.1649,
      "step": 9351
    },
    {
      "epoch": 1.3147757626880359,
      "grad_norm": 1.599583625793457,
      "learning_rate": 0.00013418987794392136,
      "loss": 1.0102,
      "step": 9352
    },
    {
      "epoch": 1.3149163503444399,
      "grad_norm": 1.718659520149231,
      "learning_rate": 0.00013397132562745128,
      "loss": 0.9556,
      "step": 9353
    },
    {
      "epoch": 1.3150569380008434,
      "grad_norm": 1.536095380783081,
      "learning_rate": 0.00013375258972301324,
      "loss": 1.1689,
      "step": 9354
    },
    {
      "epoch": 1.3151975256572472,
      "grad_norm": 1.7481613159179688,
      "learning_rate": 0.00013353367141270063,
      "loss": 1.038,
      "step": 9355
    },
    {
      "epoch": 1.315338113313651,
      "grad_norm": 1.4898244142532349,
      "learning_rate": 0.00013331457187959304,
      "loss": 1.0334,
      "step": 9356
    },
    {
      "epoch": 1.3154787009700548,
      "grad_norm": 1.629027009010315,
      "learning_rate": 0.00013309529230774828,
      "loss": 1.0053,
      "step": 9357
    },
    {
      "epoch": 1.3156192886264586,
      "grad_norm": 1.6801072359085083,
      "learning_rate": 0.00013287583388219817,
      "loss": 1.2787,
      "step": 9358
    },
    {
      "epoch": 1.3157598762828624,
      "grad_norm": 1.7846571207046509,
      "learning_rate": 0.00013265619778894048,
      "loss": 1.0239,
      "step": 9359
    },
    {
      "epoch": 1.3159004639392662,
      "grad_norm": 1.6352580785751343,
      "learning_rate": 0.00013243638521493408,
      "loss": 1.0811,
      "step": 9360
    },
    {
      "epoch": 1.31604105159567,
      "grad_norm": 1.4896951913833618,
      "learning_rate": 0.00013221639734809092,
      "loss": 1.1185,
      "step": 9361
    },
    {
      "epoch": 1.3161816392520738,
      "grad_norm": 1.7104759216308594,
      "learning_rate": 0.0001319962353772696,
      "loss": 1.0406,
      "step": 9362
    },
    {
      "epoch": 1.3163222269084773,
      "grad_norm": 1.7084583044052124,
      "learning_rate": 0.0001317759004922706,
      "loss": 0.9377,
      "step": 9363
    },
    {
      "epoch": 1.3164628145648811,
      "grad_norm": 1.619554042816162,
      "learning_rate": 0.00013155539388382863,
      "loss": 1.127,
      "step": 9364
    },
    {
      "epoch": 1.316603402221285,
      "grad_norm": 1.5518227815628052,
      "learning_rate": 0.00013133471674360627,
      "loss": 1.0654,
      "step": 9365
    },
    {
      "epoch": 1.3167439898776887,
      "grad_norm": 1.6969035863876343,
      "learning_rate": 0.00013111387026418776,
      "loss": 1.1073,
      "step": 9366
    },
    {
      "epoch": 1.3168845775340925,
      "grad_norm": 1.4762563705444336,
      "learning_rate": 0.00013089285563907253,
      "loss": 0.9492,
      "step": 9367
    },
    {
      "epoch": 1.3170251651904963,
      "grad_norm": 1.7846506834030151,
      "learning_rate": 0.00013067167406266865,
      "loss": 0.9361,
      "step": 9368
    },
    {
      "epoch": 1.3171657528469,
      "grad_norm": 1.848501443862915,
      "learning_rate": 0.00013045032673028643,
      "loss": 1.0365,
      "step": 9369
    },
    {
      "epoch": 1.3173063405033039,
      "grad_norm": 1.7586158514022827,
      "learning_rate": 0.00013022881483813197,
      "loss": 0.9837,
      "step": 9370
    },
    {
      "epoch": 1.3174469281597077,
      "grad_norm": 1.704315423965454,
      "learning_rate": 0.00013000713958330072,
      "loss": 0.9952,
      "step": 9371
    },
    {
      "epoch": 1.3175875158161112,
      "grad_norm": 1.69316828250885,
      "learning_rate": 0.00012978530216377124,
      "loss": 1.202,
      "step": 9372
    },
    {
      "epoch": 1.3177281034725152,
      "grad_norm": 1.6679176092147827,
      "learning_rate": 0.0001295633037783975,
      "loss": 1.1247,
      "step": 9373
    },
    {
      "epoch": 1.3178686911289188,
      "grad_norm": 1.5500288009643555,
      "learning_rate": 0.00012934114562690403,
      "loss": 1.0615,
      "step": 9374
    },
    {
      "epoch": 1.3180092787853226,
      "grad_norm": 1.448120355606079,
      "learning_rate": 0.0001291188289098794,
      "loss": 1.1058,
      "step": 9375
    },
    {
      "epoch": 1.3181498664417264,
      "grad_norm": 1.652866005897522,
      "learning_rate": 0.00012889635482876818,
      "loss": 1.0498,
      "step": 9376
    },
    {
      "epoch": 1.3182904540981302,
      "grad_norm": 1.7943826913833618,
      "learning_rate": 0.00012867372458586596,
      "loss": 1.2292,
      "step": 9377
    },
    {
      "epoch": 1.318431041754534,
      "grad_norm": 1.5212994813919067,
      "learning_rate": 0.00012845093938431128,
      "loss": 1.006,
      "step": 9378
    },
    {
      "epoch": 1.3185716294109378,
      "grad_norm": 1.5299098491668701,
      "learning_rate": 0.0001282280004280811,
      "loss": 1.1243,
      "step": 9379
    },
    {
      "epoch": 1.3187122170673415,
      "grad_norm": 1.407081127166748,
      "learning_rate": 0.00012800490892198304,
      "loss": 1.1072,
      "step": 9380
    },
    {
      "epoch": 1.3188528047237453,
      "grad_norm": 1.3860105276107788,
      "learning_rate": 0.00012778166607164897,
      "loss": 1.0358,
      "step": 9381
    },
    {
      "epoch": 1.3189933923801491,
      "grad_norm": 1.4850844144821167,
      "learning_rate": 0.00012755827308352847,
      "loss": 1.2318,
      "step": 9382
    },
    {
      "epoch": 1.3191339800365527,
      "grad_norm": 1.61873459815979,
      "learning_rate": 0.00012733473116488387,
      "loss": 0.9288,
      "step": 9383
    },
    {
      "epoch": 1.3192745676929565,
      "grad_norm": 1.3111106157302856,
      "learning_rate": 0.00012711104152378014,
      "loss": 0.965,
      "step": 9384
    },
    {
      "epoch": 1.3194151553493603,
      "grad_norm": 1.4676048755645752,
      "learning_rate": 0.00012688720536908202,
      "loss": 1.0662,
      "step": 9385
    },
    {
      "epoch": 1.319555743005764,
      "grad_norm": 1.874582290649414,
      "learning_rate": 0.00012666322391044582,
      "loss": 1.1403,
      "step": 9386
    },
    {
      "epoch": 1.3196963306621678,
      "grad_norm": 1.3161892890930176,
      "learning_rate": 0.00012643909835831292,
      "loss": 1.1689,
      "step": 9387
    },
    {
      "epoch": 1.3198369183185716,
      "grad_norm": 1.646425724029541,
      "learning_rate": 0.00012621482992390386,
      "loss": 1.0756,
      "step": 9388
    },
    {
      "epoch": 1.3199775059749754,
      "grad_norm": 1.7499572038650513,
      "learning_rate": 0.00012599041981920995,
      "loss": 0.9847,
      "step": 9389
    },
    {
      "epoch": 1.3201180936313792,
      "grad_norm": 2.094271183013916,
      "learning_rate": 0.0001257658692569901,
      "loss": 1.0202,
      "step": 9390
    },
    {
      "epoch": 1.320258681287783,
      "grad_norm": 1.839011788368225,
      "learning_rate": 0.00012554117945076104,
      "loss": 0.958,
      "step": 9391
    },
    {
      "epoch": 1.3203992689441866,
      "grad_norm": 1.573133945465088,
      "learning_rate": 0.00012531635161479226,
      "loss": 1.1984,
      "step": 9392
    },
    {
      "epoch": 1.3205398566005906,
      "grad_norm": 1.42122220993042,
      "learning_rate": 0.00012509138696409957,
      "loss": 1.1675,
      "step": 9393
    },
    {
      "epoch": 1.3206804442569942,
      "grad_norm": 1.4402559995651245,
      "learning_rate": 0.00012486628671443708,
      "loss": 1.1484,
      "step": 9394
    },
    {
      "epoch": 1.320821031913398,
      "grad_norm": 1.5620909929275513,
      "learning_rate": 0.00012464105208229279,
      "loss": 1.1371,
      "step": 9395
    },
    {
      "epoch": 1.3209616195698017,
      "grad_norm": 1.5208712816238403,
      "learning_rate": 0.00012441568428488053,
      "loss": 1.1135,
      "step": 9396
    },
    {
      "epoch": 1.3211022072262055,
      "grad_norm": 1.3221322298049927,
      "learning_rate": 0.00012419018454013352,
      "loss": 1.1363,
      "step": 9397
    },
    {
      "epoch": 1.3212427948826093,
      "grad_norm": 1.7388943433761597,
      "learning_rate": 0.00012396455406669897,
      "loss": 1.1483,
      "step": 9398
    },
    {
      "epoch": 1.321383382539013,
      "grad_norm": 1.8952248096466064,
      "learning_rate": 0.00012373879408393,
      "loss": 1.2149,
      "step": 9399
    },
    {
      "epoch": 1.321523970195417,
      "grad_norm": 1.6662571430206299,
      "learning_rate": 0.00012351290581187887,
      "loss": 1.0474,
      "step": 9400
    },
    {
      "epoch": 1.3216645578518207,
      "grad_norm": 1.682220697402954,
      "learning_rate": 0.00012328689047129215,
      "loss": 0.9456,
      "step": 9401
    },
    {
      "epoch": 1.3218051455082245,
      "grad_norm": 1.354252815246582,
      "learning_rate": 0.00012306074928360286,
      "loss": 1.2261,
      "step": 9402
    },
    {
      "epoch": 1.321945733164628,
      "grad_norm": 1.6057603359222412,
      "learning_rate": 0.00012283448347092407,
      "loss": 1.2232,
      "step": 9403
    },
    {
      "epoch": 1.3220863208210318,
      "grad_norm": 1.5512644052505493,
      "learning_rate": 0.0001226080942560423,
      "loss": 1.1183,
      "step": 9404
    },
    {
      "epoch": 1.3222269084774356,
      "grad_norm": 1.4755257368087769,
      "learning_rate": 0.00012238158286241108,
      "loss": 1.197,
      "step": 9405
    },
    {
      "epoch": 1.3223674961338394,
      "grad_norm": 1.794602632522583,
      "learning_rate": 0.00012215495051414406,
      "loss": 1.0878,
      "step": 9406
    },
    {
      "epoch": 1.3225080837902432,
      "grad_norm": 1.9307129383087158,
      "learning_rate": 0.00012192819843600876,
      "loss": 1.2145,
      "step": 9407
    },
    {
      "epoch": 1.322648671446647,
      "grad_norm": 1.6518027782440186,
      "learning_rate": 0.00012170132785341955,
      "loss": 1.1722,
      "step": 9408
    },
    {
      "epoch": 1.3227892591030508,
      "grad_norm": 1.5760105848312378,
      "learning_rate": 0.00012147433999243127,
      "loss": 1.1595,
      "step": 9409
    },
    {
      "epoch": 1.3229298467594546,
      "grad_norm": 1.9017776250839233,
      "learning_rate": 0.00012124723607973303,
      "loss": 1.1189,
      "step": 9410
    },
    {
      "epoch": 1.3230704344158584,
      "grad_norm": 1.8490780591964722,
      "learning_rate": 0.0001210200173426399,
      "loss": 0.9835,
      "step": 9411
    },
    {
      "epoch": 1.323211022072262,
      "grad_norm": 1.56472647190094,
      "learning_rate": 0.00012079268500908826,
      "loss": 1.0221,
      "step": 9412
    },
    {
      "epoch": 1.323351609728666,
      "grad_norm": 1.7130532264709473,
      "learning_rate": 0.00012056524030762912,
      "loss": 1.2171,
      "step": 9413
    },
    {
      "epoch": 1.3234921973850695,
      "grad_norm": 1.8801004886627197,
      "learning_rate": 0.0001203376844674196,
      "loss": 1.027,
      "step": 9414
    },
    {
      "epoch": 1.3236327850414733,
      "grad_norm": 1.4992722272872925,
      "learning_rate": 0.00012011001871821819,
      "loss": 1.1688,
      "step": 9415
    },
    {
      "epoch": 1.323773372697877,
      "grad_norm": 1.6144770383834839,
      "learning_rate": 0.00011988224429037633,
      "loss": 1.1053,
      "step": 9416
    },
    {
      "epoch": 1.3239139603542809,
      "grad_norm": 1.6734970808029175,
      "learning_rate": 0.00011965436241483381,
      "loss": 1.1502,
      "step": 9417
    },
    {
      "epoch": 1.3240545480106847,
      "grad_norm": 1.7606569528579712,
      "learning_rate": 0.00011942637432311072,
      "loss": 1.0525,
      "step": 9418
    },
    {
      "epoch": 1.3241951356670885,
      "grad_norm": 1.564880609512329,
      "learning_rate": 0.0001191982812473012,
      "loss": 1.1197,
      "step": 9419
    },
    {
      "epoch": 1.3243357233234923,
      "grad_norm": 1.395199179649353,
      "learning_rate": 0.00011897008442006665,
      "loss": 1.1005,
      "step": 9420
    },
    {
      "epoch": 1.324476310979896,
      "grad_norm": 1.7993050813674927,
      "learning_rate": 0.00011874178507462934,
      "loss": 1.1852,
      "step": 9421
    },
    {
      "epoch": 1.3246168986362998,
      "grad_norm": 1.5991263389587402,
      "learning_rate": 0.00011851338444476539,
      "loss": 0.984,
      "step": 9422
    },
    {
      "epoch": 1.3247574862927034,
      "grad_norm": 1.6588822603225708,
      "learning_rate": 0.0001182848837647984,
      "loss": 1.0208,
      "step": 9423
    },
    {
      "epoch": 1.3248980739491072,
      "grad_norm": 1.55272376537323,
      "learning_rate": 0.00011805628426959259,
      "loss": 1.2799,
      "step": 9424
    },
    {
      "epoch": 1.325038661605511,
      "grad_norm": 1.367169737815857,
      "learning_rate": 0.00011782758719454625,
      "loss": 1.1944,
      "step": 9425
    },
    {
      "epoch": 1.3251792492619148,
      "grad_norm": 1.481650471687317,
      "learning_rate": 0.00011759879377558531,
      "loss": 1.0764,
      "step": 9426
    },
    {
      "epoch": 1.3253198369183186,
      "grad_norm": 1.6161980628967285,
      "learning_rate": 0.00011736990524915504,
      "loss": 1.0904,
      "step": 9427
    },
    {
      "epoch": 1.3254604245747224,
      "grad_norm": 1.836404800415039,
      "learning_rate": 0.00011714092285221668,
      "loss": 1.0821,
      "step": 9428
    },
    {
      "epoch": 1.3256010122311261,
      "grad_norm": 1.6841974258422852,
      "learning_rate": 0.00011691184782223747,
      "loss": 1.2418,
      "step": 9429
    },
    {
      "epoch": 1.32574159988753,
      "grad_norm": 1.5066890716552734,
      "learning_rate": 0.00011668268139718555,
      "loss": 1.162,
      "step": 9430
    },
    {
      "epoch": 1.3258821875439337,
      "grad_norm": 1.3596839904785156,
      "learning_rate": 0.0001164534248155233,
      "loss": 1.2526,
      "step": 9431
    },
    {
      "epoch": 1.3260227752003373,
      "grad_norm": 1.804994821548462,
      "learning_rate": 0.0001162240793161994,
      "loss": 0.9165,
      "step": 9432
    },
    {
      "epoch": 1.3261633628567413,
      "grad_norm": 1.619978666305542,
      "learning_rate": 0.00011599464613864397,
      "loss": 1.1199,
      "step": 9433
    },
    {
      "epoch": 1.3263039505131449,
      "grad_norm": 1.6462206840515137,
      "learning_rate": 0.00011576512652276065,
      "loss": 1.1814,
      "step": 9434
    },
    {
      "epoch": 1.3264445381695487,
      "grad_norm": 1.683506965637207,
      "learning_rate": 0.00011553552170891992,
      "loss": 1.0861,
      "step": 9435
    },
    {
      "epoch": 1.3265851258259524,
      "grad_norm": 1.5136828422546387,
      "learning_rate": 0.00011530583293795405,
      "loss": 1.0139,
      "step": 9436
    },
    {
      "epoch": 1.3267257134823562,
      "grad_norm": 1.6761689186096191,
      "learning_rate": 0.00011507606145114687,
      "loss": 1.1652,
      "step": 9437
    },
    {
      "epoch": 1.32686630113876,
      "grad_norm": 1.6345937252044678,
      "learning_rate": 0.00011484620849023071,
      "loss": 0.9027,
      "step": 9438
    },
    {
      "epoch": 1.3270068887951638,
      "grad_norm": 1.9024393558502197,
      "learning_rate": 0.00011461627529737785,
      "loss": 1.1798,
      "step": 9439
    },
    {
      "epoch": 1.3271474764515676,
      "grad_norm": 1.4443153142929077,
      "learning_rate": 0.00011438626311519407,
      "loss": 1.2592,
      "step": 9440
    },
    {
      "epoch": 1.3272880641079714,
      "grad_norm": 2.1203629970550537,
      "learning_rate": 0.00011415617318671212,
      "loss": 1.0095,
      "step": 9441
    },
    {
      "epoch": 1.3274286517643752,
      "grad_norm": 1.63539457321167,
      "learning_rate": 0.00011392600675538487,
      "loss": 1.0187,
      "step": 9442
    },
    {
      "epoch": 1.3275692394207788,
      "grad_norm": 1.4429833889007568,
      "learning_rate": 0.00011369576506507862,
      "loss": 0.964,
      "step": 9443
    },
    {
      "epoch": 1.3277098270771825,
      "grad_norm": 1.4200358390808105,
      "learning_rate": 0.0001134654493600664,
      "loss": 0.9809,
      "step": 9444
    },
    {
      "epoch": 1.3278504147335863,
      "grad_norm": 1.6211297512054443,
      "learning_rate": 0.00011323506088502122,
      "loss": 1.1369,
      "step": 9445
    },
    {
      "epoch": 1.3279910023899901,
      "grad_norm": 1.4755951166152954,
      "learning_rate": 0.0001130046008850094,
      "loss": 0.9692,
      "step": 9446
    },
    {
      "epoch": 1.328131590046394,
      "grad_norm": 1.7392510175704956,
      "learning_rate": 0.00011277407060548404,
      "loss": 0.9048,
      "step": 9447
    },
    {
      "epoch": 1.3282721777027977,
      "grad_norm": 1.6646883487701416,
      "learning_rate": 0.00011254347129227706,
      "loss": 1.1656,
      "step": 9448
    },
    {
      "epoch": 1.3284127653592015,
      "grad_norm": 1.8291027545928955,
      "learning_rate": 0.00011231280419159456,
      "loss": 1.1682,
      "step": 9449
    },
    {
      "epoch": 1.3285533530156053,
      "grad_norm": 1.5661543607711792,
      "learning_rate": 0.00011208207055000817,
      "loss": 1.0375,
      "step": 9450
    },
    {
      "epoch": 1.328693940672009,
      "grad_norm": 1.6783795356750488,
      "learning_rate": 0.00011185127161445023,
      "loss": 1.0287,
      "step": 9451
    },
    {
      "epoch": 1.3288345283284126,
      "grad_norm": 1.5617811679840088,
      "learning_rate": 0.00011162040863220492,
      "loss": 1.1817,
      "step": 9452
    },
    {
      "epoch": 1.3289751159848167,
      "grad_norm": 1.4939173460006714,
      "learning_rate": 0.00011138948285090311,
      "loss": 1.2864,
      "step": 9453
    },
    {
      "epoch": 1.3291157036412202,
      "grad_norm": 1.4560140371322632,
      "learning_rate": 0.00011115849551851418,
      "loss": 1.0879,
      "step": 9454
    },
    {
      "epoch": 1.329256291297624,
      "grad_norm": 1.7495492696762085,
      "learning_rate": 0.00011092744788334107,
      "loss": 1.1035,
      "step": 9455
    },
    {
      "epoch": 1.3293968789540278,
      "grad_norm": 1.4910743236541748,
      "learning_rate": 0.00011069634119401229,
      "loss": 1.0982,
      "step": 9456
    },
    {
      "epoch": 1.3295374666104316,
      "grad_norm": 1.3742424249649048,
      "learning_rate": 0.00011046517669947549,
      "loss": 1.0766,
      "step": 9457
    },
    {
      "epoch": 1.3296780542668354,
      "grad_norm": 1.4461673498153687,
      "learning_rate": 0.00011023395564899075,
      "loss": 0.9619,
      "step": 9458
    },
    {
      "epoch": 1.3298186419232392,
      "grad_norm": 1.803123116493225,
      "learning_rate": 0.00011000267929212371,
      "loss": 1.1004,
      "step": 9459
    },
    {
      "epoch": 1.329959229579643,
      "grad_norm": 1.3984524011611938,
      "learning_rate": 0.00010977134887873898,
      "loss": 1.2968,
      "step": 9460
    },
    {
      "epoch": 1.3300998172360468,
      "grad_norm": 1.388291835784912,
      "learning_rate": 0.00010953996565899325,
      "loss": 0.9849,
      "step": 9461
    },
    {
      "epoch": 1.3302404048924505,
      "grad_norm": 1.6595875024795532,
      "learning_rate": 0.0001093085308833286,
      "loss": 0.9872,
      "step": 9462
    },
    {
      "epoch": 1.3303809925488541,
      "grad_norm": 1.8104201555252075,
      "learning_rate": 0.0001090770458024661,
      "loss": 1.1379,
      "step": 9463
    },
    {
      "epoch": 1.330521580205258,
      "grad_norm": 1.6350440979003906,
      "learning_rate": 0.00010884551166739756,
      "loss": 1.0307,
      "step": 9464
    },
    {
      "epoch": 1.3306621678616617,
      "grad_norm": 1.5503790378570557,
      "learning_rate": 0.00010861392972938067,
      "loss": 1.1739,
      "step": 9465
    },
    {
      "epoch": 1.3308027555180655,
      "grad_norm": 1.7434351444244385,
      "learning_rate": 0.00010838230123993214,
      "loss": 1.0536,
      "step": 9466
    },
    {
      "epoch": 1.3309433431744693,
      "grad_norm": 1.5016206502914429,
      "learning_rate": 0.00010815062745081919,
      "loss": 1.0757,
      "step": 9467
    },
    {
      "epoch": 1.331083930830873,
      "grad_norm": 1.6984447240829468,
      "learning_rate": 0.00010791890961405416,
      "loss": 0.9219,
      "step": 9468
    },
    {
      "epoch": 1.3312245184872769,
      "grad_norm": 1.6229923963546753,
      "learning_rate": 0.00010768714898188785,
      "loss": 1.1921,
      "step": 9469
    },
    {
      "epoch": 1.3313651061436806,
      "grad_norm": 2.095897912979126,
      "learning_rate": 0.00010745534680680125,
      "loss": 1.1267,
      "step": 9470
    },
    {
      "epoch": 1.3315056938000844,
      "grad_norm": 1.4515881538391113,
      "learning_rate": 0.00010722350434150091,
      "loss": 1.0658,
      "step": 9471
    },
    {
      "epoch": 1.331646281456488,
      "grad_norm": 1.6726460456848145,
      "learning_rate": 0.00010699162283891078,
      "loss": 1.0719,
      "step": 9472
    },
    {
      "epoch": 1.331786869112892,
      "grad_norm": 2.4278388023376465,
      "learning_rate": 0.00010675970355216538,
      "loss": 1.1315,
      "step": 9473
    },
    {
      "epoch": 1.3319274567692956,
      "grad_norm": 1.5227367877960205,
      "learning_rate": 0.00010652774773460488,
      "loss": 1.0794,
      "step": 9474
    },
    {
      "epoch": 1.3320680444256994,
      "grad_norm": 1.9727838039398193,
      "learning_rate": 0.00010629575663976474,
      "loss": 1.05,
      "step": 9475
    },
    {
      "epoch": 1.3322086320821032,
      "grad_norm": 1.5757801532745361,
      "learning_rate": 0.0001060637315213725,
      "loss": 1.103,
      "step": 9476
    },
    {
      "epoch": 1.332349219738507,
      "grad_norm": 1.872074842453003,
      "learning_rate": 0.00010583167363333914,
      "loss": 0.9477,
      "step": 9477
    },
    {
      "epoch": 1.3324898073949107,
      "grad_norm": 2.0178639888763428,
      "learning_rate": 0.00010559958422975285,
      "loss": 0.8554,
      "step": 9478
    },
    {
      "epoch": 1.3326303950513145,
      "grad_norm": 1.9439514875411987,
      "learning_rate": 0.00010536746456487237,
      "loss": 1.0453,
      "step": 9479
    },
    {
      "epoch": 1.3327709827077183,
      "grad_norm": 2.23246431350708,
      "learning_rate": 0.00010513531589311865,
      "loss": 0.8919,
      "step": 9480
    },
    {
      "epoch": 1.3329115703641221,
      "grad_norm": 1.8404288291931152,
      "learning_rate": 0.00010490313946907149,
      "loss": 1.2077,
      "step": 9481
    },
    {
      "epoch": 1.333052158020526,
      "grad_norm": 1.5528390407562256,
      "learning_rate": 0.00010467093654745923,
      "loss": 1.1341,
      "step": 9482
    },
    {
      "epoch": 1.3331927456769295,
      "grad_norm": 1.4030323028564453,
      "learning_rate": 0.00010443870838315384,
      "loss": 1.1653,
      "step": 9483
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 1.6467825174331665,
      "learning_rate": 0.00010420645623116359,
      "loss": 1.067,
      "step": 9484
    },
    {
      "epoch": 1.333473920989737,
      "grad_norm": 2.0305659770965576,
      "learning_rate": 0.00010397418134662685,
      "loss": 0.874,
      "step": 9485
    },
    {
      "epoch": 1.3336145086461408,
      "grad_norm": 1.5735069513320923,
      "learning_rate": 0.00010374188498480371,
      "loss": 1.1832,
      "step": 9486
    },
    {
      "epoch": 1.3337550963025446,
      "grad_norm": 2.0645854473114014,
      "learning_rate": 0.00010350956840107142,
      "loss": 1.1224,
      "step": 9487
    },
    {
      "epoch": 1.3338956839589484,
      "grad_norm": 1.4065673351287842,
      "learning_rate": 0.00010327723285091579,
      "loss": 1.0737,
      "step": 9488
    },
    {
      "epoch": 1.3340362716153522,
      "grad_norm": 1.4008294343948364,
      "learning_rate": 0.00010304487958992622,
      "loss": 0.9628,
      "step": 9489
    },
    {
      "epoch": 1.334176859271756,
      "grad_norm": 1.3528718948364258,
      "learning_rate": 0.00010281250987378705,
      "loss": 1.1246,
      "step": 9490
    },
    {
      "epoch": 1.3343174469281598,
      "grad_norm": 1.55100417137146,
      "learning_rate": 0.00010258012495827091,
      "loss": 1.0452,
      "step": 9491
    },
    {
      "epoch": 1.3344580345845634,
      "grad_norm": 1.563324213027954,
      "learning_rate": 0.0001023477260992336,
      "loss": 1.1804,
      "step": 9492
    },
    {
      "epoch": 1.3345986222409674,
      "grad_norm": 1.4059561491012573,
      "learning_rate": 0.00010211531455260592,
      "loss": 1.098,
      "step": 9493
    },
    {
      "epoch": 1.334739209897371,
      "grad_norm": 1.4780479669570923,
      "learning_rate": 0.00010188289157438725,
      "loss": 1.1911,
      "step": 9494
    },
    {
      "epoch": 1.3348797975537747,
      "grad_norm": 1.557391881942749,
      "learning_rate": 0.00010165045842063872,
      "loss": 1.1305,
      "step": 9495
    },
    {
      "epoch": 1.3350203852101785,
      "grad_norm": 1.631861925125122,
      "learning_rate": 0.00010141801634747647,
      "loss": 0.9837,
      "step": 9496
    },
    {
      "epoch": 1.3351609728665823,
      "grad_norm": 1.512757420539856,
      "learning_rate": 0.00010118556661106486,
      "loss": 1.048,
      "step": 9497
    },
    {
      "epoch": 1.335301560522986,
      "grad_norm": 1.4578616619110107,
      "learning_rate": 0.00010095311046760965,
      "loss": 1.0804,
      "step": 9498
    },
    {
      "epoch": 1.3354421481793899,
      "grad_norm": 1.4121650457382202,
      "learning_rate": 0.00010072064917335118,
      "loss": 1.0818,
      "step": 9499
    },
    {
      "epoch": 1.3355827358357937,
      "grad_norm": 1.517479658126831,
      "learning_rate": 0.00010048818398455768,
      "loss": 1.0629,
      "step": 9500
    },
    {
      "epoch": 1.3355827358357937,
      "eval_loss": 1.157934308052063,
      "eval_runtime": 771.8826,
      "eval_samples_per_second": 16.383,
      "eval_steps_per_second": 8.192,
      "step": 9500
    },
    {
      "epoch": 1.3357233234921975,
      "grad_norm": 1.7253167629241943,
      "learning_rate": 0.00010025571615751881,
      "loss": 1.071,
      "step": 9501
    },
    {
      "epoch": 1.3358639111486013,
      "grad_norm": 1.6272683143615723,
      "learning_rate": 0.00010002324694853733,
      "loss": 1.0018,
      "step": 9502
    },
    {
      "epoch": 1.3360044988050048,
      "grad_norm": 1.6590362787246704,
      "learning_rate": 9.979077761392425e-05,
      "loss": 1.0129,
      "step": 9503
    },
    {
      "epoch": 1.3361450864614086,
      "grad_norm": 1.6174225807189941,
      "learning_rate": 9.955830940999193e-05,
      "loss": 1.0363,
      "step": 9504
    },
    {
      "epoch": 1.3362856741178124,
      "grad_norm": 1.8192040920257568,
      "learning_rate": 9.932584359304561e-05,
      "loss": 1.024,
      "step": 9505
    },
    {
      "epoch": 1.3364262617742162,
      "grad_norm": 1.4731851816177368,
      "learning_rate": 9.909338141937827e-05,
      "loss": 1.099,
      "step": 9506
    },
    {
      "epoch": 1.33656684943062,
      "grad_norm": 1.8071084022521973,
      "learning_rate": 9.886092414526217e-05,
      "loss": 1.1714,
      "step": 9507
    },
    {
      "epoch": 1.3367074370870238,
      "grad_norm": 1.3570653200149536,
      "learning_rate": 9.862847302694417e-05,
      "loss": 1.223,
      "step": 9508
    },
    {
      "epoch": 1.3368480247434276,
      "grad_norm": 1.7239482402801514,
      "learning_rate": 9.839602932063748e-05,
      "loss": 0.9879,
      "step": 9509
    },
    {
      "epoch": 1.3369886123998314,
      "grad_norm": 1.5980068445205688,
      "learning_rate": 9.816359428251528e-05,
      "loss": 1.1466,
      "step": 9510
    },
    {
      "epoch": 1.3371292000562351,
      "grad_norm": 1.4996106624603271,
      "learning_rate": 9.793116916870354e-05,
      "loss": 1.0743,
      "step": 9511
    },
    {
      "epoch": 1.3372697877126387,
      "grad_norm": 1.7983306646347046,
      "learning_rate": 9.769875523527598e-05,
      "loss": 0.887,
      "step": 9512
    },
    {
      "epoch": 1.3374103753690427,
      "grad_norm": 1.539067029953003,
      "learning_rate": 9.746635373824385e-05,
      "loss": 1.1712,
      "step": 9513
    },
    {
      "epoch": 1.3375509630254463,
      "grad_norm": 1.718502402305603,
      "learning_rate": 9.723396593355251e-05,
      "loss": 1.0573,
      "step": 9514
    },
    {
      "epoch": 1.33769155068185,
      "grad_norm": 1.45846688747406,
      "learning_rate": 9.700159307707309e-05,
      "loss": 1.0935,
      "step": 9515
    },
    {
      "epoch": 1.3378321383382539,
      "grad_norm": 1.778152346611023,
      "learning_rate": 9.676923642459582e-05,
      "loss": 1.0274,
      "step": 9516
    },
    {
      "epoch": 1.3379727259946577,
      "grad_norm": 1.6657971143722534,
      "learning_rate": 9.65368972318238e-05,
      "loss": 1.1117,
      "step": 9517
    },
    {
      "epoch": 1.3381133136510615,
      "grad_norm": 1.679591417312622,
      "learning_rate": 9.630457675436464e-05,
      "loss": 1.1046,
      "step": 9518
    },
    {
      "epoch": 1.3382539013074652,
      "grad_norm": 1.4944400787353516,
      "learning_rate": 9.607227624772557e-05,
      "loss": 1.0125,
      "step": 9519
    },
    {
      "epoch": 1.338394488963869,
      "grad_norm": 1.4799078702926636,
      "learning_rate": 9.583999696730658e-05,
      "loss": 1.1314,
      "step": 9520
    },
    {
      "epoch": 1.3385350766202726,
      "grad_norm": 1.6989697217941284,
      "learning_rate": 9.56077401683919e-05,
      "loss": 1.0765,
      "step": 9521
    },
    {
      "epoch": 1.3386756642766766,
      "grad_norm": 1.493458867073059,
      "learning_rate": 9.537550710614462e-05,
      "loss": 1.1212,
      "step": 9522
    },
    {
      "epoch": 1.3388162519330802,
      "grad_norm": 1.4073708057403564,
      "learning_rate": 9.514329903559988e-05,
      "loss": 1.2563,
      "step": 9523
    },
    {
      "epoch": 1.338956839589484,
      "grad_norm": 1.5164157152175903,
      "learning_rate": 9.491111721165672e-05,
      "loss": 1.0907,
      "step": 9524
    },
    {
      "epoch": 1.3390974272458878,
      "grad_norm": 1.4447633028030396,
      "learning_rate": 9.46789628890734e-05,
      "loss": 1.1762,
      "step": 9525
    },
    {
      "epoch": 1.3392380149022916,
      "grad_norm": 1.6148277521133423,
      "learning_rate": 9.444683732245884e-05,
      "loss": 1.1929,
      "step": 9526
    },
    {
      "epoch": 1.3393786025586953,
      "grad_norm": 1.4486273527145386,
      "learning_rate": 9.421474176626762e-05,
      "loss": 1.2469,
      "step": 9527
    },
    {
      "epoch": 1.3395191902150991,
      "grad_norm": 1.5528985261917114,
      "learning_rate": 9.398267747479143e-05,
      "loss": 0.9865,
      "step": 9528
    },
    {
      "epoch": 1.339659777871503,
      "grad_norm": 1.4761236906051636,
      "learning_rate": 9.375064570215231e-05,
      "loss": 1.0347,
      "step": 9529
    },
    {
      "epoch": 1.3398003655279067,
      "grad_norm": 1.4309605360031128,
      "learning_rate": 9.35186477022976e-05,
      "loss": 1.1552,
      "step": 9530
    },
    {
      "epoch": 1.3399409531843105,
      "grad_norm": 1.633809208869934,
      "learning_rate": 9.32866847289918e-05,
      "loss": 1.0394,
      "step": 9531
    },
    {
      "epoch": 1.340081540840714,
      "grad_norm": 1.4856314659118652,
      "learning_rate": 9.305475803581005e-05,
      "loss": 1.1814,
      "step": 9532
    },
    {
      "epoch": 1.340222128497118,
      "grad_norm": 1.8106780052185059,
      "learning_rate": 9.282286887613185e-05,
      "loss": 1.1198,
      "step": 9533
    },
    {
      "epoch": 1.3403627161535216,
      "grad_norm": 1.4268995523452759,
      "learning_rate": 9.259101850313243e-05,
      "loss": 1.0886,
      "step": 9534
    },
    {
      "epoch": 1.3405033038099254,
      "grad_norm": 1.810339331626892,
      "learning_rate": 9.235920816977952e-05,
      "loss": 1.0226,
      "step": 9535
    },
    {
      "epoch": 1.3406438914663292,
      "grad_norm": 1.633434534072876,
      "learning_rate": 9.212743912882306e-05,
      "loss": 1.1435,
      "step": 9536
    },
    {
      "epoch": 1.340784479122733,
      "grad_norm": 1.4174047708511353,
      "learning_rate": 9.189571263279018e-05,
      "loss": 1.0873,
      "step": 9537
    },
    {
      "epoch": 1.3409250667791368,
      "grad_norm": 1.690847396850586,
      "learning_rate": 9.166402993397809e-05,
      "loss": 1.0043,
      "step": 9538
    },
    {
      "epoch": 1.3410656544355406,
      "grad_norm": 1.3658429384231567,
      "learning_rate": 9.143239228444768e-05,
      "loss": 0.9199,
      "step": 9539
    },
    {
      "epoch": 1.3412062420919444,
      "grad_norm": 1.621811866760254,
      "learning_rate": 9.120080093601533e-05,
      "loss": 1.1816,
      "step": 9540
    },
    {
      "epoch": 1.341346829748348,
      "grad_norm": 1.5044254064559937,
      "learning_rate": 9.096925714024787e-05,
      "loss": 1.1686,
      "step": 9541
    },
    {
      "epoch": 1.341487417404752,
      "grad_norm": 1.314723253250122,
      "learning_rate": 9.073776214845589e-05,
      "loss": 1.0195,
      "step": 9542
    },
    {
      "epoch": 1.3416280050611555,
      "grad_norm": 1.488452672958374,
      "learning_rate": 9.050631721168515e-05,
      "loss": 1.1867,
      "step": 9543
    },
    {
      "epoch": 1.3417685927175593,
      "grad_norm": 1.6063624620437622,
      "learning_rate": 9.027492358071161e-05,
      "loss": 1.179,
      "step": 9544
    },
    {
      "epoch": 1.3419091803739631,
      "grad_norm": 1.5527790784835815,
      "learning_rate": 9.00435825060329e-05,
      "loss": 1.1853,
      "step": 9545
    },
    {
      "epoch": 1.342049768030367,
      "grad_norm": 1.7789068222045898,
      "learning_rate": 8.981229523786372e-05,
      "loss": 1.1788,
      "step": 9546
    },
    {
      "epoch": 1.3421903556867707,
      "grad_norm": 1.612307071685791,
      "learning_rate": 8.958106302612759e-05,
      "loss": 1.0465,
      "step": 9547
    },
    {
      "epoch": 1.3423309433431745,
      "grad_norm": 1.4008742570877075,
      "learning_rate": 8.934988712045053e-05,
      "loss": 1.1391,
      "step": 9548
    },
    {
      "epoch": 1.3424715309995783,
      "grad_norm": 1.581893801689148,
      "learning_rate": 8.911876877015425e-05,
      "loss": 1.0521,
      "step": 9549
    },
    {
      "epoch": 1.342612118655982,
      "grad_norm": 1.4315265417099,
      "learning_rate": 8.88877092242494e-05,
      "loss": 1.1387,
      "step": 9550
    },
    {
      "epoch": 1.3427527063123859,
      "grad_norm": 1.8747894763946533,
      "learning_rate": 8.86567097314289e-05,
      "loss": 1.0006,
      "step": 9551
    },
    {
      "epoch": 1.3428932939687894,
      "grad_norm": 1.5391943454742432,
      "learning_rate": 8.842577154006107e-05,
      "loss": 1.0634,
      "step": 9552
    },
    {
      "epoch": 1.3430338816251934,
      "grad_norm": 1.7833216190338135,
      "learning_rate": 8.819489589818297e-05,
      "loss": 1.24,
      "step": 9553
    },
    {
      "epoch": 1.343174469281597,
      "grad_norm": 1.5512441396713257,
      "learning_rate": 8.796408405349362e-05,
      "loss": 1.2426,
      "step": 9554
    },
    {
      "epoch": 1.3433150569380008,
      "grad_norm": 1.681734561920166,
      "learning_rate": 8.773333725334763e-05,
      "loss": 1.1669,
      "step": 9555
    },
    {
      "epoch": 1.3434556445944046,
      "grad_norm": 1.77606999874115,
      "learning_rate": 8.750265674474705e-05,
      "loss": 0.8487,
      "step": 9556
    },
    {
      "epoch": 1.3435962322508084,
      "grad_norm": 1.6116399765014648,
      "learning_rate": 8.727204377433636e-05,
      "loss": 0.9512,
      "step": 9557
    },
    {
      "epoch": 1.3437368199072122,
      "grad_norm": 1.8106727600097656,
      "learning_rate": 8.704149958839575e-05,
      "loss": 1.2435,
      "step": 9558
    },
    {
      "epoch": 1.343877407563616,
      "grad_norm": 1.3696333169937134,
      "learning_rate": 8.681102543283264e-05,
      "loss": 1.1134,
      "step": 9559
    },
    {
      "epoch": 1.3440179952200197,
      "grad_norm": 1.448880910873413,
      "learning_rate": 8.658062255317672e-05,
      "loss": 1.1394,
      "step": 9560
    },
    {
      "epoch": 1.3441585828764233,
      "grad_norm": 1.7071536779403687,
      "learning_rate": 8.63502921945714e-05,
      "loss": 0.9842,
      "step": 9561
    },
    {
      "epoch": 1.3442991705328273,
      "grad_norm": 1.7495455741882324,
      "learning_rate": 8.612003560176922e-05,
      "loss": 1.0746,
      "step": 9562
    },
    {
      "epoch": 1.344439758189231,
      "grad_norm": 1.3698372840881348,
      "learning_rate": 8.588985401912376e-05,
      "loss": 0.9561,
      "step": 9563
    },
    {
      "epoch": 1.3445803458456347,
      "grad_norm": 2.0306754112243652,
      "learning_rate": 8.565974869058284e-05,
      "loss": 0.9982,
      "step": 9564
    },
    {
      "epoch": 1.3447209335020385,
      "grad_norm": 1.6623512506484985,
      "learning_rate": 8.542972085968328e-05,
      "loss": 0.9604,
      "step": 9565
    },
    {
      "epoch": 1.3448615211584423,
      "grad_norm": 1.777829647064209,
      "learning_rate": 8.519977176954235e-05,
      "loss": 0.995,
      "step": 9566
    },
    {
      "epoch": 1.345002108814846,
      "grad_norm": 1.6249463558197021,
      "learning_rate": 8.496990266285109e-05,
      "loss": 1.1755,
      "step": 9567
    },
    {
      "epoch": 1.3451426964712498,
      "grad_norm": 1.7780935764312744,
      "learning_rate": 8.474011478186938e-05,
      "loss": 1.1681,
      "step": 9568
    },
    {
      "epoch": 1.3452832841276536,
      "grad_norm": 1.5661537647247314,
      "learning_rate": 8.451040936841775e-05,
      "loss": 1.121,
      "step": 9569
    },
    {
      "epoch": 1.3454238717840574,
      "grad_norm": 1.4351078271865845,
      "learning_rate": 8.428078766387108e-05,
      "loss": 1.3156,
      "step": 9570
    },
    {
      "epoch": 1.3455644594404612,
      "grad_norm": 1.3776545524597168,
      "learning_rate": 8.405125090915223e-05,
      "loss": 1.1227,
      "step": 9571
    },
    {
      "epoch": 1.3457050470968648,
      "grad_norm": 1.5189664363861084,
      "learning_rate": 8.382180034472353e-05,
      "loss": 1.0929,
      "step": 9572
    },
    {
      "epoch": 1.3458456347532688,
      "grad_norm": 1.6094962358474731,
      "learning_rate": 8.359243721058368e-05,
      "loss": 1.0873,
      "step": 9573
    },
    {
      "epoch": 1.3459862224096724,
      "grad_norm": 1.6977248191833496,
      "learning_rate": 8.336316274625742e-05,
      "loss": 0.8821,
      "step": 9574
    },
    {
      "epoch": 1.3461268100660762,
      "grad_norm": 1.8733285665512085,
      "learning_rate": 8.31339781907907e-05,
      "loss": 1.1325,
      "step": 9575
    },
    {
      "epoch": 1.34626739772248,
      "grad_norm": 1.8915820121765137,
      "learning_rate": 8.290488478274394e-05,
      "loss": 1.0609,
      "step": 9576
    },
    {
      "epoch": 1.3464079853788837,
      "grad_norm": 1.5205413103103638,
      "learning_rate": 8.267588376018391e-05,
      "loss": 1.2094,
      "step": 9577
    },
    {
      "epoch": 1.3465485730352875,
      "grad_norm": 1.8174521923065186,
      "learning_rate": 8.244697636067913e-05,
      "loss": 0.9862,
      "step": 9578
    },
    {
      "epoch": 1.3466891606916913,
      "grad_norm": 1.608808159828186,
      "learning_rate": 8.221816382129152e-05,
      "loss": 0.8461,
      "step": 9579
    },
    {
      "epoch": 1.346829748348095,
      "grad_norm": 1.8171790838241577,
      "learning_rate": 8.19894473785714e-05,
      "loss": 1.2362,
      "step": 9580
    },
    {
      "epoch": 1.3469703360044987,
      "grad_norm": 1.5077929496765137,
      "learning_rate": 8.176082826854865e-05,
      "loss": 1.1311,
      "step": 9581
    },
    {
      "epoch": 1.3471109236609027,
      "grad_norm": 1.6913639307022095,
      "learning_rate": 8.153230772672791e-05,
      "loss": 1.0281,
      "step": 9582
    },
    {
      "epoch": 1.3472515113173062,
      "grad_norm": 1.8058322668075562,
      "learning_rate": 8.130388698808008e-05,
      "loss": 1.0742,
      "step": 9583
    },
    {
      "epoch": 1.34739209897371,
      "grad_norm": 1.6250094175338745,
      "learning_rate": 8.107556728703772e-05,
      "loss": 1.2158,
      "step": 9584
    },
    {
      "epoch": 1.3475326866301138,
      "grad_norm": 1.6050570011138916,
      "learning_rate": 8.084734985748704e-05,
      "loss": 1.01,
      "step": 9585
    },
    {
      "epoch": 1.3476732742865176,
      "grad_norm": 1.7518682479858398,
      "learning_rate": 8.061923593276155e-05,
      "loss": 1.0792,
      "step": 9586
    },
    {
      "epoch": 1.3478138619429214,
      "grad_norm": 1.4534181356430054,
      "learning_rate": 8.039122674563538e-05,
      "loss": 0.9626,
      "step": 9587
    },
    {
      "epoch": 1.3479544495993252,
      "grad_norm": 1.5650901794433594,
      "learning_rate": 8.016332352831668e-05,
      "loss": 1.0057,
      "step": 9588
    },
    {
      "epoch": 1.348095037255729,
      "grad_norm": 1.627394437789917,
      "learning_rate": 7.993552751244086e-05,
      "loss": 1.1244,
      "step": 9589
    },
    {
      "epoch": 1.3482356249121328,
      "grad_norm": 1.8329957723617554,
      "learning_rate": 7.970783992906403e-05,
      "loss": 1.291,
      "step": 9590
    },
    {
      "epoch": 1.3483762125685366,
      "grad_norm": 1.9071922302246094,
      "learning_rate": 7.94802620086563e-05,
      "loss": 1.1351,
      "step": 9591
    },
    {
      "epoch": 1.3485168002249401,
      "grad_norm": 1.5431113243103027,
      "learning_rate": 7.92527949810955e-05,
      "loss": 1.0239,
      "step": 9592
    },
    {
      "epoch": 1.3486573878813441,
      "grad_norm": 1.6765543222427368,
      "learning_rate": 7.902544007565908e-05,
      "loss": 1.0481,
      "step": 9593
    },
    {
      "epoch": 1.3487979755377477,
      "grad_norm": 1.4884288311004639,
      "learning_rate": 7.879819852101968e-05,
      "loss": 1.1904,
      "step": 9594
    },
    {
      "epoch": 1.3489385631941515,
      "grad_norm": 1.3246990442276,
      "learning_rate": 7.85710715452366e-05,
      "loss": 1.1131,
      "step": 9595
    },
    {
      "epoch": 1.3490791508505553,
      "grad_norm": 1.5413179397583008,
      "learning_rate": 7.834406037575104e-05,
      "loss": 1.0622,
      "step": 9596
    },
    {
      "epoch": 1.349219738506959,
      "grad_norm": 1.7990578413009644,
      "learning_rate": 7.811716623937724e-05,
      "loss": 1.1964,
      "step": 9597
    },
    {
      "epoch": 1.3493603261633629,
      "grad_norm": 1.4854201078414917,
      "learning_rate": 7.789039036229774e-05,
      "loss": 0.9601,
      "step": 9598
    },
    {
      "epoch": 1.3495009138197667,
      "grad_norm": 1.5121692419052124,
      "learning_rate": 7.766373397005489e-05,
      "loss": 0.9775,
      "step": 9599
    },
    {
      "epoch": 1.3496415014761705,
      "grad_norm": 1.7007043361663818,
      "learning_rate": 7.743719828754639e-05,
      "loss": 1.0361,
      "step": 9600
    },
    {
      "epoch": 1.349782089132574,
      "grad_norm": 1.5836018323898315,
      "learning_rate": 7.721078453901723e-05,
      "loss": 1.0359,
      "step": 9601
    },
    {
      "epoch": 1.349922676788978,
      "grad_norm": 1.6065958738327026,
      "learning_rate": 7.698449394805312e-05,
      "loss": 1.0927,
      "step": 9602
    },
    {
      "epoch": 1.3500632644453816,
      "grad_norm": 1.6448289155960083,
      "learning_rate": 7.675832773757559e-05,
      "loss": 1.1075,
      "step": 9603
    },
    {
      "epoch": 1.3502038521017854,
      "grad_norm": 1.4183686971664429,
      "learning_rate": 7.653228712983187e-05,
      "loss": 1.1698,
      "step": 9604
    },
    {
      "epoch": 1.3503444397581892,
      "grad_norm": 1.6227434873580933,
      "learning_rate": 7.630637334639184e-05,
      "loss": 0.9957,
      "step": 9605
    },
    {
      "epoch": 1.350485027414593,
      "grad_norm": 1.5018582344055176,
      "learning_rate": 7.608058760813965e-05,
      "loss": 1.0085,
      "step": 9606
    },
    {
      "epoch": 1.3506256150709968,
      "grad_norm": 1.6567325592041016,
      "learning_rate": 7.585493113526742e-05,
      "loss": 1.1223,
      "step": 9607
    },
    {
      "epoch": 1.3507662027274006,
      "grad_norm": 1.6160175800323486,
      "learning_rate": 7.562940514726874e-05,
      "loss": 1.1248,
      "step": 9608
    },
    {
      "epoch": 1.3509067903838043,
      "grad_norm": 1.5922893285751343,
      "learning_rate": 7.540401086293236e-05,
      "loss": 1.0202,
      "step": 9609
    },
    {
      "epoch": 1.3510473780402081,
      "grad_norm": 1.6551892757415771,
      "learning_rate": 7.51787495003339e-05,
      "loss": 1.1392,
      "step": 9610
    },
    {
      "epoch": 1.351187965696612,
      "grad_norm": 1.5558099746704102,
      "learning_rate": 7.495362227683271e-05,
      "loss": 0.994,
      "step": 9611
    },
    {
      "epoch": 1.3513285533530155,
      "grad_norm": 1.8585494756698608,
      "learning_rate": 7.472863040906183e-05,
      "loss": 0.986,
      "step": 9612
    },
    {
      "epoch": 1.3514691410094195,
      "grad_norm": 1.6992071866989136,
      "learning_rate": 7.450377511292323e-05,
      "loss": 1.0703,
      "step": 9613
    },
    {
      "epoch": 1.351609728665823,
      "grad_norm": 1.4120666980743408,
      "learning_rate": 7.427905760358104e-05,
      "loss": 0.9884,
      "step": 9614
    },
    {
      "epoch": 1.3517503163222269,
      "grad_norm": 1.7382595539093018,
      "learning_rate": 7.405447909545383e-05,
      "loss": 1.1949,
      "step": 9615
    },
    {
      "epoch": 1.3518909039786307,
      "grad_norm": 1.237067699432373,
      "learning_rate": 7.383004080220996e-05,
      "loss": 1.3431,
      "step": 9616
    },
    {
      "epoch": 1.3520314916350344,
      "grad_norm": 1.8579121828079224,
      "learning_rate": 7.360574393675974e-05,
      "loss": 0.9274,
      "step": 9617
    },
    {
      "epoch": 1.3521720792914382,
      "grad_norm": 1.3530969619750977,
      "learning_rate": 7.338158971124878e-05,
      "loss": 1.106,
      "step": 9618
    },
    {
      "epoch": 1.352312666947842,
      "grad_norm": 1.4662383794784546,
      "learning_rate": 7.315757933705326e-05,
      "loss": 1.0841,
      "step": 9619
    },
    {
      "epoch": 1.3524532546042458,
      "grad_norm": 1.6055690050125122,
      "learning_rate": 7.293371402476983e-05,
      "loss": 0.9641,
      "step": 9620
    },
    {
      "epoch": 1.3525938422606494,
      "grad_norm": 1.5013391971588135,
      "learning_rate": 7.270999498421263e-05,
      "loss": 1.1245,
      "step": 9621
    },
    {
      "epoch": 1.3527344299170534,
      "grad_norm": 1.5423126220703125,
      "learning_rate": 7.248642342440496e-05,
      "loss": 0.9969,
      "step": 9622
    },
    {
      "epoch": 1.352875017573457,
      "grad_norm": 1.4252055883407593,
      "learning_rate": 7.22630005535731e-05,
      "loss": 1.0594,
      "step": 9623
    },
    {
      "epoch": 1.3530156052298608,
      "grad_norm": 1.81932532787323,
      "learning_rate": 7.203972757913982e-05,
      "loss": 1.0413,
      "step": 9624
    },
    {
      "epoch": 1.3531561928862645,
      "grad_norm": 1.8316352367401123,
      "learning_rate": 7.18166057077177e-05,
      "loss": 0.9823,
      "step": 9625
    },
    {
      "epoch": 1.3532967805426683,
      "grad_norm": 1.6449286937713623,
      "learning_rate": 7.159363614510287e-05,
      "loss": 1.1819,
      "step": 9626
    },
    {
      "epoch": 1.3534373681990721,
      "grad_norm": 2.024200916290283,
      "learning_rate": 7.137082009626828e-05,
      "loss": 1.0874,
      "step": 9627
    },
    {
      "epoch": 1.353577955855476,
      "grad_norm": 1.4115034341812134,
      "learning_rate": 7.114815876535726e-05,
      "loss": 1.1384,
      "step": 9628
    },
    {
      "epoch": 1.3537185435118797,
      "grad_norm": 1.6122496128082275,
      "learning_rate": 7.092565335567704e-05,
      "loss": 0.9658,
      "step": 9629
    },
    {
      "epoch": 1.3538591311682835,
      "grad_norm": 1.7115747928619385,
      "learning_rate": 7.070330506969256e-05,
      "loss": 1.0716,
      "step": 9630
    },
    {
      "epoch": 1.3539997188246873,
      "grad_norm": 1.4972351789474487,
      "learning_rate": 7.048111510901857e-05,
      "loss": 0.9417,
      "step": 9631
    },
    {
      "epoch": 1.3541403064810908,
      "grad_norm": 1.671966314315796,
      "learning_rate": 7.025908467441527e-05,
      "loss": 1.0063,
      "step": 9632
    },
    {
      "epoch": 1.3542808941374949,
      "grad_norm": 2.045759677886963,
      "learning_rate": 7.003721496578002e-05,
      "loss": 1.2635,
      "step": 9633
    },
    {
      "epoch": 1.3544214817938984,
      "grad_norm": 1.5062381029129028,
      "learning_rate": 6.981550718214265e-05,
      "loss": 1.112,
      "step": 9634
    },
    {
      "epoch": 1.3545620694503022,
      "grad_norm": 1.9628827571868896,
      "learning_rate": 6.95939625216572e-05,
      "loss": 1.0195,
      "step": 9635
    },
    {
      "epoch": 1.354702657106706,
      "grad_norm": 1.341169834136963,
      "learning_rate": 6.93725821815955e-05,
      "loss": 1.0259,
      "step": 9636
    },
    {
      "epoch": 1.3548432447631098,
      "grad_norm": 1.5189272165298462,
      "learning_rate": 6.915136735834237e-05,
      "loss": 1.0227,
      "step": 9637
    },
    {
      "epoch": 1.3549838324195136,
      "grad_norm": 1.5626914501190186,
      "learning_rate": 6.89303192473878e-05,
      "loss": 0.9702,
      "step": 9638
    },
    {
      "epoch": 1.3551244200759174,
      "grad_norm": 1.679154396057129,
      "learning_rate": 6.870943904332083e-05,
      "loss": 1.0414,
      "step": 9639
    },
    {
      "epoch": 1.3552650077323212,
      "grad_norm": 1.7111049890518188,
      "learning_rate": 6.848872793982277e-05,
      "loss": 1.0403,
      "step": 9640
    },
    {
      "epoch": 1.3554055953887247,
      "grad_norm": 1.5393096208572388,
      "learning_rate": 6.826818712966244e-05,
      "loss": 1.2038,
      "step": 9641
    },
    {
      "epoch": 1.3555461830451287,
      "grad_norm": 1.3421684503555298,
      "learning_rate": 6.80478178046863e-05,
      "loss": 1.1723,
      "step": 9642
    },
    {
      "epoch": 1.3556867707015323,
      "grad_norm": 1.5074968338012695,
      "learning_rate": 6.782762115581545e-05,
      "loss": 0.8493,
      "step": 9643
    },
    {
      "epoch": 1.355827358357936,
      "grad_norm": 1.741240382194519,
      "learning_rate": 6.760759837303744e-05,
      "loss": 1.0188,
      "step": 9644
    },
    {
      "epoch": 1.35596794601434,
      "grad_norm": 1.3206706047058105,
      "learning_rate": 6.738775064540026e-05,
      "loss": 1.0067,
      "step": 9645
    },
    {
      "epoch": 1.3561085336707437,
      "grad_norm": 1.7017498016357422,
      "learning_rate": 6.716807916100616e-05,
      "loss": 1.0772,
      "step": 9646
    },
    {
      "epoch": 1.3562491213271475,
      "grad_norm": 1.5694936513900757,
      "learning_rate": 6.694858510700394e-05,
      "loss": 1.2733,
      "step": 9647
    },
    {
      "epoch": 1.3563897089835513,
      "grad_norm": 1.325884461402893,
      "learning_rate": 6.672926966958423e-05,
      "loss": 1.0457,
      "step": 9648
    },
    {
      "epoch": 1.356530296639955,
      "grad_norm": 1.8211233615875244,
      "learning_rate": 6.651013403397302e-05,
      "loss": 0.8901,
      "step": 9649
    },
    {
      "epoch": 1.3566708842963588,
      "grad_norm": 1.388443946838379,
      "learning_rate": 6.62911793844236e-05,
      "loss": 1.0023,
      "step": 9650
    },
    {
      "epoch": 1.3568114719527626,
      "grad_norm": 1.6551508903503418,
      "learning_rate": 6.607240690421152e-05,
      "loss": 1.0053,
      "step": 9651
    },
    {
      "epoch": 1.3569520596091662,
      "grad_norm": 1.6793164014816284,
      "learning_rate": 6.585381777562823e-05,
      "loss": 1.0441,
      "step": 9652
    },
    {
      "epoch": 1.3570926472655702,
      "grad_norm": 1.6991362571716309,
      "learning_rate": 6.563541317997323e-05,
      "loss": 1.1567,
      "step": 9653
    },
    {
      "epoch": 1.3572332349219738,
      "grad_norm": 1.8223956823349,
      "learning_rate": 6.541719429754981e-05,
      "loss": 1.1351,
      "step": 9654
    },
    {
      "epoch": 1.3573738225783776,
      "grad_norm": 1.9479539394378662,
      "learning_rate": 6.51991623076573e-05,
      "loss": 1.0402,
      "step": 9655
    },
    {
      "epoch": 1.3575144102347814,
      "grad_norm": 1.6129392385482788,
      "learning_rate": 6.498131838858468e-05,
      "loss": 0.9718,
      "step": 9656
    },
    {
      "epoch": 1.3576549978911852,
      "grad_norm": 1.4709032773971558,
      "learning_rate": 6.476366371760588e-05,
      "loss": 1.0718,
      "step": 9657
    },
    {
      "epoch": 1.357795585547589,
      "grad_norm": 1.3581722974777222,
      "learning_rate": 6.45461994709701e-05,
      "loss": 1.0618,
      "step": 9658
    },
    {
      "epoch": 1.3579361732039927,
      "grad_norm": 1.4999836683273315,
      "learning_rate": 6.432892682389882e-05,
      "loss": 1.0542,
      "step": 9659
    },
    {
      "epoch": 1.3580767608603965,
      "grad_norm": 1.5225951671600342,
      "learning_rate": 6.411184695057767e-05,
      "loss": 1.0531,
      "step": 9660
    },
    {
      "epoch": 1.3582173485168,
      "grad_norm": 1.5975388288497925,
      "learning_rate": 6.389496102415055e-05,
      "loss": 1.032,
      "step": 9661
    },
    {
      "epoch": 1.358357936173204,
      "grad_norm": 1.7939496040344238,
      "learning_rate": 6.367827021671357e-05,
      "loss": 1.0004,
      "step": 9662
    },
    {
      "epoch": 1.3584985238296077,
      "grad_norm": 1.7343777418136597,
      "learning_rate": 6.3461775699307e-05,
      "loss": 1.0094,
      "step": 9663
    },
    {
      "epoch": 1.3586391114860115,
      "grad_norm": 1.8370214700698853,
      "learning_rate": 6.324547864191233e-05,
      "loss": 1.0761,
      "step": 9664
    },
    {
      "epoch": 1.3587796991424153,
      "grad_norm": 1.6377567052841187,
      "learning_rate": 6.302938021344262e-05,
      "loss": 1.0164,
      "step": 9665
    },
    {
      "epoch": 1.358920286798819,
      "grad_norm": 1.3053011894226074,
      "learning_rate": 6.281348158173782e-05,
      "loss": 1.0931,
      "step": 9666
    },
    {
      "epoch": 1.3590608744552228,
      "grad_norm": 1.7171581983566284,
      "learning_rate": 6.259778391355816e-05,
      "loss": 1.1138,
      "step": 9667
    },
    {
      "epoch": 1.3592014621116266,
      "grad_norm": 1.7830172777175903,
      "learning_rate": 6.23822883745781e-05,
      "loss": 0.9474,
      "step": 9668
    },
    {
      "epoch": 1.3593420497680304,
      "grad_norm": 1.7875988483428955,
      "learning_rate": 6.216699612937883e-05,
      "loss": 1.0273,
      "step": 9669
    },
    {
      "epoch": 1.3594826374244342,
      "grad_norm": 1.5774155855178833,
      "learning_rate": 6.195190834144384e-05,
      "loss": 1.2751,
      "step": 9670
    },
    {
      "epoch": 1.359623225080838,
      "grad_norm": 1.4964518547058105,
      "learning_rate": 6.173702617315104e-05,
      "loss": 1.0836,
      "step": 9671
    },
    {
      "epoch": 1.3597638127372416,
      "grad_norm": 1.4516277313232422,
      "learning_rate": 6.152235078576816e-05,
      "loss": 0.957,
      "step": 9672
    },
    {
      "epoch": 1.3599044003936456,
      "grad_norm": 1.528395652770996,
      "learning_rate": 6.13078833394447e-05,
      "loss": 1.0735,
      "step": 9673
    },
    {
      "epoch": 1.3600449880500491,
      "grad_norm": 1.6774349212646484,
      "learning_rate": 6.109362499320582e-05,
      "loss": 1.0146,
      "step": 9674
    },
    {
      "epoch": 1.360185575706453,
      "grad_norm": 1.6353504657745361,
      "learning_rate": 6.087957690494763e-05,
      "loss": 1.136,
      "step": 9675
    },
    {
      "epoch": 1.3603261633628567,
      "grad_norm": 1.2996264696121216,
      "learning_rate": 6.0665740231429615e-05,
      "loss": 1.0748,
      "step": 9676
    },
    {
      "epoch": 1.3604667510192605,
      "grad_norm": 1.5711967945098877,
      "learning_rate": 6.0452116128268754e-05,
      "loss": 1.2359,
      "step": 9677
    },
    {
      "epoch": 1.3606073386756643,
      "grad_norm": 1.401955008506775,
      "learning_rate": 6.023870574993291e-05,
      "loss": 1.0837,
      "step": 9678
    },
    {
      "epoch": 1.360747926332068,
      "grad_norm": 1.7300440073013306,
      "learning_rate": 6.002551024973625e-05,
      "loss": 1.017,
      "step": 9679
    },
    {
      "epoch": 1.3608885139884719,
      "grad_norm": 1.5179345607757568,
      "learning_rate": 5.981253077982973e-05,
      "loss": 1.103,
      "step": 9680
    },
    {
      "epoch": 1.3610291016448754,
      "grad_norm": 1.5138826370239258,
      "learning_rate": 5.959976849119814e-05,
      "loss": 1.0338,
      "step": 9681
    },
    {
      "epoch": 1.3611696893012795,
      "grad_norm": 1.5797423124313354,
      "learning_rate": 5.938722453365226e-05,
      "loss": 1.1568,
      "step": 9682
    },
    {
      "epoch": 1.361310276957683,
      "grad_norm": 1.4181654453277588,
      "learning_rate": 5.9174900055822956e-05,
      "loss": 1.0817,
      "step": 9683
    },
    {
      "epoch": 1.3614508646140868,
      "grad_norm": 1.5679389238357544,
      "learning_rate": 5.89627962051553e-05,
      "loss": 0.9543,
      "step": 9684
    },
    {
      "epoch": 1.3615914522704906,
      "grad_norm": 1.5030919313430786,
      "learning_rate": 5.87509141279011e-05,
      "loss": 1.0046,
      "step": 9685
    },
    {
      "epoch": 1.3617320399268944,
      "grad_norm": 1.5755653381347656,
      "learning_rate": 5.853925496911428e-05,
      "loss": 1.0683,
      "step": 9686
    },
    {
      "epoch": 1.3618726275832982,
      "grad_norm": 1.339692234992981,
      "learning_rate": 5.832781987264473e-05,
      "loss": 0.885,
      "step": 9687
    },
    {
      "epoch": 1.362013215239702,
      "grad_norm": 1.6341255903244019,
      "learning_rate": 5.811660998113046e-05,
      "loss": 1.1256,
      "step": 9688
    },
    {
      "epoch": 1.3621538028961058,
      "grad_norm": 1.5191291570663452,
      "learning_rate": 5.7905626435993135e-05,
      "loss": 1.103,
      "step": 9689
    },
    {
      "epoch": 1.3622943905525096,
      "grad_norm": 1.404266595840454,
      "learning_rate": 5.7694870377430156e-05,
      "loss": 1.2168,
      "step": 9690
    },
    {
      "epoch": 1.3624349782089133,
      "grad_norm": 1.313306450843811,
      "learning_rate": 5.7484342944410575e-05,
      "loss": 1.2614,
      "step": 9691
    },
    {
      "epoch": 1.362575565865317,
      "grad_norm": 1.4646568298339844,
      "learning_rate": 5.7274045274667534e-05,
      "loss": 1.1607,
      "step": 9692
    },
    {
      "epoch": 1.362716153521721,
      "grad_norm": 1.5189151763916016,
      "learning_rate": 5.7063978504692494e-05,
      "loss": 1.1503,
      "step": 9693
    },
    {
      "epoch": 1.3628567411781245,
      "grad_norm": 1.5918550491333008,
      "learning_rate": 5.6854143769728796e-05,
      "loss": 1.1666,
      "step": 9694
    },
    {
      "epoch": 1.3629973288345283,
      "grad_norm": 1.7358561754226685,
      "learning_rate": 5.664454220376707e-05,
      "loss": 0.9759,
      "step": 9695
    },
    {
      "epoch": 1.363137916490932,
      "grad_norm": 1.3934968709945679,
      "learning_rate": 5.643517493953594e-05,
      "loss": 1.0917,
      "step": 9696
    },
    {
      "epoch": 1.3632785041473359,
      "grad_norm": 1.7671947479248047,
      "learning_rate": 5.6226043108499104e-05,
      "loss": 1.0798,
      "step": 9697
    },
    {
      "epoch": 1.3634190918037397,
      "grad_norm": 1.8302329778671265,
      "learning_rate": 5.601714784084759e-05,
      "loss": 1.2596,
      "step": 9698
    },
    {
      "epoch": 1.3635596794601434,
      "grad_norm": 1.7447234392166138,
      "learning_rate": 5.580849026549402e-05,
      "loss": 1.1779,
      "step": 9699
    },
    {
      "epoch": 1.3637002671165472,
      "grad_norm": 1.5395147800445557,
      "learning_rate": 5.5600071510066765e-05,
      "loss": 1.1406,
      "step": 9700
    },
    {
      "epoch": 1.3638408547729508,
      "grad_norm": 1.526174545288086,
      "learning_rate": 5.539189270090231e-05,
      "loss": 1.1403,
      "step": 9701
    },
    {
      "epoch": 1.3639814424293548,
      "grad_norm": 1.5978624820709229,
      "learning_rate": 5.5183954963042315e-05,
      "loss": 1.1187,
      "step": 9702
    },
    {
      "epoch": 1.3641220300857584,
      "grad_norm": 1.6472055912017822,
      "learning_rate": 5.497625942022442e-05,
      "loss": 1.1951,
      "step": 9703
    },
    {
      "epoch": 1.3642626177421622,
      "grad_norm": 1.9028902053833008,
      "learning_rate": 5.4768807194877625e-05,
      "loss": 1.0394,
      "step": 9704
    },
    {
      "epoch": 1.364403205398566,
      "grad_norm": 1.9756582975387573,
      "learning_rate": 5.4561599408116385e-05,
      "loss": 1.1808,
      "step": 9705
    },
    {
      "epoch": 1.3645437930549698,
      "grad_norm": 1.6502819061279297,
      "learning_rate": 5.435463717973316e-05,
      "loss": 1.1855,
      "step": 9706
    },
    {
      "epoch": 1.3646843807113735,
      "grad_norm": 1.6534439325332642,
      "learning_rate": 5.4147921628194356e-05,
      "loss": 1.0333,
      "step": 9707
    },
    {
      "epoch": 1.3648249683677773,
      "grad_norm": 1.3978031873703003,
      "learning_rate": 5.394145387063293e-05,
      "loss": 1.1591,
      "step": 9708
    },
    {
      "epoch": 1.3649655560241811,
      "grad_norm": 1.6307836771011353,
      "learning_rate": 5.3735235022842435e-05,
      "loss": 1.1611,
      "step": 9709
    },
    {
      "epoch": 1.365106143680585,
      "grad_norm": 1.4913150072097778,
      "learning_rate": 5.3529266199272154e-05,
      "loss": 1.0636,
      "step": 9710
    },
    {
      "epoch": 1.3652467313369887,
      "grad_norm": 1.3620764017105103,
      "learning_rate": 5.3323548513019616e-05,
      "loss": 1.0515,
      "step": 9711
    },
    {
      "epoch": 1.3653873189933923,
      "grad_norm": 1.4131839275360107,
      "learning_rate": 5.311808307582449e-05,
      "loss": 1.2414,
      "step": 9712
    },
    {
      "epoch": 1.3655279066497963,
      "grad_norm": 1.5636298656463623,
      "learning_rate": 5.291287099806419e-05,
      "loss": 1.0783,
      "step": 9713
    },
    {
      "epoch": 1.3656684943061999,
      "grad_norm": 1.5466824769973755,
      "learning_rate": 5.270791338874663e-05,
      "loss": 0.966,
      "step": 9714
    },
    {
      "epoch": 1.3658090819626036,
      "grad_norm": 1.4024591445922852,
      "learning_rate": 5.250321135550449e-05,
      "loss": 1.1797,
      "step": 9715
    },
    {
      "epoch": 1.3659496696190074,
      "grad_norm": 1.5066090822219849,
      "learning_rate": 5.2298766004589274e-05,
      "loss": 1.1413,
      "step": 9716
    },
    {
      "epoch": 1.3660902572754112,
      "grad_norm": 1.4812512397766113,
      "learning_rate": 5.2094578440865335e-05,
      "loss": 1.1575,
      "step": 9717
    },
    {
      "epoch": 1.366230844931815,
      "grad_norm": 1.6138687133789062,
      "learning_rate": 5.189064976780391e-05,
      "loss": 1.1297,
      "step": 9718
    },
    {
      "epoch": 1.3663714325882188,
      "grad_norm": 1.4750442504882812,
      "learning_rate": 5.1686981087477093e-05,
      "loss": 1.0285,
      "step": 9719
    },
    {
      "epoch": 1.3665120202446226,
      "grad_norm": 1.6720528602600098,
      "learning_rate": 5.148357350055193e-05,
      "loss": 1.1013,
      "step": 9720
    },
    {
      "epoch": 1.3666526079010262,
      "grad_norm": 1.379515290260315,
      "learning_rate": 5.1280428106284516e-05,
      "loss": 1.1349,
      "step": 9721
    },
    {
      "epoch": 1.3667931955574302,
      "grad_norm": 1.749398112297058,
      "learning_rate": 5.107754600251425e-05,
      "loss": 0.9636,
      "step": 9722
    },
    {
      "epoch": 1.3669337832138337,
      "grad_norm": 1.537859559059143,
      "learning_rate": 5.0874928285656765e-05,
      "loss": 1.264,
      "step": 9723
    },
    {
      "epoch": 1.3670743708702375,
      "grad_norm": 1.30887770652771,
      "learning_rate": 5.0672576050699504e-05,
      "loss": 1.1726,
      "step": 9724
    },
    {
      "epoch": 1.3672149585266413,
      "grad_norm": 1.4187864065170288,
      "learning_rate": 5.0470490391195814e-05,
      "loss": 1.0743,
      "step": 9725
    },
    {
      "epoch": 1.3673555461830451,
      "grad_norm": 1.5006675720214844,
      "learning_rate": 5.0268672399257456e-05,
      "loss": 1.0811,
      "step": 9726
    },
    {
      "epoch": 1.367496133839449,
      "grad_norm": 1.733027696609497,
      "learning_rate": 5.0067123165550336e-05,
      "loss": 1.1189,
      "step": 9727
    },
    {
      "epoch": 1.3676367214958527,
      "grad_norm": 1.1887341737747192,
      "learning_rate": 4.986584377928694e-05,
      "loss": 0.9618,
      "step": 9728
    },
    {
      "epoch": 1.3677773091522565,
      "grad_norm": 1.3888802528381348,
      "learning_rate": 4.9664835328222436e-05,
      "loss": 1.1185,
      "step": 9729
    },
    {
      "epoch": 1.3679178968086603,
      "grad_norm": 1.756929874420166,
      "learning_rate": 4.946409889864745e-05,
      "loss": 0.9912,
      "step": 9730
    },
    {
      "epoch": 1.368058484465064,
      "grad_norm": 1.4499647617340088,
      "learning_rate": 4.926363557538256e-05,
      "loss": 1.0306,
      "step": 9731
    },
    {
      "epoch": 1.3681990721214676,
      "grad_norm": 1.621429204940796,
      "learning_rate": 4.906344644177244e-05,
      "loss": 1.2152,
      "step": 9732
    },
    {
      "epoch": 1.3683396597778714,
      "grad_norm": 1.4132757186889648,
      "learning_rate": 4.886353257967996e-05,
      "loss": 0.9833,
      "step": 9733
    },
    {
      "epoch": 1.3684802474342752,
      "grad_norm": 1.2403100728988647,
      "learning_rate": 4.866389506948038e-05,
      "loss": 1.1006,
      "step": 9734
    },
    {
      "epoch": 1.368620835090679,
      "grad_norm": 1.537062168121338,
      "learning_rate": 4.846453499005551e-05,
      "loss": 1.1027,
      "step": 9735
    },
    {
      "epoch": 1.3687614227470828,
      "grad_norm": 1.949770212173462,
      "learning_rate": 4.826545341878783e-05,
      "loss": 1.2117,
      "step": 9736
    },
    {
      "epoch": 1.3689020104034866,
      "grad_norm": 1.3719922304153442,
      "learning_rate": 4.8066651431554746e-05,
      "loss": 1.0706,
      "step": 9737
    },
    {
      "epoch": 1.3690425980598904,
      "grad_norm": 1.6179522275924683,
      "learning_rate": 4.7868130102723005e-05,
      "loss": 1.0506,
      "step": 9738
    },
    {
      "epoch": 1.3691831857162942,
      "grad_norm": 1.9461781978607178,
      "learning_rate": 4.766989050514145e-05,
      "loss": 1.1325,
      "step": 9739
    },
    {
      "epoch": 1.369323773372698,
      "grad_norm": 1.5786162614822388,
      "learning_rate": 4.747193371013815e-05,
      "loss": 0.876,
      "step": 9740
    },
    {
      "epoch": 1.3694643610291015,
      "grad_norm": 1.5456827878952026,
      "learning_rate": 4.727426078751169e-05,
      "loss": 1.1484,
      "step": 9741
    },
    {
      "epoch": 1.3696049486855055,
      "grad_norm": 1.454178810119629,
      "learning_rate": 4.707687280552684e-05,
      "loss": 1.1239,
      "step": 9742
    },
    {
      "epoch": 1.369745536341909,
      "grad_norm": 1.4108738899230957,
      "learning_rate": 4.6879770830908776e-05,
      "loss": 1.1306,
      "step": 9743
    },
    {
      "epoch": 1.3698861239983129,
      "grad_norm": 1.6898839473724365,
      "learning_rate": 4.668295592883615e-05,
      "loss": 1.1101,
      "step": 9744
    },
    {
      "epoch": 1.3700267116547167,
      "grad_norm": 1.7421331405639648,
      "learning_rate": 4.6486429162937096e-05,
      "loss": 1.0851,
      "step": 9745
    },
    {
      "epoch": 1.3701672993111205,
      "grad_norm": 1.3016537427902222,
      "learning_rate": 4.629019159528233e-05,
      "loss": 1.0955,
      "step": 9746
    },
    {
      "epoch": 1.3703078869675243,
      "grad_norm": 1.5418072938919067,
      "learning_rate": 4.609424428637936e-05,
      "loss": 1.1082,
      "step": 9747
    },
    {
      "epoch": 1.370448474623928,
      "grad_norm": 1.4152201414108276,
      "learning_rate": 4.589858829516829e-05,
      "loss": 1.1636,
      "step": 9748
    },
    {
      "epoch": 1.3705890622803318,
      "grad_norm": 1.9992828369140625,
      "learning_rate": 4.570322467901306e-05,
      "loss": 1.1258,
      "step": 9749
    },
    {
      "epoch": 1.3707296499367356,
      "grad_norm": 1.2903668880462646,
      "learning_rate": 4.550815449369877e-05,
      "loss": 1.0749,
      "step": 9750
    },
    {
      "epoch": 1.3708702375931394,
      "grad_norm": 1.5832321643829346,
      "learning_rate": 4.531337879342449e-05,
      "loss": 1.0295,
      "step": 9751
    },
    {
      "epoch": 1.371010825249543,
      "grad_norm": 1.5305405855178833,
      "learning_rate": 4.5118898630797776e-05,
      "loss": 1.0862,
      "step": 9752
    },
    {
      "epoch": 1.3711514129059468,
      "grad_norm": 1.4977788925170898,
      "learning_rate": 4.492471505682908e-05,
      "loss": 1.2218,
      "step": 9753
    },
    {
      "epoch": 1.3712920005623506,
      "grad_norm": 1.4067314863204956,
      "learning_rate": 4.473082912092602e-05,
      "loss": 1.0816,
      "step": 9754
    },
    {
      "epoch": 1.3714325882187544,
      "grad_norm": 1.4588366746902466,
      "learning_rate": 4.45372418708877e-05,
      "loss": 1.2214,
      "step": 9755
    },
    {
      "epoch": 1.3715731758751581,
      "grad_norm": 2.537637233734131,
      "learning_rate": 4.434395435289909e-05,
      "loss": 1.0844,
      "step": 9756
    },
    {
      "epoch": 1.371713763531562,
      "grad_norm": 1.5264811515808105,
      "learning_rate": 4.415096761152532e-05,
      "loss": 0.9712,
      "step": 9757
    },
    {
      "epoch": 1.3718543511879657,
      "grad_norm": 1.447171926498413,
      "learning_rate": 4.3958282689706084e-05,
      "loss": 1.1251,
      "step": 9758
    },
    {
      "epoch": 1.3719949388443695,
      "grad_norm": 1.3974626064300537,
      "learning_rate": 4.376590062875028e-05,
      "loss": 1.0787,
      "step": 9759
    },
    {
      "epoch": 1.3721355265007733,
      "grad_norm": 1.4494932889938354,
      "learning_rate": 4.357382246832917e-05,
      "loss": 0.998,
      "step": 9760
    },
    {
      "epoch": 1.3722761141571769,
      "grad_norm": 1.3588117361068726,
      "learning_rate": 4.338204924647259e-05,
      "loss": 1.2909,
      "step": 9761
    },
    {
      "epoch": 1.3724167018135809,
      "grad_norm": 1.4302434921264648,
      "learning_rate": 4.3190581999561817e-05,
      "loss": 0.9953,
      "step": 9762
    },
    {
      "epoch": 1.3725572894699845,
      "grad_norm": 1.5946846008300781,
      "learning_rate": 4.2999421762325476e-05,
      "loss": 1.0701,
      "step": 9763
    },
    {
      "epoch": 1.3726978771263882,
      "grad_norm": 1.500896692276001,
      "learning_rate": 4.280856956783215e-05,
      "loss": 0.8712,
      "step": 9764
    },
    {
      "epoch": 1.372838464782792,
      "grad_norm": 1.3535492420196533,
      "learning_rate": 4.2618026447486295e-05,
      "loss": 1.1948,
      "step": 9765
    },
    {
      "epoch": 1.3729790524391958,
      "grad_norm": 1.5695356130599976,
      "learning_rate": 4.242779343102118e-05,
      "loss": 1.2139,
      "step": 9766
    },
    {
      "epoch": 1.3731196400955996,
      "grad_norm": 1.5713037252426147,
      "learning_rate": 4.22378715464951e-05,
      "loss": 1.1857,
      "step": 9767
    },
    {
      "epoch": 1.3732602277520034,
      "grad_norm": 1.5672669410705566,
      "learning_rate": 4.204826182028464e-05,
      "loss": 1.0555,
      "step": 9768
    },
    {
      "epoch": 1.3734008154084072,
      "grad_norm": 1.5612283945083618,
      "learning_rate": 4.185896527707939e-05,
      "loss": 0.9203,
      "step": 9769
    },
    {
      "epoch": 1.373541403064811,
      "grad_norm": 1.52847158908844,
      "learning_rate": 4.166998293987646e-05,
      "loss": 0.9384,
      "step": 9770
    },
    {
      "epoch": 1.3736819907212148,
      "grad_norm": 1.7600593566894531,
      "learning_rate": 4.148131582997492e-05,
      "loss": 0.9768,
      "step": 9771
    },
    {
      "epoch": 1.3738225783776183,
      "grad_norm": 1.435270071029663,
      "learning_rate": 4.129296496697029e-05,
      "loss": 0.9452,
      "step": 9772
    },
    {
      "epoch": 1.3739631660340221,
      "grad_norm": 1.7968436479568481,
      "learning_rate": 4.110493136874902e-05,
      "loss": 0.9852,
      "step": 9773
    },
    {
      "epoch": 1.374103753690426,
      "grad_norm": 1.5364269018173218,
      "learning_rate": 4.091721605148301e-05,
      "loss": 1.2704,
      "step": 9774
    },
    {
      "epoch": 1.3742443413468297,
      "grad_norm": 1.5526089668273926,
      "learning_rate": 4.0729820029624366e-05,
      "loss": 1.0693,
      "step": 9775
    },
    {
      "epoch": 1.3743849290032335,
      "grad_norm": 1.7089571952819824,
      "learning_rate": 4.054274431589882e-05,
      "loss": 1.0038,
      "step": 9776
    },
    {
      "epoch": 1.3745255166596373,
      "grad_norm": 1.3650823831558228,
      "learning_rate": 4.035598992130167e-05,
      "loss": 1.2455,
      "step": 9777
    },
    {
      "epoch": 1.374666104316041,
      "grad_norm": 1.6816270351409912,
      "learning_rate": 4.01695578550923e-05,
      "loss": 0.885,
      "step": 9778
    },
    {
      "epoch": 1.3748066919724449,
      "grad_norm": 1.5591188669204712,
      "learning_rate": 3.998344912478729e-05,
      "loss": 1.0163,
      "step": 9779
    },
    {
      "epoch": 1.3749472796288487,
      "grad_norm": 1.60957670211792,
      "learning_rate": 3.979766473615618e-05,
      "loss": 1.1757,
      "step": 9780
    },
    {
      "epoch": 1.3750878672852522,
      "grad_norm": 1.5986697673797607,
      "learning_rate": 3.961220569321593e-05,
      "loss": 1.0438,
      "step": 9781
    },
    {
      "epoch": 1.3752284549416562,
      "grad_norm": 1.4793602228164673,
      "learning_rate": 3.942707299822447e-05,
      "loss": 1.0567,
      "step": 9782
    },
    {
      "epoch": 1.3753690425980598,
      "grad_norm": 1.677919864654541,
      "learning_rate": 3.924226765167689e-05,
      "loss": 1.0138,
      "step": 9783
    },
    {
      "epoch": 1.3755096302544636,
      "grad_norm": 1.4489609003067017,
      "learning_rate": 3.9057790652298944e-05,
      "loss": 1.1504,
      "step": 9784
    },
    {
      "epoch": 1.3756502179108674,
      "grad_norm": 1.5931566953659058,
      "learning_rate": 3.887364299704166e-05,
      "loss": 1.0645,
      "step": 9785
    },
    {
      "epoch": 1.3757908055672712,
      "grad_norm": 1.448302984237671,
      "learning_rate": 3.868982568107735e-05,
      "loss": 1.052,
      "step": 9786
    },
    {
      "epoch": 1.375931393223675,
      "grad_norm": 1.402206301689148,
      "learning_rate": 3.850633969779141e-05,
      "loss": 0.8838,
      "step": 9787
    },
    {
      "epoch": 1.3760719808800788,
      "grad_norm": 1.769326090812683,
      "learning_rate": 3.832318603877978e-05,
      "loss": 1.0153,
      "step": 9788
    },
    {
      "epoch": 1.3762125685364826,
      "grad_norm": 1.3926880359649658,
      "learning_rate": 3.814036569384215e-05,
      "loss": 1.2054,
      "step": 9789
    },
    {
      "epoch": 1.3763531561928863,
      "grad_norm": 1.4975166320800781,
      "learning_rate": 3.795787965097693e-05,
      "loss": 1.1707,
      "step": 9790
    },
    {
      "epoch": 1.3764937438492901,
      "grad_norm": 1.6589115858078003,
      "learning_rate": 3.7775728896376196e-05,
      "loss": 1.1889,
      "step": 9791
    },
    {
      "epoch": 1.3766343315056937,
      "grad_norm": 1.8402512073516846,
      "learning_rate": 3.759391441441888e-05,
      "loss": 1.0432,
      "step": 9792
    },
    {
      "epoch": 1.3767749191620975,
      "grad_norm": 1.6640050411224365,
      "learning_rate": 3.741243718766837e-05,
      "loss": 1.0484,
      "step": 9793
    },
    {
      "epoch": 1.3769155068185013,
      "grad_norm": 1.4178345203399658,
      "learning_rate": 3.723129819686429e-05,
      "loss": 1.1618,
      "step": 9794
    },
    {
      "epoch": 1.377056094474905,
      "grad_norm": 1.2796590328216553,
      "learning_rate": 3.705049842091868e-05,
      "loss": 1.0917,
      "step": 9795
    },
    {
      "epoch": 1.3771966821313089,
      "grad_norm": 1.467964768409729,
      "learning_rate": 3.6870038836910356e-05,
      "loss": 1.0388,
      "step": 9796
    },
    {
      "epoch": 1.3773372697877126,
      "grad_norm": 1.4072680473327637,
      "learning_rate": 3.668992042007999e-05,
      "loss": 1.2072,
      "step": 9797
    },
    {
      "epoch": 1.3774778574441164,
      "grad_norm": 1.663217544555664,
      "learning_rate": 3.6510144143823675e-05,
      "loss": 0.938,
      "step": 9798
    },
    {
      "epoch": 1.3776184451005202,
      "grad_norm": 1.5580991506576538,
      "learning_rate": 3.6330710979689306e-05,
      "loss": 0.7888,
      "step": 9799
    },
    {
      "epoch": 1.377759032756924,
      "grad_norm": 1.411882996559143,
      "learning_rate": 3.615162189737001e-05,
      "loss": 1.1072,
      "step": 9800
    },
    {
      "epoch": 1.3778996204133276,
      "grad_norm": 1.5037516355514526,
      "learning_rate": 3.597287786470025e-05,
      "loss": 1.2114,
      "step": 9801
    },
    {
      "epoch": 1.3780402080697316,
      "grad_norm": 1.7125003337860107,
      "learning_rate": 3.579447984764919e-05,
      "loss": 1.1048,
      "step": 9802
    },
    {
      "epoch": 1.3781807957261352,
      "grad_norm": 1.366819143295288,
      "learning_rate": 3.5616428810315536e-05,
      "loss": 1.0081,
      "step": 9803
    },
    {
      "epoch": 1.378321383382539,
      "grad_norm": 1.5786805152893066,
      "learning_rate": 3.543872571492367e-05,
      "loss": 1.0707,
      "step": 9804
    },
    {
      "epoch": 1.3784619710389427,
      "grad_norm": 1.3908023834228516,
      "learning_rate": 3.526137152181733e-05,
      "loss": 0.9885,
      "step": 9805
    },
    {
      "epoch": 1.3786025586953465,
      "grad_norm": 1.49555242061615,
      "learning_rate": 3.508436718945472e-05,
      "loss": 1.0444,
      "step": 9806
    },
    {
      "epoch": 1.3787431463517503,
      "grad_norm": 1.973311185836792,
      "learning_rate": 3.490771367440335e-05,
      "loss": 0.9621,
      "step": 9807
    },
    {
      "epoch": 1.3788837340081541,
      "grad_norm": 2.093890905380249,
      "learning_rate": 3.4731411931334804e-05,
      "loss": 1.05,
      "step": 9808
    },
    {
      "epoch": 1.379024321664558,
      "grad_norm": 1.347802996635437,
      "learning_rate": 3.455546291301964e-05,
      "loss": 0.9427,
      "step": 9809
    },
    {
      "epoch": 1.3791649093209617,
      "grad_norm": 1.7660435438156128,
      "learning_rate": 3.437986757032222e-05,
      "loss": 0.8883,
      "step": 9810
    },
    {
      "epoch": 1.3793054969773655,
      "grad_norm": 1.4552364349365234,
      "learning_rate": 3.420462685219558e-05,
      "loss": 1.0291,
      "step": 9811
    },
    {
      "epoch": 1.379446084633769,
      "grad_norm": 2.055382490158081,
      "learning_rate": 3.402974170567624e-05,
      "loss": 0.9724,
      "step": 9812
    },
    {
      "epoch": 1.3795866722901728,
      "grad_norm": 1.5637277364730835,
      "learning_rate": 3.385521307587949e-05,
      "loss": 1.3238,
      "step": 9813
    },
    {
      "epoch": 1.3797272599465766,
      "grad_norm": 1.4472161531448364,
      "learning_rate": 3.368104190599305e-05,
      "loss": 1.1614,
      "step": 9814
    },
    {
      "epoch": 1.3798678476029804,
      "grad_norm": 1.6927751302719116,
      "learning_rate": 3.350722913727344e-05,
      "loss": 1.1311,
      "step": 9815
    },
    {
      "epoch": 1.3800084352593842,
      "grad_norm": 1.746057391166687,
      "learning_rate": 3.3333775709040825e-05,
      "loss": 1.1877,
      "step": 9816
    },
    {
      "epoch": 1.380149022915788,
      "grad_norm": 1.4514336585998535,
      "learning_rate": 3.316068255867261e-05,
      "loss": 1.2422,
      "step": 9817
    },
    {
      "epoch": 1.3802896105721918,
      "grad_norm": 1.3995461463928223,
      "learning_rate": 3.298795062159973e-05,
      "loss": 0.9337,
      "step": 9818
    },
    {
      "epoch": 1.3804301982285956,
      "grad_norm": 1.4872006177902222,
      "learning_rate": 3.2815580831300256e-05,
      "loss": 1.3469,
      "step": 9819
    },
    {
      "epoch": 1.3805707858849994,
      "grad_norm": 1.3861665725708008,
      "learning_rate": 3.2643574119295926e-05,
      "loss": 1.0386,
      "step": 9820
    },
    {
      "epoch": 1.380711373541403,
      "grad_norm": 1.7087575197219849,
      "learning_rate": 3.2471931415146075e-05,
      "loss": 1.114,
      "step": 9821
    },
    {
      "epoch": 1.380851961197807,
      "grad_norm": 1.3899266719818115,
      "learning_rate": 3.2300653646442845e-05,
      "loss": 0.9631,
      "step": 9822
    },
    {
      "epoch": 1.3809925488542105,
      "grad_norm": 1.5853668451309204,
      "learning_rate": 3.212974173880597e-05,
      "loss": 1.0519,
      "step": 9823
    },
    {
      "epoch": 1.3811331365106143,
      "grad_norm": 1.4681472778320312,
      "learning_rate": 3.195919661587902e-05,
      "loss": 1.1877,
      "step": 9824
    },
    {
      "epoch": 1.381273724167018,
      "grad_norm": 1.660093069076538,
      "learning_rate": 3.17890191993218e-05,
      "loss": 1.1201,
      "step": 9825
    },
    {
      "epoch": 1.381414311823422,
      "grad_norm": 1.6042782068252563,
      "learning_rate": 3.161921040880803e-05,
      "loss": 1.0995,
      "step": 9826
    },
    {
      "epoch": 1.3815548994798257,
      "grad_norm": 1.5472047328948975,
      "learning_rate": 3.144977116201902e-05,
      "loss": 1.1583,
      "step": 9827
    },
    {
      "epoch": 1.3816954871362295,
      "grad_norm": 1.4230202436447144,
      "learning_rate": 3.128070237463899e-05,
      "loss": 1.2722,
      "step": 9828
    },
    {
      "epoch": 1.3818360747926333,
      "grad_norm": 1.3908053636550903,
      "learning_rate": 3.1112004960350405e-05,
      "loss": 1.0905,
      "step": 9829
    },
    {
      "epoch": 1.381976662449037,
      "grad_norm": 1.3734588623046875,
      "learning_rate": 3.094367983082793e-05,
      "loss": 1.0822,
      "step": 9830
    },
    {
      "epoch": 1.3821172501054408,
      "grad_norm": 1.5584182739257812,
      "learning_rate": 3.077572789573491e-05,
      "loss": 1.0463,
      "step": 9831
    },
    {
      "epoch": 1.3822578377618444,
      "grad_norm": 1.8657093048095703,
      "learning_rate": 3.0608150062718324e-05,
      "loss": 1.2856,
      "step": 9832
    },
    {
      "epoch": 1.3823984254182482,
      "grad_norm": 1.7277350425720215,
      "learning_rate": 3.0440947237402695e-05,
      "loss": 1.1804,
      "step": 9833
    },
    {
      "epoch": 1.382539013074652,
      "grad_norm": 1.664042592048645,
      "learning_rate": 3.0274120323386158e-05,
      "loss": 1.1602,
      "step": 9834
    },
    {
      "epoch": 1.3826796007310558,
      "grad_norm": 1.9784462451934814,
      "learning_rate": 3.010767022223563e-05,
      "loss": 1.2525,
      "step": 9835
    },
    {
      "epoch": 1.3828201883874596,
      "grad_norm": 1.8136544227600098,
      "learning_rate": 2.994159783348086e-05,
      "loss": 1.1256,
      "step": 9836
    },
    {
      "epoch": 1.3829607760438634,
      "grad_norm": 1.5173379182815552,
      "learning_rate": 2.977590405461116e-05,
      "loss": 1.1892,
      "step": 9837
    },
    {
      "epoch": 1.3831013637002672,
      "grad_norm": 1.5107897520065308,
      "learning_rate": 2.961058978106921e-05,
      "loss": 1.0756,
      "step": 9838
    },
    {
      "epoch": 1.383241951356671,
      "grad_norm": 1.5004452466964722,
      "learning_rate": 2.944565590624756e-05,
      "loss": 1.0919,
      "step": 9839
    },
    {
      "epoch": 1.3833825390130747,
      "grad_norm": 1.5126349925994873,
      "learning_rate": 2.9281103321482483e-05,
      "loss": 1.175,
      "step": 9840
    },
    {
      "epoch": 1.3835231266694783,
      "grad_norm": 1.518582820892334,
      "learning_rate": 2.9116932916049188e-05,
      "loss": 1.218,
      "step": 9841
    },
    {
      "epoch": 1.3836637143258823,
      "grad_norm": 1.2178174257278442,
      "learning_rate": 2.8953145577158247e-05,
      "loss": 0.9908,
      "step": 9842
    },
    {
      "epoch": 1.3838043019822859,
      "grad_norm": 1.834449291229248,
      "learning_rate": 2.878974218994983e-05,
      "loss": 1.1254,
      "step": 9843
    },
    {
      "epoch": 1.3839448896386897,
      "grad_norm": 1.4134769439697266,
      "learning_rate": 2.862672363748915e-05,
      "loss": 1.2151,
      "step": 9844
    },
    {
      "epoch": 1.3840854772950935,
      "grad_norm": 1.5622156858444214,
      "learning_rate": 2.8464090800761943e-05,
      "loss": 1.2087,
      "step": 9845
    },
    {
      "epoch": 1.3842260649514972,
      "grad_norm": 1.5679926872253418,
      "learning_rate": 2.8301844558668455e-05,
      "loss": 0.9388,
      "step": 9846
    },
    {
      "epoch": 1.384366652607901,
      "grad_norm": 1.5460598468780518,
      "learning_rate": 2.8139985788021206e-05,
      "loss": 1.0372,
      "step": 9847
    },
    {
      "epoch": 1.3845072402643048,
      "grad_norm": 1.5920908451080322,
      "learning_rate": 2.7978515363537716e-05,
      "loss": 1.0401,
      "step": 9848
    },
    {
      "epoch": 1.3846478279207086,
      "grad_norm": 1.6871352195739746,
      "learning_rate": 2.7817434157837087e-05,
      "loss": 1.0812,
      "step": 9849
    },
    {
      "epoch": 1.3847884155771124,
      "grad_norm": 1.7129758596420288,
      "learning_rate": 2.7656743041434975e-05,
      "loss": 1.0343,
      "step": 9850
    },
    {
      "epoch": 1.3849290032335162,
      "grad_norm": 1.5658936500549316,
      "learning_rate": 2.7496442882739182e-05,
      "loss": 1.0272,
      "step": 9851
    },
    {
      "epoch": 1.3850695908899198,
      "grad_norm": 1.4374240636825562,
      "learning_rate": 2.7336534548043935e-05,
      "loss": 0.8466,
      "step": 9852
    },
    {
      "epoch": 1.3852101785463236,
      "grad_norm": 1.6383883953094482,
      "learning_rate": 2.7177018901526497e-05,
      "loss": 1.0564,
      "step": 9853
    },
    {
      "epoch": 1.3853507662027273,
      "grad_norm": 1.5175164937973022,
      "learning_rate": 2.701789680524239e-05,
      "loss": 1.029,
      "step": 9854
    },
    {
      "epoch": 1.3854913538591311,
      "grad_norm": 1.6205552816390991,
      "learning_rate": 2.6859169119119633e-05,
      "loss": 1.0953,
      "step": 9855
    },
    {
      "epoch": 1.385631941515535,
      "grad_norm": 1.41292142868042,
      "learning_rate": 2.6700836700955223e-05,
      "loss": 0.9676,
      "step": 9856
    },
    {
      "epoch": 1.3857725291719387,
      "grad_norm": 1.5954949855804443,
      "learning_rate": 2.6542900406409355e-05,
      "loss": 1.0528,
      "step": 9857
    },
    {
      "epoch": 1.3859131168283425,
      "grad_norm": 1.325286626815796,
      "learning_rate": 2.638536108900218e-05,
      "loss": 1.0633,
      "step": 9858
    },
    {
      "epoch": 1.3860537044847463,
      "grad_norm": 1.544468641281128,
      "learning_rate": 2.6228219600108284e-05,
      "loss": 1.0966,
      "step": 9859
    },
    {
      "epoch": 1.38619429214115,
      "grad_norm": 1.4735395908355713,
      "learning_rate": 2.607147678895231e-05,
      "loss": 1.1427,
      "step": 9860
    },
    {
      "epoch": 1.3863348797975537,
      "grad_norm": 1.5150527954101562,
      "learning_rate": 2.591513350260435e-05,
      "loss": 1.0275,
      "step": 9861
    },
    {
      "epoch": 1.3864754674539577,
      "grad_norm": 1.4099537134170532,
      "learning_rate": 2.575919058597539e-05,
      "loss": 1.0276,
      "step": 9862
    },
    {
      "epoch": 1.3866160551103612,
      "grad_norm": 1.3784654140472412,
      "learning_rate": 2.5603648881812748e-05,
      "loss": 1.1692,
      "step": 9863
    },
    {
      "epoch": 1.386756642766765,
      "grad_norm": 1.5665886402130127,
      "learning_rate": 2.5448509230695485e-05,
      "loss": 1.0889,
      "step": 9864
    },
    {
      "epoch": 1.3868972304231688,
      "grad_norm": 1.5626901388168335,
      "learning_rate": 2.5293772471029897e-05,
      "loss": 1.0514,
      "step": 9865
    },
    {
      "epoch": 1.3870378180795726,
      "grad_norm": 1.7137579917907715,
      "learning_rate": 2.5139439439044988e-05,
      "loss": 1.0063,
      "step": 9866
    },
    {
      "epoch": 1.3871784057359764,
      "grad_norm": 1.542858362197876,
      "learning_rate": 2.4985510968788127e-05,
      "loss": 1.0363,
      "step": 9867
    },
    {
      "epoch": 1.3873189933923802,
      "grad_norm": 1.3719234466552734,
      "learning_rate": 2.48319878921197e-05,
      "loss": 1.1786,
      "step": 9868
    },
    {
      "epoch": 1.387459581048784,
      "grad_norm": 1.381458044052124,
      "learning_rate": 2.4678871038709693e-05,
      "loss": 1.2794,
      "step": 9869
    },
    {
      "epoch": 1.3876001687051878,
      "grad_norm": 1.5311371088027954,
      "learning_rate": 2.452616123603326e-05,
      "loss": 1.1636,
      "step": 9870
    },
    {
      "epoch": 1.3877407563615916,
      "grad_norm": 1.6439026594161987,
      "learning_rate": 2.4373859309365067e-05,
      "loss": 1.2068,
      "step": 9871
    },
    {
      "epoch": 1.3878813440179951,
      "grad_norm": 1.5237759351730347,
      "learning_rate": 2.4221966081776003e-05,
      "loss": 1.211,
      "step": 9872
    },
    {
      "epoch": 1.388021931674399,
      "grad_norm": 1.529892086982727,
      "learning_rate": 2.4070482374127568e-05,
      "loss": 1.063,
      "step": 9873
    },
    {
      "epoch": 1.3881625193308027,
      "grad_norm": 1.5566343069076538,
      "learning_rate": 2.3919409005068816e-05,
      "loss": 0.9258,
      "step": 9874
    },
    {
      "epoch": 1.3883031069872065,
      "grad_norm": 1.5592765808105469,
      "learning_rate": 2.376874679103104e-05,
      "loss": 1.2166,
      "step": 9875
    },
    {
      "epoch": 1.3884436946436103,
      "grad_norm": 2.026273250579834,
      "learning_rate": 2.361849654622331e-05,
      "loss": 0.9525,
      "step": 9876
    },
    {
      "epoch": 1.388584282300014,
      "grad_norm": 1.6745465993881226,
      "learning_rate": 2.3468659082629052e-05,
      "loss": 1.1119,
      "step": 9877
    },
    {
      "epoch": 1.3887248699564179,
      "grad_norm": 1.4281635284423828,
      "learning_rate": 2.3319235210000445e-05,
      "loss": 1.0832,
      "step": 9878
    },
    {
      "epoch": 1.3888654576128217,
      "grad_norm": 1.4842385053634644,
      "learning_rate": 2.3170225735854088e-05,
      "loss": 1.0474,
      "step": 9879
    },
    {
      "epoch": 1.3890060452692254,
      "grad_norm": 1.441811442375183,
      "learning_rate": 2.3021631465467798e-05,
      "loss": 1.0663,
      "step": 9880
    },
    {
      "epoch": 1.389146632925629,
      "grad_norm": 1.4206393957138062,
      "learning_rate": 2.2873453201875295e-05,
      "loss": 1.0504,
      "step": 9881
    },
    {
      "epoch": 1.389287220582033,
      "grad_norm": 1.5895994901657104,
      "learning_rate": 2.272569174586211e-05,
      "loss": 1.1081,
      "step": 9882
    },
    {
      "epoch": 1.3894278082384366,
      "grad_norm": 1.6458477973937988,
      "learning_rate": 2.2578347895961493e-05,
      "loss": 1.1904,
      "step": 9883
    },
    {
      "epoch": 1.3895683958948404,
      "grad_norm": 1.449892282485962,
      "learning_rate": 2.243142244844898e-05,
      "loss": 1.0546,
      "step": 9884
    },
    {
      "epoch": 1.3897089835512442,
      "grad_norm": 1.5268833637237549,
      "learning_rate": 2.228491619734031e-05,
      "loss": 0.9753,
      "step": 9885
    },
    {
      "epoch": 1.389849571207648,
      "grad_norm": 1.4688504934310913,
      "learning_rate": 2.2138829934384885e-05,
      "loss": 1.2854,
      "step": 9886
    },
    {
      "epoch": 1.3899901588640518,
      "grad_norm": 1.4168851375579834,
      "learning_rate": 2.1993164449062665e-05,
      "loss": 1.1886,
      "step": 9887
    },
    {
      "epoch": 1.3901307465204555,
      "grad_norm": 1.4735040664672852,
      "learning_rate": 2.1847920528579824e-05,
      "loss": 1.0672,
      "step": 9888
    },
    {
      "epoch": 1.3902713341768593,
      "grad_norm": 1.3697091341018677,
      "learning_rate": 2.1703098957863688e-05,
      "loss": 1.2846,
      "step": 9889
    },
    {
      "epoch": 1.3904119218332631,
      "grad_norm": 1.5169847011566162,
      "learning_rate": 2.155870051955976e-05,
      "loss": 0.8875,
      "step": 9890
    },
    {
      "epoch": 1.390552509489667,
      "grad_norm": 1.4890364408493042,
      "learning_rate": 2.1414725994026406e-05,
      "loss": 1.0949,
      "step": 9891
    },
    {
      "epoch": 1.3906930971460705,
      "grad_norm": 1.657585620880127,
      "learning_rate": 2.1271176159331764e-05,
      "loss": 1.0289,
      "step": 9892
    },
    {
      "epoch": 1.3908336848024743,
      "grad_norm": 1.426161527633667,
      "learning_rate": 2.112805179124816e-05,
      "loss": 1.0607,
      "step": 9893
    },
    {
      "epoch": 1.390974272458878,
      "grad_norm": 1.8343969583511353,
      "learning_rate": 2.0985353663249087e-05,
      "loss": 1.0169,
      "step": 9894
    },
    {
      "epoch": 1.3911148601152818,
      "grad_norm": 1.487877607345581,
      "learning_rate": 2.0843082546503866e-05,
      "loss": 1.2593,
      "step": 9895
    },
    {
      "epoch": 1.3912554477716856,
      "grad_norm": 1.7885280847549438,
      "learning_rate": 2.0701239209874833e-05,
      "loss": 1.1117,
      "step": 9896
    },
    {
      "epoch": 1.3913960354280894,
      "grad_norm": 1.6688896417617798,
      "learning_rate": 2.055982441991229e-05,
      "loss": 1.0621,
      "step": 9897
    },
    {
      "epoch": 1.3915366230844932,
      "grad_norm": 1.6389696598052979,
      "learning_rate": 2.0418838940850583e-05,
      "loss": 1.1698,
      "step": 9898
    },
    {
      "epoch": 1.391677210740897,
      "grad_norm": 1.6894667148590088,
      "learning_rate": 2.0278283534603993e-05,
      "loss": 1.1882,
      "step": 9899
    },
    {
      "epoch": 1.3918177983973008,
      "grad_norm": 1.6339185237884521,
      "learning_rate": 2.0138158960762555e-05,
      "loss": 1.0746,
      "step": 9900
    },
    {
      "epoch": 1.3919583860537044,
      "grad_norm": 1.4017616510391235,
      "learning_rate": 1.9998465976588055e-05,
      "loss": 1.3906,
      "step": 9901
    },
    {
      "epoch": 1.3920989737101084,
      "grad_norm": 1.28717839717865,
      "learning_rate": 1.9859205337009844e-05,
      "loss": 1.2114,
      "step": 9902
    },
    {
      "epoch": 1.392239561366512,
      "grad_norm": 1.5368239879608154,
      "learning_rate": 1.9720377794620794e-05,
      "loss": 1.0676,
      "step": 9903
    },
    {
      "epoch": 1.3923801490229157,
      "grad_norm": 1.50644850730896,
      "learning_rate": 1.9581984099673466e-05,
      "loss": 1.0249,
      "step": 9904
    },
    {
      "epoch": 1.3925207366793195,
      "grad_norm": 1.4097174406051636,
      "learning_rate": 1.944402500007516e-05,
      "loss": 1.0533,
      "step": 9905
    },
    {
      "epoch": 1.3926613243357233,
      "grad_norm": 1.4939758777618408,
      "learning_rate": 1.9306501241385188e-05,
      "loss": 1.2946,
      "step": 9906
    },
    {
      "epoch": 1.392801911992127,
      "grad_norm": 1.4838013648986816,
      "learning_rate": 1.9169413566809768e-05,
      "loss": 1.1346,
      "step": 9907
    },
    {
      "epoch": 1.392942499648531,
      "grad_norm": 1.523271918296814,
      "learning_rate": 1.9032762717199047e-05,
      "loss": 1.1397,
      "step": 9908
    },
    {
      "epoch": 1.3930830873049347,
      "grad_norm": 1.558461308479309,
      "learning_rate": 1.8896549431041865e-05,
      "loss": 1.0509,
      "step": 9909
    },
    {
      "epoch": 1.3932236749613385,
      "grad_norm": 1.8466750383377075,
      "learning_rate": 1.8760774444462782e-05,
      "loss": 0.9692,
      "step": 9910
    },
    {
      "epoch": 1.3933642626177423,
      "grad_norm": 1.4817157983779907,
      "learning_rate": 1.8625438491217097e-05,
      "loss": 1.0556,
      "step": 9911
    },
    {
      "epoch": 1.3935048502741458,
      "grad_norm": 1.680250883102417,
      "learning_rate": 1.8490542302688084e-05,
      "loss": 0.9799,
      "step": 9912
    },
    {
      "epoch": 1.3936454379305496,
      "grad_norm": 1.624062180519104,
      "learning_rate": 1.835608660788226e-05,
      "loss": 1.2587,
      "step": 9913
    },
    {
      "epoch": 1.3937860255869534,
      "grad_norm": 1.4697734117507935,
      "learning_rate": 1.822207213342537e-05,
      "loss": 1.0444,
      "step": 9914
    },
    {
      "epoch": 1.3939266132433572,
      "grad_norm": 1.6393946409225464,
      "learning_rate": 1.8088499603559593e-05,
      "loss": 1.0745,
      "step": 9915
    },
    {
      "epoch": 1.394067200899761,
      "grad_norm": 1.5514065027236938,
      "learning_rate": 1.7955369740137473e-05,
      "loss": 1.04,
      "step": 9916
    },
    {
      "epoch": 1.3942077885561648,
      "grad_norm": 1.4882923364639282,
      "learning_rate": 1.7822683262620144e-05,
      "loss": 0.8692,
      "step": 9917
    },
    {
      "epoch": 1.3943483762125686,
      "grad_norm": 1.461812973022461,
      "learning_rate": 1.769044088807237e-05,
      "loss": 1.0065,
      "step": 9918
    },
    {
      "epoch": 1.3944889638689724,
      "grad_norm": 1.5234663486480713,
      "learning_rate": 1.7558643331158887e-05,
      "loss": 1.1887,
      "step": 9919
    },
    {
      "epoch": 1.3946295515253762,
      "grad_norm": 1.421496033668518,
      "learning_rate": 1.742729130414058e-05,
      "loss": 1.0272,
      "step": 9920
    },
    {
      "epoch": 1.3947701391817797,
      "grad_norm": 1.4930585622787476,
      "learning_rate": 1.729638551687075e-05,
      "loss": 1.2698,
      "step": 9921
    },
    {
      "epoch": 1.3949107268381837,
      "grad_norm": 1.297205924987793,
      "learning_rate": 1.716592667679038e-05,
      "loss": 1.303,
      "step": 9922
    },
    {
      "epoch": 1.3950513144945873,
      "grad_norm": 1.3696058988571167,
      "learning_rate": 1.7035915488926213e-05,
      "loss": 1.1669,
      "step": 9923
    },
    {
      "epoch": 1.395191902150991,
      "grad_norm": 1.5769436359405518,
      "learning_rate": 1.6906352655885018e-05,
      "loss": 1.0379,
      "step": 9924
    },
    {
      "epoch": 1.3953324898073949,
      "grad_norm": 1.4891561269760132,
      "learning_rate": 1.6777238877850743e-05,
      "loss": 0.8746,
      "step": 9925
    },
    {
      "epoch": 1.3954730774637987,
      "grad_norm": 1.4060031175613403,
      "learning_rate": 1.6648574852580766e-05,
      "loss": 1.0095,
      "step": 9926
    },
    {
      "epoch": 1.3956136651202025,
      "grad_norm": 1.446655035018921,
      "learning_rate": 1.652036127540131e-05,
      "loss": 0.9961,
      "step": 9927
    },
    {
      "epoch": 1.3957542527766063,
      "grad_norm": 1.3597248792648315,
      "learning_rate": 1.6392598839204876e-05,
      "loss": 1.1926,
      "step": 9928
    },
    {
      "epoch": 1.39589484043301,
      "grad_norm": 1.515208125114441,
      "learning_rate": 1.6265288234445508e-05,
      "loss": 1.1692,
      "step": 9929
    },
    {
      "epoch": 1.3960354280894138,
      "grad_norm": 1.4449068307876587,
      "learning_rate": 1.6138430149136074e-05,
      "loss": 0.9955,
      "step": 9930
    },
    {
      "epoch": 1.3961760157458176,
      "grad_norm": 1.6030352115631104,
      "learning_rate": 1.60120252688435e-05,
      "loss": 1.026,
      "step": 9931
    },
    {
      "epoch": 1.3963166034022212,
      "grad_norm": 1.4468598365783691,
      "learning_rate": 1.5886074276685148e-05,
      "loss": 1.2257,
      "step": 9932
    },
    {
      "epoch": 1.396457191058625,
      "grad_norm": 1.4078723192214966,
      "learning_rate": 1.5760577853326043e-05,
      "loss": 1.1966,
      "step": 9933
    },
    {
      "epoch": 1.3965977787150288,
      "grad_norm": 1.5549782514572144,
      "learning_rate": 1.563553667697444e-05,
      "loss": 1.1519,
      "step": 9934
    },
    {
      "epoch": 1.3967383663714326,
      "grad_norm": 1.4681469202041626,
      "learning_rate": 1.551095142337834e-05,
      "loss": 1.1396,
      "step": 9935
    },
    {
      "epoch": 1.3968789540278364,
      "grad_norm": 1.5694416761398315,
      "learning_rate": 1.538682276582184e-05,
      "loss": 0.9779,
      "step": 9936
    },
    {
      "epoch": 1.3970195416842401,
      "grad_norm": 2.0108251571655273,
      "learning_rate": 1.526315137512151e-05,
      "loss": 0.9873,
      "step": 9937
    },
    {
      "epoch": 1.397160129340644,
      "grad_norm": 1.6832231283187866,
      "learning_rate": 1.513993791962276e-05,
      "loss": 1.0874,
      "step": 9938
    },
    {
      "epoch": 1.3973007169970477,
      "grad_norm": 1.407278060913086,
      "learning_rate": 1.5017183065196217e-05,
      "loss": 1.1015,
      "step": 9939
    },
    {
      "epoch": 1.3974413046534515,
      "grad_norm": 1.6716896295547485,
      "learning_rate": 1.489488747523412e-05,
      "loss": 1.1058,
      "step": 9940
    },
    {
      "epoch": 1.397581892309855,
      "grad_norm": 1.471777081489563,
      "learning_rate": 1.4773051810646787e-05,
      "loss": 1.0913,
      "step": 9941
    },
    {
      "epoch": 1.397722479966259,
      "grad_norm": 1.6398338079452515,
      "learning_rate": 1.4651676729859154e-05,
      "loss": 1.1199,
      "step": 9942
    },
    {
      "epoch": 1.3978630676226627,
      "grad_norm": 1.476258397102356,
      "learning_rate": 1.4530762888806526e-05,
      "loss": 1.0316,
      "step": 9943
    },
    {
      "epoch": 1.3980036552790664,
      "grad_norm": 1.431758999824524,
      "learning_rate": 1.4410310940932125e-05,
      "loss": 1.1273,
      "step": 9944
    },
    {
      "epoch": 1.3981442429354702,
      "grad_norm": 1.7653008699417114,
      "learning_rate": 1.4290321537182671e-05,
      "loss": 0.9993,
      "step": 9945
    },
    {
      "epoch": 1.398284830591874,
      "grad_norm": 1.381285548210144,
      "learning_rate": 1.4170795326005693e-05,
      "loss": 0.934,
      "step": 9946
    },
    {
      "epoch": 1.3984254182482778,
      "grad_norm": 1.5265824794769287,
      "learning_rate": 1.4051732953345042e-05,
      "loss": 0.9804,
      "step": 9947
    },
    {
      "epoch": 1.3985660059046816,
      "grad_norm": 1.4404337406158447,
      "learning_rate": 1.3933135062638215e-05,
      "loss": 1.1058,
      "step": 9948
    },
    {
      "epoch": 1.3987065935610854,
      "grad_norm": 1.5033681392669678,
      "learning_rate": 1.3815002294812019e-05,
      "loss": 1.1765,
      "step": 9949
    },
    {
      "epoch": 1.398847181217489,
      "grad_norm": 1.3510181903839111,
      "learning_rate": 1.369733528828021e-05,
      "loss": 1.0923,
      "step": 9950
    },
    {
      "epoch": 1.398987768873893,
      "grad_norm": 1.7622442245483398,
      "learning_rate": 1.3580134678939261e-05,
      "loss": 0.9348,
      "step": 9951
    },
    {
      "epoch": 1.3991283565302965,
      "grad_norm": 2.0168447494506836,
      "learning_rate": 1.3463401100164975e-05,
      "loss": 0.8825,
      "step": 9952
    },
    {
      "epoch": 1.3992689441867003,
      "grad_norm": 1.5215697288513184,
      "learning_rate": 1.3347135182809944e-05,
      "loss": 1.1445,
      "step": 9953
    },
    {
      "epoch": 1.3994095318431041,
      "grad_norm": 1.52371346950531,
      "learning_rate": 1.3231337555198353e-05,
      "loss": 1.009,
      "step": 9954
    },
    {
      "epoch": 1.399550119499508,
      "grad_norm": 1.7899140119552612,
      "learning_rate": 1.311600884312435e-05,
      "loss": 0.9839,
      "step": 9955
    },
    {
      "epoch": 1.3996907071559117,
      "grad_norm": 1.3452858924865723,
      "learning_rate": 1.3001149669847813e-05,
      "loss": 1.0222,
      "step": 9956
    },
    {
      "epoch": 1.3998312948123155,
      "grad_norm": 1.5211012363433838,
      "learning_rate": 1.2886760656091123e-05,
      "loss": 1.136,
      "step": 9957
    },
    {
      "epoch": 1.3999718824687193,
      "grad_norm": 1.375019907951355,
      "learning_rate": 1.2772842420036002e-05,
      "loss": 0.9967,
      "step": 9958
    },
    {
      "epoch": 1.400112470125123,
      "grad_norm": 1.3383312225341797,
      "learning_rate": 1.2659395577319455e-05,
      "loss": 1.1737,
      "step": 9959
    },
    {
      "epoch": 1.4002530577815269,
      "grad_norm": 1.3556910753250122,
      "learning_rate": 1.2546420741031372e-05,
      "loss": 1.3124,
      "step": 9960
    },
    {
      "epoch": 1.4003936454379304,
      "grad_norm": 1.3317904472351074,
      "learning_rate": 1.2433918521711118e-05,
      "loss": 1.2581,
      "step": 9961
    },
    {
      "epoch": 1.4005342330943344,
      "grad_norm": 1.5275042057037354,
      "learning_rate": 1.2321889527343444e-05,
      "loss": 0.9289,
      "step": 9962
    },
    {
      "epoch": 1.400674820750738,
      "grad_norm": 1.4498409032821655,
      "learning_rate": 1.2210334363355891e-05,
      "loss": 0.9493,
      "step": 9963
    },
    {
      "epoch": 1.4008154084071418,
      "grad_norm": 1.6935808658599854,
      "learning_rate": 1.2099253632615448e-05,
      "loss": 0.9883,
      "step": 9964
    },
    {
      "epoch": 1.4009559960635456,
      "grad_norm": 1.7102680206298828,
      "learning_rate": 1.1988647935424702e-05,
      "loss": 1.0489,
      "step": 9965
    },
    {
      "epoch": 1.4010965837199494,
      "grad_norm": 1.5133944749832153,
      "learning_rate": 1.1878517869519546e-05,
      "loss": 1.0801,
      "step": 9966
    },
    {
      "epoch": 1.4012371713763532,
      "grad_norm": 1.3588306903839111,
      "learning_rate": 1.1768864030065307e-05,
      "loss": 1.092,
      "step": 9967
    },
    {
      "epoch": 1.401377759032757,
      "grad_norm": 2.0269277095794678,
      "learning_rate": 1.165968700965352e-05,
      "loss": 0.9445,
      "step": 9968
    },
    {
      "epoch": 1.4015183466891608,
      "grad_norm": 1.8912270069122314,
      "learning_rate": 1.1550987398299562e-05,
      "loss": 0.9566,
      "step": 9969
    },
    {
      "epoch": 1.4016589343455643,
      "grad_norm": 1.517310380935669,
      "learning_rate": 1.144276578343777e-05,
      "loss": 1.0672,
      "step": 9970
    },
    {
      "epoch": 1.4017995220019683,
      "grad_norm": 1.6604994535446167,
      "learning_rate": 1.1335022749919987e-05,
      "loss": 0.9359,
      "step": 9971
    },
    {
      "epoch": 1.401940109658372,
      "grad_norm": 1.3888057470321655,
      "learning_rate": 1.122775888001153e-05,
      "loss": 1.016,
      "step": 9972
    },
    {
      "epoch": 1.4020806973147757,
      "grad_norm": 2.1255080699920654,
      "learning_rate": 1.1120974753388203e-05,
      "loss": 1.0393,
      "step": 9973
    },
    {
      "epoch": 1.4022212849711795,
      "grad_norm": 1.5035054683685303,
      "learning_rate": 1.1014670947133376e-05,
      "loss": 1.0874,
      "step": 9974
    },
    {
      "epoch": 1.4023618726275833,
      "grad_norm": 1.5519386529922485,
      "learning_rate": 1.0908848035733976e-05,
      "loss": 0.8765,
      "step": 9975
    },
    {
      "epoch": 1.402502460283987,
      "grad_norm": 1.6020946502685547,
      "learning_rate": 1.0803506591079083e-05,
      "loss": 0.8293,
      "step": 9976
    },
    {
      "epoch": 1.4026430479403909,
      "grad_norm": 1.2626152038574219,
      "learning_rate": 1.0698647182455167e-05,
      "loss": 1.0759,
      "step": 9977
    },
    {
      "epoch": 1.4027836355967946,
      "grad_norm": 1.4717944860458374,
      "learning_rate": 1.0594270376543857e-05,
      "loss": 1.017,
      "step": 9978
    },
    {
      "epoch": 1.4029242232531984,
      "grad_norm": 1.6156845092773438,
      "learning_rate": 1.0490376737418672e-05,
      "loss": 1.0869,
      "step": 9979
    },
    {
      "epoch": 1.4030648109096022,
      "grad_norm": 1.8434520959854126,
      "learning_rate": 1.0386966826542188e-05,
      "loss": 0.9404,
      "step": 9980
    },
    {
      "epoch": 1.4032053985660058,
      "grad_norm": 1.5644443035125732,
      "learning_rate": 1.0284041202762317e-05,
      "loss": 0.8429,
      "step": 9981
    },
    {
      "epoch": 1.4033459862224098,
      "grad_norm": 1.7653180360794067,
      "learning_rate": 1.0181600422310267e-05,
      "loss": 1.228,
      "step": 9982
    },
    {
      "epoch": 1.4034865738788134,
      "grad_norm": 1.9423809051513672,
      "learning_rate": 1.0079645038796737e-05,
      "loss": 0.9513,
      "step": 9983
    },
    {
      "epoch": 1.4036271615352172,
      "grad_norm": 1.35899817943573,
      "learning_rate": 9.978175603209704e-06,
      "loss": 0.9714,
      "step": 9984
    },
    {
      "epoch": 1.403767749191621,
      "grad_norm": 2.0984251499176025,
      "learning_rate": 9.877192663910706e-06,
      "loss": 1.0042,
      "step": 9985
    },
    {
      "epoch": 1.4039083368480247,
      "grad_norm": 1.4138425588607788,
      "learning_rate": 9.776696766631799e-06,
      "loss": 1.0777,
      "step": 9986
    },
    {
      "epoch": 1.4040489245044285,
      "grad_norm": 1.422861933708191,
      "learning_rate": 9.67668845447346e-06,
      "loss": 1.1485,
      "step": 9987
    },
    {
      "epoch": 1.4041895121608323,
      "grad_norm": 1.67879056930542,
      "learning_rate": 9.577168267900983e-06,
      "loss": 1.1132,
      "step": 9988
    },
    {
      "epoch": 1.404330099817236,
      "grad_norm": 1.4001996517181396,
      "learning_rate": 9.478136744741739e-06,
      "loss": 0.9941,
      "step": 9989
    },
    {
      "epoch": 1.4044706874736397,
      "grad_norm": 1.4436583518981934,
      "learning_rate": 9.379594420182136e-06,
      "loss": 0.8994,
      "step": 9990
    },
    {
      "epoch": 1.4046112751300437,
      "grad_norm": 1.570931315422058,
      "learning_rate": 9.281541826765405e-06,
      "loss": 0.8929,
      "step": 9991
    },
    {
      "epoch": 1.4047518627864473,
      "grad_norm": 1.5872441530227661,
      "learning_rate": 9.183979494387319e-06,
      "loss": 1.3087,
      "step": 9992
    },
    {
      "epoch": 1.404892450442851,
      "grad_norm": 1.3029756546020508,
      "learning_rate": 9.086907950294742e-06,
      "loss": 1.039,
      "step": 9993
    },
    {
      "epoch": 1.4050330380992548,
      "grad_norm": 1.5566991567611694,
      "learning_rate": 8.990327719082091e-06,
      "loss": 1.0469,
      "step": 9994
    },
    {
      "epoch": 1.4051736257556586,
      "grad_norm": 1.3652079105377197,
      "learning_rate": 8.894239322688613e-06,
      "loss": 0.8799,
      "step": 9995
    },
    {
      "epoch": 1.4053142134120624,
      "grad_norm": 1.4775993824005127,
      "learning_rate": 8.798643280395746e-06,
      "loss": 1.109,
      "step": 9996
    },
    {
      "epoch": 1.4054548010684662,
      "grad_norm": 1.6411864757537842,
      "learning_rate": 8.703540108823693e-06,
      "loss": 1.0838,
      "step": 9997
    },
    {
      "epoch": 1.40559538872487,
      "grad_norm": 1.7162078619003296,
      "learning_rate": 8.608930321929376e-06,
      "loss": 1.1798,
      "step": 9998
    },
    {
      "epoch": 1.4057359763812738,
      "grad_norm": 1.5216675996780396,
      "learning_rate": 8.514814431003649e-06,
      "loss": 0.9578,
      "step": 9999
    },
    {
      "epoch": 1.4058765640376776,
      "grad_norm": 1.660534143447876,
      "learning_rate": 8.421192944667833e-06,
      "loss": 1.1326,
      "step": 10000
    },
    {
      "epoch": 1.4058765640376776,
      "eval_loss": 1.141832947731018,
      "eval_runtime": 771.9328,
      "eval_samples_per_second": 16.382,
      "eval_steps_per_second": 8.191,
      "step": 10000
    },
    {
      "epoch": 1.4060171516940811,
      "grad_norm": 1.54900324344635,
      "learning_rate": 8.328066368871667e-06,
      "loss": 1.0781,
      "step": 10001
    },
    {
      "epoch": 1.4061577393504852,
      "grad_norm": 1.4569193124771118,
      "learning_rate": 8.235435206889864e-06,
      "loss": 1.0676,
      "step": 10002
    },
    {
      "epoch": 1.4062983270068887,
      "grad_norm": 1.3925812244415283,
      "learning_rate": 8.143299959320239e-06,
      "loss": 1.0051,
      "step": 10003
    },
    {
      "epoch": 1.4064389146632925,
      "grad_norm": 1.7093617916107178,
      "learning_rate": 8.051661124080457e-06,
      "loss": 1.124,
      "step": 10004
    },
    {
      "epoch": 1.4065795023196963,
      "grad_norm": 1.6883890628814697,
      "learning_rate": 7.960519196405458e-06,
      "loss": 0.9562,
      "step": 10005
    },
    {
      "epoch": 1.4067200899761,
      "grad_norm": 1.820696473121643,
      "learning_rate": 7.869874668844657e-06,
      "loss": 1.1758,
      "step": 10006
    },
    {
      "epoch": 1.4068606776325039,
      "grad_norm": 1.4891256093978882,
      "learning_rate": 7.77972803125998e-06,
      "loss": 1.0782,
      "step": 10007
    },
    {
      "epoch": 1.4070012652889077,
      "grad_norm": 1.34604012966156,
      "learning_rate": 7.6900797708218e-06,
      "loss": 1.0434,
      "step": 10008
    },
    {
      "epoch": 1.4071418529453115,
      "grad_norm": 1.4781934022903442,
      "learning_rate": 7.600930372007742e-06,
      "loss": 1.0933,
      "step": 10009
    },
    {
      "epoch": 1.407282440601715,
      "grad_norm": 1.5137776136398315,
      "learning_rate": 7.512280316599318e-06,
      "loss": 0.9533,
      "step": 10010
    },
    {
      "epoch": 1.407423028258119,
      "grad_norm": 1.5141828060150146,
      "learning_rate": 7.424130083679515e-06,
      "loss": 1.2012,
      "step": 10011
    },
    {
      "epoch": 1.4075636159145226,
      "grad_norm": 1.3892457485198975,
      "learning_rate": 7.336480149630265e-06,
      "loss": 1.0006,
      "step": 10012
    },
    {
      "epoch": 1.4077042035709264,
      "grad_norm": 1.6995737552642822,
      "learning_rate": 7.249330988129299e-06,
      "loss": 1.0421,
      "step": 10013
    },
    {
      "epoch": 1.4078447912273302,
      "grad_norm": 1.8670313358306885,
      "learning_rate": 7.1626830701488345e-06,
      "loss": 1.0338,
      "step": 10014
    },
    {
      "epoch": 1.407985378883734,
      "grad_norm": 1.5453953742980957,
      "learning_rate": 7.0765368639517545e-06,
      "loss": 1.0141,
      "step": 10015
    },
    {
      "epoch": 1.4081259665401378,
      "grad_norm": 1.4337471723556519,
      "learning_rate": 6.990892835089724e-06,
      "loss": 1.1692,
      "step": 10016
    },
    {
      "epoch": 1.4082665541965416,
      "grad_norm": 1.706274151802063,
      "learning_rate": 6.9057514464006545e-06,
      "loss": 1.1189,
      "step": 10017
    },
    {
      "epoch": 1.4084071418529454,
      "grad_norm": 1.5855902433395386,
      "learning_rate": 6.821113158005698e-06,
      "loss": 1.0914,
      "step": 10018
    },
    {
      "epoch": 1.4085477295093491,
      "grad_norm": 1.453399896621704,
      "learning_rate": 6.73697842730755e-06,
      "loss": 1.2546,
      "step": 10019
    },
    {
      "epoch": 1.408688317165753,
      "grad_norm": 1.6314852237701416,
      "learning_rate": 6.653347708987445e-06,
      "loss": 0.9053,
      "step": 10020
    },
    {
      "epoch": 1.4088289048221565,
      "grad_norm": 1.481012225151062,
      "learning_rate": 6.570221455002712e-06,
      "loss": 0.9741,
      "step": 10021
    },
    {
      "epoch": 1.4089694924785605,
      "grad_norm": 1.5939048528671265,
      "learning_rate": 6.487600114584813e-06,
      "loss": 1.1175,
      "step": 10022
    },
    {
      "epoch": 1.409110080134964,
      "grad_norm": 1.4364169836044312,
      "learning_rate": 6.405484134236328e-06,
      "loss": 1.0112,
      "step": 10023
    },
    {
      "epoch": 1.4092506677913679,
      "grad_norm": 1.6434170007705688,
      "learning_rate": 6.323873957728476e-06,
      "loss": 1.0921,
      "step": 10024
    },
    {
      "epoch": 1.4093912554477717,
      "grad_norm": 1.486871361732483,
      "learning_rate": 6.242770026099432e-06,
      "loss": 1.0162,
      "step": 10025
    },
    {
      "epoch": 1.4095318431041755,
      "grad_norm": 1.5557019710540771,
      "learning_rate": 6.162172777651354e-06,
      "loss": 0.9861,
      "step": 10026
    },
    {
      "epoch": 1.4096724307605792,
      "grad_norm": 1.6423131227493286,
      "learning_rate": 6.082082647948217e-06,
      "loss": 0.9587,
      "step": 10027
    },
    {
      "epoch": 1.409813018416983,
      "grad_norm": 1.713781714439392,
      "learning_rate": 6.002500069813411e-06,
      "loss": 1.1247,
      "step": 10028
    },
    {
      "epoch": 1.4099536060733868,
      "grad_norm": 1.4560606479644775,
      "learning_rate": 5.923425473327382e-06,
      "loss": 0.885,
      "step": 10029
    },
    {
      "epoch": 1.4100941937297904,
      "grad_norm": 1.3980175256729126,
      "learning_rate": 5.844859285825399e-06,
      "loss": 1.0785,
      "step": 10030
    },
    {
      "epoch": 1.4102347813861944,
      "grad_norm": 1.7765486240386963,
      "learning_rate": 5.766801931895127e-06,
      "loss": 0.9044,
      "step": 10031
    },
    {
      "epoch": 1.410375369042598,
      "grad_norm": 1.3481332063674927,
      "learning_rate": 5.6892538333744415e-06,
      "loss": 0.9998,
      "step": 10032
    },
    {
      "epoch": 1.4105159566990018,
      "grad_norm": 1.4851605892181396,
      "learning_rate": 5.612215409349042e-06,
      "loss": 0.9773,
      "step": 10033
    },
    {
      "epoch": 1.4106565443554056,
      "grad_norm": 1.4450953006744385,
      "learning_rate": 5.53568707615042e-06,
      "loss": 1.1036,
      "step": 10034
    },
    {
      "epoch": 1.4107971320118093,
      "grad_norm": 1.546181082725525,
      "learning_rate": 5.45966924735305e-06,
      "loss": 1.0861,
      "step": 10035
    },
    {
      "epoch": 1.4109377196682131,
      "grad_norm": 1.5137019157409668,
      "learning_rate": 5.384162333772758e-06,
      "loss": 1.222,
      "step": 10036
    },
    {
      "epoch": 1.411078307324617,
      "grad_norm": 1.3549363613128662,
      "learning_rate": 5.309166743464533e-06,
      "loss": 0.9679,
      "step": 10037
    },
    {
      "epoch": 1.4112188949810207,
      "grad_norm": 1.432252049446106,
      "learning_rate": 5.2346828817197214e-06,
      "loss": 1.2199,
      "step": 10038
    },
    {
      "epoch": 1.4113594826374245,
      "grad_norm": 1.3918685913085938,
      "learning_rate": 5.1607111510644015e-06,
      "loss": 1.0585,
      "step": 10039
    },
    {
      "epoch": 1.4115000702938283,
      "grad_norm": 1.3609395027160645,
      "learning_rate": 5.0872519512566555e-06,
      "loss": 0.9926,
      "step": 10040
    },
    {
      "epoch": 1.4116406579502319,
      "grad_norm": 1.7054914236068726,
      "learning_rate": 5.014305679285091e-06,
      "loss": 1.0261,
      "step": 10041
    },
    {
      "epoch": 1.4117812456066359,
      "grad_norm": 1.3980647325515747,
      "learning_rate": 4.941872729366226e-06,
      "loss": 1.043,
      "step": 10042
    },
    {
      "epoch": 1.4119218332630394,
      "grad_norm": 1.5258545875549316,
      "learning_rate": 4.86995349294248e-06,
      "loss": 0.8457,
      "step": 10043
    },
    {
      "epoch": 1.4120624209194432,
      "grad_norm": 1.4309632778167725,
      "learning_rate": 4.798548358680088e-06,
      "loss": 1.1309,
      "step": 10044
    },
    {
      "epoch": 1.412203008575847,
      "grad_norm": 1.5451196432113647,
      "learning_rate": 4.7276577124669265e-06,
      "loss": 1.0838,
      "step": 10045
    },
    {
      "epoch": 1.4123435962322508,
      "grad_norm": 1.8075615167617798,
      "learning_rate": 4.657281937410507e-06,
      "loss": 1.1043,
      "step": 10046
    },
    {
      "epoch": 1.4124841838886546,
      "grad_norm": 1.6595711708068848,
      "learning_rate": 4.587421413835857e-06,
      "loss": 1.0913,
      "step": 10047
    },
    {
      "epoch": 1.4126247715450584,
      "grad_norm": 1.6672766208648682,
      "learning_rate": 4.5180765192834876e-06,
      "loss": 1.0526,
      "step": 10048
    },
    {
      "epoch": 1.4127653592014622,
      "grad_norm": 1.4196292161941528,
      "learning_rate": 4.449247628507347e-06,
      "loss": 1.0124,
      "step": 10049
    },
    {
      "epoch": 1.4129059468578657,
      "grad_norm": 1.43025541305542,
      "learning_rate": 4.380935113472895e-06,
      "loss": 1.2858,
      "step": 10050
    },
    {
      "epoch": 1.4130465345142698,
      "grad_norm": 1.745863676071167,
      "learning_rate": 4.313139343354567e-06,
      "loss": 1.2403,
      "step": 10051
    },
    {
      "epoch": 1.4131871221706733,
      "grad_norm": 1.447483777999878,
      "learning_rate": 4.2458606845348416e-06,
      "loss": 1.1717,
      "step": 10052
    },
    {
      "epoch": 1.4133277098270771,
      "grad_norm": 1.5123822689056396,
      "learning_rate": 4.179099500601203e-06,
      "loss": 1.0045,
      "step": 10053
    },
    {
      "epoch": 1.413468297483481,
      "grad_norm": 1.316619634628296,
      "learning_rate": 4.11285615234468e-06,
      "loss": 1.0046,
      "step": 10054
    },
    {
      "epoch": 1.4136088851398847,
      "grad_norm": 1.5832711458206177,
      "learning_rate": 4.047130997757953e-06,
      "loss": 1.1098,
      "step": 10055
    },
    {
      "epoch": 1.4137494727962885,
      "grad_norm": 1.4532548189163208,
      "learning_rate": 3.981924392032932e-06,
      "loss": 0.8805,
      "step": 10056
    },
    {
      "epoch": 1.4138900604526923,
      "grad_norm": 1.3301550149917603,
      "learning_rate": 3.917236687559511e-06,
      "loss": 1.0813,
      "step": 10057
    },
    {
      "epoch": 1.414030648109096,
      "grad_norm": 1.738152027130127,
      "learning_rate": 3.85306823392324e-06,
      "loss": 0.983,
      "step": 10058
    },
    {
      "epoch": 1.4141712357654999,
      "grad_norm": 1.396674394607544,
      "learning_rate": 3.789419377903425e-06,
      "loss": 0.9631,
      "step": 10059
    },
    {
      "epoch": 1.4143118234219036,
      "grad_norm": 1.5426744222640991,
      "learning_rate": 3.7262904634717377e-06,
      "loss": 1.1041,
      "step": 10060
    },
    {
      "epoch": 1.4144524110783072,
      "grad_norm": 1.6268532276153564,
      "learning_rate": 3.6636818317894004e-06,
      "loss": 1.0655,
      "step": 10061
    },
    {
      "epoch": 1.4145929987347112,
      "grad_norm": 1.651301622390747,
      "learning_rate": 3.6015938212063392e-06,
      "loss": 1.042,
      "step": 10062
    },
    {
      "epoch": 1.4147335863911148,
      "grad_norm": 1.430396556854248,
      "learning_rate": 3.540026767258808e-06,
      "loss": 1.0215,
      "step": 10063
    },
    {
      "epoch": 1.4148741740475186,
      "grad_norm": 1.6382055282592773,
      "learning_rate": 3.478981002667714e-06,
      "loss": 1.0412,
      "step": 10064
    },
    {
      "epoch": 1.4150147617039224,
      "grad_norm": 1.9527270793914795,
      "learning_rate": 3.418456857336816e-06,
      "loss": 1.0212,
      "step": 10065
    },
    {
      "epoch": 1.4151553493603262,
      "grad_norm": 1.3453137874603271,
      "learning_rate": 3.358454658350929e-06,
      "loss": 1.1764,
      "step": 10066
    },
    {
      "epoch": 1.41529593701673,
      "grad_norm": 1.5746783018112183,
      "learning_rate": 3.298974729974158e-06,
      "loss": 1.1329,
      "step": 10067
    },
    {
      "epoch": 1.4154365246731337,
      "grad_norm": 1.6384106874465942,
      "learning_rate": 3.240017393648176e-06,
      "loss": 1.0412,
      "step": 10068
    },
    {
      "epoch": 1.4155771123295375,
      "grad_norm": 1.718848705291748,
      "learning_rate": 3.1815829679904372e-06,
      "loss": 0.8389,
      "step": 10069
    },
    {
      "epoch": 1.415717699985941,
      "grad_norm": 1.4566339254379272,
      "learning_rate": 3.1236717687924797e-06,
      "loss": 1.0797,
      "step": 10070
    },
    {
      "epoch": 1.4158582876423451,
      "grad_norm": 1.5584436655044556,
      "learning_rate": 3.0662841090183358e-06,
      "loss": 1.1724,
      "step": 10071
    },
    {
      "epoch": 1.4159988752987487,
      "grad_norm": 1.6342592239379883,
      "learning_rate": 3.009420298802412e-06,
      "loss": 1.035,
      "step": 10072
    },
    {
      "epoch": 1.4161394629551525,
      "grad_norm": 1.561874270439148,
      "learning_rate": 2.9530806454484473e-06,
      "loss": 1.0055,
      "step": 10073
    },
    {
      "epoch": 1.4162800506115563,
      "grad_norm": 1.298256278038025,
      "learning_rate": 2.8972654534273113e-06,
      "loss": 1.0938,
      "step": 10074
    },
    {
      "epoch": 1.41642063826796,
      "grad_norm": 1.4308435916900635,
      "learning_rate": 2.8419750243758536e-06,
      "loss": 0.9461,
      "step": 10075
    },
    {
      "epoch": 1.4165612259243638,
      "grad_norm": 1.56977117061615,
      "learning_rate": 2.787209657094725e-06,
      "loss": 1.0536,
      "step": 10076
    },
    {
      "epoch": 1.4167018135807676,
      "grad_norm": 1.5280758142471313,
      "learning_rate": 2.7329696475472346e-06,
      "loss": 0.9245,
      "step": 10077
    },
    {
      "epoch": 1.4168424012371714,
      "grad_norm": 1.5960465669631958,
      "learning_rate": 2.679255288857263e-06,
      "loss": 0.9905,
      "step": 10078
    },
    {
      "epoch": 1.4169829888935752,
      "grad_norm": 1.6018037796020508,
      "learning_rate": 2.6260668713082636e-06,
      "loss": 1.1196,
      "step": 10079
    },
    {
      "epoch": 1.417123576549979,
      "grad_norm": 1.269830346107483,
      "learning_rate": 2.573404682341252e-06,
      "loss": 1.0781,
      "step": 10080
    },
    {
      "epoch": 1.4172641642063826,
      "grad_norm": 1.424284815788269,
      "learning_rate": 2.521269006553462e-06,
      "loss": 1.1786,
      "step": 10081
    },
    {
      "epoch": 1.4174047518627866,
      "grad_norm": 1.4491820335388184,
      "learning_rate": 2.4696601256966934e-06,
      "loss": 1.099,
      "step": 10082
    },
    {
      "epoch": 1.4175453395191902,
      "grad_norm": 1.4884589910507202,
      "learning_rate": 2.418578318675857e-06,
      "loss": 1.0535,
      "step": 10083
    },
    {
      "epoch": 1.417685927175594,
      "grad_norm": 1.6442766189575195,
      "learning_rate": 2.3680238615474637e-06,
      "loss": 1.0095,
      "step": 10084
    },
    {
      "epoch": 1.4178265148319977,
      "grad_norm": 1.584853172302246,
      "learning_rate": 2.3179970275180928e-06,
      "loss": 0.9162,
      "step": 10085
    },
    {
      "epoch": 1.4179671024884015,
      "grad_norm": 1.4322537183761597,
      "learning_rate": 2.2684980869429607e-06,
      "loss": 1.1163,
      "step": 10086
    },
    {
      "epoch": 1.4181076901448053,
      "grad_norm": 1.476090431213379,
      "learning_rate": 2.219527307324498e-06,
      "loss": 1.1379,
      "step": 10087
    },
    {
      "epoch": 1.418248277801209,
      "grad_norm": 1.4917426109313965,
      "learning_rate": 2.1710849533106313e-06,
      "loss": 0.9638,
      "step": 10088
    },
    {
      "epoch": 1.418388865457613,
      "grad_norm": 1.559622883796692,
      "learning_rate": 2.123171286693726e-06,
      "loss": 1.0481,
      "step": 10089
    },
    {
      "epoch": 1.4185294531140165,
      "grad_norm": 1.3519891500473022,
      "learning_rate": 2.0757865664091324e-06,
      "loss": 1.1547,
      "step": 10090
    },
    {
      "epoch": 1.4186700407704205,
      "grad_norm": 1.4763003587722778,
      "learning_rate": 2.028931048533478e-06,
      "loss": 1.1633,
      "step": 10091
    },
    {
      "epoch": 1.418810628426824,
      "grad_norm": 1.7164138555526733,
      "learning_rate": 1.9826049862835315e-06,
      "loss": 0.9754,
      "step": 10092
    },
    {
      "epoch": 1.4189512160832278,
      "grad_norm": 1.5654492378234863,
      "learning_rate": 1.9368086300148413e-06,
      "loss": 0.9807,
      "step": 10093
    },
    {
      "epoch": 1.4190918037396316,
      "grad_norm": 1.5150296688079834,
      "learning_rate": 1.8915422272201e-06,
      "loss": 1.1863,
      "step": 10094
    },
    {
      "epoch": 1.4192323913960354,
      "grad_norm": 1.3852258920669556,
      "learning_rate": 1.8468060225282691e-06,
      "loss": 1.0563,
      "step": 10095
    },
    {
      "epoch": 1.4193729790524392,
      "grad_norm": 1.410960078239441,
      "learning_rate": 1.8026002577028911e-06,
      "loss": 0.988,
      "step": 10096
    },
    {
      "epoch": 1.419513566708843,
      "grad_norm": 1.993335485458374,
      "learning_rate": 1.7589251716408905e-06,
      "loss": 0.9256,
      "step": 10097
    },
    {
      "epoch": 1.4196541543652468,
      "grad_norm": 1.7730276584625244,
      "learning_rate": 1.715781000371508e-06,
      "loss": 0.907,
      "step": 10098
    },
    {
      "epoch": 1.4197947420216506,
      "grad_norm": 1.5144199132919312,
      "learning_rate": 1.6731679770544573e-06,
      "loss": 1.0926,
      "step": 10099
    },
    {
      "epoch": 1.4199353296780544,
      "grad_norm": 1.6817864179611206,
      "learning_rate": 1.631086331979259e-06,
      "loss": 0.9826,
      "step": 10100
    },
    {
      "epoch": 1.420075917334458,
      "grad_norm": 1.5242640972137451,
      "learning_rate": 1.5895362925637092e-06,
      "loss": 1.1107,
      "step": 10101
    },
    {
      "epoch": 1.420216504990862,
      "grad_norm": 1.4639451503753662,
      "learning_rate": 1.548518083352679e-06,
      "loss": 1.0804,
      "step": 10102
    },
    {
      "epoch": 1.4203570926472655,
      "grad_norm": 1.5190259218215942,
      "learning_rate": 1.5080319260169396e-06,
      "loss": 1.1056,
      "step": 10103
    },
    {
      "epoch": 1.4204976803036693,
      "grad_norm": 1.4020415544509888,
      "learning_rate": 1.4680780393519388e-06,
      "loss": 1.0302,
      "step": 10104
    },
    {
      "epoch": 1.420638267960073,
      "grad_norm": 1.49056077003479,
      "learning_rate": 1.4286566392766155e-06,
      "loss": 1.068,
      "step": 10105
    },
    {
      "epoch": 1.4207788556164769,
      "grad_norm": 1.3766329288482666,
      "learning_rate": 1.3897679388322537e-06,
      "loss": 1.1483,
      "step": 10106
    },
    {
      "epoch": 1.4209194432728807,
      "grad_norm": 1.5064738988876343,
      "learning_rate": 1.3514121481813192e-06,
      "loss": 1.1569,
      "step": 10107
    },
    {
      "epoch": 1.4210600309292845,
      "grad_norm": 1.7343271970748901,
      "learning_rate": 1.3135894746063248e-06,
      "loss": 1.0202,
      "step": 10108
    },
    {
      "epoch": 1.4212006185856882,
      "grad_norm": 1.707479476928711,
      "learning_rate": 1.2763001225087557e-06,
      "loss": 0.9222,
      "step": 10109
    },
    {
      "epoch": 1.4213412062420918,
      "grad_norm": 1.7774211168289185,
      "learning_rate": 1.2395442934077684e-06,
      "loss": 1.0014,
      "step": 10110
    },
    {
      "epoch": 1.4214817938984958,
      "grad_norm": 1.6609519720077515,
      "learning_rate": 1.2033221859394039e-06,
      "loss": 1.0093,
      "step": 10111
    },
    {
      "epoch": 1.4216223815548994,
      "grad_norm": 1.4810659885406494,
      "learning_rate": 1.1676339958552661e-06,
      "loss": 1.0869,
      "step": 10112
    },
    {
      "epoch": 1.4217629692113032,
      "grad_norm": 1.5024675130844116,
      "learning_rate": 1.1324799160217115e-06,
      "loss": 1.089,
      "step": 10113
    },
    {
      "epoch": 1.421903556867707,
      "grad_norm": 1.6299402713775635,
      "learning_rate": 1.0978601364185604e-06,
      "loss": 0.8959,
      "step": 10114
    },
    {
      "epoch": 1.4220441445241108,
      "grad_norm": 1.4576629400253296,
      "learning_rate": 1.0637748441380436e-06,
      "loss": 1.0106,
      "step": 10115
    },
    {
      "epoch": 1.4221847321805146,
      "grad_norm": 1.4317842721939087,
      "learning_rate": 1.0302242233840797e-06,
      "loss": 1.1208,
      "step": 10116
    },
    {
      "epoch": 1.4223253198369183,
      "grad_norm": 1.4942706823349,
      "learning_rate": 9.97208455471066e-07,
      "loss": 1.0498,
      "step": 10117
    },
    {
      "epoch": 1.4224659074933221,
      "grad_norm": 1.4254815578460693,
      "learning_rate": 9.647277188229332e-07,
      "loss": 1.1439,
      "step": 10118
    },
    {
      "epoch": 1.422606495149726,
      "grad_norm": 2.0324721336364746,
      "learning_rate": 9.327821889722254e-07,
      "loss": 1.0498,
      "step": 10119
    },
    {
      "epoch": 1.4227470828061297,
      "grad_norm": 1.3703747987747192,
      "learning_rate": 9.013720385590784e-07,
      "loss": 0.9559,
      "step": 10120
    },
    {
      "epoch": 1.4228876704625333,
      "grad_norm": 1.4204075336456299,
      "learning_rate": 8.704974373303421e-07,
      "loss": 1.0354,
      "step": 10121
    },
    {
      "epoch": 1.4230282581189373,
      "grad_norm": 1.3954120874404907,
      "learning_rate": 8.401585521386701e-07,
      "loss": 0.98,
      "step": 10122
    },
    {
      "epoch": 1.4231688457753409,
      "grad_norm": 1.5039376020431519,
      "learning_rate": 8.103555469415769e-07,
      "loss": 1.0555,
      "step": 10123
    },
    {
      "epoch": 1.4233094334317447,
      "grad_norm": 1.5947186946868896,
      "learning_rate": 7.810885828005821e-07,
      "loss": 1.1265,
      "step": 10124
    },
    {
      "epoch": 1.4234500210881484,
      "grad_norm": 1.403609275817871,
      "learning_rate": 7.523578178803448e-07,
      "loss": 1.0734,
      "step": 10125
    },
    {
      "epoch": 1.4235906087445522,
      "grad_norm": 1.4748163223266602,
      "learning_rate": 7.241634074477199e-07,
      "loss": 0.971,
      "step": 10126
    },
    {
      "epoch": 1.423731196400956,
      "grad_norm": 1.3447434902191162,
      "learning_rate": 6.965055038710256e-07,
      "loss": 1.0506,
      "step": 10127
    },
    {
      "epoch": 1.4238717840573598,
      "grad_norm": 1.4743647575378418,
      "learning_rate": 6.693842566193098e-07,
      "loss": 1.1227,
      "step": 10128
    },
    {
      "epoch": 1.4240123717137636,
      "grad_norm": 1.6753771305084229,
      "learning_rate": 6.427998122612744e-07,
      "loss": 1.1302,
      "step": 10129
    },
    {
      "epoch": 1.4241529593701672,
      "grad_norm": 1.6611933708190918,
      "learning_rate": 6.167523144647081e-07,
      "loss": 0.9512,
      "step": 10130
    },
    {
      "epoch": 1.4242935470265712,
      "grad_norm": 1.4658652544021606,
      "learning_rate": 5.912419039955319e-07,
      "loss": 0.8932,
      "step": 10131
    },
    {
      "epoch": 1.4244341346829748,
      "grad_norm": 1.4171998500823975,
      "learning_rate": 5.662687187172222e-07,
      "loss": 1.0305,
      "step": 10132
    },
    {
      "epoch": 1.4245747223393785,
      "grad_norm": 1.9837535619735718,
      "learning_rate": 5.418328935899886e-07,
      "loss": 1.1661,
      "step": 10133
    },
    {
      "epoch": 1.4247153099957823,
      "grad_norm": 1.568318486213684,
      "learning_rate": 5.17934560669997e-07,
      "loss": 1.1471,
      "step": 10134
    },
    {
      "epoch": 1.4248558976521861,
      "grad_norm": 1.488501787185669,
      "learning_rate": 4.945738491086593e-07,
      "loss": 1.1961,
      "step": 10135
    },
    {
      "epoch": 1.42499648530859,
      "grad_norm": 1.4268805980682373,
      "learning_rate": 4.717508851521557e-07,
      "loss": 1.1955,
      "step": 10136
    },
    {
      "epoch": 1.4251370729649937,
      "grad_norm": 1.5711908340454102,
      "learning_rate": 4.4946579214032445e-07,
      "loss": 1.147,
      "step": 10137
    },
    {
      "epoch": 1.4252776606213975,
      "grad_norm": 1.5149359703063965,
      "learning_rate": 4.277186905063735e-07,
      "loss": 1.009,
      "step": 10138
    },
    {
      "epoch": 1.4254182482778013,
      "grad_norm": 1.5554217100143433,
      "learning_rate": 4.065096977760696e-07,
      "loss": 0.9109,
      "step": 10139
    },
    {
      "epoch": 1.425558835934205,
      "grad_norm": 1.5995018482208252,
      "learning_rate": 3.8583892856715044e-07,
      "loss": 1.0819,
      "step": 10140
    },
    {
      "epoch": 1.4256994235906086,
      "grad_norm": 1.4817099571228027,
      "learning_rate": 3.657064945886801e-07,
      "loss": 1.0538,
      "step": 10141
    },
    {
      "epoch": 1.4258400112470127,
      "grad_norm": 1.6059792041778564,
      "learning_rate": 3.461125046403946e-07,
      "loss": 1.1392,
      "step": 10142
    },
    {
      "epoch": 1.4259805989034162,
      "grad_norm": 1.4216828346252441,
      "learning_rate": 3.2705706461217957e-07,
      "loss": 1.1332,
      "step": 10143
    },
    {
      "epoch": 1.42612118655982,
      "grad_norm": 1.3517811298370361,
      "learning_rate": 3.085402774836044e-07,
      "loss": 0.9933,
      "step": 10144
    },
    {
      "epoch": 1.4262617742162238,
      "grad_norm": 1.4765430688858032,
      "learning_rate": 2.9056224332312254e-07,
      "loss": 1.1358,
      "step": 10145
    },
    {
      "epoch": 1.4264023618726276,
      "grad_norm": 1.7444703578948975,
      "learning_rate": 2.7312305928769435e-07,
      "loss": 1.1322,
      "step": 10146
    },
    {
      "epoch": 1.4265429495290314,
      "grad_norm": 1.7359869480133057,
      "learning_rate": 2.562228196222427e-07,
      "loss": 1.1518,
      "step": 10147
    },
    {
      "epoch": 1.4266835371854352,
      "grad_norm": 1.5000073909759521,
      "learning_rate": 2.3986161565906497e-07,
      "loss": 1.1233,
      "step": 10148
    },
    {
      "epoch": 1.426824124841839,
      "grad_norm": 1.3768244981765747,
      "learning_rate": 2.2403953581746628e-07,
      "loss": 0.9466,
      "step": 10149
    },
    {
      "epoch": 1.4269647124982425,
      "grad_norm": 1.4707409143447876,
      "learning_rate": 2.0875666560316033e-07,
      "loss": 1.0012,
      "step": 10150
    },
    {
      "epoch": 1.4271053001546465,
      "grad_norm": 1.767403483390808,
      "learning_rate": 1.9401308760795822e-07,
      "loss": 0.9993,
      "step": 10151
    },
    {
      "epoch": 1.42724588781105,
      "grad_norm": 1.2921019792556763,
      "learning_rate": 1.7980888150913587e-07,
      "loss": 1.1283,
      "step": 10152
    },
    {
      "epoch": 1.427386475467454,
      "grad_norm": 1.6893541812896729,
      "learning_rate": 1.6614412406908973e-07,
      "loss": 1.1565,
      "step": 10153
    },
    {
      "epoch": 1.4275270631238577,
      "grad_norm": 1.6268200874328613,
      "learning_rate": 1.5301888913497043e-07,
      "loss": 1.0266,
      "step": 10154
    },
    {
      "epoch": 1.4276676507802615,
      "grad_norm": 1.6154248714447021,
      "learning_rate": 1.4043324763822751e-07,
      "loss": 0.9309,
      "step": 10155
    },
    {
      "epoch": 1.4278082384366653,
      "grad_norm": 1.4116066694259644,
      "learning_rate": 1.2838726759423214e-07,
      "loss": 1.0605,
      "step": 10156
    },
    {
      "epoch": 1.427948826093069,
      "grad_norm": 1.6711479425430298,
      "learning_rate": 1.1688101410193275e-07,
      "loss": 1.0076,
      "step": 10157
    },
    {
      "epoch": 1.4280894137494728,
      "grad_norm": 1.8321365118026733,
      "learning_rate": 1.0591454934341106e-07,
      "loss": 1.0321,
      "step": 10158
    },
    {
      "epoch": 1.4282300014058766,
      "grad_norm": 1.4039188623428345,
      "learning_rate": 9.548793258372657e-08,
      "loss": 1.2772,
      "step": 10159
    },
    {
      "epoch": 1.4283705890622804,
      "grad_norm": 1.4685311317443848,
      "learning_rate": 8.560122017043926e-08,
      "loss": 1.0764,
      "step": 10160
    },
    {
      "epoch": 1.428511176718684,
      "grad_norm": 1.4240280389785767,
      "learning_rate": 7.625446553334304e-08,
      "loss": 1.1471,
      "step": 10161
    },
    {
      "epoch": 1.4286517643750878,
      "grad_norm": 1.7117431163787842,
      "learning_rate": 6.744771918422155e-08,
      "loss": 1.0293,
      "step": 10162
    },
    {
      "epoch": 1.4287923520314916,
      "grad_norm": 1.482041597366333,
      "learning_rate": 5.91810287165262e-08,
      "loss": 0.9579,
      "step": 10163
    },
    {
      "epoch": 1.4289329396878954,
      "grad_norm": 1.5075546503067017,
      "learning_rate": 5.145443880510969e-08,
      "loss": 1.0239,
      "step": 10164
    },
    {
      "epoch": 1.4290735273442992,
      "grad_norm": 1.4034110307693481,
      "learning_rate": 4.426799120605951e-08,
      "loss": 1.0284,
      "step": 10165
    },
    {
      "epoch": 1.429214115000703,
      "grad_norm": 1.7385990619659424,
      "learning_rate": 3.7621724756398135e-08,
      "loss": 1.0214,
      "step": 10166
    },
    {
      "epoch": 1.4293547026571067,
      "grad_norm": 1.3782684803009033,
      "learning_rate": 3.151567537391653e-08,
      "loss": 1.0575,
      "step": 10167
    },
    {
      "epoch": 1.4294952903135105,
      "grad_norm": 1.46995210647583,
      "learning_rate": 2.5949876056952094e-08,
      "loss": 1.0631,
      "step": 10168
    },
    {
      "epoch": 1.4296358779699143,
      "grad_norm": 1.5312614440917969,
      "learning_rate": 2.0924356884188812e-08,
      "loss": 1.1739,
      "step": 10169
    },
    {
      "epoch": 1.4297764656263179,
      "grad_norm": 1.3888866901397705,
      "learning_rate": 1.643914501457955e-08,
      "loss": 1.1936,
      "step": 10170
    },
    {
      "epoch": 1.429917053282722,
      "grad_norm": 1.4106180667877197,
      "learning_rate": 1.2494264687124002e-08,
      "loss": 1.0644,
      "step": 10171
    },
    {
      "epoch": 1.4300576409391255,
      "grad_norm": 2.000945806503296,
      "learning_rate": 9.089737220746574e-09,
      "loss": 1.1306,
      "step": 10172
    },
    {
      "epoch": 1.4301982285955293,
      "grad_norm": 1.7363742589950562,
      "learning_rate": 6.225581014229764e-09,
      "loss": 1.0027,
      "step": 10173
    },
    {
      "epoch": 1.430338816251933,
      "grad_norm": 1.5717524290084839,
      "learning_rate": 3.901811546036527e-09,
      "loss": 1.14,
      "step": 10174
    },
    {
      "epoch": 1.4304794039083368,
      "grad_norm": 1.6182053089141846,
      "learning_rate": 2.1184413742880717e-09,
      "loss": 1.0675,
      "step": 10175
    },
    {
      "epoch": 1.4306199915647406,
      "grad_norm": 1.4180879592895508,
      "learning_rate": 8.754801367083509e-10,
      "loss": 1.0211,
      "step": 10176
    },
    {
      "epoch": 1.4307605792211444,
      "grad_norm": 1.5305637121200562,
      "learning_rate": 1.7293455047973083e-10,
      "loss": 1.1818,
      "step": 10177
    },
    {
      "epoch": 1.4309011668775482,
      "grad_norm": 1.498273491859436,
      "learning_rate": 1.0808412320706396e-11,
      "loss": 1.0053,
      "step": 10178
    },
    {
      "epoch": 1.431041754533952,
      "grad_norm": 1.551695466041565,
      "learning_rate": 3.8910259839708417e-10,
      "loss": 1.1112,
      "step": 10179
    },
    {
      "epoch": 1.4311823421903558,
      "grad_norm": 1.5036088228225708,
      "learning_rate": 1.3078150643219822e-09,
      "loss": 1.0384,
      "step": 10180
    },
    {
      "epoch": 1.4313229298467594,
      "grad_norm": 1.679715633392334,
      "learning_rate": 2.7669408451780344e-09,
      "loss": 1.0028,
      "step": 10181
    },
    {
      "epoch": 1.4314635175031631,
      "grad_norm": 1.5750274658203125,
      "learning_rate": 4.766472055572901e-09,
      "loss": 0.9893,
      "step": 10182
    },
    {
      "epoch": 1.431604105159567,
      "grad_norm": 1.4517885446548462,
      "learning_rate": 7.306397889617067e-09,
      "loss": 1.1543,
      "step": 10183
    },
    {
      "epoch": 1.4317446928159707,
      "grad_norm": 1.8575185537338257,
      "learning_rate": 1.038670462103486e-08,
      "loss": 1.0667,
      "step": 10184
    },
    {
      "epoch": 1.4318852804723745,
      "grad_norm": 1.6564805507659912,
      "learning_rate": 1.4007375603219963e-08,
      "loss": 1.076,
      "step": 10185
    },
    {
      "epoch": 1.4320258681287783,
      "grad_norm": 1.5637450218200684,
      "learning_rate": 1.8168391269335338e-08,
      "loss": 0.977,
      "step": 10186
    },
    {
      "epoch": 1.432166455785182,
      "grad_norm": 1.6415244340896606,
      "learning_rate": 2.286972913240204e-08,
      "loss": 1.1242,
      "step": 10187
    },
    {
      "epoch": 1.4323070434415859,
      "grad_norm": 1.2735930681228638,
      "learning_rate": 2.8111363785443546e-08,
      "loss": 1.1482,
      "step": 10188
    },
    {
      "epoch": 1.4324476310979897,
      "grad_norm": 1.2728068828582764,
      "learning_rate": 3.389326690156347e-08,
      "loss": 1.1816,
      "step": 10189
    },
    {
      "epoch": 1.4325882187543932,
      "grad_norm": 1.561109185218811,
      "learning_rate": 4.021540723420092e-08,
      "loss": 1.0795,
      "step": 10190
    },
    {
      "epoch": 1.4327288064107973,
      "grad_norm": 1.8215242624282837,
      "learning_rate": 4.707775061723041e-08,
      "loss": 0.9832,
      "step": 10191
    },
    {
      "epoch": 1.4328693940672008,
      "grad_norm": 1.3752331733703613,
      "learning_rate": 5.4480259965139504e-08,
      "loss": 1.1301,
      "step": 10192
    },
    {
      "epoch": 1.4330099817236046,
      "grad_norm": 1.328778624534607,
      "learning_rate": 6.242289527325085e-08,
      "loss": 1.0273,
      "step": 10193
    },
    {
      "epoch": 1.4331505693800084,
      "grad_norm": 2.016369104385376,
      "learning_rate": 7.090561361795533e-08,
      "loss": 1.1026,
      "step": 10194
    },
    {
      "epoch": 1.4332911570364122,
      "grad_norm": 1.6148173809051514,
      "learning_rate": 7.992836915690082e-08,
      "loss": 1.0589,
      "step": 10195
    },
    {
      "epoch": 1.433431744692816,
      "grad_norm": 1.564756155014038,
      "learning_rate": 8.949111312931413e-08,
      "loss": 1.2154,
      "step": 10196
    },
    {
      "epoch": 1.4335723323492198,
      "grad_norm": 1.4315264225006104,
      "learning_rate": 9.959379385614532e-08,
      "loss": 1.1417,
      "step": 10197
    },
    {
      "epoch": 1.4337129200056236,
      "grad_norm": 1.4719314575195312,
      "learning_rate": 1.102363567404452e-07,
      "loss": 1.0524,
      "step": 10198
    },
    {
      "epoch": 1.4338535076620273,
      "grad_norm": 1.3936197757720947,
      "learning_rate": 1.2141874426764288e-07,
      "loss": 1.0437,
      "step": 10199
    },
    {
      "epoch": 1.4339940953184311,
      "grad_norm": 1.4097760915756226,
      "learning_rate": 1.3314089600582336e-07,
      "loss": 1.14,
      "step": 10200
    },
    {
      "epoch": 1.4341346829748347,
      "grad_norm": 1.5446397066116333,
      "learning_rate": 1.4540274860611603e-07,
      "loss": 0.8947,
      "step": 10201
    },
    {
      "epoch": 1.4342752706312385,
      "grad_norm": 1.4596489667892456,
      "learning_rate": 1.5820423580296117e-07,
      "loss": 0.9196,
      "step": 10202
    },
    {
      "epoch": 1.4344158582876423,
      "grad_norm": 1.5352485179901123,
      "learning_rate": 1.7154528841453187e-07,
      "loss": 1.0805,
      "step": 10203
    },
    {
      "epoch": 1.434556445944046,
      "grad_norm": 1.5632115602493286,
      "learning_rate": 1.8542583434301154e-07,
      "loss": 1.033,
      "step": 10204
    },
    {
      "epoch": 1.4346970336004499,
      "grad_norm": 1.397139310836792,
      "learning_rate": 1.9984579857513784e-07,
      "loss": 1.0505,
      "step": 10205
    },
    {
      "epoch": 1.4348376212568537,
      "grad_norm": 1.4149070978164673,
      "learning_rate": 2.1480510318243607e-07,
      "loss": 1.1302,
      "step": 10206
    },
    {
      "epoch": 1.4349782089132574,
      "grad_norm": 1.7312155961990356,
      "learning_rate": 2.3030366732185172e-07,
      "loss": 1.0208,
      "step": 10207
    },
    {
      "epoch": 1.4351187965696612,
      "grad_norm": 1.4434642791748047,
      "learning_rate": 2.463414072359393e-07,
      "loss": 1.0796,
      "step": 10208
    },
    {
      "epoch": 1.435259384226065,
      "grad_norm": 1.5797029733657837,
      "learning_rate": 2.6291823625349545e-07,
      "loss": 1.1466,
      "step": 10209
    },
    {
      "epoch": 1.4353999718824686,
      "grad_norm": 1.6313034296035767,
      "learning_rate": 2.8003406478994687e-07,
      "loss": 0.8742,
      "step": 10210
    },
    {
      "epoch": 1.4355405595388726,
      "grad_norm": 1.6799557209014893,
      "learning_rate": 2.9768880034787283e-07,
      "loss": 1.2183,
      "step": 10211
    },
    {
      "epoch": 1.4356811471952762,
      "grad_norm": 1.466564416885376,
      "learning_rate": 3.1588234751749323e-07,
      "loss": 0.9427,
      "step": 10212
    },
    {
      "epoch": 1.43582173485168,
      "grad_norm": 1.6028892993927002,
      "learning_rate": 3.3461460797714614e-07,
      "loss": 1.1309,
      "step": 10213
    },
    {
      "epoch": 1.4359623225080838,
      "grad_norm": 1.6265640258789062,
      "learning_rate": 3.538854804939096e-07,
      "loss": 1.1727,
      "step": 10214
    },
    {
      "epoch": 1.4361029101644875,
      "grad_norm": 2.129213333129883,
      "learning_rate": 3.736948609240343e-07,
      "loss": 0.9791,
      "step": 10215
    },
    {
      "epoch": 1.4362434978208913,
      "grad_norm": 1.6136956214904785,
      "learning_rate": 3.9404264221357677e-07,
      "loss": 0.9371,
      "step": 10216
    },
    {
      "epoch": 1.4363840854772951,
      "grad_norm": 1.5733674764633179,
      "learning_rate": 4.1492871439903216e-07,
      "loss": 1.1572,
      "step": 10217
    },
    {
      "epoch": 1.436524673133699,
      "grad_norm": 1.6122174263000488,
      "learning_rate": 4.363529646077891e-07,
      "loss": 0.9956,
      "step": 10218
    },
    {
      "epoch": 1.4366652607901027,
      "grad_norm": 1.321855068206787,
      "learning_rate": 4.583152770588517e-07,
      "loss": 1.014,
      "step": 10219
    },
    {
      "epoch": 1.4368058484465065,
      "grad_norm": 1.6076879501342773,
      "learning_rate": 4.80815533063339e-07,
      "loss": 0.9667,
      "step": 10220
    },
    {
      "epoch": 1.43694643610291,
      "grad_norm": 1.3845579624176025,
      "learning_rate": 5.03853611025329e-07,
      "loss": 1.0437,
      "step": 10221
    },
    {
      "epoch": 1.4370870237593139,
      "grad_norm": 1.5648539066314697,
      "learning_rate": 5.27429386442313e-07,
      "loss": 0.8947,
      "step": 10222
    },
    {
      "epoch": 1.4372276114157176,
      "grad_norm": 1.3853892087936401,
      "learning_rate": 5.515427319060629e-07,
      "loss": 1.0623,
      "step": 10223
    },
    {
      "epoch": 1.4373681990721214,
      "grad_norm": 1.445368766784668,
      "learning_rate": 5.761935171031074e-07,
      "loss": 1.1612,
      "step": 10224
    },
    {
      "epoch": 1.4375087867285252,
      "grad_norm": 1.6606806516647339,
      "learning_rate": 6.013816088155877e-07,
      "loss": 1.1184,
      "step": 10225
    },
    {
      "epoch": 1.437649374384929,
      "grad_norm": 1.5441336631774902,
      "learning_rate": 6.271068709219452e-07,
      "loss": 1.0797,
      "step": 10226
    },
    {
      "epoch": 1.4377899620413328,
      "grad_norm": 1.5122343301773071,
      "learning_rate": 6.533691643975215e-07,
      "loss": 1.0951,
      "step": 10227
    },
    {
      "epoch": 1.4379305496977366,
      "grad_norm": 1.4223295450210571,
      "learning_rate": 6.801683473156351e-07,
      "loss": 0.9951,
      "step": 10228
    },
    {
      "epoch": 1.4380711373541404,
      "grad_norm": 1.4972869157791138,
      "learning_rate": 7.075042748480032e-07,
      "loss": 1.0574,
      "step": 10229
    },
    {
      "epoch": 1.438211725010544,
      "grad_norm": 1.8340176343917847,
      "learning_rate": 7.353767992656968e-07,
      "loss": 1.1039,
      "step": 10230
    },
    {
      "epoch": 1.438352312666948,
      "grad_norm": 1.670950174331665,
      "learning_rate": 7.637857699399176e-07,
      "loss": 1.0549,
      "step": 10231
    },
    {
      "epoch": 1.4384929003233515,
      "grad_norm": 1.3828527927398682,
      "learning_rate": 7.927310333428084e-07,
      "loss": 1.1503,
      "step": 10232
    },
    {
      "epoch": 1.4386334879797553,
      "grad_norm": 1.6826996803283691,
      "learning_rate": 8.222124330481973e-07,
      "loss": 1.0,
      "step": 10233
    },
    {
      "epoch": 1.438774075636159,
      "grad_norm": 1.7186214923858643,
      "learning_rate": 8.522298097327519e-07,
      "loss": 1.2018,
      "step": 10234
    },
    {
      "epoch": 1.438914663292563,
      "grad_norm": 1.5383466482162476,
      "learning_rate": 8.827830011762905e-07,
      "loss": 0.8671,
      "step": 10235
    },
    {
      "epoch": 1.4390552509489667,
      "grad_norm": 1.8684937953948975,
      "learning_rate": 9.138718422632031e-07,
      "loss": 1.1591,
      "step": 10236
    },
    {
      "epoch": 1.4391958386053705,
      "grad_norm": 1.5582263469696045,
      "learning_rate": 9.454961649830507e-07,
      "loss": 0.9386,
      "step": 10237
    },
    {
      "epoch": 1.4393364262617743,
      "grad_norm": 1.5915495157241821,
      "learning_rate": 9.776557984314872e-07,
      "loss": 1.0622,
      "step": 10238
    },
    {
      "epoch": 1.439477013918178,
      "grad_norm": 1.3041760921478271,
      "learning_rate": 1.0103505688114135e-06,
      "loss": 1.1804,
      "step": 10239
    },
    {
      "epoch": 1.4396176015745819,
      "grad_norm": 1.4200129508972168,
      "learning_rate": 1.0435802994335664e-06,
      "loss": 1.1153,
      "step": 10240
    },
    {
      "epoch": 1.4397581892309854,
      "grad_norm": 1.4199450016021729,
      "learning_rate": 1.077344810717762e-06,
      "loss": 1.1604,
      "step": 10241
    },
    {
      "epoch": 1.4398987768873892,
      "grad_norm": 1.8089104890823364,
      "learning_rate": 1.1116439201935946e-06,
      "loss": 1.0604,
      "step": 10242
    },
    {
      "epoch": 1.440039364543793,
      "grad_norm": 1.4157867431640625,
      "learning_rate": 1.1464774425016922e-06,
      "loss": 1.0766,
      "step": 10243
    },
    {
      "epoch": 1.4401799522001968,
      "grad_norm": 1.45216965675354,
      "learning_rate": 1.1818451893946926e-06,
      "loss": 1.2194,
      "step": 10244
    },
    {
      "epoch": 1.4403205398566006,
      "grad_norm": 1.5795420408248901,
      "learning_rate": 1.217746969738043e-06,
      "loss": 1.0878,
      "step": 10245
    },
    {
      "epoch": 1.4404611275130044,
      "grad_norm": 1.4265018701553345,
      "learning_rate": 1.2541825895111993e-06,
      "loss": 0.9853,
      "step": 10246
    },
    {
      "epoch": 1.4406017151694082,
      "grad_norm": 1.507033348083496,
      "learning_rate": 1.2911518518086475e-06,
      "loss": 0.9558,
      "step": 10247
    },
    {
      "epoch": 1.440742302825812,
      "grad_norm": 1.3987971544265747,
      "learning_rate": 1.328654556840958e-06,
      "loss": 1.0764,
      "step": 10248
    },
    {
      "epoch": 1.4408828904822157,
      "grad_norm": 1.500589370727539,
      "learning_rate": 1.3666905019358744e-06,
      "loss": 1.1224,
      "step": 10249
    },
    {
      "epoch": 1.4410234781386193,
      "grad_norm": 1.6222292184829712,
      "learning_rate": 1.4052594815393783e-06,
      "loss": 1.1308,
      "step": 10250
    },
    {
      "epoch": 1.4411640657950233,
      "grad_norm": 1.4751057624816895,
      "learning_rate": 1.4443612872168554e-06,
      "loss": 1.0173,
      "step": 10251
    },
    {
      "epoch": 1.4413046534514269,
      "grad_norm": 1.5127661228179932,
      "learning_rate": 1.4839957076541844e-06,
      "loss": 0.9694,
      "step": 10252
    },
    {
      "epoch": 1.4414452411078307,
      "grad_norm": 1.834700584411621,
      "learning_rate": 1.524162528658868e-06,
      "loss": 1.014,
      "step": 10253
    },
    {
      "epoch": 1.4415858287642345,
      "grad_norm": 1.4987123012542725,
      "learning_rate": 1.564861533161155e-06,
      "loss": 0.9776,
      "step": 10254
    },
    {
      "epoch": 1.4417264164206383,
      "grad_norm": 1.2620009183883667,
      "learning_rate": 1.6060925012154614e-06,
      "loss": 1.1264,
      "step": 10255
    },
    {
      "epoch": 1.441867004077042,
      "grad_norm": 1.4430873394012451,
      "learning_rate": 1.6478552100011703e-06,
      "loss": 1.0235,
      "step": 10256
    },
    {
      "epoch": 1.4420075917334458,
      "grad_norm": 1.3822777271270752,
      "learning_rate": 1.6901494338241508e-06,
      "loss": 1.0054,
      "step": 10257
    },
    {
      "epoch": 1.4421481793898496,
      "grad_norm": 1.4891200065612793,
      "learning_rate": 1.7329749441176602e-06,
      "loss": 1.1141,
      "step": 10258
    },
    {
      "epoch": 1.4422887670462534,
      "grad_norm": 1.3425703048706055,
      "learning_rate": 1.776331509443996e-06,
      "loss": 0.8261,
      "step": 10259
    },
    {
      "epoch": 1.4424293547026572,
      "grad_norm": 1.7689896821975708,
      "learning_rate": 1.8202188954953624e-06,
      "loss": 1.0703,
      "step": 10260
    },
    {
      "epoch": 1.4425699423590608,
      "grad_norm": 1.4192616939544678,
      "learning_rate": 1.8646368650955147e-06,
      "loss": 0.8573,
      "step": 10261
    },
    {
      "epoch": 1.4427105300154646,
      "grad_norm": 1.3416225910186768,
      "learning_rate": 1.9095851782006237e-06,
      "loss": 1.0921,
      "step": 10262
    },
    {
      "epoch": 1.4428511176718684,
      "grad_norm": 1.3927513360977173,
      "learning_rate": 1.955063591900841e-06,
      "loss": 0.9645,
      "step": 10263
    },
    {
      "epoch": 1.4429917053282721,
      "grad_norm": 1.5512624979019165,
      "learning_rate": 2.001071860421633e-06,
      "loss": 1.0277,
      "step": 10264
    },
    {
      "epoch": 1.443132292984676,
      "grad_norm": 1.42490816116333,
      "learning_rate": 2.0476097351247116e-06,
      "loss": 1.2099,
      "step": 10265
    },
    {
      "epoch": 1.4432728806410797,
      "grad_norm": 1.4682797193527222,
      "learning_rate": 2.094676964510145e-06,
      "loss": 0.983,
      "step": 10266
    },
    {
      "epoch": 1.4434134682974835,
      "grad_norm": 1.4565926790237427,
      "learning_rate": 2.1422732942169456e-06,
      "loss": 1.03,
      "step": 10267
    },
    {
      "epoch": 1.4435540559538873,
      "grad_norm": 1.6277856826782227,
      "learning_rate": 2.1903984670248455e-06,
      "loss": 0.9023,
      "step": 10268
    },
    {
      "epoch": 1.443694643610291,
      "grad_norm": 1.4214550256729126,
      "learning_rate": 2.2390522228556087e-06,
      "loss": 0.9864,
      "step": 10269
    },
    {
      "epoch": 1.4438352312666947,
      "grad_norm": 1.4569846391677856,
      "learning_rate": 2.28823429877435e-06,
      "loss": 0.9976,
      "step": 10270
    },
    {
      "epoch": 1.4439758189230987,
      "grad_norm": 1.3452587127685547,
      "learning_rate": 2.3379444289912567e-06,
      "loss": 1.1159,
      "step": 10271
    },
    {
      "epoch": 1.4441164065795022,
      "grad_norm": 1.728153944015503,
      "learning_rate": 2.388182344862644e-06,
      "loss": 0.9249,
      "step": 10272
    },
    {
      "epoch": 1.444256994235906,
      "grad_norm": 1.3919174671173096,
      "learning_rate": 2.4389477748924415e-06,
      "loss": 1.1549,
      "step": 10273
    },
    {
      "epoch": 1.4443975818923098,
      "grad_norm": 1.3567688465118408,
      "learning_rate": 2.490240444733971e-06,
      "loss": 0.9486,
      "step": 10274
    },
    {
      "epoch": 1.4445381695487136,
      "grad_norm": 1.2948787212371826,
      "learning_rate": 2.5420600771911996e-06,
      "loss": 1.1092,
      "step": 10275
    },
    {
      "epoch": 1.4446787572051174,
      "grad_norm": 1.408065676689148,
      "learning_rate": 2.5944063922201832e-06,
      "loss": 1.2004,
      "step": 10276
    },
    {
      "epoch": 1.4448193448615212,
      "grad_norm": 1.8118568658828735,
      "learning_rate": 2.6472791069309445e-06,
      "loss": 1.1355,
      "step": 10277
    },
    {
      "epoch": 1.444959932517925,
      "grad_norm": 1.7681816816329956,
      "learning_rate": 2.7006779355884693e-06,
      "loss": 1.0264,
      "step": 10278
    },
    {
      "epoch": 1.4451005201743288,
      "grad_norm": 1.7003084421157837,
      "learning_rate": 2.7546025896145968e-06,
      "loss": 0.9515,
      "step": 10279
    },
    {
      "epoch": 1.4452411078307326,
      "grad_norm": 1.5542994737625122,
      "learning_rate": 2.8090527775895934e-06,
      "loss": 0.9424,
      "step": 10280
    },
    {
      "epoch": 1.4453816954871361,
      "grad_norm": 1.6632369756698608,
      "learning_rate": 2.8640282052532884e-06,
      "loss": 1.1789,
      "step": 10281
    },
    {
      "epoch": 1.44552228314354,
      "grad_norm": 1.575869083404541,
      "learning_rate": 2.919528575507524e-06,
      "loss": 1.0257,
      "step": 10282
    },
    {
      "epoch": 1.4456628707999437,
      "grad_norm": 1.6526542901992798,
      "learning_rate": 2.9755535884169017e-06,
      "loss": 1.2681,
      "step": 10283
    },
    {
      "epoch": 1.4458034584563475,
      "grad_norm": 1.312975525856018,
      "learning_rate": 3.0321029412108237e-06,
      "loss": 1.1137,
      "step": 10284
    },
    {
      "epoch": 1.4459440461127513,
      "grad_norm": 1.7061549425125122,
      "learning_rate": 3.0891763282850707e-06,
      "loss": 0.9675,
      "step": 10285
    },
    {
      "epoch": 1.446084633769155,
      "grad_norm": 1.6394017934799194,
      "learning_rate": 3.1467734412033436e-06,
      "loss": 1.0237,
      "step": 10286
    },
    {
      "epoch": 1.4462252214255589,
      "grad_norm": 1.4935563802719116,
      "learning_rate": 3.2048939686993516e-06,
      "loss": 1.073,
      "step": 10287
    },
    {
      "epoch": 1.4463658090819627,
      "grad_norm": 1.5500328540802002,
      "learning_rate": 3.263537596677668e-06,
      "loss": 1.0309,
      "step": 10288
    },
    {
      "epoch": 1.4465063967383665,
      "grad_norm": 1.722307801246643,
      "learning_rate": 3.32270400821626e-06,
      "loss": 1.1242,
      "step": 10289
    },
    {
      "epoch": 1.44664698439477,
      "grad_norm": 1.2156400680541992,
      "learning_rate": 3.382392883567764e-06,
      "loss": 1.2241,
      "step": 10290
    },
    {
      "epoch": 1.446787572051174,
      "grad_norm": 1.6261576414108276,
      "learning_rate": 3.4426039001613564e-06,
      "loss": 1.0827,
      "step": 10291
    },
    {
      "epoch": 1.4469281597075776,
      "grad_norm": 1.706884503364563,
      "learning_rate": 3.503336732604312e-06,
      "loss": 1.0,
      "step": 10292
    },
    {
      "epoch": 1.4470687473639814,
      "grad_norm": 1.533390760421753,
      "learning_rate": 3.5645910526842297e-06,
      "loss": 0.9044,
      "step": 10293
    },
    {
      "epoch": 1.4472093350203852,
      "grad_norm": 1.7176342010498047,
      "learning_rate": 3.6263665293701845e-06,
      "loss": 1.1194,
      "step": 10294
    },
    {
      "epoch": 1.447349922676789,
      "grad_norm": 1.422354817390442,
      "learning_rate": 3.688662828815004e-06,
      "loss": 1.1481,
      "step": 10295
    },
    {
      "epoch": 1.4474905103331928,
      "grad_norm": 1.5245649814605713,
      "learning_rate": 3.75147961435659e-06,
      "loss": 1.0084,
      "step": 10296
    },
    {
      "epoch": 1.4476310979895965,
      "grad_norm": 1.5924060344696045,
      "learning_rate": 3.814816546520228e-06,
      "loss": 0.9037,
      "step": 10297
    },
    {
      "epoch": 1.4477716856460003,
      "grad_norm": 1.3606650829315186,
      "learning_rate": 3.878673283020428e-06,
      "loss": 1.2196,
      "step": 10298
    },
    {
      "epoch": 1.4479122733024041,
      "grad_norm": 1.6929337978363037,
      "learning_rate": 3.943049478762273e-06,
      "loss": 1.203,
      "step": 10299
    },
    {
      "epoch": 1.448052860958808,
      "grad_norm": 1.4108421802520752,
      "learning_rate": 4.007944785843665e-06,
      "loss": 1.0399,
      "step": 10300
    },
    {
      "epoch": 1.4481934486152115,
      "grad_norm": 1.2617806196212769,
      "learning_rate": 4.073358853557163e-06,
      "loss": 1.2266,
      "step": 10301
    },
    {
      "epoch": 1.4483340362716153,
      "grad_norm": 1.4592440128326416,
      "learning_rate": 4.139291328391892e-06,
      "loss": 1.1139,
      "step": 10302
    },
    {
      "epoch": 1.448474623928019,
      "grad_norm": 1.3685001134872437,
      "learning_rate": 4.205741854035028e-06,
      "loss": 1.0891,
      "step": 10303
    },
    {
      "epoch": 1.4486152115844229,
      "grad_norm": 1.7530111074447632,
      "learning_rate": 4.272710071374674e-06,
      "loss": 0.9785,
      "step": 10304
    },
    {
      "epoch": 1.4487557992408266,
      "grad_norm": 1.7301125526428223,
      "learning_rate": 4.340195618500853e-06,
      "loss": 0.8579,
      "step": 10305
    },
    {
      "epoch": 1.4488963868972304,
      "grad_norm": 1.5684112310409546,
      "learning_rate": 4.408198130707897e-06,
      "loss": 0.9257,
      "step": 10306
    },
    {
      "epoch": 1.4490369745536342,
      "grad_norm": 1.6560053825378418,
      "learning_rate": 4.476717240496375e-06,
      "loss": 0.9995,
      "step": 10307
    },
    {
      "epoch": 1.449177562210038,
      "grad_norm": 1.672348976135254,
      "learning_rate": 4.5457525775749575e-06,
      "loss": 1.0655,
      "step": 10308
    },
    {
      "epoch": 1.4493181498664418,
      "grad_norm": 1.4727952480316162,
      "learning_rate": 4.615303768862799e-06,
      "loss": 0.9742,
      "step": 10309
    },
    {
      "epoch": 1.4494587375228454,
      "grad_norm": 1.5861526727676392,
      "learning_rate": 4.685370438491099e-06,
      "loss": 1.0343,
      "step": 10310
    },
    {
      "epoch": 1.4495993251792494,
      "grad_norm": 1.763558268547058,
      "learning_rate": 4.755952207805092e-06,
      "loss": 1.139,
      "step": 10311
    },
    {
      "epoch": 1.449739912835653,
      "grad_norm": 1.551116943359375,
      "learning_rate": 4.827048695366587e-06,
      "loss": 1.0761,
      "step": 10312
    },
    {
      "epoch": 1.4498805004920567,
      "grad_norm": 1.5527312755584717,
      "learning_rate": 4.898659516955595e-06,
      "loss": 1.0025,
      "step": 10313
    },
    {
      "epoch": 1.4500210881484605,
      "grad_norm": 1.5290354490280151,
      "learning_rate": 4.97078428557285e-06,
      "loss": 1.1167,
      "step": 10314
    },
    {
      "epoch": 1.4501616758048643,
      "grad_norm": 1.632248878479004,
      "learning_rate": 5.043422611441273e-06,
      "loss": 1.1267,
      "step": 10315
    },
    {
      "epoch": 1.4503022634612681,
      "grad_norm": 1.4731777906417847,
      "learning_rate": 5.116574102008576e-06,
      "loss": 0.9824,
      "step": 10316
    },
    {
      "epoch": 1.450442851117672,
      "grad_norm": 1.3077819347381592,
      "learning_rate": 5.190238361949174e-06,
      "loss": 1.1368,
      "step": 10317
    },
    {
      "epoch": 1.4505834387740757,
      "grad_norm": 1.2616283893585205,
      "learning_rate": 5.2644149931665e-06,
      "loss": 0.9249,
      "step": 10318
    },
    {
      "epoch": 1.4507240264304795,
      "grad_norm": 1.424047589302063,
      "learning_rate": 5.339103594794604e-06,
      "loss": 1.1147,
      "step": 10319
    },
    {
      "epoch": 1.4508646140868833,
      "grad_norm": 1.5799695253372192,
      "learning_rate": 5.414303763201367e-06,
      "loss": 0.9843,
      "step": 10320
    },
    {
      "epoch": 1.4510052017432868,
      "grad_norm": 1.3530974388122559,
      "learning_rate": 5.4900150919896646e-06,
      "loss": 1.1111,
      "step": 10321
    },
    {
      "epoch": 1.4511457893996906,
      "grad_norm": 1.4810025691986084,
      "learning_rate": 5.566237172000044e-06,
      "loss": 1.0278,
      "step": 10322
    },
    {
      "epoch": 1.4512863770560944,
      "grad_norm": 1.721482515335083,
      "learning_rate": 5.64296959131283e-06,
      "loss": 0.9687,
      "step": 10323
    },
    {
      "epoch": 1.4514269647124982,
      "grad_norm": 1.861079454421997,
      "learning_rate": 5.720211935250275e-06,
      "loss": 1.017,
      "step": 10324
    },
    {
      "epoch": 1.451567552368902,
      "grad_norm": 1.5462132692337036,
      "learning_rate": 5.797963786379379e-06,
      "loss": 1.0705,
      "step": 10325
    },
    {
      "epoch": 1.4517081400253058,
      "grad_norm": 1.369571566581726,
      "learning_rate": 5.876224724512913e-06,
      "loss": 1.1783,
      "step": 10326
    },
    {
      "epoch": 1.4518487276817096,
      "grad_norm": 1.5267610549926758,
      "learning_rate": 5.954994326712948e-06,
      "loss": 1.0662,
      "step": 10327
    },
    {
      "epoch": 1.4519893153381134,
      "grad_norm": 1.5697799921035767,
      "learning_rate": 6.03427216729251e-06,
      "loss": 1.1473,
      "step": 10328
    },
    {
      "epoch": 1.4521299029945172,
      "grad_norm": 1.6746786832809448,
      "learning_rate": 6.114057817817842e-06,
      "loss": 1.0706,
      "step": 10329
    },
    {
      "epoch": 1.4522704906509207,
      "grad_norm": 1.953366994857788,
      "learning_rate": 6.194350847111274e-06,
      "loss": 1.0602,
      "step": 10330
    },
    {
      "epoch": 1.4524110783073247,
      "grad_norm": 1.5575381517410278,
      "learning_rate": 6.275150821252806e-06,
      "loss": 1.0723,
      "step": 10331
    },
    {
      "epoch": 1.4525516659637283,
      "grad_norm": 1.356842041015625,
      "learning_rate": 6.356457303582897e-06,
      "loss": 0.9316,
      "step": 10332
    },
    {
      "epoch": 1.452692253620132,
      "grad_norm": 1.3558571338653564,
      "learning_rate": 6.43826985470487e-06,
      "loss": 1.0017,
      "step": 10333
    },
    {
      "epoch": 1.452832841276536,
      "grad_norm": 1.283203363418579,
      "learning_rate": 6.520588032486774e-06,
      "loss": 1.0569,
      "step": 10334
    },
    {
      "epoch": 1.4529734289329397,
      "grad_norm": 1.514217734336853,
      "learning_rate": 6.603411392064396e-06,
      "loss": 0.8963,
      "step": 10335
    },
    {
      "epoch": 1.4531140165893435,
      "grad_norm": 1.358948826789856,
      "learning_rate": 6.6867394858436624e-06,
      "loss": 1.0312,
      "step": 10336
    },
    {
      "epoch": 1.4532546042457473,
      "grad_norm": 1.7063028812408447,
      "learning_rate": 6.770571863502462e-06,
      "loss": 1.0769,
      "step": 10337
    },
    {
      "epoch": 1.453395191902151,
      "grad_norm": 1.6304601430892944,
      "learning_rate": 6.854908071993527e-06,
      "loss": 1.0938,
      "step": 10338
    },
    {
      "epoch": 1.4535357795585548,
      "grad_norm": 1.3582091331481934,
      "learning_rate": 6.9397476555468025e-06,
      "loss": 1.0324,
      "step": 10339
    },
    {
      "epoch": 1.4536763672149586,
      "grad_norm": 1.5061485767364502,
      "learning_rate": 7.0250901556718876e-06,
      "loss": 0.9839,
      "step": 10340
    },
    {
      "epoch": 1.4538169548713622,
      "grad_norm": 1.4617798328399658,
      "learning_rate": 7.110935111160499e-06,
      "loss": 1.0284,
      "step": 10341
    },
    {
      "epoch": 1.453957542527766,
      "grad_norm": 1.6126019954681396,
      "learning_rate": 7.197282058089028e-06,
      "loss": 0.8677,
      "step": 10342
    },
    {
      "epoch": 1.4540981301841698,
      "grad_norm": 1.5216981172561646,
      "learning_rate": 7.284130529820943e-06,
      "loss": 1.0013,
      "step": 10343
    },
    {
      "epoch": 1.4542387178405736,
      "grad_norm": 1.5246988534927368,
      "learning_rate": 7.371480057009428e-06,
      "loss": 1.1151,
      "step": 10344
    },
    {
      "epoch": 1.4543793054969774,
      "grad_norm": 1.5124579668045044,
      "learning_rate": 7.459330167599821e-06,
      "loss": 1.1505,
      "step": 10345
    },
    {
      "epoch": 1.4545198931533811,
      "grad_norm": 1.4093259572982788,
      "learning_rate": 7.547680386832079e-06,
      "loss": 1.1138,
      "step": 10346
    },
    {
      "epoch": 1.454660480809785,
      "grad_norm": 1.491317868232727,
      "learning_rate": 7.636530237243889e-06,
      "loss": 0.9661,
      "step": 10347
    },
    {
      "epoch": 1.4548010684661887,
      "grad_norm": 1.4392428398132324,
      "learning_rate": 7.72587923867254e-06,
      "loss": 1.1867,
      "step": 10348
    },
    {
      "epoch": 1.4549416561225925,
      "grad_norm": 1.4143415689468384,
      "learning_rate": 7.81572690825756e-06,
      "loss": 1.0087,
      "step": 10349
    },
    {
      "epoch": 1.455082243778996,
      "grad_norm": 1.674912691116333,
      "learning_rate": 7.90607276044395e-06,
      "loss": 1.0448,
      "step": 10350
    },
    {
      "epoch": 1.4552228314354,
      "grad_norm": 1.4864217042922974,
      "learning_rate": 7.996916306984181e-06,
      "loss": 1.0194,
      "step": 10351
    },
    {
      "epoch": 1.4553634190918037,
      "grad_norm": 1.4605083465576172,
      "learning_rate": 8.088257056941472e-06,
      "loss": 1.1511,
      "step": 10352
    },
    {
      "epoch": 1.4555040067482075,
      "grad_norm": 1.2425248622894287,
      "learning_rate": 8.180094516691684e-06,
      "loss": 0.9418,
      "step": 10353
    },
    {
      "epoch": 1.4556445944046112,
      "grad_norm": 1.6996854543685913,
      "learning_rate": 8.27242818992644e-06,
      "loss": 1.1964,
      "step": 10354
    },
    {
      "epoch": 1.455785182061015,
      "grad_norm": 1.656131386756897,
      "learning_rate": 8.36525757765576e-06,
      "loss": 1.1156,
      "step": 10355
    },
    {
      "epoch": 1.4559257697174188,
      "grad_norm": 1.568254828453064,
      "learning_rate": 8.458582178210706e-06,
      "loss": 0.9836,
      "step": 10356
    },
    {
      "epoch": 1.4560663573738226,
      "grad_norm": 1.2977967262268066,
      "learning_rate": 8.55240148724612e-06,
      "loss": 1.1734,
      "step": 10357
    },
    {
      "epoch": 1.4562069450302264,
      "grad_norm": 1.4433351755142212,
      "learning_rate": 8.646714997743332e-06,
      "loss": 1.2737,
      "step": 10358
    },
    {
      "epoch": 1.45634753268663,
      "grad_norm": 1.4111127853393555,
      "learning_rate": 8.741522200012908e-06,
      "loss": 1.1443,
      "step": 10359
    },
    {
      "epoch": 1.456488120343034,
      "grad_norm": 1.5251811742782593,
      "learning_rate": 8.836822581697402e-06,
      "loss": 1.0042,
      "step": 10360
    },
    {
      "epoch": 1.4566287079994376,
      "grad_norm": 1.4797871112823486,
      "learning_rate": 8.932615627774121e-06,
      "loss": 1.0019,
      "step": 10361
    },
    {
      "epoch": 1.4567692956558413,
      "grad_norm": 1.4060481786727905,
      "learning_rate": 9.028900820557785e-06,
      "loss": 0.9829,
      "step": 10362
    },
    {
      "epoch": 1.4569098833122451,
      "grad_norm": 1.6589360237121582,
      "learning_rate": 9.125677639704012e-06,
      "loss": 1.0078,
      "step": 10363
    },
    {
      "epoch": 1.457050470968649,
      "grad_norm": 1.5847176313400269,
      "learning_rate": 9.222945562210717e-06,
      "loss": 1.1233,
      "step": 10364
    },
    {
      "epoch": 1.4571910586250527,
      "grad_norm": 1.5468378067016602,
      "learning_rate": 9.320704062422359e-06,
      "loss": 1.1176,
      "step": 10365
    },
    {
      "epoch": 1.4573316462814565,
      "grad_norm": 1.3979849815368652,
      "learning_rate": 9.418952612032095e-06,
      "loss": 1.0597,
      "step": 10366
    },
    {
      "epoch": 1.4574722339378603,
      "grad_norm": 1.687989592552185,
      "learning_rate": 9.517690680084557e-06,
      "loss": 0.9342,
      "step": 10367
    },
    {
      "epoch": 1.457612821594264,
      "grad_norm": 1.5572830438613892,
      "learning_rate": 9.616917732979425e-06,
      "loss": 0.9576,
      "step": 10368
    },
    {
      "epoch": 1.4577534092506679,
      "grad_norm": 1.8137319087982178,
      "learning_rate": 9.716633234473327e-06,
      "loss": 1.0918,
      "step": 10369
    },
    {
      "epoch": 1.4578939969070714,
      "grad_norm": 1.7496871948242188,
      "learning_rate": 9.816836645683359e-06,
      "loss": 1.0875,
      "step": 10370
    },
    {
      "epoch": 1.4580345845634755,
      "grad_norm": 1.494284987449646,
      "learning_rate": 9.917527425090012e-06,
      "loss": 1.1541,
      "step": 10371
    },
    {
      "epoch": 1.458175172219879,
      "grad_norm": 1.2608883380889893,
      "learning_rate": 1.00187050285395e-05,
      "loss": 1.2241,
      "step": 10372
    },
    {
      "epoch": 1.4583157598762828,
      "grad_norm": 1.6708377599716187,
      "learning_rate": 1.0120368909247424e-05,
      "loss": 1.1742,
      "step": 10373
    },
    {
      "epoch": 1.4584563475326866,
      "grad_norm": 1.533461332321167,
      "learning_rate": 1.0222518517801749e-05,
      "loss": 1.0814,
      "step": 10374
    },
    {
      "epoch": 1.4585969351890904,
      "grad_norm": 1.2450114488601685,
      "learning_rate": 1.0325153302165035e-05,
      "loss": 0.9789,
      "step": 10375
    },
    {
      "epoch": 1.4587375228454942,
      "grad_norm": 1.4791147708892822,
      "learning_rate": 1.0428272707677977e-05,
      "loss": 1.1,
      "step": 10376
    },
    {
      "epoch": 1.458878110501898,
      "grad_norm": 1.6402852535247803,
      "learning_rate": 1.0531876177062283e-05,
      "loss": 1.1126,
      "step": 10377
    },
    {
      "epoch": 1.4590186981583018,
      "grad_norm": 1.444614291191101,
      "learning_rate": 1.0635963150423688e-05,
      "loss": 1.0385,
      "step": 10378
    },
    {
      "epoch": 1.4591592858147053,
      "grad_norm": 1.7188196182250977,
      "learning_rate": 1.0740533065254988e-05,
      "loss": 1.1818,
      "step": 10379
    },
    {
      "epoch": 1.4592998734711093,
      "grad_norm": 1.5787166357040405,
      "learning_rate": 1.0845585356439036e-05,
      "loss": 1.1374,
      "step": 10380
    },
    {
      "epoch": 1.459440461127513,
      "grad_norm": 1.5249888896942139,
      "learning_rate": 1.0951119456251845e-05,
      "loss": 1.0664,
      "step": 10381
    },
    {
      "epoch": 1.4595810487839167,
      "grad_norm": 1.4652730226516724,
      "learning_rate": 1.105713479436562e-05,
      "loss": 1.1367,
      "step": 10382
    },
    {
      "epoch": 1.4597216364403205,
      "grad_norm": 2.1432292461395264,
      "learning_rate": 1.116363079785171e-05,
      "loss": 1.0322,
      "step": 10383
    },
    {
      "epoch": 1.4598622240967243,
      "grad_norm": 1.6609971523284912,
      "learning_rate": 1.1270606891184321e-05,
      "loss": 1.0054,
      "step": 10384
    },
    {
      "epoch": 1.460002811753128,
      "grad_norm": 1.4955976009368896,
      "learning_rate": 1.1378062496242692e-05,
      "loss": 1.0469,
      "step": 10385
    },
    {
      "epoch": 1.4601433994095319,
      "grad_norm": 1.6020840406417847,
      "learning_rate": 1.1485997032314988e-05,
      "loss": 1.0508,
      "step": 10386
    },
    {
      "epoch": 1.4602839870659357,
      "grad_norm": 1.5678852796554565,
      "learning_rate": 1.1594409916100635e-05,
      "loss": 1.0099,
      "step": 10387
    },
    {
      "epoch": 1.4604245747223394,
      "grad_norm": 1.4836972951889038,
      "learning_rate": 1.1703300561714504e-05,
      "loss": 0.908,
      "step": 10388
    },
    {
      "epoch": 1.4605651623787432,
      "grad_norm": 1.42807137966156,
      "learning_rate": 1.1812668380689162e-05,
      "loss": 1.0266,
      "step": 10389
    },
    {
      "epoch": 1.4607057500351468,
      "grad_norm": 1.6273620128631592,
      "learning_rate": 1.1922512781978956e-05,
      "loss": 1.0197,
      "step": 10390
    },
    {
      "epoch": 1.4608463376915508,
      "grad_norm": 1.5669749975204468,
      "learning_rate": 1.20328331719622e-05,
      "loss": 1.0536,
      "step": 10391
    },
    {
      "epoch": 1.4609869253479544,
      "grad_norm": 1.3496661186218262,
      "learning_rate": 1.2143628954445019e-05,
      "loss": 1.0693,
      "step": 10392
    },
    {
      "epoch": 1.4611275130043582,
      "grad_norm": 1.5607836246490479,
      "learning_rate": 1.225489953066441e-05,
      "loss": 1.1745,
      "step": 10393
    },
    {
      "epoch": 1.461268100660762,
      "grad_norm": 1.4452247619628906,
      "learning_rate": 1.2366644299291507e-05,
      "loss": 1.1542,
      "step": 10394
    },
    {
      "epoch": 1.4614086883171657,
      "grad_norm": 1.9637616872787476,
      "learning_rate": 1.2478862656434797e-05,
      "loss": 0.7622,
      "step": 10395
    },
    {
      "epoch": 1.4615492759735695,
      "grad_norm": 1.2949360609054565,
      "learning_rate": 1.2591553995643412e-05,
      "loss": 0.8999,
      "step": 10396
    },
    {
      "epoch": 1.4616898636299733,
      "grad_norm": 1.6426887512207031,
      "learning_rate": 1.2704717707910385e-05,
      "loss": 1.0133,
      "step": 10397
    },
    {
      "epoch": 1.4618304512863771,
      "grad_norm": 1.4990018606185913,
      "learning_rate": 1.2818353181675947e-05,
      "loss": 1.0004,
      "step": 10398
    },
    {
      "epoch": 1.4619710389427807,
      "grad_norm": 1.4176762104034424,
      "learning_rate": 1.2932459802830655e-05,
      "loss": 1.168,
      "step": 10399
    },
    {
      "epoch": 1.4621116265991847,
      "grad_norm": 2.5119223594665527,
      "learning_rate": 1.3047036954719426e-05,
      "loss": 1.0251,
      "step": 10400
    },
    {
      "epoch": 1.4622522142555883,
      "grad_norm": 1.4008722305297852,
      "learning_rate": 1.3162084018143995e-05,
      "loss": 1.1821,
      "step": 10401
    },
    {
      "epoch": 1.462392801911992,
      "grad_norm": 1.6934832334518433,
      "learning_rate": 1.3277600371366195e-05,
      "loss": 1.1305,
      "step": 10402
    },
    {
      "epoch": 1.4625333895683958,
      "grad_norm": 1.2672381401062012,
      "learning_rate": 1.3393585390112307e-05,
      "loss": 0.9936,
      "step": 10403
    },
    {
      "epoch": 1.4626739772247996,
      "grad_norm": 1.7006349563598633,
      "learning_rate": 1.3510038447575657e-05,
      "loss": 1.0672,
      "step": 10404
    },
    {
      "epoch": 1.4628145648812034,
      "grad_norm": 1.5843462944030762,
      "learning_rate": 1.3626958914419973e-05,
      "loss": 0.9174,
      "step": 10405
    },
    {
      "epoch": 1.4629551525376072,
      "grad_norm": 1.6395609378814697,
      "learning_rate": 1.3744346158783616e-05,
      "loss": 0.9255,
      "step": 10406
    },
    {
      "epoch": 1.463095740194011,
      "grad_norm": 1.6609455347061157,
      "learning_rate": 1.3862199546281795e-05,
      "loss": 1.2084,
      "step": 10407
    },
    {
      "epoch": 1.4632363278504148,
      "grad_norm": 1.522264003753662,
      "learning_rate": 1.3980518440010782e-05,
      "loss": 1.0593,
      "step": 10408
    },
    {
      "epoch": 1.4633769155068186,
      "grad_norm": 1.5061603784561157,
      "learning_rate": 1.4099302200551335e-05,
      "loss": 1.1329,
      "step": 10409
    },
    {
      "epoch": 1.4635175031632222,
      "grad_norm": 1.856245756149292,
      "learning_rate": 1.4218550185971269e-05,
      "loss": 1.02,
      "step": 10410
    },
    {
      "epoch": 1.4636580908196262,
      "grad_norm": 1.6929734945297241,
      "learning_rate": 1.4338261751830696e-05,
      "loss": 0.9988,
      "step": 10411
    },
    {
      "epoch": 1.4637986784760297,
      "grad_norm": 2.063398599624634,
      "learning_rate": 1.4458436251183748e-05,
      "loss": 0.9043,
      "step": 10412
    },
    {
      "epoch": 1.4639392661324335,
      "grad_norm": 1.3843883275985718,
      "learning_rate": 1.4579073034582924e-05,
      "loss": 1.0137,
      "step": 10413
    },
    {
      "epoch": 1.4640798537888373,
      "grad_norm": 2.2145211696624756,
      "learning_rate": 1.4700171450082457e-05,
      "loss": 1.0326,
      "step": 10414
    },
    {
      "epoch": 1.464220441445241,
      "grad_norm": 1.5569299459457397,
      "learning_rate": 1.4821730843241832e-05,
      "loss": 1.0013,
      "step": 10415
    },
    {
      "epoch": 1.464361029101645,
      "grad_norm": 1.5636297464370728,
      "learning_rate": 1.494375055712931e-05,
      "loss": 1.0179,
      "step": 10416
    },
    {
      "epoch": 1.4645016167580487,
      "grad_norm": 1.372693419456482,
      "learning_rate": 1.5066229932325493e-05,
      "loss": 1.0729,
      "step": 10417
    },
    {
      "epoch": 1.4646422044144525,
      "grad_norm": 1.6608867645263672,
      "learning_rate": 1.518916830692686e-05,
      "loss": 1.0292,
      "step": 10418
    },
    {
      "epoch": 1.464782792070856,
      "grad_norm": 1.340990662574768,
      "learning_rate": 1.5312565016549385e-05,
      "loss": 0.9803,
      "step": 10419
    },
    {
      "epoch": 1.46492337972726,
      "grad_norm": 1.4069570302963257,
      "learning_rate": 1.543641939433209e-05,
      "loss": 1.2069,
      "step": 10420
    },
    {
      "epoch": 1.4650639673836636,
      "grad_norm": 1.5035182237625122,
      "learning_rate": 1.556073077094049e-05,
      "loss": 1.0427,
      "step": 10421
    },
    {
      "epoch": 1.4652045550400674,
      "grad_norm": 1.4825201034545898,
      "learning_rate": 1.5685498474570946e-05,
      "loss": 0.9721,
      "step": 10422
    },
    {
      "epoch": 1.4653451426964712,
      "grad_norm": 1.4986767768859863,
      "learning_rate": 1.5810721830953167e-05,
      "loss": 1.0886,
      "step": 10423
    },
    {
      "epoch": 1.465485730352875,
      "grad_norm": 1.35432767868042,
      "learning_rate": 1.5936400163354802e-05,
      "loss": 1.207,
      "step": 10424
    },
    {
      "epoch": 1.4656263180092788,
      "grad_norm": 1.4285051822662354,
      "learning_rate": 1.606253279258415e-05,
      "loss": 1.1048,
      "step": 10425
    },
    {
      "epoch": 1.4657669056656826,
      "grad_norm": 1.5201107263565063,
      "learning_rate": 1.618911903699476e-05,
      "loss": 1.0543,
      "step": 10426
    },
    {
      "epoch": 1.4659074933220864,
      "grad_norm": 1.2805304527282715,
      "learning_rate": 1.631615821248915e-05,
      "loss": 1.1533,
      "step": 10427
    },
    {
      "epoch": 1.4660480809784902,
      "grad_norm": 1.549578070640564,
      "learning_rate": 1.6443649632521542e-05,
      "loss": 0.9662,
      "step": 10428
    },
    {
      "epoch": 1.466188668634894,
      "grad_norm": 1.5746618509292603,
      "learning_rate": 1.6571592608102314e-05,
      "loss": 1.1238,
      "step": 10429
    },
    {
      "epoch": 1.4663292562912975,
      "grad_norm": 1.5531160831451416,
      "learning_rate": 1.669998644780154e-05,
      "loss": 0.896,
      "step": 10430
    },
    {
      "epoch": 1.4664698439477015,
      "grad_norm": 1.7925150394439697,
      "learning_rate": 1.682883045775274e-05,
      "loss": 1.0736,
      "step": 10431
    },
    {
      "epoch": 1.466610431604105,
      "grad_norm": 1.7147248983383179,
      "learning_rate": 1.6958123941656623e-05,
      "loss": 0.9639,
      "step": 10432
    },
    {
      "epoch": 1.4667510192605089,
      "grad_norm": 1.5342885255813599,
      "learning_rate": 1.7087866200784864e-05,
      "loss": 0.9937,
      "step": 10433
    },
    {
      "epoch": 1.4668916069169127,
      "grad_norm": 1.9248141050338745,
      "learning_rate": 1.7218056533983827e-05,
      "loss": 1.1488,
      "step": 10434
    },
    {
      "epoch": 1.4670321945733165,
      "grad_norm": 1.516310691833496,
      "learning_rate": 1.734869423767843e-05,
      "loss": 0.9907,
      "step": 10435
    },
    {
      "epoch": 1.4671727822297203,
      "grad_norm": 1.6493241786956787,
      "learning_rate": 1.7479778605875886e-05,
      "loss": 1.0111,
      "step": 10436
    },
    {
      "epoch": 1.467313369886124,
      "grad_norm": 1.3773247003555298,
      "learning_rate": 1.7611308930169357e-05,
      "loss": 1.0301,
      "step": 10437
    },
    {
      "epoch": 1.4674539575425278,
      "grad_norm": 1.7527176141738892,
      "learning_rate": 1.7743284499742564e-05,
      "loss": 1.0414,
      "step": 10438
    },
    {
      "epoch": 1.4675945451989314,
      "grad_norm": 1.8076649904251099,
      "learning_rate": 1.7875704601372623e-05,
      "loss": 0.9651,
      "step": 10439
    },
    {
      "epoch": 1.4677351328553354,
      "grad_norm": 1.492119550704956,
      "learning_rate": 1.800856851943391e-05,
      "loss": 1.0906,
      "step": 10440
    },
    {
      "epoch": 1.467875720511739,
      "grad_norm": 1.3945789337158203,
      "learning_rate": 1.814187553590294e-05,
      "loss": 1.0063,
      "step": 10441
    },
    {
      "epoch": 1.4680163081681428,
      "grad_norm": 1.7609256505966187,
      "learning_rate": 1.8275624930361213e-05,
      "loss": 1.087,
      "step": 10442
    },
    {
      "epoch": 1.4681568958245466,
      "grad_norm": 1.4566822052001953,
      "learning_rate": 1.8409815980000154e-05,
      "loss": 1.0452,
      "step": 10443
    },
    {
      "epoch": 1.4682974834809503,
      "grad_norm": 1.5159865617752075,
      "learning_rate": 1.854444795962377e-05,
      "loss": 1.1987,
      "step": 10444
    },
    {
      "epoch": 1.4684380711373541,
      "grad_norm": 1.366823434829712,
      "learning_rate": 1.8679520141653407e-05,
      "loss": 1.2456,
      "step": 10445
    },
    {
      "epoch": 1.468578658793758,
      "grad_norm": 1.564721941947937,
      "learning_rate": 1.8815031796131454e-05,
      "loss": 0.9928,
      "step": 10446
    },
    {
      "epoch": 1.4687192464501617,
      "grad_norm": 1.3770562410354614,
      "learning_rate": 1.895098219072553e-05,
      "loss": 1.0616,
      "step": 10447
    },
    {
      "epoch": 1.4688598341065655,
      "grad_norm": 1.6224275827407837,
      "learning_rate": 1.9087370590731356e-05,
      "loss": 1.0447,
      "step": 10448
    },
    {
      "epoch": 1.4690004217629693,
      "grad_norm": 1.4605978727340698,
      "learning_rate": 1.922419625907884e-05,
      "loss": 1.0577,
      "step": 10449
    },
    {
      "epoch": 1.4691410094193729,
      "grad_norm": 1.4719752073287964,
      "learning_rate": 1.9361458456333992e-05,
      "loss": 0.9992,
      "step": 10450
    },
    {
      "epoch": 1.4692815970757769,
      "grad_norm": 1.630458116531372,
      "learning_rate": 1.94991564407039e-05,
      "loss": 1.0765,
      "step": 10451
    },
    {
      "epoch": 1.4694221847321804,
      "grad_norm": 1.4993475675582886,
      "learning_rate": 1.963728946804061e-05,
      "loss": 1.1034,
      "step": 10452
    },
    {
      "epoch": 1.4695627723885842,
      "grad_norm": 1.6898308992385864,
      "learning_rate": 1.977585679184486e-05,
      "loss": 0.9759,
      "step": 10453
    },
    {
      "epoch": 1.469703360044988,
      "grad_norm": 1.3526380062103271,
      "learning_rate": 1.9914857663271223e-05,
      "loss": 1.0731,
      "step": 10454
    },
    {
      "epoch": 1.4698439477013918,
      "grad_norm": 1.740816593170166,
      "learning_rate": 2.005429133113004e-05,
      "loss": 1.0842,
      "step": 10455
    },
    {
      "epoch": 1.4699845353577956,
      "grad_norm": 1.7966285943984985,
      "learning_rate": 2.0194157041893534e-05,
      "loss": 0.9898,
      "step": 10456
    },
    {
      "epoch": 1.4701251230141994,
      "grad_norm": 1.4617735147476196,
      "learning_rate": 2.0334454039698924e-05,
      "loss": 1.091,
      "step": 10457
    },
    {
      "epoch": 1.4702657106706032,
      "grad_norm": 1.5920898914337158,
      "learning_rate": 2.047518156635262e-05,
      "loss": 1.1491,
      "step": 10458
    },
    {
      "epoch": 1.4704062983270068,
      "grad_norm": 1.755500316619873,
      "learning_rate": 2.0616338861334172e-05,
      "loss": 1.0266,
      "step": 10459
    },
    {
      "epoch": 1.4705468859834108,
      "grad_norm": 1.5518134832382202,
      "learning_rate": 2.0757925161801218e-05,
      "loss": 0.8704,
      "step": 10460
    },
    {
      "epoch": 1.4706874736398143,
      "grad_norm": 1.7937818765640259,
      "learning_rate": 2.0899939702592318e-05,
      "loss": 0.9844,
      "step": 10461
    },
    {
      "epoch": 1.4708280612962181,
      "grad_norm": 1.4427874088287354,
      "learning_rate": 2.1042381716232163e-05,
      "loss": 1.1867,
      "step": 10462
    },
    {
      "epoch": 1.470968648952622,
      "grad_norm": 1.4416509866714478,
      "learning_rate": 2.1185250432934635e-05,
      "loss": 1.1374,
      "step": 10463
    },
    {
      "epoch": 1.4711092366090257,
      "grad_norm": 1.8439520597457886,
      "learning_rate": 2.1328545080608088e-05,
      "loss": 1.0395,
      "step": 10464
    },
    {
      "epoch": 1.4712498242654295,
      "grad_norm": 1.4909169673919678,
      "learning_rate": 2.147226488485945e-05,
      "loss": 1.1826,
      "step": 10465
    },
    {
      "epoch": 1.4713904119218333,
      "grad_norm": 1.6054495573043823,
      "learning_rate": 2.1616409068997413e-05,
      "loss": 1.064,
      "step": 10466
    },
    {
      "epoch": 1.471530999578237,
      "grad_norm": 1.6931400299072266,
      "learning_rate": 2.1760976854037396e-05,
      "loss": 1.1071,
      "step": 10467
    },
    {
      "epoch": 1.4716715872346409,
      "grad_norm": 1.5231198072433472,
      "learning_rate": 2.1905967458705658e-05,
      "loss": 0.9656,
      "step": 10468
    },
    {
      "epoch": 1.4718121748910447,
      "grad_norm": 1.3611574172973633,
      "learning_rate": 2.2051380099443154e-05,
      "loss": 1.2007,
      "step": 10469
    },
    {
      "epoch": 1.4719527625474482,
      "grad_norm": 1.593234658241272,
      "learning_rate": 2.2197213990411038e-05,
      "loss": 1.1683,
      "step": 10470
    },
    {
      "epoch": 1.4720933502038522,
      "grad_norm": 2.0894510746002197,
      "learning_rate": 2.2343468343492558e-05,
      "loss": 1.0121,
      "step": 10471
    },
    {
      "epoch": 1.4722339378602558,
      "grad_norm": 1.6755738258361816,
      "learning_rate": 2.2490142368299593e-05,
      "loss": 1.0195,
      "step": 10472
    },
    {
      "epoch": 1.4723745255166596,
      "grad_norm": 1.3518680334091187,
      "learning_rate": 2.2637235272175826e-05,
      "loss": 1.0377,
      "step": 10473
    },
    {
      "epoch": 1.4725151131730634,
      "grad_norm": 1.5655227899551392,
      "learning_rate": 2.2784746260201228e-05,
      "loss": 1.0485,
      "step": 10474
    },
    {
      "epoch": 1.4726557008294672,
      "grad_norm": 1.6871753931045532,
      "learning_rate": 2.2932674535196108e-05,
      "loss": 0.9908,
      "step": 10475
    },
    {
      "epoch": 1.472796288485871,
      "grad_norm": 1.4779125452041626,
      "learning_rate": 2.3081019297726392e-05,
      "loss": 1.0254,
      "step": 10476
    },
    {
      "epoch": 1.4729368761422748,
      "grad_norm": 1.168226957321167,
      "learning_rate": 2.322977974610674e-05,
      "loss": 0.9925,
      "step": 10477
    },
    {
      "epoch": 1.4730774637986785,
      "grad_norm": 1.7971279621124268,
      "learning_rate": 2.3378955076404897e-05,
      "loss": 1.0637,
      "step": 10478
    },
    {
      "epoch": 1.4732180514550821,
      "grad_norm": 1.5872100591659546,
      "learning_rate": 2.3528544482447223e-05,
      "loss": 1.1368,
      "step": 10479
    },
    {
      "epoch": 1.4733586391114861,
      "grad_norm": 1.640265703201294,
      "learning_rate": 2.367854715582183e-05,
      "loss": 0.9129,
      "step": 10480
    },
    {
      "epoch": 1.4734992267678897,
      "grad_norm": 1.3687539100646973,
      "learning_rate": 2.382896228588417e-05,
      "loss": 1.0066,
      "step": 10481
    },
    {
      "epoch": 1.4736398144242935,
      "grad_norm": 1.6683900356292725,
      "learning_rate": 2.3979789059759984e-05,
      "loss": 1.0364,
      "step": 10482
    },
    {
      "epoch": 1.4737804020806973,
      "grad_norm": 1.429453730583191,
      "learning_rate": 2.413102666235064e-05,
      "loss": 1.0836,
      "step": 10483
    },
    {
      "epoch": 1.473920989737101,
      "grad_norm": 1.6257283687591553,
      "learning_rate": 2.4282674276337316e-05,
      "loss": 1.0636,
      "step": 10484
    },
    {
      "epoch": 1.4740615773935049,
      "grad_norm": 1.525564432144165,
      "learning_rate": 2.443473108218539e-05,
      "loss": 1.1224,
      "step": 10485
    },
    {
      "epoch": 1.4742021650499086,
      "grad_norm": 1.5793704986572266,
      "learning_rate": 2.4587196258148893e-05,
      "loss": 1.1363,
      "step": 10486
    },
    {
      "epoch": 1.4743427527063124,
      "grad_norm": 1.7939532995224,
      "learning_rate": 2.4740068980274933e-05,
      "loss": 0.9116,
      "step": 10487
    },
    {
      "epoch": 1.4744833403627162,
      "grad_norm": 1.487920880317688,
      "learning_rate": 2.489334842240817e-05,
      "loss": 1.1363,
      "step": 10488
    },
    {
      "epoch": 1.47462392801912,
      "grad_norm": 1.4792110919952393,
      "learning_rate": 2.504703375619525e-05,
      "loss": 1.0508,
      "step": 10489
    },
    {
      "epoch": 1.4747645156755236,
      "grad_norm": 1.6476715803146362,
      "learning_rate": 2.5201124151089306e-05,
      "loss": 0.961,
      "step": 10490
    },
    {
      "epoch": 1.4749051033319276,
      "grad_norm": 1.8628675937652588,
      "learning_rate": 2.5355618774354194e-05,
      "loss": 1.1309,
      "step": 10491
    },
    {
      "epoch": 1.4750456909883312,
      "grad_norm": 1.3955357074737549,
      "learning_rate": 2.551051679106996e-05,
      "loss": 1.0067,
      "step": 10492
    },
    {
      "epoch": 1.475186278644735,
      "grad_norm": 1.3126161098480225,
      "learning_rate": 2.5665817364136148e-05,
      "loss": 1.1241,
      "step": 10493
    },
    {
      "epoch": 1.4753268663011387,
      "grad_norm": 1.5754982233047485,
      "learning_rate": 2.5821519654276328e-05,
      "loss": 1.0583,
      "step": 10494
    },
    {
      "epoch": 1.4754674539575425,
      "grad_norm": 1.4617975950241089,
      "learning_rate": 2.5977622820043835e-05,
      "loss": 1.0825,
      "step": 10495
    },
    {
      "epoch": 1.4756080416139463,
      "grad_norm": 1.4845815896987915,
      "learning_rate": 2.613412601782509e-05,
      "loss": 1.1486,
      "step": 10496
    },
    {
      "epoch": 1.47574862927035,
      "grad_norm": 1.7076060771942139,
      "learning_rate": 2.6291028401845396e-05,
      "loss": 1.0316,
      "step": 10497
    },
    {
      "epoch": 1.475889216926754,
      "grad_norm": 1.405234456062317,
      "learning_rate": 2.6448329124172057e-05,
      "loss": 1.0651,
      "step": 10498
    },
    {
      "epoch": 1.4760298045831575,
      "grad_norm": 1.378941535949707,
      "learning_rate": 2.660602733471991e-05,
      "loss": 0.9766,
      "step": 10499
    },
    {
      "epoch": 1.4761703922395615,
      "grad_norm": 1.8583295345306396,
      "learning_rate": 2.6764122181255903e-05,
      "loss": 1.0096,
      "step": 10500
    },
    {
      "epoch": 1.4761703922395615,
      "eval_loss": 1.1397053003311157,
      "eval_runtime": 772.3936,
      "eval_samples_per_second": 16.372,
      "eval_steps_per_second": 8.186,
      "step": 10500
    },
    {
      "epoch": 1.476310979895965,
      "grad_norm": 1.488490104675293,
      "learning_rate": 2.6922612809402802e-05,
      "loss": 0.9959,
      "step": 10501
    },
    {
      "epoch": 1.4764515675523688,
      "grad_norm": 1.6607037782669067,
      "learning_rate": 2.708149836264494e-05,
      "loss": 1.0771,
      "step": 10502
    },
    {
      "epoch": 1.4765921552087726,
      "grad_norm": 1.6200852394104004,
      "learning_rate": 2.7240777982332876e-05,
      "loss": 1.0715,
      "step": 10503
    },
    {
      "epoch": 1.4767327428651764,
      "grad_norm": 1.633510947227478,
      "learning_rate": 2.740045080768685e-05,
      "loss": 0.9731,
      "step": 10504
    },
    {
      "epoch": 1.4768733305215802,
      "grad_norm": 1.5603874921798706,
      "learning_rate": 2.7560515975802358e-05,
      "loss": 1.2057,
      "step": 10505
    },
    {
      "epoch": 1.477013918177984,
      "grad_norm": 1.443109393119812,
      "learning_rate": 2.7720972621654618e-05,
      "loss": 1.0994,
      "step": 10506
    },
    {
      "epoch": 1.4771545058343878,
      "grad_norm": 1.4898219108581543,
      "learning_rate": 2.7881819878102956e-05,
      "loss": 1.0123,
      "step": 10507
    },
    {
      "epoch": 1.4772950934907916,
      "grad_norm": 1.7160197496414185,
      "learning_rate": 2.8043056875896746e-05,
      "loss": 1.1085,
      "step": 10508
    },
    {
      "epoch": 1.4774356811471954,
      "grad_norm": 1.460928201675415,
      "learning_rate": 2.8204682743677667e-05,
      "loss": 1.1855,
      "step": 10509
    },
    {
      "epoch": 1.477576268803599,
      "grad_norm": 1.435725450515747,
      "learning_rate": 2.836669660798682e-05,
      "loss": 1.1008,
      "step": 10510
    },
    {
      "epoch": 1.477716856460003,
      "grad_norm": 1.301481008529663,
      "learning_rate": 2.8529097593268262e-05,
      "loss": 1.1345,
      "step": 10511
    },
    {
      "epoch": 1.4778574441164065,
      "grad_norm": 1.6443469524383545,
      "learning_rate": 2.8691884821873716e-05,
      "loss": 0.8709,
      "step": 10512
    },
    {
      "epoch": 1.4779980317728103,
      "grad_norm": 1.3610665798187256,
      "learning_rate": 2.885505741406832e-05,
      "loss": 1.1193,
      "step": 10513
    },
    {
      "epoch": 1.478138619429214,
      "grad_norm": 1.3660609722137451,
      "learning_rate": 2.9018614488033867e-05,
      "loss": 1.1355,
      "step": 10514
    },
    {
      "epoch": 1.4782792070856179,
      "grad_norm": 1.4755486249923706,
      "learning_rate": 2.9182555159874858e-05,
      "loss": 1.2275,
      "step": 10515
    },
    {
      "epoch": 1.4784197947420217,
      "grad_norm": 1.346136212348938,
      "learning_rate": 2.934687854362198e-05,
      "loss": 1.136,
      "step": 10516
    },
    {
      "epoch": 1.4785603823984255,
      "grad_norm": 1.9146493673324585,
      "learning_rate": 2.9511583751238424e-05,
      "loss": 0.9656,
      "step": 10517
    },
    {
      "epoch": 1.4787009700548293,
      "grad_norm": 1.3875479698181152,
      "learning_rate": 2.967666989262342e-05,
      "loss": 1.1207,
      "step": 10518
    },
    {
      "epoch": 1.4788415577112328,
      "grad_norm": 1.5751687288284302,
      "learning_rate": 2.984213607561832e-05,
      "loss": 0.9899,
      "step": 10519
    },
    {
      "epoch": 1.4789821453676368,
      "grad_norm": 1.5266032218933105,
      "learning_rate": 3.0007981406009888e-05,
      "loss": 1.0362,
      "step": 10520
    },
    {
      "epoch": 1.4791227330240404,
      "grad_norm": 1.4643558263778687,
      "learning_rate": 3.017420498753617e-05,
      "loss": 1.0486,
      "step": 10521
    },
    {
      "epoch": 1.4792633206804442,
      "grad_norm": 1.5673073530197144,
      "learning_rate": 3.0340805921891035e-05,
      "loss": 1.0824,
      "step": 10522
    },
    {
      "epoch": 1.479403908336848,
      "grad_norm": 1.5356438159942627,
      "learning_rate": 3.0507783308729076e-05,
      "loss": 1.0638,
      "step": 10523
    },
    {
      "epoch": 1.4795444959932518,
      "grad_norm": 1.5380535125732422,
      "learning_rate": 3.067513624567047e-05,
      "loss": 1.0244,
      "step": 10524
    },
    {
      "epoch": 1.4796850836496556,
      "grad_norm": 1.6175836324691772,
      "learning_rate": 3.084286382830582e-05,
      "loss": 0.9828,
      "step": 10525
    },
    {
      "epoch": 1.4798256713060594,
      "grad_norm": 1.4708508253097534,
      "learning_rate": 3.1010965150201096e-05,
      "loss": 1.2774,
      "step": 10526
    },
    {
      "epoch": 1.4799662589624631,
      "grad_norm": 1.4761275053024292,
      "learning_rate": 3.1179439302902494e-05,
      "loss": 1.2248,
      "step": 10527
    },
    {
      "epoch": 1.480106846618867,
      "grad_norm": 1.5137882232666016,
      "learning_rate": 3.134828537594134e-05,
      "loss": 1.115,
      "step": 10528
    },
    {
      "epoch": 1.4802474342752707,
      "grad_norm": 1.7884562015533447,
      "learning_rate": 3.151750245683879e-05,
      "loss": 1.0588,
      "step": 10529
    },
    {
      "epoch": 1.4803880219316743,
      "grad_norm": 1.6870702505111694,
      "learning_rate": 3.1687089631111754e-05,
      "loss": 1.0081,
      "step": 10530
    },
    {
      "epoch": 1.4805286095880783,
      "grad_norm": 1.5797990560531616,
      "learning_rate": 3.1857045982276604e-05,
      "loss": 1.2546,
      "step": 10531
    },
    {
      "epoch": 1.4806691972444819,
      "grad_norm": 1.4004253149032593,
      "learning_rate": 3.202737059185402e-05,
      "loss": 0.9839,
      "step": 10532
    },
    {
      "epoch": 1.4808097849008857,
      "grad_norm": 1.4176243543624878,
      "learning_rate": 3.2198062539375384e-05,
      "loss": 1.1376,
      "step": 10533
    },
    {
      "epoch": 1.4809503725572895,
      "grad_norm": 1.362858533859253,
      "learning_rate": 3.236912090238634e-05,
      "loss": 0.9612,
      "step": 10534
    },
    {
      "epoch": 1.4810909602136932,
      "grad_norm": 1.5414729118347168,
      "learning_rate": 3.254054475645317e-05,
      "loss": 1.1314,
      "step": 10535
    },
    {
      "epoch": 1.481231547870097,
      "grad_norm": 1.4361134767532349,
      "learning_rate": 3.271233317516614e-05,
      "loss": 1.0661,
      "step": 10536
    },
    {
      "epoch": 1.4813721355265008,
      "grad_norm": 1.3230502605438232,
      "learning_rate": 3.288448523014565e-05,
      "loss": 1.1173,
      "step": 10537
    },
    {
      "epoch": 1.4815127231829046,
      "grad_norm": 2.062312126159668,
      "learning_rate": 3.3056999991047145e-05,
      "loss": 1.0862,
      "step": 10538
    },
    {
      "epoch": 1.4816533108393082,
      "grad_norm": 1.5864125490188599,
      "learning_rate": 3.322987652556491e-05,
      "loss": 1.3003,
      "step": 10539
    },
    {
      "epoch": 1.4817938984957122,
      "grad_norm": 1.5831522941589355,
      "learning_rate": 3.340311389943971e-05,
      "loss": 1.0009,
      "step": 10540
    },
    {
      "epoch": 1.4819344861521158,
      "grad_norm": 1.6033504009246826,
      "learning_rate": 3.3576711176461205e-05,
      "loss": 1.1455,
      "step": 10541
    },
    {
      "epoch": 1.4820750738085195,
      "grad_norm": 1.521458625793457,
      "learning_rate": 3.3750667418474315e-05,
      "loss": 1.0101,
      "step": 10542
    },
    {
      "epoch": 1.4822156614649233,
      "grad_norm": 1.4447726011276245,
      "learning_rate": 3.392498168538406e-05,
      "loss": 1.0201,
      "step": 10543
    },
    {
      "epoch": 1.4823562491213271,
      "grad_norm": 1.7224035263061523,
      "learning_rate": 3.4099653035160615e-05,
      "loss": 1.1153,
      "step": 10544
    },
    {
      "epoch": 1.482496836777731,
      "grad_norm": 1.6254481077194214,
      "learning_rate": 3.427468052384413e-05,
      "loss": 1.1115,
      "step": 10545
    },
    {
      "epoch": 1.4826374244341347,
      "grad_norm": 1.5535153150558472,
      "learning_rate": 3.4450063205551175e-05,
      "loss": 0.8971,
      "step": 10546
    },
    {
      "epoch": 1.4827780120905385,
      "grad_norm": 1.6815993785858154,
      "learning_rate": 3.46258001324772e-05,
      "loss": 1.1323,
      "step": 10547
    },
    {
      "epoch": 1.4829185997469423,
      "grad_norm": 1.520137071609497,
      "learning_rate": 3.48018903549043e-05,
      "loss": 1.3062,
      "step": 10548
    },
    {
      "epoch": 1.483059187403346,
      "grad_norm": 1.385745644569397,
      "learning_rate": 3.4978332921205025e-05,
      "loss": 0.9776,
      "step": 10549
    },
    {
      "epoch": 1.4831997750597496,
      "grad_norm": 1.6285285949707031,
      "learning_rate": 3.51551268778475e-05,
      "loss": 1.144,
      "step": 10550
    },
    {
      "epoch": 1.4833403627161537,
      "grad_norm": 1.3977088928222656,
      "learning_rate": 3.533227126940168e-05,
      "loss": 1.141,
      "step": 10551
    },
    {
      "epoch": 1.4834809503725572,
      "grad_norm": 1.522425889968872,
      "learning_rate": 3.5509765138542926e-05,
      "loss": 1.0523,
      "step": 10552
    },
    {
      "epoch": 1.483621538028961,
      "grad_norm": 1.3676189184188843,
      "learning_rate": 3.5687607526058444e-05,
      "loss": 1.1499,
      "step": 10553
    },
    {
      "epoch": 1.4837621256853648,
      "grad_norm": 1.5499619245529175,
      "learning_rate": 3.586579747085118e-05,
      "loss": 1.0314,
      "step": 10554
    },
    {
      "epoch": 1.4839027133417686,
      "grad_norm": 1.4486464262008667,
      "learning_rate": 3.604433400994636e-05,
      "loss": 1.1492,
      "step": 10555
    },
    {
      "epoch": 1.4840433009981724,
      "grad_norm": 1.5438824892044067,
      "learning_rate": 3.6223216178496675e-05,
      "loss": 1.1682,
      "step": 10556
    },
    {
      "epoch": 1.4841838886545762,
      "grad_norm": 1.4213165044784546,
      "learning_rate": 3.6402443009786144e-05,
      "loss": 1.0927,
      "step": 10557
    },
    {
      "epoch": 1.48432447631098,
      "grad_norm": 1.4865446090698242,
      "learning_rate": 3.658201353523646e-05,
      "loss": 1.0092,
      "step": 10558
    },
    {
      "epoch": 1.4844650639673835,
      "grad_norm": 1.7509008646011353,
      "learning_rate": 3.6761926784411885e-05,
      "loss": 1.0738,
      "step": 10559
    },
    {
      "epoch": 1.4846056516237875,
      "grad_norm": 1.289127230644226,
      "learning_rate": 3.694218178502455e-05,
      "loss": 0.9719,
      "step": 10560
    },
    {
      "epoch": 1.4847462392801911,
      "grad_norm": 1.701581358909607,
      "learning_rate": 3.7122777562939716e-05,
      "loss": 0.8271,
      "step": 10561
    },
    {
      "epoch": 1.484886826936595,
      "grad_norm": 1.7928905487060547,
      "learning_rate": 3.7303713142180954e-05,
      "loss": 1.2358,
      "step": 10562
    },
    {
      "epoch": 1.4850274145929987,
      "grad_norm": 1.4914878606796265,
      "learning_rate": 3.748498754493553e-05,
      "loss": 1.1516,
      "step": 10563
    },
    {
      "epoch": 1.4851680022494025,
      "grad_norm": 1.9383339881896973,
      "learning_rate": 3.766659979155963e-05,
      "loss": 1.0778,
      "step": 10564
    },
    {
      "epoch": 1.4853085899058063,
      "grad_norm": 1.4204450845718384,
      "learning_rate": 3.7848548900583644e-05,
      "loss": 1.1713,
      "step": 10565
    },
    {
      "epoch": 1.48544917756221,
      "grad_norm": 1.599272608757019,
      "learning_rate": 3.803083388871723e-05,
      "loss": 0.9167,
      "step": 10566
    },
    {
      "epoch": 1.4855897652186139,
      "grad_norm": 1.7552790641784668,
      "learning_rate": 3.82134537708557e-05,
      "loss": 1.0236,
      "step": 10567
    },
    {
      "epoch": 1.4857303528750176,
      "grad_norm": 1.4642447233200073,
      "learning_rate": 3.8396407560083734e-05,
      "loss": 0.9287,
      "step": 10568
    },
    {
      "epoch": 1.4858709405314214,
      "grad_norm": 1.5264118909835815,
      "learning_rate": 3.857969426768203e-05,
      "loss": 1.0448,
      "step": 10569
    },
    {
      "epoch": 1.486011528187825,
      "grad_norm": 1.4481860399246216,
      "learning_rate": 3.87633129031313e-05,
      "loss": 1.1261,
      "step": 10570
    },
    {
      "epoch": 1.4861521158442288,
      "grad_norm": 1.4683901071548462,
      "learning_rate": 3.8947262474119294e-05,
      "loss": 1.0603,
      "step": 10571
    },
    {
      "epoch": 1.4862927035006326,
      "grad_norm": 1.717209815979004,
      "learning_rate": 3.9131541986544764e-05,
      "loss": 0.9664,
      "step": 10572
    },
    {
      "epoch": 1.4864332911570364,
      "grad_norm": 1.8545403480529785,
      "learning_rate": 3.931615044452422e-05,
      "loss": 1.1675,
      "step": 10573
    },
    {
      "epoch": 1.4865738788134402,
      "grad_norm": 1.6193761825561523,
      "learning_rate": 3.950108685039563e-05,
      "loss": 1.0885,
      "step": 10574
    },
    {
      "epoch": 1.486714466469844,
      "grad_norm": 1.7553958892822266,
      "learning_rate": 3.968635020472497e-05,
      "loss": 1.0715,
      "step": 10575
    },
    {
      "epoch": 1.4868550541262477,
      "grad_norm": 1.52716064453125,
      "learning_rate": 3.987193950631162e-05,
      "loss": 1.0093,
      "step": 10576
    },
    {
      "epoch": 1.4869956417826515,
      "grad_norm": 1.5764665603637695,
      "learning_rate": 4.005785375219229e-05,
      "loss": 0.9171,
      "step": 10577
    },
    {
      "epoch": 1.4871362294390553,
      "grad_norm": 1.6757320165634155,
      "learning_rate": 4.0244091937649364e-05,
      "loss": 0.9806,
      "step": 10578
    },
    {
      "epoch": 1.487276817095459,
      "grad_norm": 1.4796011447906494,
      "learning_rate": 4.043065305621343e-05,
      "loss": 1.1816,
      "step": 10579
    },
    {
      "epoch": 1.487417404751863,
      "grad_norm": 1.470689058303833,
      "learning_rate": 4.0617536099670184e-05,
      "loss": 1.1418,
      "step": 10580
    },
    {
      "epoch": 1.4875579924082665,
      "grad_norm": 1.397346019744873,
      "learning_rate": 4.0804740058065574e-05,
      "loss": 1.093,
      "step": 10581
    },
    {
      "epoch": 1.4876985800646703,
      "grad_norm": 1.4564025402069092,
      "learning_rate": 4.099226391971096e-05,
      "loss": 1.1586,
      "step": 10582
    },
    {
      "epoch": 1.487839167721074,
      "grad_norm": 1.5729215145111084,
      "learning_rate": 4.1180106671189746e-05,
      "loss": 0.9922,
      "step": 10583
    },
    {
      "epoch": 1.4879797553774778,
      "grad_norm": 1.5171951055526733,
      "learning_rate": 4.1368267297361404e-05,
      "loss": 0.9908,
      "step": 10584
    },
    {
      "epoch": 1.4881203430338816,
      "grad_norm": 1.7311362028121948,
      "learning_rate": 4.1556744781367004e-05,
      "loss": 0.8303,
      "step": 10585
    },
    {
      "epoch": 1.4882609306902854,
      "grad_norm": 1.5613759756088257,
      "learning_rate": 4.1745538104636093e-05,
      "loss": 0.9786,
      "step": 10586
    },
    {
      "epoch": 1.4884015183466892,
      "grad_norm": 1.9080883264541626,
      "learning_rate": 4.193464624689105e-05,
      "loss": 0.997,
      "step": 10587
    },
    {
      "epoch": 1.488542106003093,
      "grad_norm": 1.5661650896072388,
      "learning_rate": 4.212406818615265e-05,
      "loss": 1.0972,
      "step": 10588
    },
    {
      "epoch": 1.4886826936594968,
      "grad_norm": 1.4889676570892334,
      "learning_rate": 4.2313802898746714e-05,
      "loss": 1.0065,
      "step": 10589
    },
    {
      "epoch": 1.4888232813159004,
      "grad_norm": 1.7255403995513916,
      "learning_rate": 4.2503849359307865e-05,
      "loss": 1.0407,
      "step": 10590
    },
    {
      "epoch": 1.4889638689723041,
      "grad_norm": 1.5053445100784302,
      "learning_rate": 4.269420654078633e-05,
      "loss": 0.9845,
      "step": 10591
    },
    {
      "epoch": 1.489104456628708,
      "grad_norm": 1.8247337341308594,
      "learning_rate": 4.288487341445336e-05,
      "loss": 1.218,
      "step": 10592
    },
    {
      "epoch": 1.4892450442851117,
      "grad_norm": 1.5757209062576294,
      "learning_rate": 4.307584894990545e-05,
      "loss": 1.0598,
      "step": 10593
    },
    {
      "epoch": 1.4893856319415155,
      "grad_norm": 1.463157296180725,
      "learning_rate": 4.326713211507275e-05,
      "loss": 1.1612,
      "step": 10594
    },
    {
      "epoch": 1.4895262195979193,
      "grad_norm": 1.4016557931900024,
      "learning_rate": 4.3458721876221766e-05,
      "loss": 0.9916,
      "step": 10595
    },
    {
      "epoch": 1.489666807254323,
      "grad_norm": 1.6127749681472778,
      "learning_rate": 4.365061719796235e-05,
      "loss": 1.1553,
      "step": 10596
    },
    {
      "epoch": 1.489807394910727,
      "grad_norm": 1.617604374885559,
      "learning_rate": 4.384281704325311e-05,
      "loss": 1.0986,
      "step": 10597
    },
    {
      "epoch": 1.4899479825671307,
      "grad_norm": 1.7309385538101196,
      "learning_rate": 4.403532037340658e-05,
      "loss": 1.1457,
      "step": 10598
    },
    {
      "epoch": 1.4900885702235342,
      "grad_norm": 1.5888882875442505,
      "learning_rate": 4.422812614809641e-05,
      "loss": 1.1529,
      "step": 10599
    },
    {
      "epoch": 1.4902291578799383,
      "grad_norm": 2.1697933673858643,
      "learning_rate": 4.4421233325360023e-05,
      "loss": 1.0299,
      "step": 10600
    },
    {
      "epoch": 1.4903697455363418,
      "grad_norm": 1.6613805294036865,
      "learning_rate": 4.461464086160717e-05,
      "loss": 0.9409,
      "step": 10601
    },
    {
      "epoch": 1.4905103331927456,
      "grad_norm": 1.4778348207473755,
      "learning_rate": 4.480834771162411e-05,
      "loss": 0.913,
      "step": 10602
    },
    {
      "epoch": 1.4906509208491494,
      "grad_norm": 1.3979027271270752,
      "learning_rate": 4.500235282857952e-05,
      "loss": 0.8956,
      "step": 10603
    },
    {
      "epoch": 1.4907915085055532,
      "grad_norm": 1.3215312957763672,
      "learning_rate": 4.519665516402992e-05,
      "loss": 1.0293,
      "step": 10604
    },
    {
      "epoch": 1.490932096161957,
      "grad_norm": 1.711146593093872,
      "learning_rate": 4.539125366792648e-05,
      "loss": 1.0916,
      "step": 10605
    },
    {
      "epoch": 1.4910726838183608,
      "grad_norm": 1.7500568628311157,
      "learning_rate": 4.558614728861892e-05,
      "loss": 1.0141,
      "step": 10606
    },
    {
      "epoch": 1.4912132714747646,
      "grad_norm": 1.898939609527588,
      "learning_rate": 4.57813349728627e-05,
      "loss": 1.0405,
      "step": 10607
    },
    {
      "epoch": 1.4913538591311684,
      "grad_norm": 1.5058945417404175,
      "learning_rate": 4.597681566582317e-05,
      "loss": 1.0659,
      "step": 10608
    },
    {
      "epoch": 1.4914944467875721,
      "grad_norm": 1.5602099895477295,
      "learning_rate": 4.6172588311082844e-05,
      "loss": 1.1413,
      "step": 10609
    },
    {
      "epoch": 1.4916350344439757,
      "grad_norm": 1.5453628301620483,
      "learning_rate": 4.636865185064703e-05,
      "loss": 1.1525,
      "step": 10610
    },
    {
      "epoch": 1.4917756221003795,
      "grad_norm": 1.735623836517334,
      "learning_rate": 4.656500522494809e-05,
      "loss": 1.0245,
      "step": 10611
    },
    {
      "epoch": 1.4919162097567833,
      "grad_norm": 1.4934223890304565,
      "learning_rate": 4.676164737285239e-05,
      "loss": 1.0769,
      "step": 10612
    },
    {
      "epoch": 1.492056797413187,
      "grad_norm": 1.4694888591766357,
      "learning_rate": 4.695857723166566e-05,
      "loss": 1.1128,
      "step": 10613
    },
    {
      "epoch": 1.4921973850695909,
      "grad_norm": 1.5022019147872925,
      "learning_rate": 4.715579373713912e-05,
      "loss": 1.1252,
      "step": 10614
    },
    {
      "epoch": 1.4923379727259947,
      "grad_norm": 1.4712289571762085,
      "learning_rate": 4.735329582347365e-05,
      "loss": 1.1641,
      "step": 10615
    },
    {
      "epoch": 1.4924785603823985,
      "grad_norm": 1.3689802885055542,
      "learning_rate": 4.755108242332865e-05,
      "loss": 1.0965,
      "step": 10616
    },
    {
      "epoch": 1.4926191480388022,
      "grad_norm": 1.4666149616241455,
      "learning_rate": 4.774915246782472e-05,
      "loss": 1.0567,
      "step": 10617
    },
    {
      "epoch": 1.492759735695206,
      "grad_norm": 1.5235646963119507,
      "learning_rate": 4.794750488655094e-05,
      "loss": 1.0045,
      "step": 10618
    },
    {
      "epoch": 1.4929003233516096,
      "grad_norm": 1.8612687587738037,
      "learning_rate": 4.814613860757043e-05,
      "loss": 1.0811,
      "step": 10619
    },
    {
      "epoch": 1.4930409110080136,
      "grad_norm": 1.4608724117279053,
      "learning_rate": 4.834505255742578e-05,
      "loss": 0.9623,
      "step": 10620
    },
    {
      "epoch": 1.4931814986644172,
      "grad_norm": 1.4396440982818604,
      "learning_rate": 4.8544245661146036e-05,
      "loss": 0.9128,
      "step": 10621
    },
    {
      "epoch": 1.493322086320821,
      "grad_norm": 1.4354327917099,
      "learning_rate": 4.874371684225108e-05,
      "loss": 1.1587,
      "step": 10622
    },
    {
      "epoch": 1.4934626739772248,
      "grad_norm": 1.4658520221710205,
      "learning_rate": 4.8943465022757404e-05,
      "loss": 0.9549,
      "step": 10623
    },
    {
      "epoch": 1.4936032616336286,
      "grad_norm": 1.8986270427703857,
      "learning_rate": 4.9143489123185416e-05,
      "loss": 1.0875,
      "step": 10624
    },
    {
      "epoch": 1.4937438492900323,
      "grad_norm": 1.4446455240249634,
      "learning_rate": 4.934378806256379e-05,
      "loss": 1.1154,
      "step": 10625
    },
    {
      "epoch": 1.4938844369464361,
      "grad_norm": 1.5752959251403809,
      "learning_rate": 4.954436075843685e-05,
      "loss": 0.9192,
      "step": 10626
    },
    {
      "epoch": 1.49402502460284,
      "grad_norm": 1.725961446762085,
      "learning_rate": 4.974520612686857e-05,
      "loss": 1.0378,
      "step": 10627
    },
    {
      "epoch": 1.4941656122592437,
      "grad_norm": 1.4293148517608643,
      "learning_rate": 4.9946323082449634e-05,
      "loss": 1.1971,
      "step": 10628
    },
    {
      "epoch": 1.4943061999156475,
      "grad_norm": 1.3200846910476685,
      "learning_rate": 5.014771053830302e-05,
      "loss": 1.2396,
      "step": 10629
    },
    {
      "epoch": 1.494446787572051,
      "grad_norm": 1.56728994846344,
      "learning_rate": 5.034936740609017e-05,
      "loss": 1.1302,
      "step": 10630
    },
    {
      "epoch": 1.4945873752284549,
      "grad_norm": 1.602130651473999,
      "learning_rate": 5.055129259601537e-05,
      "loss": 1.2927,
      "step": 10631
    },
    {
      "epoch": 1.4947279628848587,
      "grad_norm": 1.8759703636169434,
      "learning_rate": 5.075348501683461e-05,
      "loss": 1.0096,
      "step": 10632
    },
    {
      "epoch": 1.4948685505412624,
      "grad_norm": 1.4517635107040405,
      "learning_rate": 5.095594357585859e-05,
      "loss": 0.898,
      "step": 10633
    },
    {
      "epoch": 1.4950091381976662,
      "grad_norm": 1.4148039817810059,
      "learning_rate": 5.115866717895994e-05,
      "loss": 1.0567,
      "step": 10634
    },
    {
      "epoch": 1.49514972585407,
      "grad_norm": 1.5100675821304321,
      "learning_rate": 5.136165473057901e-05,
      "loss": 1.0318,
      "step": 10635
    },
    {
      "epoch": 1.4952903135104738,
      "grad_norm": 1.635603666305542,
      "learning_rate": 5.1564905133729404e-05,
      "loss": 1.0037,
      "step": 10636
    },
    {
      "epoch": 1.4954309011668776,
      "grad_norm": 2.0758912563323975,
      "learning_rate": 5.176841729000541e-05,
      "loss": 1.2265,
      "step": 10637
    },
    {
      "epoch": 1.4955714888232814,
      "grad_norm": 1.9455875158309937,
      "learning_rate": 5.197219009958494e-05,
      "loss": 0.9039,
      "step": 10638
    },
    {
      "epoch": 1.495712076479685,
      "grad_norm": 1.751761555671692,
      "learning_rate": 5.21762224612385e-05,
      "loss": 1.0413,
      "step": 10639
    },
    {
      "epoch": 1.495852664136089,
      "grad_norm": 1.6620869636535645,
      "learning_rate": 5.238051327233364e-05,
      "loss": 1.0592,
      "step": 10640
    },
    {
      "epoch": 1.4959932517924925,
      "grad_norm": 1.5283517837524414,
      "learning_rate": 5.2585061428840854e-05,
      "loss": 0.8417,
      "step": 10641
    },
    {
      "epoch": 1.4961338394488963,
      "grad_norm": 1.5054832696914673,
      "learning_rate": 5.2789865825340825e-05,
      "loss": 1.0554,
      "step": 10642
    },
    {
      "epoch": 1.4962744271053001,
      "grad_norm": 1.581929087638855,
      "learning_rate": 5.2994925355028545e-05,
      "loss": 1.1449,
      "step": 10643
    },
    {
      "epoch": 1.496415014761704,
      "grad_norm": 1.7090556621551514,
      "learning_rate": 5.320023890972054e-05,
      "loss": 0.8851,
      "step": 10644
    },
    {
      "epoch": 1.4965556024181077,
      "grad_norm": 1.8747646808624268,
      "learning_rate": 5.34058053798608e-05,
      "loss": 0.9836,
      "step": 10645
    },
    {
      "epoch": 1.4966961900745115,
      "grad_norm": 1.601007342338562,
      "learning_rate": 5.361162365452559e-05,
      "loss": 1.2446,
      "step": 10646
    },
    {
      "epoch": 1.4968367777309153,
      "grad_norm": 1.5503389835357666,
      "learning_rate": 5.381769262143102e-05,
      "loss": 1.1227,
      "step": 10647
    },
    {
      "epoch": 1.496977365387319,
      "grad_norm": 1.4235795736312866,
      "learning_rate": 5.4024011166939006e-05,
      "loss": 1.175,
      "step": 10648
    },
    {
      "epoch": 1.4971179530437229,
      "grad_norm": 1.5877149105072021,
      "learning_rate": 5.423057817606176e-05,
      "loss": 1.1709,
      "step": 10649
    },
    {
      "epoch": 1.4972585407001264,
      "grad_norm": 1.8496034145355225,
      "learning_rate": 5.443739253246908e-05,
      "loss": 1.2413,
      "step": 10650
    },
    {
      "epoch": 1.4973991283565302,
      "grad_norm": 1.5725327730178833,
      "learning_rate": 5.464445311849401e-05,
      "loss": 0.9421,
      "step": 10651
    },
    {
      "epoch": 1.497539716012934,
      "grad_norm": 1.6002109050750732,
      "learning_rate": 5.485175881513895e-05,
      "loss": 1.2365,
      "step": 10652
    },
    {
      "epoch": 1.4976803036693378,
      "grad_norm": 1.6009202003479004,
      "learning_rate": 5.505930850208165e-05,
      "loss": 1.0848,
      "step": 10653
    },
    {
      "epoch": 1.4978208913257416,
      "grad_norm": 1.4896212816238403,
      "learning_rate": 5.526710105768131e-05,
      "loss": 1.2231,
      "step": 10654
    },
    {
      "epoch": 1.4979614789821454,
      "grad_norm": 1.6030954122543335,
      "learning_rate": 5.54751353589846e-05,
      "loss": 1.1655,
      "step": 10655
    },
    {
      "epoch": 1.4981020666385492,
      "grad_norm": 1.4445242881774902,
      "learning_rate": 5.5683410281731754e-05,
      "loss": 1.1842,
      "step": 10656
    },
    {
      "epoch": 1.498242654294953,
      "grad_norm": 1.3312560319900513,
      "learning_rate": 5.589192470036263e-05,
      "loss": 1.0741,
      "step": 10657
    },
    {
      "epoch": 1.4983832419513567,
      "grad_norm": 1.364254355430603,
      "learning_rate": 5.6100677488022505e-05,
      "loss": 1.0183,
      "step": 10658
    },
    {
      "epoch": 1.4985238296077603,
      "grad_norm": 1.517311453819275,
      "learning_rate": 5.630966751656939e-05,
      "loss": 1.038,
      "step": 10659
    },
    {
      "epoch": 1.4986644172641643,
      "grad_norm": 1.5599550008773804,
      "learning_rate": 5.6518893656578564e-05,
      "loss": 1.0358,
      "step": 10660
    },
    {
      "epoch": 1.498805004920568,
      "grad_norm": 1.3307210206985474,
      "learning_rate": 5.672835477734871e-05,
      "loss": 1.1466,
      "step": 10661
    },
    {
      "epoch": 1.4989455925769717,
      "grad_norm": 1.5232433080673218,
      "learning_rate": 5.693804974690955e-05,
      "loss": 0.944,
      "step": 10662
    },
    {
      "epoch": 1.4990861802333755,
      "grad_norm": 1.747713327407837,
      "learning_rate": 5.7147977432026376e-05,
      "loss": 1.0878,
      "step": 10663
    },
    {
      "epoch": 1.4992267678897793,
      "grad_norm": 1.7337857484817505,
      "learning_rate": 5.735813669820788e-05,
      "loss": 1.0896,
      "step": 10664
    },
    {
      "epoch": 1.499367355546183,
      "grad_norm": 1.4363701343536377,
      "learning_rate": 5.756852640971019e-05,
      "loss": 0.9847,
      "step": 10665
    },
    {
      "epoch": 1.4995079432025868,
      "grad_norm": 1.5787566900253296,
      "learning_rate": 5.7779145429544435e-05,
      "loss": 1.037,
      "step": 10666
    },
    {
      "epoch": 1.4996485308589906,
      "grad_norm": 1.4708073139190674,
      "learning_rate": 5.79899926194825e-05,
      "loss": 1.3997,
      "step": 10667
    },
    {
      "epoch": 1.4997891185153944,
      "grad_norm": 1.621916651725769,
      "learning_rate": 5.820106684006321e-05,
      "loss": 0.9831,
      "step": 10668
    },
    {
      "epoch": 1.4999297061717982,
      "grad_norm": 1.527570366859436,
      "learning_rate": 5.841236695059844e-05,
      "loss": 1.0892,
      "step": 10669
    },
    {
      "epoch": 1.5000702938282018,
      "grad_norm": 1.5823347568511963,
      "learning_rate": 5.862389180917931e-05,
      "loss": 1.1792,
      "step": 10670
    },
    {
      "epoch": 1.5002108814846058,
      "grad_norm": 1.5079998970031738,
      "learning_rate": 5.88356402726824e-05,
      "loss": 1.0281,
      "step": 10671
    },
    {
      "epoch": 1.5003514691410094,
      "grad_norm": 1.4621288776397705,
      "learning_rate": 5.904761119677584e-05,
      "loss": 1.0498,
      "step": 10672
    },
    {
      "epoch": 1.5004920567974132,
      "grad_norm": 1.669344425201416,
      "learning_rate": 5.925980343592554e-05,
      "loss": 1.2577,
      "step": 10673
    },
    {
      "epoch": 1.500632644453817,
      "grad_norm": 1.5574697256088257,
      "learning_rate": 5.9472215843401104e-05,
      "loss": 1.0078,
      "step": 10674
    },
    {
      "epoch": 1.5007732321102207,
      "grad_norm": 1.6526941061019897,
      "learning_rate": 5.9684847271283494e-05,
      "loss": 1.0056,
      "step": 10675
    },
    {
      "epoch": 1.5009138197666245,
      "grad_norm": 1.492713451385498,
      "learning_rate": 5.989769657046822e-05,
      "loss": 1.0169,
      "step": 10676
    },
    {
      "epoch": 1.501054407423028,
      "grad_norm": 1.367316484451294,
      "learning_rate": 6.011076259067459e-05,
      "loss": 1.04,
      "step": 10677
    },
    {
      "epoch": 1.501194995079432,
      "grad_norm": 1.6412028074264526,
      "learning_rate": 6.032404418045039e-05,
      "loss": 1.2196,
      "step": 10678
    },
    {
      "epoch": 1.5013355827358357,
      "grad_norm": 1.4865831136703491,
      "learning_rate": 6.0537540187178124e-05,
      "loss": 1.2014,
      "step": 10679
    },
    {
      "epoch": 1.5014761703922397,
      "grad_norm": 1.5957878828048706,
      "learning_rate": 6.0751249457082495e-05,
      "loss": 0.9853,
      "step": 10680
    },
    {
      "epoch": 1.5016167580486433,
      "grad_norm": 1.5139509439468384,
      "learning_rate": 6.096517083523474e-05,
      "loss": 1.1494,
      "step": 10681
    },
    {
      "epoch": 1.501757345705047,
      "grad_norm": 1.6759827136993408,
      "learning_rate": 6.117930316556011e-05,
      "loss": 1.1261,
      "step": 10682
    },
    {
      "epoch": 1.5018979333614508,
      "grad_norm": 1.7463959455490112,
      "learning_rate": 6.13936452908442e-05,
      "loss": 1.2173,
      "step": 10683
    },
    {
      "epoch": 1.5020385210178546,
      "grad_norm": 1.652694582939148,
      "learning_rate": 6.160819605273779e-05,
      "loss": 1.2152,
      "step": 10684
    },
    {
      "epoch": 1.5021791086742584,
      "grad_norm": 1.469763994216919,
      "learning_rate": 6.182295429176485e-05,
      "loss": 0.839,
      "step": 10685
    },
    {
      "epoch": 1.5023196963306622,
      "grad_norm": 1.3641668558120728,
      "learning_rate": 6.203791884732874e-05,
      "loss": 1.0375,
      "step": 10686
    },
    {
      "epoch": 1.502460283987066,
      "grad_norm": 1.7228209972381592,
      "learning_rate": 6.225308855771684e-05,
      "loss": 1.1352,
      "step": 10687
    },
    {
      "epoch": 1.5026008716434696,
      "grad_norm": 1.4861639738082886,
      "learning_rate": 6.24684622601082e-05,
      "loss": 1.2029,
      "step": 10688
    },
    {
      "epoch": 1.5027414592998736,
      "grad_norm": 1.4266462326049805,
      "learning_rate": 6.268403879057943e-05,
      "loss": 1.0787,
      "step": 10689
    },
    {
      "epoch": 1.5028820469562771,
      "grad_norm": 1.4258784055709839,
      "learning_rate": 6.2899816984111e-05,
      "loss": 0.8726,
      "step": 10690
    },
    {
      "epoch": 1.5030226346126812,
      "grad_norm": 1.507703423500061,
      "learning_rate": 6.31157956745936e-05,
      "loss": 0.9141,
      "step": 10691
    },
    {
      "epoch": 1.5031632222690847,
      "grad_norm": 1.8441699743270874,
      "learning_rate": 6.333197369483435e-05,
      "loss": 1.2724,
      "step": 10692
    },
    {
      "epoch": 1.5033038099254885,
      "grad_norm": 1.6330543756484985,
      "learning_rate": 6.354834987656316e-05,
      "loss": 0.9781,
      "step": 10693
    },
    {
      "epoch": 1.5034443975818923,
      "grad_norm": 1.5072332620620728,
      "learning_rate": 6.376492305043904e-05,
      "loss": 1.0628,
      "step": 10694
    },
    {
      "epoch": 1.503584985238296,
      "grad_norm": 1.4147489070892334,
      "learning_rate": 6.398169204605609e-05,
      "loss": 1.0045,
      "step": 10695
    },
    {
      "epoch": 1.5037255728946999,
      "grad_norm": 1.2818764448165894,
      "learning_rate": 6.419865569195113e-05,
      "loss": 1.1969,
      "step": 10696
    },
    {
      "epoch": 1.5038661605511034,
      "grad_norm": 1.2626632452011108,
      "learning_rate": 6.441581281560807e-05,
      "loss": 1.2204,
      "step": 10697
    },
    {
      "epoch": 1.5040067482075075,
      "grad_norm": 1.766434669494629,
      "learning_rate": 6.46331622434659e-05,
      "loss": 1.0076,
      "step": 10698
    },
    {
      "epoch": 1.504147335863911,
      "grad_norm": 1.4835904836654663,
      "learning_rate": 6.485070280092331e-05,
      "loss": 1.1547,
      "step": 10699
    },
    {
      "epoch": 1.504287923520315,
      "grad_norm": 1.4861149787902832,
      "learning_rate": 6.506843331234715e-05,
      "loss": 0.9989,
      "step": 10700
    },
    {
      "epoch": 1.5044285111767186,
      "grad_norm": 1.4591089487075806,
      "learning_rate": 6.528635260107702e-05,
      "loss": 0.9824,
      "step": 10701
    },
    {
      "epoch": 1.5045690988331224,
      "grad_norm": 1.5793836116790771,
      "learning_rate": 6.55044594894333e-05,
      "loss": 0.9195,
      "step": 10702
    },
    {
      "epoch": 1.5047096864895262,
      "grad_norm": 1.8031669855117798,
      "learning_rate": 6.572275279872163e-05,
      "loss": 0.9666,
      "step": 10703
    },
    {
      "epoch": 1.50485027414593,
      "grad_norm": 1.5744909048080444,
      "learning_rate": 6.594123134924044e-05,
      "loss": 1.0829,
      "step": 10704
    },
    {
      "epoch": 1.5049908618023338,
      "grad_norm": 1.4387176036834717,
      "learning_rate": 6.61598939602871e-05,
      "loss": 1.0226,
      "step": 10705
    },
    {
      "epoch": 1.5051314494587376,
      "grad_norm": 1.5602360963821411,
      "learning_rate": 6.63787394501643e-05,
      "loss": 1.1352,
      "step": 10706
    },
    {
      "epoch": 1.5052720371151413,
      "grad_norm": 1.360698938369751,
      "learning_rate": 6.659776663618642e-05,
      "loss": 1.0444,
      "step": 10707
    },
    {
      "epoch": 1.505412624771545,
      "grad_norm": 1.4791622161865234,
      "learning_rate": 6.68169743346859e-05,
      "loss": 1.0783,
      "step": 10708
    },
    {
      "epoch": 1.505553212427949,
      "grad_norm": 1.5830293893814087,
      "learning_rate": 6.703636136101965e-05,
      "loss": 1.2441,
      "step": 10709
    },
    {
      "epoch": 1.5056938000843525,
      "grad_norm": 1.5696672201156616,
      "learning_rate": 6.725592652957547e-05,
      "loss": 1.0718,
      "step": 10710
    },
    {
      "epoch": 1.5058343877407565,
      "grad_norm": 1.7159664630889893,
      "learning_rate": 6.747566865377809e-05,
      "loss": 1.0523,
      "step": 10711
    },
    {
      "epoch": 1.50597497539716,
      "grad_norm": 1.6180130243301392,
      "learning_rate": 6.769558654609695e-05,
      "loss": 1.1956,
      "step": 10712
    },
    {
      "epoch": 1.5061155630535639,
      "grad_norm": 1.4515661001205444,
      "learning_rate": 6.791567901805094e-05,
      "loss": 0.9544,
      "step": 10713
    },
    {
      "epoch": 1.5062561507099677,
      "grad_norm": 1.461544394493103,
      "learning_rate": 6.813594488021483e-05,
      "loss": 1.0012,
      "step": 10714
    },
    {
      "epoch": 1.5063967383663714,
      "grad_norm": 2.0159945487976074,
      "learning_rate": 6.83563829422273e-05,
      "loss": 1.0794,
      "step": 10715
    },
    {
      "epoch": 1.5065373260227752,
      "grad_norm": 1.4778703451156616,
      "learning_rate": 6.857699201279617e-05,
      "loss": 0.8464,
      "step": 10716
    },
    {
      "epoch": 1.5066779136791788,
      "grad_norm": 1.650404453277588,
      "learning_rate": 6.879777089970471e-05,
      "loss": 1.1912,
      "step": 10717
    },
    {
      "epoch": 1.5068185013355828,
      "grad_norm": 1.6238603591918945,
      "learning_rate": 6.901871840981948e-05,
      "loss": 0.993,
      "step": 10718
    },
    {
      "epoch": 1.5069590889919864,
      "grad_norm": 1.842543363571167,
      "learning_rate": 6.923983334909475e-05,
      "loss": 0.8889,
      "step": 10719
    },
    {
      "epoch": 1.5070996766483904,
      "grad_norm": 1.4306719303131104,
      "learning_rate": 6.946111452258035e-05,
      "loss": 0.9928,
      "step": 10720
    },
    {
      "epoch": 1.507240264304794,
      "grad_norm": 1.4941471815109253,
      "learning_rate": 6.968256073442803e-05,
      "loss": 1.0382,
      "step": 10721
    },
    {
      "epoch": 1.5073808519611978,
      "grad_norm": 1.636344313621521,
      "learning_rate": 6.990417078789633e-05,
      "loss": 0.9357,
      "step": 10722
    },
    {
      "epoch": 1.5075214396176015,
      "grad_norm": 1.4293397665023804,
      "learning_rate": 7.012594348536035e-05,
      "loss": 1.0594,
      "step": 10723
    },
    {
      "epoch": 1.5076620272740053,
      "grad_norm": 1.458123803138733,
      "learning_rate": 7.034787762831489e-05,
      "loss": 1.0445,
      "step": 10724
    },
    {
      "epoch": 1.5078026149304091,
      "grad_norm": 1.4574735164642334,
      "learning_rate": 7.056997201738262e-05,
      "loss": 1.1388,
      "step": 10725
    },
    {
      "epoch": 1.507943202586813,
      "grad_norm": 1.5945658683776855,
      "learning_rate": 7.079222545232018e-05,
      "loss": 0.9544,
      "step": 10726
    },
    {
      "epoch": 1.5080837902432167,
      "grad_norm": 1.4412202835083008,
      "learning_rate": 7.101463673202473e-05,
      "loss": 1.0698,
      "step": 10727
    },
    {
      "epoch": 1.5082243778996203,
      "grad_norm": 1.4603132009506226,
      "learning_rate": 7.123720465454034e-05,
      "loss": 1.0481,
      "step": 10728
    },
    {
      "epoch": 1.5083649655560243,
      "grad_norm": 1.7970402240753174,
      "learning_rate": 7.145992801706462e-05,
      "loss": 0.994,
      "step": 10729
    },
    {
      "epoch": 1.5085055532124279,
      "grad_norm": 1.3839410543441772,
      "learning_rate": 7.16828056159551e-05,
      "loss": 1.0493,
      "step": 10730
    },
    {
      "epoch": 1.5086461408688319,
      "grad_norm": 1.6928962469100952,
      "learning_rate": 7.190583624673586e-05,
      "loss": 1.0056,
      "step": 10731
    },
    {
      "epoch": 1.5087867285252354,
      "grad_norm": 1.4801462888717651,
      "learning_rate": 7.212901870410384e-05,
      "loss": 0.9927,
      "step": 10732
    },
    {
      "epoch": 1.5089273161816392,
      "grad_norm": 1.4996318817138672,
      "learning_rate": 7.235235178193526e-05,
      "loss": 1.0605,
      "step": 10733
    },
    {
      "epoch": 1.509067903838043,
      "grad_norm": 1.4032033681869507,
      "learning_rate": 7.257583427329331e-05,
      "loss": 0.9894,
      "step": 10734
    },
    {
      "epoch": 1.5092084914944468,
      "grad_norm": 1.478448510169983,
      "learning_rate": 7.279946497043271e-05,
      "loss": 0.9906,
      "step": 10735
    },
    {
      "epoch": 1.5093490791508506,
      "grad_norm": 1.5736150741577148,
      "learning_rate": 7.302324266480796e-05,
      "loss": 1.1514,
      "step": 10736
    },
    {
      "epoch": 1.5094896668072542,
      "grad_norm": 1.5261647701263428,
      "learning_rate": 7.324716614707805e-05,
      "loss": 1.074,
      "step": 10737
    },
    {
      "epoch": 1.5096302544636582,
      "grad_norm": 1.3422458171844482,
      "learning_rate": 7.34712342071149e-05,
      "loss": 0.9894,
      "step": 10738
    },
    {
      "epoch": 1.5097708421200617,
      "grad_norm": 1.5099530220031738,
      "learning_rate": 7.369544563400968e-05,
      "loss": 0.9613,
      "step": 10739
    },
    {
      "epoch": 1.5099114297764658,
      "grad_norm": 1.6723047494888306,
      "learning_rate": 7.391979921607783e-05,
      "loss": 1.0974,
      "step": 10740
    },
    {
      "epoch": 1.5100520174328693,
      "grad_norm": 1.6010929346084595,
      "learning_rate": 7.414429374086685e-05,
      "loss": 1.265,
      "step": 10741
    },
    {
      "epoch": 1.510192605089273,
      "grad_norm": 1.5540852546691895,
      "learning_rate": 7.436892799516256e-05,
      "loss": 1.1394,
      "step": 10742
    },
    {
      "epoch": 1.510333192745677,
      "grad_norm": 1.6969796419143677,
      "learning_rate": 7.459370076499568e-05,
      "loss": 1.1918,
      "step": 10743
    },
    {
      "epoch": 1.5104737804020807,
      "grad_norm": 1.4620122909545898,
      "learning_rate": 7.481861083564834e-05,
      "loss": 0.9529,
      "step": 10744
    },
    {
      "epoch": 1.5106143680584845,
      "grad_norm": 1.5671056509017944,
      "learning_rate": 7.504365699166067e-05,
      "loss": 1.1416,
      "step": 10745
    },
    {
      "epoch": 1.5107549557148883,
      "grad_norm": 1.3517292737960815,
      "learning_rate": 7.526883801683739e-05,
      "loss": 1.0299,
      "step": 10746
    },
    {
      "epoch": 1.510895543371292,
      "grad_norm": 1.9229778051376343,
      "learning_rate": 7.549415269425431e-05,
      "loss": 1.1545,
      "step": 10747
    },
    {
      "epoch": 1.5110361310276956,
      "grad_norm": 1.317956805229187,
      "learning_rate": 7.571959980626502e-05,
      "loss": 1.0613,
      "step": 10748
    },
    {
      "epoch": 1.5111767186840996,
      "grad_norm": 1.5624722242355347,
      "learning_rate": 7.594517813450703e-05,
      "loss": 1.1254,
      "step": 10749
    },
    {
      "epoch": 1.5113173063405032,
      "grad_norm": 1.8038361072540283,
      "learning_rate": 7.617088645990975e-05,
      "loss": 0.8996,
      "step": 10750
    },
    {
      "epoch": 1.5114578939969072,
      "grad_norm": 1.764878511428833,
      "learning_rate": 7.639672356269939e-05,
      "loss": 0.9384,
      "step": 10751
    },
    {
      "epoch": 1.5115984816533108,
      "grad_norm": 1.1592507362365723,
      "learning_rate": 7.66226882224055e-05,
      "loss": 1.0646,
      "step": 10752
    },
    {
      "epoch": 1.5117390693097146,
      "grad_norm": 1.4682551622390747,
      "learning_rate": 7.684877921786938e-05,
      "loss": 1.0735,
      "step": 10753
    },
    {
      "epoch": 1.5118796569661184,
      "grad_norm": 1.6034331321716309,
      "learning_rate": 7.707499532724886e-05,
      "loss": 1.133,
      "step": 10754
    },
    {
      "epoch": 1.5120202446225222,
      "grad_norm": 1.652776837348938,
      "learning_rate": 7.730133532802666e-05,
      "loss": 1.0435,
      "step": 10755
    },
    {
      "epoch": 1.512160832278926,
      "grad_norm": 1.6399041414260864,
      "learning_rate": 7.752779799701492e-05,
      "loss": 1.0091,
      "step": 10756
    },
    {
      "epoch": 1.5123014199353295,
      "grad_norm": 1.6833751201629639,
      "learning_rate": 7.775438211036324e-05,
      "loss": 1.0878,
      "step": 10757
    },
    {
      "epoch": 1.5124420075917335,
      "grad_norm": 1.7666616439819336,
      "learning_rate": 7.798108644356488e-05,
      "loss": 1.1846,
      "step": 10758
    },
    {
      "epoch": 1.512582595248137,
      "grad_norm": 1.5387054681777954,
      "learning_rate": 7.820790977146376e-05,
      "loss": 1.1048,
      "step": 10759
    },
    {
      "epoch": 1.512723182904541,
      "grad_norm": 1.4533710479736328,
      "learning_rate": 7.843485086825934e-05,
      "loss": 0.9672,
      "step": 10760
    },
    {
      "epoch": 1.5128637705609447,
      "grad_norm": 1.5548895597457886,
      "learning_rate": 7.866190850751673e-05,
      "loss": 1.2278,
      "step": 10761
    },
    {
      "epoch": 1.5130043582173485,
      "grad_norm": 1.696678638458252,
      "learning_rate": 7.888908146216981e-05,
      "loss": 1.2478,
      "step": 10762
    },
    {
      "epoch": 1.5131449458737523,
      "grad_norm": 1.514961838722229,
      "learning_rate": 7.911636850452963e-05,
      "loss": 1.0956,
      "step": 10763
    },
    {
      "epoch": 1.513285533530156,
      "grad_norm": 1.7779998779296875,
      "learning_rate": 7.934376840629068e-05,
      "loss": 1.0202,
      "step": 10764
    },
    {
      "epoch": 1.5134261211865598,
      "grad_norm": 1.6405259370803833,
      "learning_rate": 7.957127993853721e-05,
      "loss": 1.0579,
      "step": 10765
    },
    {
      "epoch": 1.5135667088429636,
      "grad_norm": 1.9109050035476685,
      "learning_rate": 7.979890187175157e-05,
      "loss": 0.882,
      "step": 10766
    },
    {
      "epoch": 1.5137072964993674,
      "grad_norm": 1.7142951488494873,
      "learning_rate": 8.002663297581736e-05,
      "loss": 1.1658,
      "step": 10767
    },
    {
      "epoch": 1.513847884155771,
      "grad_norm": 1.6061015129089355,
      "learning_rate": 8.025447202002966e-05,
      "loss": 0.9208,
      "step": 10768
    },
    {
      "epoch": 1.513988471812175,
      "grad_norm": 1.5086523294448853,
      "learning_rate": 8.048241777309983e-05,
      "loss": 0.9865,
      "step": 10769
    },
    {
      "epoch": 1.5141290594685786,
      "grad_norm": 1.92876398563385,
      "learning_rate": 8.071046900316254e-05,
      "loss": 0.9843,
      "step": 10770
    },
    {
      "epoch": 1.5142696471249826,
      "grad_norm": 1.4463984966278076,
      "learning_rate": 8.093862447778214e-05,
      "loss": 0.9459,
      "step": 10771
    },
    {
      "epoch": 1.5144102347813861,
      "grad_norm": 1.548923373222351,
      "learning_rate": 8.11668829639606e-05,
      "loss": 1.0797,
      "step": 10772
    },
    {
      "epoch": 1.51455082243779,
      "grad_norm": 1.6774462461471558,
      "learning_rate": 8.139524322814222e-05,
      "loss": 1.0678,
      "step": 10773
    },
    {
      "epoch": 1.5146914100941937,
      "grad_norm": 1.4740864038467407,
      "learning_rate": 8.162370403622193e-05,
      "loss": 0.951,
      "step": 10774
    },
    {
      "epoch": 1.5148319977505975,
      "grad_norm": 1.7456861734390259,
      "learning_rate": 8.185226415355024e-05,
      "loss": 0.8393,
      "step": 10775
    },
    {
      "epoch": 1.5149725854070013,
      "grad_norm": 1.6878995895385742,
      "learning_rate": 8.208092234494172e-05,
      "loss": 1.1525,
      "step": 10776
    },
    {
      "epoch": 1.5151131730634049,
      "grad_norm": 1.5170305967330933,
      "learning_rate": 8.230967737468156e-05,
      "loss": 1.052,
      "step": 10777
    },
    {
      "epoch": 1.5152537607198089,
      "grad_norm": 1.6391876935958862,
      "learning_rate": 8.253852800653063e-05,
      "loss": 1.0597,
      "step": 10778
    },
    {
      "epoch": 1.5153943483762125,
      "grad_norm": 1.4168461561203003,
      "learning_rate": 8.276747300373346e-05,
      "loss": 1.0575,
      "step": 10779
    },
    {
      "epoch": 1.5155349360326165,
      "grad_norm": 2.1287195682525635,
      "learning_rate": 8.299651112902462e-05,
      "loss": 0.9405,
      "step": 10780
    },
    {
      "epoch": 1.51567552368902,
      "grad_norm": 1.865790605545044,
      "learning_rate": 8.322564114463505e-05,
      "loss": 0.9243,
      "step": 10781
    },
    {
      "epoch": 1.5158161113454238,
      "grad_norm": 1.472880482673645,
      "learning_rate": 8.345486181230051e-05,
      "loss": 1.0982,
      "step": 10782
    },
    {
      "epoch": 1.5159566990018276,
      "grad_norm": 1.4737783670425415,
      "learning_rate": 8.368417189326471e-05,
      "loss": 1.1482,
      "step": 10783
    },
    {
      "epoch": 1.5160972866582314,
      "grad_norm": 1.3995740413665771,
      "learning_rate": 8.391357014828958e-05,
      "loss": 1.1825,
      "step": 10784
    },
    {
      "epoch": 1.5162378743146352,
      "grad_norm": 1.528324007987976,
      "learning_rate": 8.414305533766023e-05,
      "loss": 0.9762,
      "step": 10785
    },
    {
      "epoch": 1.516378461971039,
      "grad_norm": 1.5327192544937134,
      "learning_rate": 8.437262622119188e-05,
      "loss": 1.0444,
      "step": 10786
    },
    {
      "epoch": 1.5165190496274428,
      "grad_norm": 1.6026651859283447,
      "learning_rate": 8.460228155823636e-05,
      "loss": 1.0621,
      "step": 10787
    },
    {
      "epoch": 1.5166596372838463,
      "grad_norm": 1.6572128534317017,
      "learning_rate": 8.483202010769011e-05,
      "loss": 1.1396,
      "step": 10788
    },
    {
      "epoch": 1.5168002249402504,
      "grad_norm": 1.6624387502670288,
      "learning_rate": 8.50618406279992e-05,
      "loss": 1.0789,
      "step": 10789
    },
    {
      "epoch": 1.516940812596654,
      "grad_norm": 1.4770822525024414,
      "learning_rate": 8.529174187716595e-05,
      "loss": 0.8917,
      "step": 10790
    },
    {
      "epoch": 1.517081400253058,
      "grad_norm": 1.3175373077392578,
      "learning_rate": 8.552172261275754e-05,
      "loss": 1.2311,
      "step": 10791
    },
    {
      "epoch": 1.5172219879094615,
      "grad_norm": 1.4564132690429688,
      "learning_rate": 8.575178159191083e-05,
      "loss": 0.9609,
      "step": 10792
    },
    {
      "epoch": 1.5173625755658653,
      "grad_norm": 1.6009472608566284,
      "learning_rate": 8.598191757134092e-05,
      "loss": 1.1652,
      "step": 10793
    },
    {
      "epoch": 1.517503163222269,
      "grad_norm": 1.782965064048767,
      "learning_rate": 8.62121293073457e-05,
      "loss": 1.1004,
      "step": 10794
    },
    {
      "epoch": 1.5176437508786729,
      "grad_norm": 1.4535560607910156,
      "learning_rate": 8.644241555581404e-05,
      "loss": 1.0794,
      "step": 10795
    },
    {
      "epoch": 1.5177843385350767,
      "grad_norm": 1.782856822013855,
      "learning_rate": 8.66727750722321e-05,
      "loss": 1.0979,
      "step": 10796
    },
    {
      "epoch": 1.5179249261914802,
      "grad_norm": 1.4223142862319946,
      "learning_rate": 8.690320661169048e-05,
      "loss": 1.028,
      "step": 10797
    },
    {
      "epoch": 1.5180655138478842,
      "grad_norm": 1.487168312072754,
      "learning_rate": 8.71337089288891e-05,
      "loss": 1.1882,
      "step": 10798
    },
    {
      "epoch": 1.5182061015042878,
      "grad_norm": 1.562492847442627,
      "learning_rate": 8.736428077814749e-05,
      "loss": 1.0149,
      "step": 10799
    },
    {
      "epoch": 1.5183466891606918,
      "grad_norm": 1.4701591730117798,
      "learning_rate": 8.759492091340806e-05,
      "loss": 0.9485,
      "step": 10800
    },
    {
      "epoch": 1.5184872768170954,
      "grad_norm": 1.4391694068908691,
      "learning_rate": 8.78256280882445e-05,
      "loss": 1.0698,
      "step": 10801
    },
    {
      "epoch": 1.5186278644734992,
      "grad_norm": 1.4072544574737549,
      "learning_rate": 8.80564010558682e-05,
      "loss": 0.9955,
      "step": 10802
    },
    {
      "epoch": 1.518768452129903,
      "grad_norm": 1.9778896570205688,
      "learning_rate": 8.828723856913465e-05,
      "loss": 1.0945,
      "step": 10803
    },
    {
      "epoch": 1.5189090397863068,
      "grad_norm": 1.6737735271453857,
      "learning_rate": 8.851813938055162e-05,
      "loss": 1.0504,
      "step": 10804
    },
    {
      "epoch": 1.5190496274427105,
      "grad_norm": 1.5132615566253662,
      "learning_rate": 8.874910224228403e-05,
      "loss": 1.0162,
      "step": 10805
    },
    {
      "epoch": 1.5191902150991143,
      "grad_norm": 1.4927513599395752,
      "learning_rate": 8.898012590616079e-05,
      "loss": 1.1734,
      "step": 10806
    },
    {
      "epoch": 1.5193308027555181,
      "grad_norm": 1.6427561044692993,
      "learning_rate": 8.92112091236833e-05,
      "loss": 0.939,
      "step": 10807
    },
    {
      "epoch": 1.5194713904119217,
      "grad_norm": 1.5180801153182983,
      "learning_rate": 8.94423506460304e-05,
      "loss": 0.9616,
      "step": 10808
    },
    {
      "epoch": 1.5196119780683257,
      "grad_norm": 1.5121294260025024,
      "learning_rate": 8.96735492240669e-05,
      "loss": 1.0866,
      "step": 10809
    },
    {
      "epoch": 1.5197525657247293,
      "grad_norm": 1.6166971921920776,
      "learning_rate": 8.990480360834816e-05,
      "loss": 1.0328,
      "step": 10810
    },
    {
      "epoch": 1.5198931533811333,
      "grad_norm": 1.6289936304092407,
      "learning_rate": 9.013611254912842e-05,
      "loss": 1.1592,
      "step": 10811
    },
    {
      "epoch": 1.5200337410375369,
      "grad_norm": 1.7743773460388184,
      "learning_rate": 9.036747479636733e-05,
      "loss": 1.063,
      "step": 10812
    },
    {
      "epoch": 1.5201743286939406,
      "grad_norm": 1.6667941808700562,
      "learning_rate": 9.059888909973543e-05,
      "loss": 1.1428,
      "step": 10813
    },
    {
      "epoch": 1.5203149163503444,
      "grad_norm": 1.663644552230835,
      "learning_rate": 9.08303542086227e-05,
      "loss": 1.0738,
      "step": 10814
    },
    {
      "epoch": 1.5204555040067482,
      "grad_norm": 1.801830768585205,
      "learning_rate": 9.10618688721452e-05,
      "loss": 1.268,
      "step": 10815
    },
    {
      "epoch": 1.520596091663152,
      "grad_norm": 1.4675630331039429,
      "learning_rate": 9.129343183915013e-05,
      "loss": 1.0655,
      "step": 10816
    },
    {
      "epoch": 1.5207366793195556,
      "grad_norm": 1.3302571773529053,
      "learning_rate": 9.152504185822403e-05,
      "loss": 1.1084,
      "step": 10817
    },
    {
      "epoch": 1.5208772669759596,
      "grad_norm": 1.5470603704452515,
      "learning_rate": 9.175669767769915e-05,
      "loss": 1.0508,
      "step": 10818
    },
    {
      "epoch": 1.5210178546323632,
      "grad_norm": 1.3291113376617432,
      "learning_rate": 9.198839804565986e-05,
      "loss": 1.048,
      "step": 10819
    },
    {
      "epoch": 1.5211584422887672,
      "grad_norm": 1.4509037733078003,
      "learning_rate": 9.222014170995122e-05,
      "loss": 1.065,
      "step": 10820
    },
    {
      "epoch": 1.5212990299451707,
      "grad_norm": 1.6026532649993896,
      "learning_rate": 9.245192741818217e-05,
      "loss": 0.9438,
      "step": 10821
    },
    {
      "epoch": 1.5214396176015745,
      "grad_norm": 1.3763145208358765,
      "learning_rate": 9.268375391773585e-05,
      "loss": 1.051,
      "step": 10822
    },
    {
      "epoch": 1.5215802052579783,
      "grad_norm": 1.591196894645691,
      "learning_rate": 9.29156199557746e-05,
      "loss": 1.1182,
      "step": 10823
    },
    {
      "epoch": 1.5217207929143821,
      "grad_norm": 1.5807808637619019,
      "learning_rate": 9.314752427924673e-05,
      "loss": 1.0195,
      "step": 10824
    },
    {
      "epoch": 1.521861380570786,
      "grad_norm": 1.6047322750091553,
      "learning_rate": 9.337946563489474e-05,
      "loss": 1.2647,
      "step": 10825
    },
    {
      "epoch": 1.5220019682271897,
      "grad_norm": 1.7449835538864136,
      "learning_rate": 9.361144276925988e-05,
      "loss": 1.2265,
      "step": 10826
    },
    {
      "epoch": 1.5221425558835935,
      "grad_norm": 1.853641390800476,
      "learning_rate": 9.384345442869082e-05,
      "loss": 1.0944,
      "step": 10827
    },
    {
      "epoch": 1.522283143539997,
      "grad_norm": 1.4762400388717651,
      "learning_rate": 9.407549935934855e-05,
      "loss": 1.0854,
      "step": 10828
    },
    {
      "epoch": 1.522423731196401,
      "grad_norm": 1.4519394636154175,
      "learning_rate": 9.430757630721529e-05,
      "loss": 1.2536,
      "step": 10829
    },
    {
      "epoch": 1.5225643188528046,
      "grad_norm": 1.496123194694519,
      "learning_rate": 9.453968401809958e-05,
      "loss": 1.0091,
      "step": 10830
    },
    {
      "epoch": 1.5227049065092086,
      "grad_norm": 1.627318024635315,
      "learning_rate": 9.477182123764476e-05,
      "loss": 1.0727,
      "step": 10831
    },
    {
      "epoch": 1.5228454941656122,
      "grad_norm": 1.4890882968902588,
      "learning_rate": 9.500398671133361e-05,
      "loss": 1.0687,
      "step": 10832
    },
    {
      "epoch": 1.522986081822016,
      "grad_norm": 1.6288347244262695,
      "learning_rate": 9.52361791844966e-05,
      "loss": 1.2312,
      "step": 10833
    },
    {
      "epoch": 1.5231266694784198,
      "grad_norm": 1.7587419748306274,
      "learning_rate": 9.546839740231823e-05,
      "loss": 1.1989,
      "step": 10834
    },
    {
      "epoch": 1.5232672571348236,
      "grad_norm": 1.6665445566177368,
      "learning_rate": 9.570064010984399e-05,
      "loss": 1.1377,
      "step": 10835
    },
    {
      "epoch": 1.5234078447912274,
      "grad_norm": 1.703679084777832,
      "learning_rate": 9.593290605198691e-05,
      "loss": 0.9437,
      "step": 10836
    },
    {
      "epoch": 1.523548432447631,
      "grad_norm": 1.5494745969772339,
      "learning_rate": 9.616519397353451e-05,
      "loss": 0.9812,
      "step": 10837
    },
    {
      "epoch": 1.523689020104035,
      "grad_norm": 1.5545969009399414,
      "learning_rate": 9.639750261915552e-05,
      "loss": 1.0249,
      "step": 10838
    },
    {
      "epoch": 1.5238296077604385,
      "grad_norm": 1.7744053602218628,
      "learning_rate": 9.662983073340665e-05,
      "loss": 1.0405,
      "step": 10839
    },
    {
      "epoch": 1.5239701954168425,
      "grad_norm": 1.6222777366638184,
      "learning_rate": 9.686217706073944e-05,
      "loss": 0.9735,
      "step": 10840
    },
    {
      "epoch": 1.524110783073246,
      "grad_norm": 1.4051661491394043,
      "learning_rate": 9.70945403455066e-05,
      "loss": 1.0249,
      "step": 10841
    },
    {
      "epoch": 1.52425137072965,
      "grad_norm": 1.5510990619659424,
      "learning_rate": 9.732691933197027e-05,
      "loss": 1.0742,
      "step": 10842
    },
    {
      "epoch": 1.5243919583860537,
      "grad_norm": 1.400909662246704,
      "learning_rate": 9.755931276430711e-05,
      "loss": 1.2344,
      "step": 10843
    },
    {
      "epoch": 1.5245325460424575,
      "grad_norm": 1.4166713953018188,
      "learning_rate": 9.779171938661488e-05,
      "loss": 1.1019,
      "step": 10844
    },
    {
      "epoch": 1.5246731336988613,
      "grad_norm": 1.6037116050720215,
      "learning_rate": 9.802413794292117e-05,
      "loss": 1.1882,
      "step": 10845
    },
    {
      "epoch": 1.524813721355265,
      "grad_norm": 1.4845781326293945,
      "learning_rate": 9.825656717718842e-05,
      "loss": 1.0784,
      "step": 10846
    },
    {
      "epoch": 1.5249543090116688,
      "grad_norm": 1.5673562288284302,
      "learning_rate": 9.848900583332236e-05,
      "loss": 1.2473,
      "step": 10847
    },
    {
      "epoch": 1.5250948966680724,
      "grad_norm": 1.7440364360809326,
      "learning_rate": 9.872145265517673e-05,
      "loss": 0.9062,
      "step": 10848
    },
    {
      "epoch": 1.5252354843244764,
      "grad_norm": 1.6442065238952637,
      "learning_rate": 9.895390638656158e-05,
      "loss": 1.0039,
      "step": 10849
    },
    {
      "epoch": 1.52537607198088,
      "grad_norm": 1.254284143447876,
      "learning_rate": 9.918636577124989e-05,
      "loss": 1.0621,
      "step": 10850
    },
    {
      "epoch": 1.525516659637284,
      "grad_norm": 1.5106351375579834,
      "learning_rate": 9.941882955298269e-05,
      "loss": 1.0035,
      "step": 10851
    },
    {
      "epoch": 1.5256572472936876,
      "grad_norm": 1.4828431606292725,
      "learning_rate": 9.96512964754794e-05,
      "loss": 1.1825,
      "step": 10852
    },
    {
      "epoch": 1.5257978349500914,
      "grad_norm": 1.6781704425811768,
      "learning_rate": 9.988376528244105e-05,
      "loss": 0.9251,
      "step": 10853
    },
    {
      "epoch": 1.5259384226064951,
      "grad_norm": 1.453352928161621,
      "learning_rate": 0.00010011623471755877,
      "loss": 0.947,
      "step": 10854
    },
    {
      "epoch": 1.526079010262899,
      "grad_norm": 1.6548233032226562,
      "learning_rate": 0.00010034870352452041,
      "loss": 1.0408,
      "step": 10855
    },
    {
      "epoch": 1.5262195979193027,
      "grad_norm": 1.710423469543457,
      "learning_rate": 0.00010058117044701714,
      "loss": 1.0848,
      "step": 10856
    },
    {
      "epoch": 1.5263601855757063,
      "grad_norm": 1.54841148853302,
      "learning_rate": 0.00010081363422874995,
      "loss": 1.2078,
      "step": 10857
    },
    {
      "epoch": 1.5265007732321103,
      "grad_norm": 1.5555810928344727,
      "learning_rate": 0.00010104609361343824,
      "loss": 1.1101,
      "step": 10858
    },
    {
      "epoch": 1.5266413608885139,
      "grad_norm": 1.414229154586792,
      "learning_rate": 0.0001012785473448231,
      "loss": 1.1984,
      "step": 10859
    },
    {
      "epoch": 1.5267819485449179,
      "grad_norm": 1.58904230594635,
      "learning_rate": 0.00010151099416667747,
      "loss": 0.9736,
      "step": 10860
    },
    {
      "epoch": 1.5269225362013215,
      "grad_norm": 1.334327220916748,
      "learning_rate": 0.0001017434328228114,
      "loss": 1.1462,
      "step": 10861
    },
    {
      "epoch": 1.5270631238577252,
      "grad_norm": 1.6760047674179077,
      "learning_rate": 0.00010197586205707865,
      "loss": 1.0434,
      "step": 10862
    },
    {
      "epoch": 1.527203711514129,
      "grad_norm": 1.3994529247283936,
      "learning_rate": 0.00010220828061338494,
      "loss": 1.2185,
      "step": 10863
    },
    {
      "epoch": 1.5273442991705328,
      "grad_norm": 1.3739293813705444,
      "learning_rate": 0.00010244068723569271,
      "loss": 0.9442,
      "step": 10864
    },
    {
      "epoch": 1.5274848868269366,
      "grad_norm": 1.4232442378997803,
      "learning_rate": 0.00010267308066802955,
      "loss": 1.0064,
      "step": 10865
    },
    {
      "epoch": 1.5276254744833404,
      "grad_norm": 1.4652150869369507,
      "learning_rate": 0.00010290545965449322,
      "loss": 0.9066,
      "step": 10866
    },
    {
      "epoch": 1.5277660621397442,
      "grad_norm": 1.5769121646881104,
      "learning_rate": 0.0001031378229392604,
      "loss": 1.0758,
      "step": 10867
    },
    {
      "epoch": 1.5279066497961478,
      "grad_norm": 1.5728360414505005,
      "learning_rate": 0.00010337016926659318,
      "loss": 1.1011,
      "step": 10868
    },
    {
      "epoch": 1.5280472374525518,
      "grad_norm": 1.7061412334442139,
      "learning_rate": 0.0001036024973808443,
      "loss": 1.078,
      "step": 10869
    },
    {
      "epoch": 1.5281878251089553,
      "grad_norm": 1.5770227909088135,
      "learning_rate": 0.00010383480602646531,
      "loss": 1.0353,
      "step": 10870
    },
    {
      "epoch": 1.5283284127653594,
      "grad_norm": 1.576220989227295,
      "learning_rate": 0.00010406709394801291,
      "loss": 1.0333,
      "step": 10871
    },
    {
      "epoch": 1.528469000421763,
      "grad_norm": 1.6021530628204346,
      "learning_rate": 0.00010429935989015583,
      "loss": 1.1118,
      "step": 10872
    },
    {
      "epoch": 1.5286095880781667,
      "grad_norm": 1.4032373428344727,
      "learning_rate": 0.00010453160259768157,
      "loss": 1.0219,
      "step": 10873
    },
    {
      "epoch": 1.5287501757345705,
      "grad_norm": 1.8707542419433594,
      "learning_rate": 0.00010476382081550324,
      "loss": 0.9192,
      "step": 10874
    },
    {
      "epoch": 1.5288907633909743,
      "grad_norm": 1.407729983329773,
      "learning_rate": 0.00010499601328866621,
      "loss": 1.1019,
      "step": 10875
    },
    {
      "epoch": 1.529031351047378,
      "grad_norm": 1.4098560810089111,
      "learning_rate": 0.00010522817876235507,
      "loss": 0.9698,
      "step": 10876
    },
    {
      "epoch": 1.5291719387037817,
      "grad_norm": 1.4808642864227295,
      "learning_rate": 0.00010546031598190024,
      "loss": 1.0616,
      "step": 10877
    },
    {
      "epoch": 1.5293125263601857,
      "grad_norm": 1.6528857946395874,
      "learning_rate": 0.00010569242369278454,
      "loss": 1.0561,
      "step": 10878
    },
    {
      "epoch": 1.5294531140165892,
      "grad_norm": 1.4039679765701294,
      "learning_rate": 0.00010592450064065129,
      "loss": 1.0177,
      "step": 10879
    },
    {
      "epoch": 1.5295937016729932,
      "grad_norm": 1.7279568910598755,
      "learning_rate": 0.00010615654557130901,
      "loss": 0.8478,
      "step": 10880
    },
    {
      "epoch": 1.5297342893293968,
      "grad_norm": 1.7036890983581543,
      "learning_rate": 0.00010638855723073993,
      "loss": 1.1143,
      "step": 10881
    },
    {
      "epoch": 1.5298748769858006,
      "grad_norm": 1.4972507953643799,
      "learning_rate": 0.00010662053436510508,
      "loss": 1.0671,
      "step": 10882
    },
    {
      "epoch": 1.5300154646422044,
      "grad_norm": 1.634390950202942,
      "learning_rate": 0.0001068524757207531,
      "loss": 1.0227,
      "step": 10883
    },
    {
      "epoch": 1.5301560522986082,
      "grad_norm": 1.5753270387649536,
      "learning_rate": 0.00010708438004422524,
      "loss": 1.1164,
      "step": 10884
    },
    {
      "epoch": 1.530296639955012,
      "grad_norm": 1.8060431480407715,
      "learning_rate": 0.00010731624608226398,
      "loss": 1.0645,
      "step": 10885
    },
    {
      "epoch": 1.5304372276114158,
      "grad_norm": 2.148818016052246,
      "learning_rate": 0.00010754807258181765,
      "loss": 0.9406,
      "step": 10886
    },
    {
      "epoch": 1.5305778152678196,
      "grad_norm": 2.0585334300994873,
      "learning_rate": 0.0001077798582900486,
      "loss": 0.9896,
      "step": 10887
    },
    {
      "epoch": 1.5307184029242231,
      "grad_norm": 1.6936863660812378,
      "learning_rate": 0.00010801160195433996,
      "loss": 1.0671,
      "step": 10888
    },
    {
      "epoch": 1.5308589905806271,
      "grad_norm": 1.674674391746521,
      "learning_rate": 0.00010824330232230068,
      "loss": 1.1414,
      "step": 10889
    },
    {
      "epoch": 1.5309995782370307,
      "grad_norm": 1.4929717779159546,
      "learning_rate": 0.00010847495814177579,
      "loss": 1.1913,
      "step": 10890
    },
    {
      "epoch": 1.5311401658934347,
      "grad_norm": 1.37068772315979,
      "learning_rate": 0.00010870656816084969,
      "loss": 1.2542,
      "step": 10891
    },
    {
      "epoch": 1.5312807535498383,
      "grad_norm": 1.627548098564148,
      "learning_rate": 0.00010893813112785462,
      "loss": 1.063,
      "step": 10892
    },
    {
      "epoch": 1.531421341206242,
      "grad_norm": 1.5920268297195435,
      "learning_rate": 0.00010916964579137712,
      "loss": 1.1525,
      "step": 10893
    },
    {
      "epoch": 1.5315619288626459,
      "grad_norm": 1.5175460577011108,
      "learning_rate": 0.00010940111090026439,
      "loss": 1.2136,
      "step": 10894
    },
    {
      "epoch": 1.5317025165190497,
      "grad_norm": 1.77749502658844,
      "learning_rate": 0.0001096325252036325,
      "loss": 1.2049,
      "step": 10895
    },
    {
      "epoch": 1.5318431041754534,
      "grad_norm": 1.6331020593643188,
      "learning_rate": 0.00010986388745087142,
      "loss": 1.034,
      "step": 10896
    },
    {
      "epoch": 1.531983691831857,
      "grad_norm": 1.4580248594284058,
      "learning_rate": 0.00011009519639165166,
      "loss": 1.0708,
      "step": 10897
    },
    {
      "epoch": 1.532124279488261,
      "grad_norm": 1.647313117980957,
      "learning_rate": 0.00011032645077593294,
      "loss": 1.065,
      "step": 10898
    },
    {
      "epoch": 1.5322648671446646,
      "grad_norm": 1.5276020765304565,
      "learning_rate": 0.00011055764935396942,
      "loss": 1.0671,
      "step": 10899
    },
    {
      "epoch": 1.5324054548010686,
      "grad_norm": 1.857673168182373,
      "learning_rate": 0.00011078879087631653,
      "loss": 1.1048,
      "step": 10900
    },
    {
      "epoch": 1.5325460424574722,
      "grad_norm": 1.6207084655761719,
      "learning_rate": 0.00011101987409383903,
      "loss": 1.0555,
      "step": 10901
    },
    {
      "epoch": 1.532686630113876,
      "grad_norm": 1.4094078540802002,
      "learning_rate": 0.0001112508977577158,
      "loss": 1.1978,
      "step": 10902
    },
    {
      "epoch": 1.5328272177702797,
      "grad_norm": 1.5856974124908447,
      "learning_rate": 0.00011148186061944821,
      "loss": 1.0904,
      "step": 10903
    },
    {
      "epoch": 1.5329678054266835,
      "grad_norm": 1.6526046991348267,
      "learning_rate": 0.00011171276143086518,
      "loss": 0.9516,
      "step": 10904
    },
    {
      "epoch": 1.5331083930830873,
      "grad_norm": 1.5090992450714111,
      "learning_rate": 0.00011194359894413162,
      "loss": 1.0118,
      "step": 10905
    },
    {
      "epoch": 1.5332489807394911,
      "grad_norm": 1.538120150566101,
      "learning_rate": 0.00011217437191175533,
      "loss": 1.111,
      "step": 10906
    },
    {
      "epoch": 1.533389568395895,
      "grad_norm": 1.501155138015747,
      "learning_rate": 0.00011240507908659177,
      "loss": 0.9102,
      "step": 10907
    },
    {
      "epoch": 1.5335301560522985,
      "grad_norm": 1.362136721611023,
      "learning_rate": 0.00011263571922185234,
      "loss": 1.0633,
      "step": 10908
    },
    {
      "epoch": 1.5336707437087025,
      "grad_norm": 1.6330100297927856,
      "learning_rate": 0.00011286629107111074,
      "loss": 0.9281,
      "step": 10909
    },
    {
      "epoch": 1.533811331365106,
      "grad_norm": 1.5059654712677002,
      "learning_rate": 0.00011309679338830934,
      "loss": 1.115,
      "step": 10910
    },
    {
      "epoch": 1.53395191902151,
      "grad_norm": 1.6659921407699585,
      "learning_rate": 0.00011332722492776773,
      "loss": 1.0416,
      "step": 10911
    },
    {
      "epoch": 1.5340925066779136,
      "grad_norm": 1.5972557067871094,
      "learning_rate": 0.00011355758444418578,
      "loss": 1.0191,
      "step": 10912
    },
    {
      "epoch": 1.5342330943343174,
      "grad_norm": 1.6778215169906616,
      "learning_rate": 0.00011378787069265412,
      "loss": 1.1916,
      "step": 10913
    },
    {
      "epoch": 1.5343736819907212,
      "grad_norm": 1.3208612203598022,
      "learning_rate": 0.00011401808242865893,
      "loss": 1.2026,
      "step": 10914
    },
    {
      "epoch": 1.534514269647125,
      "grad_norm": 1.4511122703552246,
      "learning_rate": 0.00011424821840808899,
      "loss": 1.1079,
      "step": 10915
    },
    {
      "epoch": 1.5346548573035288,
      "grad_norm": 1.5567235946655273,
      "learning_rate": 0.00011447827738724228,
      "loss": 1.0201,
      "step": 10916
    },
    {
      "epoch": 1.5347954449599324,
      "grad_norm": 1.5082628726959229,
      "learning_rate": 0.00011470825812283388,
      "loss": 1.084,
      "step": 10917
    },
    {
      "epoch": 1.5349360326163364,
      "grad_norm": 1.504544973373413,
      "learning_rate": 0.00011493815937200063,
      "loss": 1.0858,
      "step": 10918
    },
    {
      "epoch": 1.53507662027274,
      "grad_norm": 1.408734679222107,
      "learning_rate": 0.00011516797989230971,
      "loss": 1.0803,
      "step": 10919
    },
    {
      "epoch": 1.535217207929144,
      "grad_norm": 1.8142286539077759,
      "learning_rate": 0.00011539771844176346,
      "loss": 1.3436,
      "step": 10920
    },
    {
      "epoch": 1.5353577955855475,
      "grad_norm": 1.6533668041229248,
      "learning_rate": 0.00011562737377880795,
      "loss": 1.1512,
      "step": 10921
    },
    {
      "epoch": 1.5354983832419513,
      "grad_norm": 1.5224800109863281,
      "learning_rate": 0.0001158569446623396,
      "loss": 1.1599,
      "step": 10922
    },
    {
      "epoch": 1.535638970898355,
      "grad_norm": 1.556315302848816,
      "learning_rate": 0.00011608642985171023,
      "loss": 1.1926,
      "step": 10923
    },
    {
      "epoch": 1.535779558554759,
      "grad_norm": 1.7710145711898804,
      "learning_rate": 0.00011631582810673514,
      "loss": 0.9449,
      "step": 10924
    },
    {
      "epoch": 1.5359201462111627,
      "grad_norm": 1.6116034984588623,
      "learning_rate": 0.00011654513818769932,
      "loss": 1.0533,
      "step": 10925
    },
    {
      "epoch": 1.5360607338675665,
      "grad_norm": 1.757584571838379,
      "learning_rate": 0.00011677435885536477,
      "loss": 1.0244,
      "step": 10926
    },
    {
      "epoch": 1.5362013215239703,
      "grad_norm": 1.6305367946624756,
      "learning_rate": 0.00011700348887097521,
      "loss": 1.0883,
      "step": 10927
    },
    {
      "epoch": 1.5363419091803738,
      "grad_norm": 1.5277791023254395,
      "learning_rate": 0.00011723252699626637,
      "loss": 1.0494,
      "step": 10928
    },
    {
      "epoch": 1.5364824968367778,
      "grad_norm": 1.2515043020248413,
      "learning_rate": 0.00011746147199346919,
      "loss": 0.9703,
      "step": 10929
    },
    {
      "epoch": 1.5366230844931814,
      "grad_norm": 1.671792984008789,
      "learning_rate": 0.00011769032262531827,
      "loss": 1.0659,
      "step": 10930
    },
    {
      "epoch": 1.5367636721495854,
      "grad_norm": 1.5582164525985718,
      "learning_rate": 0.00011791907765505812,
      "loss": 0.9682,
      "step": 10931
    },
    {
      "epoch": 1.536904259805989,
      "grad_norm": 1.7147759199142456,
      "learning_rate": 0.00011814773584644957,
      "loss": 1.0602,
      "step": 10932
    },
    {
      "epoch": 1.5370448474623928,
      "grad_norm": 1.5502796173095703,
      "learning_rate": 0.0001183762959637779,
      "loss": 1.1075,
      "step": 10933
    },
    {
      "epoch": 1.5371854351187966,
      "grad_norm": 1.6970365047454834,
      "learning_rate": 0.0001186047567718576,
      "loss": 1.1472,
      "step": 10934
    },
    {
      "epoch": 1.5373260227752004,
      "grad_norm": 1.4433379173278809,
      "learning_rate": 0.00011883311703603923,
      "loss": 1.1479,
      "step": 10935
    },
    {
      "epoch": 1.5374666104316042,
      "grad_norm": 1.9879825115203857,
      "learning_rate": 0.0001190613755222177,
      "loss": 0.9788,
      "step": 10936
    },
    {
      "epoch": 1.5376071980880077,
      "grad_norm": 1.6183501482009888,
      "learning_rate": 0.00011928953099683728,
      "loss": 1.0295,
      "step": 10937
    },
    {
      "epoch": 1.5377477857444117,
      "grad_norm": 1.6441611051559448,
      "learning_rate": 0.0001195175822269,
      "loss": 1.0582,
      "step": 10938
    },
    {
      "epoch": 1.5378883734008153,
      "grad_norm": 1.3525892496109009,
      "learning_rate": 0.00011974552797997017,
      "loss": 0.9724,
      "step": 10939
    },
    {
      "epoch": 1.5380289610572193,
      "grad_norm": 1.6953545808792114,
      "learning_rate": 0.00011997336702418247,
      "loss": 1.1705,
      "step": 10940
    },
    {
      "epoch": 1.5381695487136229,
      "grad_norm": 1.7311574220657349,
      "learning_rate": 0.00012020109812824827,
      "loss": 1.0082,
      "step": 10941
    },
    {
      "epoch": 1.5383101363700267,
      "grad_norm": 1.4725397825241089,
      "learning_rate": 0.00012042872006146262,
      "loss": 1.167,
      "step": 10942
    },
    {
      "epoch": 1.5384507240264305,
      "grad_norm": 1.6408593654632568,
      "learning_rate": 0.00012065623159370914,
      "loss": 1.0777,
      "step": 10943
    },
    {
      "epoch": 1.5385913116828343,
      "grad_norm": 1.5069847106933594,
      "learning_rate": 0.0001208836314954702,
      "loss": 1.0447,
      "step": 10944
    },
    {
      "epoch": 1.538731899339238,
      "grad_norm": 1.4732102155685425,
      "learning_rate": 0.00012111091853783001,
      "loss": 1.2011,
      "step": 10945
    },
    {
      "epoch": 1.5388724869956418,
      "grad_norm": 1.6488702297210693,
      "learning_rate": 0.0001213380914924831,
      "loss": 1.021,
      "step": 10946
    },
    {
      "epoch": 1.5390130746520456,
      "grad_norm": 1.6410331726074219,
      "learning_rate": 0.00012156514913174049,
      "loss": 1.3149,
      "step": 10947
    },
    {
      "epoch": 1.5391536623084492,
      "grad_norm": 1.3800333738327026,
      "learning_rate": 0.00012179209022853606,
      "loss": 1.0209,
      "step": 10948
    },
    {
      "epoch": 1.5392942499648532,
      "grad_norm": 1.447636604309082,
      "learning_rate": 0.00012201891355643495,
      "loss": 1.0558,
      "step": 10949
    },
    {
      "epoch": 1.5394348376212568,
      "grad_norm": 1.5425364971160889,
      "learning_rate": 0.0001222456178896366,
      "loss": 1.1,
      "step": 10950
    },
    {
      "epoch": 1.5395754252776608,
      "grad_norm": 1.5845969915390015,
      "learning_rate": 0.0001224722020029849,
      "loss": 0.9468,
      "step": 10951
    },
    {
      "epoch": 1.5397160129340643,
      "grad_norm": 1.6309762001037598,
      "learning_rate": 0.00012269866467197316,
      "loss": 0.9265,
      "step": 10952
    },
    {
      "epoch": 1.5398566005904681,
      "grad_norm": 1.4628946781158447,
      "learning_rate": 0.00012292500467275098,
      "loss": 1.2279,
      "step": 10953
    },
    {
      "epoch": 1.539997188246872,
      "grad_norm": 1.2244774103164673,
      "learning_rate": 0.00012315122078213043,
      "loss": 1.149,
      "step": 10954
    },
    {
      "epoch": 1.5401377759032757,
      "grad_norm": 1.5646049976348877,
      "learning_rate": 0.00012337731177759434,
      "loss": 1.2179,
      "step": 10955
    },
    {
      "epoch": 1.5402783635596795,
      "grad_norm": 1.643129587173462,
      "learning_rate": 0.00012360327643730045,
      "loss": 0.9686,
      "step": 10956
    },
    {
      "epoch": 1.540418951216083,
      "grad_norm": 1.4297584295272827,
      "learning_rate": 0.0001238291135400901,
      "loss": 1.0808,
      "step": 10957
    },
    {
      "epoch": 1.540559538872487,
      "grad_norm": 1.5249603986740112,
      "learning_rate": 0.0001240548218654928,
      "loss": 1.1752,
      "step": 10958
    },
    {
      "epoch": 1.5407001265288907,
      "grad_norm": 1.490809440612793,
      "learning_rate": 0.0001242804001937348,
      "loss": 1.0515,
      "step": 10959
    },
    {
      "epoch": 1.5408407141852947,
      "grad_norm": 1.520548939704895,
      "learning_rate": 0.00012450584730574552,
      "loss": 1.1241,
      "step": 10960
    },
    {
      "epoch": 1.5409813018416982,
      "grad_norm": 1.569280743598938,
      "learning_rate": 0.00012473116198316245,
      "loss": 1.0158,
      "step": 10961
    },
    {
      "epoch": 1.541121889498102,
      "grad_norm": 1.6150498390197754,
      "learning_rate": 0.00012495634300833917,
      "loss": 0.9235,
      "step": 10962
    },
    {
      "epoch": 1.5412624771545058,
      "grad_norm": 1.6275986433029175,
      "learning_rate": 0.0001251813891643515,
      "loss": 0.9957,
      "step": 10963
    },
    {
      "epoch": 1.5414030648109096,
      "grad_norm": 1.93184494972229,
      "learning_rate": 0.00012540629923500415,
      "loss": 1.0121,
      "step": 10964
    },
    {
      "epoch": 1.5415436524673134,
      "grad_norm": 1.6830533742904663,
      "learning_rate": 0.00012563107200483725,
      "loss": 1.2844,
      "step": 10965
    },
    {
      "epoch": 1.5416842401237172,
      "grad_norm": 1.5816279649734497,
      "learning_rate": 0.00012585570625913298,
      "loss": 0.8247,
      "step": 10966
    },
    {
      "epoch": 1.541824827780121,
      "grad_norm": 1.3933120965957642,
      "learning_rate": 0.000126080200783922,
      "loss": 1.2337,
      "step": 10967
    },
    {
      "epoch": 1.5419654154365245,
      "grad_norm": 1.384400486946106,
      "learning_rate": 0.00012630455436599017,
      "loss": 1.0992,
      "step": 10968
    },
    {
      "epoch": 1.5421060030929286,
      "grad_norm": 1.4917304515838623,
      "learning_rate": 0.00012652876579288493,
      "loss": 1.0581,
      "step": 10969
    },
    {
      "epoch": 1.5422465907493321,
      "grad_norm": 1.5521337985992432,
      "learning_rate": 0.0001267528338529218,
      "loss": 1.1403,
      "step": 10970
    },
    {
      "epoch": 1.5423871784057361,
      "grad_norm": 1.554934024810791,
      "learning_rate": 0.00012697675733519187,
      "loss": 1.1227,
      "step": 10971
    },
    {
      "epoch": 1.5425277660621397,
      "grad_norm": 1.3901373147964478,
      "learning_rate": 0.00012720053502956712,
      "loss": 1.1954,
      "step": 10972
    },
    {
      "epoch": 1.5426683537185435,
      "grad_norm": 1.6035373210906982,
      "learning_rate": 0.00012742416572670653,
      "loss": 0.8989,
      "step": 10973
    },
    {
      "epoch": 1.5428089413749473,
      "grad_norm": 1.7118778228759766,
      "learning_rate": 0.00012764764821806456,
      "loss": 1.017,
      "step": 10974
    },
    {
      "epoch": 1.542949529031351,
      "grad_norm": 1.5757193565368652,
      "learning_rate": 0.00012787098129589598,
      "loss": 1.091,
      "step": 10975
    },
    {
      "epoch": 1.5430901166877549,
      "grad_norm": 1.696341872215271,
      "learning_rate": 0.00012809416375326397,
      "loss": 0.9919,
      "step": 10976
    },
    {
      "epoch": 1.5432307043441584,
      "grad_norm": 1.489339828491211,
      "learning_rate": 0.00012831719438404472,
      "loss": 1.1044,
      "step": 10977
    },
    {
      "epoch": 1.5433712920005624,
      "grad_norm": 1.8486915826797485,
      "learning_rate": 0.0001285400719829352,
      "loss": 1.0284,
      "step": 10978
    },
    {
      "epoch": 1.543511879656966,
      "grad_norm": 1.7055610418319702,
      "learning_rate": 0.0001287627953454595,
      "loss": 1.0591,
      "step": 10979
    },
    {
      "epoch": 1.54365246731337,
      "grad_norm": 1.5333278179168701,
      "learning_rate": 0.00012898536326797513,
      "loss": 1.227,
      "step": 10980
    },
    {
      "epoch": 1.5437930549697736,
      "grad_norm": 1.3947112560272217,
      "learning_rate": 0.00012920777454767965,
      "loss": 1.0983,
      "step": 10981
    },
    {
      "epoch": 1.5439336426261774,
      "grad_norm": 1.7340742349624634,
      "learning_rate": 0.0001294300279826172,
      "loss": 1.0828,
      "step": 10982
    },
    {
      "epoch": 1.5440742302825812,
      "grad_norm": 1.5152851343154907,
      "learning_rate": 0.00012965212237168493,
      "loss": 1.0465,
      "step": 10983
    },
    {
      "epoch": 1.544214817938985,
      "grad_norm": 1.6177973747253418,
      "learning_rate": 0.0001298740565146395,
      "loss": 1.1074,
      "step": 10984
    },
    {
      "epoch": 1.5443554055953888,
      "grad_norm": 1.7350330352783203,
      "learning_rate": 0.00013009582921210352,
      "loss": 0.9763,
      "step": 10985
    },
    {
      "epoch": 1.5444959932517925,
      "grad_norm": 1.5360567569732666,
      "learning_rate": 0.0001303174392655718,
      "loss": 1.0955,
      "step": 10986
    },
    {
      "epoch": 1.5446365809081963,
      "grad_norm": 1.5181204080581665,
      "learning_rate": 0.00013053888547741947,
      "loss": 1.1011,
      "step": 10987
    },
    {
      "epoch": 1.5447771685646,
      "grad_norm": 1.7218685150146484,
      "learning_rate": 0.00013076016665090507,
      "loss": 1.1662,
      "step": 10988
    },
    {
      "epoch": 1.544917756221004,
      "grad_norm": 1.3929789066314697,
      "learning_rate": 0.00013098128159018036,
      "loss": 0.9852,
      "step": 10989
    },
    {
      "epoch": 1.5450583438774075,
      "grad_norm": 1.914067268371582,
      "learning_rate": 0.00013120222910029513,
      "loss": 1.1247,
      "step": 10990
    },
    {
      "epoch": 1.5451989315338115,
      "grad_norm": 1.4650546312332153,
      "learning_rate": 0.00013142300798720365,
      "loss": 1.0783,
      "step": 10991
    },
    {
      "epoch": 1.545339519190215,
      "grad_norm": 1.4233120679855347,
      "learning_rate": 0.00013164361705777253,
      "loss": 1.1785,
      "step": 10992
    },
    {
      "epoch": 1.5454801068466189,
      "grad_norm": 2.00744366645813,
      "learning_rate": 0.00013186405511978502,
      "loss": 1.0435,
      "step": 10993
    },
    {
      "epoch": 1.5456206945030226,
      "grad_norm": 1.6940242052078247,
      "learning_rate": 0.0001320843209819489,
      "loss": 1.0591,
      "step": 10994
    },
    {
      "epoch": 1.5457612821594264,
      "grad_norm": 1.5640307664871216,
      "learning_rate": 0.0001323044134539029,
      "loss": 1.1541,
      "step": 10995
    },
    {
      "epoch": 1.5459018698158302,
      "grad_norm": 1.418169379234314,
      "learning_rate": 0.00013252433134622175,
      "loss": 1.1435,
      "step": 10996
    },
    {
      "epoch": 1.5460424574722338,
      "grad_norm": 2.0093462467193604,
      "learning_rate": 0.00013274407347042437,
      "loss": 0.9891,
      "step": 10997
    },
    {
      "epoch": 1.5461830451286378,
      "grad_norm": 1.7611724138259888,
      "learning_rate": 0.00013296363863898018,
      "loss": 1.0902,
      "step": 10998
    },
    {
      "epoch": 1.5463236327850414,
      "grad_norm": 1.706025242805481,
      "learning_rate": 0.00013318302566531394,
      "loss": 0.9793,
      "step": 10999
    },
    {
      "epoch": 1.5464642204414454,
      "grad_norm": 1.66072416305542,
      "learning_rate": 0.0001334022333638134,
      "loss": 1.1656,
      "step": 11000
    },
    {
      "epoch": 1.5464642204414454,
      "eval_loss": 1.1484254598617554,
      "eval_runtime": 773.2716,
      "eval_samples_per_second": 16.354,
      "eval_steps_per_second": 8.177,
      "step": 11000
    },
    {
      "epoch": 1.546604808097849,
      "grad_norm": 1.653329610824585,
      "learning_rate": 0.00013362126054983553,
      "loss": 1.0555,
      "step": 11001
    },
    {
      "epoch": 1.5467453957542527,
      "grad_norm": 1.6193832159042358,
      "learning_rate": 0.00013384010603971274,
      "loss": 1.0177,
      "step": 11002
    },
    {
      "epoch": 1.5468859834106565,
      "grad_norm": 1.4337109327316284,
      "learning_rate": 0.0001340587686507594,
      "loss": 0.9692,
      "step": 11003
    },
    {
      "epoch": 1.5470265710670603,
      "grad_norm": 1.620583176612854,
      "learning_rate": 0.00013427724720127822,
      "loss": 1.2349,
      "step": 11004
    },
    {
      "epoch": 1.547167158723464,
      "grad_norm": 1.5686932802200317,
      "learning_rate": 0.00013449554051056653,
      "loss": 0.9515,
      "step": 11005
    },
    {
      "epoch": 1.547307746379868,
      "grad_norm": 1.5918805599212646,
      "learning_rate": 0.00013471364739892284,
      "loss": 1.0192,
      "step": 11006
    },
    {
      "epoch": 1.5474483340362717,
      "grad_norm": 1.5631685256958008,
      "learning_rate": 0.00013493156668765267,
      "loss": 1.1323,
      "step": 11007
    },
    {
      "epoch": 1.5475889216926753,
      "grad_norm": 1.514339566230774,
      "learning_rate": 0.0001351492971990765,
      "loss": 1.2253,
      "step": 11008
    },
    {
      "epoch": 1.5477295093490793,
      "grad_norm": 1.5589519739151,
      "learning_rate": 0.00013536683775653395,
      "loss": 1.1808,
      "step": 11009
    },
    {
      "epoch": 1.5478700970054828,
      "grad_norm": 1.6125164031982422,
      "learning_rate": 0.00013558418718439177,
      "loss": 1.1857,
      "step": 11010
    },
    {
      "epoch": 1.5480106846618868,
      "grad_norm": 1.6071752309799194,
      "learning_rate": 0.00013580134430804872,
      "loss": 0.9723,
      "step": 11011
    },
    {
      "epoch": 1.5481512723182904,
      "grad_norm": 1.6540707349777222,
      "learning_rate": 0.00013601830795394374,
      "loss": 1.0908,
      "step": 11012
    },
    {
      "epoch": 1.5482918599746942,
      "grad_norm": 1.615687370300293,
      "learning_rate": 0.0001362350769495608,
      "loss": 1.1224,
      "step": 11013
    },
    {
      "epoch": 1.548432447631098,
      "grad_norm": 1.715671181678772,
      "learning_rate": 0.0001364516501234367,
      "loss": 1.2814,
      "step": 11014
    },
    {
      "epoch": 1.5485730352875018,
      "grad_norm": 1.5944671630859375,
      "learning_rate": 0.00013666802630516548,
      "loss": 1.0637,
      "step": 11015
    },
    {
      "epoch": 1.5487136229439056,
      "grad_norm": 1.4288296699523926,
      "learning_rate": 0.00013688420432540622,
      "loss": 1.0475,
      "step": 11016
    },
    {
      "epoch": 1.5488542106003091,
      "grad_norm": 1.3616857528686523,
      "learning_rate": 0.00013710018301588882,
      "loss": 1.0403,
      "step": 11017
    },
    {
      "epoch": 1.5489947982567132,
      "grad_norm": 1.5334278345108032,
      "learning_rate": 0.0001373159612094204,
      "loss": 1.0721,
      "step": 11018
    },
    {
      "epoch": 1.5491353859131167,
      "grad_norm": 1.4591853618621826,
      "learning_rate": 0.00013753153773989163,
      "loss": 1.0933,
      "step": 11019
    },
    {
      "epoch": 1.5492759735695207,
      "grad_norm": 1.5079902410507202,
      "learning_rate": 0.000137746911442283,
      "loss": 1.1626,
      "step": 11020
    },
    {
      "epoch": 1.5494165612259243,
      "grad_norm": 1.5344898700714111,
      "learning_rate": 0.0001379620811526711,
      "loss": 1.2128,
      "step": 11021
    },
    {
      "epoch": 1.549557148882328,
      "grad_norm": 1.6470212936401367,
      "learning_rate": 0.000138177045708235,
      "loss": 1.1601,
      "step": 11022
    },
    {
      "epoch": 1.5496977365387319,
      "grad_norm": 2.4482674598693848,
      "learning_rate": 0.00013839180394726206,
      "loss": 1.0248,
      "step": 11023
    },
    {
      "epoch": 1.5498383241951357,
      "grad_norm": 1.5414868593215942,
      "learning_rate": 0.00013860635470915565,
      "loss": 1.1836,
      "step": 11024
    },
    {
      "epoch": 1.5499789118515395,
      "grad_norm": 1.7610398530960083,
      "learning_rate": 0.00013882069683443974,
      "loss": 1.1807,
      "step": 11025
    },
    {
      "epoch": 1.5501194995079433,
      "grad_norm": 1.5157225131988525,
      "learning_rate": 0.00013903482916476513,
      "loss": 1.086,
      "step": 11026
    },
    {
      "epoch": 1.550260087164347,
      "grad_norm": 1.6437453031539917,
      "learning_rate": 0.00013924875054291736,
      "loss": 1.0366,
      "step": 11027
    },
    {
      "epoch": 1.5504006748207506,
      "grad_norm": 1.7667601108551025,
      "learning_rate": 0.00013946245981282172,
      "loss": 1.0637,
      "step": 11028
    },
    {
      "epoch": 1.5505412624771546,
      "grad_norm": 1.5848318338394165,
      "learning_rate": 0.00013967595581954945,
      "loss": 0.8591,
      "step": 11029
    },
    {
      "epoch": 1.5506818501335582,
      "grad_norm": 1.4352951049804688,
      "learning_rate": 0.00013988923740932525,
      "loss": 1.0551,
      "step": 11030
    },
    {
      "epoch": 1.5508224377899622,
      "grad_norm": 1.3864407539367676,
      "learning_rate": 0.0001401023034295316,
      "loss": 1.0255,
      "step": 11031
    },
    {
      "epoch": 1.5509630254463658,
      "grad_norm": 1.4770947694778442,
      "learning_rate": 0.00014031515272871633,
      "loss": 0.9424,
      "step": 11032
    },
    {
      "epoch": 1.5511036131027696,
      "grad_norm": 1.5918058156967163,
      "learning_rate": 0.00014052778415659875,
      "loss": 1.1627,
      "step": 11033
    },
    {
      "epoch": 1.5512442007591734,
      "grad_norm": 1.4650038480758667,
      "learning_rate": 0.0001407401965640743,
      "loss": 1.0962,
      "step": 11034
    },
    {
      "epoch": 1.5513847884155771,
      "grad_norm": 1.634562611579895,
      "learning_rate": 0.00014095238880322398,
      "loss": 1.2056,
      "step": 11035
    },
    {
      "epoch": 1.551525376071981,
      "grad_norm": 1.4256668090820312,
      "learning_rate": 0.00014116435972731742,
      "loss": 1.0386,
      "step": 11036
    },
    {
      "epoch": 1.5516659637283845,
      "grad_norm": 1.5408961772918701,
      "learning_rate": 0.00014137610819082053,
      "loss": 1.0862,
      "step": 11037
    },
    {
      "epoch": 1.5518065513847885,
      "grad_norm": 1.8940584659576416,
      "learning_rate": 0.0001415876330494014,
      "loss": 1.1264,
      "step": 11038
    },
    {
      "epoch": 1.551947139041192,
      "grad_norm": 1.7989847660064697,
      "learning_rate": 0.00014179893315993664,
      "loss": 1.0825,
      "step": 11039
    },
    {
      "epoch": 1.552087726697596,
      "grad_norm": 1.6124279499053955,
      "learning_rate": 0.00014201000738051732,
      "loss": 1.0235,
      "step": 11040
    },
    {
      "epoch": 1.5522283143539997,
      "grad_norm": 1.3548918962478638,
      "learning_rate": 0.0001422208545704554,
      "loss": 1.1828,
      "step": 11041
    },
    {
      "epoch": 1.5523689020104035,
      "grad_norm": 1.436309576034546,
      "learning_rate": 0.00014243147359028964,
      "loss": 1.1307,
      "step": 11042
    },
    {
      "epoch": 1.5525094896668072,
      "grad_norm": 1.449833869934082,
      "learning_rate": 0.00014264186330179197,
      "loss": 1.0134,
      "step": 11043
    },
    {
      "epoch": 1.552650077323211,
      "grad_norm": 1.7385622262954712,
      "learning_rate": 0.00014285202256797346,
      "loss": 1.2928,
      "step": 11044
    },
    {
      "epoch": 1.5527906649796148,
      "grad_norm": 1.529418706893921,
      "learning_rate": 0.00014306195025309032,
      "loss": 1.1677,
      "step": 11045
    },
    {
      "epoch": 1.5529312526360186,
      "grad_norm": 1.6532604694366455,
      "learning_rate": 0.00014327164522265112,
      "loss": 1.0852,
      "step": 11046
    },
    {
      "epoch": 1.5530718402924224,
      "grad_norm": 1.7179001569747925,
      "learning_rate": 0.00014348110634342126,
      "loss": 1.0654,
      "step": 11047
    },
    {
      "epoch": 1.553212427948826,
      "grad_norm": 1.5562342405319214,
      "learning_rate": 0.00014369033248343046,
      "loss": 1.018,
      "step": 11048
    },
    {
      "epoch": 1.55335301560523,
      "grad_norm": 1.4413845539093018,
      "learning_rate": 0.00014389932251197734,
      "loss": 0.9592,
      "step": 11049
    },
    {
      "epoch": 1.5534936032616335,
      "grad_norm": 1.5695236921310425,
      "learning_rate": 0.0001441080752996372,
      "loss": 1.0206,
      "step": 11050
    },
    {
      "epoch": 1.5536341909180376,
      "grad_norm": 1.829594373703003,
      "learning_rate": 0.00014431658971826808,
      "loss": 1.0596,
      "step": 11051
    },
    {
      "epoch": 1.5537747785744411,
      "grad_norm": 1.7348949909210205,
      "learning_rate": 0.00014452486464101524,
      "loss": 1.0117,
      "step": 11052
    },
    {
      "epoch": 1.553915366230845,
      "grad_norm": 1.4450081586837769,
      "learning_rate": 0.00014473289894231854,
      "loss": 1.2867,
      "step": 11053
    },
    {
      "epoch": 1.5540559538872487,
      "grad_norm": 1.6997029781341553,
      "learning_rate": 0.0001449406914979182,
      "loss": 0.957,
      "step": 11054
    },
    {
      "epoch": 1.5541965415436525,
      "grad_norm": 1.7091313600540161,
      "learning_rate": 0.0001451482411848609,
      "loss": 1.0651,
      "step": 11055
    },
    {
      "epoch": 1.5543371292000563,
      "grad_norm": 1.5990012884140015,
      "learning_rate": 0.00014535554688150583,
      "loss": 1.1458,
      "step": 11056
    },
    {
      "epoch": 1.5544777168564599,
      "grad_norm": 2.093473434448242,
      "learning_rate": 0.00014556260746753077,
      "loss": 1.013,
      "step": 11057
    },
    {
      "epoch": 1.5546183045128639,
      "grad_norm": 1.5013878345489502,
      "learning_rate": 0.00014576942182393808,
      "loss": 1.1487,
      "step": 11058
    },
    {
      "epoch": 1.5547588921692674,
      "grad_norm": 1.9742430448532104,
      "learning_rate": 0.00014597598883306085,
      "loss": 0.9545,
      "step": 11059
    },
    {
      "epoch": 1.5548994798256714,
      "grad_norm": 1.8699086904525757,
      "learning_rate": 0.00014618230737856885,
      "loss": 0.9002,
      "step": 11060
    },
    {
      "epoch": 1.555040067482075,
      "grad_norm": 1.5444469451904297,
      "learning_rate": 0.00014638837634547427,
      "loss": 0.9144,
      "step": 11061
    },
    {
      "epoch": 1.5551806551384788,
      "grad_norm": 1.6798226833343506,
      "learning_rate": 0.00014659419462013907,
      "loss": 1.0944,
      "step": 11062
    },
    {
      "epoch": 1.5553212427948826,
      "grad_norm": 1.6341110467910767,
      "learning_rate": 0.00014679976109027932,
      "loss": 1.171,
      "step": 11063
    },
    {
      "epoch": 1.5554618304512864,
      "grad_norm": 1.4733262062072754,
      "learning_rate": 0.0001470050746449713,
      "loss": 1.1513,
      "step": 11064
    },
    {
      "epoch": 1.5556024181076902,
      "grad_norm": 1.7260534763336182,
      "learning_rate": 0.00014721013417465904,
      "loss": 1.1137,
      "step": 11065
    },
    {
      "epoch": 1.555743005764094,
      "grad_norm": 1.63914954662323,
      "learning_rate": 0.000147414938571159,
      "loss": 1.0848,
      "step": 11066
    },
    {
      "epoch": 1.5558835934204978,
      "grad_norm": 1.4914166927337646,
      "learning_rate": 0.0001476194867276662,
      "loss": 1.1495,
      "step": 11067
    },
    {
      "epoch": 1.5560241810769013,
      "grad_norm": 1.4882532358169556,
      "learning_rate": 0.00014782377753876132,
      "loss": 0.895,
      "step": 11068
    },
    {
      "epoch": 1.5561647687333053,
      "grad_norm": 1.7343051433563232,
      "learning_rate": 0.00014802780990041492,
      "loss": 1.0951,
      "step": 11069
    },
    {
      "epoch": 1.556305356389709,
      "grad_norm": 1.4633737802505493,
      "learning_rate": 0.00014823158270999443,
      "loss": 0.9537,
      "step": 11070
    },
    {
      "epoch": 1.556445944046113,
      "grad_norm": 1.7978973388671875,
      "learning_rate": 0.00014843509486627045,
      "loss": 1.0015,
      "step": 11071
    },
    {
      "epoch": 1.5565865317025165,
      "grad_norm": 1.5419678688049316,
      "learning_rate": 0.00014863834526942084,
      "loss": 1.0331,
      "step": 11072
    },
    {
      "epoch": 1.5567271193589203,
      "grad_norm": 1.6077638864517212,
      "learning_rate": 0.0001488413328210399,
      "loss": 1.0891,
      "step": 11073
    },
    {
      "epoch": 1.556867707015324,
      "grad_norm": 1.7321844100952148,
      "learning_rate": 0.00014904405642414127,
      "loss": 1.2312,
      "step": 11074
    },
    {
      "epoch": 1.5570082946717279,
      "grad_norm": 1.3889403343200684,
      "learning_rate": 0.00014924651498316523,
      "loss": 1.2621,
      "step": 11075
    },
    {
      "epoch": 1.5571488823281316,
      "grad_norm": 1.8870141506195068,
      "learning_rate": 0.00014944870740398448,
      "loss": 1.0237,
      "step": 11076
    },
    {
      "epoch": 1.5572894699845352,
      "grad_norm": 1.4822747707366943,
      "learning_rate": 0.0001496506325939097,
      "loss": 1.2032,
      "step": 11077
    },
    {
      "epoch": 1.5574300576409392,
      "grad_norm": 1.3741190433502197,
      "learning_rate": 0.00014985228946169681,
      "loss": 1.1876,
      "step": 11078
    },
    {
      "epoch": 1.5575706452973428,
      "grad_norm": 1.3865041732788086,
      "learning_rate": 0.00015005367691755023,
      "loss": 1.0684,
      "step": 11079
    },
    {
      "epoch": 1.5577112329537468,
      "grad_norm": 1.4177765846252441,
      "learning_rate": 0.00015025479387313127,
      "loss": 1.0878,
      "step": 11080
    },
    {
      "epoch": 1.5578518206101504,
      "grad_norm": 1.608927845954895,
      "learning_rate": 0.00015045563924156297,
      "loss": 1.0274,
      "step": 11081
    },
    {
      "epoch": 1.5579924082665542,
      "grad_norm": 1.5365089178085327,
      "learning_rate": 0.00015065621193743607,
      "loss": 0.9887,
      "step": 11082
    },
    {
      "epoch": 1.558132995922958,
      "grad_norm": 1.3430389165878296,
      "learning_rate": 0.00015085651087681442,
      "loss": 1.1225,
      "step": 11083
    },
    {
      "epoch": 1.5582735835793617,
      "grad_norm": 1.6678038835525513,
      "learning_rate": 0.00015105653497724246,
      "loss": 1.0662,
      "step": 11084
    },
    {
      "epoch": 1.5584141712357655,
      "grad_norm": 1.5528711080551147,
      "learning_rate": 0.00015125628315774877,
      "loss": 1.0676,
      "step": 11085
    },
    {
      "epoch": 1.5585547588921693,
      "grad_norm": 1.5715298652648926,
      "learning_rate": 0.00015145575433885382,
      "loss": 0.8953,
      "step": 11086
    },
    {
      "epoch": 1.5586953465485731,
      "grad_norm": 1.6108944416046143,
      "learning_rate": 0.00015165494744257408,
      "loss": 0.908,
      "step": 11087
    },
    {
      "epoch": 1.5588359342049767,
      "grad_norm": 1.6524218320846558,
      "learning_rate": 0.00015185386139242942,
      "loss": 1.0387,
      "step": 11088
    },
    {
      "epoch": 1.5589765218613807,
      "grad_norm": 1.649186372756958,
      "learning_rate": 0.0001520524951134489,
      "loss": 1.1644,
      "step": 11089
    },
    {
      "epoch": 1.5591171095177843,
      "grad_norm": 1.467983603477478,
      "learning_rate": 0.00015225084753217513,
      "loss": 1.1788,
      "step": 11090
    },
    {
      "epoch": 1.559257697174188,
      "grad_norm": 1.6878776550292969,
      "learning_rate": 0.00015244891757667118,
      "loss": 1.098,
      "step": 11091
    },
    {
      "epoch": 1.5593982848305918,
      "grad_norm": 1.573547124862671,
      "learning_rate": 0.00015264670417652621,
      "loss": 1.0305,
      "step": 11092
    },
    {
      "epoch": 1.5595388724869956,
      "grad_norm": 1.7527406215667725,
      "learning_rate": 0.00015284420626286073,
      "loss": 1.1422,
      "step": 11093
    },
    {
      "epoch": 1.5596794601433994,
      "grad_norm": 1.423065185546875,
      "learning_rate": 0.0001530414227683342,
      "loss": 1.1939,
      "step": 11094
    },
    {
      "epoch": 1.5598200477998032,
      "grad_norm": 1.7648426294326782,
      "learning_rate": 0.00015323835262714747,
      "loss": 1.0189,
      "step": 11095
    },
    {
      "epoch": 1.559960635456207,
      "grad_norm": 1.461107850074768,
      "learning_rate": 0.00015343499477505176,
      "loss": 1.2003,
      "step": 11096
    },
    {
      "epoch": 1.5601012231126106,
      "grad_norm": 1.5262997150421143,
      "learning_rate": 0.00015363134814935282,
      "loss": 1.0673,
      "step": 11097
    },
    {
      "epoch": 1.5602418107690146,
      "grad_norm": 1.3440932035446167,
      "learning_rate": 0.00015382741168891702,
      "loss": 1.0284,
      "step": 11098
    },
    {
      "epoch": 1.5603823984254181,
      "grad_norm": 1.4746657609939575,
      "learning_rate": 0.00015402318433417668,
      "loss": 1.1823,
      "step": 11099
    },
    {
      "epoch": 1.5605229860818222,
      "grad_norm": 1.4558287858963013,
      "learning_rate": 0.00015421866502713717,
      "loss": 1.0475,
      "step": 11100
    },
    {
      "epoch": 1.5606635737382257,
      "grad_norm": 1.524864912033081,
      "learning_rate": 0.00015441385271138094,
      "loss": 0.9909,
      "step": 11101
    },
    {
      "epoch": 1.5608041613946295,
      "grad_norm": 1.505613923072815,
      "learning_rate": 0.00015460874633207338,
      "loss": 0.9586,
      "step": 11102
    },
    {
      "epoch": 1.5609447490510333,
      "grad_norm": 1.5383111238479614,
      "learning_rate": 0.00015480334483596993,
      "loss": 1.0386,
      "step": 11103
    },
    {
      "epoch": 1.561085336707437,
      "grad_norm": 1.649477481842041,
      "learning_rate": 0.00015499764717142033,
      "loss": 1.0323,
      "step": 11104
    },
    {
      "epoch": 1.561225924363841,
      "grad_norm": 1.6791261434555054,
      "learning_rate": 0.00015519165228837576,
      "loss": 1.0067,
      "step": 11105
    },
    {
      "epoch": 1.5613665120202445,
      "grad_norm": 1.5962741374969482,
      "learning_rate": 0.0001553853591383927,
      "loss": 1.0327,
      "step": 11106
    },
    {
      "epoch": 1.5615070996766485,
      "grad_norm": 1.3187850713729858,
      "learning_rate": 0.0001555787666746398,
      "loss": 1.0849,
      "step": 11107
    },
    {
      "epoch": 1.561647687333052,
      "grad_norm": 1.5639359951019287,
      "learning_rate": 0.00015577187385190343,
      "loss": 1.2593,
      "step": 11108
    },
    {
      "epoch": 1.561788274989456,
      "grad_norm": 1.4978466033935547,
      "learning_rate": 0.00015596467962659328,
      "loss": 1.1876,
      "step": 11109
    },
    {
      "epoch": 1.5619288626458596,
      "grad_norm": 1.52152681350708,
      "learning_rate": 0.00015615718295674676,
      "loss": 1.1281,
      "step": 11110
    },
    {
      "epoch": 1.5620694503022634,
      "grad_norm": 1.653485655784607,
      "learning_rate": 0.0001563493828020375,
      "loss": 1.044,
      "step": 11111
    },
    {
      "epoch": 1.5622100379586672,
      "grad_norm": 1.4081062078475952,
      "learning_rate": 0.0001565412781237781,
      "loss": 1.0479,
      "step": 11112
    },
    {
      "epoch": 1.562350625615071,
      "grad_norm": 1.6085258722305298,
      "learning_rate": 0.0001567328678849271,
      "loss": 1.0885,
      "step": 11113
    },
    {
      "epoch": 1.5624912132714748,
      "grad_norm": 1.490868330001831,
      "learning_rate": 0.0001569241510500944,
      "loss": 1.2486,
      "step": 11114
    },
    {
      "epoch": 1.5626318009278786,
      "grad_norm": 1.6893823146820068,
      "learning_rate": 0.0001571151265855465,
      "loss": 1.1773,
      "step": 11115
    },
    {
      "epoch": 1.5627723885842824,
      "grad_norm": 1.6487082242965698,
      "learning_rate": 0.00015730579345921353,
      "loss": 1.2632,
      "step": 11116
    },
    {
      "epoch": 1.562912976240686,
      "grad_norm": 1.5757665634155273,
      "learning_rate": 0.00015749615064069198,
      "loss": 0.9071,
      "step": 11117
    },
    {
      "epoch": 1.56305356389709,
      "grad_norm": 1.403390645980835,
      "learning_rate": 0.00015768619710125313,
      "loss": 1.1274,
      "step": 11118
    },
    {
      "epoch": 1.5631941515534935,
      "grad_norm": 1.5037916898727417,
      "learning_rate": 0.00015787593181384722,
      "loss": 1.2797,
      "step": 11119
    },
    {
      "epoch": 1.5633347392098975,
      "grad_norm": 1.6761560440063477,
      "learning_rate": 0.0001580653537531088,
      "loss": 0.9393,
      "step": 11120
    },
    {
      "epoch": 1.563475326866301,
      "grad_norm": 1.8491538763046265,
      "learning_rate": 0.00015825446189536376,
      "loss": 0.9585,
      "step": 11121
    },
    {
      "epoch": 1.5636159145227049,
      "grad_norm": 1.6531749963760376,
      "learning_rate": 0.00015844325521863282,
      "loss": 1.1532,
      "step": 11122
    },
    {
      "epoch": 1.5637565021791087,
      "grad_norm": 1.384067177772522,
      "learning_rate": 0.00015863173270263845,
      "loss": 1.061,
      "step": 11123
    },
    {
      "epoch": 1.5638970898355125,
      "grad_norm": 1.5770621299743652,
      "learning_rate": 0.00015881989332881013,
      "loss": 0.9744,
      "step": 11124
    },
    {
      "epoch": 1.5640376774919162,
      "grad_norm": 1.8136926889419556,
      "learning_rate": 0.0001590077360802889,
      "loss": 1.0835,
      "step": 11125
    },
    {
      "epoch": 1.5641782651483198,
      "grad_norm": 1.5765408277511597,
      "learning_rate": 0.00015919525994193427,
      "loss": 0.9247,
      "step": 11126
    },
    {
      "epoch": 1.5643188528047238,
      "grad_norm": 1.8908796310424805,
      "learning_rate": 0.00015938246390032968,
      "loss": 0.9944,
      "step": 11127
    },
    {
      "epoch": 1.5644594404611274,
      "grad_norm": 1.5184874534606934,
      "learning_rate": 0.00015956934694378645,
      "loss": 1.1516,
      "step": 11128
    },
    {
      "epoch": 1.5646000281175314,
      "grad_norm": 1.5528875589370728,
      "learning_rate": 0.0001597559080623505,
      "loss": 1.2341,
      "step": 11129
    },
    {
      "epoch": 1.564740615773935,
      "grad_norm": 1.4913010597229004,
      "learning_rate": 0.00015994214624780756,
      "loss": 1.1073,
      "step": 11130
    },
    {
      "epoch": 1.5648812034303388,
      "grad_norm": 1.5197783708572388,
      "learning_rate": 0.00016012806049368826,
      "loss": 1.0967,
      "step": 11131
    },
    {
      "epoch": 1.5650217910867426,
      "grad_norm": 1.7276352643966675,
      "learning_rate": 0.0001603136497952749,
      "loss": 0.9803,
      "step": 11132
    },
    {
      "epoch": 1.5651623787431463,
      "grad_norm": 1.6893566846847534,
      "learning_rate": 0.00016049891314960424,
      "loss": 1.1225,
      "step": 11133
    },
    {
      "epoch": 1.5653029663995501,
      "grad_norm": 1.6628031730651855,
      "learning_rate": 0.00016068384955547567,
      "loss": 0.929,
      "step": 11134
    },
    {
      "epoch": 1.565443554055954,
      "grad_norm": 1.599227786064148,
      "learning_rate": 0.0001608684580134551,
      "loss": 0.9866,
      "step": 11135
    },
    {
      "epoch": 1.5655841417123577,
      "grad_norm": 1.544724702835083,
      "learning_rate": 0.00016105273752588058,
      "loss": 1.112,
      "step": 11136
    },
    {
      "epoch": 1.5657247293687613,
      "grad_norm": 1.5917929410934448,
      "learning_rate": 0.00016123668709686856,
      "loss": 1.0926,
      "step": 11137
    },
    {
      "epoch": 1.5658653170251653,
      "grad_norm": 1.411421775817871,
      "learning_rate": 0.00016142030573231784,
      "loss": 0.9051,
      "step": 11138
    },
    {
      "epoch": 1.5660059046815689,
      "grad_norm": 1.4430694580078125,
      "learning_rate": 0.00016160359243991615,
      "loss": 1.0451,
      "step": 11139
    },
    {
      "epoch": 1.5661464923379729,
      "grad_norm": 1.4049752950668335,
      "learning_rate": 0.00016178654622914416,
      "loss": 1.2572,
      "step": 11140
    },
    {
      "epoch": 1.5662870799943764,
      "grad_norm": 1.8237736225128174,
      "learning_rate": 0.00016196916611128264,
      "loss": 1.1724,
      "step": 11141
    },
    {
      "epoch": 1.5664276676507802,
      "grad_norm": 1.6807763576507568,
      "learning_rate": 0.0001621514510994162,
      "loss": 1.1024,
      "step": 11142
    },
    {
      "epoch": 1.566568255307184,
      "grad_norm": 1.7447583675384521,
      "learning_rate": 0.00016233340020844025,
      "loss": 1.1015,
      "step": 11143
    },
    {
      "epoch": 1.5667088429635878,
      "grad_norm": 1.5665806531906128,
      "learning_rate": 0.00016251501245506434,
      "loss": 1.1439,
      "step": 11144
    },
    {
      "epoch": 1.5668494306199916,
      "grad_norm": 1.6753745079040527,
      "learning_rate": 0.00016269628685781893,
      "loss": 1.1612,
      "step": 11145
    },
    {
      "epoch": 1.5669900182763952,
      "grad_norm": 1.6128814220428467,
      "learning_rate": 0.00016287722243706014,
      "loss": 1.1324,
      "step": 11146
    },
    {
      "epoch": 1.5671306059327992,
      "grad_norm": 1.6709791421890259,
      "learning_rate": 0.0001630578182149753,
      "loss": 1.1035,
      "step": 11147
    },
    {
      "epoch": 1.5672711935892027,
      "grad_norm": 1.597968578338623,
      "learning_rate": 0.00016323807321558798,
      "loss": 1.0256,
      "step": 11148
    },
    {
      "epoch": 1.5674117812456068,
      "grad_norm": 1.5975217819213867,
      "learning_rate": 0.0001634179864647634,
      "loss": 1.118,
      "step": 11149
    },
    {
      "epoch": 1.5675523689020103,
      "grad_norm": 1.9236409664154053,
      "learning_rate": 0.00016359755699021372,
      "loss": 0.9045,
      "step": 11150
    },
    {
      "epoch": 1.5676929565584141,
      "grad_norm": 2.0503313541412354,
      "learning_rate": 0.0001637767838215032,
      "loss": 1.186,
      "step": 11151
    },
    {
      "epoch": 1.567833544214818,
      "grad_norm": 1.5827736854553223,
      "learning_rate": 0.0001639556659900535,
      "loss": 1.0273,
      "step": 11152
    },
    {
      "epoch": 1.5679741318712217,
      "grad_norm": 1.8141555786132812,
      "learning_rate": 0.0001641342025291487,
      "loss": 1.1037,
      "step": 11153
    },
    {
      "epoch": 1.5681147195276255,
      "grad_norm": 1.654831051826477,
      "learning_rate": 0.0001643123924739414,
      "loss": 1.1581,
      "step": 11154
    },
    {
      "epoch": 1.5682553071840293,
      "grad_norm": 1.5658169984817505,
      "learning_rate": 0.00016449023486145693,
      "loss": 1.1032,
      "step": 11155
    },
    {
      "epoch": 1.568395894840433,
      "grad_norm": 1.4446959495544434,
      "learning_rate": 0.00016466772873059818,
      "loss": 1.1916,
      "step": 11156
    },
    {
      "epoch": 1.5685364824968366,
      "grad_norm": 1.4863795042037964,
      "learning_rate": 0.0001648448731221524,
      "loss": 1.079,
      "step": 11157
    },
    {
      "epoch": 1.5686770701532406,
      "grad_norm": 1.4073742628097534,
      "learning_rate": 0.00016502166707879485,
      "loss": 1.028,
      "step": 11158
    },
    {
      "epoch": 1.5688176578096442,
      "grad_norm": 1.4117302894592285,
      "learning_rate": 0.00016519810964509557,
      "loss": 1.183,
      "step": 11159
    },
    {
      "epoch": 1.5689582454660482,
      "grad_norm": 1.6044799089431763,
      "learning_rate": 0.00016537419986752268,
      "loss": 1.139,
      "step": 11160
    },
    {
      "epoch": 1.5690988331224518,
      "grad_norm": 1.5317707061767578,
      "learning_rate": 0.00016554993679444868,
      "loss": 1.0186,
      "step": 11161
    },
    {
      "epoch": 1.5692394207788556,
      "grad_norm": 1.678572177886963,
      "learning_rate": 0.00016572531947615574,
      "loss": 1.1155,
      "step": 11162
    },
    {
      "epoch": 1.5693800084352594,
      "grad_norm": 1.7850255966186523,
      "learning_rate": 0.00016590034696483926,
      "loss": 0.9031,
      "step": 11163
    },
    {
      "epoch": 1.5695205960916632,
      "grad_norm": 1.7122552394866943,
      "learning_rate": 0.0001660750183146158,
      "loss": 1.1179,
      "step": 11164
    },
    {
      "epoch": 1.569661183748067,
      "grad_norm": 1.9504735469818115,
      "learning_rate": 0.00016624933258152554,
      "loss": 1.0348,
      "step": 11165
    },
    {
      "epoch": 1.5698017714044705,
      "grad_norm": 1.7436999082565308,
      "learning_rate": 0.00016642328882353867,
      "loss": 1.1558,
      "step": 11166
    },
    {
      "epoch": 1.5699423590608745,
      "grad_norm": 1.311036467552185,
      "learning_rate": 0.00016659688610056014,
      "loss": 1.2125,
      "step": 11167
    },
    {
      "epoch": 1.570082946717278,
      "grad_norm": 1.6858413219451904,
      "learning_rate": 0.00016677012347443495,
      "loss": 1.021,
      "step": 11168
    },
    {
      "epoch": 1.5702235343736821,
      "grad_norm": 1.5909063816070557,
      "learning_rate": 0.00016694300000895274,
      "loss": 1.1373,
      "step": 11169
    },
    {
      "epoch": 1.5703641220300857,
      "grad_norm": 1.5430397987365723,
      "learning_rate": 0.0001671155147698542,
      "loss": 1.0461,
      "step": 11170
    },
    {
      "epoch": 1.5705047096864895,
      "grad_norm": 1.4151294231414795,
      "learning_rate": 0.00016728766682483373,
      "loss": 1.0108,
      "step": 11171
    },
    {
      "epoch": 1.5706452973428933,
      "grad_norm": 1.4874473810195923,
      "learning_rate": 0.00016745945524354673,
      "loss": 1.0543,
      "step": 11172
    },
    {
      "epoch": 1.570785884999297,
      "grad_norm": 1.5033719539642334,
      "learning_rate": 0.00016763087909761352,
      "loss": 1.0786,
      "step": 11173
    },
    {
      "epoch": 1.5709264726557008,
      "grad_norm": 1.6680091619491577,
      "learning_rate": 0.00016780193746062448,
      "loss": 1.14,
      "step": 11174
    },
    {
      "epoch": 1.5710670603121046,
      "grad_norm": 1.4858533143997192,
      "learning_rate": 0.00016797262940814584,
      "loss": 1.1438,
      "step": 11175
    },
    {
      "epoch": 1.5712076479685084,
      "grad_norm": 1.5834181308746338,
      "learning_rate": 0.00016814295401772328,
      "loss": 1.0161,
      "step": 11176
    },
    {
      "epoch": 1.571348235624912,
      "grad_norm": 1.5913559198379517,
      "learning_rate": 0.00016831291036888813,
      "loss": 1.0451,
      "step": 11177
    },
    {
      "epoch": 1.571488823281316,
      "grad_norm": 1.4309613704681396,
      "learning_rate": 0.0001684824975431611,
      "loss": 1.0596,
      "step": 11178
    },
    {
      "epoch": 1.5716294109377196,
      "grad_norm": 1.7909436225891113,
      "learning_rate": 0.00016865171462405852,
      "loss": 1.124,
      "step": 11179
    },
    {
      "epoch": 1.5717699985941236,
      "grad_norm": 1.7870521545410156,
      "learning_rate": 0.0001688205606970974,
      "loss": 1.3937,
      "step": 11180
    },
    {
      "epoch": 1.5719105862505272,
      "grad_norm": 1.5628060102462769,
      "learning_rate": 0.00016898903484979878,
      "loss": 1.1762,
      "step": 11181
    },
    {
      "epoch": 1.572051173906931,
      "grad_norm": 1.5643409490585327,
      "learning_rate": 0.00016915713617169406,
      "loss": 0.9354,
      "step": 11182
    },
    {
      "epoch": 1.5721917615633347,
      "grad_norm": 1.4956519603729248,
      "learning_rate": 0.00016932486375432942,
      "loss": 1.1602,
      "step": 11183
    },
    {
      "epoch": 1.5723323492197385,
      "grad_norm": 1.8895618915557861,
      "learning_rate": 0.0001694922166912708,
      "loss": 1.0992,
      "step": 11184
    },
    {
      "epoch": 1.5724729368761423,
      "grad_norm": 1.663021445274353,
      "learning_rate": 0.00016965919407810885,
      "loss": 1.2989,
      "step": 11185
    },
    {
      "epoch": 1.5726135245325459,
      "grad_norm": 1.5842223167419434,
      "learning_rate": 0.0001698257950124637,
      "loss": 0.9748,
      "step": 11186
    },
    {
      "epoch": 1.57275411218895,
      "grad_norm": 1.562410593032837,
      "learning_rate": 0.00016999201859398998,
      "loss": 1.1696,
      "step": 11187
    },
    {
      "epoch": 1.5728946998453535,
      "grad_norm": 1.4078700542449951,
      "learning_rate": 0.00017015786392438157,
      "loss": 1.0809,
      "step": 11188
    },
    {
      "epoch": 1.5730352875017575,
      "grad_norm": 1.8666155338287354,
      "learning_rate": 0.00017032333010737647,
      "loss": 1.0558,
      "step": 11189
    },
    {
      "epoch": 1.573175875158161,
      "grad_norm": 1.6922121047973633,
      "learning_rate": 0.00017048841624876145,
      "loss": 0.9726,
      "step": 11190
    },
    {
      "epoch": 1.5733164628145648,
      "grad_norm": 1.405645728111267,
      "learning_rate": 0.00017065312145637788,
      "loss": 1.1648,
      "step": 11191
    },
    {
      "epoch": 1.5734570504709686,
      "grad_norm": 1.5178159475326538,
      "learning_rate": 0.000170817444840125,
      "loss": 1.2206,
      "step": 11192
    },
    {
      "epoch": 1.5735976381273724,
      "grad_norm": 1.5494074821472168,
      "learning_rate": 0.000170981385511966,
      "loss": 1.1294,
      "step": 11193
    },
    {
      "epoch": 1.5737382257837762,
      "grad_norm": 1.7755677700042725,
      "learning_rate": 0.00017114494258593156,
      "loss": 1.2105,
      "step": 11194
    },
    {
      "epoch": 1.57387881344018,
      "grad_norm": 1.5380120277404785,
      "learning_rate": 0.00017130811517812616,
      "loss": 1.0976,
      "step": 11195
    },
    {
      "epoch": 1.5740194010965838,
      "grad_norm": 1.7413156032562256,
      "learning_rate": 0.00017147090240673163,
      "loss": 0.898,
      "step": 11196
    },
    {
      "epoch": 1.5741599887529873,
      "grad_norm": 1.526593804359436,
      "learning_rate": 0.00017163330339201307,
      "loss": 1.1126,
      "step": 11197
    },
    {
      "epoch": 1.5743005764093914,
      "grad_norm": 1.6513755321502686,
      "learning_rate": 0.0001717953172563222,
      "loss": 1.0605,
      "step": 11198
    },
    {
      "epoch": 1.574441164065795,
      "grad_norm": 1.749165415763855,
      "learning_rate": 0.00017195694312410314,
      "loss": 1.2122,
      "step": 11199
    },
    {
      "epoch": 1.574581751722199,
      "grad_norm": 1.5772275924682617,
      "learning_rate": 0.00017211818012189694,
      "loss": 1.059,
      "step": 11200
    },
    {
      "epoch": 1.5747223393786025,
      "grad_norm": 1.4918162822723389,
      "learning_rate": 0.00017227902737834526,
      "loss": 1.1262,
      "step": 11201
    },
    {
      "epoch": 1.5748629270350063,
      "grad_norm": 1.5880168676376343,
      "learning_rate": 0.00017243948402419755,
      "loss": 1.1034,
      "step": 11202
    },
    {
      "epoch": 1.57500351469141,
      "grad_norm": 1.7761470079421997,
      "learning_rate": 0.00017259954919231305,
      "loss": 0.962,
      "step": 11203
    },
    {
      "epoch": 1.5751441023478139,
      "grad_norm": 1.642108678817749,
      "learning_rate": 0.000172759222017667,
      "loss": 1.0274,
      "step": 11204
    },
    {
      "epoch": 1.5752846900042177,
      "grad_norm": 1.4864747524261475,
      "learning_rate": 0.00017291850163735493,
      "loss": 1.1836,
      "step": 11205
    },
    {
      "epoch": 1.5754252776606212,
      "grad_norm": 1.430097222328186,
      "learning_rate": 0.00017307738719059707,
      "loss": 1.0247,
      "step": 11206
    },
    {
      "epoch": 1.5755658653170252,
      "grad_norm": 1.4682739973068237,
      "learning_rate": 0.00017323587781874398,
      "loss": 1.0549,
      "step": 11207
    },
    {
      "epoch": 1.5757064529734288,
      "grad_norm": 1.5778555870056152,
      "learning_rate": 0.00017339397266527998,
      "loss": 1.1896,
      "step": 11208
    },
    {
      "epoch": 1.5758470406298328,
      "grad_norm": 1.5291752815246582,
      "learning_rate": 0.00017355167087582782,
      "loss": 1.2066,
      "step": 11209
    },
    {
      "epoch": 1.5759876282862364,
      "grad_norm": 1.4719704389572144,
      "learning_rate": 0.00017370897159815448,
      "loss": 1.046,
      "step": 11210
    },
    {
      "epoch": 1.5761282159426402,
      "grad_norm": 2.2798221111297607,
      "learning_rate": 0.0001738658739821748,
      "loss": 1.0068,
      "step": 11211
    },
    {
      "epoch": 1.576268803599044,
      "grad_norm": 1.2899715900421143,
      "learning_rate": 0.00017402237717995607,
      "loss": 1.3172,
      "step": 11212
    },
    {
      "epoch": 1.5764093912554478,
      "grad_norm": 1.7105344533920288,
      "learning_rate": 0.00017417848034572355,
      "loss": 1.0626,
      "step": 11213
    },
    {
      "epoch": 1.5765499789118516,
      "grad_norm": 1.3362653255462646,
      "learning_rate": 0.00017433418263586374,
      "loss": 1.3319,
      "step": 11214
    },
    {
      "epoch": 1.5766905665682553,
      "grad_norm": 1.6927449703216553,
      "learning_rate": 0.00017448948320892993,
      "loss": 1.1647,
      "step": 11215
    },
    {
      "epoch": 1.5768311542246591,
      "grad_norm": 1.6760843992233276,
      "learning_rate": 0.0001746443812256457,
      "loss": 1.3081,
      "step": 11216
    },
    {
      "epoch": 1.5769717418810627,
      "grad_norm": 1.5210741758346558,
      "learning_rate": 0.00017479887584891056,
      "loss": 1.2272,
      "step": 11217
    },
    {
      "epoch": 1.5771123295374667,
      "grad_norm": 1.3432996273040771,
      "learning_rate": 0.00017495296624380464,
      "loss": 1.31,
      "step": 11218
    },
    {
      "epoch": 1.5772529171938703,
      "grad_norm": 1.4189338684082031,
      "learning_rate": 0.00017510665157759173,
      "loss": 1.05,
      "step": 11219
    },
    {
      "epoch": 1.5773935048502743,
      "grad_norm": 1.5245953798294067,
      "learning_rate": 0.00017525993101972495,
      "loss": 1.1187,
      "step": 11220
    },
    {
      "epoch": 1.5775340925066779,
      "grad_norm": 1.6571815013885498,
      "learning_rate": 0.000175412803741851,
      "loss": 1.2836,
      "step": 11221
    },
    {
      "epoch": 1.5776746801630817,
      "grad_norm": 1.3343816995620728,
      "learning_rate": 0.00017556526891781448,
      "loss": 1.2244,
      "step": 11222
    },
    {
      "epoch": 1.5778152678194854,
      "grad_norm": 1.6865217685699463,
      "learning_rate": 0.00017571732572366257,
      "loss": 1.004,
      "step": 11223
    },
    {
      "epoch": 1.5779558554758892,
      "grad_norm": 1.66984224319458,
      "learning_rate": 0.00017586897333764925,
      "loss": 1.0022,
      "step": 11224
    },
    {
      "epoch": 1.578096443132293,
      "grad_norm": 1.7683607339859009,
      "learning_rate": 0.0001760202109402399,
      "loss": 1.1699,
      "step": 11225
    },
    {
      "epoch": 1.5782370307886966,
      "grad_norm": 1.4700138568878174,
      "learning_rate": 0.00017617103771411572,
      "loss": 1.179,
      "step": 11226
    },
    {
      "epoch": 1.5783776184451006,
      "grad_norm": 1.4324924945831299,
      "learning_rate": 0.00017632145284417807,
      "loss": 1.2238,
      "step": 11227
    },
    {
      "epoch": 1.5785182061015042,
      "grad_norm": 1.5654125213623047,
      "learning_rate": 0.00017647145551755267,
      "loss": 1.144,
      "step": 11228
    },
    {
      "epoch": 1.5786587937579082,
      "grad_norm": 1.4184354543685913,
      "learning_rate": 0.000176621044923595,
      "loss": 1.2607,
      "step": 11229
    },
    {
      "epoch": 1.5787993814143118,
      "grad_norm": 1.5921906232833862,
      "learning_rate": 0.00017677022025389317,
      "loss": 1.0948,
      "step": 11230
    },
    {
      "epoch": 1.5789399690707155,
      "grad_norm": 1.9256610870361328,
      "learning_rate": 0.0001769189807022735,
      "loss": 1.0032,
      "step": 11231
    },
    {
      "epoch": 1.5790805567271193,
      "grad_norm": 1.5890194177627563,
      "learning_rate": 0.00017706732546480378,
      "loss": 1.0441,
      "step": 11232
    },
    {
      "epoch": 1.5792211443835231,
      "grad_norm": 1.7362010478973389,
      "learning_rate": 0.0001772152537397987,
      "loss": 0.9339,
      "step": 11233
    },
    {
      "epoch": 1.579361732039927,
      "grad_norm": 1.5046957731246948,
      "learning_rate": 0.00017736276472782406,
      "loss": 1.0981,
      "step": 11234
    },
    {
      "epoch": 1.5795023196963307,
      "grad_norm": 1.4906485080718994,
      "learning_rate": 0.0001775098576317003,
      "loss": 1.1677,
      "step": 11235
    },
    {
      "epoch": 1.5796429073527345,
      "grad_norm": 1.669114112854004,
      "learning_rate": 0.00017765653165650736,
      "loss": 1.1523,
      "step": 11236
    },
    {
      "epoch": 1.579783495009138,
      "grad_norm": 1.7559677362442017,
      "learning_rate": 0.00017780278600958887,
      "loss": 1.0017,
      "step": 11237
    },
    {
      "epoch": 1.579924082665542,
      "grad_norm": 1.6552928686141968,
      "learning_rate": 0.00017794861990055673,
      "loss": 1.129,
      "step": 11238
    },
    {
      "epoch": 1.5800646703219456,
      "grad_norm": 1.7965080738067627,
      "learning_rate": 0.00017809403254129424,
      "loss": 1.0063,
      "step": 11239
    },
    {
      "epoch": 1.5802052579783497,
      "grad_norm": 1.594189167022705,
      "learning_rate": 0.00017823902314596248,
      "loss": 1.0715,
      "step": 11240
    },
    {
      "epoch": 1.5803458456347532,
      "grad_norm": 1.8257075548171997,
      "learning_rate": 0.00017838359093100248,
      "loss": 1.097,
      "step": 11241
    },
    {
      "epoch": 1.580486433291157,
      "grad_norm": 2.0087623596191406,
      "learning_rate": 0.00017852773511514044,
      "loss": 1.2322,
      "step": 11242
    },
    {
      "epoch": 1.5806270209475608,
      "grad_norm": 1.667259931564331,
      "learning_rate": 0.0001786714549193918,
      "loss": 1.1315,
      "step": 11243
    },
    {
      "epoch": 1.5807676086039646,
      "grad_norm": 1.5380561351776123,
      "learning_rate": 0.00017881474956706525,
      "loss": 1.1122,
      "step": 11244
    },
    {
      "epoch": 1.5809081962603684,
      "grad_norm": 1.568600058555603,
      "learning_rate": 0.00017895761828376773,
      "loss": 1.2114,
      "step": 11245
    },
    {
      "epoch": 1.581048783916772,
      "grad_norm": 1.5656474828720093,
      "learning_rate": 0.00017910006029740757,
      "loss": 1.1282,
      "step": 11246
    },
    {
      "epoch": 1.581189371573176,
      "grad_norm": 1.7158927917480469,
      "learning_rate": 0.00017924207483819868,
      "loss": 1.0439,
      "step": 11247
    },
    {
      "epoch": 1.5813299592295795,
      "grad_norm": 1.4788751602172852,
      "learning_rate": 0.0001793836611386657,
      "loss": 0.9825,
      "step": 11248
    },
    {
      "epoch": 1.5814705468859835,
      "grad_norm": 1.636396884918213,
      "learning_rate": 0.00017952481843364727,
      "loss": 1.054,
      "step": 11249
    },
    {
      "epoch": 1.581611134542387,
      "grad_norm": 2.067760467529297,
      "learning_rate": 0.000179665545960301,
      "loss": 0.9738,
      "step": 11250
    },
    {
      "epoch": 1.581751722198791,
      "grad_norm": 1.7092480659484863,
      "learning_rate": 0.00017980584295810637,
      "loss": 1.0734,
      "step": 11251
    },
    {
      "epoch": 1.5818923098551947,
      "grad_norm": 1.5570095777511597,
      "learning_rate": 0.00017994570866886986,
      "loss": 1.0905,
      "step": 11252
    },
    {
      "epoch": 1.5820328975115985,
      "grad_norm": 1.7285065650939941,
      "learning_rate": 0.0001800851423367287,
      "loss": 1.1351,
      "step": 11253
    },
    {
      "epoch": 1.5821734851680023,
      "grad_norm": 1.4878143072128296,
      "learning_rate": 0.00018022414320815504,
      "loss": 0.9034,
      "step": 11254
    },
    {
      "epoch": 1.582314072824406,
      "grad_norm": 1.7109991312026978,
      "learning_rate": 0.00018036271053195927,
      "loss": 1.0314,
      "step": 11255
    },
    {
      "epoch": 1.5824546604808098,
      "grad_norm": 1.4669198989868164,
      "learning_rate": 0.000180500843559296,
      "loss": 1.1107,
      "step": 11256
    },
    {
      "epoch": 1.5825952481372134,
      "grad_norm": 1.663629174232483,
      "learning_rate": 0.0001806385415436659,
      "loss": 1.1313,
      "step": 11257
    },
    {
      "epoch": 1.5827358357936174,
      "grad_norm": 1.6230056285858154,
      "learning_rate": 0.00018077580374092106,
      "loss": 1.1072,
      "step": 11258
    },
    {
      "epoch": 1.582876423450021,
      "grad_norm": 1.474447250366211,
      "learning_rate": 0.00018091262940926855,
      "loss": 1.1128,
      "step": 11259
    },
    {
      "epoch": 1.583017011106425,
      "grad_norm": 1.7871763706207275,
      "learning_rate": 0.0001810490178092744,
      "loss": 1.1397,
      "step": 11260
    },
    {
      "epoch": 1.5831575987628286,
      "grad_norm": 1.5145716667175293,
      "learning_rate": 0.00018118496820386843,
      "loss": 1.163,
      "step": 11261
    },
    {
      "epoch": 1.5832981864192324,
      "grad_norm": 1.7250721454620361,
      "learning_rate": 0.0001813204798583465,
      "loss": 0.9731,
      "step": 11262
    },
    {
      "epoch": 1.5834387740756362,
      "grad_norm": 1.6067893505096436,
      "learning_rate": 0.00018145555204037613,
      "loss": 1.1432,
      "step": 11263
    },
    {
      "epoch": 1.58357936173204,
      "grad_norm": 1.514653205871582,
      "learning_rate": 0.00018159018401999973,
      "loss": 1.0616,
      "step": 11264
    },
    {
      "epoch": 1.5837199493884437,
      "grad_norm": 1.4139747619628906,
      "learning_rate": 0.00018172437506963867,
      "loss": 1.2991,
      "step": 11265
    },
    {
      "epoch": 1.5838605370448473,
      "grad_norm": 1.7265456914901733,
      "learning_rate": 0.00018185812446409698,
      "loss": 1.068,
      "step": 11266
    },
    {
      "epoch": 1.5840011247012513,
      "grad_norm": 1.8986504077911377,
      "learning_rate": 0.000181991431480566,
      "loss": 1.1201,
      "step": 11267
    },
    {
      "epoch": 1.5841417123576549,
      "grad_norm": 1.695817470550537,
      "learning_rate": 0.0001821242953986273,
      "loss": 1.1387,
      "step": 11268
    },
    {
      "epoch": 1.584282300014059,
      "grad_norm": 1.6203776597976685,
      "learning_rate": 0.00018225671550025735,
      "loss": 1.1485,
      "step": 11269
    },
    {
      "epoch": 1.5844228876704625,
      "grad_norm": 1.6820428371429443,
      "learning_rate": 0.00018238869106983055,
      "loss": 1.2231,
      "step": 11270
    },
    {
      "epoch": 1.5845634753268663,
      "grad_norm": 1.5395708084106445,
      "learning_rate": 0.00018252022139412402,
      "loss": 1.0162,
      "step": 11271
    },
    {
      "epoch": 1.58470406298327,
      "grad_norm": 1.5691232681274414,
      "learning_rate": 0.00018265130576232148,
      "loss": 1.0664,
      "step": 11272
    },
    {
      "epoch": 1.5848446506396738,
      "grad_norm": 1.4601305723190308,
      "learning_rate": 0.00018278194346601606,
      "loss": 1.1694,
      "step": 11273
    },
    {
      "epoch": 1.5849852382960776,
      "grad_norm": 1.7356410026550293,
      "learning_rate": 0.00018291213379921506,
      "loss": 0.9786,
      "step": 11274
    },
    {
      "epoch": 1.5851258259524814,
      "grad_norm": 1.4919729232788086,
      "learning_rate": 0.00018304187605834327,
      "loss": 0.9154,
      "step": 11275
    },
    {
      "epoch": 1.5852664136088852,
      "grad_norm": 1.6968674659729004,
      "learning_rate": 0.00018317116954224717,
      "loss": 1.0316,
      "step": 11276
    },
    {
      "epoch": 1.5854070012652888,
      "grad_norm": 1.606909155845642,
      "learning_rate": 0.00018330001355219838,
      "loss": 1.0781,
      "step": 11277
    },
    {
      "epoch": 1.5855475889216928,
      "grad_norm": 2.066586494445801,
      "learning_rate": 0.0001834284073918976,
      "loss": 1.1005,
      "step": 11278
    },
    {
      "epoch": 1.5856881765780964,
      "grad_norm": 1.720657229423523,
      "learning_rate": 0.00018355635036747839,
      "loss": 1.1865,
      "step": 11279
    },
    {
      "epoch": 1.5858287642345004,
      "grad_norm": 1.5208431482315063,
      "learning_rate": 0.00018368384178751076,
      "loss": 1.1088,
      "step": 11280
    },
    {
      "epoch": 1.585969351890904,
      "grad_norm": 1.5826643705368042,
      "learning_rate": 0.00018381088096300517,
      "loss": 0.9787,
      "step": 11281
    },
    {
      "epoch": 1.5861099395473077,
      "grad_norm": 1.380640983581543,
      "learning_rate": 0.00018393746720741575,
      "loss": 1.0253,
      "step": 11282
    },
    {
      "epoch": 1.5862505272037115,
      "grad_norm": 1.6014409065246582,
      "learning_rate": 0.0001840635998366451,
      "loss": 1.2117,
      "step": 11283
    },
    {
      "epoch": 1.5863911148601153,
      "grad_norm": 1.6450952291488647,
      "learning_rate": 0.00018418927816904674,
      "loss": 1.1598,
      "step": 11284
    },
    {
      "epoch": 1.586531702516519,
      "grad_norm": 1.6066018342971802,
      "learning_rate": 0.00018431450152542897,
      "loss": 1.1499,
      "step": 11285
    },
    {
      "epoch": 1.5866722901729227,
      "grad_norm": 1.690147042274475,
      "learning_rate": 0.00018443926922905943,
      "loss": 1.0303,
      "step": 11286
    },
    {
      "epoch": 1.5868128778293267,
      "grad_norm": 1.5635234117507935,
      "learning_rate": 0.00018456358060566782,
      "loss": 1.1305,
      "step": 11287
    },
    {
      "epoch": 1.5869534654857302,
      "grad_norm": 1.4187078475952148,
      "learning_rate": 0.00018468743498345052,
      "loss": 0.9671,
      "step": 11288
    },
    {
      "epoch": 1.5870940531421343,
      "grad_norm": 1.8519173860549927,
      "learning_rate": 0.00018481083169307304,
      "loss": 1.1237,
      "step": 11289
    },
    {
      "epoch": 1.5872346407985378,
      "grad_norm": 1.6019847393035889,
      "learning_rate": 0.0001849337700676744,
      "loss": 1.0764,
      "step": 11290
    },
    {
      "epoch": 1.5873752284549416,
      "grad_norm": 1.8216081857681274,
      "learning_rate": 0.0001850562494428706,
      "loss": 1.2531,
      "step": 11291
    },
    {
      "epoch": 1.5875158161113454,
      "grad_norm": 1.8897240161895752,
      "learning_rate": 0.00018517826915675806,
      "loss": 1.1358,
      "step": 11292
    },
    {
      "epoch": 1.5876564037677492,
      "grad_norm": 1.918866515159607,
      "learning_rate": 0.00018529982854991747,
      "loss": 0.9293,
      "step": 11293
    },
    {
      "epoch": 1.587796991424153,
      "grad_norm": 1.8004902601242065,
      "learning_rate": 0.000185420926965417,
      "loss": 1.2141,
      "step": 11294
    },
    {
      "epoch": 1.5879375790805568,
      "grad_norm": 1.7412290573120117,
      "learning_rate": 0.00018554156374881616,
      "loss": 1.1202,
      "step": 11295
    },
    {
      "epoch": 1.5880781667369606,
      "grad_norm": 1.7303358316421509,
      "learning_rate": 0.00018566173824816922,
      "loss": 1.0271,
      "step": 11296
    },
    {
      "epoch": 1.5882187543933641,
      "grad_norm": 1.6636393070220947,
      "learning_rate": 0.00018578144981402865,
      "loss": 1.1306,
      "step": 11297
    },
    {
      "epoch": 1.5883593420497681,
      "grad_norm": 1.7497577667236328,
      "learning_rate": 0.0001859006977994486,
      "loss": 0.9254,
      "step": 11298
    },
    {
      "epoch": 1.5884999297061717,
      "grad_norm": 1.7233092784881592,
      "learning_rate": 0.00018601948155998915,
      "loss": 0.9293,
      "step": 11299
    },
    {
      "epoch": 1.5886405173625757,
      "grad_norm": 1.6135616302490234,
      "learning_rate": 0.0001861378004537181,
      "loss": 1.0591,
      "step": 11300
    },
    {
      "epoch": 1.5887811050189793,
      "grad_norm": 1.6737998723983765,
      "learning_rate": 0.0001862556538412163,
      "loss": 1.021,
      "step": 11301
    },
    {
      "epoch": 1.588921692675383,
      "grad_norm": 1.5216976404190063,
      "learning_rate": 0.00018637304108557994,
      "loss": 0.947,
      "step": 11302
    },
    {
      "epoch": 1.5890622803317869,
      "grad_norm": 1.2638415098190308,
      "learning_rate": 0.00018648996155242427,
      "loss": 0.9892,
      "step": 11303
    },
    {
      "epoch": 1.5892028679881907,
      "grad_norm": 1.577117919921875,
      "learning_rate": 0.0001866064146098876,
      "loss": 1.0799,
      "step": 11304
    },
    {
      "epoch": 1.5893434556445944,
      "grad_norm": 1.4756019115447998,
      "learning_rate": 0.0001867223996286337,
      "loss": 1.0672,
      "step": 11305
    },
    {
      "epoch": 1.589484043300998,
      "grad_norm": 1.665968418121338,
      "learning_rate": 0.00018683791598185593,
      "loss": 1.1354,
      "step": 11306
    },
    {
      "epoch": 1.589624630957402,
      "grad_norm": 1.571300745010376,
      "learning_rate": 0.00018695296304528048,
      "loss": 1.0415,
      "step": 11307
    },
    {
      "epoch": 1.5897652186138056,
      "grad_norm": 1.6117056608200073,
      "learning_rate": 0.00018706754019716927,
      "loss": 1.2564,
      "step": 11308
    },
    {
      "epoch": 1.5899058062702096,
      "grad_norm": 1.977041244506836,
      "learning_rate": 0.00018718164681832397,
      "loss": 0.9873,
      "step": 11309
    },
    {
      "epoch": 1.5900463939266132,
      "grad_norm": 1.6195236444473267,
      "learning_rate": 0.00018729528229208953,
      "loss": 0.9479,
      "step": 11310
    },
    {
      "epoch": 1.590186981583017,
      "grad_norm": 1.669266939163208,
      "learning_rate": 0.00018740844600435649,
      "loss": 0.9357,
      "step": 11311
    },
    {
      "epoch": 1.5903275692394208,
      "grad_norm": 1.8645442724227905,
      "learning_rate": 0.0001875211373435651,
      "loss": 1.0587,
      "step": 11312
    },
    {
      "epoch": 1.5904681568958245,
      "grad_norm": 1.5454990863800049,
      "learning_rate": 0.0001876333557007084,
      "loss": 1.0752,
      "step": 11313
    },
    {
      "epoch": 1.5906087445522283,
      "grad_norm": 1.875240445137024,
      "learning_rate": 0.0001877451004693355,
      "loss": 1.0725,
      "step": 11314
    },
    {
      "epoch": 1.5907493322086321,
      "grad_norm": 1.8291610479354858,
      "learning_rate": 0.0001878563710455549,
      "loss": 1.081,
      "step": 11315
    },
    {
      "epoch": 1.590889919865036,
      "grad_norm": 1.8577849864959717,
      "learning_rate": 0.00018796716682803772,
      "loss": 1.0473,
      "step": 11316
    },
    {
      "epoch": 1.5910305075214395,
      "grad_norm": 1.5833419561386108,
      "learning_rate": 0.00018807748721802096,
      "loss": 1.0534,
      "step": 11317
    },
    {
      "epoch": 1.5911710951778435,
      "grad_norm": 1.7117962837219238,
      "learning_rate": 0.00018818733161931077,
      "loss": 1.0225,
      "step": 11318
    },
    {
      "epoch": 1.591311682834247,
      "grad_norm": 1.469361662864685,
      "learning_rate": 0.0001882966994382854,
      "loss": 1.2489,
      "step": 11319
    },
    {
      "epoch": 1.591452270490651,
      "grad_norm": 1.7442625761032104,
      "learning_rate": 0.00018840559008389927,
      "loss": 1.2615,
      "step": 11320
    },
    {
      "epoch": 1.5915928581470546,
      "grad_norm": 2.0972306728363037,
      "learning_rate": 0.00018851400296768494,
      "loss": 0.988,
      "step": 11321
    },
    {
      "epoch": 1.5917334458034584,
      "grad_norm": 1.6784690618515015,
      "learning_rate": 0.00018862193750375722,
      "loss": 1.0316,
      "step": 11322
    },
    {
      "epoch": 1.5918740334598622,
      "grad_norm": 1.7244960069656372,
      "learning_rate": 0.0001887293931088156,
      "loss": 1.2144,
      "step": 11323
    },
    {
      "epoch": 1.592014621116266,
      "grad_norm": 1.632240891456604,
      "learning_rate": 0.00018883636920214822,
      "loss": 1.1861,
      "step": 11324
    },
    {
      "epoch": 1.5921552087726698,
      "grad_norm": 1.549880862236023,
      "learning_rate": 0.00018894286520563432,
      "loss": 1.0877,
      "step": 11325
    },
    {
      "epoch": 1.5922957964290734,
      "grad_norm": 1.6774520874023438,
      "learning_rate": 0.0001890488805437481,
      "loss": 1.205,
      "step": 11326
    },
    {
      "epoch": 1.5924363840854774,
      "grad_norm": 1.9744856357574463,
      "learning_rate": 0.00018915441464356089,
      "loss": 1.1173,
      "step": 11327
    },
    {
      "epoch": 1.592576971741881,
      "grad_norm": 1.5071431398391724,
      "learning_rate": 0.00018925946693474494,
      "loss": 1.0672,
      "step": 11328
    },
    {
      "epoch": 1.592717559398285,
      "grad_norm": 2.0145044326782227,
      "learning_rate": 0.00018936403684957623,
      "loss": 1.0687,
      "step": 11329
    },
    {
      "epoch": 1.5928581470546885,
      "grad_norm": 1.5372421741485596,
      "learning_rate": 0.00018946812382293767,
      "loss": 1.0874,
      "step": 11330
    },
    {
      "epoch": 1.5929987347110923,
      "grad_norm": 1.7597078084945679,
      "learning_rate": 0.00018957172729232197,
      "loss": 1.1907,
      "step": 11331
    },
    {
      "epoch": 1.5931393223674961,
      "grad_norm": 2.2368009090423584,
      "learning_rate": 0.00018967484669783492,
      "loss": 0.9637,
      "step": 11332
    },
    {
      "epoch": 1.5932799100239,
      "grad_norm": 1.7064697742462158,
      "learning_rate": 0.00018977748148219816,
      "loss": 1.0837,
      "step": 11333
    },
    {
      "epoch": 1.5934204976803037,
      "grad_norm": 1.7582602500915527,
      "learning_rate": 0.0001898796310907525,
      "loss": 1.1829,
      "step": 11334
    },
    {
      "epoch": 1.5935610853367075,
      "grad_norm": 1.7969255447387695,
      "learning_rate": 0.00018998129497146043,
      "loss": 1.0557,
      "step": 11335
    },
    {
      "epoch": 1.5937016729931113,
      "grad_norm": 1.5039342641830444,
      "learning_rate": 0.00019008247257490992,
      "loss": 1.0921,
      "step": 11336
    },
    {
      "epoch": 1.5938422606495148,
      "grad_norm": 1.4781098365783691,
      "learning_rate": 0.00019018316335431656,
      "loss": 1.1561,
      "step": 11337
    },
    {
      "epoch": 1.5939828483059189,
      "grad_norm": 1.522934913635254,
      "learning_rate": 0.0001902833667655266,
      "loss": 1.1707,
      "step": 11338
    },
    {
      "epoch": 1.5941234359623224,
      "grad_norm": 1.960006594657898,
      "learning_rate": 0.0001903830822670205,
      "loss": 1.1486,
      "step": 11339
    },
    {
      "epoch": 1.5942640236187264,
      "grad_norm": 1.929443359375,
      "learning_rate": 0.00019048230931991537,
      "loss": 1.1768,
      "step": 11340
    },
    {
      "epoch": 1.59440461127513,
      "grad_norm": 1.6519248485565186,
      "learning_rate": 0.00019058104738796784,
      "loss": 1.1287,
      "step": 11341
    },
    {
      "epoch": 1.5945451989315338,
      "grad_norm": 2.055297374725342,
      "learning_rate": 0.00019067929593757755,
      "loss": 0.8768,
      "step": 11342
    },
    {
      "epoch": 1.5946857865879376,
      "grad_norm": 2.098001718521118,
      "learning_rate": 0.0001907770544377892,
      "loss": 1.037,
      "step": 11343
    },
    {
      "epoch": 1.5948263742443414,
      "grad_norm": 1.6063152551651,
      "learning_rate": 0.00019087432236029593,
      "loss": 1.0714,
      "step": 11344
    },
    {
      "epoch": 1.5949669619007452,
      "grad_norm": 1.5093061923980713,
      "learning_rate": 0.00019097109917944215,
      "loss": 1.2223,
      "step": 11345
    },
    {
      "epoch": 1.5951075495571487,
      "grad_norm": 1.4910269975662231,
      "learning_rate": 0.0001910673843722258,
      "loss": 1.1825,
      "step": 11346
    },
    {
      "epoch": 1.5952481372135527,
      "grad_norm": 1.6733776330947876,
      "learning_rate": 0.00019116317741830254,
      "loss": 1.0171,
      "step": 11347
    },
    {
      "epoch": 1.5953887248699563,
      "grad_norm": 1.8382136821746826,
      "learning_rate": 0.00019125847779998703,
      "loss": 1.0484,
      "step": 11348
    },
    {
      "epoch": 1.5955293125263603,
      "grad_norm": 1.5484768152236938,
      "learning_rate": 0.0001913532850022566,
      "loss": 1.0217,
      "step": 11349
    },
    {
      "epoch": 1.595669900182764,
      "grad_norm": 1.7154713869094849,
      "learning_rate": 0.00019144759851275381,
      "loss": 1.1325,
      "step": 11350
    },
    {
      "epoch": 1.5958104878391677,
      "grad_norm": 1.4490323066711426,
      "learning_rate": 0.00019154141782178923,
      "loss": 1.2127,
      "step": 11351
    },
    {
      "epoch": 1.5959510754955715,
      "grad_norm": 1.66677987575531,
      "learning_rate": 0.00019163474242234419,
      "loss": 1.306,
      "step": 11352
    },
    {
      "epoch": 1.5960916631519753,
      "grad_norm": 1.5200798511505127,
      "learning_rate": 0.0001917275718100735,
      "loss": 1.2256,
      "step": 11353
    },
    {
      "epoch": 1.596232250808379,
      "grad_norm": 1.5961408615112305,
      "learning_rate": 0.00019181990548330826,
      "loss": 1.084,
      "step": 11354
    },
    {
      "epoch": 1.5963728384647828,
      "grad_norm": 1.6845033168792725,
      "learning_rate": 0.00019191174294305846,
      "loss": 1.2087,
      "step": 11355
    },
    {
      "epoch": 1.5965134261211866,
      "grad_norm": 1.4205639362335205,
      "learning_rate": 0.00019200308369301576,
      "loss": 1.0979,
      "step": 11356
    },
    {
      "epoch": 1.5966540137775902,
      "grad_norm": 1.551903247833252,
      "learning_rate": 0.000192093927239556,
      "loss": 0.9539,
      "step": 11357
    },
    {
      "epoch": 1.5967946014339942,
      "grad_norm": 1.9262170791625977,
      "learning_rate": 0.0001921842730917424,
      "loss": 1.0526,
      "step": 11358
    },
    {
      "epoch": 1.5969351890903978,
      "grad_norm": 1.5280029773712158,
      "learning_rate": 0.0001922741207613274,
      "loss": 0.9814,
      "step": 11359
    },
    {
      "epoch": 1.5970757767468018,
      "grad_norm": 1.606208086013794,
      "learning_rate": 0.00019236346976275607,
      "loss": 1.1707,
      "step": 11360
    },
    {
      "epoch": 1.5972163644032054,
      "grad_norm": 1.6778652667999268,
      "learning_rate": 0.00019245231961316787,
      "loss": 1.0841,
      "step": 11361
    },
    {
      "epoch": 1.5973569520596091,
      "grad_norm": 1.7587711811065674,
      "learning_rate": 0.0001925406698324001,
      "loss": 1.1643,
      "step": 11362
    },
    {
      "epoch": 1.597497539716013,
      "grad_norm": 1.5593013763427734,
      "learning_rate": 0.00019262851994299052,
      "loss": 1.2175,
      "step": 11363
    },
    {
      "epoch": 1.5976381273724167,
      "grad_norm": 1.566903829574585,
      "learning_rate": 0.000192715869470179,
      "loss": 1.1724,
      "step": 11364
    },
    {
      "epoch": 1.5977787150288205,
      "grad_norm": 1.5950525999069214,
      "learning_rate": 0.00019280271794191092,
      "loss": 1.0785,
      "step": 11365
    },
    {
      "epoch": 1.597919302685224,
      "grad_norm": 1.6216455698013306,
      "learning_rate": 0.00019288906488883944,
      "loss": 1.0543,
      "step": 11366
    },
    {
      "epoch": 1.598059890341628,
      "grad_norm": 1.7004808187484741,
      "learning_rate": 0.00019297490984432805,
      "loss": 0.9938,
      "step": 11367
    },
    {
      "epoch": 1.5982004779980317,
      "grad_norm": 1.5340927839279175,
      "learning_rate": 0.00019306025234445314,
      "loss": 1.0588,
      "step": 11368
    },
    {
      "epoch": 1.5983410656544357,
      "grad_norm": 1.641005516052246,
      "learning_rate": 0.0001931450919280064,
      "loss": 1.0342,
      "step": 11369
    },
    {
      "epoch": 1.5984816533108392,
      "grad_norm": 1.4248895645141602,
      "learning_rate": 0.00019322942813649748,
      "loss": 1.1207,
      "step": 11370
    },
    {
      "epoch": 1.598622240967243,
      "grad_norm": 1.801426649093628,
      "learning_rate": 0.00019331326051415628,
      "loss": 1.0621,
      "step": 11371
    },
    {
      "epoch": 1.5987628286236468,
      "grad_norm": 1.5273417234420776,
      "learning_rate": 0.00019339658860793553,
      "loss": 1.1309,
      "step": 11372
    },
    {
      "epoch": 1.5989034162800506,
      "grad_norm": 1.7750523090362549,
      "learning_rate": 0.00019347941196751317,
      "loss": 1.1714,
      "step": 11373
    },
    {
      "epoch": 1.5990440039364544,
      "grad_norm": 1.5614910125732422,
      "learning_rate": 0.00019356173014529508,
      "loss": 1.0821,
      "step": 11374
    },
    {
      "epoch": 1.5991845915928582,
      "grad_norm": 1.95614755153656,
      "learning_rate": 0.00019364354269641704,
      "loss": 1.1356,
      "step": 11375
    },
    {
      "epoch": 1.599325179249262,
      "grad_norm": 2.378208637237549,
      "learning_rate": 0.00019372484917874714,
      "loss": 0.9253,
      "step": 11376
    },
    {
      "epoch": 1.5994657669056656,
      "grad_norm": 1.8503092527389526,
      "learning_rate": 0.00019380564915288866,
      "loss": 1.0493,
      "step": 11377
    },
    {
      "epoch": 1.5996063545620696,
      "grad_norm": 1.6975005865097046,
      "learning_rate": 0.0001938859421821821,
      "loss": 0.8989,
      "step": 11378
    },
    {
      "epoch": 1.5997469422184731,
      "grad_norm": 1.6610468626022339,
      "learning_rate": 0.00019396572783270742,
      "loss": 1.074,
      "step": 11379
    },
    {
      "epoch": 1.5998875298748771,
      "grad_norm": 1.6533291339874268,
      "learning_rate": 0.00019404500567328698,
      "loss": 1.1275,
      "step": 11380
    },
    {
      "epoch": 1.6000281175312807,
      "grad_norm": 1.403310775756836,
      "learning_rate": 0.00019412377527548702,
      "loss": 1.0259,
      "step": 11381
    },
    {
      "epoch": 1.6001687051876845,
      "grad_norm": 1.4299594163894653,
      "learning_rate": 0.0001942020362136206,
      "loss": 1.1097,
      "step": 11382
    },
    {
      "epoch": 1.6003092928440883,
      "grad_norm": 1.767162561416626,
      "learning_rate": 0.00019427978806474966,
      "loss": 1.0513,
      "step": 11383
    },
    {
      "epoch": 1.600449880500492,
      "grad_norm": 1.5228339433670044,
      "learning_rate": 0.0001943570304086871,
      "loss": 1.0675,
      "step": 11384
    },
    {
      "epoch": 1.6005904681568959,
      "grad_norm": 1.5486805438995361,
      "learning_rate": 0.00019443376282799992,
      "loss": 1.0279,
      "step": 11385
    },
    {
      "epoch": 1.6007310558132994,
      "grad_norm": 1.4730579853057861,
      "learning_rate": 0.00019450998490801026,
      "loss": 1.1138,
      "step": 11386
    },
    {
      "epoch": 1.6008716434697035,
      "grad_norm": 1.5419602394104004,
      "learning_rate": 0.0001945856962367986,
      "loss": 1.0548,
      "step": 11387
    },
    {
      "epoch": 1.601012231126107,
      "grad_norm": 1.729146957397461,
      "learning_rate": 0.00019466089640520536,
      "loss": 0.9279,
      "step": 11388
    },
    {
      "epoch": 1.601152818782511,
      "grad_norm": 1.817867398262024,
      "learning_rate": 0.00019473558500683345,
      "loss": 1.0965,
      "step": 11389
    },
    {
      "epoch": 1.6012934064389146,
      "grad_norm": 1.5394227504730225,
      "learning_rate": 0.00019480976163805078,
      "loss": 1.1733,
      "step": 11390
    },
    {
      "epoch": 1.6014339940953184,
      "grad_norm": 1.650858759880066,
      "learning_rate": 0.00019488342589799138,
      "loss": 1.1512,
      "step": 11391
    },
    {
      "epoch": 1.6015745817517222,
      "grad_norm": 1.6449236869812012,
      "learning_rate": 0.00019495657738855866,
      "loss": 1.2632,
      "step": 11392
    },
    {
      "epoch": 1.601715169408126,
      "grad_norm": 1.747866153717041,
      "learning_rate": 0.0001950292157144271,
      "loss": 1.0632,
      "step": 11393
    },
    {
      "epoch": 1.6018557570645298,
      "grad_norm": 1.6312298774719238,
      "learning_rate": 0.00019510134048304436,
      "loss": 1.1434,
      "step": 11394
    },
    {
      "epoch": 1.6019963447209336,
      "grad_norm": 1.9590922594070435,
      "learning_rate": 0.00019517295130463337,
      "loss": 1.1199,
      "step": 11395
    },
    {
      "epoch": 1.6021369323773373,
      "grad_norm": 2.140927791595459,
      "learning_rate": 0.00019524404779219486,
      "loss": 0.971,
      "step": 11396
    },
    {
      "epoch": 1.602277520033741,
      "grad_norm": 1.5246871709823608,
      "learning_rate": 0.00019531462956150886,
      "loss": 1.0559,
      "step": 11397
    },
    {
      "epoch": 1.602418107690145,
      "grad_norm": 1.887992024421692,
      "learning_rate": 0.00019538469623113717,
      "loss": 1.1495,
      "step": 11398
    },
    {
      "epoch": 1.6025586953465485,
      "grad_norm": 1.5062464475631714,
      "learning_rate": 0.00019545424742242499,
      "loss": 1.2543,
      "step": 11399
    },
    {
      "epoch": 1.6026992830029525,
      "grad_norm": 1.5860474109649658,
      "learning_rate": 0.00019552328275950357,
      "loss": 1.0584,
      "step": 11400
    },
    {
      "epoch": 1.602839870659356,
      "grad_norm": 1.7896262407302856,
      "learning_rate": 0.00019559180186929208,
      "loss": 1.1154,
      "step": 11401
    },
    {
      "epoch": 1.6029804583157599,
      "grad_norm": 1.5363013744354248,
      "learning_rate": 0.0001956598043814991,
      "loss": 1.0355,
      "step": 11402
    },
    {
      "epoch": 1.6031210459721636,
      "grad_norm": 1.496346354484558,
      "learning_rate": 0.00019572728992862528,
      "loss": 1.161,
      "step": 11403
    },
    {
      "epoch": 1.6032616336285674,
      "grad_norm": 1.4066355228424072,
      "learning_rate": 0.00019579425814596495,
      "loss": 1.1156,
      "step": 11404
    },
    {
      "epoch": 1.6034022212849712,
      "grad_norm": 1.7818915843963623,
      "learning_rate": 0.00019586070867160806,
      "loss": 1.1211,
      "step": 11405
    },
    {
      "epoch": 1.6035428089413748,
      "grad_norm": 1.9525383710861206,
      "learning_rate": 0.0001959266411464428,
      "loss": 1.0592,
      "step": 11406
    },
    {
      "epoch": 1.6036833965977788,
      "grad_norm": 1.9654150009155273,
      "learning_rate": 0.0001959920552141563,
      "loss": 1.121,
      "step": 11407
    },
    {
      "epoch": 1.6038239842541824,
      "grad_norm": 1.7626750469207764,
      "learning_rate": 0.00019605695052123768,
      "loss": 0.9926,
      "step": 11408
    },
    {
      "epoch": 1.6039645719105864,
      "grad_norm": 1.8151085376739502,
      "learning_rate": 0.00019612132671697954,
      "loss": 1.0972,
      "step": 11409
    },
    {
      "epoch": 1.60410515956699,
      "grad_norm": 1.5781811475753784,
      "learning_rate": 0.0001961851834534797,
      "loss": 0.9644,
      "step": 11410
    },
    {
      "epoch": 1.6042457472233937,
      "grad_norm": 1.5850276947021484,
      "learning_rate": 0.00019624852038564337,
      "loss": 1.1002,
      "step": 11411
    },
    {
      "epoch": 1.6043863348797975,
      "grad_norm": 1.3772740364074707,
      "learning_rate": 0.00019631133717118497,
      "loss": 1.18,
      "step": 11412
    },
    {
      "epoch": 1.6045269225362013,
      "grad_norm": 1.9282052516937256,
      "learning_rate": 0.00019637363347062978,
      "loss": 0.8599,
      "step": 11413
    },
    {
      "epoch": 1.6046675101926051,
      "grad_norm": 1.5905706882476807,
      "learning_rate": 0.00019643540894731572,
      "loss": 1.0203,
      "step": 11414
    },
    {
      "epoch": 1.604808097849009,
      "grad_norm": 1.70396888256073,
      "learning_rate": 0.00019649666326739565,
      "loss": 1.1999,
      "step": 11415
    },
    {
      "epoch": 1.6049486855054127,
      "grad_norm": 1.4846376180648804,
      "learning_rate": 0.00019655739609983862,
      "loss": 1.024,
      "step": 11416
    },
    {
      "epoch": 1.6050892731618163,
      "grad_norm": 1.4993922710418701,
      "learning_rate": 0.0001966176071164322,
      "loss": 1.0485,
      "step": 11417
    },
    {
      "epoch": 1.6052298608182203,
      "grad_norm": 1.7656415700912476,
      "learning_rate": 0.0001966772959917837,
      "loss": 1.0691,
      "step": 11418
    },
    {
      "epoch": 1.6053704484746238,
      "grad_norm": 1.8956241607666016,
      "learning_rate": 0.0001967364624033223,
      "loss": 1.1257,
      "step": 11419
    },
    {
      "epoch": 1.6055110361310279,
      "grad_norm": 1.5959031581878662,
      "learning_rate": 0.0001967951060313006,
      "loss": 1.0233,
      "step": 11420
    },
    {
      "epoch": 1.6056516237874314,
      "grad_norm": 1.3925626277923584,
      "learning_rate": 0.0001968532265587966,
      "loss": 1.2427,
      "step": 11421
    },
    {
      "epoch": 1.6057922114438352,
      "grad_norm": 1.963856816291809,
      "learning_rate": 0.00019691082367171488,
      "loss": 1.0347,
      "step": 11422
    },
    {
      "epoch": 1.605932799100239,
      "grad_norm": 1.368369460105896,
      "learning_rate": 0.00019696789705878916,
      "loss": 1.233,
      "step": 11423
    },
    {
      "epoch": 1.6060733867566428,
      "grad_norm": 1.53813898563385,
      "learning_rate": 0.00019702444641158306,
      "loss": 1.1188,
      "step": 11424
    },
    {
      "epoch": 1.6062139744130466,
      "grad_norm": 1.475963830947876,
      "learning_rate": 0.00019708047142449244,
      "loss": 1.021,
      "step": 11425
    },
    {
      "epoch": 1.6063545620694502,
      "grad_norm": 1.5763846635818481,
      "learning_rate": 0.00019713597179474667,
      "loss": 1.1058,
      "step": 11426
    },
    {
      "epoch": 1.6064951497258542,
      "grad_norm": 1.729927659034729,
      "learning_rate": 0.00019719094722241037,
      "loss": 1.0195,
      "step": 11427
    },
    {
      "epoch": 1.6066357373822577,
      "grad_norm": 2.0830695629119873,
      "learning_rate": 0.00019724539741038536,
      "loss": 1.1812,
      "step": 11428
    },
    {
      "epoch": 1.6067763250386617,
      "grad_norm": 2.0303590297698975,
      "learning_rate": 0.00019729932206441147,
      "loss": 1.1587,
      "step": 11429
    },
    {
      "epoch": 1.6069169126950653,
      "grad_norm": 1.5057555437088013,
      "learning_rate": 0.000197352720893069,
      "loss": 1.1994,
      "step": 11430
    },
    {
      "epoch": 1.607057500351469,
      "grad_norm": 1.9380385875701904,
      "learning_rate": 0.0001974055936077798,
      "loss": 1.164,
      "step": 11431
    },
    {
      "epoch": 1.607198088007873,
      "grad_norm": 1.5596222877502441,
      "learning_rate": 0.00019745793992280878,
      "loss": 1.0783,
      "step": 11432
    },
    {
      "epoch": 1.6073386756642767,
      "grad_norm": 1.7176707983016968,
      "learning_rate": 0.000197509759555266,
      "loss": 1.061,
      "step": 11433
    },
    {
      "epoch": 1.6074792633206805,
      "grad_norm": 1.7793844938278198,
      "learning_rate": 0.00019756105222510755,
      "loss": 1.0303,
      "step": 11434
    },
    {
      "epoch": 1.6076198509770843,
      "grad_norm": 1.8357162475585938,
      "learning_rate": 0.00019761181765513733,
      "loss": 1.0262,
      "step": 11435
    },
    {
      "epoch": 1.607760438633488,
      "grad_norm": 1.558347225189209,
      "learning_rate": 0.0001976620555710087,
      "loss": 0.8802,
      "step": 11436
    },
    {
      "epoch": 1.6079010262898916,
      "grad_norm": 1.6513605117797852,
      "learning_rate": 0.00019771176570122561,
      "loss": 1.0279,
      "step": 11437
    },
    {
      "epoch": 1.6080416139462956,
      "grad_norm": NaN,
      "learning_rate": 0.00019771176570122561,
      "loss": 1.1752,
      "step": 11438
    },
    {
      "epoch": 1.6081822016026992,
      "grad_norm": 1.9953981637954712,
      "learning_rate": 0.00019776094777714437,
      "loss": 1.1619,
      "step": 11439
    },
    {
      "epoch": 1.6083227892591032,
      "grad_norm": 1.8171299695968628,
      "learning_rate": 0.0001978096015329751,
      "loss": 1.2657,
      "step": 11440
    },
    {
      "epoch": 1.6084633769155068,
      "grad_norm": 1.5960911512374878,
      "learning_rate": 0.00019785772670578302,
      "loss": 1.2059,
      "step": 11441
    },
    {
      "epoch": 1.6086039645719106,
      "grad_norm": 1.4794015884399414,
      "learning_rate": 0.00019790532303548983,
      "loss": 1.1672,
      "step": 11442
    },
    {
      "epoch": 1.6087445522283144,
      "grad_norm": 1.820619821548462,
      "learning_rate": 0.00019795239026487526,
      "loss": 1.0355,
      "step": 11443
    },
    {
      "epoch": 1.6088851398847182,
      "grad_norm": 1.5763671398162842,
      "learning_rate": 0.00019799892813957833,
      "loss": 1.1183,
      "step": 11444
    },
    {
      "epoch": 1.609025727541122,
      "grad_norm": 1.8005112409591675,
      "learning_rate": 0.00019804493640809912,
      "loss": 0.9802,
      "step": 11445
    },
    {
      "epoch": 1.6091663151975255,
      "grad_norm": 1.708537220954895,
      "learning_rate": 0.00019809041482179936,
      "loss": 1.1836,
      "step": 11446
    },
    {
      "epoch": 1.6093069028539295,
      "grad_norm": 1.8644568920135498,
      "learning_rate": 0.00019813536313490447,
      "loss": 1.0385,
      "step": 11447
    },
    {
      "epoch": 1.609447490510333,
      "grad_norm": 1.8252661228179932,
      "learning_rate": 0.0001981797811045046,
      "loss": 1.0282,
      "step": 11448
    },
    {
      "epoch": 1.609588078166737,
      "grad_norm": 1.7454450130462646,
      "learning_rate": 0.000198223668490556,
      "loss": 1.2066,
      "step": 11449
    },
    {
      "epoch": 1.6097286658231407,
      "grad_norm": 1.4690676927566528,
      "learning_rate": 0.0001982670250558823,
      "loss": 0.9836,
      "step": 11450
    },
    {
      "epoch": 1.6098692534795445,
      "grad_norm": 1.8017442226409912,
      "learning_rate": 0.00019830985056617584,
      "loss": 1.0479,
      "step": 11451
    },
    {
      "epoch": 1.6100098411359482,
      "grad_norm": 1.8436405658721924,
      "learning_rate": 0.00019835214478999878,
      "loss": 1.0428,
      "step": 11452
    },
    {
      "epoch": 1.610150428792352,
      "grad_norm": 2.0847086906433105,
      "learning_rate": 0.00019839390749878453,
      "loss": 0.8723,
      "step": 11453
    },
    {
      "epoch": 1.6102910164487558,
      "grad_norm": 1.738724708557129,
      "learning_rate": 0.0001984351384668388,
      "loss": 0.8777,
      "step": 11454
    },
    {
      "epoch": 1.6104316041051596,
      "grad_norm": 2.1439356803894043,
      "learning_rate": 0.0001984758374713411,
      "loss": 1.1972,
      "step": 11455
    },
    {
      "epoch": 1.6105721917615634,
      "grad_norm": 1.66351318359375,
      "learning_rate": 0.00019851600429234578,
      "loss": 1.206,
      "step": 11456
    },
    {
      "epoch": 1.610712779417967,
      "grad_norm": 1.897087812423706,
      "learning_rate": 0.0001985556387127831,
      "loss": 1.1781,
      "step": 11457
    },
    {
      "epoch": 1.610853367074371,
      "grad_norm": 1.6419035196304321,
      "learning_rate": 0.0001985947405184606,
      "loss": 1.0529,
      "step": 11458
    },
    {
      "epoch": 1.6109939547307746,
      "grad_norm": 1.7601696252822876,
      "learning_rate": 0.00019863330949806412,
      "loss": 1.2348,
      "step": 11459
    },
    {
      "epoch": 1.6111345423871786,
      "grad_norm": 1.637105107307434,
      "learning_rate": 0.00019867134544315903,
      "loss": 1.2566,
      "step": 11460
    },
    {
      "epoch": 1.6112751300435821,
      "grad_norm": 1.51503324508667,
      "learning_rate": 0.00019870884814819132,
      "loss": 1.2345,
      "step": 11461
    },
    {
      "epoch": 1.611415717699986,
      "grad_norm": 1.5391629934310913,
      "learning_rate": 0.00019874581741048878,
      "loss": 1.1787,
      "step": 11462
    },
    {
      "epoch": 1.6115563053563897,
      "grad_norm": 1.5459134578704834,
      "learning_rate": 0.0001987822530302619,
      "loss": 1.3236,
      "step": 11463
    },
    {
      "epoch": 1.6116968930127935,
      "grad_norm": 1.6222749948501587,
      "learning_rate": 0.00019881815481060527,
      "loss": 1.0688,
      "step": 11464
    },
    {
      "epoch": 1.6118374806691973,
      "grad_norm": 1.837816834449768,
      "learning_rate": 0.0001988535225574983,
      "loss": 1.0373,
      "step": 11465
    },
    {
      "epoch": 1.6119780683256009,
      "grad_norm": 1.2934561967849731,
      "learning_rate": 0.0001988883560798064,
      "loss": 1.2339,
      "step": 11466
    },
    {
      "epoch": 1.6121186559820049,
      "grad_norm": 1.456042766571045,
      "learning_rate": 0.00019892265518928223,
      "loss": 1.2204,
      "step": 11467
    },
    {
      "epoch": 1.6122592436384084,
      "grad_norm": 1.711230754852295,
      "learning_rate": 0.00019895641970056643,
      "loss": 0.9666,
      "step": 11468
    },
    {
      "epoch": 1.6123998312948125,
      "grad_norm": 1.4962573051452637,
      "learning_rate": 0.00019898964943118857,
      "loss": 1.1765,
      "step": 11469
    },
    {
      "epoch": 1.612540418951216,
      "grad_norm": 1.5315759181976318,
      "learning_rate": 0.00019902234420156849,
      "loss": 1.239,
      "step": 11470
    },
    {
      "epoch": 1.6126810066076198,
      "grad_norm": 1.8598365783691406,
      "learning_rate": 0.00019905450383501694,
      "loss": 1.1173,
      "step": 11471
    },
    {
      "epoch": 1.6128215942640236,
      "grad_norm": 1.5557382106781006,
      "learning_rate": 0.0001990861281577368,
      "loss": 1.0896,
      "step": 11472
    },
    {
      "epoch": 1.6129621819204274,
      "grad_norm": 1.6005467176437378,
      "learning_rate": 0.0001991172169988237,
      "loss": 1.0109,
      "step": 11473
    },
    {
      "epoch": 1.6131027695768312,
      "grad_norm": 1.904568076133728,
      "learning_rate": 0.00019914777019026722,
      "loss": 1.1128,
      "step": 11474
    },
    {
      "epoch": 1.613243357233235,
      "grad_norm": 1.8355140686035156,
      "learning_rate": 0.00019917778756695177,
      "loss": 1.1407,
      "step": 11475
    },
    {
      "epoch": 1.6133839448896388,
      "grad_norm": 1.593261480331421,
      "learning_rate": 0.00019920726896665718,
      "loss": 1.0977,
      "step": 11476
    },
    {
      "epoch": 1.6135245325460423,
      "grad_norm": 1.4574861526489258,
      "learning_rate": 0.00019923621423006006,
      "loss": 1.1246,
      "step": 11477
    },
    {
      "epoch": 1.6136651202024463,
      "grad_norm": 1.6873795986175537,
      "learning_rate": 0.00019926462320073429,
      "loss": 1.2762,
      "step": 11478
    },
    {
      "epoch": 1.61380570785885,
      "grad_norm": 1.841678261756897,
      "learning_rate": 0.00019929249572515198,
      "loss": 1.0965,
      "step": 11479
    },
    {
      "epoch": 1.613946295515254,
      "grad_norm": 1.8429591655731201,
      "learning_rate": 0.00019931983165268435,
      "loss": 1.1172,
      "step": 11480
    },
    {
      "epoch": 1.6140868831716575,
      "grad_norm": 1.5548362731933594,
      "learning_rate": 0.00019934663083560246,
      "loss": 1.2778,
      "step": 11481
    },
    {
      "epoch": 1.6142274708280613,
      "grad_norm": 1.490110158920288,
      "learning_rate": 0.00019937289312907806,
      "loss": 0.9649,
      "step": 11482
    },
    {
      "epoch": 1.614368058484465,
      "grad_norm": 1.7058336734771729,
      "learning_rate": 0.00019939861839118441,
      "loss": 1.1269,
      "step": 11483
    },
    {
      "epoch": 1.6145086461408689,
      "grad_norm": 1.7357079982757568,
      "learning_rate": 0.00019942380648289688,
      "loss": 1.0249,
      "step": 11484
    },
    {
      "epoch": 1.6146492337972727,
      "grad_norm": 1.5114519596099854,
      "learning_rate": 0.0001994484572680939,
      "loss": 1.1405,
      "step": 11485
    },
    {
      "epoch": 1.6147898214536762,
      "grad_norm": 1.5448952913284302,
      "learning_rate": 0.00019947257061355767,
      "loss": 1.0993,
      "step": 11486
    },
    {
      "epoch": 1.6149304091100802,
      "grad_norm": 1.7791260480880737,
      "learning_rate": 0.00019949614638897466,
      "loss": 1.0195,
      "step": 11487
    },
    {
      "epoch": 1.6150709967664838,
      "grad_norm": 1.5370275974273682,
      "learning_rate": 0.00019951918446693665,
      "loss": 1.2184,
      "step": 11488
    },
    {
      "epoch": 1.6152115844228878,
      "grad_norm": 1.7332797050476074,
      "learning_rate": 0.00019954168472294115,
      "loss": 1.0753,
      "step": 11489
    },
    {
      "epoch": 1.6153521720792914,
      "grad_norm": 1.4811347723007202,
      "learning_rate": 0.0001995636470353922,
      "loss": 1.2371,
      "step": 11490
    },
    {
      "epoch": 1.6154927597356952,
      "grad_norm": 1.490562915802002,
      "learning_rate": 0.00019958507128560096,
      "loss": 1.0434,
      "step": 11491
    },
    {
      "epoch": 1.615633347392099,
      "grad_norm": 2.2984161376953125,
      "learning_rate": 0.00019960595735778641,
      "loss": 1.1275,
      "step": 11492
    },
    {
      "epoch": 1.6157739350485028,
      "grad_norm": 1.5872948169708252,
      "learning_rate": 0.00019962630513907596,
      "loss": 1.1939,
      "step": 11493
    },
    {
      "epoch": 1.6159145227049065,
      "grad_norm": 1.6352219581604004,
      "learning_rate": 0.0001996461145195061,
      "loss": 1.1198,
      "step": 11494
    },
    {
      "epoch": 1.6160551103613103,
      "grad_norm": 1.5953787565231323,
      "learning_rate": 0.00019966538539202284,
      "loss": 1.1902,
      "step": 11495
    },
    {
      "epoch": 1.6161956980177141,
      "grad_norm": 1.3340016603469849,
      "learning_rate": 0.0001996841176524825,
      "loss": 1.0747,
      "step": 11496
    },
    {
      "epoch": 1.6163362856741177,
      "grad_norm": 1.5127164125442505,
      "learning_rate": 0.00019970231119965213,
      "loss": 1.1357,
      "step": 11497
    },
    {
      "epoch": 1.6164768733305217,
      "grad_norm": 1.7940458059310913,
      "learning_rate": 0.00019971996593521005,
      "loss": 1.1961,
      "step": 11498
    },
    {
      "epoch": 1.6166174609869253,
      "grad_norm": 1.448789358139038,
      "learning_rate": 0.0001997370817637465,
      "loss": 1.0631,
      "step": 11499
    },
    {
      "epoch": 1.6167580486433293,
      "grad_norm": 1.5868500471115112,
      "learning_rate": 0.00019975365859276406,
      "loss": 1.2838,
      "step": 11500
    },
    {
      "epoch": 1.6167580486433293,
      "eval_loss": 1.1691222190856934,
      "eval_runtime": 773.7411,
      "eval_samples_per_second": 16.344,
      "eval_steps_per_second": 8.172,
      "step": 11500
    },
    {
      "epoch": 1.6168986362997328,
      "grad_norm": 1.5143684148788452,
      "learning_rate": 0.00019976969633267815,
      "loss": 1.0385,
      "step": 11501
    },
    {
      "epoch": 1.6170392239561366,
      "grad_norm": 1.4356614351272583,
      "learning_rate": 0.00019978519489681756,
      "loss": 0.9503,
      "step": 11502
    },
    {
      "epoch": 1.6171798116125404,
      "grad_norm": 1.5426409244537354,
      "learning_rate": 0.00019980015420142485,
      "loss": 0.9955,
      "step": 11503
    },
    {
      "epoch": 1.6173203992689442,
      "grad_norm": 1.393998146057129,
      "learning_rate": 0.00019981457416565698,
      "loss": 1.2518,
      "step": 11504
    },
    {
      "epoch": 1.617460986925348,
      "grad_norm": 1.669160008430481,
      "learning_rate": 0.00019982845471158545,
      "loss": 1.2404,
      "step": 11505
    },
    {
      "epoch": 1.6176015745817516,
      "grad_norm": 1.5579686164855957,
      "learning_rate": 0.00019984179576419705,
      "loss": 1.2169,
      "step": 11506
    },
    {
      "epoch": 1.6177421622381556,
      "grad_norm": 1.62696373462677,
      "learning_rate": 0.0001998545972513939,
      "loss": 0.9717,
      "step": 11507
    },
    {
      "epoch": 1.6178827498945592,
      "grad_norm": 2.016979694366455,
      "learning_rate": 0.00019986685910399418,
      "loss": 1.1876,
      "step": 11508
    },
    {
      "epoch": 1.6180233375509632,
      "grad_norm": 1.5848029851913452,
      "learning_rate": 0.00019987858125573236,
      "loss": 1.0034,
      "step": 11509
    },
    {
      "epoch": 1.6181639252073667,
      "grad_norm": 1.64735746383667,
      "learning_rate": 0.00019988976364325954,
      "loss": 1.0906,
      "step": 11510
    },
    {
      "epoch": 1.6183045128637705,
      "grad_norm": 1.5019783973693848,
      "learning_rate": 0.00019990040620614384,
      "loss": 1.0242,
      "step": 11511
    },
    {
      "epoch": 1.6184451005201743,
      "grad_norm": 1.7496155500411987,
      "learning_rate": 0.0001999105088868707,
      "loss": 1.1178,
      "step": 11512
    },
    {
      "epoch": 1.618585688176578,
      "grad_norm": 1.5652399063110352,
      "learning_rate": 0.0001999200716308431,
      "loss": 1.0829,
      "step": 11513
    },
    {
      "epoch": 1.618726275832982,
      "grad_norm": 1.662383794784546,
      "learning_rate": 0.00019992909438638204,
      "loss": 1.0169,
      "step": 11514
    },
    {
      "epoch": 1.6188668634893855,
      "grad_norm": 1.9208687543869019,
      "learning_rate": 0.00019993757710472677,
      "loss": 0.9919,
      "step": 11515
    },
    {
      "epoch": 1.6190074511457895,
      "grad_norm": 1.5572115182876587,
      "learning_rate": 0.00019994551974003488,
      "loss": 1.0628,
      "step": 11516
    },
    {
      "epoch": 1.619148038802193,
      "grad_norm": 1.7654829025268555,
      "learning_rate": 0.00019995292224938278,
      "loss": 1.0213,
      "step": 11517
    },
    {
      "epoch": 1.619288626458597,
      "grad_norm": 1.6922255754470825,
      "learning_rate": 0.0001999597845927658,
      "loss": 1.2477,
      "step": 11518
    },
    {
      "epoch": 1.6194292141150006,
      "grad_norm": 1.5495442152023315,
      "learning_rate": 0.00019996610673309842,
      "loss": 1.3186,
      "step": 11519
    },
    {
      "epoch": 1.6195698017714044,
      "grad_norm": 1.7273021936416626,
      "learning_rate": 0.00019997188863621457,
      "loss": 1.1291,
      "step": 11520
    },
    {
      "epoch": 1.6197103894278082,
      "grad_norm": 2.0058677196502686,
      "learning_rate": 0.0001999771302708676,
      "loss": 1.0691,
      "step": 11521
    },
    {
      "epoch": 1.619850977084212,
      "grad_norm": 1.7442290782928467,
      "learning_rate": 0.00019998183160873067,
      "loss": 1.0698,
      "step": 11522
    },
    {
      "epoch": 1.6199915647406158,
      "grad_norm": 1.5543110370635986,
      "learning_rate": 0.00019998599262439679,
      "loss": 1.2589,
      "step": 11523
    },
    {
      "epoch": 1.6201321523970196,
      "grad_norm": 2.2498607635498047,
      "learning_rate": 0.00019998961329537897,
      "loss": 1.1385,
      "step": 11524
    },
    {
      "epoch": 1.6202727400534234,
      "grad_norm": 1.7657626867294312,
      "learning_rate": 0.0001999926936021104,
      "loss": 0.9213,
      "step": 11525
    },
    {
      "epoch": 1.620413327709827,
      "grad_norm": 1.6774935722351074,
      "learning_rate": 0.00019999523352794442,
      "loss": 1.154,
      "step": 11526
    },
    {
      "epoch": 1.620553915366231,
      "grad_norm": 1.9997879266738892,
      "learning_rate": 0.0001999972330591548,
      "loss": 1.1295,
      "step": 11527
    },
    {
      "epoch": 1.6206945030226345,
      "grad_norm": 1.5316803455352783,
      "learning_rate": 0.00019999869218493568,
      "loss": 0.9611,
      "step": 11528
    },
    {
      "epoch": 1.6208350906790385,
      "grad_norm": 1.8248276710510254,
      "learning_rate": 0.0001999996108974016,
      "loss": 0.9535,
      "step": 11529
    },
    {
      "epoch": 1.620975678335442,
      "grad_norm": 1.4938879013061523,
      "learning_rate": 0.00019999998919158768,
      "loss": 1.201,
      "step": 11530
    },
    {
      "epoch": 1.6211162659918459,
      "grad_norm": 1.6902661323547363,
      "learning_rate": 0.00019999982706544954,
      "loss": 1.1496,
      "step": 11531
    },
    {
      "epoch": 1.6212568536482497,
      "grad_norm": 2.084188222885132,
      "learning_rate": 0.0001999991245198633,
      "loss": 1.1913,
      "step": 11532
    },
    {
      "epoch": 1.6213974413046535,
      "grad_norm": 1.4830570220947266,
      "learning_rate": 0.00019999788155862573,
      "loss": 1.0517,
      "step": 11533
    },
    {
      "epoch": 1.6215380289610573,
      "grad_norm": 1.61758553981781,
      "learning_rate": 0.00019999609818845398,
      "loss": 1.1301,
      "step": 11534
    },
    {
      "epoch": 1.6216786166174608,
      "grad_norm": 1.6063004732131958,
      "learning_rate": 0.0001999937744189858,
      "loss": 1.2281,
      "step": 11535
    },
    {
      "epoch": 1.6218192042738648,
      "grad_norm": 1.6512904167175293,
      "learning_rate": 0.00019999091026277928,
      "loss": 1.0664,
      "step": 11536
    },
    {
      "epoch": 1.6219597919302684,
      "grad_norm": 1.8470137119293213,
      "learning_rate": 0.0001999875057353129,
      "loss": 1.0325,
      "step": 11537
    },
    {
      "epoch": 1.6221003795866724,
      "grad_norm": 1.5047330856323242,
      "learning_rate": 0.0001999835608549854,
      "loss": 1.0336,
      "step": 11538
    },
    {
      "epoch": 1.622240967243076,
      "grad_norm": 1.8024976253509521,
      "learning_rate": 0.00019997907564311583,
      "loss": 1.1531,
      "step": 11539
    },
    {
      "epoch": 1.6223815548994798,
      "grad_norm": 2.146285057067871,
      "learning_rate": 0.00019997405012394306,
      "loss": 1.0724,
      "step": 11540
    },
    {
      "epoch": 1.6225221425558836,
      "grad_norm": 1.84409761428833,
      "learning_rate": 0.0001999684843246261,
      "loss": 1.013,
      "step": 11541
    },
    {
      "epoch": 1.6226627302122874,
      "grad_norm": 1.5556142330169678,
      "learning_rate": 0.0001999623782752436,
      "loss": 0.9991,
      "step": 11542
    },
    {
      "epoch": 1.6228033178686911,
      "grad_norm": 1.6673028469085693,
      "learning_rate": 0.00019995573200879397,
      "loss": 0.9048,
      "step": 11543
    },
    {
      "epoch": 1.622943905525095,
      "grad_norm": 1.6877856254577637,
      "learning_rate": 0.0001999485455611949,
      "loss": 0.9993,
      "step": 11544
    },
    {
      "epoch": 1.6230844931814987,
      "grad_norm": 1.6066869497299194,
      "learning_rate": 0.0001999408189712835,
      "loss": 1.0459,
      "step": 11545
    },
    {
      "epoch": 1.6232250808379023,
      "grad_norm": 1.9427369832992554,
      "learning_rate": 0.0001999325522808158,
      "loss": 1.1991,
      "step": 11546
    },
    {
      "epoch": 1.6233656684943063,
      "grad_norm": 1.4114487171173096,
      "learning_rate": 0.00019992374553446667,
      "loss": 1.0379,
      "step": 11547
    },
    {
      "epoch": 1.6235062561507099,
      "grad_norm": 1.7764158248901367,
      "learning_rate": 0.0001999143987798296,
      "loss": 1.0683,
      "step": 11548
    },
    {
      "epoch": 1.6236468438071139,
      "grad_norm": 1.6663304567337036,
      "learning_rate": 0.0001999045120674163,
      "loss": 1.1653,
      "step": 11549
    },
    {
      "epoch": 1.6237874314635174,
      "grad_norm": 1.6366177797317505,
      "learning_rate": 0.0001998940854506566,
      "loss": 0.9197,
      "step": 11550
    },
    {
      "epoch": 1.6239280191199212,
      "grad_norm": 1.539278507232666,
      "learning_rate": 0.00019988311898589808,
      "loss": 0.9871,
      "step": 11551
    },
    {
      "epoch": 1.624068606776325,
      "grad_norm": 1.8329850435256958,
      "learning_rate": 0.00019987161273240579,
      "loss": 1.1268,
      "step": 11552
    },
    {
      "epoch": 1.6242091944327288,
      "grad_norm": 1.421342372894287,
      "learning_rate": 0.0001998595667523618,
      "loss": 1.0072,
      "step": 11553
    },
    {
      "epoch": 1.6243497820891326,
      "grad_norm": 1.5908750295639038,
      "learning_rate": 0.00019984698111086507,
      "loss": 1.227,
      "step": 11554
    },
    {
      "epoch": 1.6244903697455362,
      "grad_norm": 1.9082781076431274,
      "learning_rate": 0.00019983385587593093,
      "loss": 1.1436,
      "step": 11555
    },
    {
      "epoch": 1.6246309574019402,
      "grad_norm": 1.6616095304489136,
      "learning_rate": 0.00019982019111849087,
      "loss": 1.0954,
      "step": 11556
    },
    {
      "epoch": 1.6247715450583438,
      "grad_norm": 1.658206820487976,
      "learning_rate": 0.00019980598691239206,
      "loss": 1.0548,
      "step": 11557
    },
    {
      "epoch": 1.6249121327147478,
      "grad_norm": 1.8234614133834839,
      "learning_rate": 0.00019979124333439687,
      "loss": 1.0883,
      "step": 11558
    },
    {
      "epoch": 1.6250527203711513,
      "grad_norm": 1.6147617101669312,
      "learning_rate": 0.00019977596046418257,
      "loss": 0.9817,
      "step": 11559
    },
    {
      "epoch": 1.6251933080275551,
      "grad_norm": 1.8918585777282715,
      "learning_rate": 0.00019976013838434096,
      "loss": 0.9137,
      "step": 11560
    },
    {
      "epoch": 1.625333895683959,
      "grad_norm": 1.8006000518798828,
      "learning_rate": 0.00019974377718037778,
      "loss": 0.9799,
      "step": 11561
    },
    {
      "epoch": 1.6254744833403627,
      "grad_norm": 1.754235863685608,
      "learning_rate": 0.00019972687694071235,
      "loss": 1.0821,
      "step": 11562
    },
    {
      "epoch": 1.6256150709967665,
      "grad_norm": 1.5316778421401978,
      "learning_rate": 0.0001997094377566769,
      "loss": 1.1058,
      "step": 11563
    },
    {
      "epoch": 1.6257556586531703,
      "grad_norm": 1.4350230693817139,
      "learning_rate": 0.00019969145972251643,
      "loss": 1.0952,
      "step": 11564
    },
    {
      "epoch": 1.625896246309574,
      "grad_norm": 1.4882842302322388,
      "learning_rate": 0.00019967294293538786,
      "loss": 1.0229,
      "step": 11565
    },
    {
      "epoch": 1.6260368339659776,
      "grad_norm": 1.4233832359313965,
      "learning_rate": 0.00019965388749535964,
      "loss": 1.0822,
      "step": 11566
    },
    {
      "epoch": 1.6261774216223817,
      "grad_norm": 1.7682384252548218,
      "learning_rate": 0.00019963429350541135,
      "loss": 1.1979,
      "step": 11567
    },
    {
      "epoch": 1.6263180092787852,
      "grad_norm": 1.6450464725494385,
      "learning_rate": 0.00019961416107143286,
      "loss": 1.1403,
      "step": 11568
    },
    {
      "epoch": 1.6264585969351892,
      "grad_norm": 1.6589523553848267,
      "learning_rate": 0.00019959349030222395,
      "loss": 0.9943,
      "step": 11569
    },
    {
      "epoch": 1.6265991845915928,
      "grad_norm": 1.9226528406143188,
      "learning_rate": 0.00019957228130949365,
      "loss": 0.9728,
      "step": 11570
    },
    {
      "epoch": 1.6267397722479966,
      "grad_norm": 1.6897060871124268,
      "learning_rate": 0.00019955053420785972,
      "loss": 1.283,
      "step": 11571
    },
    {
      "epoch": 1.6268803599044004,
      "grad_norm": 1.679732322692871,
      "learning_rate": 0.00019952824911484787,
      "loss": 1.054,
      "step": 11572
    },
    {
      "epoch": 1.6270209475608042,
      "grad_norm": 1.8235729932785034,
      "learning_rate": 0.00019950542615089137,
      "loss": 1.0222,
      "step": 11573
    },
    {
      "epoch": 1.627161535217208,
      "grad_norm": 1.450197696685791,
      "learning_rate": 0.00019948206543933005,
      "loss": 0.9684,
      "step": 11574
    },
    {
      "epoch": 1.6273021228736115,
      "grad_norm": 1.7478034496307373,
      "learning_rate": 0.00019945816710641005,
      "loss": 1.0729,
      "step": 11575
    },
    {
      "epoch": 1.6274427105300155,
      "grad_norm": 1.5058701038360596,
      "learning_rate": 0.0001994337312812828,
      "loss": 1.1681,
      "step": 11576
    },
    {
      "epoch": 1.6275832981864191,
      "grad_norm": 1.475265383720398,
      "learning_rate": 0.0001994087580960045,
      "loss": 1.1029,
      "step": 11577
    },
    {
      "epoch": 1.6277238858428231,
      "grad_norm": 1.7312318086624146,
      "learning_rate": 0.0001993832476855353,
      "loss": 1.1795,
      "step": 11578
    },
    {
      "epoch": 1.6278644734992267,
      "grad_norm": 1.646943211555481,
      "learning_rate": 0.00019935720018773875,
      "loss": 1.1658,
      "step": 11579
    },
    {
      "epoch": 1.6280050611556305,
      "grad_norm": 1.5638278722763062,
      "learning_rate": 0.0001993306157433807,
      "loss": 1.0738,
      "step": 11580
    },
    {
      "epoch": 1.6281456488120343,
      "grad_norm": 1.5656551122665405,
      "learning_rate": 0.000199303494496129,
      "loss": 1.2882,
      "step": 11581
    },
    {
      "epoch": 1.628286236468438,
      "grad_norm": 1.5945464372634888,
      "learning_rate": 0.00019927583659255232,
      "loss": 1.1661,
      "step": 11582
    },
    {
      "epoch": 1.6284268241248419,
      "grad_norm": 1.780752420425415,
      "learning_rate": 0.00019924764218211968,
      "loss": 1.0545,
      "step": 11583
    },
    {
      "epoch": 1.6285674117812456,
      "grad_norm": 1.864133596420288,
      "learning_rate": 0.00019921891141719947,
      "loss": 1.1875,
      "step": 11584
    },
    {
      "epoch": 1.6287079994376494,
      "grad_norm": 1.549390435218811,
      "learning_rate": 0.00019918964445305844,
      "loss": 1.0608,
      "step": 11585
    },
    {
      "epoch": 1.628848587094053,
      "grad_norm": 1.4475234746932983,
      "learning_rate": 0.00019915984144786136,
      "loss": 1.0837,
      "step": 11586
    },
    {
      "epoch": 1.628989174750457,
      "grad_norm": 1.564306378364563,
      "learning_rate": 0.00019912950256266969,
      "loss": 1.1305,
      "step": 11587
    },
    {
      "epoch": 1.6291297624068606,
      "grad_norm": 1.7668448686599731,
      "learning_rate": 0.00019909862796144097,
      "loss": 0.9656,
      "step": 11588
    },
    {
      "epoch": 1.6292703500632646,
      "grad_norm": 1.4988579750061035,
      "learning_rate": 0.00019906721781102782,
      "loss": 1.3066,
      "step": 11589
    },
    {
      "epoch": 1.6294109377196682,
      "grad_norm": 1.5762418508529663,
      "learning_rate": 0.00019903527228117712,
      "loss": 1.1593,
      "step": 11590
    },
    {
      "epoch": 1.629551525376072,
      "grad_norm": 1.3943302631378174,
      "learning_rate": 0.000199002791544529,
      "loss": 1.0341,
      "step": 11591
    },
    {
      "epoch": 1.6296921130324757,
      "grad_norm": 1.6780521869659424,
      "learning_rate": 0.00019896977577661597,
      "loss": 1.0132,
      "step": 11592
    },
    {
      "epoch": 1.6298327006888795,
      "grad_norm": 1.6295498609542847,
      "learning_rate": 0.000198936225155862,
      "loss": 1.1874,
      "step": 11593
    },
    {
      "epoch": 1.6299732883452833,
      "grad_norm": 1.7224525213241577,
      "learning_rate": 0.00019890213986358148,
      "loss": 1.2846,
      "step": 11594
    },
    {
      "epoch": 1.630113876001687,
      "grad_norm": 1.6199854612350464,
      "learning_rate": 0.00019886752008397833,
      "loss": 1.2179,
      "step": 11595
    },
    {
      "epoch": 1.630254463658091,
      "grad_norm": 1.7042243480682373,
      "learning_rate": 0.00019883236600414477,
      "loss": 1.0306,
      "step": 11596
    },
    {
      "epoch": 1.6303950513144945,
      "grad_norm": 1.7910301685333252,
      "learning_rate": 0.00019879667781406063,
      "loss": 1.1022,
      "step": 11597
    },
    {
      "epoch": 1.6305356389708985,
      "grad_norm": 1.6364398002624512,
      "learning_rate": 0.00019876045570659228,
      "loss": 1.0278,
      "step": 11598
    },
    {
      "epoch": 1.630676226627302,
      "grad_norm": 1.638880729675293,
      "learning_rate": 0.0001987236998774913,
      "loss": 1.0342,
      "step": 11599
    },
    {
      "epoch": 1.6308168142837058,
      "grad_norm": 1.8231768608093262,
      "learning_rate": 0.0001986864105253937,
      "loss": 1.1813,
      "step": 11600
    },
    {
      "epoch": 1.6309574019401096,
      "grad_norm": 1.5518715381622314,
      "learning_rate": 0.00019864858785181874,
      "loss": 1.2943,
      "step": 11601
    },
    {
      "epoch": 1.6310979895965134,
      "grad_norm": 1.6551806926727295,
      "learning_rate": 0.0001986102320611678,
      "loss": 0.9283,
      "step": 11602
    },
    {
      "epoch": 1.6312385772529172,
      "grad_norm": 1.589986801147461,
      "learning_rate": 0.00019857134336072343,
      "loss": 1.2588,
      "step": 11603
    },
    {
      "epoch": 1.631379164909321,
      "grad_norm": 1.6621434688568115,
      "learning_rate": 0.00019853192196064813,
      "loss": 1.2334,
      "step": 11604
    },
    {
      "epoch": 1.6315197525657248,
      "grad_norm": 1.6879348754882812,
      "learning_rate": 0.00019849196807398309,
      "loss": 1.3579,
      "step": 11605
    },
    {
      "epoch": 1.6316603402221284,
      "grad_norm": 1.816342830657959,
      "learning_rate": 0.00019845148191664736,
      "loss": 1.2571,
      "step": 11606
    },
    {
      "epoch": 1.6318009278785324,
      "grad_norm": 1.5273529291152954,
      "learning_rate": 0.00019841046370743635,
      "loss": 1.17,
      "step": 11607
    },
    {
      "epoch": 1.631941515534936,
      "grad_norm": 1.641940712928772,
      "learning_rate": 0.00019836891366802078,
      "loss": 1.1984,
      "step": 11608
    },
    {
      "epoch": 1.63208210319134,
      "grad_norm": 1.7001405954360962,
      "learning_rate": 0.00019832683202294557,
      "loss": 1.0513,
      "step": 11609
    },
    {
      "epoch": 1.6322226908477435,
      "grad_norm": 1.6843397617340088,
      "learning_rate": 0.00019828421899962853,
      "loss": 1.1103,
      "step": 11610
    },
    {
      "epoch": 1.6323632785041473,
      "grad_norm": 1.7394931316375732,
      "learning_rate": 0.00019824107482835915,
      "loss": 1.2516,
      "step": 11611
    },
    {
      "epoch": 1.632503866160551,
      "grad_norm": 1.6199748516082764,
      "learning_rate": 0.00019819739974229715,
      "loss": 1.0343,
      "step": 11612
    },
    {
      "epoch": 1.6326444538169549,
      "grad_norm": 1.5110076665878296,
      "learning_rate": 0.00019815319397747177,
      "loss": 1.2023,
      "step": 11613
    },
    {
      "epoch": 1.6327850414733587,
      "grad_norm": 1.897329330444336,
      "learning_rate": 0.00019810845777277995,
      "loss": 1.117,
      "step": 11614
    },
    {
      "epoch": 1.6329256291297622,
      "grad_norm": 1.8298983573913574,
      "learning_rate": 0.0001980631913699852,
      "loss": 1.2101,
      "step": 11615
    },
    {
      "epoch": 1.6330662167861663,
      "grad_norm": 1.4865463972091675,
      "learning_rate": 0.00019801739501371652,
      "loss": 1.1761,
      "step": 11616
    },
    {
      "epoch": 1.6332068044425698,
      "grad_norm": 1.4454034566879272,
      "learning_rate": 0.00019797106895146657,
      "loss": 1.0872,
      "step": 11617
    },
    {
      "epoch": 1.6333473920989738,
      "grad_norm": 1.8528426885604858,
      "learning_rate": 0.0001979242134335909,
      "loss": 1.1182,
      "step": 11618
    },
    {
      "epoch": 1.6334879797553774,
      "grad_norm": 1.6468589305877686,
      "learning_rate": 0.00019787682871330633,
      "loss": 1.1137,
      "step": 11619
    },
    {
      "epoch": 1.6336285674117812,
      "grad_norm": 1.7710142135620117,
      "learning_rate": 0.0001978289150466894,
      "loss": 1.0925,
      "step": 11620
    },
    {
      "epoch": 1.633769155068185,
      "grad_norm": 1.4429818391799927,
      "learning_rate": 0.00019778047269267555,
      "loss": 0.9185,
      "step": 11621
    },
    {
      "epoch": 1.6339097427245888,
      "grad_norm": 1.841142177581787,
      "learning_rate": 0.0001977315019130571,
      "loss": 1.2195,
      "step": 11622
    },
    {
      "epoch": 1.6340503303809926,
      "grad_norm": 1.567521333694458,
      "learning_rate": 0.00019768200297248196,
      "loss": 1.1617,
      "step": 11623
    },
    {
      "epoch": 1.6341909180373964,
      "grad_norm": 1.479504942893982,
      "learning_rate": 0.0001976319761384526,
      "loss": 1.0865,
      "step": 11624
    },
    {
      "epoch": 1.6343315056938001,
      "grad_norm": 1.5698633193969727,
      "learning_rate": 0.0001975814216813242,
      "loss": 1.1377,
      "step": 11625
    },
    {
      "epoch": 1.6344720933502037,
      "grad_norm": 1.7102010250091553,
      "learning_rate": 0.00019753033987430336,
      "loss": 1.1512,
      "step": 11626
    },
    {
      "epoch": 1.6346126810066077,
      "grad_norm": 1.4312920570373535,
      "learning_rate": 0.00019747873099344658,
      "loss": 1.0125,
      "step": 11627
    },
    {
      "epoch": 1.6347532686630113,
      "grad_norm": 1.593525767326355,
      "learning_rate": 0.00019742659531765878,
      "loss": 1.2314,
      "step": 11628
    },
    {
      "epoch": 1.6348938563194153,
      "grad_norm": 1.7273913621902466,
      "learning_rate": 0.00019737393312869178,
      "loss": 1.028,
      "step": 11629
    },
    {
      "epoch": 1.6350344439758189,
      "grad_norm": 1.6209031343460083,
      "learning_rate": 0.00019732074471114278,
      "loss": 1.1229,
      "step": 11630
    },
    {
      "epoch": 1.6351750316322227,
      "grad_norm": 1.5195337533950806,
      "learning_rate": 0.00019726703035245283,
      "loss": 1.2541,
      "step": 11631
    },
    {
      "epoch": 1.6353156192886265,
      "grad_norm": 1.5877984762191772,
      "learning_rate": 0.00019721279034290535,
      "loss": 1.179,
      "step": 11632
    },
    {
      "epoch": 1.6354562069450302,
      "grad_norm": 1.4559481143951416,
      "learning_rate": 0.0001971580249756242,
      "loss": 0.9861,
      "step": 11633
    },
    {
      "epoch": 1.635596794601434,
      "grad_norm": 1.5422742366790771,
      "learning_rate": 0.00019710273454657276,
      "loss": 0.9744,
      "step": 11634
    },
    {
      "epoch": 1.6357373822578376,
      "grad_norm": 1.8844616413116455,
      "learning_rate": 0.00019704691935455162,
      "loss": 1.0837,
      "step": 11635
    },
    {
      "epoch": 1.6358779699142416,
      "grad_norm": 1.4862346649169922,
      "learning_rate": 0.00019699057970119763,
      "loss": 1.028,
      "step": 11636
    },
    {
      "epoch": 1.6360185575706452,
      "grad_norm": 1.5849294662475586,
      "learning_rate": 0.00019693371589098174,
      "loss": 1.1922,
      "step": 11637
    },
    {
      "epoch": 1.6361591452270492,
      "grad_norm": 1.7869104146957397,
      "learning_rate": 0.00019687632823120756,
      "loss": 0.9939,
      "step": 11638
    },
    {
      "epoch": 1.6362997328834528,
      "grad_norm": 1.796248435974121,
      "learning_rate": 0.00019681841703200962,
      "loss": 1.2928,
      "step": 11639
    },
    {
      "epoch": 1.6364403205398566,
      "grad_norm": 1.6356145143508911,
      "learning_rate": 0.00019675998260635188,
      "loss": 1.0311,
      "step": 11640
    },
    {
      "epoch": 1.6365809081962603,
      "grad_norm": 1.599381446838379,
      "learning_rate": 0.0001967010252700259,
      "loss": 1.1277,
      "step": 11641
    },
    {
      "epoch": 1.6367214958526641,
      "grad_norm": 1.678095817565918,
      "learning_rate": 0.00019664154534164915,
      "loss": 1.0559,
      "step": 11642
    },
    {
      "epoch": 1.636862083509068,
      "grad_norm": 1.6508991718292236,
      "learning_rate": 0.00019658154314266323,
      "loss": 1.1158,
      "step": 11643
    },
    {
      "epoch": 1.6370026711654717,
      "grad_norm": 1.608749508857727,
      "learning_rate": 0.00019652101899733234,
      "loss": 1.1087,
      "step": 11644
    },
    {
      "epoch": 1.6371432588218755,
      "grad_norm": 1.70063054561615,
      "learning_rate": 0.00019645997323274123,
      "loss": 1.0661,
      "step": 11645
    },
    {
      "epoch": 1.637283846478279,
      "grad_norm": 1.7467187643051147,
      "learning_rate": 0.0001963984061787937,
      "loss": 1.223,
      "step": 11646
    },
    {
      "epoch": 1.637424434134683,
      "grad_norm": 1.410304307937622,
      "learning_rate": 0.00019633631816821066,
      "loss": 1.1211,
      "step": 11647
    },
    {
      "epoch": 1.6375650217910867,
      "grad_norm": 1.605039358139038,
      "learning_rate": 0.00019627370953652834,
      "loss": 1.0223,
      "step": 11648
    },
    {
      "epoch": 1.6377056094474907,
      "grad_norm": 1.5725982189178467,
      "learning_rate": 0.00019621058062209662,
      "loss": 1.0804,
      "step": 11649
    },
    {
      "epoch": 1.6378461971038942,
      "grad_norm": 1.7973521947860718,
      "learning_rate": 0.00019614693176607683,
      "loss": 0.9288,
      "step": 11650
    },
    {
      "epoch": 1.637986784760298,
      "grad_norm": 1.607271671295166,
      "learning_rate": 0.00019608276331244056,
      "loss": 1.1096,
      "step": 11651
    },
    {
      "epoch": 1.6381273724167018,
      "grad_norm": 2.0023016929626465,
      "learning_rate": 0.00019601807560796713,
      "loss": 0.8844,
      "step": 11652
    },
    {
      "epoch": 1.6382679600731056,
      "grad_norm": 1.5995283126831055,
      "learning_rate": 0.00019595286900224212,
      "loss": 1.1139,
      "step": 11653
    },
    {
      "epoch": 1.6384085477295094,
      "grad_norm": 1.8800557851791382,
      "learning_rate": 0.0001958871438476554,
      "loss": 1.0937,
      "step": 11654
    },
    {
      "epoch": 1.638549135385913,
      "grad_norm": 2.2949326038360596,
      "learning_rate": 0.00019582090049939885,
      "loss": 1.1348,
      "step": 11655
    },
    {
      "epoch": 1.638689723042317,
      "grad_norm": 2.0213942527770996,
      "learning_rate": 0.00019575413931546522,
      "loss": 1.0833,
      "step": 11656
    },
    {
      "epoch": 1.6388303106987205,
      "grad_norm": 1.7081094980239868,
      "learning_rate": 0.0001956868606566455,
      "loss": 1.0319,
      "step": 11657
    },
    {
      "epoch": 1.6389708983551246,
      "grad_norm": 1.718505620956421,
      "learning_rate": 0.00019561906488652718,
      "loss": 1.0619,
      "step": 11658
    },
    {
      "epoch": 1.6391114860115281,
      "grad_norm": 2.387927532196045,
      "learning_rate": 0.00019555075237149273,
      "loss": 1.1155,
      "step": 11659
    },
    {
      "epoch": 1.639252073667932,
      "grad_norm": 1.7193681001663208,
      "learning_rate": 0.0001954819234807166,
      "loss": 1.2037,
      "step": 11660
    },
    {
      "epoch": 1.6393926613243357,
      "grad_norm": 1.4961495399475098,
      "learning_rate": 0.0001954125785861642,
      "loss": 1.1179,
      "step": 11661
    },
    {
      "epoch": 1.6395332489807395,
      "grad_norm": 1.3919161558151245,
      "learning_rate": 0.00019534271806258957,
      "loss": 1.2739,
      "step": 11662
    },
    {
      "epoch": 1.6396738366371433,
      "grad_norm": 1.524675726890564,
      "learning_rate": 0.00019527234228753314,
      "loss": 1.1731,
      "step": 11663
    },
    {
      "epoch": 1.639814424293547,
      "grad_norm": 1.5644303560256958,
      "learning_rate": 0.00019520145164131998,
      "loss": 1.0273,
      "step": 11664
    },
    {
      "epoch": 1.6399550119499509,
      "grad_norm": 1.5408228635787964,
      "learning_rate": 0.00019513004650705757,
      "loss": 1.1628,
      "step": 11665
    },
    {
      "epoch": 1.6400955996063544,
      "grad_norm": 1.7238807678222656,
      "learning_rate": 0.00019505812727063383,
      "loss": 1.2336,
      "step": 11666
    },
    {
      "epoch": 1.6402361872627584,
      "grad_norm": 1.4697487354278564,
      "learning_rate": 0.000194985694320715,
      "loss": 1.0112,
      "step": 11667
    },
    {
      "epoch": 1.640376774919162,
      "grad_norm": 1.5639746189117432,
      "learning_rate": 0.0001949127480487434,
      "loss": 1.0234,
      "step": 11668
    },
    {
      "epoch": 1.640517362575566,
      "grad_norm": 1.7400630712509155,
      "learning_rate": 0.00019483928884893566,
      "loss": 1.1231,
      "step": 11669
    },
    {
      "epoch": 1.6406579502319696,
      "grad_norm": 1.5346964597702026,
      "learning_rate": 0.00019476531711828035,
      "loss": 1.0911,
      "step": 11670
    },
    {
      "epoch": 1.6407985378883734,
      "grad_norm": 1.7915570735931396,
      "learning_rate": 0.00019469083325653554,
      "loss": 1.2277,
      "step": 11671
    },
    {
      "epoch": 1.6409391255447772,
      "grad_norm": 1.672912359237671,
      "learning_rate": 0.0001946158376662273,
      "loss": 1.0998,
      "step": 11672
    },
    {
      "epoch": 1.641079713201181,
      "grad_norm": 1.5365856885910034,
      "learning_rate": 0.00019454033075264705,
      "loss": 0.9286,
      "step": 11673
    },
    {
      "epoch": 1.6412203008575847,
      "grad_norm": 1.792723536491394,
      "learning_rate": 0.00019446431292384966,
      "loss": 1.0307,
      "step": 11674
    },
    {
      "epoch": 1.6413608885139883,
      "grad_norm": 1.5634117126464844,
      "learning_rate": 0.00019438778459065104,
      "loss": 0.963,
      "step": 11675
    },
    {
      "epoch": 1.6415014761703923,
      "grad_norm": 1.5862414836883545,
      "learning_rate": 0.00019431074616662564,
      "loss": 1.0539,
      "step": 11676
    },
    {
      "epoch": 1.641642063826796,
      "grad_norm": 1.590870976448059,
      "learning_rate": 0.00019423319806810495,
      "loss": 1.1129,
      "step": 11677
    },
    {
      "epoch": 1.6417826514832,
      "grad_norm": 1.769247055053711,
      "learning_rate": 0.00019415514071417466,
      "loss": 0.9715,
      "step": 11678
    },
    {
      "epoch": 1.6419232391396035,
      "grad_norm": 1.648077130317688,
      "learning_rate": 0.00019407657452667269,
      "loss": 1.0875,
      "step": 11679
    },
    {
      "epoch": 1.6420638267960073,
      "grad_norm": 2.1318395137786865,
      "learning_rate": 0.00019399749993018666,
      "loss": 0.8623,
      "step": 11680
    },
    {
      "epoch": 1.642204414452411,
      "grad_norm": 1.8807989358901978,
      "learning_rate": 0.00019391791735205185,
      "loss": 1.0908,
      "step": 11681
    },
    {
      "epoch": 1.6423450021088148,
      "grad_norm": 2.121076822280884,
      "learning_rate": 0.0001938378272223487,
      "loss": 1.0824,
      "step": 11682
    },
    {
      "epoch": 1.6424855897652186,
      "grad_norm": 1.8026621341705322,
      "learning_rate": 0.00019375722997390065,
      "loss": 1.2106,
      "step": 11683
    },
    {
      "epoch": 1.6426261774216224,
      "grad_norm": 1.5540440082550049,
      "learning_rate": 0.0001936761260422716,
      "loss": 1.1402,
      "step": 11684
    },
    {
      "epoch": 1.6427667650780262,
      "grad_norm": 1.406972885131836,
      "learning_rate": 0.00019359451586576377,
      "loss": 1.0911,
      "step": 11685
    },
    {
      "epoch": 1.6429073527344298,
      "grad_norm": 2.1710495948791504,
      "learning_rate": 0.00019351239988541524,
      "loss": 1.0063,
      "step": 11686
    },
    {
      "epoch": 1.6430479403908338,
      "grad_norm": 1.7333775758743286,
      "learning_rate": 0.00019342977854499736,
      "loss": 1.3847,
      "step": 11687
    },
    {
      "epoch": 1.6431885280472374,
      "grad_norm": 1.7829949855804443,
      "learning_rate": 0.00019334665229101263,
      "loss": 1.0635,
      "step": 11688
    },
    {
      "epoch": 1.6433291157036414,
      "grad_norm": 1.5142816305160522,
      "learning_rate": 0.00019326302157269255,
      "loss": 1.2119,
      "step": 11689
    },
    {
      "epoch": 1.643469703360045,
      "grad_norm": 1.5862358808517456,
      "learning_rate": 0.00019317888684199436,
      "loss": 1.1502,
      "step": 11690
    },
    {
      "epoch": 1.6436102910164487,
      "grad_norm": 1.8824803829193115,
      "learning_rate": 0.0001930942485535994,
      "loss": 1.1649,
      "step": 11691
    },
    {
      "epoch": 1.6437508786728525,
      "grad_norm": 2.146298408508301,
      "learning_rate": 0.00019300910716491036,
      "loss": 1.0231,
      "step": 11692
    },
    {
      "epoch": 1.6438914663292563,
      "grad_norm": 1.6581543684005737,
      "learning_rate": 0.00019292346313604832,
      "loss": 1.2061,
      "step": 11693
    },
    {
      "epoch": 1.64403205398566,
      "grad_norm": 1.6150192022323608,
      "learning_rate": 0.00019283731692985124,
      "loss": 1.1591,
      "step": 11694
    },
    {
      "epoch": 1.6441726416420637,
      "grad_norm": 1.5974018573760986,
      "learning_rate": 0.00019275066901187077,
      "loss": 0.9826,
      "step": 11695
    },
    {
      "epoch": 1.6443132292984677,
      "grad_norm": 1.7387462854385376,
      "learning_rate": 0.00019266351985036982,
      "loss": 1.0438,
      "step": 11696
    },
    {
      "epoch": 1.6444538169548713,
      "grad_norm": 1.6341696977615356,
      "learning_rate": 0.00019257586991632057,
      "loss": 1.0181,
      "step": 11697
    },
    {
      "epoch": 1.6445944046112753,
      "grad_norm": 1.7914018630981445,
      "learning_rate": 0.00019248771968340077,
      "loss": 1.138,
      "step": 11698
    },
    {
      "epoch": 1.6447349922676788,
      "grad_norm": 1.7234265804290771,
      "learning_rate": 0.00019239906962799234,
      "loss": 1.0935,
      "step": 11699
    },
    {
      "epoch": 1.6448755799240826,
      "grad_norm": 2.0748937129974365,
      "learning_rate": 0.00019230992022917828,
      "loss": 1.1065,
      "step": 11700
    },
    {
      "epoch": 1.6450161675804864,
      "grad_norm": 1.9242807626724243,
      "learning_rate": 0.00019222027196874012,
      "loss": 1.0831,
      "step": 11701
    },
    {
      "epoch": 1.6451567552368902,
      "grad_norm": 1.6601589918136597,
      "learning_rate": 0.0001921301253311554,
      "loss": 1.0115,
      "step": 11702
    },
    {
      "epoch": 1.645297342893294,
      "grad_norm": 1.5963389873504639,
      "learning_rate": 0.0001920394808035946,
      "loss": 1.0208,
      "step": 11703
    },
    {
      "epoch": 1.6454379305496978,
      "grad_norm": 1.5713002681732178,
      "learning_rate": 0.00019194833887591962,
      "loss": 0.9931,
      "step": 11704
    },
    {
      "epoch": 1.6455785182061016,
      "grad_norm": 1.8509238958358765,
      "learning_rate": 0.00019185670004067984,
      "loss": 1.1164,
      "step": 11705
    },
    {
      "epoch": 1.6457191058625051,
      "grad_norm": 1.531258225440979,
      "learning_rate": 0.00019176456479311023,
      "loss": 0.952,
      "step": 11706
    },
    {
      "epoch": 1.6458596935189092,
      "grad_norm": 1.8666609525680542,
      "learning_rate": 0.0001916719336311284,
      "loss": 1.0048,
      "step": 11707
    },
    {
      "epoch": 1.6460002811753127,
      "grad_norm": 1.4184993505477905,
      "learning_rate": 0.00019157880705533223,
      "loss": 1.0714,
      "step": 11708
    },
    {
      "epoch": 1.6461408688317167,
      "grad_norm": 1.3734252452850342,
      "learning_rate": 0.00019148518556899645,
      "loss": 1.095,
      "step": 11709
    },
    {
      "epoch": 1.6462814564881203,
      "grad_norm": 1.8046261072158813,
      "learning_rate": 0.0001913910696780707,
      "loss": 1.2547,
      "step": 11710
    },
    {
      "epoch": 1.646422044144524,
      "grad_norm": 1.2685518264770508,
      "learning_rate": 0.0001912964598911764,
      "loss": 0.9299,
      "step": 11711
    },
    {
      "epoch": 1.6465626318009279,
      "grad_norm": 1.603243112564087,
      "learning_rate": 0.00019120135671960432,
      "loss": 1.0182,
      "step": 11712
    },
    {
      "epoch": 1.6467032194573317,
      "grad_norm": 1.890575647354126,
      "learning_rate": 0.00019110576067731146,
      "loss": 1.1194,
      "step": 11713
    },
    {
      "epoch": 1.6468438071137355,
      "grad_norm": 1.5166107416152954,
      "learning_rate": 0.000191009672280918,
      "loss": 1.1763,
      "step": 11714
    },
    {
      "epoch": 1.646984394770139,
      "grad_norm": 1.5871070623397827,
      "learning_rate": 0.00019091309204970536,
      "loss": 1.1242,
      "step": 11715
    },
    {
      "epoch": 1.647124982426543,
      "grad_norm": 1.6591026782989502,
      "learning_rate": 0.0001908160205056128,
      "loss": 1.0807,
      "step": 11716
    },
    {
      "epoch": 1.6472655700829466,
      "grad_norm": 1.802456259727478,
      "learning_rate": 0.0001907184581732347,
      "loss": 1.1303,
      "step": 11717
    },
    {
      "epoch": 1.6474061577393506,
      "grad_norm": 1.783187985420227,
      "learning_rate": 0.00019062040557981795,
      "loss": 1.0635,
      "step": 11718
    },
    {
      "epoch": 1.6475467453957542,
      "grad_norm": 1.7950727939605713,
      "learning_rate": 0.00019052186325525835,
      "loss": 1.3002,
      "step": 11719
    },
    {
      "epoch": 1.647687333052158,
      "grad_norm": 1.7959250211715698,
      "learning_rate": 0.00019042283173209911,
      "loss": 1.2152,
      "step": 11720
    },
    {
      "epoch": 1.6478279207085618,
      "grad_norm": 1.5708909034729004,
      "learning_rate": 0.00019032331154552663,
      "loss": 0.9356,
      "step": 11721
    },
    {
      "epoch": 1.6479685083649656,
      "grad_norm": 1.6337559223175049,
      "learning_rate": 0.0001902233032333683,
      "loss": 1.2106,
      "step": 11722
    },
    {
      "epoch": 1.6481090960213693,
      "grad_norm": 1.547193169593811,
      "learning_rate": 0.00019012280733608937,
      "loss": 1.1025,
      "step": 11723
    },
    {
      "epoch": 1.6482496836777731,
      "grad_norm": 1.5589425563812256,
      "learning_rate": 0.0001900218243967904,
      "loss": 1.1371,
      "step": 11724
    },
    {
      "epoch": 1.648390271334177,
      "grad_norm": 1.6819084882736206,
      "learning_rate": 0.00018992035496120339,
      "loss": 1.0652,
      "step": 11725
    },
    {
      "epoch": 1.6485308589905805,
      "grad_norm": 2.0734994411468506,
      "learning_rate": 0.00018981839957768983,
      "loss": 0.9936,
      "step": 11726
    },
    {
      "epoch": 1.6486714466469845,
      "grad_norm": 1.4194837808609009,
      "learning_rate": 0.00018971595879723778,
      "loss": 1.2316,
      "step": 11727
    },
    {
      "epoch": 1.648812034303388,
      "grad_norm": 1.5603468418121338,
      "learning_rate": 0.0001896130331734579,
      "loss": 1.1822,
      "step": 11728
    },
    {
      "epoch": 1.648952621959792,
      "grad_norm": 1.7417210340499878,
      "learning_rate": 0.00018950962326258142,
      "loss": 1.1507,
      "step": 11729
    },
    {
      "epoch": 1.6490932096161957,
      "grad_norm": 1.493865728378296,
      "learning_rate": 0.00018940572962345624,
      "loss": 1.108,
      "step": 11730
    },
    {
      "epoch": 1.6492337972725994,
      "grad_norm": 1.691108226776123,
      "learning_rate": 0.0001893013528175449,
      "loss": 1.0109,
      "step": 11731
    },
    {
      "epoch": 1.6493743849290032,
      "grad_norm": 1.8127241134643555,
      "learning_rate": 0.00018919649340892102,
      "loss": 1.1664,
      "step": 11732
    },
    {
      "epoch": 1.649514972585407,
      "grad_norm": 1.585924744606018,
      "learning_rate": 0.00018909115196426613,
      "loss": 1.2217,
      "step": 11733
    },
    {
      "epoch": 1.6496555602418108,
      "grad_norm": 1.6262249946594238,
      "learning_rate": 0.00018898532905286674,
      "loss": 0.9085,
      "step": 11734
    },
    {
      "epoch": 1.6497961478982144,
      "grad_norm": 1.9744746685028076,
      "learning_rate": 0.0001888790252466119,
      "loss": 1.1768,
      "step": 11735
    },
    {
      "epoch": 1.6499367355546184,
      "grad_norm": 1.84495210647583,
      "learning_rate": 0.00018877224111998856,
      "loss": 1.142,
      "step": 11736
    },
    {
      "epoch": 1.650077323211022,
      "grad_norm": 1.8641852140426636,
      "learning_rate": 0.0001886649772500801,
      "loss": 1.12,
      "step": 11737
    },
    {
      "epoch": 1.650217910867426,
      "grad_norm": 1.7334622144699097,
      "learning_rate": 0.00018855723421656233,
      "loss": 1.1021,
      "step": 11738
    },
    {
      "epoch": 1.6503584985238295,
      "grad_norm": 1.7768957614898682,
      "learning_rate": 0.00018844901260170054,
      "loss": 1.0945,
      "step": 11739
    },
    {
      "epoch": 1.6504990861802333,
      "grad_norm": 1.5250481367111206,
      "learning_rate": 0.0001883403129903466,
      "loss": 1.031,
      "step": 11740
    },
    {
      "epoch": 1.6506396738366371,
      "grad_norm": 1.5491888523101807,
      "learning_rate": 0.00018823113596993477,
      "loss": 1.2549,
      "step": 11741
    },
    {
      "epoch": 1.650780261493041,
      "grad_norm": 2.076143980026245,
      "learning_rate": 0.00018812148213048056,
      "loss": 1.1226,
      "step": 11742
    },
    {
      "epoch": 1.6509208491494447,
      "grad_norm": 1.9230597019195557,
      "learning_rate": 0.00018801135206457538,
      "loss": 1.2366,
      "step": 11743
    },
    {
      "epoch": 1.6510614368058485,
      "grad_norm": 1.9267319440841675,
      "learning_rate": 0.00018790074636738466,
      "loss": 0.9344,
      "step": 11744
    },
    {
      "epoch": 1.6512020244622523,
      "grad_norm": 1.459540843963623,
      "learning_rate": 0.0001877896656366442,
      "loss": 1.1693,
      "step": 11745
    },
    {
      "epoch": 1.6513426121186559,
      "grad_norm": 1.8009849786758423,
      "learning_rate": 0.00018767811047265664,
      "loss": 1.0435,
      "step": 11746
    },
    {
      "epoch": 1.6514831997750599,
      "grad_norm": 1.5796175003051758,
      "learning_rate": 0.00018756608147828897,
      "loss": 1.209,
      "step": 11747
    },
    {
      "epoch": 1.6516237874314634,
      "grad_norm": 1.5535259246826172,
      "learning_rate": 0.00018745357925896872,
      "loss": 1.1806,
      "step": 11748
    },
    {
      "epoch": 1.6517643750878674,
      "grad_norm": 1.5454810857772827,
      "learning_rate": 0.00018734060442268064,
      "loss": 1.0886,
      "step": 11749
    },
    {
      "epoch": 1.651904962744271,
      "grad_norm": 1.6227366924285889,
      "learning_rate": 0.0001872271575799641,
      "loss": 1.0564,
      "step": 11750
    },
    {
      "epoch": 1.6520455504006748,
      "grad_norm": 1.6011407375335693,
      "learning_rate": 0.00018711323934390896,
      "loss": 1.2759,
      "step": 11751
    },
    {
      "epoch": 1.6521861380570786,
      "grad_norm": 1.6415534019470215,
      "learning_rate": 0.0001869988503301523,
      "loss": 1.0724,
      "step": 11752
    },
    {
      "epoch": 1.6523267257134824,
      "grad_norm": 1.582748532295227,
      "learning_rate": 0.00018688399115687575,
      "loss": 1.2482,
      "step": 11753
    },
    {
      "epoch": 1.6524673133698862,
      "grad_norm": 1.655048131942749,
      "learning_rate": 0.00018676866244480173,
      "loss": 1.0416,
      "step": 11754
    },
    {
      "epoch": 1.6526079010262897,
      "grad_norm": 1.6339164972305298,
      "learning_rate": 0.00018665286481719015,
      "loss": 1.2038,
      "step": 11755
    },
    {
      "epoch": 1.6527484886826938,
      "grad_norm": 1.9647080898284912,
      "learning_rate": 0.0001865365988998351,
      "loss": 1.0745,
      "step": 11756
    },
    {
      "epoch": 1.6528890763390973,
      "grad_norm": 1.471436619758606,
      "learning_rate": 0.00018641986532106085,
      "loss": 1.1432,
      "step": 11757
    },
    {
      "epoch": 1.6530296639955013,
      "grad_norm": 1.5959771871566772,
      "learning_rate": 0.00018630266471171992,
      "loss": 0.9648,
      "step": 11758
    },
    {
      "epoch": 1.653170251651905,
      "grad_norm": 1.560049057006836,
      "learning_rate": 0.00018618499770518807,
      "loss": 1.2028,
      "step": 11759
    },
    {
      "epoch": 1.6533108393083087,
      "grad_norm": 1.6722699403762817,
      "learning_rate": 0.0001860668649373619,
      "loss": 1.0434,
      "step": 11760
    },
    {
      "epoch": 1.6534514269647125,
      "grad_norm": 1.5258287191390991,
      "learning_rate": 0.00018594826704665507,
      "loss": 0.9605,
      "step": 11761
    },
    {
      "epoch": 1.6535920146211163,
      "grad_norm": 1.630470633506775,
      "learning_rate": 0.0001858292046739944,
      "loss": 1.0929,
      "step": 11762
    },
    {
      "epoch": 1.65373260227752,
      "grad_norm": 1.4674980640411377,
      "learning_rate": 0.00018570967846281744,
      "loss": 1.237,
      "step": 11763
    },
    {
      "epoch": 1.6538731899339238,
      "grad_norm": 1.7155797481536865,
      "learning_rate": 0.00018558968905906797,
      "loss": 1.0296,
      "step": 11764
    },
    {
      "epoch": 1.6540137775903276,
      "grad_norm": 1.628527283668518,
      "learning_rate": 0.0001854692371111936,
      "loss": 1.2844,
      "step": 11765
    },
    {
      "epoch": 1.6541543652467312,
      "grad_norm": 1.9819083213806152,
      "learning_rate": 0.00018534832327014096,
      "loss": 1.1359,
      "step": 11766
    },
    {
      "epoch": 1.6542949529031352,
      "grad_norm": 1.540039300918579,
      "learning_rate": 0.00018522694818935335,
      "loss": 1.2701,
      "step": 11767
    },
    {
      "epoch": 1.6544355405595388,
      "grad_norm": 1.5708045959472656,
      "learning_rate": 0.00018510511252476598,
      "loss": 1.3479,
      "step": 11768
    },
    {
      "epoch": 1.6545761282159428,
      "grad_norm": 1.632270097732544,
      "learning_rate": 0.0001849828169348039,
      "loss": 1.2039,
      "step": 11769
    },
    {
      "epoch": 1.6547167158723464,
      "grad_norm": 1.930690050125122,
      "learning_rate": 0.00018486006208037737,
      "loss": 1.0816,
      "step": 11770
    },
    {
      "epoch": 1.6548573035287502,
      "grad_norm": 1.666070580482483,
      "learning_rate": 0.0001847368486248786,
      "loss": 1.0355,
      "step": 11771
    },
    {
      "epoch": 1.654997891185154,
      "grad_norm": 1.5705745220184326,
      "learning_rate": 0.0001846131772341783,
      "loss": 1.2019,
      "step": 11772
    },
    {
      "epoch": 1.6551384788415577,
      "grad_norm": 1.6798900365829468,
      "learning_rate": 0.00018448904857662176,
      "loss": 1.2474,
      "step": 11773
    },
    {
      "epoch": 1.6552790664979615,
      "grad_norm": 1.5750303268432617,
      "learning_rate": 0.00018436446332302566,
      "loss": 1.1223,
      "step": 11774
    },
    {
      "epoch": 1.655419654154365,
      "grad_norm": 1.6664471626281738,
      "learning_rate": 0.00018423942214667406,
      "loss": 1.249,
      "step": 11775
    },
    {
      "epoch": 1.655560241810769,
      "grad_norm": 1.9599359035491943,
      "learning_rate": 0.00018411392572331497,
      "loss": 1.1595,
      "step": 11776
    },
    {
      "epoch": 1.6557008294671727,
      "grad_norm": 1.6690821647644043,
      "learning_rate": 0.00018398797473115661,
      "loss": 1.2056,
      "step": 11777
    },
    {
      "epoch": 1.6558414171235767,
      "grad_norm": 1.7271329164505005,
      "learning_rate": 0.000183861569850864,
      "loss": 1.1058,
      "step": 11778
    },
    {
      "epoch": 1.6559820047799803,
      "grad_norm": 1.6580156087875366,
      "learning_rate": 0.0001837347117655546,
      "loss": 1.0597,
      "step": 11779
    },
    {
      "epoch": 1.656122592436384,
      "grad_norm": 1.5909699201583862,
      "learning_rate": 0.00018360740116079525,
      "loss": 1.0872,
      "step": 11780
    },
    {
      "epoch": 1.6562631800927878,
      "grad_norm": 1.6576110124588013,
      "learning_rate": 0.0001834796387245988,
      "loss": 1.3142,
      "step": 11781
    },
    {
      "epoch": 1.6564037677491916,
      "grad_norm": 1.7527357339859009,
      "learning_rate": 0.00018335142514741933,
      "loss": 1.1841,
      "step": 11782
    },
    {
      "epoch": 1.6565443554055954,
      "grad_norm": 1.6072649955749512,
      "learning_rate": 0.00018322276112214936,
      "loss": 1.0644,
      "step": 11783
    },
    {
      "epoch": 1.6566849430619992,
      "grad_norm": 1.7913228273391724,
      "learning_rate": 0.0001830936473441151,
      "loss": 1.1136,
      "step": 11784
    },
    {
      "epoch": 1.656825530718403,
      "grad_norm": 1.7527903318405151,
      "learning_rate": 0.0001829640845110739,
      "loss": 1.0627,
      "step": 11785
    },
    {
      "epoch": 1.6569661183748066,
      "grad_norm": 1.471692442893982,
      "learning_rate": 0.00018283407332320972,
      "loss": 0.9906,
      "step": 11786
    },
    {
      "epoch": 1.6571067060312106,
      "grad_norm": 1.6241720914840698,
      "learning_rate": 0.00018270361448312935,
      "loss": 1.2452,
      "step": 11787
    },
    {
      "epoch": 1.6572472936876141,
      "grad_norm": 1.644593596458435,
      "learning_rate": 0.00018257270869585955,
      "loss": 1.0924,
      "step": 11788
    },
    {
      "epoch": 1.6573878813440182,
      "grad_norm": 1.6765389442443848,
      "learning_rate": 0.0001824413566688412,
      "loss": 1.0999,
      "step": 11789
    },
    {
      "epoch": 1.6575284690004217,
      "grad_norm": 1.645157814025879,
      "learning_rate": 0.00018230955911192775,
      "loss": 1.0962,
      "step": 11790
    },
    {
      "epoch": 1.6576690566568255,
      "grad_norm": 1.7102305889129639,
      "learning_rate": 0.00018217731673737998,
      "loss": 1.1464,
      "step": 11791
    },
    {
      "epoch": 1.6578096443132293,
      "grad_norm": 1.6991312503814697,
      "learning_rate": 0.00018204463025986263,
      "loss": 1.0734,
      "step": 11792
    },
    {
      "epoch": 1.657950231969633,
      "grad_norm": 1.6909769773483276,
      "learning_rate": 0.00018191150039644052,
      "loss": 1.0758,
      "step": 11793
    },
    {
      "epoch": 1.6580908196260369,
      "grad_norm": 1.4853566884994507,
      "learning_rate": 0.00018177792786657472,
      "loss": 1.0247,
      "step": 11794
    },
    {
      "epoch": 1.6582314072824405,
      "grad_norm": 1.6766756772994995,
      "learning_rate": 0.00018164391339211785,
      "loss": 1.1419,
      "step": 11795
    },
    {
      "epoch": 1.6583719949388445,
      "grad_norm": 1.735650658607483,
      "learning_rate": 0.00018150945769731202,
      "loss": 1.1087,
      "step": 11796
    },
    {
      "epoch": 1.658512582595248,
      "grad_norm": 1.8258692026138306,
      "learning_rate": 0.000181374561508783,
      "loss": 1.0446,
      "step": 11797
    },
    {
      "epoch": 1.658653170251652,
      "grad_norm": 1.7709332704544067,
      "learning_rate": 0.00018123922555553733,
      "loss": 1.0806,
      "step": 11798
    },
    {
      "epoch": 1.6587937579080556,
      "grad_norm": 1.5978292226791382,
      "learning_rate": 0.00018110345056895826,
      "loss": 1.0408,
      "step": 11799
    },
    {
      "epoch": 1.6589343455644594,
      "grad_norm": 1.8196368217468262,
      "learning_rate": 0.00018096723728280108,
      "loss": 0.9648,
      "step": 11800
    },
    {
      "epoch": 1.6590749332208632,
      "grad_norm": 1.6948010921478271,
      "learning_rate": 0.00018083058643319035,
      "loss": 0.938,
      "step": 11801
    },
    {
      "epoch": 1.659215520877267,
      "grad_norm": 1.6160110235214233,
      "learning_rate": 0.00018069349875861495,
      "loss": 1.187,
      "step": 11802
    },
    {
      "epoch": 1.6593561085336708,
      "grad_norm": 1.9182488918304443,
      "learning_rate": 0.00018055597499992495,
      "loss": 1.0379,
      "step": 11803
    },
    {
      "epoch": 1.6594966961900746,
      "grad_norm": 1.7742211818695068,
      "learning_rate": 0.00018041801590032666,
      "loss": 1.2435,
      "step": 11804
    },
    {
      "epoch": 1.6596372838464784,
      "grad_norm": 1.9191220998764038,
      "learning_rate": 0.00018027962220537933,
      "loss": 1.1067,
      "step": 11805
    },
    {
      "epoch": 1.659777871502882,
      "grad_norm": 1.5803322792053223,
      "learning_rate": 0.00018014079466299027,
      "loss": 1.0352,
      "step": 11806
    },
    {
      "epoch": 1.659918459159286,
      "grad_norm": 1.6988335847854614,
      "learning_rate": 0.00018000153402341208,
      "loss": 1.2174,
      "step": 11807
    },
    {
      "epoch": 1.6600590468156895,
      "grad_norm": 1.845529317855835,
      "learning_rate": 0.00017986184103923757,
      "loss": 1.0095,
      "step": 11808
    },
    {
      "epoch": 1.6601996344720935,
      "grad_norm": 1.6111633777618408,
      "learning_rate": 0.00017972171646539614,
      "loss": 1.0593,
      "step": 11809
    },
    {
      "epoch": 1.660340222128497,
      "grad_norm": 1.8677116632461548,
      "learning_rate": 0.00017958116105914952,
      "loss": 0.9595,
      "step": 11810
    },
    {
      "epoch": 1.6604808097849009,
      "grad_norm": 2.1835570335388184,
      "learning_rate": 0.00017944017558008783,
      "loss": 0.9597,
      "step": 11811
    },
    {
      "epoch": 1.6606213974413047,
      "grad_norm": 1.8167998790740967,
      "learning_rate": 0.00017929876079012528,
      "loss": 1.1082,
      "step": 11812
    },
    {
      "epoch": 1.6607619850977084,
      "grad_norm": 1.5876342058181763,
      "learning_rate": 0.00017915691745349626,
      "loss": 1.0929,
      "step": 11813
    },
    {
      "epoch": 1.6609025727541122,
      "grad_norm": 1.6618244647979736,
      "learning_rate": 0.000179014646336751,
      "loss": 1.1336,
      "step": 11814
    },
    {
      "epoch": 1.6610431604105158,
      "grad_norm": 1.5479387044906616,
      "learning_rate": 0.00017887194820875198,
      "loss": 1.0952,
      "step": 11815
    },
    {
      "epoch": 1.6611837480669198,
      "grad_norm": 1.487290382385254,
      "learning_rate": 0.00017872882384066836,
      "loss": 1.0853,
      "step": 11816
    },
    {
      "epoch": 1.6613243357233234,
      "grad_norm": 1.5462335348129272,
      "learning_rate": 0.0001785852740059737,
      "loss": 1.1203,
      "step": 11817
    },
    {
      "epoch": 1.6614649233797274,
      "grad_norm": 1.790018081665039,
      "learning_rate": 0.00017844129948044036,
      "loss": 0.9673,
      "step": 11818
    },
    {
      "epoch": 1.661605511036131,
      "grad_norm": 1.8254492282867432,
      "learning_rate": 0.00017829690104213643,
      "loss": 0.9982,
      "step": 11819
    },
    {
      "epoch": 1.6617460986925348,
      "grad_norm": 1.5776041746139526,
      "learning_rate": 0.0001781520794714203,
      "loss": 1.0933,
      "step": 11820
    },
    {
      "epoch": 1.6618866863489385,
      "grad_norm": 1.609555721282959,
      "learning_rate": 0.00017800683555093747,
      "loss": 1.2721,
      "step": 11821
    },
    {
      "epoch": 1.6620272740053423,
      "grad_norm": 2.160881280899048,
      "learning_rate": 0.00017786117006561523,
      "loss": 1.1017,
      "step": 11822
    },
    {
      "epoch": 1.6621678616617461,
      "grad_norm": 1.5965642929077148,
      "learning_rate": 0.00017771508380265983,
      "loss": 1.1664,
      "step": 11823
    },
    {
      "epoch": 1.66230844931815,
      "grad_norm": 1.797222375869751,
      "learning_rate": 0.00017756857755155114,
      "loss": 1.0925,
      "step": 11824
    },
    {
      "epoch": 1.6624490369745537,
      "grad_norm": 1.599274754524231,
      "learning_rate": 0.00017742165210403864,
      "loss": 1.1281,
      "step": 11825
    },
    {
      "epoch": 1.6625896246309573,
      "grad_norm": 1.7279024124145508,
      "learning_rate": 0.00017727430825413803,
      "loss": 1.2465,
      "step": 11826
    },
    {
      "epoch": 1.6627302122873613,
      "grad_norm": 1.699995517730713,
      "learning_rate": 0.00017712654679812484,
      "loss": 1.2245,
      "step": 11827
    },
    {
      "epoch": 1.6628707999437649,
      "grad_norm": 2.7263011932373047,
      "learning_rate": 0.00017697836853453232,
      "loss": 1.1555,
      "step": 11828
    },
    {
      "epoch": 1.6630113876001689,
      "grad_norm": 1.9059841632843018,
      "learning_rate": 0.00017682977426414605,
      "loss": 1.0108,
      "step": 11829
    },
    {
      "epoch": 1.6631519752565724,
      "grad_norm": 1.997257113456726,
      "learning_rate": 0.0001766807647899997,
      "loss": 1.1814,
      "step": 11830
    },
    {
      "epoch": 1.6632925629129762,
      "grad_norm": 1.6173940896987915,
      "learning_rate": 0.00017653134091737108,
      "loss": 1.0249,
      "step": 11831
    },
    {
      "epoch": 1.66343315056938,
      "grad_norm": 1.477893590927124,
      "learning_rate": 0.0001763815034537768,
      "loss": 1.0494,
      "step": 11832
    },
    {
      "epoch": 1.6635737382257838,
      "grad_norm": 1.4488072395324707,
      "learning_rate": 0.00017623125320896906,
      "loss": 1.1708,
      "step": 11833
    },
    {
      "epoch": 1.6637143258821876,
      "grad_norm": 2.0903375148773193,
      "learning_rate": 0.0001760805909949313,
      "loss": 1.025,
      "step": 11834
    },
    {
      "epoch": 1.6638549135385912,
      "grad_norm": 1.6301279067993164,
      "learning_rate": 0.00017592951762587257,
      "loss": 1.2163,
      "step": 11835
    },
    {
      "epoch": 1.6639955011949952,
      "grad_norm": 1.6675766706466675,
      "learning_rate": 0.00017577803391822413,
      "loss": 1.0008,
      "step": 11836
    },
    {
      "epoch": 1.6641360888513987,
      "grad_norm": 2.0554277896881104,
      "learning_rate": 0.00017562614069063505,
      "loss": 1.0888,
      "step": 11837
    },
    {
      "epoch": 1.6642766765078028,
      "grad_norm": 1.8051965236663818,
      "learning_rate": 0.00017547383876396686,
      "loss": 1.2121,
      "step": 11838
    },
    {
      "epoch": 1.6644172641642063,
      "grad_norm": 1.6282209157943726,
      "learning_rate": 0.00017532112896129042,
      "loss": 1.0867,
      "step": 11839
    },
    {
      "epoch": 1.6645578518206101,
      "grad_norm": 1.7000160217285156,
      "learning_rate": 0.00017516801210788044,
      "loss": 1.0922,
      "step": 11840
    },
    {
      "epoch": 1.664698439477014,
      "grad_norm": 1.7121024131774902,
      "learning_rate": 0.000175014489031212,
      "loss": 1.1536,
      "step": 11841
    },
    {
      "epoch": 1.6648390271334177,
      "grad_norm": 1.5407826900482178,
      "learning_rate": 0.00017486056056095515,
      "loss": 0.9898,
      "step": 11842
    },
    {
      "epoch": 1.6649796147898215,
      "grad_norm": 1.6811097860336304,
      "learning_rate": 0.00017470622752897022,
      "loss": 1.0844,
      "step": 11843
    },
    {
      "epoch": 1.6651202024462253,
      "grad_norm": 1.8283195495605469,
      "learning_rate": 0.00017455149076930465,
      "loss": 1.0087,
      "step": 11844
    },
    {
      "epoch": 1.665260790102629,
      "grad_norm": 1.797470211982727,
      "learning_rate": 0.00017439635111818737,
      "loss": 1.0971,
      "step": 11845
    },
    {
      "epoch": 1.6654013777590326,
      "grad_norm": 1.561596393585205,
      "learning_rate": 0.00017424080941402475,
      "loss": 1.1009,
      "step": 11846
    },
    {
      "epoch": 1.6655419654154366,
      "grad_norm": 1.7343213558197021,
      "learning_rate": 0.0001740848664973958,
      "loss": 1.1038,
      "step": 11847
    },
    {
      "epoch": 1.6656825530718402,
      "grad_norm": 1.807313084602356,
      "learning_rate": 0.00017392852321104782,
      "loss": 1.0629,
      "step": 11848
    },
    {
      "epoch": 1.6658231407282442,
      "grad_norm": 1.8479487895965576,
      "learning_rate": 0.00017377178039989187,
      "loss": 1.0538,
      "step": 11849
    },
    {
      "epoch": 1.6659637283846478,
      "grad_norm": 1.4808696508407593,
      "learning_rate": 0.00017361463891099797,
      "loss": 1.0956,
      "step": 11850
    },
    {
      "epoch": 1.6661043160410516,
      "grad_norm": 1.7416552305221558,
      "learning_rate": 0.00017345709959359077,
      "loss": 1.1111,
      "step": 11851
    },
    {
      "epoch": 1.6662449036974554,
      "grad_norm": 1.6073617935180664,
      "learning_rate": 0.0001732991632990449,
      "loss": 1.1214,
      "step": 11852
    },
    {
      "epoch": 1.6663854913538592,
      "grad_norm": 1.3027503490447998,
      "learning_rate": 0.0001731408308808805,
      "loss": 1.2621,
      "step": 11853
    },
    {
      "epoch": 1.666526079010263,
      "grad_norm": 1.8183854818344116,
      "learning_rate": 0.00017298210319475774,
      "loss": 1.0992,
      "step": 11854
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 1.370829463005066,
      "learning_rate": 0.00017282298109847367,
      "loss": 1.1252,
      "step": 11855
    },
    {
      "epoch": 1.6668072543230705,
      "grad_norm": 1.900659203529358,
      "learning_rate": 0.0001726634654519562,
      "loss": 1.0654,
      "step": 11856
    },
    {
      "epoch": 1.666947841979474,
      "grad_norm": 1.654539942741394,
      "learning_rate": 0.00017250355711726097,
      "loss": 0.9521,
      "step": 11857
    },
    {
      "epoch": 1.667088429635878,
      "grad_norm": 1.630298376083374,
      "learning_rate": 0.00017234325695856514,
      "loss": 1.304,
      "step": 11858
    },
    {
      "epoch": 1.6672290172922817,
      "grad_norm": 2.1271681785583496,
      "learning_rate": 0.00017218256584216305,
      "loss": 1.055,
      "step": 11859
    },
    {
      "epoch": 1.6673696049486855,
      "grad_norm": 1.5212570428848267,
      "learning_rate": 0.0001720214846364624,
      "loss": 1.2397,
      "step": 11860
    },
    {
      "epoch": 1.6675101926050893,
      "grad_norm": 1.5902458429336548,
      "learning_rate": 0.00017186001421197892,
      "loss": 1.0865,
      "step": 11861
    },
    {
      "epoch": 1.667650780261493,
      "grad_norm": 1.704093098640442,
      "learning_rate": 0.00017169815544133168,
      "loss": 1.0595,
      "step": 11862
    },
    {
      "epoch": 1.6677913679178968,
      "grad_norm": 1.7060284614562988,
      "learning_rate": 0.0001715359091992382,
      "loss": 1.1,
      "step": 11863
    },
    {
      "epoch": 1.6679319555743006,
      "grad_norm": 1.6794886589050293,
      "learning_rate": 0.00017137327636251097,
      "loss": 1.1242,
      "step": 11864
    },
    {
      "epoch": 1.6680725432307044,
      "grad_norm": 1.7452802658081055,
      "learning_rate": 0.0001712102578100503,
      "loss": 0.9926,
      "step": 11865
    },
    {
      "epoch": 1.668213130887108,
      "grad_norm": 1.5361666679382324,
      "learning_rate": 0.0001710468544228419,
      "loss": 1.1042,
      "step": 11866
    },
    {
      "epoch": 1.668353718543512,
      "grad_norm": 1.5304734706878662,
      "learning_rate": 0.00017088306708395095,
      "loss": 0.9849,
      "step": 11867
    },
    {
      "epoch": 1.6684943061999156,
      "grad_norm": 1.6890031099319458,
      "learning_rate": 0.00017071889667851767,
      "loss": 0.9292,
      "step": 11868
    },
    {
      "epoch": 1.6686348938563196,
      "grad_norm": 1.406815767288208,
      "learning_rate": 0.00017055434409375258,
      "loss": 1.009,
      "step": 11869
    },
    {
      "epoch": 1.6687754815127231,
      "grad_norm": 1.7518075704574585,
      "learning_rate": 0.00017038941021893094,
      "loss": 1.1111,
      "step": 11870
    },
    {
      "epoch": 1.668916069169127,
      "grad_norm": 1.7963316440582275,
      "learning_rate": 0.000170224095945389,
      "loss": 1.0745,
      "step": 11871
    },
    {
      "epoch": 1.6690566568255307,
      "grad_norm": 1.7396314144134521,
      "learning_rate": 0.0001700584021665193,
      "loss": 1.1647,
      "step": 11872
    },
    {
      "epoch": 1.6691972444819345,
      "grad_norm": 1.7635875940322876,
      "learning_rate": 0.0001698923297777645,
      "loss": 1.0556,
      "step": 11873
    },
    {
      "epoch": 1.6693378321383383,
      "grad_norm": 2.227457046508789,
      "learning_rate": 0.000169725879676614,
      "loss": 1.1443,
      "step": 11874
    },
    {
      "epoch": 1.6694784197947419,
      "grad_norm": 2.0598695278167725,
      "learning_rate": 0.00016955905276259744,
      "loss": 1.1387,
      "step": 11875
    },
    {
      "epoch": 1.6696190074511459,
      "grad_norm": 1.5943483114242554,
      "learning_rate": 0.00016939184993728184,
      "loss": 1.1144,
      "step": 11876
    },
    {
      "epoch": 1.6697595951075495,
      "grad_norm": 1.435315728187561,
      "learning_rate": 0.00016922427210426524,
      "loss": 1.0881,
      "step": 11877
    },
    {
      "epoch": 1.6699001827639535,
      "grad_norm": 1.474018931388855,
      "learning_rate": 0.0001690563201691722,
      "loss": 1.0798,
      "step": 11878
    },
    {
      "epoch": 1.670040770420357,
      "grad_norm": 1.7421057224273682,
      "learning_rate": 0.00016888799503964974,
      "loss": 0.9715,
      "step": 11879
    },
    {
      "epoch": 1.6701813580767608,
      "grad_norm": 2.2276523113250732,
      "learning_rate": 0.00016871929762536113,
      "loss": 1.0399,
      "step": 11880
    },
    {
      "epoch": 1.6703219457331646,
      "grad_norm": 1.5112344026565552,
      "learning_rate": 0.00016855022883798112,
      "loss": 1.0984,
      "step": 11881
    },
    {
      "epoch": 1.6704625333895684,
      "grad_norm": 1.7384624481201172,
      "learning_rate": 0.0001683807895911921,
      "loss": 1.1777,
      "step": 11882
    },
    {
      "epoch": 1.6706031210459722,
      "grad_norm": 1.7057842016220093,
      "learning_rate": 0.00016821098080067835,
      "loss": 1.2077,
      "step": 11883
    },
    {
      "epoch": 1.670743708702376,
      "grad_norm": 1.883988618850708,
      "learning_rate": 0.0001680408033841211,
      "loss": 1.2574,
      "step": 11884
    },
    {
      "epoch": 1.6708842963587798,
      "grad_norm": 1.6640417575836182,
      "learning_rate": 0.00016787025826119417,
      "loss": 0.9866,
      "step": 11885
    },
    {
      "epoch": 1.6710248840151833,
      "grad_norm": 1.8877215385437012,
      "learning_rate": 0.00016769934635355732,
      "loss": 1.0364,
      "step": 11886
    },
    {
      "epoch": 1.6711654716715874,
      "grad_norm": 1.5759154558181763,
      "learning_rate": 0.0001675280685848541,
      "loss": 1.1458,
      "step": 11887
    },
    {
      "epoch": 1.671306059327991,
      "grad_norm": 1.6200501918792725,
      "learning_rate": 0.00016735642588070423,
      "loss": 0.9484,
      "step": 11888
    },
    {
      "epoch": 1.671446646984395,
      "grad_norm": 1.749748945236206,
      "learning_rate": 0.0001671844191686999,
      "loss": 1.129,
      "step": 11889
    },
    {
      "epoch": 1.6715872346407985,
      "grad_norm": 1.6327780485153198,
      "learning_rate": 0.00016701204937840042,
      "loss": 1.0917,
      "step": 11890
    },
    {
      "epoch": 1.6717278222972023,
      "grad_norm": 1.6086937189102173,
      "learning_rate": 0.00016683931744132754,
      "loss": 0.9927,
      "step": 11891
    },
    {
      "epoch": 1.671868409953606,
      "grad_norm": 1.3906869888305664,
      "learning_rate": 0.00016666622429095933,
      "loss": 1.0858,
      "step": 11892
    },
    {
      "epoch": 1.6720089976100099,
      "grad_norm": 1.6588125228881836,
      "learning_rate": 0.00016649277086272672,
      "loss": 0.9723,
      "step": 11893
    },
    {
      "epoch": 1.6721495852664137,
      "grad_norm": 1.5306833982467651,
      "learning_rate": 0.00016631895809400711,
      "loss": 1.0813,
      "step": 11894
    },
    {
      "epoch": 1.6722901729228172,
      "grad_norm": 1.3922065496444702,
      "learning_rate": 0.00016614478692412066,
      "loss": 1.2016,
      "step": 11895
    },
    {
      "epoch": 1.6724307605792212,
      "grad_norm": 1.7131353616714478,
      "learning_rate": 0.00016597025829432388,
      "loss": 1.2336,
      "step": 11896
    },
    {
      "epoch": 1.6725713482356248,
      "grad_norm": 1.8388797044754028,
      "learning_rate": 0.0001657953731478046,
      "loss": 1.114,
      "step": 11897
    },
    {
      "epoch": 1.6727119358920288,
      "grad_norm": 1.7482446432113647,
      "learning_rate": 0.0001656201324296779,
      "loss": 1.186,
      "step": 11898
    },
    {
      "epoch": 1.6728525235484324,
      "grad_norm": 1.564070701599121,
      "learning_rate": 0.00016544453708698052,
      "loss": 1.1535,
      "step": 11899
    },
    {
      "epoch": 1.6729931112048362,
      "grad_norm": 1.6369441747665405,
      "learning_rate": 0.00016526858806866534,
      "loss": 1.1786,
      "step": 11900
    },
    {
      "epoch": 1.67313369886124,
      "grad_norm": 1.5560939311981201,
      "learning_rate": 0.00016509228632559682,
      "loss": 1.2516,
      "step": 11901
    },
    {
      "epoch": 1.6732742865176438,
      "grad_norm": 2.3154094219207764,
      "learning_rate": 0.00016491563281054544,
      "loss": 1.1045,
      "step": 11902
    },
    {
      "epoch": 1.6734148741740476,
      "grad_norm": 1.724534273147583,
      "learning_rate": 0.00016473862847818282,
      "loss": 1.2914,
      "step": 11903
    },
    {
      "epoch": 1.6735554618304513,
      "grad_norm": 2.394421100616455,
      "learning_rate": 0.00016456127428507648,
      "loss": 1.1376,
      "step": 11904
    },
    {
      "epoch": 1.6736960494868551,
      "grad_norm": 1.5521585941314697,
      "learning_rate": 0.00016438357118968462,
      "loss": 1.1438,
      "step": 11905
    },
    {
      "epoch": 1.6738366371432587,
      "grad_norm": 1.835226058959961,
      "learning_rate": 0.00016420552015235096,
      "loss": 1.048,
      "step": 11906
    },
    {
      "epoch": 1.6739772247996627,
      "grad_norm": 1.7328083515167236,
      "learning_rate": 0.0001640271221352999,
      "loss": 0.9958,
      "step": 11907
    },
    {
      "epoch": 1.6741178124560663,
      "grad_norm": 1.79180908203125,
      "learning_rate": 0.00016384837810263015,
      "loss": 1.0444,
      "step": 11908
    },
    {
      "epoch": 1.6742584001124703,
      "grad_norm": 1.8403728008270264,
      "learning_rate": 0.00016366928902031085,
      "loss": 1.1273,
      "step": 11909
    },
    {
      "epoch": 1.6743989877688739,
      "grad_norm": 1.6134531497955322,
      "learning_rate": 0.00016348985585617646,
      "loss": 1.2327,
      "step": 11910
    },
    {
      "epoch": 1.6745395754252776,
      "grad_norm": 1.7853327989578247,
      "learning_rate": 0.00016331007957992016,
      "loss": 1.0724,
      "step": 11911
    },
    {
      "epoch": 1.6746801630816814,
      "grad_norm": 1.6370738744735718,
      "learning_rate": 0.0001631299611630898,
      "loss": 1.0005,
      "step": 11912
    },
    {
      "epoch": 1.6748207507380852,
      "grad_norm": 1.6351803541183472,
      "learning_rate": 0.00016294950157908148,
      "loss": 1.1218,
      "step": 11913
    },
    {
      "epoch": 1.674961338394489,
      "grad_norm": 1.675756812095642,
      "learning_rate": 0.00016276870180313585,
      "loss": 1.2518,
      "step": 11914
    },
    {
      "epoch": 1.6751019260508926,
      "grad_norm": 2.105257034301758,
      "learning_rate": 0.0001625875628123318,
      "loss": 1.1026,
      "step": 11915
    },
    {
      "epoch": 1.6752425137072966,
      "grad_norm": 1.4301601648330688,
      "learning_rate": 0.00016240608558558127,
      "loss": 1.2085,
      "step": 11916
    },
    {
      "epoch": 1.6753831013637002,
      "grad_norm": 1.8460315465927124,
      "learning_rate": 0.00016222427110362396,
      "loss": 1.1186,
      "step": 11917
    },
    {
      "epoch": 1.6755236890201042,
      "grad_norm": 1.654160737991333,
      "learning_rate": 0.00016204212034902322,
      "loss": 1.0367,
      "step": 11918
    },
    {
      "epoch": 1.6756642766765077,
      "grad_norm": 1.756369948387146,
      "learning_rate": 0.000161859634306158,
      "loss": 1.0637,
      "step": 11919
    },
    {
      "epoch": 1.6758048643329115,
      "grad_norm": 1.6457507610321045,
      "learning_rate": 0.00016167681396122037,
      "loss": 0.9586,
      "step": 11920
    },
    {
      "epoch": 1.6759454519893153,
      "grad_norm": 1.7332909107208252,
      "learning_rate": 0.00016149366030220876,
      "loss": 1.0588,
      "step": 11921
    },
    {
      "epoch": 1.6760860396457191,
      "grad_norm": 1.8978500366210938,
      "learning_rate": 0.00016131017431892278,
      "loss": 1.0881,
      "step": 11922
    },
    {
      "epoch": 1.676226627302123,
      "grad_norm": 1.652522087097168,
      "learning_rate": 0.00016112635700295852,
      "loss": 1.1404,
      "step": 11923
    },
    {
      "epoch": 1.6763672149585267,
      "grad_norm": 1.501683235168457,
      "learning_rate": 0.00016094220934770122,
      "loss": 1.1059,
      "step": 11924
    },
    {
      "epoch": 1.6765078026149305,
      "grad_norm": 1.734613060951233,
      "learning_rate": 0.00016075773234832327,
      "loss": 1.1541,
      "step": 11925
    },
    {
      "epoch": 1.676648390271334,
      "grad_norm": 1.844878911972046,
      "learning_rate": 0.00016057292700177568,
      "loss": 1.1876,
      "step": 11926
    },
    {
      "epoch": 1.676788977927738,
      "grad_norm": 1.4859589338302612,
      "learning_rate": 0.00016038779430678423,
      "loss": 1.0266,
      "step": 11927
    },
    {
      "epoch": 1.6769295655841416,
      "grad_norm": 1.6194374561309814,
      "learning_rate": 0.00016020233526384397,
      "loss": 1.1734,
      "step": 11928
    },
    {
      "epoch": 1.6770701532405454,
      "grad_norm": 1.5448344945907593,
      "learning_rate": 0.00016001655087521284,
      "loss": 0.9745,
      "step": 11929
    },
    {
      "epoch": 1.6772107408969492,
      "grad_norm": 1.7348476648330688,
      "learning_rate": 0.00015983044214490786,
      "loss": 1.1031,
      "step": 11930
    },
    {
      "epoch": 1.677351328553353,
      "grad_norm": 1.5978398323059082,
      "learning_rate": 0.0001596440100786985,
      "loss": 1.1669,
      "step": 11931
    },
    {
      "epoch": 1.6774919162097568,
      "grad_norm": 2.09132719039917,
      "learning_rate": 0.00015945725568410133,
      "loss": 1.176,
      "step": 11932
    },
    {
      "epoch": 1.6776325038661606,
      "grad_norm": 1.5300873517990112,
      "learning_rate": 0.0001592701799703758,
      "loss": 1.2648,
      "step": 11933
    },
    {
      "epoch": 1.6777730915225644,
      "grad_norm": 1.5444339513778687,
      "learning_rate": 0.00015908278394851717,
      "loss": 1.0684,
      "step": 11934
    },
    {
      "epoch": 1.677913679178968,
      "grad_norm": 1.6730003356933594,
      "learning_rate": 0.00015889506863125114,
      "loss": 1.1128,
      "step": 11935
    },
    {
      "epoch": 1.678054266835372,
      "grad_norm": 1.5224299430847168,
      "learning_rate": 0.00015870703503302987,
      "loss": 1.0092,
      "step": 11936
    },
    {
      "epoch": 1.6781948544917755,
      "grad_norm": 1.4869462251663208,
      "learning_rate": 0.00015851868417002526,
      "loss": 1.0327,
      "step": 11937
    },
    {
      "epoch": 1.6783354421481795,
      "grad_norm": 1.9020503759384155,
      "learning_rate": 0.00015833001706012372,
      "loss": 1.04,
      "step": 11938
    },
    {
      "epoch": 1.678476029804583,
      "grad_norm": 1.7262872457504272,
      "learning_rate": 0.00015814103472292076,
      "loss": 1.0456,
      "step": 11939
    },
    {
      "epoch": 1.678616617460987,
      "grad_norm": 1.5183374881744385,
      "learning_rate": 0.00015795173817971555,
      "loss": 1.1045,
      "step": 11940
    },
    {
      "epoch": 1.6787572051173907,
      "grad_norm": 1.7293986082077026,
      "learning_rate": 0.00015776212845350503,
      "loss": 1.0852,
      "step": 11941
    },
    {
      "epoch": 1.6788977927737945,
      "grad_norm": 1.5392416715621948,
      "learning_rate": 0.00015757220656897896,
      "loss": 1.1054,
      "step": 11942
    },
    {
      "epoch": 1.6790383804301983,
      "grad_norm": 1.5512638092041016,
      "learning_rate": 0.00015738197355251386,
      "loss": 1.0419,
      "step": 11943
    },
    {
      "epoch": 1.6791789680866018,
      "grad_norm": 1.4982812404632568,
      "learning_rate": 0.000157191430432168,
      "loss": 1.1925,
      "step": 11944
    },
    {
      "epoch": 1.6793195557430058,
      "grad_norm": 1.8220406770706177,
      "learning_rate": 0.0001570005782376747,
      "loss": 0.9998,
      "step": 11945
    },
    {
      "epoch": 1.6794601433994094,
      "grad_norm": 2.0652761459350586,
      "learning_rate": 0.00015680941800043832,
      "loss": 1.1741,
      "step": 11946
    },
    {
      "epoch": 1.6796007310558134,
      "grad_norm": 1.617567777633667,
      "learning_rate": 0.00015661795075352756,
      "loss": 1.2717,
      "step": 11947
    },
    {
      "epoch": 1.679741318712217,
      "grad_norm": 1.9996333122253418,
      "learning_rate": 0.000156426177531671,
      "loss": 1.0819,
      "step": 11948
    },
    {
      "epoch": 1.6798819063686208,
      "grad_norm": 1.5676016807556152,
      "learning_rate": 0.00015623409937124988,
      "loss": 1.1539,
      "step": 11949
    },
    {
      "epoch": 1.6800224940250246,
      "grad_norm": 1.5710862874984741,
      "learning_rate": 0.00015604171731029405,
      "loss": 1.0283,
      "step": 11950
    },
    {
      "epoch": 1.6801630816814284,
      "grad_norm": 1.6634352207183838,
      "learning_rate": 0.00015584903238847483,
      "loss": 1.1021,
      "step": 11951
    },
    {
      "epoch": 1.6803036693378322,
      "grad_norm": 1.5380610227584839,
      "learning_rate": 0.0001556560456471011,
      "loss": 1.1113,
      "step": 11952
    },
    {
      "epoch": 1.680444256994236,
      "grad_norm": 2.107191801071167,
      "learning_rate": 0.00015546275812911245,
      "loss": 1.1243,
      "step": 11953
    },
    {
      "epoch": 1.6805848446506397,
      "grad_norm": 1.456672191619873,
      "learning_rate": 0.00015526917087907416,
      "loss": 1.1001,
      "step": 11954
    },
    {
      "epoch": 1.6807254323070433,
      "grad_norm": 1.48556649684906,
      "learning_rate": 0.0001550752849431711,
      "loss": 1.079,
      "step": 11955
    },
    {
      "epoch": 1.6808660199634473,
      "grad_norm": 1.3797798156738281,
      "learning_rate": 0.00015488110136920236,
      "loss": 0.9411,
      "step": 11956
    },
    {
      "epoch": 1.6810066076198509,
      "grad_norm": 1.259718894958496,
      "learning_rate": 0.00015468662120657567,
      "loss": 1.1609,
      "step": 11957
    },
    {
      "epoch": 1.681147195276255,
      "grad_norm": 1.5092188119888306,
      "learning_rate": 0.0001544918455063014,
      "loss": 1.1385,
      "step": 11958
    },
    {
      "epoch": 1.6812877829326585,
      "grad_norm": 1.4882378578186035,
      "learning_rate": 0.0001542967753209871,
      "loss": 1.158,
      "step": 11959
    },
    {
      "epoch": 1.6814283705890622,
      "grad_norm": 1.519060492515564,
      "learning_rate": 0.00015410141170483187,
      "loss": 1.0425,
      "step": 11960
    },
    {
      "epoch": 1.681568958245466,
      "grad_norm": 1.4202880859375,
      "learning_rate": 0.0001539057557136208,
      "loss": 1.0943,
      "step": 11961
    },
    {
      "epoch": 1.6817095459018698,
      "grad_norm": 1.8425143957138062,
      "learning_rate": 0.00015370980840471784,
      "loss": 1.1004,
      "step": 11962
    },
    {
      "epoch": 1.6818501335582736,
      "grad_norm": 2.0170841217041016,
      "learning_rate": 0.00015351357083706306,
      "loss": 1.105,
      "step": 11963
    },
    {
      "epoch": 1.6819907212146772,
      "grad_norm": 1.502912998199463,
      "learning_rate": 0.00015331704407116402,
      "loss": 1.1531,
      "step": 11964
    },
    {
      "epoch": 1.6821313088710812,
      "grad_norm": 1.6605615615844727,
      "learning_rate": 0.0001531202291690914,
      "loss": 1.3602,
      "step": 11965
    },
    {
      "epoch": 1.6822718965274848,
      "grad_norm": 1.5121233463287354,
      "learning_rate": 0.00015292312719447334,
      "loss": 1.2047,
      "step": 11966
    },
    {
      "epoch": 1.6824124841838888,
      "grad_norm": 1.4225176572799683,
      "learning_rate": 0.00015272573921248846,
      "loss": 1.1606,
      "step": 11967
    },
    {
      "epoch": 1.6825530718402923,
      "grad_norm": 1.60908043384552,
      "learning_rate": 0.000152528066289862,
      "loss": 1.0764,
      "step": 11968
    },
    {
      "epoch": 1.6826936594966961,
      "grad_norm": 1.512543797492981,
      "learning_rate": 0.00015233010949485874,
      "loss": 1.1322,
      "step": 11969
    },
    {
      "epoch": 1.6828342471531,
      "grad_norm": 1.3815115690231323,
      "learning_rate": 0.00015213186989727716,
      "loss": 1.1486,
      "step": 11970
    },
    {
      "epoch": 1.6829748348095037,
      "grad_norm": 1.7024438381195068,
      "learning_rate": 0.0001519333485684454,
      "loss": 1.129,
      "step": 11971
    },
    {
      "epoch": 1.6831154224659075,
      "grad_norm": 1.45749831199646,
      "learning_rate": 0.00015173454658121233,
      "loss": 1.235,
      "step": 11972
    },
    {
      "epoch": 1.6832560101223113,
      "grad_norm": 1.8795644044876099,
      "learning_rate": 0.00015153546500994467,
      "loss": 1.1337,
      "step": 11973
    },
    {
      "epoch": 1.683396597778715,
      "grad_norm": 2.0736618041992188,
      "learning_rate": 0.0001513361049305198,
      "loss": 1.0486,
      "step": 11974
    },
    {
      "epoch": 1.6835371854351187,
      "grad_norm": 1.54550302028656,
      "learning_rate": 0.0001511364674203202,
      "loss": 0.9842,
      "step": 11975
    },
    {
      "epoch": 1.6836777730915227,
      "grad_norm": 1.56390380859375,
      "learning_rate": 0.00015093655355822772,
      "loss": 1.0912,
      "step": 11976
    },
    {
      "epoch": 1.6838183607479262,
      "grad_norm": 2.000276565551758,
      "learning_rate": 0.0001507363644246176,
      "loss": 0.9629,
      "step": 11977
    },
    {
      "epoch": 1.6839589484043302,
      "grad_norm": 1.563603162765503,
      "learning_rate": 0.00015053590110135272,
      "loss": 1.0724,
      "step": 11978
    },
    {
      "epoch": 1.6840995360607338,
      "grad_norm": 1.685716152191162,
      "learning_rate": 0.00015033516467177776,
      "loss": 0.9293,
      "step": 11979
    },
    {
      "epoch": 1.6842401237171376,
      "grad_norm": 1.6048393249511719,
      "learning_rate": 0.00015013415622071324,
      "loss": 1.0241,
      "step": 11980
    },
    {
      "epoch": 1.6843807113735414,
      "grad_norm": 1.7037512063980103,
      "learning_rate": 0.00014993287683444986,
      "loss": 1.0597,
      "step": 11981
    },
    {
      "epoch": 1.6845212990299452,
      "grad_norm": 1.8579063415527344,
      "learning_rate": 0.0001497313276007427,
      "loss": 1.0457,
      "step": 11982
    },
    {
      "epoch": 1.684661886686349,
      "grad_norm": 1.608082890510559,
      "learning_rate": 0.00014952950960880437,
      "loss": 1.2187,
      "step": 11983
    },
    {
      "epoch": 1.6848024743427525,
      "grad_norm": 1.5201523303985596,
      "learning_rate": 0.00014932742394930065,
      "loss": 1.0857,
      "step": 11984
    },
    {
      "epoch": 1.6849430619991566,
      "grad_norm": 1.6265628337860107,
      "learning_rate": 0.0001491250717143434,
      "loss": 1.1023,
      "step": 11985
    },
    {
      "epoch": 1.6850836496555601,
      "grad_norm": 1.563424825668335,
      "learning_rate": 0.00014892245399748594,
      "loss": 1.2478,
      "step": 11986
    },
    {
      "epoch": 1.6852242373119641,
      "grad_norm": 1.520473599433899,
      "learning_rate": 0.00014871957189371566,
      "loss": 1.038,
      "step": 11987
    },
    {
      "epoch": 1.6853648249683677,
      "grad_norm": 1.4935195446014404,
      "learning_rate": 0.00014851642649944824,
      "loss": 1.0188,
      "step": 11988
    },
    {
      "epoch": 1.6855054126247715,
      "grad_norm": 1.4762190580368042,
      "learning_rate": 0.00014831301891252307,
      "loss": 0.9427,
      "step": 11989
    },
    {
      "epoch": 1.6856460002811753,
      "grad_norm": 1.6330081224441528,
      "learning_rate": 0.00014810935023219625,
      "loss": 1.1032,
      "step": 11990
    },
    {
      "epoch": 1.685786587937579,
      "grad_norm": 1.6872469186782837,
      "learning_rate": 0.00014790542155913484,
      "loss": 1.1732,
      "step": 11991
    },
    {
      "epoch": 1.6859271755939829,
      "grad_norm": 1.6906665563583374,
      "learning_rate": 0.00014770123399541088,
      "loss": 1.1005,
      "step": 11992
    },
    {
      "epoch": 1.6860677632503867,
      "grad_norm": 1.5581398010253906,
      "learning_rate": 0.0001474967886444957,
      "loss": 1.0342,
      "step": 11993
    },
    {
      "epoch": 1.6862083509067904,
      "grad_norm": 1.4610244035720825,
      "learning_rate": 0.00014729208661125353,
      "loss": 0.8598,
      "step": 11994
    },
    {
      "epoch": 1.686348938563194,
      "grad_norm": 1.7590419054031372,
      "learning_rate": 0.000147087129001936,
      "loss": 1.0861,
      "step": 11995
    },
    {
      "epoch": 1.686489526219598,
      "grad_norm": 1.511307716369629,
      "learning_rate": 0.00014688191692417568,
      "loss": 1.2109,
      "step": 11996
    },
    {
      "epoch": 1.6866301138760016,
      "grad_norm": 1.8041020631790161,
      "learning_rate": 0.00014667645148698055,
      "loss": 1.1137,
      "step": 11997
    },
    {
      "epoch": 1.6867707015324056,
      "grad_norm": 2.0261991024017334,
      "learning_rate": 0.00014647073380072802,
      "loss": 1.2416,
      "step": 11998
    },
    {
      "epoch": 1.6869112891888092,
      "grad_norm": 1.7756000757217407,
      "learning_rate": 0.00014626476497715774,
      "loss": 1.2104,
      "step": 11999
    },
    {
      "epoch": 1.687051876845213,
      "grad_norm": 1.7768549919128418,
      "learning_rate": 0.00014605854612936725,
      "loss": 1.1057,
      "step": 12000
    },
    {
      "epoch": 1.687051876845213,
      "eval_loss": 1.1653504371643066,
      "eval_runtime": 773.847,
      "eval_samples_per_second": 16.342,
      "eval_steps_per_second": 8.171,
      "step": 12000
    },
    {
      "epoch": 1.6871924645016168,
      "grad_norm": 1.5335626602172852,
      "learning_rate": 0.00014585207837180582,
      "loss": 1.1547,
      "step": 12001
    },
    {
      "epoch": 1.6873330521580205,
      "grad_norm": 1.3413527011871338,
      "learning_rate": 0.00014564536282026701,
      "loss": 1.2684,
      "step": 12002
    },
    {
      "epoch": 1.6874736398144243,
      "grad_norm": 1.6535406112670898,
      "learning_rate": 0.00014543840059188381,
      "loss": 1.2173,
      "step": 12003
    },
    {
      "epoch": 1.687614227470828,
      "grad_norm": 1.8492497205734253,
      "learning_rate": 0.00014523119280512257,
      "loss": 1.0561,
      "step": 12004
    },
    {
      "epoch": 1.687754815127232,
      "grad_norm": 1.88474440574646,
      "learning_rate": 0.00014502374057977576,
      "loss": 1.0178,
      "step": 12005
    },
    {
      "epoch": 1.6878954027836355,
      "grad_norm": 2.277343511581421,
      "learning_rate": 0.00014481604503695784,
      "loss": 1.156,
      "step": 12006
    },
    {
      "epoch": 1.6880359904400395,
      "grad_norm": 1.5260440111160278,
      "learning_rate": 0.00014460810729909787,
      "loss": 0.958,
      "step": 12007
    },
    {
      "epoch": 1.688176578096443,
      "grad_norm": 1.599835753440857,
      "learning_rate": 0.00014439992848993343,
      "loss": 0.988,
      "step": 12008
    },
    {
      "epoch": 1.6883171657528468,
      "grad_norm": 1.7210043668746948,
      "learning_rate": 0.00014419150973450618,
      "loss": 1.1199,
      "step": 12009
    },
    {
      "epoch": 1.6884577534092506,
      "grad_norm": 1.5536251068115234,
      "learning_rate": 0.0001439828521591526,
      "loss": 1.1241,
      "step": 12010
    },
    {
      "epoch": 1.6885983410656544,
      "grad_norm": 1.7495074272155762,
      "learning_rate": 0.00014377395689150107,
      "loss": 1.1082,
      "step": 12011
    },
    {
      "epoch": 1.6887389287220582,
      "grad_norm": 1.4190393686294556,
      "learning_rate": 0.00014356482506046423,
      "loss": 1.1617,
      "step": 12012
    },
    {
      "epoch": 1.688879516378462,
      "grad_norm": 1.7858151197433472,
      "learning_rate": 0.0001433554577962331,
      "loss": 1.0176,
      "step": 12013
    },
    {
      "epoch": 1.6890201040348658,
      "grad_norm": 1.6669762134552002,
      "learning_rate": 0.0001431458562302714,
      "loss": 1.0305,
      "step": 12014
    },
    {
      "epoch": 1.6891606916912694,
      "grad_norm": 1.5813089609146118,
      "learning_rate": 0.00014293602149530768,
      "loss": 1.1189,
      "step": 12015
    },
    {
      "epoch": 1.6893012793476734,
      "grad_norm": 1.6514520645141602,
      "learning_rate": 0.00014272595472533266,
      "loss": 1.1385,
      "step": 12016
    },
    {
      "epoch": 1.689441867004077,
      "grad_norm": 2.0388717651367188,
      "learning_rate": 0.0001425156570555896,
      "loss": 1.0828,
      "step": 12017
    },
    {
      "epoch": 1.689582454660481,
      "grad_norm": 1.6319881677627563,
      "learning_rate": 0.00014230512962257002,
      "loss": 1.2587,
      "step": 12018
    },
    {
      "epoch": 1.6897230423168845,
      "grad_norm": 1.5768144130706787,
      "learning_rate": 0.00014209437356400706,
      "loss": 1.0192,
      "step": 12019
    },
    {
      "epoch": 1.6898636299732883,
      "grad_norm": 1.398682951927185,
      "learning_rate": 0.00014188339001886972,
      "loss": 1.2351,
      "step": 12020
    },
    {
      "epoch": 1.690004217629692,
      "grad_norm": 1.9463515281677246,
      "learning_rate": 0.00014167218012735546,
      "loss": 0.9834,
      "step": 12021
    },
    {
      "epoch": 1.690144805286096,
      "grad_norm": 1.6816236972808838,
      "learning_rate": 0.00014146074503088588,
      "loss": 1.0114,
      "step": 12022
    },
    {
      "epoch": 1.6902853929424997,
      "grad_norm": 1.66744863986969,
      "learning_rate": 0.0001412490858720991,
      "loss": 1.07,
      "step": 12023
    },
    {
      "epoch": 1.6904259805989033,
      "grad_norm": 1.776241660118103,
      "learning_rate": 0.00014103720379484488,
      "loss": 1.0707,
      "step": 12024
    },
    {
      "epoch": 1.6905665682553073,
      "grad_norm": 1.626198410987854,
      "learning_rate": 0.00014082509994417722,
      "loss": 1.2162,
      "step": 12025
    },
    {
      "epoch": 1.6907071559117108,
      "grad_norm": 1.4015530347824097,
      "learning_rate": 0.00014061277546634793,
      "loss": 1.1913,
      "step": 12026
    },
    {
      "epoch": 1.6908477435681148,
      "grad_norm": 1.7845611572265625,
      "learning_rate": 0.00014040023150880204,
      "loss": 1.0593,
      "step": 12027
    },
    {
      "epoch": 1.6909883312245184,
      "grad_norm": 1.7894026041030884,
      "learning_rate": 0.00014018746922017045,
      "loss": 1.0587,
      "step": 12028
    },
    {
      "epoch": 1.6911289188809222,
      "grad_norm": 1.3493332862854004,
      "learning_rate": 0.00013997448975026393,
      "loss": 1.2082,
      "step": 12029
    },
    {
      "epoch": 1.691269506537326,
      "grad_norm": 1.3472275733947754,
      "learning_rate": 0.00013976129425006728,
      "loss": 1.051,
      "step": 12030
    },
    {
      "epoch": 1.6914100941937298,
      "grad_norm": 1.634459137916565,
      "learning_rate": 0.00013954788387173143,
      "loss": 1.1891,
      "step": 12031
    },
    {
      "epoch": 1.6915506818501336,
      "grad_norm": 1.5233348608016968,
      "learning_rate": 0.00013933425976857055,
      "loss": 1.2364,
      "step": 12032
    },
    {
      "epoch": 1.6916912695065374,
      "grad_norm": 1.5698796510696411,
      "learning_rate": 0.00013912042309505255,
      "loss": 1.1357,
      "step": 12033
    },
    {
      "epoch": 1.6918318571629412,
      "grad_norm": 1.519178867340088,
      "learning_rate": 0.00013890637500679436,
      "loss": 1.1353,
      "step": 12034
    },
    {
      "epoch": 1.6919724448193447,
      "grad_norm": 1.4927363395690918,
      "learning_rate": 0.00013869211666055548,
      "loss": 1.0803,
      "step": 12035
    },
    {
      "epoch": 1.6921130324757487,
      "grad_norm": 1.5254878997802734,
      "learning_rate": 0.00013847764921423204,
      "loss": 1.1724,
      "step": 12036
    },
    {
      "epoch": 1.6922536201321523,
      "grad_norm": 1.6745212078094482,
      "learning_rate": 0.00013826297382684913,
      "loss": 1.1961,
      "step": 12037
    },
    {
      "epoch": 1.6923942077885563,
      "grad_norm": 1.5567301511764526,
      "learning_rate": 0.00013804809165855633,
      "loss": 1.1308,
      "step": 12038
    },
    {
      "epoch": 1.6925347954449599,
      "grad_norm": 1.599063515663147,
      "learning_rate": 0.00013783300387062135,
      "loss": 0.8413,
      "step": 12039
    },
    {
      "epoch": 1.6926753831013637,
      "grad_norm": 2.2810659408569336,
      "learning_rate": 0.00013761771162542206,
      "loss": 0.9246,
      "step": 12040
    },
    {
      "epoch": 1.6928159707577675,
      "grad_norm": 1.7323285341262817,
      "learning_rate": 0.00013740221608644202,
      "loss": 1.1271,
      "step": 12041
    },
    {
      "epoch": 1.6929565584141713,
      "grad_norm": 1.2342332601547241,
      "learning_rate": 0.00013718651841826236,
      "loss": 1.2254,
      "step": 12042
    },
    {
      "epoch": 1.693097146070575,
      "grad_norm": 1.5417929887771606,
      "learning_rate": 0.00013697061978655756,
      "loss": 1.0514,
      "step": 12043
    },
    {
      "epoch": 1.6932377337269786,
      "grad_norm": 1.3697972297668457,
      "learning_rate": 0.00013675452135808786,
      "loss": 1.0899,
      "step": 12044
    },
    {
      "epoch": 1.6933783213833826,
      "grad_norm": 1.8627995252609253,
      "learning_rate": 0.00013653822430069319,
      "loss": 1.2991,
      "step": 12045
    },
    {
      "epoch": 1.6935189090397862,
      "grad_norm": 1.5389701128005981,
      "learning_rate": 0.0001363217297832866,
      "loss": 1.1246,
      "step": 12046
    },
    {
      "epoch": 1.6936594966961902,
      "grad_norm": 1.5453976392745972,
      "learning_rate": 0.00013610503897584962,
      "loss": 1.1344,
      "step": 12047
    },
    {
      "epoch": 1.6938000843525938,
      "grad_norm": 1.3883336782455444,
      "learning_rate": 0.0001358881530494225,
      "loss": 1.0721,
      "step": 12048
    },
    {
      "epoch": 1.6939406720089976,
      "grad_norm": 1.6882067918777466,
      "learning_rate": 0.00013567107317610138,
      "loss": 0.9393,
      "step": 12049
    },
    {
      "epoch": 1.6940812596654014,
      "grad_norm": 1.601224422454834,
      "learning_rate": 0.00013545380052903006,
      "loss": 1.0835,
      "step": 12050
    },
    {
      "epoch": 1.6942218473218051,
      "grad_norm": 1.4540040493011475,
      "learning_rate": 0.00013523633628239432,
      "loss": 1.0855,
      "step": 12051
    },
    {
      "epoch": 1.694362434978209,
      "grad_norm": 1.799760341644287,
      "learning_rate": 0.00013501868161141552,
      "loss": 1.0317,
      "step": 12052
    },
    {
      "epoch": 1.6945030226346127,
      "grad_norm": 1.7620823383331299,
      "learning_rate": 0.0001348008376923429,
      "loss": 1.2025,
      "step": 12053
    },
    {
      "epoch": 1.6946436102910165,
      "grad_norm": 1.8253517150878906,
      "learning_rate": 0.00013458280570245037,
      "loss": 1.0831,
      "step": 12054
    },
    {
      "epoch": 1.69478419794742,
      "grad_norm": 1.5643669366836548,
      "learning_rate": 0.00013436458682002694,
      "loss": 1.2467,
      "step": 12055
    },
    {
      "epoch": 1.694924785603824,
      "grad_norm": 1.5246387720108032,
      "learning_rate": 0.00013414618222437196,
      "loss": 1.0833,
      "step": 12056
    },
    {
      "epoch": 1.6950653732602277,
      "grad_norm": 1.5194363594055176,
      "learning_rate": 0.00013392759309578867,
      "loss": 1.0739,
      "step": 12057
    },
    {
      "epoch": 1.6952059609166317,
      "grad_norm": 1.7491867542266846,
      "learning_rate": 0.0001337088206155766,
      "loss": 1.0083,
      "step": 12058
    },
    {
      "epoch": 1.6953465485730352,
      "grad_norm": 1.9400997161865234,
      "learning_rate": 0.00013348986596602717,
      "loss": 1.138,
      "step": 12059
    },
    {
      "epoch": 1.695487136229439,
      "grad_norm": 1.411746621131897,
      "learning_rate": 0.00013327073033041594,
      "loss": 1.2375,
      "step": 12060
    },
    {
      "epoch": 1.6956277238858428,
      "grad_norm": 1.3142530918121338,
      "learning_rate": 0.00013305141489299623,
      "loss": 1.1616,
      "step": 12061
    },
    {
      "epoch": 1.6957683115422466,
      "grad_norm": 1.5937386751174927,
      "learning_rate": 0.00013283192083899403,
      "loss": 1.2068,
      "step": 12062
    },
    {
      "epoch": 1.6959088991986504,
      "grad_norm": 1.5007182359695435,
      "learning_rate": 0.00013261224935459993,
      "loss": 1.1039,
      "step": 12063
    },
    {
      "epoch": 1.696049486855054,
      "grad_norm": 1.6844420433044434,
      "learning_rate": 0.00013239240162696273,
      "loss": 1.1039,
      "step": 12064
    },
    {
      "epoch": 1.696190074511458,
      "grad_norm": 1.8661611080169678,
      "learning_rate": 0.00013217237884418474,
      "loss": 1.0788,
      "step": 12065
    },
    {
      "epoch": 1.6963306621678615,
      "grad_norm": 1.6248472929000854,
      "learning_rate": 0.0001319521821953139,
      "loss": 1.1388,
      "step": 12066
    },
    {
      "epoch": 1.6964712498242656,
      "grad_norm": 1.6467061042785645,
      "learning_rate": 0.00013173181287033775,
      "loss": 1.0971,
      "step": 12067
    },
    {
      "epoch": 1.6966118374806691,
      "grad_norm": 1.8599600791931152,
      "learning_rate": 0.00013151127206017743,
      "loss": 1.2914,
      "step": 12068
    },
    {
      "epoch": 1.696752425137073,
      "grad_norm": 1.7282339334487915,
      "learning_rate": 0.00013129056095667935,
      "loss": 1.0214,
      "step": 12069
    },
    {
      "epoch": 1.6968930127934767,
      "grad_norm": 1.7351856231689453,
      "learning_rate": 0.0001310696807526124,
      "loss": 1.1015,
      "step": 12070
    },
    {
      "epoch": 1.6970336004498805,
      "grad_norm": 1.7653290033340454,
      "learning_rate": 0.00013084863264165782,
      "loss": 1.0977,
      "step": 12071
    },
    {
      "epoch": 1.6971741881062843,
      "grad_norm": 1.3515690565109253,
      "learning_rate": 0.00013062741781840468,
      "loss": 1.1583,
      "step": 12072
    },
    {
      "epoch": 1.697314775762688,
      "grad_norm": 1.4944343566894531,
      "learning_rate": 0.00013040603747834298,
      "loss": 1.1667,
      "step": 12073
    },
    {
      "epoch": 1.6974553634190919,
      "grad_norm": 1.7634261846542358,
      "learning_rate": 0.00013018449281785755,
      "loss": 1.165,
      "step": 12074
    },
    {
      "epoch": 1.6975959510754954,
      "grad_norm": 2.0240731239318848,
      "learning_rate": 0.00012996278503422018,
      "loss": 1.0516,
      "step": 12075
    },
    {
      "epoch": 1.6977365387318994,
      "grad_norm": 2.132575035095215,
      "learning_rate": 0.00012974091532558493,
      "loss": 0.9324,
      "step": 12076
    },
    {
      "epoch": 1.697877126388303,
      "grad_norm": 1.5028491020202637,
      "learning_rate": 0.0001295188848909816,
      "loss": 0.9883,
      "step": 12077
    },
    {
      "epoch": 1.698017714044707,
      "grad_norm": 1.6082814931869507,
      "learning_rate": 0.00012929669493030762,
      "loss": 1.119,
      "step": 12078
    },
    {
      "epoch": 1.6981583017011106,
      "grad_norm": 1.6436774730682373,
      "learning_rate": 0.00012907434664432313,
      "loss": 1.0259,
      "step": 12079
    },
    {
      "epoch": 1.6982988893575144,
      "grad_norm": 1.3287341594696045,
      "learning_rate": 0.00012885184123464293,
      "loss": 1.1693,
      "step": 12080
    },
    {
      "epoch": 1.6984394770139182,
      "grad_norm": 1.6837153434753418,
      "learning_rate": 0.00012862917990373192,
      "loss": 0.986,
      "step": 12081
    },
    {
      "epoch": 1.698580064670322,
      "grad_norm": 1.9021183252334595,
      "learning_rate": 0.00012840636385489732,
      "loss": 1.159,
      "step": 12082
    },
    {
      "epoch": 1.6987206523267258,
      "grad_norm": 1.5278416872024536,
      "learning_rate": 0.0001281833942922825,
      "loss": 1.127,
      "step": 12083
    },
    {
      "epoch": 1.6988612399831293,
      "grad_norm": 1.6163673400878906,
      "learning_rate": 0.00012796027242086038,
      "loss": 1.1584,
      "step": 12084
    },
    {
      "epoch": 1.6990018276395333,
      "grad_norm": 1.4777392148971558,
      "learning_rate": 0.00012773699944642708,
      "loss": 0.9589,
      "step": 12085
    },
    {
      "epoch": 1.699142415295937,
      "grad_norm": 1.628964900970459,
      "learning_rate": 0.0001275135765755952,
      "loss": 1.1143,
      "step": 12086
    },
    {
      "epoch": 1.699283002952341,
      "grad_norm": 1.6511116027832031,
      "learning_rate": 0.00012729000501578757,
      "loss": 1.0249,
      "step": 12087
    },
    {
      "epoch": 1.6994235906087445,
      "grad_norm": 1.6056931018829346,
      "learning_rate": 0.00012706628597523035,
      "loss": 1.1678,
      "step": 12088
    },
    {
      "epoch": 1.6995641782651483,
      "grad_norm": 1.6268808841705322,
      "learning_rate": 0.00012684242066294694,
      "loss": 1.0559,
      "step": 12089
    },
    {
      "epoch": 1.699704765921552,
      "grad_norm": 1.5462760925292969,
      "learning_rate": 0.0001266184102887514,
      "loss": 1.1297,
      "step": 12090
    },
    {
      "epoch": 1.6998453535779559,
      "grad_norm": 1.5493295192718506,
      "learning_rate": 0.00012639425606324046,
      "loss": 0.9986,
      "step": 12091
    },
    {
      "epoch": 1.6999859412343596,
      "grad_norm": 1.610312581062317,
      "learning_rate": 0.0001261699591977902,
      "loss": 1.2535,
      "step": 12092
    },
    {
      "epoch": 1.7001265288907634,
      "grad_norm": 2.0135374069213867,
      "learning_rate": 0.00012594552090454636,
      "loss": 1.1026,
      "step": 12093
    },
    {
      "epoch": 1.7002671165471672,
      "grad_norm": 1.5868780612945557,
      "learning_rate": 0.00012572094239641913,
      "loss": 0.9096,
      "step": 12094
    },
    {
      "epoch": 1.7004077042035708,
      "grad_norm": 1.987863540649414,
      "learning_rate": 0.00012549622488707698,
      "loss": 1.2527,
      "step": 12095
    },
    {
      "epoch": 1.7005482918599748,
      "grad_norm": 1.684482455253601,
      "learning_rate": 0.00012527136959093835,
      "loss": 1.1144,
      "step": 12096
    },
    {
      "epoch": 1.7006888795163784,
      "grad_norm": 1.4254306554794312,
      "learning_rate": 0.00012504637772316746,
      "loss": 1.0817,
      "step": 12097
    },
    {
      "epoch": 1.7008294671727824,
      "grad_norm": 1.59893000125885,
      "learning_rate": 0.0001248212504996663,
      "loss": 1.0312,
      "step": 12098
    },
    {
      "epoch": 1.700970054829186,
      "grad_norm": 1.757555603981018,
      "learning_rate": 0.00012459598913706782,
      "loss": 0.9615,
      "step": 12099
    },
    {
      "epoch": 1.7011106424855897,
      "grad_norm": 1.554226040840149,
      "learning_rate": 0.00012437059485273145,
      "loss": 1.0556,
      "step": 12100
    },
    {
      "epoch": 1.7012512301419935,
      "grad_norm": 1.5318129062652588,
      "learning_rate": 0.00012414506886473275,
      "loss": 1.2489,
      "step": 12101
    },
    {
      "epoch": 1.7013918177983973,
      "grad_norm": 1.6580106019973755,
      "learning_rate": 0.00012391941239186055,
      "loss": 1.0191,
      "step": 12102
    },
    {
      "epoch": 1.7015324054548011,
      "grad_norm": 1.5783531665802002,
      "learning_rate": 0.00012369362665360834,
      "loss": 1.0689,
      "step": 12103
    },
    {
      "epoch": 1.7016729931112047,
      "grad_norm": 1.6759028434753418,
      "learning_rate": 0.00012346771287016832,
      "loss": 1.0981,
      "step": 12104
    },
    {
      "epoch": 1.7018135807676087,
      "grad_norm": 1.567994236946106,
      "learning_rate": 0.0001232416722624246,
      "loss": 0.9288,
      "step": 12105
    },
    {
      "epoch": 1.7019541684240123,
      "grad_norm": 1.4989793300628662,
      "learning_rate": 0.00012301550605194706,
      "loss": 0.9478,
      "step": 12106
    },
    {
      "epoch": 1.7020947560804163,
      "grad_norm": 1.5614603757858276,
      "learning_rate": 0.00012278921546098298,
      "loss": 1.2065,
      "step": 12107
    },
    {
      "epoch": 1.7022353437368198,
      "grad_norm": 1.4550189971923828,
      "learning_rate": 0.0001225628017124538,
      "loss": 1.203,
      "step": 12108
    },
    {
      "epoch": 1.7023759313932236,
      "grad_norm": 1.7669161558151245,
      "learning_rate": 0.0001223362660299453,
      "loss": 1.1076,
      "step": 12109
    },
    {
      "epoch": 1.7025165190496274,
      "grad_norm": 1.7904173135757446,
      "learning_rate": 0.00012210960963770245,
      "loss": 1.1269,
      "step": 12110
    },
    {
      "epoch": 1.7026571067060312,
      "grad_norm": 1.7447450160980225,
      "learning_rate": 0.00012188283376062294,
      "loss": 1.0453,
      "step": 12111
    },
    {
      "epoch": 1.702797694362435,
      "grad_norm": 1.7045437097549438,
      "learning_rate": 0.00012165593962424915,
      "loss": 1.0744,
      "step": 12112
    },
    {
      "epoch": 1.7029382820188388,
      "grad_norm": 1.541297435760498,
      "learning_rate": 0.00012142892845476357,
      "loss": 1.0633,
      "step": 12113
    },
    {
      "epoch": 1.7030788696752426,
      "grad_norm": 1.6676117181777954,
      "learning_rate": 0.00012120180147898051,
      "loss": 1.0624,
      "step": 12114
    },
    {
      "epoch": 1.7032194573316461,
      "grad_norm": 1.5178179740905762,
      "learning_rate": 0.0001209745599243411,
      "loss": 1.0797,
      "step": 12115
    },
    {
      "epoch": 1.7033600449880502,
      "grad_norm": 1.4482090473175049,
      "learning_rate": 0.00012074720501890469,
      "loss": 1.0825,
      "step": 12116
    },
    {
      "epoch": 1.7035006326444537,
      "grad_norm": 1.6564395427703857,
      "learning_rate": 0.00012051973799134389,
      "loss": 1.1964,
      "step": 12117
    },
    {
      "epoch": 1.7036412203008577,
      "grad_norm": 1.4673117399215698,
      "learning_rate": 0.00012029216007093615,
      "loss": 0.9829,
      "step": 12118
    },
    {
      "epoch": 1.7037818079572613,
      "grad_norm": 1.4796274900436401,
      "learning_rate": 0.00012006447248755934,
      "loss": 1.088,
      "step": 12119
    },
    {
      "epoch": 1.703922395613665,
      "grad_norm": 1.4816086292266846,
      "learning_rate": 0.0001198366764716835,
      "loss": 1.052,
      "step": 12120
    },
    {
      "epoch": 1.7040629832700689,
      "grad_norm": 1.4716876745224,
      "learning_rate": 0.00011960877325436482,
      "loss": 1.1124,
      "step": 12121
    },
    {
      "epoch": 1.7042035709264727,
      "grad_norm": 1.6241624355316162,
      "learning_rate": 0.00011938076406723864,
      "loss": 1.024,
      "step": 12122
    },
    {
      "epoch": 1.7043441585828765,
      "grad_norm": 1.914200782775879,
      "learning_rate": 0.00011915265014251315,
      "loss": 1.0749,
      "step": 12123
    },
    {
      "epoch": 1.70448474623928,
      "grad_norm": 1.6231510639190674,
      "learning_rate": 0.00011892443271296247,
      "loss": 0.9423,
      "step": 12124
    },
    {
      "epoch": 1.704625333895684,
      "grad_norm": 1.4978160858154297,
      "learning_rate": 0.00011869611301192012,
      "loss": 1.112,
      "step": 12125
    },
    {
      "epoch": 1.7047659215520876,
      "grad_norm": 1.4334957599639893,
      "learning_rate": 0.00011846769227327228,
      "loss": 1.0678,
      "step": 12126
    },
    {
      "epoch": 1.7049065092084916,
      "grad_norm": 1.610398769378662,
      "learning_rate": 0.00011823917173145153,
      "loss": 0.9749,
      "step": 12127
    },
    {
      "epoch": 1.7050470968648952,
      "grad_norm": 1.4884238243103027,
      "learning_rate": 0.00011801055262142879,
      "loss": 1.1597,
      "step": 12128
    },
    {
      "epoch": 1.705187684521299,
      "grad_norm": 1.591083288192749,
      "learning_rate": 0.00011778183617870866,
      "loss": 1.1669,
      "step": 12129
    },
    {
      "epoch": 1.7053282721777028,
      "grad_norm": 1.433282494544983,
      "learning_rate": 0.00011755302363932106,
      "loss": 1.1526,
      "step": 12130
    },
    {
      "epoch": 1.7054688598341066,
      "grad_norm": 1.9242640733718872,
      "learning_rate": 0.00011732411623981629,
      "loss": 1.0754,
      "step": 12131
    },
    {
      "epoch": 1.7056094474905104,
      "grad_norm": 1.4749847650527954,
      "learning_rate": 0.00011709511521725624,
      "loss": 1.0392,
      "step": 12132
    },
    {
      "epoch": 1.7057500351469141,
      "grad_norm": 1.5013972520828247,
      "learning_rate": 0.0001168660218092095,
      "loss": 1.1675,
      "step": 12133
    },
    {
      "epoch": 1.705890622803318,
      "grad_norm": 1.74779212474823,
      "learning_rate": 0.00011663683725374278,
      "loss": 0.9354,
      "step": 12134
    },
    {
      "epoch": 1.7060312104597215,
      "grad_norm": 1.729273796081543,
      "learning_rate": 0.00011640756278941651,
      "loss": 1.0467,
      "step": 12135
    },
    {
      "epoch": 1.7061717981161255,
      "grad_norm": 1.4131702184677124,
      "learning_rate": 0.00011617819965527666,
      "loss": 0.9838,
      "step": 12136
    },
    {
      "epoch": 1.706312385772529,
      "grad_norm": 1.8161015510559082,
      "learning_rate": 0.00011594874909084797,
      "loss": 1.2774,
      "step": 12137
    },
    {
      "epoch": 1.706452973428933,
      "grad_norm": 1.7511835098266602,
      "learning_rate": 0.0001157192123361291,
      "loss": 1.1247,
      "step": 12138
    },
    {
      "epoch": 1.7065935610853367,
      "grad_norm": 1.7047055959701538,
      "learning_rate": 0.00011548959063158244,
      "loss": 1.0953,
      "step": 12139
    },
    {
      "epoch": 1.7067341487417405,
      "grad_norm": 1.716776967048645,
      "learning_rate": 0.0001152598852181308,
      "loss": 1.1019,
      "step": 12140
    },
    {
      "epoch": 1.7068747363981442,
      "grad_norm": 1.517422080039978,
      "learning_rate": 0.00011503009733714911,
      "loss": 1.0816,
      "step": 12141
    },
    {
      "epoch": 1.707015324054548,
      "grad_norm": 1.668585181236267,
      "learning_rate": 0.00011480022823045785,
      "loss": 1.032,
      "step": 12142
    },
    {
      "epoch": 1.7071559117109518,
      "grad_norm": 1.6758275032043457,
      "learning_rate": 0.00011457027914031692,
      "loss": 1.0722,
      "step": 12143
    },
    {
      "epoch": 1.7072964993673554,
      "grad_norm": 1.756662368774414,
      "learning_rate": 0.00011434025130941735,
      "loss": 1.1466,
      "step": 12144
    },
    {
      "epoch": 1.7074370870237594,
      "grad_norm": 1.6847338676452637,
      "learning_rate": 0.00011411014598087644,
      "loss": 1.0543,
      "step": 12145
    },
    {
      "epoch": 1.707577674680163,
      "grad_norm": 1.647722840309143,
      "learning_rate": 0.00011387996439823096,
      "loss": 1.2047,
      "step": 12146
    },
    {
      "epoch": 1.707718262336567,
      "grad_norm": 1.7615110874176025,
      "learning_rate": 0.00011364970780542878,
      "loss": 1.0201,
      "step": 12147
    },
    {
      "epoch": 1.7078588499929706,
      "grad_norm": 1.7039499282836914,
      "learning_rate": 0.00011341937744682348,
      "loss": 1.0572,
      "step": 12148
    },
    {
      "epoch": 1.7079994376493743,
      "grad_norm": 1.464548110961914,
      "learning_rate": 0.00011318897456716755,
      "loss": 1.1334,
      "step": 12149
    },
    {
      "epoch": 1.7081400253057781,
      "grad_norm": 1.6849027872085571,
      "learning_rate": 0.00011295850041160445,
      "loss": 1.003,
      "step": 12150
    },
    {
      "epoch": 1.708280612962182,
      "grad_norm": 1.5774571895599365,
      "learning_rate": 0.00011272795622566383,
      "loss": 0.9782,
      "step": 12151
    },
    {
      "epoch": 1.7084212006185857,
      "grad_norm": 1.9091181755065918,
      "learning_rate": 0.00011249734325525314,
      "loss": 1.1492,
      "step": 12152
    },
    {
      "epoch": 1.7085617882749895,
      "grad_norm": 1.59172785282135,
      "learning_rate": 0.00011226666274665256,
      "loss": 1.007,
      "step": 12153
    },
    {
      "epoch": 1.7087023759313933,
      "grad_norm": 1.5987064838409424,
      "learning_rate": 0.00011203591594650657,
      "loss": 1.1213,
      "step": 12154
    },
    {
      "epoch": 1.7088429635877969,
      "grad_norm": 1.3880897760391235,
      "learning_rate": 0.00011180510410181723,
      "loss": 1.1775,
      "step": 12155
    },
    {
      "epoch": 1.7089835512442009,
      "grad_norm": 1.503066062927246,
      "learning_rate": 0.00011157422845993912,
      "loss": 1.0646,
      "step": 12156
    },
    {
      "epoch": 1.7091241389006044,
      "grad_norm": 1.3312278985977173,
      "learning_rate": 0.00011134329026857129,
      "loss": 1.1461,
      "step": 12157
    },
    {
      "epoch": 1.7092647265570085,
      "grad_norm": 1.4403629302978516,
      "learning_rate": 0.00011111229077575078,
      "loss": 1.0558,
      "step": 12158
    },
    {
      "epoch": 1.709405314213412,
      "grad_norm": 1.9477276802062988,
      "learning_rate": 0.00011088123122984595,
      "loss": 1.1466,
      "step": 12159
    },
    {
      "epoch": 1.7095459018698158,
      "grad_norm": 1.9214155673980713,
      "learning_rate": 0.00011065011287954965,
      "loss": 1.1478,
      "step": 12160
    },
    {
      "epoch": 1.7096864895262196,
      "grad_norm": 1.421475887298584,
      "learning_rate": 0.00011041893697387259,
      "loss": 1.0487,
      "step": 12161
    },
    {
      "epoch": 1.7098270771826234,
      "grad_norm": 1.7775379419326782,
      "learning_rate": 0.00011018770476213647,
      "loss": 1.0818,
      "step": 12162
    },
    {
      "epoch": 1.7099676648390272,
      "grad_norm": 1.5726362466812134,
      "learning_rate": 0.0001099564174939673,
      "loss": 1.0948,
      "step": 12163
    },
    {
      "epoch": 1.7101082524954307,
      "grad_norm": 1.634203553199768,
      "learning_rate": 0.00010972507641928859,
      "loss": 1.0805,
      "step": 12164
    },
    {
      "epoch": 1.7102488401518348,
      "grad_norm": 1.6724568605422974,
      "learning_rate": 0.00010949368278831504,
      "loss": 1.1323,
      "step": 12165
    },
    {
      "epoch": 1.7103894278082383,
      "grad_norm": 1.7360714673995972,
      "learning_rate": 0.0001092622378515443,
      "loss": 1.0638,
      "step": 12166
    },
    {
      "epoch": 1.7105300154646423,
      "grad_norm": 1.5847365856170654,
      "learning_rate": 0.00010903074285975233,
      "loss": 1.0258,
      "step": 12167
    },
    {
      "epoch": 1.710670603121046,
      "grad_norm": 1.471903920173645,
      "learning_rate": 0.00010879919906398486,
      "loss": 1.1663,
      "step": 12168
    },
    {
      "epoch": 1.7108111907774497,
      "grad_norm": 1.5239393711090088,
      "learning_rate": 0.0001085676077155525,
      "loss": 1.0384,
      "step": 12169
    },
    {
      "epoch": 1.7109517784338535,
      "grad_norm": 1.4093941450119019,
      "learning_rate": 0.0001083359700660221,
      "loss": 1.1288,
      "step": 12170
    },
    {
      "epoch": 1.7110923660902573,
      "grad_norm": 1.5430234670639038,
      "learning_rate": 0.00010810428736721,
      "loss": 1.1174,
      "step": 12171
    },
    {
      "epoch": 1.711232953746661,
      "grad_norm": 1.4193096160888672,
      "learning_rate": 0.00010787256087117714,
      "loss": 1.1452,
      "step": 12172
    },
    {
      "epoch": 1.7113735414030649,
      "grad_norm": 1.5261437892913818,
      "learning_rate": 0.00010764079183022066,
      "loss": 0.8385,
      "step": 12173
    },
    {
      "epoch": 1.7115141290594686,
      "grad_norm": 1.6387696266174316,
      "learning_rate": 0.00010740898149686776,
      "loss": 1.1364,
      "step": 12174
    },
    {
      "epoch": 1.7116547167158722,
      "grad_norm": 1.9504319429397583,
      "learning_rate": 0.00010717713112386834,
      "loss": 1.0891,
      "step": 12175
    },
    {
      "epoch": 1.7117953043722762,
      "grad_norm": 1.6952064037322998,
      "learning_rate": 0.00010694524196419015,
      "loss": 1.127,
      "step": 12176
    },
    {
      "epoch": 1.7119358920286798,
      "grad_norm": 1.5881030559539795,
      "learning_rate": 0.0001067133152710084,
      "loss": 1.1931,
      "step": 12177
    },
    {
      "epoch": 1.7120764796850838,
      "grad_norm": 1.4264063835144043,
      "learning_rate": 0.00010648135229770258,
      "loss": 1.0668,
      "step": 12178
    },
    {
      "epoch": 1.7122170673414874,
      "grad_norm": 1.7639551162719727,
      "learning_rate": 0.00010624935429784787,
      "loss": 1.1643,
      "step": 12179
    },
    {
      "epoch": 1.7123576549978912,
      "grad_norm": 1.502526044845581,
      "learning_rate": 0.00010601732252520875,
      "loss": 1.0772,
      "step": 12180
    },
    {
      "epoch": 1.712498242654295,
      "grad_norm": 1.5382970571517944,
      "learning_rate": 0.00010578525823373258,
      "loss": 0.9555,
      "step": 12181
    },
    {
      "epoch": 1.7126388303106987,
      "grad_norm": 1.7033005952835083,
      "learning_rate": 0.00010555316267754136,
      "loss": 1.1643,
      "step": 12182
    },
    {
      "epoch": 1.7127794179671025,
      "grad_norm": 1.5539331436157227,
      "learning_rate": 0.0001053210371109268,
      "loss": 1.0124,
      "step": 12183
    },
    {
      "epoch": 1.712920005623506,
      "grad_norm": 1.4712072610855103,
      "learning_rate": 0.00010508888278834348,
      "loss": 1.1866,
      "step": 12184
    },
    {
      "epoch": 1.7130605932799101,
      "grad_norm": 1.5595808029174805,
      "learning_rate": 0.00010485670096440032,
      "loss": 1.0024,
      "step": 12185
    },
    {
      "epoch": 1.7132011809363137,
      "grad_norm": 1.340291142463684,
      "learning_rate": 0.00010462449289385557,
      "loss": 1.2282,
      "step": 12186
    },
    {
      "epoch": 1.7133417685927177,
      "grad_norm": 1.6898601055145264,
      "learning_rate": 0.0001043922598316083,
      "loss": 1.0389,
      "step": 12187
    },
    {
      "epoch": 1.7134823562491213,
      "grad_norm": 1.5440770387649536,
      "learning_rate": 0.00010416000303269362,
      "loss": 1.2315,
      "step": 12188
    },
    {
      "epoch": 1.713622943905525,
      "grad_norm": 1.5848592519760132,
      "learning_rate": 0.00010392772375227462,
      "loss": 1.1165,
      "step": 12189
    },
    {
      "epoch": 1.7137635315619288,
      "grad_norm": 1.6831705570220947,
      "learning_rate": 0.00010369542324563558,
      "loss": 1.0312,
      "step": 12190
    },
    {
      "epoch": 1.7139041192183326,
      "grad_norm": 1.5201689004898071,
      "learning_rate": 0.00010346310276817641,
      "loss": 1.0998,
      "step": 12191
    },
    {
      "epoch": 1.7140447068747364,
      "grad_norm": 1.3079885244369507,
      "learning_rate": 0.00010323076357540438,
      "loss": 1.1737,
      "step": 12192
    },
    {
      "epoch": 1.7141852945311402,
      "grad_norm": 1.6582727432250977,
      "learning_rate": 0.00010299840692292711,
      "loss": 1.1139,
      "step": 12193
    },
    {
      "epoch": 1.714325882187544,
      "grad_norm": 1.483054280281067,
      "learning_rate": 0.00010276603406644767,
      "loss": 1.2051,
      "step": 12194
    },
    {
      "epoch": 1.7144664698439476,
      "grad_norm": 1.6722198724746704,
      "learning_rate": 0.00010253364626175634,
      "loss": 0.9495,
      "step": 12195
    },
    {
      "epoch": 1.7146070575003516,
      "grad_norm": 1.9045995473861694,
      "learning_rate": 0.00010230124476472423,
      "loss": 1.2367,
      "step": 12196
    },
    {
      "epoch": 1.7147476451567552,
      "grad_norm": 1.6804656982421875,
      "learning_rate": 0.00010206883083129666,
      "loss": 0.9904,
      "step": 12197
    },
    {
      "epoch": 1.7148882328131592,
      "grad_norm": 1.6816800832748413,
      "learning_rate": 0.00010183640571748492,
      "loss": 0.8825,
      "step": 12198
    },
    {
      "epoch": 1.7150288204695627,
      "grad_norm": 1.5366675853729248,
      "learning_rate": 0.00010160397067936272,
      "loss": 1.1093,
      "step": 12199
    },
    {
      "epoch": 1.7151694081259665,
      "grad_norm": 1.7254050970077515,
      "learning_rate": 0.00010137152697305603,
      "loss": 1.1931,
      "step": 12200
    },
    {
      "epoch": 1.7153099957823703,
      "grad_norm": 1.6424586772918701,
      "learning_rate": 0.00010113907585473803,
      "loss": 1.2888,
      "step": 12201
    },
    {
      "epoch": 1.715450583438774,
      "grad_norm": 1.4426305294036865,
      "learning_rate": 0.00010090661858062193,
      "loss": 1.2385,
      "step": 12202
    },
    {
      "epoch": 1.715591171095178,
      "grad_norm": 1.5016169548034668,
      "learning_rate": 0.00010067415640695459,
      "loss": 1.1837,
      "step": 12203
    },
    {
      "epoch": 1.7157317587515815,
      "grad_norm": 1.5548758506774902,
      "learning_rate": 0.00010044169059000826,
      "loss": 1.1373,
      "step": 12204
    },
    {
      "epoch": 1.7158723464079855,
      "grad_norm": 1.6826531887054443,
      "learning_rate": 0.00010020922238607595,
      "loss": 1.2218,
      "step": 12205
    },
    {
      "epoch": 1.716012934064389,
      "grad_norm": 1.618653655052185,
      "learning_rate": 9.997675305146287e-05,
      "loss": 1.0564,
      "step": 12206
    },
    {
      "epoch": 1.716153521720793,
      "grad_norm": 1.5702799558639526,
      "learning_rate": 9.974428384248139e-05,
      "loss": 1.0522,
      "step": 12207
    },
    {
      "epoch": 1.7162941093771966,
      "grad_norm": 1.5353301763534546,
      "learning_rate": 9.95118160154425e-05,
      "loss": 0.9541,
      "step": 12208
    },
    {
      "epoch": 1.7164346970336004,
      "grad_norm": 1.7161325216293335,
      "learning_rate": 9.927935082664902e-05,
      "loss": 1.1619,
      "step": 12209
    },
    {
      "epoch": 1.7165752846900042,
      "grad_norm": 1.4664812088012695,
      "learning_rate": 9.904688953239055e-05,
      "loss": 1.3418,
      "step": 12210
    },
    {
      "epoch": 1.716715872346408,
      "grad_norm": 1.7082364559173584,
      "learning_rate": 9.881443338893534e-05,
      "loss": 1.0962,
      "step": 12211
    },
    {
      "epoch": 1.7168564600028118,
      "grad_norm": 1.571139931678772,
      "learning_rate": 9.858198365252372e-05,
      "loss": 1.1188,
      "step": 12212
    },
    {
      "epoch": 1.7169970476592156,
      "grad_norm": 1.6637277603149414,
      "learning_rate": 9.834954157936148e-05,
      "loss": 1.004,
      "step": 12213
    },
    {
      "epoch": 1.7171376353156194,
      "grad_norm": 1.5096032619476318,
      "learning_rate": 9.811710842561295e-05,
      "loss": 1.0226,
      "step": 12214
    },
    {
      "epoch": 1.717278222972023,
      "grad_norm": 1.8052197694778442,
      "learning_rate": 9.788468544739427e-05,
      "loss": 1.0636,
      "step": 12215
    },
    {
      "epoch": 1.717418810628427,
      "grad_norm": 1.6047954559326172,
      "learning_rate": 9.765227390076659e-05,
      "loss": 1.1584,
      "step": 12216
    },
    {
      "epoch": 1.7175593982848305,
      "grad_norm": 1.527119517326355,
      "learning_rate": 9.741987504172929e-05,
      "loss": 1.0806,
      "step": 12217
    },
    {
      "epoch": 1.7176999859412345,
      "grad_norm": 1.797804594039917,
      "learning_rate": 9.718749012621314e-05,
      "loss": 1.1339,
      "step": 12218
    },
    {
      "epoch": 1.717840573597638,
      "grad_norm": 1.4813990592956543,
      "learning_rate": 9.695512041007398e-05,
      "loss": 1.0132,
      "step": 12219
    },
    {
      "epoch": 1.7179811612540419,
      "grad_norm": 1.3565545082092285,
      "learning_rate": 9.67227671490844e-05,
      "loss": 1.0885,
      "step": 12220
    },
    {
      "epoch": 1.7181217489104457,
      "grad_norm": 1.5147370100021362,
      "learning_rate": 9.649043159892876e-05,
      "loss": 1.1862,
      "step": 12221
    },
    {
      "epoch": 1.7182623365668495,
      "grad_norm": 1.9436638355255127,
      "learning_rate": 9.625811501519648e-05,
      "loss": 1.0298,
      "step": 12222
    },
    {
      "epoch": 1.7184029242232532,
      "grad_norm": 1.4886449575424194,
      "learning_rate": 9.602581865337334e-05,
      "loss": 1.303,
      "step": 12223
    },
    {
      "epoch": 1.7185435118796568,
      "grad_norm": 1.3982572555541992,
      "learning_rate": 9.579354376883661e-05,
      "loss": 1.2241,
      "step": 12224
    },
    {
      "epoch": 1.7186840995360608,
      "grad_norm": 1.8165655136108398,
      "learning_rate": 9.556129161684635e-05,
      "loss": 1.1757,
      "step": 12225
    },
    {
      "epoch": 1.7188246871924644,
      "grad_norm": 1.4361460208892822,
      "learning_rate": 9.532906345254096e-05,
      "loss": 0.8404,
      "step": 12226
    },
    {
      "epoch": 1.7189652748488684,
      "grad_norm": 1.4589483737945557,
      "learning_rate": 9.509686053092871e-05,
      "loss": 0.9982,
      "step": 12227
    },
    {
      "epoch": 1.719105862505272,
      "grad_norm": 1.7920869588851929,
      "learning_rate": 9.486468410688154e-05,
      "loss": 0.9415,
      "step": 12228
    },
    {
      "epoch": 1.7192464501616758,
      "grad_norm": 2.1646056175231934,
      "learning_rate": 9.463253543512782e-05,
      "loss": 1.0501,
      "step": 12229
    },
    {
      "epoch": 1.7193870378180796,
      "grad_norm": 1.7294695377349854,
      "learning_rate": 9.440041577024735e-05,
      "loss": 1.1513,
      "step": 12230
    },
    {
      "epoch": 1.7195276254744833,
      "grad_norm": 1.4548101425170898,
      "learning_rate": 9.416832636666104e-05,
      "loss": 1.0687,
      "step": 12231
    },
    {
      "epoch": 1.7196682131308871,
      "grad_norm": 1.5294896364212036,
      "learning_rate": 9.393626847862769e-05,
      "loss": 1.0722,
      "step": 12232
    },
    {
      "epoch": 1.719808800787291,
      "grad_norm": 1.4762828350067139,
      "learning_rate": 9.370424336023544e-05,
      "loss": 1.2256,
      "step": 12233
    },
    {
      "epoch": 1.7199493884436947,
      "grad_norm": 1.3645328283309937,
      "learning_rate": 9.347225226539532e-05,
      "loss": 1.0995,
      "step": 12234
    },
    {
      "epoch": 1.7200899761000983,
      "grad_norm": 1.5513005256652832,
      "learning_rate": 9.324029644783484e-05,
      "loss": 1.0919,
      "step": 12235
    },
    {
      "epoch": 1.7202305637565023,
      "grad_norm": 1.3772011995315552,
      "learning_rate": 9.300837716108943e-05,
      "loss": 1.1443,
      "step": 12236
    },
    {
      "epoch": 1.7203711514129059,
      "grad_norm": 1.691211462020874,
      "learning_rate": 9.277649565849929e-05,
      "loss": 1.1275,
      "step": 12237
    },
    {
      "epoch": 1.7205117390693099,
      "grad_norm": 1.6244663000106812,
      "learning_rate": 9.254465319319894e-05,
      "loss": 1.1145,
      "step": 12238
    },
    {
      "epoch": 1.7206523267257134,
      "grad_norm": 1.5591429471969604,
      "learning_rate": 9.231285101811234e-05,
      "loss": 1.2199,
      "step": 12239
    },
    {
      "epoch": 1.7207929143821172,
      "grad_norm": 1.4965463876724243,
      "learning_rate": 9.208109038594603e-05,
      "loss": 1.1036,
      "step": 12240
    },
    {
      "epoch": 1.720933502038521,
      "grad_norm": 1.7273123264312744,
      "learning_rate": 9.1849372549181e-05,
      "loss": 1.1027,
      "step": 12241
    },
    {
      "epoch": 1.7210740896949248,
      "grad_norm": 1.3901714086532593,
      "learning_rate": 9.161769876006805e-05,
      "loss": 1.1593,
      "step": 12242
    },
    {
      "epoch": 1.7212146773513286,
      "grad_norm": 1.7831674814224243,
      "learning_rate": 9.138607027061952e-05,
      "loss": 0.9821,
      "step": 12243
    },
    {
      "epoch": 1.7213552650077322,
      "grad_norm": 1.7546155452728271,
      "learning_rate": 9.115448833260264e-05,
      "loss": 1.0712,
      "step": 12244
    },
    {
      "epoch": 1.7214958526641362,
      "grad_norm": 1.8645291328430176,
      "learning_rate": 9.09229541975341e-05,
      "loss": 1.0583,
      "step": 12245
    },
    {
      "epoch": 1.7216364403205398,
      "grad_norm": 1.48162841796875,
      "learning_rate": 9.069146911667159e-05,
      "loss": 1.0001,
      "step": 12246
    },
    {
      "epoch": 1.7217770279769438,
      "grad_norm": 1.366317868232727,
      "learning_rate": 9.046003434100694e-05,
      "loss": 1.0433,
      "step": 12247
    },
    {
      "epoch": 1.7219176156333473,
      "grad_norm": 1.555745005607605,
      "learning_rate": 9.022865112126121e-05,
      "loss": 1.0643,
      "step": 12248
    },
    {
      "epoch": 1.7220582032897511,
      "grad_norm": 1.4277799129486084,
      "learning_rate": 8.999732070787647e-05,
      "loss": 1.1224,
      "step": 12249
    },
    {
      "epoch": 1.722198790946155,
      "grad_norm": 1.596834421157837,
      "learning_rate": 8.976604435100943e-05,
      "loss": 1.211,
      "step": 12250
    },
    {
      "epoch": 1.7223393786025587,
      "grad_norm": 1.4525012969970703,
      "learning_rate": 8.953482330052469e-05,
      "loss": 1.1945,
      "step": 12251
    },
    {
      "epoch": 1.7224799662589625,
      "grad_norm": 1.4855746030807495,
      "learning_rate": 8.93036588059879e-05,
      "loss": 1.1402,
      "step": 12252
    },
    {
      "epoch": 1.7226205539153663,
      "grad_norm": 1.495172142982483,
      "learning_rate": 8.907255211665913e-05,
      "loss": 1.1249,
      "step": 12253
    },
    {
      "epoch": 1.72276114157177,
      "grad_norm": 1.7117558717727661,
      "learning_rate": 8.8841504481486e-05,
      "loss": 1.1674,
      "step": 12254
    },
    {
      "epoch": 1.7229017292281736,
      "grad_norm": 1.7306091785430908,
      "learning_rate": 8.861051714909707e-05,
      "loss": 0.9422,
      "step": 12255
    },
    {
      "epoch": 1.7230423168845777,
      "grad_norm": 1.673706293106079,
      "learning_rate": 8.837959136779528e-05,
      "loss": 0.976,
      "step": 12256
    },
    {
      "epoch": 1.7231829045409812,
      "grad_norm": 1.5755878686904907,
      "learning_rate": 8.814872838554995e-05,
      "loss": 1.0721,
      "step": 12257
    },
    {
      "epoch": 1.7233234921973852,
      "grad_norm": 1.4342070817947388,
      "learning_rate": 8.791792944999202e-05,
      "loss": 1.1821,
      "step": 12258
    },
    {
      "epoch": 1.7234640798537888,
      "grad_norm": 1.6068992614746094,
      "learning_rate": 8.768719580840563e-05,
      "loss": 0.8977,
      "step": 12259
    },
    {
      "epoch": 1.7236046675101926,
      "grad_norm": 1.4446178674697876,
      "learning_rate": 8.745652870772312e-05,
      "loss": 1.0162,
      "step": 12260
    },
    {
      "epoch": 1.7237452551665964,
      "grad_norm": 1.5209367275238037,
      "learning_rate": 8.722592939451616e-05,
      "loss": 1.0196,
      "step": 12261
    },
    {
      "epoch": 1.7238858428230002,
      "grad_norm": 1.5526264905929565,
      "learning_rate": 8.69953991149908e-05,
      "loss": 1.2516,
      "step": 12262
    },
    {
      "epoch": 1.724026430479404,
      "grad_norm": 1.6795095205307007,
      "learning_rate": 8.676493911497896e-05,
      "loss": 1.0616,
      "step": 12263
    },
    {
      "epoch": 1.7241670181358075,
      "grad_norm": 1.4900236129760742,
      "learning_rate": 8.653455063993379e-05,
      "loss": 1.0461,
      "step": 12264
    },
    {
      "epoch": 1.7243076057922115,
      "grad_norm": 1.678234338760376,
      "learning_rate": 8.630423493492157e-05,
      "loss": 1.0057,
      "step": 12265
    },
    {
      "epoch": 1.724448193448615,
      "grad_norm": 1.623534083366394,
      "learning_rate": 8.607399324461533e-05,
      "loss": 0.995,
      "step": 12266
    },
    {
      "epoch": 1.7245887811050191,
      "grad_norm": 1.5323171615600586,
      "learning_rate": 8.584382681328808e-05,
      "loss": 1.2436,
      "step": 12267
    },
    {
      "epoch": 1.7247293687614227,
      "grad_norm": 1.7605420351028442,
      "learning_rate": 8.561373688480613e-05,
      "loss": 1.0165,
      "step": 12268
    },
    {
      "epoch": 1.7248699564178265,
      "grad_norm": 1.3774493932724,
      "learning_rate": 8.538372470262235e-05,
      "loss": 1.0214,
      "step": 12269
    },
    {
      "epoch": 1.7250105440742303,
      "grad_norm": 1.4314719438552856,
      "learning_rate": 8.515379150976947e-05,
      "loss": 1.166,
      "step": 12270
    },
    {
      "epoch": 1.725151131730634,
      "grad_norm": 1.5837587118148804,
      "learning_rate": 8.492393854885333e-05,
      "loss": 1.0162,
      "step": 12271
    },
    {
      "epoch": 1.7252917193870378,
      "grad_norm": 1.5996487140655518,
      "learning_rate": 8.469416706204615e-05,
      "loss": 1.0502,
      "step": 12272
    },
    {
      "epoch": 1.7254323070434416,
      "grad_norm": 1.5198692083358765,
      "learning_rate": 8.446447829108025e-05,
      "loss": 1.1097,
      "step": 12273
    },
    {
      "epoch": 1.7255728946998454,
      "grad_norm": 1.9222277402877808,
      "learning_rate": 8.423487347723953e-05,
      "loss": 1.0658,
      "step": 12274
    },
    {
      "epoch": 1.725713482356249,
      "grad_norm": 1.5634418725967407,
      "learning_rate": 8.400535386135623e-05,
      "loss": 1.1179,
      "step": 12275
    },
    {
      "epoch": 1.725854070012653,
      "grad_norm": 1.6455570459365845,
      "learning_rate": 8.37759206838008e-05,
      "loss": 1.1254,
      "step": 12276
    },
    {
      "epoch": 1.7259946576690566,
      "grad_norm": 1.5294413566589355,
      "learning_rate": 8.354657518447687e-05,
      "loss": 1.0542,
      "step": 12277
    },
    {
      "epoch": 1.7261352453254606,
      "grad_norm": 1.3841676712036133,
      "learning_rate": 8.331731860281465e-05,
      "loss": 1.0286,
      "step": 12278
    },
    {
      "epoch": 1.7262758329818642,
      "grad_norm": 1.7684122323989868,
      "learning_rate": 8.308815217776273e-05,
      "loss": 1.2683,
      "step": 12279
    },
    {
      "epoch": 1.726416420638268,
      "grad_norm": 1.8601815700531006,
      "learning_rate": 8.285907714778351e-05,
      "loss": 1.0656,
      "step": 12280
    },
    {
      "epoch": 1.7265570082946717,
      "grad_norm": 1.3583251237869263,
      "learning_rate": 8.263009475084515e-05,
      "loss": 1.2341,
      "step": 12281
    },
    {
      "epoch": 1.7266975959510755,
      "grad_norm": 1.5954780578613281,
      "learning_rate": 8.240120622441488e-05,
      "loss": 1.1379,
      "step": 12282
    },
    {
      "epoch": 1.7268381836074793,
      "grad_norm": 1.5127347707748413,
      "learning_rate": 8.217241280545395e-05,
      "loss": 1.0876,
      "step": 12283
    },
    {
      "epoch": 1.7269787712638829,
      "grad_norm": 1.4718456268310547,
      "learning_rate": 8.19437157304076e-05,
      "loss": 0.9962,
      "step": 12284
    },
    {
      "epoch": 1.727119358920287,
      "grad_norm": 1.5729894638061523,
      "learning_rate": 8.171511623520179e-05,
      "loss": 1.0447,
      "step": 12285
    },
    {
      "epoch": 1.7272599465766905,
      "grad_norm": 1.8396998643875122,
      "learning_rate": 8.148661555523481e-05,
      "loss": 1.0382,
      "step": 12286
    },
    {
      "epoch": 1.7274005342330945,
      "grad_norm": 1.527026891708374,
      "learning_rate": 8.125821492537086e-05,
      "loss": 1.0599,
      "step": 12287
    },
    {
      "epoch": 1.727541121889498,
      "grad_norm": 1.628642201423645,
      "learning_rate": 8.102991557993352e-05,
      "loss": 1.0148,
      "step": 12288
    },
    {
      "epoch": 1.7276817095459018,
      "grad_norm": 1.758425235748291,
      "learning_rate": 8.0801718752699e-05,
      "loss": 1.243,
      "step": 12289
    },
    {
      "epoch": 1.7278222972023056,
      "grad_norm": 1.482932448387146,
      "learning_rate": 8.057362567688947e-05,
      "loss": 1.0103,
      "step": 12290
    },
    {
      "epoch": 1.7279628848587094,
      "grad_norm": 1.4225611686706543,
      "learning_rate": 8.034563758516638e-05,
      "loss": 1.0316,
      "step": 12291
    },
    {
      "epoch": 1.7281034725151132,
      "grad_norm": 1.564996361732483,
      "learning_rate": 8.011775570962386e-05,
      "loss": 1.0357,
      "step": 12292
    },
    {
      "epoch": 1.728244060171517,
      "grad_norm": 1.541090965270996,
      "learning_rate": 7.9889981281782e-05,
      "loss": 0.9669,
      "step": 12293
    },
    {
      "epoch": 1.7283846478279208,
      "grad_norm": 1.7012736797332764,
      "learning_rate": 7.96623155325806e-05,
      "loss": 1.2487,
      "step": 12294
    },
    {
      "epoch": 1.7285252354843244,
      "grad_norm": 1.5335321426391602,
      "learning_rate": 7.943475969237108e-05,
      "loss": 1.1181,
      "step": 12295
    },
    {
      "epoch": 1.7286658231407284,
      "grad_norm": 1.4630584716796875,
      "learning_rate": 7.920731499091191e-05,
      "loss": 1.0407,
      "step": 12296
    },
    {
      "epoch": 1.728806410797132,
      "grad_norm": 1.4411075115203857,
      "learning_rate": 7.897998265736028e-05,
      "loss": 1.1393,
      "step": 12297
    },
    {
      "epoch": 1.728946998453536,
      "grad_norm": 1.59038507938385,
      "learning_rate": 7.875276392026715e-05,
      "loss": 1.1195,
      "step": 12298
    },
    {
      "epoch": 1.7290875861099395,
      "grad_norm": 1.7821540832519531,
      "learning_rate": 7.85256600075689e-05,
      "loss": 1.028,
      "step": 12299
    },
    {
      "epoch": 1.7292281737663433,
      "grad_norm": 1.512197494506836,
      "learning_rate": 7.829867214658065e-05,
      "loss": 1.0887,
      "step": 12300
    },
    {
      "epoch": 1.729368761422747,
      "grad_norm": 1.5457152128219604,
      "learning_rate": 7.807180156399143e-05,
      "loss": 1.08,
      "step": 12301
    },
    {
      "epoch": 1.7295093490791509,
      "grad_norm": 1.4961349964141846,
      "learning_rate": 7.78450494858561e-05,
      "loss": 0.933,
      "step": 12302
    },
    {
      "epoch": 1.7296499367355547,
      "grad_norm": 1.408154845237732,
      "learning_rate": 7.761841713758912e-05,
      "loss": 1.1524,
      "step": 12303
    },
    {
      "epoch": 1.7297905243919582,
      "grad_norm": 1.659247636795044,
      "learning_rate": 7.739190574395787e-05,
      "loss": 1.0898,
      "step": 12304
    },
    {
      "epoch": 1.7299311120483623,
      "grad_norm": 1.5953243970870972,
      "learning_rate": 7.716551652907613e-05,
      "loss": 1.2154,
      "step": 12305
    },
    {
      "epoch": 1.7300716997047658,
      "grad_norm": 1.5160232782363892,
      "learning_rate": 7.693925071639731e-05,
      "loss": 1.2495,
      "step": 12306
    },
    {
      "epoch": 1.7302122873611698,
      "grad_norm": 1.9862990379333496,
      "learning_rate": 7.671310952870803e-05,
      "loss": 0.9252,
      "step": 12307
    },
    {
      "epoch": 1.7303528750175734,
      "grad_norm": 1.400748610496521,
      "learning_rate": 7.648709418812132e-05,
      "loss": 1.1755,
      "step": 12308
    },
    {
      "epoch": 1.7304934626739772,
      "grad_norm": 1.6954270601272583,
      "learning_rate": 7.626120591607017e-05,
      "loss": 0.9799,
      "step": 12309
    },
    {
      "epoch": 1.730634050330381,
      "grad_norm": 1.5416268110275269,
      "learning_rate": 7.60354459333012e-05,
      "loss": 0.9678,
      "step": 12310
    },
    {
      "epoch": 1.7307746379867848,
      "grad_norm": 1.6860052347183228,
      "learning_rate": 7.580981545986667e-05,
      "loss": 1.1134,
      "step": 12311
    },
    {
      "epoch": 1.7309152256431886,
      "grad_norm": 1.4356820583343506,
      "learning_rate": 7.558431571511964e-05,
      "loss": 0.9615,
      "step": 12312
    },
    {
      "epoch": 1.7310558132995923,
      "grad_norm": 1.6673287153244019,
      "learning_rate": 7.53589479177074e-05,
      "loss": 1.0942,
      "step": 12313
    },
    {
      "epoch": 1.7311964009559961,
      "grad_norm": 1.4634636640548706,
      "learning_rate": 7.513371328556312e-05,
      "loss": 1.0112,
      "step": 12314
    },
    {
      "epoch": 1.7313369886123997,
      "grad_norm": 1.3798372745513916,
      "learning_rate": 7.490861303590062e-05,
      "loss": 1.2107,
      "step": 12315
    },
    {
      "epoch": 1.7314775762688037,
      "grad_norm": 2.023270606994629,
      "learning_rate": 7.468364838520793e-05,
      "loss": 1.04,
      "step": 12316
    },
    {
      "epoch": 1.7316181639252073,
      "grad_norm": 1.4064218997955322,
      "learning_rate": 7.445882054923915e-05,
      "loss": 1.0766,
      "step": 12317
    },
    {
      "epoch": 1.7317587515816113,
      "grad_norm": 1.5192598104476929,
      "learning_rate": 7.42341307430101e-05,
      "loss": 0.9483,
      "step": 12318
    },
    {
      "epoch": 1.7318993392380149,
      "grad_norm": 1.6854236125946045,
      "learning_rate": 7.400958018079027e-05,
      "loss": 1.2232,
      "step": 12319
    },
    {
      "epoch": 1.7320399268944187,
      "grad_norm": 1.784664511680603,
      "learning_rate": 7.378517007609633e-05,
      "loss": 0.9988,
      "step": 12320
    },
    {
      "epoch": 1.7321805145508224,
      "grad_norm": 1.464421272277832,
      "learning_rate": 7.356090164168725e-05,
      "loss": 1.0761,
      "step": 12321
    },
    {
      "epoch": 1.7323211022072262,
      "grad_norm": 1.6807143688201904,
      "learning_rate": 7.333677608955436e-05,
      "loss": 0.9335,
      "step": 12322
    },
    {
      "epoch": 1.73246168986363,
      "grad_norm": 1.4771015644073486,
      "learning_rate": 7.311279463091816e-05,
      "loss": 1.0915,
      "step": 12323
    },
    {
      "epoch": 1.7326022775200336,
      "grad_norm": 1.5912957191467285,
      "learning_rate": 7.288895847622007e-05,
      "loss": 1.2461,
      "step": 12324
    },
    {
      "epoch": 1.7327428651764376,
      "grad_norm": 1.6830767393112183,
      "learning_rate": 7.266526883511633e-05,
      "loss": 1.1654,
      "step": 12325
    },
    {
      "epoch": 1.7328834528328412,
      "grad_norm": 1.5205782651901245,
      "learning_rate": 7.244172691647168e-05,
      "loss": 1.1387,
      "step": 12326
    },
    {
      "epoch": 1.7330240404892452,
      "grad_norm": 1.7336591482162476,
      "learning_rate": 7.221833392835122e-05,
      "loss": 1.0523,
      "step": 12327
    },
    {
      "epoch": 1.7331646281456488,
      "grad_norm": 1.6028996706008911,
      "learning_rate": 7.199509107801716e-05,
      "loss": 1.1247,
      "step": 12328
    },
    {
      "epoch": 1.7333052158020525,
      "grad_norm": 1.5917118787765503,
      "learning_rate": 7.177199957191907e-05,
      "loss": 1.0696,
      "step": 12329
    },
    {
      "epoch": 1.7334458034584563,
      "grad_norm": 1.486875295639038,
      "learning_rate": 7.154906061568892e-05,
      "loss": 1.1448,
      "step": 12330
    },
    {
      "epoch": 1.7335863911148601,
      "grad_norm": 1.645142674446106,
      "learning_rate": 7.132627541413422e-05,
      "loss": 1.0255,
      "step": 12331
    },
    {
      "epoch": 1.733726978771264,
      "grad_norm": 1.8444617986679077,
      "learning_rate": 7.1103645171232e-05,
      "loss": 1.1768,
      "step": 12332
    },
    {
      "epoch": 1.7338675664276677,
      "grad_norm": 1.6591284275054932,
      "learning_rate": 7.088117109012078e-05,
      "loss": 1.0252,
      "step": 12333
    },
    {
      "epoch": 1.7340081540840715,
      "grad_norm": 1.4669004678726196,
      "learning_rate": 7.065885437309617e-05,
      "loss": 1.1512,
      "step": 12334
    },
    {
      "epoch": 1.734148741740475,
      "grad_norm": 1.6833014488220215,
      "learning_rate": 7.04366962216027e-05,
      "loss": 1.0754,
      "step": 12335
    },
    {
      "epoch": 1.734289329396879,
      "grad_norm": 1.6843348741531372,
      "learning_rate": 7.021469783622895e-05,
      "loss": 0.9649,
      "step": 12336
    },
    {
      "epoch": 1.7344299170532826,
      "grad_norm": 1.6505831480026245,
      "learning_rate": 6.999286041669947e-05,
      "loss": 0.8975,
      "step": 12337
    },
    {
      "epoch": 1.7345705047096867,
      "grad_norm": 1.4535019397735596,
      "learning_rate": 6.97711851618682e-05,
      "loss": 1.0537,
      "step": 12338
    },
    {
      "epoch": 1.7347110923660902,
      "grad_norm": 1.5138769149780273,
      "learning_rate": 6.954967326971375e-05,
      "loss": 1.2657,
      "step": 12339
    },
    {
      "epoch": 1.734851680022494,
      "grad_norm": 1.7194744348526,
      "learning_rate": 6.932832593733154e-05,
      "loss": 1.1401,
      "step": 12340
    },
    {
      "epoch": 1.7349922676788978,
      "grad_norm": 1.6373240947723389,
      "learning_rate": 6.910714436092766e-05,
      "loss": 0.9818,
      "step": 12341
    },
    {
      "epoch": 1.7351328553353016,
      "grad_norm": 1.4032845497131348,
      "learning_rate": 6.888612973581243e-05,
      "loss": 1.095,
      "step": 12342
    },
    {
      "epoch": 1.7352734429917054,
      "grad_norm": 1.5104215145111084,
      "learning_rate": 6.866528325639393e-05,
      "loss": 1.0943,
      "step": 12343
    },
    {
      "epoch": 1.735414030648109,
      "grad_norm": 1.616909146308899,
      "learning_rate": 6.844460611617156e-05,
      "loss": 1.239,
      "step": 12344
    },
    {
      "epoch": 1.735554618304513,
      "grad_norm": 1.6431347131729126,
      "learning_rate": 6.822409950772956e-05,
      "loss": 1.1239,
      "step": 12345
    },
    {
      "epoch": 1.7356952059609165,
      "grad_norm": 1.4830875396728516,
      "learning_rate": 6.800376462273058e-05,
      "loss": 1.0317,
      "step": 12346
    },
    {
      "epoch": 1.7358357936173205,
      "grad_norm": 1.5287758111953735,
      "learning_rate": 6.778360265190926e-05,
      "loss": 0.9558,
      "step": 12347
    },
    {
      "epoch": 1.7359763812737241,
      "grad_norm": 1.6985586881637573,
      "learning_rate": 6.75636147850661e-05,
      "loss": 1.0171,
      "step": 12348
    },
    {
      "epoch": 1.736116968930128,
      "grad_norm": 1.5501084327697754,
      "learning_rate": 6.73438022110597e-05,
      "loss": 1.0464,
      "step": 12349
    },
    {
      "epoch": 1.7362575565865317,
      "grad_norm": 1.5086445808410645,
      "learning_rate": 6.712416611780201e-05,
      "loss": 1.2179,
      "step": 12350
    },
    {
      "epoch": 1.7363981442429355,
      "grad_norm": 1.8795744180679321,
      "learning_rate": 6.690470769225192e-05,
      "loss": 1.0241,
      "step": 12351
    },
    {
      "epoch": 1.7365387318993393,
      "grad_norm": 1.5189627408981323,
      "learning_rate": 6.668542812040714e-05,
      "loss": 1.108,
      "step": 12352
    },
    {
      "epoch": 1.7366793195557428,
      "grad_norm": 1.4488215446472168,
      "learning_rate": 6.646632858729954e-05,
      "loss": 0.9891,
      "step": 12353
    },
    {
      "epoch": 1.7368199072121469,
      "grad_norm": 1.6218611001968384,
      "learning_rate": 6.624741027698695e-05,
      "loss": 1.0796,
      "step": 12354
    },
    {
      "epoch": 1.7369604948685504,
      "grad_norm": 1.5980743169784546,
      "learning_rate": 6.60286743725489e-05,
      "loss": 1.1426,
      "step": 12355
    },
    {
      "epoch": 1.7371010825249544,
      "grad_norm": 1.2959527969360352,
      "learning_rate": 6.58101220560788e-05,
      "loss": 1.2497,
      "step": 12356
    },
    {
      "epoch": 1.737241670181358,
      "grad_norm": 1.86065673828125,
      "learning_rate": 6.559175450867791e-05,
      "loss": 1.181,
      "step": 12357
    },
    {
      "epoch": 1.7373822578377618,
      "grad_norm": 1.4903912544250488,
      "learning_rate": 6.537357291044861e-05,
      "loss": 1.0948,
      "step": 12358
    },
    {
      "epoch": 1.7375228454941656,
      "grad_norm": 1.754939079284668,
      "learning_rate": 6.515557844048976e-05,
      "loss": 1.0485,
      "step": 12359
    },
    {
      "epoch": 1.7376634331505694,
      "grad_norm": 1.793554663658142,
      "learning_rate": 6.493777227688691e-05,
      "loss": 1.2003,
      "step": 12360
    },
    {
      "epoch": 1.7378040208069732,
      "grad_norm": 1.4225022792816162,
      "learning_rate": 6.472015559670928e-05,
      "loss": 0.89,
      "step": 12361
    },
    {
      "epoch": 1.737944608463377,
      "grad_norm": 1.2932795286178589,
      "learning_rate": 6.450272957600174e-05,
      "loss": 1.0665,
      "step": 12362
    },
    {
      "epoch": 1.7380851961197807,
      "grad_norm": 1.8508555889129639,
      "learning_rate": 6.428549538977888e-05,
      "loss": 1.1879,
      "step": 12363
    },
    {
      "epoch": 1.7382257837761843,
      "grad_norm": 1.6350359916687012,
      "learning_rate": 6.406845421201881e-05,
      "loss": 0.9738,
      "step": 12364
    },
    {
      "epoch": 1.7383663714325883,
      "grad_norm": 1.725931167602539,
      "learning_rate": 6.385160721565533e-05,
      "loss": 1.0424,
      "step": 12365
    },
    {
      "epoch": 1.7385069590889919,
      "grad_norm": 1.4184691905975342,
      "learning_rate": 6.363495557257477e-05,
      "loss": 1.0796,
      "step": 12366
    },
    {
      "epoch": 1.738647546745396,
      "grad_norm": 1.54237961769104,
      "learning_rate": 6.341850045360646e-05,
      "loss": 1.292,
      "step": 12367
    },
    {
      "epoch": 1.7387881344017995,
      "grad_norm": 1.3660997152328491,
      "learning_rate": 6.320224302851796e-05,
      "loss": 1.1688,
      "step": 12368
    },
    {
      "epoch": 1.7389287220582033,
      "grad_norm": 1.4693973064422607,
      "learning_rate": 6.298618446600883e-05,
      "loss": 1.0048,
      "step": 12369
    },
    {
      "epoch": 1.739069309714607,
      "grad_norm": 1.7828046083450317,
      "learning_rate": 6.277032593370292e-05,
      "loss": 1.0643,
      "step": 12370
    },
    {
      "epoch": 1.7392098973710108,
      "grad_norm": 1.457342505455017,
      "learning_rate": 6.255466859814402e-05,
      "loss": 0.927,
      "step": 12371
    },
    {
      "epoch": 1.7393504850274146,
      "grad_norm": 1.6931202411651611,
      "learning_rate": 6.233921362478839e-05,
      "loss": 1.0446,
      "step": 12372
    },
    {
      "epoch": 1.7394910726838182,
      "grad_norm": 1.6610876321792603,
      "learning_rate": 6.212396217799826e-05,
      "loss": 1.0157,
      "step": 12373
    },
    {
      "epoch": 1.7396316603402222,
      "grad_norm": 1.3163844347000122,
      "learning_rate": 6.190891542103698e-05,
      "loss": 0.9444,
      "step": 12374
    },
    {
      "epoch": 1.7397722479966258,
      "grad_norm": 1.3938895463943481,
      "learning_rate": 6.169407451606107e-05,
      "loss": 0.9158,
      "step": 12375
    },
    {
      "epoch": 1.7399128356530298,
      "grad_norm": 1.9499300718307495,
      "learning_rate": 6.147944062411391e-05,
      "loss": 0.9535,
      "step": 12376
    },
    {
      "epoch": 1.7400534233094334,
      "grad_norm": 2.063044309616089,
      "learning_rate": 6.126501490512111e-05,
      "loss": 0.9681,
      "step": 12377
    },
    {
      "epoch": 1.7401940109658371,
      "grad_norm": 1.450860857963562,
      "learning_rate": 6.105079851788298e-05,
      "loss": 0.9136,
      "step": 12378
    },
    {
      "epoch": 1.740334598622241,
      "grad_norm": 1.7495800256729126,
      "learning_rate": 6.0836792620068536e-05,
      "loss": 0.9968,
      "step": 12379
    },
    {
      "epoch": 1.7404751862786447,
      "grad_norm": 1.707250952720642,
      "learning_rate": 6.062299836820959e-05,
      "loss": 0.9989,
      "step": 12380
    },
    {
      "epoch": 1.7406157739350485,
      "grad_norm": 1.4416673183441162,
      "learning_rate": 6.0409416917692885e-05,
      "loss": 1.1688,
      "step": 12381
    },
    {
      "epoch": 1.7407563615914523,
      "grad_norm": 1.6648168563842773,
      "learning_rate": 6.01960494227571e-05,
      "loss": 0.8068,
      "step": 12382
    },
    {
      "epoch": 1.740896949247856,
      "grad_norm": 1.8461543321609497,
      "learning_rate": 5.998289703648331e-05,
      "loss": 0.9266,
      "step": 12383
    },
    {
      "epoch": 1.7410375369042597,
      "grad_norm": 1.6635489463806152,
      "learning_rate": 5.976996091079048e-05,
      "loss": 1.1436,
      "step": 12384
    },
    {
      "epoch": 1.7411781245606637,
      "grad_norm": 1.4621760845184326,
      "learning_rate": 5.955724219642884e-05,
      "loss": 1.1119,
      "step": 12385
    },
    {
      "epoch": 1.7413187122170672,
      "grad_norm": 1.4964628219604492,
      "learning_rate": 5.9344742042974e-05,
      "loss": 1.0035,
      "step": 12386
    },
    {
      "epoch": 1.7414592998734713,
      "grad_norm": 1.4802427291870117,
      "learning_rate": 5.9132461598819446e-05,
      "loss": 0.9479,
      "step": 12387
    },
    {
      "epoch": 1.7415998875298748,
      "grad_norm": 1.4631190299987793,
      "learning_rate": 5.892040201117197e-05,
      "loss": 1.0488,
      "step": 12388
    },
    {
      "epoch": 1.7417404751862786,
      "grad_norm": 1.662528395652771,
      "learning_rate": 5.8708564426045454e-05,
      "loss": 1.2138,
      "step": 12389
    },
    {
      "epoch": 1.7418810628426824,
      "grad_norm": 1.845126986503601,
      "learning_rate": 5.849694998825306e-05,
      "loss": 1.0375,
      "step": 12390
    },
    {
      "epoch": 1.7420216504990862,
      "grad_norm": 1.5160140991210938,
      "learning_rate": 5.828555984140268e-05,
      "loss": 0.9979,
      "step": 12391
    },
    {
      "epoch": 1.74216223815549,
      "grad_norm": 1.7756463289260864,
      "learning_rate": 5.8074395127889094e-05,
      "loss": 1.0264,
      "step": 12392
    },
    {
      "epoch": 1.7423028258118936,
      "grad_norm": 1.321549415588379,
      "learning_rate": 5.7863456988889775e-05,
      "loss": 1.0638,
      "step": 12393
    },
    {
      "epoch": 1.7424434134682976,
      "grad_norm": 1.444644808769226,
      "learning_rate": 5.765274656435744e-05,
      "loss": 1.0676,
      "step": 12394
    },
    {
      "epoch": 1.7425840011247011,
      "grad_norm": 1.7007888555526733,
      "learning_rate": 5.744226499301415e-05,
      "loss": 1.0832,
      "step": 12395
    },
    {
      "epoch": 1.7427245887811051,
      "grad_norm": 1.6501232385635376,
      "learning_rate": 5.723201341234523e-05,
      "loss": 1.0289,
      "step": 12396
    },
    {
      "epoch": 1.7428651764375087,
      "grad_norm": 1.7095298767089844,
      "learning_rate": 5.702199295859307e-05,
      "loss": 1.0964,
      "step": 12397
    },
    {
      "epoch": 1.7430057640939125,
      "grad_norm": 1.460541009902954,
      "learning_rate": 5.681220476675102e-05,
      "loss": 1.1467,
      "step": 12398
    },
    {
      "epoch": 1.7431463517503163,
      "grad_norm": 1.3652880191802979,
      "learning_rate": 5.660264997055723e-05,
      "loss": 0.9728,
      "step": 12399
    },
    {
      "epoch": 1.74328693940672,
      "grad_norm": 1.591975212097168,
      "learning_rate": 5.6393329702488517e-05,
      "loss": 1.0457,
      "step": 12400
    },
    {
      "epoch": 1.7434275270631239,
      "grad_norm": 1.6333556175231934,
      "learning_rate": 5.618424509375428e-05,
      "loss": 1.0073,
      "step": 12401
    },
    {
      "epoch": 1.7435681147195277,
      "grad_norm": 1.7627644538879395,
      "learning_rate": 5.5975397274290686e-05,
      "loss": 1.0118,
      "step": 12402
    },
    {
      "epoch": 1.7437087023759315,
      "grad_norm": 1.2968131303787231,
      "learning_rate": 5.576678737275296e-05,
      "loss": 1.1026,
      "step": 12403
    },
    {
      "epoch": 1.743849290032335,
      "grad_norm": 1.6778831481933594,
      "learning_rate": 5.555841651651248e-05,
      "loss": 1.2504,
      "step": 12404
    },
    {
      "epoch": 1.743989877688739,
      "grad_norm": 1.3950468301773071,
      "learning_rate": 5.535028583164752e-05,
      "loss": 1.0824,
      "step": 12405
    },
    {
      "epoch": 1.7441304653451426,
      "grad_norm": 1.5970110893249512,
      "learning_rate": 5.514239644293873e-05,
      "loss": 0.9724,
      "step": 12406
    },
    {
      "epoch": 1.7442710530015466,
      "grad_norm": 1.5308796167373657,
      "learning_rate": 5.493474947386301e-05,
      "loss": 1.0978,
      "step": 12407
    },
    {
      "epoch": 1.7444116406579502,
      "grad_norm": 1.477608561515808,
      "learning_rate": 5.4727346046586324e-05,
      "loss": 1.0147,
      "step": 12408
    },
    {
      "epoch": 1.744552228314354,
      "grad_norm": 1.7913645505905151,
      "learning_rate": 5.4520187281959356e-05,
      "loss": 1.0912,
      "step": 12409
    },
    {
      "epoch": 1.7446928159707578,
      "grad_norm": 1.7845873832702637,
      "learning_rate": 5.431327429951032e-05,
      "loss": 1.1195,
      "step": 12410
    },
    {
      "epoch": 1.7448334036271615,
      "grad_norm": 1.7927255630493164,
      "learning_rate": 5.41066082174388e-05,
      "loss": 1.215,
      "step": 12411
    },
    {
      "epoch": 1.7449739912835653,
      "grad_norm": 1.2507704496383667,
      "learning_rate": 5.3900190152611385e-05,
      "loss": 1.2321,
      "step": 12412
    },
    {
      "epoch": 1.745114578939969,
      "grad_norm": 1.5820989608764648,
      "learning_rate": 5.369402122055244e-05,
      "loss": 1.0196,
      "step": 12413
    },
    {
      "epoch": 1.745255166596373,
      "grad_norm": 1.5138355493545532,
      "learning_rate": 5.3488102535441184e-05,
      "loss": 1.027,
      "step": 12414
    },
    {
      "epoch": 1.7453957542527765,
      "grad_norm": 1.4766995906829834,
      "learning_rate": 5.3282435210104184e-05,
      "loss": 1.2443,
      "step": 12415
    },
    {
      "epoch": 1.7455363419091805,
      "grad_norm": 1.38260018825531,
      "learning_rate": 5.307702035600959e-05,
      "loss": 1.0896,
      "step": 12416
    },
    {
      "epoch": 1.745676929565584,
      "grad_norm": 1.5070487260818481,
      "learning_rate": 5.2871859083261135e-05,
      "loss": 1.0653,
      "step": 12417
    },
    {
      "epoch": 1.7458175172219879,
      "grad_norm": 1.4991346597671509,
      "learning_rate": 5.266695250059246e-05,
      "loss": 1.0048,
      "step": 12418
    },
    {
      "epoch": 1.7459581048783916,
      "grad_norm": 1.8158903121948242,
      "learning_rate": 5.246230171535957e-05,
      "loss": 1.0807,
      "step": 12419
    },
    {
      "epoch": 1.7460986925347954,
      "grad_norm": 1.5414313077926636,
      "learning_rate": 5.225790783353797e-05,
      "loss": 1.2275,
      "step": 12420
    },
    {
      "epoch": 1.7462392801911992,
      "grad_norm": 1.503345012664795,
      "learning_rate": 5.2053771959713524e-05,
      "loss": 1.0581,
      "step": 12421
    },
    {
      "epoch": 1.746379867847603,
      "grad_norm": 1.6668076515197754,
      "learning_rate": 5.1849895197078125e-05,
      "loss": 0.9428,
      "step": 12422
    },
    {
      "epoch": 1.7465204555040068,
      "grad_norm": 1.7110084295272827,
      "learning_rate": 5.164627864742364e-05,
      "loss": 1.0845,
      "step": 12423
    },
    {
      "epoch": 1.7466610431604104,
      "grad_norm": 1.476316213607788,
      "learning_rate": 5.1442923411134815e-05,
      "loss": 1.1047,
      "step": 12424
    },
    {
      "epoch": 1.7468016308168144,
      "grad_norm": 1.4357513189315796,
      "learning_rate": 5.1239830587185064e-05,
      "loss": 0.9612,
      "step": 12425
    },
    {
      "epoch": 1.746942218473218,
      "grad_norm": 1.5768017768859863,
      "learning_rate": 5.103700127312915e-05,
      "loss": 1.1138,
      "step": 12426
    },
    {
      "epoch": 1.747082806129622,
      "grad_norm": 1.4990695714950562,
      "learning_rate": 5.083443656509861e-05,
      "loss": 1.1904,
      "step": 12427
    },
    {
      "epoch": 1.7472233937860255,
      "grad_norm": 1.422960877418518,
      "learning_rate": 5.06321375577941e-05,
      "loss": 0.9157,
      "step": 12428
    },
    {
      "epoch": 1.7473639814424293,
      "grad_norm": 1.6523096561431885,
      "learning_rate": 5.043010534448101e-05,
      "loss": 1.0371,
      "step": 12429
    },
    {
      "epoch": 1.7475045690988331,
      "grad_norm": 1.7911878824234009,
      "learning_rate": 5.0228341016981995e-05,
      "loss": 1.0596,
      "step": 12430
    },
    {
      "epoch": 1.747645156755237,
      "grad_norm": 1.534316897392273,
      "learning_rate": 5.00268456656729e-05,
      "loss": 1.0396,
      "step": 12431
    },
    {
      "epoch": 1.7477857444116407,
      "grad_norm": 1.5083028078079224,
      "learning_rate": 4.982562037947567e-05,
      "loss": 1.0046,
      "step": 12432
    },
    {
      "epoch": 1.7479263320680443,
      "grad_norm": 1.5273517370224,
      "learning_rate": 4.962466624585277e-05,
      "loss": 0.9151,
      "step": 12433
    },
    {
      "epoch": 1.7480669197244483,
      "grad_norm": 1.2455806732177734,
      "learning_rate": 4.94239843508013e-05,
      "loss": 1.1304,
      "step": 12434
    },
    {
      "epoch": 1.7482075073808518,
      "grad_norm": 1.5189422369003296,
      "learning_rate": 4.922357577884713e-05,
      "loss": 1.1475,
      "step": 12435
    },
    {
      "epoch": 1.7483480950372559,
      "grad_norm": 1.6017506122589111,
      "learning_rate": 4.902344161303903e-05,
      "loss": 1.0209,
      "step": 12436
    },
    {
      "epoch": 1.7484886826936594,
      "grad_norm": 1.5457578897476196,
      "learning_rate": 4.882358293494281e-05,
      "loss": 1.0965,
      "step": 12437
    },
    {
      "epoch": 1.7486292703500632,
      "grad_norm": 1.3131704330444336,
      "learning_rate": 4.8624000824635527e-05,
      "loss": 1.1577,
      "step": 12438
    },
    {
      "epoch": 1.748769858006467,
      "grad_norm": 1.4293029308319092,
      "learning_rate": 4.842469636069987e-05,
      "loss": 0.9215,
      "step": 12439
    },
    {
      "epoch": 1.7489104456628708,
      "grad_norm": 1.5554358959197998,
      "learning_rate": 4.822567062021719e-05,
      "loss": 1.163,
      "step": 12440
    },
    {
      "epoch": 1.7490510333192746,
      "grad_norm": 1.5011752843856812,
      "learning_rate": 4.802692467876344e-05,
      "loss": 1.1541,
      "step": 12441
    },
    {
      "epoch": 1.7491916209756784,
      "grad_norm": 1.8108834028244019,
      "learning_rate": 4.782845961040192e-05,
      "loss": 1.0195,
      "step": 12442
    },
    {
      "epoch": 1.7493322086320822,
      "grad_norm": 2.286402940750122,
      "learning_rate": 4.7630276487678906e-05,
      "loss": 1.0762,
      "step": 12443
    },
    {
      "epoch": 1.7494727962884857,
      "grad_norm": 1.3561394214630127,
      "learning_rate": 4.7432376381616076e-05,
      "loss": 0.9751,
      "step": 12444
    },
    {
      "epoch": 1.7496133839448897,
      "grad_norm": 1.4661355018615723,
      "learning_rate": 4.723476036170625e-05,
      "loss": 0.9913,
      "step": 12445
    },
    {
      "epoch": 1.7497539716012933,
      "grad_norm": 1.5615613460540771,
      "learning_rate": 4.703742949590608e-05,
      "loss": 1.0942,
      "step": 12446
    },
    {
      "epoch": 1.7498945592576973,
      "grad_norm": 1.458500623703003,
      "learning_rate": 4.684038485063209e-05,
      "loss": 1.0177,
      "step": 12447
    },
    {
      "epoch": 1.750035146914101,
      "grad_norm": 1.6232893466949463,
      "learning_rate": 4.6643627490753694e-05,
      "loss": 1.0668,
      "step": 12448
    },
    {
      "epoch": 1.7501757345705047,
      "grad_norm": 1.79775071144104,
      "learning_rate": 4.644715847958747e-05,
      "loss": 1.044,
      "step": 12449
    },
    {
      "epoch": 1.7503163222269085,
      "grad_norm": 1.3860065937042236,
      "learning_rate": 4.6250978878892914e-05,
      "loss": 1.0392,
      "step": 12450
    },
    {
      "epoch": 1.7504569098833123,
      "grad_norm": 1.5073540210723877,
      "learning_rate": 4.6055089748863654e-05,
      "loss": 0.9639,
      "step": 12451
    },
    {
      "epoch": 1.750597497539716,
      "grad_norm": 1.8374851942062378,
      "learning_rate": 4.585949214812481e-05,
      "loss": 1.0453,
      "step": 12452
    },
    {
      "epoch": 1.7507380851961196,
      "grad_norm": 1.4233098030090332,
      "learning_rate": 4.5664187133725645e-05,
      "loss": 1.0197,
      "step": 12453
    },
    {
      "epoch": 1.7508786728525236,
      "grad_norm": 1.48355233669281,
      "learning_rate": 4.5469175761134286e-05,
      "loss": 1.1058,
      "step": 12454
    },
    {
      "epoch": 1.7510192605089272,
      "grad_norm": 1.4639848470687866,
      "learning_rate": 4.5274459084232225e-05,
      "loss": 1.2312,
      "step": 12455
    },
    {
      "epoch": 1.7511598481653312,
      "grad_norm": 1.470642328262329,
      "learning_rate": 4.508003815530749e-05,
      "loss": 1.01,
      "step": 12456
    },
    {
      "epoch": 1.7513004358217348,
      "grad_norm": 1.6025882959365845,
      "learning_rate": 4.488591402505041e-05,
      "loss": 1.0379,
      "step": 12457
    },
    {
      "epoch": 1.7514410234781386,
      "grad_norm": 1.6272468566894531,
      "learning_rate": 4.469208774254794e-05,
      "loss": 1.0591,
      "step": 12458
    },
    {
      "epoch": 1.7515816111345424,
      "grad_norm": 1.428269624710083,
      "learning_rate": 4.4498560355276516e-05,
      "loss": 1.0631,
      "step": 12459
    },
    {
      "epoch": 1.7517221987909462,
      "grad_norm": 1.5937761068344116,
      "learning_rate": 4.43053329090976e-05,
      "loss": 0.9875,
      "step": 12460
    },
    {
      "epoch": 1.75186278644735,
      "grad_norm": 1.5159387588500977,
      "learning_rate": 4.4112406448251975e-05,
      "loss": 1.0318,
      "step": 12461
    },
    {
      "epoch": 1.7520033741037537,
      "grad_norm": 1.3954706192016602,
      "learning_rate": 4.391978201535297e-05,
      "loss": 1.0058,
      "step": 12462
    },
    {
      "epoch": 1.7521439617601575,
      "grad_norm": 1.3649369478225708,
      "learning_rate": 4.372746065138259e-05,
      "loss": 0.9073,
      "step": 12463
    },
    {
      "epoch": 1.752284549416561,
      "grad_norm": 1.387306809425354,
      "learning_rate": 4.353544339568438e-05,
      "loss": 0.9622,
      "step": 12464
    },
    {
      "epoch": 1.752425137072965,
      "grad_norm": 1.5369911193847656,
      "learning_rate": 4.3343731285959334e-05,
      "loss": 1.0655,
      "step": 12465
    },
    {
      "epoch": 1.7525657247293687,
      "grad_norm": 1.4227696657180786,
      "learning_rate": 4.3152325358258736e-05,
      "loss": 1.1,
      "step": 12466
    },
    {
      "epoch": 1.7527063123857727,
      "grad_norm": 1.583113670349121,
      "learning_rate": 4.296122664697868e-05,
      "loss": 1.1082,
      "step": 12467
    },
    {
      "epoch": 1.7528469000421762,
      "grad_norm": 1.6068549156188965,
      "learning_rate": 4.2770436184855825e-05,
      "loss": 0.9662,
      "step": 12468
    },
    {
      "epoch": 1.75298748769858,
      "grad_norm": 1.503350019454956,
      "learning_rate": 4.2579955002960725e-05,
      "loss": 0.9548,
      "step": 12469
    },
    {
      "epoch": 1.7531280753549838,
      "grad_norm": 1.6737936735153198,
      "learning_rate": 4.238978413069251e-05,
      "loss": 1.1793,
      "step": 12470
    },
    {
      "epoch": 1.7532686630113876,
      "grad_norm": 1.5832432508468628,
      "learning_rate": 4.219992459577333e-05,
      "loss": 1.0689,
      "step": 12471
    },
    {
      "epoch": 1.7534092506677914,
      "grad_norm": 1.5135329961776733,
      "learning_rate": 4.201037742424282e-05,
      "loss": 1.1571,
      "step": 12472
    },
    {
      "epoch": 1.753549838324195,
      "grad_norm": 1.4780405759811401,
      "learning_rate": 4.182114364045252e-05,
      "loss": 0.9652,
      "step": 12473
    },
    {
      "epoch": 1.753690425980599,
      "grad_norm": 1.369671106338501,
      "learning_rate": 4.1632224267060384e-05,
      "loss": 0.9807,
      "step": 12474
    },
    {
      "epoch": 1.7538310136370026,
      "grad_norm": 1.5252585411071777,
      "learning_rate": 4.144362032502519e-05,
      "loss": 1.0338,
      "step": 12475
    },
    {
      "epoch": 1.7539716012934066,
      "grad_norm": 1.5619804859161377,
      "learning_rate": 4.1255332833601104e-05,
      "loss": 1.0301,
      "step": 12476
    },
    {
      "epoch": 1.7541121889498101,
      "grad_norm": 1.7798078060150146,
      "learning_rate": 4.106736281033239e-05,
      "loss": 0.9778,
      "step": 12477
    },
    {
      "epoch": 1.754252776606214,
      "grad_norm": 1.4348806142807007,
      "learning_rate": 4.087971127104677e-05,
      "loss": 1.0048,
      "step": 12478
    },
    {
      "epoch": 1.7543933642626177,
      "grad_norm": 1.5017009973526,
      "learning_rate": 4.069237922985174e-05,
      "loss": 1.0082,
      "step": 12479
    },
    {
      "epoch": 1.7545339519190215,
      "grad_norm": 1.429492712020874,
      "learning_rate": 4.0505367699127506e-05,
      "loss": 1.0536,
      "step": 12480
    },
    {
      "epoch": 1.7546745395754253,
      "grad_norm": 1.720290184020996,
      "learning_rate": 4.03186776895231e-05,
      "loss": 1.1588,
      "step": 12481
    },
    {
      "epoch": 1.754815127231829,
      "grad_norm": 1.6017701625823975,
      "learning_rate": 4.013231020994938e-05,
      "loss": 1.1016,
      "step": 12482
    },
    {
      "epoch": 1.7549557148882329,
      "grad_norm": 1.4821813106536865,
      "learning_rate": 3.9946266267573596e-05,
      "loss": 1.0738,
      "step": 12483
    },
    {
      "epoch": 1.7550963025446364,
      "grad_norm": 1.5636342763900757,
      "learning_rate": 3.976054686781545e-05,
      "loss": 0.8848,
      "step": 12484
    },
    {
      "epoch": 1.7552368902010405,
      "grad_norm": 1.4570978879928589,
      "learning_rate": 3.9575153014340415e-05,
      "loss": 1.135,
      "step": 12485
    },
    {
      "epoch": 1.755377477857444,
      "grad_norm": 1.463745355606079,
      "learning_rate": 3.939008570905467e-05,
      "loss": 1.2186,
      "step": 12486
    },
    {
      "epoch": 1.755518065513848,
      "grad_norm": 1.5411726236343384,
      "learning_rate": 3.920534595209938e-05,
      "loss": 1.3446,
      "step": 12487
    },
    {
      "epoch": 1.7556586531702516,
      "grad_norm": 1.3981139659881592,
      "learning_rate": 3.902093474184667e-05,
      "loss": 1.0112,
      "step": 12488
    },
    {
      "epoch": 1.7557992408266554,
      "grad_norm": 1.575783133506775,
      "learning_rate": 3.883685307489149e-05,
      "loss": 1.0642,
      "step": 12489
    },
    {
      "epoch": 1.7559398284830592,
      "grad_norm": 1.5434865951538086,
      "learning_rate": 3.865310194604895e-05,
      "loss": 1.1213,
      "step": 12490
    },
    {
      "epoch": 1.756080416139463,
      "grad_norm": 1.6633492708206177,
      "learning_rate": 3.846968234834761e-05,
      "loss": 1.1713,
      "step": 12491
    },
    {
      "epoch": 1.7562210037958668,
      "grad_norm": 1.4231855869293213,
      "learning_rate": 3.8286595273024374e-05,
      "loss": 0.969,
      "step": 12492
    },
    {
      "epoch": 1.7563615914522703,
      "grad_norm": 1.530231237411499,
      "learning_rate": 3.810384170951937e-05,
      "loss": 1.3267,
      "step": 12493
    },
    {
      "epoch": 1.7565021791086743,
      "grad_norm": 1.6885608434677124,
      "learning_rate": 3.7921422645469564e-05,
      "loss": 1.1249,
      "step": 12494
    },
    {
      "epoch": 1.756642766765078,
      "grad_norm": 1.7451000213623047,
      "learning_rate": 3.7739339066704746e-05,
      "loss": 1.1897,
      "step": 12495
    },
    {
      "epoch": 1.756783354421482,
      "grad_norm": 1.6264516115188599,
      "learning_rate": 3.755759195724228e-05,
      "loss": 0.9917,
      "step": 12496
    },
    {
      "epoch": 1.7569239420778855,
      "grad_norm": 1.5189273357391357,
      "learning_rate": 3.737618229928027e-05,
      "loss": 1.0084,
      "step": 12497
    },
    {
      "epoch": 1.7570645297342893,
      "grad_norm": 1.4791351556777954,
      "learning_rate": 3.71951110731935e-05,
      "loss": 1.0144,
      "step": 12498
    },
    {
      "epoch": 1.757205117390693,
      "grad_norm": 1.5465911626815796,
      "learning_rate": 3.701437925752807e-05,
      "loss": 1.0959,
      "step": 12499
    },
    {
      "epoch": 1.7573457050470969,
      "grad_norm": 1.618025302886963,
      "learning_rate": 3.683398782899498e-05,
      "loss": 1.0581,
      "step": 12500
    },
    {
      "epoch": 1.7573457050470969,
      "eval_loss": 1.1446324586868286,
      "eval_runtime": 773.406,
      "eval_samples_per_second": 16.351,
      "eval_steps_per_second": 8.176,
      "step": 12500
    }
  ],
  "logging_steps": 1,
  "max_steps": 14226,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.1130487862439772e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
