{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.124701251230142,
  "eval_steps": 500,
  "global_step": 8000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00014058765640376775,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.298,
      "step": 1
    },
    {
      "epoch": 0.0002811753128075355,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.226,
      "step": 2
    },
    {
      "epoch": 0.00042176296921130323,
      "grad_norm": 7.245819091796875,
      "learning_rate": 2.8089887640449437e-07,
      "loss": 2.2579,
      "step": 3
    },
    {
      "epoch": 0.000562350625615071,
      "grad_norm": NaN,
      "learning_rate": 2.8089887640449437e-07,
      "loss": 2.3189,
      "step": 4
    },
    {
      "epoch": 0.0007029382820188388,
      "grad_norm": 8.10453987121582,
      "learning_rate": 5.617977528089887e-07,
      "loss": 2.2984,
      "step": 5
    },
    {
      "epoch": 0.0008435259384226065,
      "grad_norm": 8.996807098388672,
      "learning_rate": 8.426966292134832e-07,
      "loss": 2.2909,
      "step": 6
    },
    {
      "epoch": 0.0009841135948263741,
      "grad_norm": 8.052919387817383,
      "learning_rate": 1.1235955056179775e-06,
      "loss": 2.2472,
      "step": 7
    },
    {
      "epoch": 0.001124701251230142,
      "grad_norm": 8.764071464538574,
      "learning_rate": 1.404494382022472e-06,
      "loss": 2.2049,
      "step": 8
    },
    {
      "epoch": 0.0012652889076339097,
      "grad_norm": 7.241598129272461,
      "learning_rate": 1.6853932584269663e-06,
      "loss": 2.3267,
      "step": 9
    },
    {
      "epoch": 0.0014058765640376776,
      "grad_norm": 9.075315475463867,
      "learning_rate": 1.966292134831461e-06,
      "loss": 2.3615,
      "step": 10
    },
    {
      "epoch": 0.0015464642204414453,
      "grad_norm": 7.958353519439697,
      "learning_rate": 2.247191011235955e-06,
      "loss": 2.2994,
      "step": 11
    },
    {
      "epoch": 0.001687051876845213,
      "grad_norm": 8.55456256866455,
      "learning_rate": 2.5280898876404495e-06,
      "loss": 2.192,
      "step": 12
    },
    {
      "epoch": 0.0018276395332489808,
      "grad_norm": 9.233574867248535,
      "learning_rate": 2.808988764044944e-06,
      "loss": 2.0955,
      "step": 13
    },
    {
      "epoch": 0.0019682271896527483,
      "grad_norm": 8.310784339904785,
      "learning_rate": 3.089887640449438e-06,
      "loss": 2.2296,
      "step": 14
    },
    {
      "epoch": 0.002108814846056516,
      "grad_norm": 8.425408363342285,
      "learning_rate": 3.3707865168539327e-06,
      "loss": 2.1196,
      "step": 15
    },
    {
      "epoch": 0.002249402502460284,
      "grad_norm": 6.740445613861084,
      "learning_rate": 3.651685393258427e-06,
      "loss": 2.1978,
      "step": 16
    },
    {
      "epoch": 0.002389990158864052,
      "grad_norm": 7.740170001983643,
      "learning_rate": 3.932584269662922e-06,
      "loss": 2.1673,
      "step": 17
    },
    {
      "epoch": 0.0025305778152678194,
      "grad_norm": 8.986848831176758,
      "learning_rate": 4.213483146067416e-06,
      "loss": 2.3169,
      "step": 18
    },
    {
      "epoch": 0.0026711654716715873,
      "grad_norm": 7.2965898513793945,
      "learning_rate": 4.49438202247191e-06,
      "loss": 2.2532,
      "step": 19
    },
    {
      "epoch": 0.002811753128075355,
      "grad_norm": 7.668450355529785,
      "learning_rate": 4.7752808988764044e-06,
      "loss": 2.3154,
      "step": 20
    },
    {
      "epoch": 0.0029523407844791226,
      "grad_norm": 5.917820930480957,
      "learning_rate": 5.056179775280899e-06,
      "loss": 2.1,
      "step": 21
    },
    {
      "epoch": 0.0030929284408828905,
      "grad_norm": 5.082604885101318,
      "learning_rate": 5.3370786516853935e-06,
      "loss": 2.2146,
      "step": 22
    },
    {
      "epoch": 0.0032335160972866584,
      "grad_norm": 8.554009437561035,
      "learning_rate": 5.617977528089888e-06,
      "loss": 2.0414,
      "step": 23
    },
    {
      "epoch": 0.003374103753690426,
      "grad_norm": NaN,
      "learning_rate": 5.617977528089888e-06,
      "loss": 2.2316,
      "step": 24
    },
    {
      "epoch": 0.0035146914100941938,
      "grad_norm": 17.223711013793945,
      "learning_rate": 5.8988764044943826e-06,
      "loss": 2.1168,
      "step": 25
    },
    {
      "epoch": 0.0036552790664979616,
      "grad_norm": NaN,
      "learning_rate": 5.8988764044943826e-06,
      "loss": 2.1397,
      "step": 26
    },
    {
      "epoch": 0.003795866722901729,
      "grad_norm": 35.933746337890625,
      "learning_rate": 6.179775280898876e-06,
      "loss": 1.9748,
      "step": 27
    },
    {
      "epoch": 0.0039364543793054966,
      "grad_norm": 30.63168716430664,
      "learning_rate": 6.460674157303372e-06,
      "loss": 2.1746,
      "step": 28
    },
    {
      "epoch": 0.004077042035709265,
      "grad_norm": 29.3762264251709,
      "learning_rate": 6.741573033707865e-06,
      "loss": 2.0077,
      "step": 29
    },
    {
      "epoch": 0.004217629692113032,
      "grad_norm": 27.41930389404297,
      "learning_rate": 7.022471910112361e-06,
      "loss": 2.055,
      "step": 30
    },
    {
      "epoch": 0.004358217348516801,
      "grad_norm": 26.99652862548828,
      "learning_rate": 7.303370786516854e-06,
      "loss": 2.0234,
      "step": 31
    },
    {
      "epoch": 0.004498805004920568,
      "grad_norm": 19.323719024658203,
      "learning_rate": 7.584269662921349e-06,
      "loss": 2.1001,
      "step": 32
    },
    {
      "epoch": 0.004639392661324336,
      "grad_norm": 7.347014427185059,
      "learning_rate": 7.865168539325843e-06,
      "loss": 1.8841,
      "step": 33
    },
    {
      "epoch": 0.004779980317728104,
      "grad_norm": 4.512160778045654,
      "learning_rate": 8.146067415730338e-06,
      "loss": 2.0841,
      "step": 34
    },
    {
      "epoch": 0.004920567974131871,
      "grad_norm": 8.604472160339355,
      "learning_rate": 8.426966292134832e-06,
      "loss": 1.8288,
      "step": 35
    },
    {
      "epoch": 0.005061155630535639,
      "grad_norm": 10.339221000671387,
      "learning_rate": 8.707865168539327e-06,
      "loss": 1.8591,
      "step": 36
    },
    {
      "epoch": 0.005201743286939407,
      "grad_norm": 10.072115898132324,
      "learning_rate": 8.98876404494382e-06,
      "loss": 1.8997,
      "step": 37
    },
    {
      "epoch": 0.005342330943343175,
      "grad_norm": 14.209969520568848,
      "learning_rate": 9.269662921348316e-06,
      "loss": 1.985,
      "step": 38
    },
    {
      "epoch": 0.005482918599746942,
      "grad_norm": 11.842117309570312,
      "learning_rate": 9.550561797752809e-06,
      "loss": 1.8838,
      "step": 39
    },
    {
      "epoch": 0.00562350625615071,
      "grad_norm": 11.571284294128418,
      "learning_rate": 9.831460674157303e-06,
      "loss": 1.8013,
      "step": 40
    },
    {
      "epoch": 0.005764093912554478,
      "grad_norm": 9.730956077575684,
      "learning_rate": 1.0112359550561798e-05,
      "loss": 1.8069,
      "step": 41
    },
    {
      "epoch": 0.005904681568958245,
      "grad_norm": 3.7914469242095947,
      "learning_rate": 1.0393258426966292e-05,
      "loss": 1.7939,
      "step": 42
    },
    {
      "epoch": 0.006045269225362014,
      "grad_norm": 8.825640678405762,
      "learning_rate": 1.0674157303370787e-05,
      "loss": 1.7602,
      "step": 43
    },
    {
      "epoch": 0.006185856881765781,
      "grad_norm": 16.071815490722656,
      "learning_rate": 1.0955056179775282e-05,
      "loss": 1.7593,
      "step": 44
    },
    {
      "epoch": 0.0063264445381695485,
      "grad_norm": 10.431527137756348,
      "learning_rate": 1.1235955056179776e-05,
      "loss": 1.7729,
      "step": 45
    },
    {
      "epoch": 0.006467032194573317,
      "grad_norm": 8.959321022033691,
      "learning_rate": 1.151685393258427e-05,
      "loss": 1.6891,
      "step": 46
    },
    {
      "epoch": 0.006607619850977084,
      "grad_norm": 9.0274076461792,
      "learning_rate": 1.1797752808988765e-05,
      "loss": 1.6288,
      "step": 47
    },
    {
      "epoch": 0.006748207507380852,
      "grad_norm": 6.833653450012207,
      "learning_rate": 1.207865168539326e-05,
      "loss": 1.6786,
      "step": 48
    },
    {
      "epoch": 0.00688879516378462,
      "grad_norm": 11.65913200378418,
      "learning_rate": 1.2359550561797752e-05,
      "loss": 1.5992,
      "step": 49
    },
    {
      "epoch": 0.0070293828201883875,
      "grad_norm": 6.385910987854004,
      "learning_rate": 1.2640449438202249e-05,
      "loss": 1.5568,
      "step": 50
    },
    {
      "epoch": 0.007169970476592155,
      "grad_norm": 8.652697563171387,
      "learning_rate": 1.2921348314606743e-05,
      "loss": 1.7493,
      "step": 51
    },
    {
      "epoch": 0.007310558132995923,
      "grad_norm": 7.682476043701172,
      "learning_rate": 1.3202247191011236e-05,
      "loss": 1.6931,
      "step": 52
    },
    {
      "epoch": 0.007451145789399691,
      "grad_norm": 9.820348739624023,
      "learning_rate": 1.348314606741573e-05,
      "loss": 1.567,
      "step": 53
    },
    {
      "epoch": 0.007591733445803458,
      "grad_norm": 6.381404876708984,
      "learning_rate": 1.3764044943820225e-05,
      "loss": 1.5004,
      "step": 54
    },
    {
      "epoch": 0.0077323211022072265,
      "grad_norm": 8.900049209594727,
      "learning_rate": 1.4044943820224721e-05,
      "loss": 1.4591,
      "step": 55
    },
    {
      "epoch": 0.007872908758610993,
      "grad_norm": 10.145153999328613,
      "learning_rate": 1.4325842696629212e-05,
      "loss": 1.3139,
      "step": 56
    },
    {
      "epoch": 0.008013496415014762,
      "grad_norm": 5.016868591308594,
      "learning_rate": 1.4606741573033709e-05,
      "loss": 1.3611,
      "step": 57
    },
    {
      "epoch": 0.00815408407141853,
      "grad_norm": 7.837327003479004,
      "learning_rate": 1.4887640449438203e-05,
      "loss": 1.3761,
      "step": 58
    },
    {
      "epoch": 0.008294671727822297,
      "grad_norm": 6.839343547821045,
      "learning_rate": 1.5168539325842698e-05,
      "loss": 1.3073,
      "step": 59
    },
    {
      "epoch": 0.008435259384226065,
      "grad_norm": 4.361875057220459,
      "learning_rate": 1.544943820224719e-05,
      "loss": 1.2112,
      "step": 60
    },
    {
      "epoch": 0.008575847040629832,
      "grad_norm": 3.535879611968994,
      "learning_rate": 1.5730337078651687e-05,
      "loss": 1.3656,
      "step": 61
    },
    {
      "epoch": 0.008716434697033601,
      "grad_norm": 13.417065620422363,
      "learning_rate": 1.601123595505618e-05,
      "loss": 1.299,
      "step": 62
    },
    {
      "epoch": 0.008857022353437369,
      "grad_norm": 10.369181632995605,
      "learning_rate": 1.6292134831460676e-05,
      "loss": 1.3113,
      "step": 63
    },
    {
      "epoch": 0.008997610009841136,
      "grad_norm": 9.918169975280762,
      "learning_rate": 1.657303370786517e-05,
      "loss": 1.1965,
      "step": 64
    },
    {
      "epoch": 0.009138197666244904,
      "grad_norm": 4.8357319831848145,
      "learning_rate": 1.6853932584269665e-05,
      "loss": 1.1972,
      "step": 65
    },
    {
      "epoch": 0.009278785322648671,
      "grad_norm": 6.956521511077881,
      "learning_rate": 1.7134831460674158e-05,
      "loss": 1.2793,
      "step": 66
    },
    {
      "epoch": 0.009419372979052439,
      "grad_norm": 5.534328937530518,
      "learning_rate": 1.7415730337078654e-05,
      "loss": 1.3016,
      "step": 67
    },
    {
      "epoch": 0.009559960635456208,
      "grad_norm": 2.918738842010498,
      "learning_rate": 1.7696629213483147e-05,
      "loss": 1.1916,
      "step": 68
    },
    {
      "epoch": 0.009700548291859975,
      "grad_norm": 3.8146347999572754,
      "learning_rate": 1.797752808988764e-05,
      "loss": 1.3315,
      "step": 69
    },
    {
      "epoch": 0.009841135948263743,
      "grad_norm": 2.690953493118286,
      "learning_rate": 1.8258426966292136e-05,
      "loss": 1.3645,
      "step": 70
    },
    {
      "epoch": 0.00998172360466751,
      "grad_norm": 2.82820987701416,
      "learning_rate": 1.8539325842696632e-05,
      "loss": 1.2824,
      "step": 71
    },
    {
      "epoch": 0.010122311261071278,
      "grad_norm": 2.4890432357788086,
      "learning_rate": 1.8820224719101125e-05,
      "loss": 1.1715,
      "step": 72
    },
    {
      "epoch": 0.010262898917475045,
      "grad_norm": 2.3097381591796875,
      "learning_rate": 1.9101123595505618e-05,
      "loss": 1.2078,
      "step": 73
    },
    {
      "epoch": 0.010403486573878814,
      "grad_norm": 2.7446506023406982,
      "learning_rate": 1.9382022471910114e-05,
      "loss": 1.0115,
      "step": 74
    },
    {
      "epoch": 0.010544074230282582,
      "grad_norm": 2.422837734222412,
      "learning_rate": 1.9662921348314607e-05,
      "loss": 1.4363,
      "step": 75
    },
    {
      "epoch": 0.01068466188668635,
      "grad_norm": 3.793195962905884,
      "learning_rate": 1.99438202247191e-05,
      "loss": 1.0729,
      "step": 76
    },
    {
      "epoch": 0.010825249543090117,
      "grad_norm": 2.3601086139678955,
      "learning_rate": 2.0224719101123596e-05,
      "loss": 1.2179,
      "step": 77
    },
    {
      "epoch": 0.010965837199493884,
      "grad_norm": 2.886718511581421,
      "learning_rate": 2.0505617977528092e-05,
      "loss": 1.1277,
      "step": 78
    },
    {
      "epoch": 0.011106424855897652,
      "grad_norm": 3.333782911300659,
      "learning_rate": 2.0786516853932585e-05,
      "loss": 1.1409,
      "step": 79
    },
    {
      "epoch": 0.01124701251230142,
      "grad_norm": 2.8088269233703613,
      "learning_rate": 2.1067415730337078e-05,
      "loss": 1.0883,
      "step": 80
    },
    {
      "epoch": 0.011387600168705188,
      "grad_norm": 2.827481746673584,
      "learning_rate": 2.1348314606741574e-05,
      "loss": 1.1035,
      "step": 81
    },
    {
      "epoch": 0.011528187825108956,
      "grad_norm": 2.746737003326416,
      "learning_rate": 2.1629213483146067e-05,
      "loss": 1.2402,
      "step": 82
    },
    {
      "epoch": 0.011668775481512723,
      "grad_norm": 2.808919906616211,
      "learning_rate": 2.1910112359550563e-05,
      "loss": 1.2405,
      "step": 83
    },
    {
      "epoch": 0.01180936313791649,
      "grad_norm": 2.9713563919067383,
      "learning_rate": 2.2191011235955056e-05,
      "loss": 1.4111,
      "step": 84
    },
    {
      "epoch": 0.011949950794320258,
      "grad_norm": 2.4026224613189697,
      "learning_rate": 2.2471910112359552e-05,
      "loss": 1.197,
      "step": 85
    },
    {
      "epoch": 0.012090538450724027,
      "grad_norm": 2.6723475456237793,
      "learning_rate": 2.2752808988764045e-05,
      "loss": 1.1703,
      "step": 86
    },
    {
      "epoch": 0.012231126107127795,
      "grad_norm": 2.7952113151550293,
      "learning_rate": 2.303370786516854e-05,
      "loss": 1.1918,
      "step": 87
    },
    {
      "epoch": 0.012371713763531562,
      "grad_norm": 2.122035264968872,
      "learning_rate": 2.3314606741573034e-05,
      "loss": 1.351,
      "step": 88
    },
    {
      "epoch": 0.01251230141993533,
      "grad_norm": 2.3143932819366455,
      "learning_rate": 2.359550561797753e-05,
      "loss": 1.1774,
      "step": 89
    },
    {
      "epoch": 0.012652889076339097,
      "grad_norm": 2.3971498012542725,
      "learning_rate": 2.3876404494382023e-05,
      "loss": 1.1994,
      "step": 90
    },
    {
      "epoch": 0.012793476732742864,
      "grad_norm": 2.1432976722717285,
      "learning_rate": 2.415730337078652e-05,
      "loss": 1.2284,
      "step": 91
    },
    {
      "epoch": 0.012934064389146634,
      "grad_norm": 2.5893337726593018,
      "learning_rate": 2.4438202247191012e-05,
      "loss": 1.1681,
      "step": 92
    },
    {
      "epoch": 0.013074652045550401,
      "grad_norm": 3.9374842643737793,
      "learning_rate": 2.4719101123595505e-05,
      "loss": 1.0558,
      "step": 93
    },
    {
      "epoch": 0.013215239701954169,
      "grad_norm": 2.2501206398010254,
      "learning_rate": 2.5e-05,
      "loss": 1.0544,
      "step": 94
    },
    {
      "epoch": 0.013355827358357936,
      "grad_norm": 2.6196744441986084,
      "learning_rate": 2.5280898876404497e-05,
      "loss": 1.0449,
      "step": 95
    },
    {
      "epoch": 0.013496415014761703,
      "grad_norm": 2.6912648677825928,
      "learning_rate": 2.556179775280899e-05,
      "loss": 1.3097,
      "step": 96
    },
    {
      "epoch": 0.013637002671165471,
      "grad_norm": 2.8213303089141846,
      "learning_rate": 2.5842696629213486e-05,
      "loss": 1.078,
      "step": 97
    },
    {
      "epoch": 0.01377759032756924,
      "grad_norm": 2.288163185119629,
      "learning_rate": 2.6123595505617983e-05,
      "loss": 1.2944,
      "step": 98
    },
    {
      "epoch": 0.013918177983973008,
      "grad_norm": 2.4742157459259033,
      "learning_rate": 2.6404494382022472e-05,
      "loss": 1.112,
      "step": 99
    },
    {
      "epoch": 0.014058765640376775,
      "grad_norm": 2.6970505714416504,
      "learning_rate": 2.6685393258426965e-05,
      "loss": 1.2189,
      "step": 100
    },
    {
      "epoch": 0.014199353296780542,
      "grad_norm": 2.327686071395874,
      "learning_rate": 2.696629213483146e-05,
      "loss": 1.3227,
      "step": 101
    },
    {
      "epoch": 0.01433994095318431,
      "grad_norm": 2.6703150272369385,
      "learning_rate": 2.7247191011235957e-05,
      "loss": 1.0781,
      "step": 102
    },
    {
      "epoch": 0.014480528609588077,
      "grad_norm": 2.652482032775879,
      "learning_rate": 2.752808988764045e-05,
      "loss": 1.1364,
      "step": 103
    },
    {
      "epoch": 0.014621116265991847,
      "grad_norm": 2.9150097370147705,
      "learning_rate": 2.7808988764044946e-05,
      "loss": 1.0665,
      "step": 104
    },
    {
      "epoch": 0.014761703922395614,
      "grad_norm": 2.5454089641571045,
      "learning_rate": 2.8089887640449443e-05,
      "loss": 1.1735,
      "step": 105
    },
    {
      "epoch": 0.014902291578799381,
      "grad_norm": 2.6714460849761963,
      "learning_rate": 2.8370786516853936e-05,
      "loss": 1.1242,
      "step": 106
    },
    {
      "epoch": 0.015042879235203149,
      "grad_norm": 2.2454006671905518,
      "learning_rate": 2.8651685393258425e-05,
      "loss": 1.3172,
      "step": 107
    },
    {
      "epoch": 0.015183466891606916,
      "grad_norm": 2.5474889278411865,
      "learning_rate": 2.893258426966292e-05,
      "loss": 1.1229,
      "step": 108
    },
    {
      "epoch": 0.015324054548010684,
      "grad_norm": 2.181563377380371,
      "learning_rate": 2.9213483146067417e-05,
      "loss": 1.2548,
      "step": 109
    },
    {
      "epoch": 0.015464642204414453,
      "grad_norm": 2.527151107788086,
      "learning_rate": 2.949438202247191e-05,
      "loss": 1.1055,
      "step": 110
    },
    {
      "epoch": 0.01560522986081822,
      "grad_norm": 2.00559139251709,
      "learning_rate": 2.9775280898876406e-05,
      "loss": 1.0931,
      "step": 111
    },
    {
      "epoch": 0.015745817517221986,
      "grad_norm": 2.2237608432769775,
      "learning_rate": 3.0056179775280903e-05,
      "loss": 1.1805,
      "step": 112
    },
    {
      "epoch": 0.015886405173625755,
      "grad_norm": 1.9891678094863892,
      "learning_rate": 3.0337078651685396e-05,
      "loss": 1.1655,
      "step": 113
    },
    {
      "epoch": 0.016026992830029525,
      "grad_norm": 2.9956021308898926,
      "learning_rate": 3.061797752808989e-05,
      "loss": 1.0302,
      "step": 114
    },
    {
      "epoch": 0.01616758048643329,
      "grad_norm": 2.1211533546447754,
      "learning_rate": 3.089887640449438e-05,
      "loss": 0.9253,
      "step": 115
    },
    {
      "epoch": 0.01630816814283706,
      "grad_norm": 2.7525973320007324,
      "learning_rate": 3.1179775280898874e-05,
      "loss": 1.038,
      "step": 116
    },
    {
      "epoch": 0.016448755799240825,
      "grad_norm": 2.4122238159179688,
      "learning_rate": 3.1460674157303374e-05,
      "loss": 1.1103,
      "step": 117
    },
    {
      "epoch": 0.016589343455644594,
      "grad_norm": 2.704362392425537,
      "learning_rate": 3.1741573033707866e-05,
      "loss": 1.0881,
      "step": 118
    },
    {
      "epoch": 0.016729931112048364,
      "grad_norm": 2.2417240142822266,
      "learning_rate": 3.202247191011236e-05,
      "loss": 1.2084,
      "step": 119
    },
    {
      "epoch": 0.01687051876845213,
      "grad_norm": 2.310286521911621,
      "learning_rate": 3.230337078651686e-05,
      "loss": 1.2365,
      "step": 120
    },
    {
      "epoch": 0.0170111064248559,
      "grad_norm": 2.479375123977661,
      "learning_rate": 3.258426966292135e-05,
      "loss": 1.0883,
      "step": 121
    },
    {
      "epoch": 0.017151694081259664,
      "grad_norm": 3.379467248916626,
      "learning_rate": 3.2865168539325845e-05,
      "loss": 1.1684,
      "step": 122
    },
    {
      "epoch": 0.017292281737663433,
      "grad_norm": 3.8731865882873535,
      "learning_rate": 3.314606741573034e-05,
      "loss": 1.1307,
      "step": 123
    },
    {
      "epoch": 0.017432869394067203,
      "grad_norm": 2.757347583770752,
      "learning_rate": 3.342696629213483e-05,
      "loss": 1.2052,
      "step": 124
    },
    {
      "epoch": 0.01757345705047097,
      "grad_norm": 3.3242719173431396,
      "learning_rate": 3.370786516853933e-05,
      "loss": 0.9349,
      "step": 125
    },
    {
      "epoch": 0.017714044706874738,
      "grad_norm": 2.385014295578003,
      "learning_rate": 3.398876404494382e-05,
      "loss": 1.407,
      "step": 126
    },
    {
      "epoch": 0.017854632363278503,
      "grad_norm": 2.5655360221862793,
      "learning_rate": 3.4269662921348316e-05,
      "loss": 1.0885,
      "step": 127
    },
    {
      "epoch": 0.017995220019682272,
      "grad_norm": 2.4665701389312744,
      "learning_rate": 3.455056179775281e-05,
      "loss": 1.0578,
      "step": 128
    },
    {
      "epoch": 0.018135807676086038,
      "grad_norm": 3.0541493892669678,
      "learning_rate": 3.483146067415731e-05,
      "loss": 1.1622,
      "step": 129
    },
    {
      "epoch": 0.018276395332489807,
      "grad_norm": 2.253767967224121,
      "learning_rate": 3.51123595505618e-05,
      "loss": 1.328,
      "step": 130
    },
    {
      "epoch": 0.018416982988893577,
      "grad_norm": 2.155627727508545,
      "learning_rate": 3.5393258426966294e-05,
      "loss": 1.1824,
      "step": 131
    },
    {
      "epoch": 0.018557570645297342,
      "grad_norm": 2.2615280151367188,
      "learning_rate": 3.5674157303370787e-05,
      "loss": 1.1132,
      "step": 132
    },
    {
      "epoch": 0.01869815830170111,
      "grad_norm": 2.0365400314331055,
      "learning_rate": 3.595505617977528e-05,
      "loss": 1.2145,
      "step": 133
    },
    {
      "epoch": 0.018838745958104877,
      "grad_norm": 2.1860344409942627,
      "learning_rate": 3.623595505617978e-05,
      "loss": 1.0693,
      "step": 134
    },
    {
      "epoch": 0.018979333614508646,
      "grad_norm": 2.77993106842041,
      "learning_rate": 3.651685393258427e-05,
      "loss": 1.132,
      "step": 135
    },
    {
      "epoch": 0.019119921270912416,
      "grad_norm": 2.168142318725586,
      "learning_rate": 3.6797752808988765e-05,
      "loss": 1.2314,
      "step": 136
    },
    {
      "epoch": 0.01926050892731618,
      "grad_norm": 2.6242620944976807,
      "learning_rate": 3.7078651685393264e-05,
      "loss": 1.2417,
      "step": 137
    },
    {
      "epoch": 0.01940109658371995,
      "grad_norm": 2.464482069015503,
      "learning_rate": 3.735955056179776e-05,
      "loss": 1.1672,
      "step": 138
    },
    {
      "epoch": 0.019541684240123716,
      "grad_norm": 2.4668045043945312,
      "learning_rate": 3.764044943820225e-05,
      "loss": 1.1354,
      "step": 139
    },
    {
      "epoch": 0.019682271896527485,
      "grad_norm": 2.401266098022461,
      "learning_rate": 3.792134831460674e-05,
      "loss": 1.1899,
      "step": 140
    },
    {
      "epoch": 0.01982285955293125,
      "grad_norm": 2.495306968688965,
      "learning_rate": 3.8202247191011236e-05,
      "loss": 0.974,
      "step": 141
    },
    {
      "epoch": 0.01996344720933502,
      "grad_norm": 2.9178707599639893,
      "learning_rate": 3.8483146067415735e-05,
      "loss": 1.1458,
      "step": 142
    },
    {
      "epoch": 0.02010403486573879,
      "grad_norm": 2.1856331825256348,
      "learning_rate": 3.876404494382023e-05,
      "loss": 1.2444,
      "step": 143
    },
    {
      "epoch": 0.020244622522142555,
      "grad_norm": 2.6706271171569824,
      "learning_rate": 3.904494382022472e-05,
      "loss": 1.0776,
      "step": 144
    },
    {
      "epoch": 0.020385210178546324,
      "grad_norm": 2.398362398147583,
      "learning_rate": 3.9325842696629214e-05,
      "loss": 0.9715,
      "step": 145
    },
    {
      "epoch": 0.02052579783495009,
      "grad_norm": 2.7096288204193115,
      "learning_rate": 3.960674157303371e-05,
      "loss": 1.1082,
      "step": 146
    },
    {
      "epoch": 0.02066638549135386,
      "grad_norm": 2.482909679412842,
      "learning_rate": 3.98876404494382e-05,
      "loss": 1.1334,
      "step": 147
    },
    {
      "epoch": 0.02080697314775763,
      "grad_norm": 2.3365519046783447,
      "learning_rate": 4.01685393258427e-05,
      "loss": 1.2429,
      "step": 148
    },
    {
      "epoch": 0.020947560804161394,
      "grad_norm": 2.356829881668091,
      "learning_rate": 4.044943820224719e-05,
      "loss": 1.1163,
      "step": 149
    },
    {
      "epoch": 0.021088148460565163,
      "grad_norm": 2.738759994506836,
      "learning_rate": 4.0730337078651685e-05,
      "loss": 0.9433,
      "step": 150
    },
    {
      "epoch": 0.02122873611696893,
      "grad_norm": 2.3433585166931152,
      "learning_rate": 4.1011235955056184e-05,
      "loss": 1.0974,
      "step": 151
    },
    {
      "epoch": 0.0213693237733727,
      "grad_norm": 2.164194107055664,
      "learning_rate": 4.129213483146068e-05,
      "loss": 1.1723,
      "step": 152
    },
    {
      "epoch": 0.021509911429776464,
      "grad_norm": 2.3013522624969482,
      "learning_rate": 4.157303370786517e-05,
      "loss": 1.2237,
      "step": 153
    },
    {
      "epoch": 0.021650499086180233,
      "grad_norm": 2.5231783390045166,
      "learning_rate": 4.185393258426967e-05,
      "loss": 1.0693,
      "step": 154
    },
    {
      "epoch": 0.021791086742584002,
      "grad_norm": 3.1130521297454834,
      "learning_rate": 4.2134831460674156e-05,
      "loss": 1.0361,
      "step": 155
    },
    {
      "epoch": 0.021931674398987768,
      "grad_norm": 2.4152162075042725,
      "learning_rate": 4.2415730337078655e-05,
      "loss": 1.1798,
      "step": 156
    },
    {
      "epoch": 0.022072262055391537,
      "grad_norm": 2.383089542388916,
      "learning_rate": 4.269662921348315e-05,
      "loss": 1.325,
      "step": 157
    },
    {
      "epoch": 0.022212849711795303,
      "grad_norm": 2.531989336013794,
      "learning_rate": 4.297752808988764e-05,
      "loss": 1.2037,
      "step": 158
    },
    {
      "epoch": 0.022353437368199072,
      "grad_norm": 2.649341106414795,
      "learning_rate": 4.3258426966292134e-05,
      "loss": 1.1645,
      "step": 159
    },
    {
      "epoch": 0.02249402502460284,
      "grad_norm": 2.839721918106079,
      "learning_rate": 4.353932584269663e-05,
      "loss": 1.1336,
      "step": 160
    },
    {
      "epoch": 0.022634612681006607,
      "grad_norm": 2.6971545219421387,
      "learning_rate": 4.3820224719101126e-05,
      "loss": 1.0353,
      "step": 161
    },
    {
      "epoch": 0.022775200337410376,
      "grad_norm": 2.523728847503662,
      "learning_rate": 4.410112359550562e-05,
      "loss": 1.1275,
      "step": 162
    },
    {
      "epoch": 0.022915787993814142,
      "grad_norm": 2.7905707359313965,
      "learning_rate": 4.438202247191011e-05,
      "loss": 0.899,
      "step": 163
    },
    {
      "epoch": 0.02305637565021791,
      "grad_norm": 2.790402412414551,
      "learning_rate": 4.4662921348314605e-05,
      "loss": 1.0566,
      "step": 164
    },
    {
      "epoch": 0.023196963306621677,
      "grad_norm": 2.46990966796875,
      "learning_rate": 4.4943820224719104e-05,
      "loss": 1.3215,
      "step": 165
    },
    {
      "epoch": 0.023337550963025446,
      "grad_norm": 2.8140146732330322,
      "learning_rate": 4.52247191011236e-05,
      "loss": 1.1687,
      "step": 166
    },
    {
      "epoch": 0.023478138619429215,
      "grad_norm": 2.4004974365234375,
      "learning_rate": 4.550561797752809e-05,
      "loss": 1.072,
      "step": 167
    },
    {
      "epoch": 0.02361872627583298,
      "grad_norm": 2.1957356929779053,
      "learning_rate": 4.578651685393259e-05,
      "loss": 1.1038,
      "step": 168
    },
    {
      "epoch": 0.02375931393223675,
      "grad_norm": 2.382897138595581,
      "learning_rate": 4.606741573033708e-05,
      "loss": 1.1327,
      "step": 169
    },
    {
      "epoch": 0.023899901588640516,
      "grad_norm": 2.21454119682312,
      "learning_rate": 4.6348314606741575e-05,
      "loss": 1.188,
      "step": 170
    },
    {
      "epoch": 0.024040489245044285,
      "grad_norm": 2.0289082527160645,
      "learning_rate": 4.662921348314607e-05,
      "loss": 1.186,
      "step": 171
    },
    {
      "epoch": 0.024181076901448054,
      "grad_norm": 2.2515382766723633,
      "learning_rate": 4.691011235955056e-05,
      "loss": 1.0877,
      "step": 172
    },
    {
      "epoch": 0.02432166455785182,
      "grad_norm": 3.099454402923584,
      "learning_rate": 4.719101123595506e-05,
      "loss": 1.1245,
      "step": 173
    },
    {
      "epoch": 0.02446225221425559,
      "grad_norm": 2.608018636703491,
      "learning_rate": 4.747191011235955e-05,
      "loss": 1.0546,
      "step": 174
    },
    {
      "epoch": 0.024602839870659355,
      "grad_norm": 2.5215113162994385,
      "learning_rate": 4.7752808988764046e-05,
      "loss": 0.9787,
      "step": 175
    },
    {
      "epoch": 0.024743427527063124,
      "grad_norm": 2.3730759620666504,
      "learning_rate": 4.803370786516854e-05,
      "loss": 1.1932,
      "step": 176
    },
    {
      "epoch": 0.02488401518346689,
      "grad_norm": 3.0721728801727295,
      "learning_rate": 4.831460674157304e-05,
      "loss": 1.08,
      "step": 177
    },
    {
      "epoch": 0.02502460283987066,
      "grad_norm": 2.733149766921997,
      "learning_rate": 4.859550561797753e-05,
      "loss": 1.1693,
      "step": 178
    },
    {
      "epoch": 0.02516519049627443,
      "grad_norm": 2.5139222145080566,
      "learning_rate": 4.8876404494382024e-05,
      "loss": 1.0516,
      "step": 179
    },
    {
      "epoch": 0.025305778152678194,
      "grad_norm": 2.671976327896118,
      "learning_rate": 4.915730337078652e-05,
      "loss": 1.1726,
      "step": 180
    },
    {
      "epoch": 0.025446365809081963,
      "grad_norm": 2.4634792804718018,
      "learning_rate": 4.943820224719101e-05,
      "loss": 0.8458,
      "step": 181
    },
    {
      "epoch": 0.02558695346548573,
      "grad_norm": 2.570363998413086,
      "learning_rate": 4.971910112359551e-05,
      "loss": 1.2142,
      "step": 182
    },
    {
      "epoch": 0.025727541121889498,
      "grad_norm": 2.3932912349700928,
      "learning_rate": 5e-05,
      "loss": 1.1424,
      "step": 183
    },
    {
      "epoch": 0.025868128778293267,
      "grad_norm": 2.595952272415161,
      "learning_rate": 5.0280898876404495e-05,
      "loss": 1.1049,
      "step": 184
    },
    {
      "epoch": 0.026008716434697033,
      "grad_norm": 2.281751871109009,
      "learning_rate": 5.0561797752808995e-05,
      "loss": 1.2497,
      "step": 185
    },
    {
      "epoch": 0.026149304091100802,
      "grad_norm": 2.7008118629455566,
      "learning_rate": 5.084269662921348e-05,
      "loss": 1.1453,
      "step": 186
    },
    {
      "epoch": 0.026289891747504568,
      "grad_norm": 2.398432970046997,
      "learning_rate": 5.112359550561798e-05,
      "loss": 1.1492,
      "step": 187
    },
    {
      "epoch": 0.026430479403908337,
      "grad_norm": 2.0010013580322266,
      "learning_rate": 5.140449438202247e-05,
      "loss": 1.2415,
      "step": 188
    },
    {
      "epoch": 0.026571067060312106,
      "grad_norm": 2.4201300144195557,
      "learning_rate": 5.168539325842697e-05,
      "loss": 0.8927,
      "step": 189
    },
    {
      "epoch": 0.026711654716715872,
      "grad_norm": 2.545823812484741,
      "learning_rate": 5.1966292134831466e-05,
      "loss": 1.225,
      "step": 190
    },
    {
      "epoch": 0.02685224237311964,
      "grad_norm": 2.206869602203369,
      "learning_rate": 5.2247191011235965e-05,
      "loss": 1.2117,
      "step": 191
    },
    {
      "epoch": 0.026992830029523407,
      "grad_norm": 2.336686134338379,
      "learning_rate": 5.252808988764045e-05,
      "loss": 1.1213,
      "step": 192
    },
    {
      "epoch": 0.027133417685927176,
      "grad_norm": 2.0921790599823,
      "learning_rate": 5.2808988764044944e-05,
      "loss": 1.0929,
      "step": 193
    },
    {
      "epoch": 0.027274005342330942,
      "grad_norm": 2.2613587379455566,
      "learning_rate": 5.3089887640449444e-05,
      "loss": 1.1412,
      "step": 194
    },
    {
      "epoch": 0.02741459299873471,
      "grad_norm": 2.6331818103790283,
      "learning_rate": 5.337078651685393e-05,
      "loss": 1.0595,
      "step": 195
    },
    {
      "epoch": 0.02755518065513848,
      "grad_norm": 2.4552652835845947,
      "learning_rate": 5.365168539325843e-05,
      "loss": 0.9995,
      "step": 196
    },
    {
      "epoch": 0.027695768311542246,
      "grad_norm": 2.0642356872558594,
      "learning_rate": 5.393258426966292e-05,
      "loss": 1.221,
      "step": 197
    },
    {
      "epoch": 0.027836355967946015,
      "grad_norm": 2.213430881500244,
      "learning_rate": 5.421348314606742e-05,
      "loss": 1.3011,
      "step": 198
    },
    {
      "epoch": 0.02797694362434978,
      "grad_norm": 2.8076305389404297,
      "learning_rate": 5.4494382022471915e-05,
      "loss": 1.099,
      "step": 199
    },
    {
      "epoch": 0.02811753128075355,
      "grad_norm": 2.231855630874634,
      "learning_rate": 5.47752808988764e-05,
      "loss": 1.0115,
      "step": 200
    },
    {
      "epoch": 0.02825811893715732,
      "grad_norm": 2.500011682510376,
      "learning_rate": 5.50561797752809e-05,
      "loss": 1.1292,
      "step": 201
    },
    {
      "epoch": 0.028398706593561085,
      "grad_norm": 2.4029178619384766,
      "learning_rate": 5.533707865168539e-05,
      "loss": 1.22,
      "step": 202
    },
    {
      "epoch": 0.028539294249964854,
      "grad_norm": 1.6618330478668213,
      "learning_rate": 5.561797752808989e-05,
      "loss": 1.1847,
      "step": 203
    },
    {
      "epoch": 0.02867988190636862,
      "grad_norm": 2.378413438796997,
      "learning_rate": 5.5898876404494386e-05,
      "loss": 1.1032,
      "step": 204
    },
    {
      "epoch": 0.02882046956277239,
      "grad_norm": 2.0340278148651123,
      "learning_rate": 5.6179775280898885e-05,
      "loss": 1.1781,
      "step": 205
    },
    {
      "epoch": 0.028961057219176155,
      "grad_norm": 2.2804934978485107,
      "learning_rate": 5.646067415730337e-05,
      "loss": 1.1772,
      "step": 206
    },
    {
      "epoch": 0.029101644875579924,
      "grad_norm": 2.3874778747558594,
      "learning_rate": 5.674157303370787e-05,
      "loss": 1.0767,
      "step": 207
    },
    {
      "epoch": 0.029242232531983693,
      "grad_norm": 2.644440174102783,
      "learning_rate": 5.7022471910112364e-05,
      "loss": 1.1308,
      "step": 208
    },
    {
      "epoch": 0.02938282018838746,
      "grad_norm": 2.157053232192993,
      "learning_rate": 5.730337078651685e-05,
      "loss": 1.0378,
      "step": 209
    },
    {
      "epoch": 0.029523407844791228,
      "grad_norm": 3.0746865272521973,
      "learning_rate": 5.758426966292135e-05,
      "loss": 1.2433,
      "step": 210
    },
    {
      "epoch": 0.029663995501194994,
      "grad_norm": 2.237730026245117,
      "learning_rate": 5.786516853932584e-05,
      "loss": 1.0372,
      "step": 211
    },
    {
      "epoch": 0.029804583157598763,
      "grad_norm": 2.3634049892425537,
      "learning_rate": 5.814606741573034e-05,
      "loss": 1.0036,
      "step": 212
    },
    {
      "epoch": 0.029945170814002532,
      "grad_norm": 2.357161521911621,
      "learning_rate": 5.8426966292134835e-05,
      "loss": 1.2991,
      "step": 213
    },
    {
      "epoch": 0.030085758470406298,
      "grad_norm": 2.7408816814422607,
      "learning_rate": 5.8707865168539334e-05,
      "loss": 1.079,
      "step": 214
    },
    {
      "epoch": 0.030226346126810067,
      "grad_norm": 2.2548444271087646,
      "learning_rate": 5.898876404494382e-05,
      "loss": 1.2007,
      "step": 215
    },
    {
      "epoch": 0.030366933783213833,
      "grad_norm": 2.246663808822632,
      "learning_rate": 5.926966292134831e-05,
      "loss": 1.1618,
      "step": 216
    },
    {
      "epoch": 0.030507521439617602,
      "grad_norm": 2.304464340209961,
      "learning_rate": 5.955056179775281e-05,
      "loss": 1.2105,
      "step": 217
    },
    {
      "epoch": 0.030648109096021368,
      "grad_norm": 2.4901199340820312,
      "learning_rate": 5.9831460674157306e-05,
      "loss": 1.0181,
      "step": 218
    },
    {
      "epoch": 0.030788696752425137,
      "grad_norm": 1.9396816492080688,
      "learning_rate": 6.0112359550561805e-05,
      "loss": 1.1744,
      "step": 219
    },
    {
      "epoch": 0.030929284408828906,
      "grad_norm": 2.299004077911377,
      "learning_rate": 6.039325842696629e-05,
      "loss": 1.1852,
      "step": 220
    },
    {
      "epoch": 0.031069872065232672,
      "grad_norm": 2.522064208984375,
      "learning_rate": 6.067415730337079e-05,
      "loss": 1.0475,
      "step": 221
    },
    {
      "epoch": 0.03121045972163644,
      "grad_norm": 2.192898988723755,
      "learning_rate": 6.0955056179775284e-05,
      "loss": 1.0947,
      "step": 222
    },
    {
      "epoch": 0.03135104737804021,
      "grad_norm": 2.0952770709991455,
      "learning_rate": 6.123595505617978e-05,
      "loss": 1.0183,
      "step": 223
    },
    {
      "epoch": 0.03149163503444397,
      "grad_norm": 2.294480085372925,
      "learning_rate": 6.151685393258427e-05,
      "loss": 1.0435,
      "step": 224
    },
    {
      "epoch": 0.03163222269084774,
      "grad_norm": 2.631321668624878,
      "learning_rate": 6.179775280898876e-05,
      "loss": 1.0553,
      "step": 225
    },
    {
      "epoch": 0.03177281034725151,
      "grad_norm": 2.6891255378723145,
      "learning_rate": 6.207865168539327e-05,
      "loss": 1.1566,
      "step": 226
    },
    {
      "epoch": 0.03191339800365528,
      "grad_norm": 2.035349130630493,
      "learning_rate": 6.235955056179775e-05,
      "loss": 1.0791,
      "step": 227
    },
    {
      "epoch": 0.03205398566005905,
      "grad_norm": 2.2789969444274902,
      "learning_rate": 6.264044943820225e-05,
      "loss": 1.0835,
      "step": 228
    },
    {
      "epoch": 0.03219457331646281,
      "grad_norm": 2.1781015396118164,
      "learning_rate": 6.292134831460675e-05,
      "loss": 1.0812,
      "step": 229
    },
    {
      "epoch": 0.03233516097286658,
      "grad_norm": 2.4605274200439453,
      "learning_rate": 6.320224719101124e-05,
      "loss": 0.9776,
      "step": 230
    },
    {
      "epoch": 0.03247574862927035,
      "grad_norm": 2.287975788116455,
      "learning_rate": 6.348314606741573e-05,
      "loss": 1.183,
      "step": 231
    },
    {
      "epoch": 0.03261633628567412,
      "grad_norm": 2.30143141746521,
      "learning_rate": 6.376404494382023e-05,
      "loss": 1.1468,
      "step": 232
    },
    {
      "epoch": 0.03275692394207789,
      "grad_norm": 2.6938745975494385,
      "learning_rate": 6.404494382022472e-05,
      "loss": 1.1487,
      "step": 233
    },
    {
      "epoch": 0.03289751159848165,
      "grad_norm": 2.258444309234619,
      "learning_rate": 6.432584269662921e-05,
      "loss": 1.1181,
      "step": 234
    },
    {
      "epoch": 0.03303809925488542,
      "grad_norm": 2.083493947982788,
      "learning_rate": 6.460674157303372e-05,
      "loss": 1.1556,
      "step": 235
    },
    {
      "epoch": 0.03317868691128919,
      "grad_norm": 3.012044906616211,
      "learning_rate": 6.48876404494382e-05,
      "loss": 1.215,
      "step": 236
    },
    {
      "epoch": 0.03331927456769296,
      "grad_norm": 2.19004487991333,
      "learning_rate": 6.51685393258427e-05,
      "loss": 1.0915,
      "step": 237
    },
    {
      "epoch": 0.03345986222409673,
      "grad_norm": 2.1062936782836914,
      "learning_rate": 6.54494382022472e-05,
      "loss": 1.0245,
      "step": 238
    },
    {
      "epoch": 0.03360044988050049,
      "grad_norm": 2.043398380279541,
      "learning_rate": 6.573033707865169e-05,
      "loss": 1.1205,
      "step": 239
    },
    {
      "epoch": 0.03374103753690426,
      "grad_norm": 2.2317874431610107,
      "learning_rate": 6.601123595505618e-05,
      "loss": 1.0545,
      "step": 240
    },
    {
      "epoch": 0.03388162519330803,
      "grad_norm": 1.9159241914749146,
      "learning_rate": 6.629213483146067e-05,
      "loss": 1.1124,
      "step": 241
    },
    {
      "epoch": 0.0340222128497118,
      "grad_norm": 2.085078239440918,
      "learning_rate": 6.657303370786517e-05,
      "loss": 1.2404,
      "step": 242
    },
    {
      "epoch": 0.034162800506115566,
      "grad_norm": 2.2065136432647705,
      "learning_rate": 6.685393258426966e-05,
      "loss": 1.1184,
      "step": 243
    },
    {
      "epoch": 0.03430338816251933,
      "grad_norm": 2.4882514476776123,
      "learning_rate": 6.713483146067417e-05,
      "loss": 1.0858,
      "step": 244
    },
    {
      "epoch": 0.0344439758189231,
      "grad_norm": 2.479884386062622,
      "learning_rate": 6.741573033707866e-05,
      "loss": 1.1801,
      "step": 245
    },
    {
      "epoch": 0.03458456347532687,
      "grad_norm": 2.1303014755249023,
      "learning_rate": 6.769662921348315e-05,
      "loss": 1.016,
      "step": 246
    },
    {
      "epoch": 0.034725151131730636,
      "grad_norm": 2.3555989265441895,
      "learning_rate": 6.797752808988765e-05,
      "loss": 1.0965,
      "step": 247
    },
    {
      "epoch": 0.034865738788134405,
      "grad_norm": 2.18965220451355,
      "learning_rate": 6.825842696629214e-05,
      "loss": 1.1348,
      "step": 248
    },
    {
      "epoch": 0.03500632644453817,
      "grad_norm": 2.250523090362549,
      "learning_rate": 6.853932584269663e-05,
      "loss": 1.0747,
      "step": 249
    },
    {
      "epoch": 0.03514691410094194,
      "grad_norm": 2.521613597869873,
      "learning_rate": 6.882022471910112e-05,
      "loss": 1.1777,
      "step": 250
    },
    {
      "epoch": 0.035287501757345706,
      "grad_norm": 2.0158755779266357,
      "learning_rate": 6.910112359550562e-05,
      "loss": 1.0722,
      "step": 251
    },
    {
      "epoch": 0.035428089413749475,
      "grad_norm": 2.098524808883667,
      "learning_rate": 6.938202247191011e-05,
      "loss": 1.1995,
      "step": 252
    },
    {
      "epoch": 0.03556867707015324,
      "grad_norm": 2.0240628719329834,
      "learning_rate": 6.966292134831462e-05,
      "loss": 1.0318,
      "step": 253
    },
    {
      "epoch": 0.035709264726557007,
      "grad_norm": 2.0322062969207764,
      "learning_rate": 6.994382022471911e-05,
      "loss": 1.0994,
      "step": 254
    },
    {
      "epoch": 0.035849852382960776,
      "grad_norm": 2.2563600540161133,
      "learning_rate": 7.02247191011236e-05,
      "loss": 0.9107,
      "step": 255
    },
    {
      "epoch": 0.035990440039364545,
      "grad_norm": 2.184483766555786,
      "learning_rate": 7.05056179775281e-05,
      "loss": 1.118,
      "step": 256
    },
    {
      "epoch": 0.036131027695768314,
      "grad_norm": 2.503980875015259,
      "learning_rate": 7.078651685393259e-05,
      "loss": 1.0554,
      "step": 257
    },
    {
      "epoch": 0.036271615352172076,
      "grad_norm": 2.0762598514556885,
      "learning_rate": 7.106741573033708e-05,
      "loss": 1.2578,
      "step": 258
    },
    {
      "epoch": 0.036412203008575846,
      "grad_norm": 2.0117056369781494,
      "learning_rate": 7.134831460674157e-05,
      "loss": 1.172,
      "step": 259
    },
    {
      "epoch": 0.036552790664979615,
      "grad_norm": 2.1322569847106934,
      "learning_rate": 7.162921348314608e-05,
      "loss": 1.197,
      "step": 260
    },
    {
      "epoch": 0.036693378321383384,
      "grad_norm": 2.1132771968841553,
      "learning_rate": 7.191011235955056e-05,
      "loss": 1.3144,
      "step": 261
    },
    {
      "epoch": 0.03683396597778715,
      "grad_norm": 2.0350265502929688,
      "learning_rate": 7.219101123595507e-05,
      "loss": 1.1994,
      "step": 262
    },
    {
      "epoch": 0.036974553634190915,
      "grad_norm": 2.016650915145874,
      "learning_rate": 7.247191011235956e-05,
      "loss": 1.116,
      "step": 263
    },
    {
      "epoch": 0.037115141290594685,
      "grad_norm": 1.9997117519378662,
      "learning_rate": 7.275280898876404e-05,
      "loss": 1.2052,
      "step": 264
    },
    {
      "epoch": 0.037255728946998454,
      "grad_norm": 2.043107509613037,
      "learning_rate": 7.303370786516854e-05,
      "loss": 0.9719,
      "step": 265
    },
    {
      "epoch": 0.03739631660340222,
      "grad_norm": 1.9556926488876343,
      "learning_rate": 7.331460674157304e-05,
      "loss": 1.098,
      "step": 266
    },
    {
      "epoch": 0.03753690425980599,
      "grad_norm": 2.0700576305389404,
      "learning_rate": 7.359550561797753e-05,
      "loss": 1.0785,
      "step": 267
    },
    {
      "epoch": 0.037677491916209754,
      "grad_norm": 2.1003987789154053,
      "learning_rate": 7.387640449438202e-05,
      "loss": 1.1215,
      "step": 268
    },
    {
      "epoch": 0.037818079572613524,
      "grad_norm": 2.437917709350586,
      "learning_rate": 7.415730337078653e-05,
      "loss": 1.0728,
      "step": 269
    },
    {
      "epoch": 0.03795866722901729,
      "grad_norm": 2.0074656009674072,
      "learning_rate": 7.443820224719101e-05,
      "loss": 1.1405,
      "step": 270
    },
    {
      "epoch": 0.03809925488542106,
      "grad_norm": 2.084153652191162,
      "learning_rate": 7.471910112359551e-05,
      "loss": 1.1492,
      "step": 271
    },
    {
      "epoch": 0.03823984254182483,
      "grad_norm": 2.122835159301758,
      "learning_rate": 7.500000000000001e-05,
      "loss": 0.9276,
      "step": 272
    },
    {
      "epoch": 0.03838043019822859,
      "grad_norm": 2.180314064025879,
      "learning_rate": 7.52808988764045e-05,
      "loss": 1.1885,
      "step": 273
    },
    {
      "epoch": 0.03852101785463236,
      "grad_norm": 2.369109630584717,
      "learning_rate": 7.556179775280899e-05,
      "loss": 1.2765,
      "step": 274
    },
    {
      "epoch": 0.03866160551103613,
      "grad_norm": 2.5607521533966064,
      "learning_rate": 7.584269662921349e-05,
      "loss": 1.1109,
      "step": 275
    },
    {
      "epoch": 0.0388021931674399,
      "grad_norm": 1.9126816987991333,
      "learning_rate": 7.612359550561798e-05,
      "loss": 1.2163,
      "step": 276
    },
    {
      "epoch": 0.03894278082384366,
      "grad_norm": 2.0749309062957764,
      "learning_rate": 7.640449438202247e-05,
      "loss": 1.1678,
      "step": 277
    },
    {
      "epoch": 0.03908336848024743,
      "grad_norm": 1.990980863571167,
      "learning_rate": 7.668539325842698e-05,
      "loss": 1.1352,
      "step": 278
    },
    {
      "epoch": 0.0392239561366512,
      "grad_norm": 1.94240140914917,
      "learning_rate": 7.696629213483147e-05,
      "loss": 0.9731,
      "step": 279
    },
    {
      "epoch": 0.03936454379305497,
      "grad_norm": 1.923457384109497,
      "learning_rate": 7.724719101123596e-05,
      "loss": 1.1118,
      "step": 280
    },
    {
      "epoch": 0.03950513144945874,
      "grad_norm": 2.148200035095215,
      "learning_rate": 7.752808988764046e-05,
      "loss": 1.0145,
      "step": 281
    },
    {
      "epoch": 0.0396457191058625,
      "grad_norm": 2.017226219177246,
      "learning_rate": 7.780898876404495e-05,
      "loss": 1.059,
      "step": 282
    },
    {
      "epoch": 0.03978630676226627,
      "grad_norm": 1.9769501686096191,
      "learning_rate": 7.808988764044944e-05,
      "loss": 1.103,
      "step": 283
    },
    {
      "epoch": 0.03992689441867004,
      "grad_norm": 1.8001704216003418,
      "learning_rate": 7.837078651685393e-05,
      "loss": 1.0417,
      "step": 284
    },
    {
      "epoch": 0.04006748207507381,
      "grad_norm": 1.9419679641723633,
      "learning_rate": 7.865168539325843e-05,
      "loss": 1.198,
      "step": 285
    },
    {
      "epoch": 0.04020806973147758,
      "grad_norm": 1.985488772392273,
      "learning_rate": 7.893258426966292e-05,
      "loss": 1.2187,
      "step": 286
    },
    {
      "epoch": 0.04034865738788134,
      "grad_norm": 2.0303990840911865,
      "learning_rate": 7.921348314606743e-05,
      "loss": 1.2952,
      "step": 287
    },
    {
      "epoch": 0.04048924504428511,
      "grad_norm": 2.178532123565674,
      "learning_rate": 7.949438202247192e-05,
      "loss": 1.1285,
      "step": 288
    },
    {
      "epoch": 0.04062983270068888,
      "grad_norm": 1.9010953903198242,
      "learning_rate": 7.97752808988764e-05,
      "loss": 1.3672,
      "step": 289
    },
    {
      "epoch": 0.04077042035709265,
      "grad_norm": 2.2792856693267822,
      "learning_rate": 8.00561797752809e-05,
      "loss": 1.1916,
      "step": 290
    },
    {
      "epoch": 0.04091100801349642,
      "grad_norm": 2.3124701976776123,
      "learning_rate": 8.03370786516854e-05,
      "loss": 1.071,
      "step": 291
    },
    {
      "epoch": 0.04105159566990018,
      "grad_norm": 2.770250082015991,
      "learning_rate": 8.061797752808989e-05,
      "loss": 1.112,
      "step": 292
    },
    {
      "epoch": 0.04119218332630395,
      "grad_norm": 2.3447165489196777,
      "learning_rate": 8.089887640449438e-05,
      "loss": 1.2814,
      "step": 293
    },
    {
      "epoch": 0.04133277098270772,
      "grad_norm": 2.8198485374450684,
      "learning_rate": 8.117977528089889e-05,
      "loss": 1.0931,
      "step": 294
    },
    {
      "epoch": 0.04147335863911149,
      "grad_norm": 2.048471689224243,
      "learning_rate": 8.146067415730337e-05,
      "loss": 1.2159,
      "step": 295
    },
    {
      "epoch": 0.04161394629551526,
      "grad_norm": 1.9437874555587769,
      "learning_rate": 8.174157303370788e-05,
      "loss": 1.2323,
      "step": 296
    },
    {
      "epoch": 0.04175453395191902,
      "grad_norm": 2.513530731201172,
      "learning_rate": 8.202247191011237e-05,
      "loss": 1.2376,
      "step": 297
    },
    {
      "epoch": 0.04189512160832279,
      "grad_norm": 1.920276403427124,
      "learning_rate": 8.230337078651685e-05,
      "loss": 1.3009,
      "step": 298
    },
    {
      "epoch": 0.04203570926472656,
      "grad_norm": 2.0422301292419434,
      "learning_rate": 8.258426966292135e-05,
      "loss": 1.0507,
      "step": 299
    },
    {
      "epoch": 0.04217629692113033,
      "grad_norm": 1.961079716682434,
      "learning_rate": 8.286516853932585e-05,
      "loss": 1.1055,
      "step": 300
    },
    {
      "epoch": 0.04231688457753409,
      "grad_norm": 2.009704351425171,
      "learning_rate": 8.314606741573034e-05,
      "loss": 1.0891,
      "step": 301
    },
    {
      "epoch": 0.04245747223393786,
      "grad_norm": 1.8657206296920776,
      "learning_rate": 8.342696629213483e-05,
      "loss": 1.0174,
      "step": 302
    },
    {
      "epoch": 0.04259805989034163,
      "grad_norm": 1.7488481998443604,
      "learning_rate": 8.370786516853934e-05,
      "loss": 1.2348,
      "step": 303
    },
    {
      "epoch": 0.0427386475467454,
      "grad_norm": 2.3507354259490967,
      "learning_rate": 8.398876404494382e-05,
      "loss": 1.0175,
      "step": 304
    },
    {
      "epoch": 0.042879235203149166,
      "grad_norm": 2.4048309326171875,
      "learning_rate": 8.426966292134831e-05,
      "loss": 1.1215,
      "step": 305
    },
    {
      "epoch": 0.04301982285955293,
      "grad_norm": 2.105654239654541,
      "learning_rate": 8.455056179775282e-05,
      "loss": 1.168,
      "step": 306
    },
    {
      "epoch": 0.0431604105159567,
      "grad_norm": 2.3572912216186523,
      "learning_rate": 8.483146067415731e-05,
      "loss": 1.0993,
      "step": 307
    },
    {
      "epoch": 0.043300998172360466,
      "grad_norm": 1.7615408897399902,
      "learning_rate": 8.51123595505618e-05,
      "loss": 1.1127,
      "step": 308
    },
    {
      "epoch": 0.043441585828764236,
      "grad_norm": 2.054922580718994,
      "learning_rate": 8.53932584269663e-05,
      "loss": 1.1241,
      "step": 309
    },
    {
      "epoch": 0.043582173485168005,
      "grad_norm": 2.2332963943481445,
      "learning_rate": 8.567415730337079e-05,
      "loss": 1.1968,
      "step": 310
    },
    {
      "epoch": 0.04372276114157177,
      "grad_norm": 2.0861120223999023,
      "learning_rate": 8.595505617977528e-05,
      "loss": 1.093,
      "step": 311
    },
    {
      "epoch": 0.043863348797975536,
      "grad_norm": 1.9801466464996338,
      "learning_rate": 8.623595505617979e-05,
      "loss": 1.0117,
      "step": 312
    },
    {
      "epoch": 0.044003936454379305,
      "grad_norm": 2.4185404777526855,
      "learning_rate": 8.651685393258427e-05,
      "loss": 1.0047,
      "step": 313
    },
    {
      "epoch": 0.044144524110783075,
      "grad_norm": 2.201678991317749,
      "learning_rate": 8.679775280898876e-05,
      "loss": 1.0933,
      "step": 314
    },
    {
      "epoch": 0.044285111767186844,
      "grad_norm": 2.145491361618042,
      "learning_rate": 8.707865168539327e-05,
      "loss": 1.2162,
      "step": 315
    },
    {
      "epoch": 0.044425699423590606,
      "grad_norm": 2.2059593200683594,
      "learning_rate": 8.735955056179776e-05,
      "loss": 1.1908,
      "step": 316
    },
    {
      "epoch": 0.044566287079994375,
      "grad_norm": 2.0032339096069336,
      "learning_rate": 8.764044943820225e-05,
      "loss": 1.1617,
      "step": 317
    },
    {
      "epoch": 0.044706874736398144,
      "grad_norm": 2.3210153579711914,
      "learning_rate": 8.792134831460675e-05,
      "loss": 1.0734,
      "step": 318
    },
    {
      "epoch": 0.044847462392801914,
      "grad_norm": 2.0029022693634033,
      "learning_rate": 8.820224719101124e-05,
      "loss": 1.1205,
      "step": 319
    },
    {
      "epoch": 0.04498805004920568,
      "grad_norm": 1.9865777492523193,
      "learning_rate": 8.848314606741573e-05,
      "loss": 1.1574,
      "step": 320
    },
    {
      "epoch": 0.045128637705609445,
      "grad_norm": 2.024789810180664,
      "learning_rate": 8.876404494382022e-05,
      "loss": 1.1571,
      "step": 321
    },
    {
      "epoch": 0.045269225362013214,
      "grad_norm": 2.3479764461517334,
      "learning_rate": 8.904494382022473e-05,
      "loss": 1.2468,
      "step": 322
    },
    {
      "epoch": 0.045409813018416983,
      "grad_norm": 2.052243709564209,
      "learning_rate": 8.932584269662921e-05,
      "loss": 1.1533,
      "step": 323
    },
    {
      "epoch": 0.04555040067482075,
      "grad_norm": 1.8498810529708862,
      "learning_rate": 8.960674157303372e-05,
      "loss": 1.1715,
      "step": 324
    },
    {
      "epoch": 0.04569098833122452,
      "grad_norm": 2.0218467712402344,
      "learning_rate": 8.988764044943821e-05,
      "loss": 1.1039,
      "step": 325
    },
    {
      "epoch": 0.045831575987628284,
      "grad_norm": 2.101044178009033,
      "learning_rate": 9.01685393258427e-05,
      "loss": 1.1501,
      "step": 326
    },
    {
      "epoch": 0.04597216364403205,
      "grad_norm": 2.143216609954834,
      "learning_rate": 9.04494382022472e-05,
      "loss": 1.1552,
      "step": 327
    },
    {
      "epoch": 0.04611275130043582,
      "grad_norm": 1.831234335899353,
      "learning_rate": 9.07303370786517e-05,
      "loss": 1.099,
      "step": 328
    },
    {
      "epoch": 0.04625333895683959,
      "grad_norm": 1.945765495300293,
      "learning_rate": 9.101123595505618e-05,
      "loss": 1.0431,
      "step": 329
    },
    {
      "epoch": 0.046393926613243354,
      "grad_norm": 2.026486873626709,
      "learning_rate": 9.129213483146067e-05,
      "loss": 1.1586,
      "step": 330
    },
    {
      "epoch": 0.04653451426964712,
      "grad_norm": 1.7436408996582031,
      "learning_rate": 9.157303370786518e-05,
      "loss": 1.1859,
      "step": 331
    },
    {
      "epoch": 0.04667510192605089,
      "grad_norm": 2.026146411895752,
      "learning_rate": 9.185393258426966e-05,
      "loss": 1.0987,
      "step": 332
    },
    {
      "epoch": 0.04681568958245466,
      "grad_norm": 1.8193981647491455,
      "learning_rate": 9.213483146067416e-05,
      "loss": 0.8461,
      "step": 333
    },
    {
      "epoch": 0.04695627723885843,
      "grad_norm": 1.8346997499465942,
      "learning_rate": 9.241573033707866e-05,
      "loss": 1.1395,
      "step": 334
    },
    {
      "epoch": 0.04709686489526219,
      "grad_norm": 2.025394916534424,
      "learning_rate": 9.269662921348315e-05,
      "loss": 1.1004,
      "step": 335
    },
    {
      "epoch": 0.04723745255166596,
      "grad_norm": 1.9196642637252808,
      "learning_rate": 9.297752808988764e-05,
      "loss": 1.2217,
      "step": 336
    },
    {
      "epoch": 0.04737804020806973,
      "grad_norm": 2.174111843109131,
      "learning_rate": 9.325842696629214e-05,
      "loss": 0.9076,
      "step": 337
    },
    {
      "epoch": 0.0475186278644735,
      "grad_norm": 1.9009904861450195,
      "learning_rate": 9.353932584269663e-05,
      "loss": 1.0436,
      "step": 338
    },
    {
      "epoch": 0.04765921552087727,
      "grad_norm": 1.8688868284225464,
      "learning_rate": 9.382022471910112e-05,
      "loss": 0.9755,
      "step": 339
    },
    {
      "epoch": 0.04779980317728103,
      "grad_norm": 1.7450469732284546,
      "learning_rate": 9.410112359550563e-05,
      "loss": 1.2946,
      "step": 340
    },
    {
      "epoch": 0.0479403908336848,
      "grad_norm": 1.7886625528335571,
      "learning_rate": 9.438202247191012e-05,
      "loss": 1.0457,
      "step": 341
    },
    {
      "epoch": 0.04808097849008857,
      "grad_norm": 1.8535023927688599,
      "learning_rate": 9.466292134831461e-05,
      "loss": 1.1785,
      "step": 342
    },
    {
      "epoch": 0.04822156614649234,
      "grad_norm": 1.8058137893676758,
      "learning_rate": 9.49438202247191e-05,
      "loss": 1.0187,
      "step": 343
    },
    {
      "epoch": 0.04836215380289611,
      "grad_norm": 2.078225612640381,
      "learning_rate": 9.52247191011236e-05,
      "loss": 1.1926,
      "step": 344
    },
    {
      "epoch": 0.04850274145929987,
      "grad_norm": 1.9953665733337402,
      "learning_rate": 9.550561797752809e-05,
      "loss": 0.9939,
      "step": 345
    },
    {
      "epoch": 0.04864332911570364,
      "grad_norm": 1.8920668363571167,
      "learning_rate": 9.578651685393259e-05,
      "loss": 1.1864,
      "step": 346
    },
    {
      "epoch": 0.04878391677210741,
      "grad_norm": 2.0446574687957764,
      "learning_rate": 9.606741573033708e-05,
      "loss": 1.1335,
      "step": 347
    },
    {
      "epoch": 0.04892450442851118,
      "grad_norm": 2.398526906967163,
      "learning_rate": 9.634831460674157e-05,
      "loss": 1.0252,
      "step": 348
    },
    {
      "epoch": 0.04906509208491495,
      "grad_norm": 1.8374841213226318,
      "learning_rate": 9.662921348314608e-05,
      "loss": 1.0459,
      "step": 349
    },
    {
      "epoch": 0.04920567974131871,
      "grad_norm": 1.9044111967086792,
      "learning_rate": 9.691011235955057e-05,
      "loss": 1.0905,
      "step": 350
    },
    {
      "epoch": 0.04934626739772248,
      "grad_norm": 1.8970874547958374,
      "learning_rate": 9.719101123595506e-05,
      "loss": 1.2355,
      "step": 351
    },
    {
      "epoch": 0.04948685505412625,
      "grad_norm": 1.911007046699524,
      "learning_rate": 9.747191011235956e-05,
      "loss": 1.2255,
      "step": 352
    },
    {
      "epoch": 0.04962744271053002,
      "grad_norm": 1.7863138914108276,
      "learning_rate": 9.775280898876405e-05,
      "loss": 1.0012,
      "step": 353
    },
    {
      "epoch": 0.04976803036693378,
      "grad_norm": 1.8198041915893555,
      "learning_rate": 9.803370786516854e-05,
      "loss": 1.1793,
      "step": 354
    },
    {
      "epoch": 0.04990861802333755,
      "grad_norm": 1.8485674858093262,
      "learning_rate": 9.831460674157303e-05,
      "loss": 1.1655,
      "step": 355
    },
    {
      "epoch": 0.05004920567974132,
      "grad_norm": 1.8849211931228638,
      "learning_rate": 9.859550561797754e-05,
      "loss": 1.1498,
      "step": 356
    },
    {
      "epoch": 0.05018979333614509,
      "grad_norm": 1.7031837701797485,
      "learning_rate": 9.887640449438202e-05,
      "loss": 1.2748,
      "step": 357
    },
    {
      "epoch": 0.05033038099254886,
      "grad_norm": 1.8861690759658813,
      "learning_rate": 9.915730337078653e-05,
      "loss": 1.2024,
      "step": 358
    },
    {
      "epoch": 0.05047096864895262,
      "grad_norm": 1.842575192451477,
      "learning_rate": 9.943820224719102e-05,
      "loss": 1.1389,
      "step": 359
    },
    {
      "epoch": 0.05061155630535639,
      "grad_norm": 1.7442415952682495,
      "learning_rate": 9.971910112359551e-05,
      "loss": 1.0326,
      "step": 360
    },
    {
      "epoch": 0.05075214396176016,
      "grad_norm": 1.705456256866455,
      "learning_rate": 0.0001,
      "loss": 1.1371,
      "step": 361
    },
    {
      "epoch": 0.050892731618163926,
      "grad_norm": 1.7730551958084106,
      "learning_rate": 0.00010028089887640451,
      "loss": 1.0203,
      "step": 362
    },
    {
      "epoch": 0.051033319274567696,
      "grad_norm": 1.8496006727218628,
      "learning_rate": 0.00010056179775280899,
      "loss": 0.9351,
      "step": 363
    },
    {
      "epoch": 0.05117390693097146,
      "grad_norm": 1.6997305154800415,
      "learning_rate": 0.00010084269662921348,
      "loss": 1.2247,
      "step": 364
    },
    {
      "epoch": 0.05131449458737523,
      "grad_norm": 1.8198490142822266,
      "learning_rate": 0.00010112359550561799,
      "loss": 1.1204,
      "step": 365
    },
    {
      "epoch": 0.051455082243778996,
      "grad_norm": 1.684792399406433,
      "learning_rate": 0.00010140449438202248,
      "loss": 1.1637,
      "step": 366
    },
    {
      "epoch": 0.051595669900182765,
      "grad_norm": 2.0518200397491455,
      "learning_rate": 0.00010168539325842696,
      "loss": 1.1182,
      "step": 367
    },
    {
      "epoch": 0.051736257556586535,
      "grad_norm": 1.8118394613265991,
      "learning_rate": 0.00010196629213483147,
      "loss": 1.0646,
      "step": 368
    },
    {
      "epoch": 0.0518768452129903,
      "grad_norm": 1.8747377395629883,
      "learning_rate": 0.00010224719101123596,
      "loss": 1.0507,
      "step": 369
    },
    {
      "epoch": 0.052017432869394066,
      "grad_norm": 1.9094622135162354,
      "learning_rate": 0.00010252808988764044,
      "loss": 0.9639,
      "step": 370
    },
    {
      "epoch": 0.052158020525797835,
      "grad_norm": 1.9474825859069824,
      "learning_rate": 0.00010280898876404495,
      "loss": 1.2047,
      "step": 371
    },
    {
      "epoch": 0.052298608182201604,
      "grad_norm": 2.3158607482910156,
      "learning_rate": 0.00010308988764044944,
      "loss": 1.191,
      "step": 372
    },
    {
      "epoch": 0.052439195838605374,
      "grad_norm": 2.24672269821167,
      "learning_rate": 0.00010337078651685395,
      "loss": 1.1924,
      "step": 373
    },
    {
      "epoch": 0.052579783495009136,
      "grad_norm": 1.8184025287628174,
      "learning_rate": 0.00010365168539325843,
      "loss": 1.0638,
      "step": 374
    },
    {
      "epoch": 0.052720371151412905,
      "grad_norm": 1.8283076286315918,
      "learning_rate": 0.00010393258426966293,
      "loss": 1.0986,
      "step": 375
    },
    {
      "epoch": 0.052860958807816674,
      "grad_norm": 1.9615730047225952,
      "learning_rate": 0.00010421348314606742,
      "loss": 1.082,
      "step": 376
    },
    {
      "epoch": 0.05300154646422044,
      "grad_norm": 2.1095666885375977,
      "learning_rate": 0.00010449438202247193,
      "loss": 1.0846,
      "step": 377
    },
    {
      "epoch": 0.05314213412062421,
      "grad_norm": 1.998531699180603,
      "learning_rate": 0.00010477528089887641,
      "loss": 1.0905,
      "step": 378
    },
    {
      "epoch": 0.053282721777027975,
      "grad_norm": 1.856663465499878,
      "learning_rate": 0.0001050561797752809,
      "loss": 1.0076,
      "step": 379
    },
    {
      "epoch": 0.053423309433431744,
      "grad_norm": 2.3263988494873047,
      "learning_rate": 0.00010533707865168541,
      "loss": 1.2179,
      "step": 380
    },
    {
      "epoch": 0.05356389708983551,
      "grad_norm": 2.1896450519561768,
      "learning_rate": 0.00010561797752808989,
      "loss": 0.9622,
      "step": 381
    },
    {
      "epoch": 0.05370448474623928,
      "grad_norm": 2.013543128967285,
      "learning_rate": 0.00010589887640449438,
      "loss": 1.0705,
      "step": 382
    },
    {
      "epoch": 0.053845072402643045,
      "grad_norm": 1.7349814176559448,
      "learning_rate": 0.00010617977528089889,
      "loss": 1.2628,
      "step": 383
    },
    {
      "epoch": 0.053985660059046814,
      "grad_norm": 2.307243585586548,
      "learning_rate": 0.00010646067415730338,
      "loss": 1.1435,
      "step": 384
    },
    {
      "epoch": 0.05412624771545058,
      "grad_norm": 1.7858071327209473,
      "learning_rate": 0.00010674157303370786,
      "loss": 0.9407,
      "step": 385
    },
    {
      "epoch": 0.05426683537185435,
      "grad_norm": 1.7288439273834229,
      "learning_rate": 0.00010702247191011237,
      "loss": 1.0302,
      "step": 386
    },
    {
      "epoch": 0.05440742302825812,
      "grad_norm": 2.2780795097351074,
      "learning_rate": 0.00010730337078651686,
      "loss": 0.9335,
      "step": 387
    },
    {
      "epoch": 0.054548010684661884,
      "grad_norm": 1.8261387348175049,
      "learning_rate": 0.00010758426966292134,
      "loss": 1.0925,
      "step": 388
    },
    {
      "epoch": 0.05468859834106565,
      "grad_norm": 1.704158067703247,
      "learning_rate": 0.00010786516853932584,
      "loss": 1.1602,
      "step": 389
    },
    {
      "epoch": 0.05482918599746942,
      "grad_norm": 2.0686705112457275,
      "learning_rate": 0.00010814606741573035,
      "loss": 1.0942,
      "step": 390
    },
    {
      "epoch": 0.05496977365387319,
      "grad_norm": 1.8318482637405396,
      "learning_rate": 0.00010842696629213484,
      "loss": 1.1306,
      "step": 391
    },
    {
      "epoch": 0.05511036131027696,
      "grad_norm": 2.0179178714752197,
      "learning_rate": 0.00010870786516853932,
      "loss": 0.9653,
      "step": 392
    },
    {
      "epoch": 0.05525094896668072,
      "grad_norm": 1.7563968896865845,
      "learning_rate": 0.00010898876404494383,
      "loss": 1.2019,
      "step": 393
    },
    {
      "epoch": 0.05539153662308449,
      "grad_norm": 2.065092086791992,
      "learning_rate": 0.00010926966292134832,
      "loss": 1.1277,
      "step": 394
    },
    {
      "epoch": 0.05553212427948826,
      "grad_norm": 1.8390979766845703,
      "learning_rate": 0.0001095505617977528,
      "loss": 1.237,
      "step": 395
    },
    {
      "epoch": 0.05567271193589203,
      "grad_norm": 1.858350396156311,
      "learning_rate": 0.00010983146067415731,
      "loss": 1.161,
      "step": 396
    },
    {
      "epoch": 0.0558132995922958,
      "grad_norm": 1.7543202638626099,
      "learning_rate": 0.0001101123595505618,
      "loss": 0.9903,
      "step": 397
    },
    {
      "epoch": 0.05595388724869956,
      "grad_norm": 1.5905228853225708,
      "learning_rate": 0.00011039325842696631,
      "loss": 1.1832,
      "step": 398
    },
    {
      "epoch": 0.05609447490510333,
      "grad_norm": 1.792826533317566,
      "learning_rate": 0.00011067415730337079,
      "loss": 1.0221,
      "step": 399
    },
    {
      "epoch": 0.0562350625615071,
      "grad_norm": 1.9710779190063477,
      "learning_rate": 0.00011095505617977528,
      "loss": 1.0721,
      "step": 400
    },
    {
      "epoch": 0.05637565021791087,
      "grad_norm": 1.7682522535324097,
      "learning_rate": 0.00011123595505617979,
      "loss": 1.0015,
      "step": 401
    },
    {
      "epoch": 0.05651623787431464,
      "grad_norm": 1.6694178581237793,
      "learning_rate": 0.00011151685393258427,
      "loss": 1.223,
      "step": 402
    },
    {
      "epoch": 0.0566568255307184,
      "grad_norm": 1.5102684497833252,
      "learning_rate": 0.00011179775280898877,
      "loss": 1.2068,
      "step": 403
    },
    {
      "epoch": 0.05679741318712217,
      "grad_norm": 2.240840435028076,
      "learning_rate": 0.00011207865168539326,
      "loss": 1.0236,
      "step": 404
    },
    {
      "epoch": 0.05693800084352594,
      "grad_norm": 1.8824405670166016,
      "learning_rate": 0.00011235955056179777,
      "loss": 1.2227,
      "step": 405
    },
    {
      "epoch": 0.05707858849992971,
      "grad_norm": 1.743129014968872,
      "learning_rate": 0.00011264044943820225,
      "loss": 1.1667,
      "step": 406
    },
    {
      "epoch": 0.05721917615633347,
      "grad_norm": 1.730711817741394,
      "learning_rate": 0.00011292134831460674,
      "loss": 1.1722,
      "step": 407
    },
    {
      "epoch": 0.05735976381273724,
      "grad_norm": 1.920936107635498,
      "learning_rate": 0.00011320224719101125,
      "loss": 1.3821,
      "step": 408
    },
    {
      "epoch": 0.05750035146914101,
      "grad_norm": 2.180739641189575,
      "learning_rate": 0.00011348314606741574,
      "loss": 1.0573,
      "step": 409
    },
    {
      "epoch": 0.05764093912554478,
      "grad_norm": 2.103883981704712,
      "learning_rate": 0.00011376404494382022,
      "loss": 0.9449,
      "step": 410
    },
    {
      "epoch": 0.05778152678194855,
      "grad_norm": 1.7258987426757812,
      "learning_rate": 0.00011404494382022473,
      "loss": 1.2481,
      "step": 411
    },
    {
      "epoch": 0.05792211443835231,
      "grad_norm": 1.70970618724823,
      "learning_rate": 0.00011432584269662922,
      "loss": 1.0839,
      "step": 412
    },
    {
      "epoch": 0.05806270209475608,
      "grad_norm": 1.6732537746429443,
      "learning_rate": 0.0001146067415730337,
      "loss": 1.0663,
      "step": 413
    },
    {
      "epoch": 0.05820328975115985,
      "grad_norm": 2.24345326423645,
      "learning_rate": 0.0001148876404494382,
      "loss": 1.1719,
      "step": 414
    },
    {
      "epoch": 0.05834387740756362,
      "grad_norm": 2.0123746395111084,
      "learning_rate": 0.0001151685393258427,
      "loss": 1.0762,
      "step": 415
    },
    {
      "epoch": 0.058484465063967386,
      "grad_norm": 1.678879737854004,
      "learning_rate": 0.0001154494382022472,
      "loss": 1.1174,
      "step": 416
    },
    {
      "epoch": 0.05862505272037115,
      "grad_norm": 1.9418655633926392,
      "learning_rate": 0.00011573033707865168,
      "loss": 1.0919,
      "step": 417
    },
    {
      "epoch": 0.05876564037677492,
      "grad_norm": 1.7571818828582764,
      "learning_rate": 0.00011601123595505619,
      "loss": 1.0777,
      "step": 418
    },
    {
      "epoch": 0.05890622803317869,
      "grad_norm": 1.7855383157730103,
      "learning_rate": 0.00011629213483146068,
      "loss": 0.943,
      "step": 419
    },
    {
      "epoch": 0.059046815689582456,
      "grad_norm": 1.8255548477172852,
      "learning_rate": 0.00011657303370786516,
      "loss": 1.0377,
      "step": 420
    },
    {
      "epoch": 0.059187403345986225,
      "grad_norm": 1.7663806676864624,
      "learning_rate": 0.00011685393258426967,
      "loss": 0.9912,
      "step": 421
    },
    {
      "epoch": 0.05932799100238999,
      "grad_norm": 1.754003882408142,
      "learning_rate": 0.00011713483146067416,
      "loss": 1.1618,
      "step": 422
    },
    {
      "epoch": 0.05946857865879376,
      "grad_norm": 1.8772457838058472,
      "learning_rate": 0.00011741573033707867,
      "loss": 0.9797,
      "step": 423
    },
    {
      "epoch": 0.059609166315197526,
      "grad_norm": 2.236052989959717,
      "learning_rate": 0.00011769662921348315,
      "loss": 0.9297,
      "step": 424
    },
    {
      "epoch": 0.059749753971601295,
      "grad_norm": 1.421188473701477,
      "learning_rate": 0.00011797752808988764,
      "loss": 1.1765,
      "step": 425
    },
    {
      "epoch": 0.059890341628005064,
      "grad_norm": 2.3465042114257812,
      "learning_rate": 0.00011825842696629215,
      "loss": 1.0487,
      "step": 426
    },
    {
      "epoch": 0.06003092928440883,
      "grad_norm": 1.721390962600708,
      "learning_rate": 0.00011853932584269663,
      "loss": 1.0607,
      "step": 427
    },
    {
      "epoch": 0.060171516940812596,
      "grad_norm": 1.7980175018310547,
      "learning_rate": 0.00011882022471910112,
      "loss": 1.1593,
      "step": 428
    },
    {
      "epoch": 0.060312104597216365,
      "grad_norm": 2.1639087200164795,
      "learning_rate": 0.00011910112359550563,
      "loss": 0.9912,
      "step": 429
    },
    {
      "epoch": 0.060452692253620134,
      "grad_norm": 1.771360158920288,
      "learning_rate": 0.00011938202247191013,
      "loss": 1.1965,
      "step": 430
    },
    {
      "epoch": 0.0605932799100239,
      "grad_norm": 1.9211609363555908,
      "learning_rate": 0.00011966292134831461,
      "loss": 1.0698,
      "step": 431
    },
    {
      "epoch": 0.060733867566427666,
      "grad_norm": 2.00108003616333,
      "learning_rate": 0.0001199438202247191,
      "loss": 0.9576,
      "step": 432
    },
    {
      "epoch": 0.060874455222831435,
      "grad_norm": 1.5641167163848877,
      "learning_rate": 0.00012022471910112361,
      "loss": 1.0633,
      "step": 433
    },
    {
      "epoch": 0.061015042879235204,
      "grad_norm": 1.8282947540283203,
      "learning_rate": 0.00012050561797752809,
      "loss": 1.0306,
      "step": 434
    },
    {
      "epoch": 0.06115563053563897,
      "grad_norm": 1.892506718635559,
      "learning_rate": 0.00012078651685393258,
      "loss": 1.1866,
      "step": 435
    },
    {
      "epoch": 0.061296218192042735,
      "grad_norm": 1.9772627353668213,
      "learning_rate": 0.00012106741573033709,
      "loss": 1.0723,
      "step": 436
    },
    {
      "epoch": 0.061436805848446505,
      "grad_norm": 1.923082947731018,
      "learning_rate": 0.00012134831460674158,
      "loss": 0.9867,
      "step": 437
    },
    {
      "epoch": 0.061577393504850274,
      "grad_norm": 1.7222837209701538,
      "learning_rate": 0.00012162921348314606,
      "loss": 1.1901,
      "step": 438
    },
    {
      "epoch": 0.06171798116125404,
      "grad_norm": 1.622645378112793,
      "learning_rate": 0.00012191011235955057,
      "loss": 1.1384,
      "step": 439
    },
    {
      "epoch": 0.06185856881765781,
      "grad_norm": 1.6764731407165527,
      "learning_rate": 0.00012219101123595507,
      "loss": 1.087,
      "step": 440
    },
    {
      "epoch": 0.061999156474061574,
      "grad_norm": 1.8648253679275513,
      "learning_rate": 0.00012247191011235955,
      "loss": 0.9271,
      "step": 441
    },
    {
      "epoch": 0.062139744130465344,
      "grad_norm": 2.3056352138519287,
      "learning_rate": 0.00012275280898876403,
      "loss": 1.1476,
      "step": 442
    },
    {
      "epoch": 0.06228033178686911,
      "grad_norm": 1.8037205934524536,
      "learning_rate": 0.00012303370786516854,
      "loss": 1.2106,
      "step": 443
    },
    {
      "epoch": 0.06242091944327288,
      "grad_norm": 1.7559583187103271,
      "learning_rate": 0.00012331460674157305,
      "loss": 1.1545,
      "step": 444
    },
    {
      "epoch": 0.06256150709967664,
      "grad_norm": 2.0375609397888184,
      "learning_rate": 0.00012359550561797752,
      "loss": 1.2179,
      "step": 445
    },
    {
      "epoch": 0.06270209475608042,
      "grad_norm": 1.6765224933624268,
      "learning_rate": 0.00012387640449438203,
      "loss": 1.0761,
      "step": 446
    },
    {
      "epoch": 0.06284268241248418,
      "grad_norm": 1.9799795150756836,
      "learning_rate": 0.00012415730337078654,
      "loss": 1.1826,
      "step": 447
    },
    {
      "epoch": 0.06298327006888794,
      "grad_norm": 1.898092269897461,
      "learning_rate": 0.00012443820224719102,
      "loss": 1.0829,
      "step": 448
    },
    {
      "epoch": 0.06312385772529172,
      "grad_norm": 1.8634573221206665,
      "learning_rate": 0.0001247191011235955,
      "loss": 1.0304,
      "step": 449
    },
    {
      "epoch": 0.06326444538169548,
      "grad_norm": 1.8832370042800903,
      "learning_rate": 0.000125,
      "loss": 1.1452,
      "step": 450
    },
    {
      "epoch": 0.06340503303809926,
      "grad_norm": 1.8605411052703857,
      "learning_rate": 0.0001252808988764045,
      "loss": 0.9622,
      "step": 451
    },
    {
      "epoch": 0.06354562069450302,
      "grad_norm": 1.773528814315796,
      "learning_rate": 0.000125561797752809,
      "loss": 1.1905,
      "step": 452
    },
    {
      "epoch": 0.06368620835090678,
      "grad_norm": 1.866137146949768,
      "learning_rate": 0.0001258426966292135,
      "loss": 1.1053,
      "step": 453
    },
    {
      "epoch": 0.06382679600731056,
      "grad_norm": 1.7090567350387573,
      "learning_rate": 0.00012612359550561797,
      "loss": 1.1265,
      "step": 454
    },
    {
      "epoch": 0.06396738366371432,
      "grad_norm": 1.7179371118545532,
      "learning_rate": 0.00012640449438202248,
      "loss": 0.9163,
      "step": 455
    },
    {
      "epoch": 0.0641079713201181,
      "grad_norm": 1.9382511377334595,
      "learning_rate": 0.00012668539325842696,
      "loss": 1.1028,
      "step": 456
    },
    {
      "epoch": 0.06424855897652186,
      "grad_norm": 1.6923611164093018,
      "learning_rate": 0.00012696629213483147,
      "loss": 0.9442,
      "step": 457
    },
    {
      "epoch": 0.06438914663292562,
      "grad_norm": 1.7376362085342407,
      "learning_rate": 0.00012724719101123597,
      "loss": 0.914,
      "step": 458
    },
    {
      "epoch": 0.0645297342893294,
      "grad_norm": 1.9974411725997925,
      "learning_rate": 0.00012752808988764045,
      "loss": 0.9906,
      "step": 459
    },
    {
      "epoch": 0.06467032194573316,
      "grad_norm": 1.7774581909179688,
      "learning_rate": 0.00012780898876404496,
      "loss": 1.1434,
      "step": 460
    },
    {
      "epoch": 0.06481090960213694,
      "grad_norm": 1.5525000095367432,
      "learning_rate": 0.00012808988764044944,
      "loss": 1.1474,
      "step": 461
    },
    {
      "epoch": 0.0649514972585407,
      "grad_norm": 2.11816668510437,
      "learning_rate": 0.00012837078651685394,
      "loss": 1.0836,
      "step": 462
    },
    {
      "epoch": 0.06509208491494446,
      "grad_norm": 1.9258075952529907,
      "learning_rate": 0.00012865168539325842,
      "loss": 1.1757,
      "step": 463
    },
    {
      "epoch": 0.06523267257134824,
      "grad_norm": 1.952625036239624,
      "learning_rate": 0.00012893258426966293,
      "loss": 1.0129,
      "step": 464
    },
    {
      "epoch": 0.065373260227752,
      "grad_norm": 1.868016242980957,
      "learning_rate": 0.00012921348314606744,
      "loss": 1.2818,
      "step": 465
    },
    {
      "epoch": 0.06551384788415578,
      "grad_norm": 1.664654016494751,
      "learning_rate": 0.00012949438202247192,
      "loss": 1.0918,
      "step": 466
    },
    {
      "epoch": 0.06565443554055954,
      "grad_norm": 1.638277292251587,
      "learning_rate": 0.0001297752808988764,
      "loss": 1.1189,
      "step": 467
    },
    {
      "epoch": 0.0657950231969633,
      "grad_norm": 1.5944491624832153,
      "learning_rate": 0.0001300561797752809,
      "loss": 1.1991,
      "step": 468
    },
    {
      "epoch": 0.06593561085336708,
      "grad_norm": 1.966022253036499,
      "learning_rate": 0.0001303370786516854,
      "loss": 0.9867,
      "step": 469
    },
    {
      "epoch": 0.06607619850977084,
      "grad_norm": 1.5669136047363281,
      "learning_rate": 0.00013061797752808989,
      "loss": 1.2034,
      "step": 470
    },
    {
      "epoch": 0.06621678616617462,
      "grad_norm": 2.050879716873169,
      "learning_rate": 0.0001308988764044944,
      "loss": 1.3139,
      "step": 471
    },
    {
      "epoch": 0.06635737382257838,
      "grad_norm": 1.7758675813674927,
      "learning_rate": 0.0001311797752808989,
      "loss": 1.0843,
      "step": 472
    },
    {
      "epoch": 0.06649796147898214,
      "grad_norm": 1.9246795177459717,
      "learning_rate": 0.00013146067415730338,
      "loss": 1.1369,
      "step": 473
    },
    {
      "epoch": 0.06663854913538592,
      "grad_norm": 1.8221930265426636,
      "learning_rate": 0.00013174157303370786,
      "loss": 1.1094,
      "step": 474
    },
    {
      "epoch": 0.06677913679178968,
      "grad_norm": 1.9627691507339478,
      "learning_rate": 0.00013202247191011236,
      "loss": 1.1141,
      "step": 475
    },
    {
      "epoch": 0.06691972444819345,
      "grad_norm": 1.95071280002594,
      "learning_rate": 0.00013230337078651687,
      "loss": 1.0104,
      "step": 476
    },
    {
      "epoch": 0.06706031210459722,
      "grad_norm": 2.104509115219116,
      "learning_rate": 0.00013258426966292135,
      "loss": 1.1799,
      "step": 477
    },
    {
      "epoch": 0.06720089976100098,
      "grad_norm": 1.7447333335876465,
      "learning_rate": 0.00013286516853932586,
      "loss": 1.1466,
      "step": 478
    },
    {
      "epoch": 0.06734148741740476,
      "grad_norm": 1.6265498399734497,
      "learning_rate": 0.00013314606741573034,
      "loss": 1.2452,
      "step": 479
    },
    {
      "epoch": 0.06748207507380852,
      "grad_norm": 1.9182617664337158,
      "learning_rate": 0.00013342696629213484,
      "loss": 0.9609,
      "step": 480
    },
    {
      "epoch": 0.0676226627302123,
      "grad_norm": 1.8662413358688354,
      "learning_rate": 0.00013370786516853932,
      "loss": 0.9989,
      "step": 481
    },
    {
      "epoch": 0.06776325038661606,
      "grad_norm": 1.6455252170562744,
      "learning_rate": 0.00013398876404494383,
      "loss": 1.1521,
      "step": 482
    },
    {
      "epoch": 0.06790383804301982,
      "grad_norm": 1.578442931175232,
      "learning_rate": 0.00013426966292134833,
      "loss": 1.1742,
      "step": 483
    },
    {
      "epoch": 0.0680444256994236,
      "grad_norm": 1.5100979804992676,
      "learning_rate": 0.0001345505617977528,
      "loss": 1.1147,
      "step": 484
    },
    {
      "epoch": 0.06818501335582736,
      "grad_norm": 1.8408451080322266,
      "learning_rate": 0.00013483146067415732,
      "loss": 1.2507,
      "step": 485
    },
    {
      "epoch": 0.06832560101223113,
      "grad_norm": 1.7546958923339844,
      "learning_rate": 0.0001351123595505618,
      "loss": 1.1536,
      "step": 486
    },
    {
      "epoch": 0.0684661886686349,
      "grad_norm": 1.6906131505966187,
      "learning_rate": 0.0001353932584269663,
      "loss": 1.078,
      "step": 487
    },
    {
      "epoch": 0.06860677632503866,
      "grad_norm": 1.734832525253296,
      "learning_rate": 0.00013567415730337078,
      "loss": 1.0737,
      "step": 488
    },
    {
      "epoch": 0.06874736398144243,
      "grad_norm": 1.392675757408142,
      "learning_rate": 0.0001359550561797753,
      "loss": 1.207,
      "step": 489
    },
    {
      "epoch": 0.0688879516378462,
      "grad_norm": 1.7717937231063843,
      "learning_rate": 0.0001362359550561798,
      "loss": 1.189,
      "step": 490
    },
    {
      "epoch": 0.06902853929424997,
      "grad_norm": 1.5874918699264526,
      "learning_rate": 0.00013651685393258428,
      "loss": 1.2526,
      "step": 491
    },
    {
      "epoch": 0.06916912695065373,
      "grad_norm": 1.9996018409729004,
      "learning_rate": 0.00013679775280898876,
      "loss": 1.2061,
      "step": 492
    },
    {
      "epoch": 0.0693097146070575,
      "grad_norm": 1.8376109600067139,
      "learning_rate": 0.00013707865168539326,
      "loss": 1.0134,
      "step": 493
    },
    {
      "epoch": 0.06945030226346127,
      "grad_norm": 1.7855921983718872,
      "learning_rate": 0.00013735955056179777,
      "loss": 0.9627,
      "step": 494
    },
    {
      "epoch": 0.06959088991986503,
      "grad_norm": 1.604891300201416,
      "learning_rate": 0.00013764044943820225,
      "loss": 1.0695,
      "step": 495
    },
    {
      "epoch": 0.06973147757626881,
      "grad_norm": 1.878560185432434,
      "learning_rate": 0.00013792134831460675,
      "loss": 0.9567,
      "step": 496
    },
    {
      "epoch": 0.06987206523267257,
      "grad_norm": 1.6329827308654785,
      "learning_rate": 0.00013820224719101123,
      "loss": 1.1128,
      "step": 497
    },
    {
      "epoch": 0.07001265288907633,
      "grad_norm": 1.6349804401397705,
      "learning_rate": 0.00013848314606741574,
      "loss": 1.1287,
      "step": 498
    },
    {
      "epoch": 0.07015324054548011,
      "grad_norm": 2.2976999282836914,
      "learning_rate": 0.00013876404494382022,
      "loss": 1.2004,
      "step": 499
    },
    {
      "epoch": 0.07029382820188387,
      "grad_norm": 1.8524725437164307,
      "learning_rate": 0.00013904494382022473,
      "loss": 1.1196,
      "step": 500
    },
    {
      "epoch": 0.07029382820188387,
      "eval_loss": 1.1718494892120361,
      "eval_runtime": 769.9388,
      "eval_samples_per_second": 16.425,
      "eval_steps_per_second": 8.212,
      "step": 500
    },
    {
      "epoch": 0.07043441585828764,
      "grad_norm": 1.7514917850494385,
      "learning_rate": 0.00013932584269662923,
      "loss": 1.2251,
      "step": 501
    },
    {
      "epoch": 0.07057500351469141,
      "grad_norm": 2.02458119392395,
      "learning_rate": 0.0001396067415730337,
      "loss": 1.1656,
      "step": 502
    },
    {
      "epoch": 0.07071559117109517,
      "grad_norm": 1.8836143016815186,
      "learning_rate": 0.00013988764044943822,
      "loss": 1.0837,
      "step": 503
    },
    {
      "epoch": 0.07085617882749895,
      "grad_norm": 2.006038188934326,
      "learning_rate": 0.0001401685393258427,
      "loss": 1.0856,
      "step": 504
    },
    {
      "epoch": 0.07099676648390271,
      "grad_norm": 1.9374271631240845,
      "learning_rate": 0.0001404494382022472,
      "loss": 1.0558,
      "step": 505
    },
    {
      "epoch": 0.07113735414030647,
      "grad_norm": 1.9645938873291016,
      "learning_rate": 0.00014073033707865168,
      "loss": 1.1528,
      "step": 506
    },
    {
      "epoch": 0.07127794179671025,
      "grad_norm": 1.6080759763717651,
      "learning_rate": 0.0001410112359550562,
      "loss": 1.1595,
      "step": 507
    },
    {
      "epoch": 0.07141852945311401,
      "grad_norm": 2.364879608154297,
      "learning_rate": 0.0001412921348314607,
      "loss": 1.0813,
      "step": 508
    },
    {
      "epoch": 0.07155911710951779,
      "grad_norm": 2.1399950981140137,
      "learning_rate": 0.00014157303370786517,
      "loss": 1.1126,
      "step": 509
    },
    {
      "epoch": 0.07169970476592155,
      "grad_norm": 1.5613672733306885,
      "learning_rate": 0.00014185393258426965,
      "loss": 1.1565,
      "step": 510
    },
    {
      "epoch": 0.07184029242232531,
      "grad_norm": 1.8278011083602905,
      "learning_rate": 0.00014213483146067416,
      "loss": 1.1397,
      "step": 511
    },
    {
      "epoch": 0.07198088007872909,
      "grad_norm": 1.7835471630096436,
      "learning_rate": 0.00014241573033707867,
      "loss": 1.0844,
      "step": 512
    },
    {
      "epoch": 0.07212146773513285,
      "grad_norm": 1.7860360145568848,
      "learning_rate": 0.00014269662921348315,
      "loss": 1.1197,
      "step": 513
    },
    {
      "epoch": 0.07226205539153663,
      "grad_norm": 1.5854675769805908,
      "learning_rate": 0.00014297752808988765,
      "loss": 1.169,
      "step": 514
    },
    {
      "epoch": 0.07240264304794039,
      "grad_norm": 1.7821388244628906,
      "learning_rate": 0.00014325842696629216,
      "loss": 1.1698,
      "step": 515
    },
    {
      "epoch": 0.07254323070434415,
      "grad_norm": 1.5901366472244263,
      "learning_rate": 0.00014353932584269664,
      "loss": 1.287,
      "step": 516
    },
    {
      "epoch": 0.07268381836074793,
      "grad_norm": 1.6734418869018555,
      "learning_rate": 0.00014382022471910112,
      "loss": 0.9163,
      "step": 517
    },
    {
      "epoch": 0.07282440601715169,
      "grad_norm": 1.5354588031768799,
      "learning_rate": 0.00014410112359550562,
      "loss": 1.1316,
      "step": 518
    },
    {
      "epoch": 0.07296499367355547,
      "grad_norm": 1.9380322694778442,
      "learning_rate": 0.00014438202247191013,
      "loss": 1.0502,
      "step": 519
    },
    {
      "epoch": 0.07310558132995923,
      "grad_norm": 1.779509425163269,
      "learning_rate": 0.0001446629213483146,
      "loss": 0.9424,
      "step": 520
    },
    {
      "epoch": 0.07324616898636299,
      "grad_norm": 1.7071030139923096,
      "learning_rate": 0.00014494382022471912,
      "loss": 1.2362,
      "step": 521
    },
    {
      "epoch": 0.07338675664276677,
      "grad_norm": 1.8892433643341064,
      "learning_rate": 0.0001452247191011236,
      "loss": 0.9979,
      "step": 522
    },
    {
      "epoch": 0.07352734429917053,
      "grad_norm": 2.017568588256836,
      "learning_rate": 0.00014550561797752807,
      "loss": 1.1366,
      "step": 523
    },
    {
      "epoch": 0.0736679319555743,
      "grad_norm": 1.9562225341796875,
      "learning_rate": 0.00014578651685393258,
      "loss": 1.2052,
      "step": 524
    },
    {
      "epoch": 0.07380851961197807,
      "grad_norm": 1.6277035474777222,
      "learning_rate": 0.0001460674157303371,
      "loss": 1.066,
      "step": 525
    },
    {
      "epoch": 0.07394910726838183,
      "grad_norm": 1.742151141166687,
      "learning_rate": 0.0001463483146067416,
      "loss": 1.2969,
      "step": 526
    },
    {
      "epoch": 0.0740896949247856,
      "grad_norm": 1.7842857837677002,
      "learning_rate": 0.00014662921348314607,
      "loss": 1.1289,
      "step": 527
    },
    {
      "epoch": 0.07423028258118937,
      "grad_norm": 1.598276972770691,
      "learning_rate": 0.00014691011235955058,
      "loss": 1.0112,
      "step": 528
    },
    {
      "epoch": 0.07437087023759315,
      "grad_norm": 1.7384521961212158,
      "learning_rate": 0.00014719101123595506,
      "loss": 1.2159,
      "step": 529
    },
    {
      "epoch": 0.07451145789399691,
      "grad_norm": 1.5853840112686157,
      "learning_rate": 0.00014747191011235956,
      "loss": 1.1601,
      "step": 530
    },
    {
      "epoch": 0.07465204555040067,
      "grad_norm": 1.6281217336654663,
      "learning_rate": 0.00014775280898876404,
      "loss": 1.0389,
      "step": 531
    },
    {
      "epoch": 0.07479263320680445,
      "grad_norm": 1.8669724464416504,
      "learning_rate": 0.00014803370786516855,
      "loss": 1.1562,
      "step": 532
    },
    {
      "epoch": 0.07493322086320821,
      "grad_norm": 1.8381009101867676,
      "learning_rate": 0.00014831460674157306,
      "loss": 1.0725,
      "step": 533
    },
    {
      "epoch": 0.07507380851961198,
      "grad_norm": 2.249843120574951,
      "learning_rate": 0.00014859550561797754,
      "loss": 1.0988,
      "step": 534
    },
    {
      "epoch": 0.07521439617601575,
      "grad_norm": 1.9389251470565796,
      "learning_rate": 0.00014887640449438202,
      "loss": 1.1899,
      "step": 535
    },
    {
      "epoch": 0.07535498383241951,
      "grad_norm": 1.8265089988708496,
      "learning_rate": 0.00014915730337078652,
      "loss": 0.9782,
      "step": 536
    },
    {
      "epoch": 0.07549557148882328,
      "grad_norm": 1.7461419105529785,
      "learning_rate": 0.00014943820224719103,
      "loss": 1.0795,
      "step": 537
    },
    {
      "epoch": 0.07563615914522705,
      "grad_norm": 1.792741298675537,
      "learning_rate": 0.0001497191011235955,
      "loss": 1.2463,
      "step": 538
    },
    {
      "epoch": 0.07577674680163082,
      "grad_norm": 1.8163418769836426,
      "learning_rate": 0.00015000000000000001,
      "loss": 1.1509,
      "step": 539
    },
    {
      "epoch": 0.07591733445803459,
      "grad_norm": 1.9132137298583984,
      "learning_rate": 0.00015028089887640452,
      "loss": 1.122,
      "step": 540
    },
    {
      "epoch": 0.07605792211443835,
      "grad_norm": 1.6287851333618164,
      "learning_rate": 0.000150561797752809,
      "loss": 1.0958,
      "step": 541
    },
    {
      "epoch": 0.07619850977084212,
      "grad_norm": 1.8988511562347412,
      "learning_rate": 0.00015084269662921348,
      "loss": 1.0721,
      "step": 542
    },
    {
      "epoch": 0.07633909742724589,
      "grad_norm": 1.8509657382965088,
      "learning_rate": 0.00015112359550561799,
      "loss": 1.1495,
      "step": 543
    },
    {
      "epoch": 0.07647968508364966,
      "grad_norm": 1.6124266386032104,
      "learning_rate": 0.0001514044943820225,
      "loss": 1.0116,
      "step": 544
    },
    {
      "epoch": 0.07662027274005342,
      "grad_norm": 2.014547109603882,
      "learning_rate": 0.00015168539325842697,
      "loss": 1.0104,
      "step": 545
    },
    {
      "epoch": 0.07676086039645719,
      "grad_norm": 2.216648817062378,
      "learning_rate": 0.00015196629213483148,
      "loss": 1.1623,
      "step": 546
    },
    {
      "epoch": 0.07690144805286096,
      "grad_norm": 1.8466733694076538,
      "learning_rate": 0.00015224719101123596,
      "loss": 1.1612,
      "step": 547
    },
    {
      "epoch": 0.07704203570926473,
      "grad_norm": 1.629024624824524,
      "learning_rate": 0.00015252808988764044,
      "loss": 1.0822,
      "step": 548
    },
    {
      "epoch": 0.0771826233656685,
      "grad_norm": 1.821184515953064,
      "learning_rate": 0.00015280898876404494,
      "loss": 1.0843,
      "step": 549
    },
    {
      "epoch": 0.07732321102207226,
      "grad_norm": 1.9131529331207275,
      "learning_rate": 0.00015308988764044945,
      "loss": 1.1521,
      "step": 550
    },
    {
      "epoch": 0.07746379867847603,
      "grad_norm": 1.9130793809890747,
      "learning_rate": 0.00015337078651685396,
      "loss": 1.1443,
      "step": 551
    },
    {
      "epoch": 0.0776043863348798,
      "grad_norm": 2.003965139389038,
      "learning_rate": 0.00015365168539325843,
      "loss": 1.0441,
      "step": 552
    },
    {
      "epoch": 0.07774497399128356,
      "grad_norm": 2.0951876640319824,
      "learning_rate": 0.00015393258426966294,
      "loss": 0.9906,
      "step": 553
    },
    {
      "epoch": 0.07788556164768733,
      "grad_norm": 1.7366385459899902,
      "learning_rate": 0.00015421348314606742,
      "loss": 1.1766,
      "step": 554
    },
    {
      "epoch": 0.0780261493040911,
      "grad_norm": 1.8172181844711304,
      "learning_rate": 0.00015449438202247193,
      "loss": 1.1716,
      "step": 555
    },
    {
      "epoch": 0.07816673696049486,
      "grad_norm": 1.5847293138504028,
      "learning_rate": 0.0001547752808988764,
      "loss": 1.2206,
      "step": 556
    },
    {
      "epoch": 0.07830732461689864,
      "grad_norm": 1.6581417322158813,
      "learning_rate": 0.0001550561797752809,
      "loss": 0.8883,
      "step": 557
    },
    {
      "epoch": 0.0784479122733024,
      "grad_norm": 1.7546782493591309,
      "learning_rate": 0.00015533707865168542,
      "loss": 1.2298,
      "step": 558
    },
    {
      "epoch": 0.07858849992970617,
      "grad_norm": 1.741114854812622,
      "learning_rate": 0.0001556179775280899,
      "loss": 0.9923,
      "step": 559
    },
    {
      "epoch": 0.07872908758610994,
      "grad_norm": 1.5951638221740723,
      "learning_rate": 0.00015589887640449438,
      "loss": 1.3051,
      "step": 560
    },
    {
      "epoch": 0.0788696752425137,
      "grad_norm": 2.006472587585449,
      "learning_rate": 0.00015617977528089888,
      "loss": 1.0406,
      "step": 561
    },
    {
      "epoch": 0.07901026289891748,
      "grad_norm": 1.8330334424972534,
      "learning_rate": 0.0001564606741573034,
      "loss": 1.0594,
      "step": 562
    },
    {
      "epoch": 0.07915085055532124,
      "grad_norm": 1.9694157838821411,
      "learning_rate": 0.00015674157303370787,
      "loss": 1.2719,
      "step": 563
    },
    {
      "epoch": 0.079291438211725,
      "grad_norm": 1.6476695537567139,
      "learning_rate": 0.00015702247191011238,
      "loss": 1.1895,
      "step": 564
    },
    {
      "epoch": 0.07943202586812878,
      "grad_norm": 1.9823668003082275,
      "learning_rate": 0.00015730337078651685,
      "loss": 1.0257,
      "step": 565
    },
    {
      "epoch": 0.07957261352453254,
      "grad_norm": 1.9492820501327515,
      "learning_rate": 0.00015758426966292133,
      "loss": 1.1818,
      "step": 566
    },
    {
      "epoch": 0.07971320118093632,
      "grad_norm": 1.8337290287017822,
      "learning_rate": 0.00015786516853932584,
      "loss": 0.9912,
      "step": 567
    },
    {
      "epoch": 0.07985378883734008,
      "grad_norm": 1.722230076789856,
      "learning_rate": 0.00015814606741573035,
      "loss": 1.1455,
      "step": 568
    },
    {
      "epoch": 0.07999437649374384,
      "grad_norm": 1.7482761144638062,
      "learning_rate": 0.00015842696629213485,
      "loss": 0.9863,
      "step": 569
    },
    {
      "epoch": 0.08013496415014762,
      "grad_norm": 1.862717628479004,
      "learning_rate": 0.00015870786516853933,
      "loss": 1.1273,
      "step": 570
    },
    {
      "epoch": 0.08027555180655138,
      "grad_norm": 1.6712069511413574,
      "learning_rate": 0.00015898876404494384,
      "loss": 1.063,
      "step": 571
    },
    {
      "epoch": 0.08041613946295516,
      "grad_norm": 1.6191927194595337,
      "learning_rate": 0.00015926966292134832,
      "loss": 1.1544,
      "step": 572
    },
    {
      "epoch": 0.08055672711935892,
      "grad_norm": 2.0997235774993896,
      "learning_rate": 0.0001595505617977528,
      "loss": 1.1348,
      "step": 573
    },
    {
      "epoch": 0.08069731477576268,
      "grad_norm": 1.7952507734298706,
      "learning_rate": 0.0001598314606741573,
      "loss": 1.1795,
      "step": 574
    },
    {
      "epoch": 0.08083790243216646,
      "grad_norm": 1.8866488933563232,
      "learning_rate": 0.0001601123595505618,
      "loss": 1.2695,
      "step": 575
    },
    {
      "epoch": 0.08097849008857022,
      "grad_norm": 1.900767207145691,
      "learning_rate": 0.00016039325842696632,
      "loss": 1.0304,
      "step": 576
    },
    {
      "epoch": 0.081119077744974,
      "grad_norm": 1.5800843238830566,
      "learning_rate": 0.0001606741573033708,
      "loss": 1.2319,
      "step": 577
    },
    {
      "epoch": 0.08125966540137776,
      "grad_norm": 1.7135732173919678,
      "learning_rate": 0.00016095505617977528,
      "loss": 1.1433,
      "step": 578
    },
    {
      "epoch": 0.08140025305778152,
      "grad_norm": 1.6325292587280273,
      "learning_rate": 0.00016123595505617978,
      "loss": 1.2152,
      "step": 579
    },
    {
      "epoch": 0.0815408407141853,
      "grad_norm": 1.806302547454834,
      "learning_rate": 0.00016151685393258426,
      "loss": 0.9553,
      "step": 580
    },
    {
      "epoch": 0.08168142837058906,
      "grad_norm": 1.9692156314849854,
      "learning_rate": 0.00016179775280898877,
      "loss": 0.9634,
      "step": 581
    },
    {
      "epoch": 0.08182201602699284,
      "grad_norm": 1.9859001636505127,
      "learning_rate": 0.00016207865168539327,
      "loss": 1.2463,
      "step": 582
    },
    {
      "epoch": 0.0819626036833966,
      "grad_norm": 1.7884678840637207,
      "learning_rate": 0.00016235955056179778,
      "loss": 1.0531,
      "step": 583
    },
    {
      "epoch": 0.08210319133980036,
      "grad_norm": 1.6303131580352783,
      "learning_rate": 0.00016264044943820226,
      "loss": 1.1544,
      "step": 584
    },
    {
      "epoch": 0.08224377899620414,
      "grad_norm": 1.8227020502090454,
      "learning_rate": 0.00016292134831460674,
      "loss": 1.0264,
      "step": 585
    },
    {
      "epoch": 0.0823843666526079,
      "grad_norm": 1.8657948970794678,
      "learning_rate": 0.00016320224719101124,
      "loss": 1.0811,
      "step": 586
    },
    {
      "epoch": 0.08252495430901167,
      "grad_norm": 1.9294456243515015,
      "learning_rate": 0.00016348314606741575,
      "loss": 1.1492,
      "step": 587
    },
    {
      "epoch": 0.08266554196541544,
      "grad_norm": 1.7479487657546997,
      "learning_rate": 0.00016376404494382023,
      "loss": 1.2064,
      "step": 588
    },
    {
      "epoch": 0.0828061296218192,
      "grad_norm": 1.9480308294296265,
      "learning_rate": 0.00016404494382022474,
      "loss": 1.1464,
      "step": 589
    },
    {
      "epoch": 0.08294671727822298,
      "grad_norm": 1.9083218574523926,
      "learning_rate": 0.00016432584269662922,
      "loss": 1.2696,
      "step": 590
    },
    {
      "epoch": 0.08308730493462674,
      "grad_norm": 1.9076848030090332,
      "learning_rate": 0.0001646067415730337,
      "loss": 1.189,
      "step": 591
    },
    {
      "epoch": 0.08322789259103051,
      "grad_norm": 1.7921706438064575,
      "learning_rate": 0.0001648876404494382,
      "loss": 1.1898,
      "step": 592
    },
    {
      "epoch": 0.08336848024743428,
      "grad_norm": 1.7707024812698364,
      "learning_rate": 0.0001651685393258427,
      "loss": 1.0333,
      "step": 593
    },
    {
      "epoch": 0.08350906790383804,
      "grad_norm": 1.7503043413162231,
      "learning_rate": 0.00016544943820224721,
      "loss": 1.0849,
      "step": 594
    },
    {
      "epoch": 0.08364965556024181,
      "grad_norm": 1.9450523853302002,
      "learning_rate": 0.0001657303370786517,
      "loss": 1.2105,
      "step": 595
    },
    {
      "epoch": 0.08379024321664558,
      "grad_norm": 1.636885404586792,
      "learning_rate": 0.0001660112359550562,
      "loss": 1.2523,
      "step": 596
    },
    {
      "epoch": 0.08393083087304935,
      "grad_norm": 1.8565523624420166,
      "learning_rate": 0.00016629213483146068,
      "loss": 1.0361,
      "step": 597
    },
    {
      "epoch": 0.08407141852945312,
      "grad_norm": 1.6406056880950928,
      "learning_rate": 0.00016657303370786516,
      "loss": 1.1161,
      "step": 598
    },
    {
      "epoch": 0.08421200618585688,
      "grad_norm": 1.6104568243026733,
      "learning_rate": 0.00016685393258426967,
      "loss": 1.0603,
      "step": 599
    },
    {
      "epoch": 0.08435259384226065,
      "grad_norm": 1.8450829982757568,
      "learning_rate": 0.00016713483146067417,
      "loss": 1.0511,
      "step": 600
    },
    {
      "epoch": 0.08449318149866442,
      "grad_norm": 1.745871663093567,
      "learning_rate": 0.00016741573033707868,
      "loss": 1.0386,
      "step": 601
    },
    {
      "epoch": 0.08463376915506818,
      "grad_norm": 1.6379019021987915,
      "learning_rate": 0.00016769662921348316,
      "loss": 1.2001,
      "step": 602
    },
    {
      "epoch": 0.08477435681147195,
      "grad_norm": 1.8524993658065796,
      "learning_rate": 0.00016797752808988764,
      "loss": 1.2422,
      "step": 603
    },
    {
      "epoch": 0.08491494446787572,
      "grad_norm": 1.74254310131073,
      "learning_rate": 0.00016825842696629214,
      "loss": 1.1174,
      "step": 604
    },
    {
      "epoch": 0.08505553212427949,
      "grad_norm": 1.9355604648590088,
      "learning_rate": 0.00016853932584269662,
      "loss": 0.9207,
      "step": 605
    },
    {
      "epoch": 0.08519611978068325,
      "grad_norm": 2.0613884925842285,
      "learning_rate": 0.00016882022471910113,
      "loss": 1.0576,
      "step": 606
    },
    {
      "epoch": 0.08533670743708702,
      "grad_norm": 2.111945390701294,
      "learning_rate": 0.00016910112359550564,
      "loss": 1.2108,
      "step": 607
    },
    {
      "epoch": 0.0854772950934908,
      "grad_norm": 1.6916882991790771,
      "learning_rate": 0.00016938202247191014,
      "loss": 1.1034,
      "step": 608
    },
    {
      "epoch": 0.08561788274989456,
      "grad_norm": 1.4921767711639404,
      "learning_rate": 0.00016966292134831462,
      "loss": 1.1564,
      "step": 609
    },
    {
      "epoch": 0.08575847040629833,
      "grad_norm": 1.8833372592926025,
      "learning_rate": 0.0001699438202247191,
      "loss": 1.0603,
      "step": 610
    },
    {
      "epoch": 0.0858990580627021,
      "grad_norm": 1.9325768947601318,
      "learning_rate": 0.0001702247191011236,
      "loss": 1.2213,
      "step": 611
    },
    {
      "epoch": 0.08603964571910586,
      "grad_norm": 1.5197745561599731,
      "learning_rate": 0.00017050561797752809,
      "loss": 1.0998,
      "step": 612
    },
    {
      "epoch": 0.08618023337550963,
      "grad_norm": 1.6265339851379395,
      "learning_rate": 0.0001707865168539326,
      "loss": 1.1814,
      "step": 613
    },
    {
      "epoch": 0.0863208210319134,
      "grad_norm": 1.7477728128433228,
      "learning_rate": 0.0001710674157303371,
      "loss": 1.144,
      "step": 614
    },
    {
      "epoch": 0.08646140868831717,
      "grad_norm": 1.9814451932907104,
      "learning_rate": 0.00017134831460674158,
      "loss": 1.0513,
      "step": 615
    },
    {
      "epoch": 0.08660199634472093,
      "grad_norm": 1.913831353187561,
      "learning_rate": 0.00017162921348314606,
      "loss": 1.0525,
      "step": 616
    },
    {
      "epoch": 0.0867425840011247,
      "grad_norm": 1.803867220878601,
      "learning_rate": 0.00017191011235955056,
      "loss": 1.1482,
      "step": 617
    },
    {
      "epoch": 0.08688317165752847,
      "grad_norm": 1.9065260887145996,
      "learning_rate": 0.00017219101123595507,
      "loss": 1.0226,
      "step": 618
    },
    {
      "epoch": 0.08702375931393223,
      "grad_norm": 2.0222091674804688,
      "learning_rate": 0.00017247191011235958,
      "loss": 1.0375,
      "step": 619
    },
    {
      "epoch": 0.08716434697033601,
      "grad_norm": 1.7485917806625366,
      "learning_rate": 0.00017275280898876406,
      "loss": 0.9999,
      "step": 620
    },
    {
      "epoch": 0.08730493462673977,
      "grad_norm": 1.9900448322296143,
      "learning_rate": 0.00017303370786516853,
      "loss": 1.0697,
      "step": 621
    },
    {
      "epoch": 0.08744552228314353,
      "grad_norm": 1.8691809177398682,
      "learning_rate": 0.00017331460674157304,
      "loss": 1.1992,
      "step": 622
    },
    {
      "epoch": 0.08758610993954731,
      "grad_norm": 1.6825190782546997,
      "learning_rate": 0.00017359550561797752,
      "loss": 1.2592,
      "step": 623
    },
    {
      "epoch": 0.08772669759595107,
      "grad_norm": 1.7848769426345825,
      "learning_rate": 0.00017387640449438203,
      "loss": 1.0815,
      "step": 624
    },
    {
      "epoch": 0.08786728525235485,
      "grad_norm": 1.9021390676498413,
      "learning_rate": 0.00017415730337078653,
      "loss": 1.0105,
      "step": 625
    },
    {
      "epoch": 0.08800787290875861,
      "grad_norm": 1.6025691032409668,
      "learning_rate": 0.00017443820224719104,
      "loss": 1.1498,
      "step": 626
    },
    {
      "epoch": 0.08814846056516237,
      "grad_norm": 1.560238003730774,
      "learning_rate": 0.00017471910112359552,
      "loss": 1.1215,
      "step": 627
    },
    {
      "epoch": 0.08828904822156615,
      "grad_norm": 1.6160931587219238,
      "learning_rate": 0.000175,
      "loss": 1.0533,
      "step": 628
    },
    {
      "epoch": 0.08842963587796991,
      "grad_norm": 1.7445076704025269,
      "learning_rate": 0.0001752808988764045,
      "loss": 1.001,
      "step": 629
    },
    {
      "epoch": 0.08857022353437369,
      "grad_norm": 1.9323867559432983,
      "learning_rate": 0.00017556179775280898,
      "loss": 1.3074,
      "step": 630
    },
    {
      "epoch": 0.08871081119077745,
      "grad_norm": 1.6116667985916138,
      "learning_rate": 0.0001758426966292135,
      "loss": 1.1119,
      "step": 631
    },
    {
      "epoch": 0.08885139884718121,
      "grad_norm": 1.7237968444824219,
      "learning_rate": 0.000176123595505618,
      "loss": 1.135,
      "step": 632
    },
    {
      "epoch": 0.08899198650358499,
      "grad_norm": 1.9896175861358643,
      "learning_rate": 0.00017640449438202248,
      "loss": 1.0288,
      "step": 633
    },
    {
      "epoch": 0.08913257415998875,
      "grad_norm": 1.9382209777832031,
      "learning_rate": 0.00017668539325842696,
      "loss": 1.0016,
      "step": 634
    },
    {
      "epoch": 0.08927316181639253,
      "grad_norm": 1.6360071897506714,
      "learning_rate": 0.00017696629213483146,
      "loss": 1.3079,
      "step": 635
    },
    {
      "epoch": 0.08941374947279629,
      "grad_norm": 1.778294563293457,
      "learning_rate": 0.00017724719101123597,
      "loss": 0.9889,
      "step": 636
    },
    {
      "epoch": 0.08955433712920005,
      "grad_norm": 1.5348527431488037,
      "learning_rate": 0.00017752808988764045,
      "loss": 1.1335,
      "step": 637
    },
    {
      "epoch": 0.08969492478560383,
      "grad_norm": 1.6764742136001587,
      "learning_rate": 0.00017780898876404495,
      "loss": 1.1802,
      "step": 638
    },
    {
      "epoch": 0.08983551244200759,
      "grad_norm": 1.8254311084747314,
      "learning_rate": 0.00017808988764044946,
      "loss": 1.055,
      "step": 639
    },
    {
      "epoch": 0.08997610009841137,
      "grad_norm": 1.6666100025177002,
      "learning_rate": 0.00017837078651685394,
      "loss": 1.1079,
      "step": 640
    },
    {
      "epoch": 0.09011668775481513,
      "grad_norm": 1.8659517765045166,
      "learning_rate": 0.00017865168539325842,
      "loss": 1.118,
      "step": 641
    },
    {
      "epoch": 0.09025727541121889,
      "grad_norm": 1.7432852983474731,
      "learning_rate": 0.00017893258426966293,
      "loss": 1.0121,
      "step": 642
    },
    {
      "epoch": 0.09039786306762267,
      "grad_norm": 1.7917046546936035,
      "learning_rate": 0.00017921348314606743,
      "loss": 1.0073,
      "step": 643
    },
    {
      "epoch": 0.09053845072402643,
      "grad_norm": 1.8301162719726562,
      "learning_rate": 0.00017949438202247194,
      "loss": 1.0701,
      "step": 644
    },
    {
      "epoch": 0.0906790383804302,
      "grad_norm": 2.3536174297332764,
      "learning_rate": 0.00017977528089887642,
      "loss": 0.9693,
      "step": 645
    },
    {
      "epoch": 0.09081962603683397,
      "grad_norm": 1.8948757648468018,
      "learning_rate": 0.0001800561797752809,
      "loss": 1.1838,
      "step": 646
    },
    {
      "epoch": 0.09096021369323773,
      "grad_norm": 1.6695860624313354,
      "learning_rate": 0.0001803370786516854,
      "loss": 0.9769,
      "step": 647
    },
    {
      "epoch": 0.0911008013496415,
      "grad_norm": 1.7207157611846924,
      "learning_rate": 0.00018061797752808988,
      "loss": 1.066,
      "step": 648
    },
    {
      "epoch": 0.09124138900604527,
      "grad_norm": 1.9401487112045288,
      "learning_rate": 0.0001808988764044944,
      "loss": 1.2778,
      "step": 649
    },
    {
      "epoch": 0.09138197666244904,
      "grad_norm": 1.84334397315979,
      "learning_rate": 0.0001811797752808989,
      "loss": 1.0494,
      "step": 650
    },
    {
      "epoch": 0.0915225643188528,
      "grad_norm": 1.870233178138733,
      "learning_rate": 0.0001814606741573034,
      "loss": 1.0135,
      "step": 651
    },
    {
      "epoch": 0.09166315197525657,
      "grad_norm": 1.5925264358520508,
      "learning_rate": 0.00018174157303370788,
      "loss": 1.1819,
      "step": 652
    },
    {
      "epoch": 0.09180373963166034,
      "grad_norm": 1.8965181112289429,
      "learning_rate": 0.00018202247191011236,
      "loss": 1.0563,
      "step": 653
    },
    {
      "epoch": 0.0919443272880641,
      "grad_norm": 1.8356221914291382,
      "learning_rate": 0.00018230337078651687,
      "loss": 1.1288,
      "step": 654
    },
    {
      "epoch": 0.09208491494446787,
      "grad_norm": 1.7094335556030273,
      "learning_rate": 0.00018258426966292135,
      "loss": 1.1203,
      "step": 655
    },
    {
      "epoch": 0.09222550260087164,
      "grad_norm": 1.9737379550933838,
      "learning_rate": 0.00018286516853932585,
      "loss": 1.1346,
      "step": 656
    },
    {
      "epoch": 0.09236609025727541,
      "grad_norm": 1.8414993286132812,
      "learning_rate": 0.00018314606741573036,
      "loss": 1.0781,
      "step": 657
    },
    {
      "epoch": 0.09250667791367918,
      "grad_norm": 1.623682975769043,
      "learning_rate": 0.00018342696629213484,
      "loss": 1.2254,
      "step": 658
    },
    {
      "epoch": 0.09264726557008295,
      "grad_norm": 1.7520537376403809,
      "learning_rate": 0.00018370786516853932,
      "loss": 1.0468,
      "step": 659
    },
    {
      "epoch": 0.09278785322648671,
      "grad_norm": 1.6468555927276611,
      "learning_rate": 0.00018398876404494382,
      "loss": 1.1156,
      "step": 660
    },
    {
      "epoch": 0.09292844088289048,
      "grad_norm": 2.0489284992218018,
      "learning_rate": 0.00018426966292134833,
      "loss": 0.9821,
      "step": 661
    },
    {
      "epoch": 0.09306902853929425,
      "grad_norm": 2.0867116451263428,
      "learning_rate": 0.0001845505617977528,
      "loss": 1.0497,
      "step": 662
    },
    {
      "epoch": 0.09320961619569802,
      "grad_norm": 1.656579852104187,
      "learning_rate": 0.00018483146067415732,
      "loss": 1.206,
      "step": 663
    },
    {
      "epoch": 0.09335020385210178,
      "grad_norm": 1.8074613809585571,
      "learning_rate": 0.00018511235955056182,
      "loss": 1.0693,
      "step": 664
    },
    {
      "epoch": 0.09349079150850555,
      "grad_norm": 1.6910744905471802,
      "learning_rate": 0.0001853932584269663,
      "loss": 1.2117,
      "step": 665
    },
    {
      "epoch": 0.09363137916490932,
      "grad_norm": 1.582107663154602,
      "learning_rate": 0.00018567415730337078,
      "loss": 1.0747,
      "step": 666
    },
    {
      "epoch": 0.09377196682131309,
      "grad_norm": 1.6360985040664673,
      "learning_rate": 0.0001859550561797753,
      "loss": 1.2478,
      "step": 667
    },
    {
      "epoch": 0.09391255447771686,
      "grad_norm": 1.818196177482605,
      "learning_rate": 0.0001862359550561798,
      "loss": 1.1352,
      "step": 668
    },
    {
      "epoch": 0.09405314213412062,
      "grad_norm": 1.8028786182403564,
      "learning_rate": 0.00018651685393258427,
      "loss": 1.1622,
      "step": 669
    },
    {
      "epoch": 0.09419372979052439,
      "grad_norm": 1.7853981256484985,
      "learning_rate": 0.00018679775280898878,
      "loss": 1.1282,
      "step": 670
    },
    {
      "epoch": 0.09433431744692816,
      "grad_norm": 1.794421911239624,
      "learning_rate": 0.00018707865168539326,
      "loss": 1.1044,
      "step": 671
    },
    {
      "epoch": 0.09447490510333192,
      "grad_norm": 1.6608794927597046,
      "learning_rate": 0.00018735955056179776,
      "loss": 1.1318,
      "step": 672
    },
    {
      "epoch": 0.0946154927597357,
      "grad_norm": 1.6244912147521973,
      "learning_rate": 0.00018764044943820224,
      "loss": 1.1027,
      "step": 673
    },
    {
      "epoch": 0.09475608041613946,
      "grad_norm": 1.7887078523635864,
      "learning_rate": 0.00018792134831460675,
      "loss": 1.3462,
      "step": 674
    },
    {
      "epoch": 0.09489666807254322,
      "grad_norm": 1.7533149719238281,
      "learning_rate": 0.00018820224719101126,
      "loss": 1.1196,
      "step": 675
    },
    {
      "epoch": 0.095037255728947,
      "grad_norm": 1.4965059757232666,
      "learning_rate": 0.00018848314606741576,
      "loss": 1.1716,
      "step": 676
    },
    {
      "epoch": 0.09517784338535076,
      "grad_norm": 1.6005154848098755,
      "learning_rate": 0.00018876404494382024,
      "loss": 1.1042,
      "step": 677
    },
    {
      "epoch": 0.09531843104175454,
      "grad_norm": 1.5777097940444946,
      "learning_rate": 0.00018904494382022472,
      "loss": 1.1525,
      "step": 678
    },
    {
      "epoch": 0.0954590186981583,
      "grad_norm": 1.9517756700515747,
      "learning_rate": 0.00018932584269662923,
      "loss": 1.0807,
      "step": 679
    },
    {
      "epoch": 0.09559960635456206,
      "grad_norm": 2.1595444679260254,
      "learning_rate": 0.0001896067415730337,
      "loss": 1.0131,
      "step": 680
    },
    {
      "epoch": 0.09574019401096584,
      "grad_norm": 1.7587543725967407,
      "learning_rate": 0.0001898876404494382,
      "loss": 1.0882,
      "step": 681
    },
    {
      "epoch": 0.0958807816673696,
      "grad_norm": 1.9410675764083862,
      "learning_rate": 0.00019016853932584272,
      "loss": 1.1872,
      "step": 682
    },
    {
      "epoch": 0.09602136932377338,
      "grad_norm": 1.6068451404571533,
      "learning_rate": 0.0001904494382022472,
      "loss": 1.2421,
      "step": 683
    },
    {
      "epoch": 0.09616195698017714,
      "grad_norm": 1.8151118755340576,
      "learning_rate": 0.00019073033707865168,
      "loss": 1.251,
      "step": 684
    },
    {
      "epoch": 0.0963025446365809,
      "grad_norm": 1.8112213611602783,
      "learning_rate": 0.00019101123595505618,
      "loss": 1.136,
      "step": 685
    },
    {
      "epoch": 0.09644313229298468,
      "grad_norm": 1.7125979661941528,
      "learning_rate": 0.0001912921348314607,
      "loss": 1.0098,
      "step": 686
    },
    {
      "epoch": 0.09658371994938844,
      "grad_norm": 1.7419513463974,
      "learning_rate": 0.00019157303370786517,
      "loss": 1.0809,
      "step": 687
    },
    {
      "epoch": 0.09672430760579222,
      "grad_norm": 2.0882179737091064,
      "learning_rate": 0.00019185393258426968,
      "loss": 1.0208,
      "step": 688
    },
    {
      "epoch": 0.09686489526219598,
      "grad_norm": 1.7340087890625,
      "learning_rate": 0.00019213483146067416,
      "loss": 1.0653,
      "step": 689
    },
    {
      "epoch": 0.09700548291859974,
      "grad_norm": 1.830165147781372,
      "learning_rate": 0.00019241573033707866,
      "loss": 1.2455,
      "step": 690
    },
    {
      "epoch": 0.09714607057500352,
      "grad_norm": 1.7749359607696533,
      "learning_rate": 0.00019269662921348314,
      "loss": 1.0964,
      "step": 691
    },
    {
      "epoch": 0.09728665823140728,
      "grad_norm": 1.8344537019729614,
      "learning_rate": 0.00019297752808988765,
      "loss": 1.2556,
      "step": 692
    },
    {
      "epoch": 0.09742724588781106,
      "grad_norm": 1.7142237424850464,
      "learning_rate": 0.00019325842696629215,
      "loss": 1.1526,
      "step": 693
    },
    {
      "epoch": 0.09756783354421482,
      "grad_norm": 1.8167173862457275,
      "learning_rate": 0.00019353932584269663,
      "loss": 1.1217,
      "step": 694
    },
    {
      "epoch": 0.09770842120061858,
      "grad_norm": 1.626859426498413,
      "learning_rate": 0.00019382022471910114,
      "loss": 1.015,
      "step": 695
    },
    {
      "epoch": 0.09784900885702236,
      "grad_norm": 1.9020107984542847,
      "learning_rate": 0.00019410112359550562,
      "loss": 1.1219,
      "step": 696
    },
    {
      "epoch": 0.09798959651342612,
      "grad_norm": 1.4585378170013428,
      "learning_rate": 0.00019438202247191013,
      "loss": 1.1194,
      "step": 697
    },
    {
      "epoch": 0.0981301841698299,
      "grad_norm": 1.6336524486541748,
      "learning_rate": 0.0001946629213483146,
      "loss": 1.2226,
      "step": 698
    },
    {
      "epoch": 0.09827077182623366,
      "grad_norm": 1.5979409217834473,
      "learning_rate": 0.0001949438202247191,
      "loss": 0.9857,
      "step": 699
    },
    {
      "epoch": 0.09841135948263742,
      "grad_norm": 1.7562235593795776,
      "learning_rate": 0.00019522471910112362,
      "loss": 1.2133,
      "step": 700
    },
    {
      "epoch": 0.0985519471390412,
      "grad_norm": 1.6409391164779663,
      "learning_rate": 0.0001955056179775281,
      "loss": 1.1855,
      "step": 701
    },
    {
      "epoch": 0.09869253479544496,
      "grad_norm": 1.8380610942840576,
      "learning_rate": 0.00019578651685393258,
      "loss": 1.1689,
      "step": 702
    },
    {
      "epoch": 0.09883312245184873,
      "grad_norm": 1.833868145942688,
      "learning_rate": 0.00019606741573033708,
      "loss": 1.0304,
      "step": 703
    },
    {
      "epoch": 0.0989737101082525,
      "grad_norm": 1.953900694847107,
      "learning_rate": 0.0001963483146067416,
      "loss": 1.1952,
      "step": 704
    },
    {
      "epoch": 0.09911429776465626,
      "grad_norm": 1.9749133586883545,
      "learning_rate": 0.00019662921348314607,
      "loss": 1.0658,
      "step": 705
    },
    {
      "epoch": 0.09925488542106004,
      "grad_norm": 1.7559146881103516,
      "learning_rate": 0.00019691011235955057,
      "loss": 1.0223,
      "step": 706
    },
    {
      "epoch": 0.0993954730774638,
      "grad_norm": 1.5474345684051514,
      "learning_rate": 0.00019719101123595508,
      "loss": 1.1272,
      "step": 707
    },
    {
      "epoch": 0.09953606073386756,
      "grad_norm": 1.5787681341171265,
      "learning_rate": 0.00019747191011235956,
      "loss": 1.3885,
      "step": 708
    },
    {
      "epoch": 0.09967664839027134,
      "grad_norm": 2.1223723888397217,
      "learning_rate": 0.00019775280898876404,
      "loss": 1.0917,
      "step": 709
    },
    {
      "epoch": 0.0998172360466751,
      "grad_norm": 1.616790771484375,
      "learning_rate": 0.00019803370786516855,
      "loss": 1.0732,
      "step": 710
    },
    {
      "epoch": 0.09995782370307887,
      "grad_norm": 1.7142314910888672,
      "learning_rate": 0.00019831460674157305,
      "loss": 1.0919,
      "step": 711
    },
    {
      "epoch": 0.10009841135948264,
      "grad_norm": 1.6671388149261475,
      "learning_rate": 0.00019859550561797753,
      "loss": 1.1458,
      "step": 712
    },
    {
      "epoch": 0.1002389990158864,
      "grad_norm": 1.910531997680664,
      "learning_rate": 0.00019887640449438204,
      "loss": 1.0609,
      "step": 713
    },
    {
      "epoch": 0.10037958667229017,
      "grad_norm": 1.9436869621276855,
      "learning_rate": 0.00019915730337078652,
      "loss": 1.0601,
      "step": 714
    },
    {
      "epoch": 0.10052017432869394,
      "grad_norm": 1.6710931062698364,
      "learning_rate": 0.00019943820224719102,
      "loss": 1.1476,
      "step": 715
    },
    {
      "epoch": 0.10066076198509771,
      "grad_norm": 1.756036400794983,
      "learning_rate": 0.0001997191011235955,
      "loss": 0.8512,
      "step": 716
    },
    {
      "epoch": 0.10080134964150148,
      "grad_norm": 1.8097018003463745,
      "learning_rate": 0.0002,
      "loss": 1.1795,
      "step": 717
    },
    {
      "epoch": 0.10094193729790524,
      "grad_norm": 2.1565442085266113,
      "learning_rate": 0.00019999972978980868,
      "loss": 1.1092,
      "step": 718
    },
    {
      "epoch": 0.10108252495430901,
      "grad_norm": 1.9004820585250854,
      "learning_rate": 0.000199998919160695,
      "loss": 1.0295,
      "step": 719
    },
    {
      "epoch": 0.10122311261071278,
      "grad_norm": 1.6766606569290161,
      "learning_rate": 0.00019999756811703976,
      "loss": 1.1088,
      "step": 720
    },
    {
      "epoch": 0.10136370026711655,
      "grad_norm": 1.7533552646636963,
      "learning_rate": 0.00019999567666614424,
      "loss": 1.2282,
      "step": 721
    },
    {
      "epoch": 0.10150428792352031,
      "grad_norm": 1.7137117385864258,
      "learning_rate": 0.00019999324481823028,
      "loss": 1.1871,
      "step": 722
    },
    {
      "epoch": 0.10164487557992408,
      "grad_norm": 1.9025242328643799,
      "learning_rate": 0.0001999902725864401,
      "loss": 1.2295,
      "step": 723
    },
    {
      "epoch": 0.10178546323632785,
      "grad_norm": 2.310025930404663,
      "learning_rate": 0.00019998675998683614,
      "loss": 1.0922,
      "step": 724
    },
    {
      "epoch": 0.10192605089273162,
      "grad_norm": 1.6799606084823608,
      "learning_rate": 0.00019998270703840127,
      "loss": 1.0283,
      "step": 725
    },
    {
      "epoch": 0.10206663854913539,
      "grad_norm": 1.6531925201416016,
      "learning_rate": 0.0001999781137630385,
      "loss": 1.119,
      "step": 726
    },
    {
      "epoch": 0.10220722620553915,
      "grad_norm": 1.9506559371948242,
      "learning_rate": 0.00019997298018557073,
      "loss": 0.9707,
      "step": 727
    },
    {
      "epoch": 0.10234781386194292,
      "grad_norm": 1.9363943338394165,
      "learning_rate": 0.00019996730633374093,
      "loss": 1.1395,
      "step": 728
    },
    {
      "epoch": 0.10248840151834669,
      "grad_norm": 1.620685338973999,
      "learning_rate": 0.00019996109223821172,
      "loss": 1.012,
      "step": 729
    },
    {
      "epoch": 0.10262898917475045,
      "grad_norm": 1.6238377094268799,
      "learning_rate": 0.00019995433793256536,
      "loss": 1.1666,
      "step": 730
    },
    {
      "epoch": 0.10276957683115423,
      "grad_norm": 1.7145230770111084,
      "learning_rate": 0.00019994704345330347,
      "loss": 1.119,
      "step": 731
    },
    {
      "epoch": 0.10291016448755799,
      "grad_norm": 1.7807226181030273,
      "learning_rate": 0.0001999392088398469,
      "loss": 0.912,
      "step": 732
    },
    {
      "epoch": 0.10305075214396175,
      "grad_norm": 1.8043700456619263,
      "learning_rate": 0.00019993083413453552,
      "loss": 1.2289,
      "step": 733
    },
    {
      "epoch": 0.10319133980036553,
      "grad_norm": 1.7714390754699707,
      "learning_rate": 0.00019992191938262793,
      "loss": 1.1216,
      "step": 734
    },
    {
      "epoch": 0.10333192745676929,
      "grad_norm": 1.6482774019241333,
      "learning_rate": 0.00019991246463230125,
      "loss": 1.1996,
      "step": 735
    },
    {
      "epoch": 0.10347251511317307,
      "grad_norm": 1.609484076499939,
      "learning_rate": 0.00019990246993465096,
      "loss": 1.1219,
      "step": 736
    },
    {
      "epoch": 0.10361310276957683,
      "grad_norm": 1.9731221199035645,
      "learning_rate": 0.00019989193534369033,
      "loss": 1.0847,
      "step": 737
    },
    {
      "epoch": 0.1037536904259806,
      "grad_norm": 1.664136290550232,
      "learning_rate": 0.00019988086091635052,
      "loss": 1.0822,
      "step": 738
    },
    {
      "epoch": 0.10389427808238437,
      "grad_norm": 1.7071564197540283,
      "learning_rate": 0.00019986924671247996,
      "loss": 1.221,
      "step": 739
    },
    {
      "epoch": 0.10403486573878813,
      "grad_norm": 1.7084354162216187,
      "learning_rate": 0.00019985709279484416,
      "loss": 1.0769,
      "step": 740
    },
    {
      "epoch": 0.10417545339519191,
      "grad_norm": 1.6223692893981934,
      "learning_rate": 0.0001998443992291254,
      "loss": 0.9717,
      "step": 741
    },
    {
      "epoch": 0.10431604105159567,
      "grad_norm": 1.9365100860595703,
      "learning_rate": 0.00019983116608392228,
      "loss": 1.0572,
      "step": 742
    },
    {
      "epoch": 0.10445662870799943,
      "grad_norm": 1.8046034574508667,
      "learning_rate": 0.0001998173934307494,
      "loss": 1.0397,
      "step": 743
    },
    {
      "epoch": 0.10459721636440321,
      "grad_norm": 1.9192757606506348,
      "learning_rate": 0.00019980308134403703,
      "loss": 1.0233,
      "step": 744
    },
    {
      "epoch": 0.10473780402080697,
      "grad_norm": 1.8963937759399414,
      "learning_rate": 0.00019978822990113053,
      "loss": 1.1353,
      "step": 745
    },
    {
      "epoch": 0.10487839167721075,
      "grad_norm": 1.8415473699569702,
      "learning_rate": 0.00019977283918229022,
      "loss": 1.1226,
      "step": 746
    },
    {
      "epoch": 0.10501897933361451,
      "grad_norm": 1.7553741931915283,
      "learning_rate": 0.0001997569092706906,
      "loss": 1.0111,
      "step": 747
    },
    {
      "epoch": 0.10515956699001827,
      "grad_norm": 1.7204335927963257,
      "learning_rate": 0.0001997404402524202,
      "loss": 1.0447,
      "step": 748
    },
    {
      "epoch": 0.10530015464642205,
      "grad_norm": 1.7673938274383545,
      "learning_rate": 0.00019972343221648093,
      "loss": 0.9877,
      "step": 749
    },
    {
      "epoch": 0.10544074230282581,
      "grad_norm": 1.609743356704712,
      "learning_rate": 0.0001997058852547877,
      "loss": 1.2175,
      "step": 750
    },
    {
      "epoch": 0.10558132995922959,
      "grad_norm": 2.0031604766845703,
      "learning_rate": 0.00019968779946216784,
      "loss": 1.0592,
      "step": 751
    },
    {
      "epoch": 0.10572191761563335,
      "grad_norm": 2.078827142715454,
      "learning_rate": 0.00019966917493636073,
      "loss": 1.1613,
      "step": 752
    },
    {
      "epoch": 0.10586250527203711,
      "grad_norm": 1.7516379356384277,
      "learning_rate": 0.00019965001177801704,
      "loss": 1.2842,
      "step": 753
    },
    {
      "epoch": 0.10600309292844089,
      "grad_norm": 1.6744953393936157,
      "learning_rate": 0.00019963031009069838,
      "loss": 1.1167,
      "step": 754
    },
    {
      "epoch": 0.10614368058484465,
      "grad_norm": 1.8029953241348267,
      "learning_rate": 0.00019961006998087672,
      "loss": 1.1007,
      "step": 755
    },
    {
      "epoch": 0.10628426824124843,
      "grad_norm": 1.764657735824585,
      "learning_rate": 0.0001995892915579337,
      "loss": 1.1764,
      "step": 756
    },
    {
      "epoch": 0.10642485589765219,
      "grad_norm": 1.6398683786392212,
      "learning_rate": 0.0001995679749341602,
      "loss": 1.1326,
      "step": 757
    },
    {
      "epoch": 0.10656544355405595,
      "grad_norm": 1.682534098625183,
      "learning_rate": 0.00019954612022475557,
      "loss": 1.3239,
      "step": 758
    },
    {
      "epoch": 0.10670603121045973,
      "grad_norm": 1.9262290000915527,
      "learning_rate": 0.00019952372754782709,
      "loss": 1.2834,
      "step": 759
    },
    {
      "epoch": 0.10684661886686349,
      "grad_norm": 1.614549994468689,
      "learning_rate": 0.0001995007970243894,
      "loss": 1.303,
      "step": 760
    },
    {
      "epoch": 0.10698720652326725,
      "grad_norm": 1.81157648563385,
      "learning_rate": 0.0001994773287783637,
      "loss": 1.2104,
      "step": 761
    },
    {
      "epoch": 0.10712779417967103,
      "grad_norm": 1.6479265689849854,
      "learning_rate": 0.00019945332293657718,
      "loss": 0.9948,
      "step": 762
    },
    {
      "epoch": 0.10726838183607479,
      "grad_norm": 1.6148130893707275,
      "learning_rate": 0.00019942877962876232,
      "loss": 0.9941,
      "step": 763
    },
    {
      "epoch": 0.10740896949247856,
      "grad_norm": 1.8652920722961426,
      "learning_rate": 0.0001994036989875561,
      "loss": 1.0309,
      "step": 764
    },
    {
      "epoch": 0.10754955714888233,
      "grad_norm": 1.566698431968689,
      "learning_rate": 0.00019937808114849946,
      "loss": 1.118,
      "step": 765
    },
    {
      "epoch": 0.10769014480528609,
      "grad_norm": 1.644484043121338,
      "learning_rate": 0.00019935192625003642,
      "loss": 1.2316,
      "step": 766
    },
    {
      "epoch": 0.10783073246168987,
      "grad_norm": 1.6326709985733032,
      "learning_rate": 0.0001993252344335134,
      "loss": 1.0213,
      "step": 767
    },
    {
      "epoch": 0.10797132011809363,
      "grad_norm": 1.6306262016296387,
      "learning_rate": 0.0001992980058431784,
      "loss": 1.2588,
      "step": 768
    },
    {
      "epoch": 0.1081119077744974,
      "grad_norm": 1.6410911083221436,
      "learning_rate": 0.00019927024062618026,
      "loss": 1.0411,
      "step": 769
    },
    {
      "epoch": 0.10825249543090117,
      "grad_norm": 1.528041958808899,
      "learning_rate": 0.00019924193893256783,
      "loss": 1.2674,
      "step": 770
    },
    {
      "epoch": 0.10839308308730493,
      "grad_norm": 1.6769347190856934,
      "learning_rate": 0.00019921310091528934,
      "loss": 1.0128,
      "step": 771
    },
    {
      "epoch": 0.1085336707437087,
      "grad_norm": 1.6287634372711182,
      "learning_rate": 0.00019918372673019126,
      "loss": 1.124,
      "step": 772
    },
    {
      "epoch": 0.10867425840011247,
      "grad_norm": 1.501391053199768,
      "learning_rate": 0.0001991538165360176,
      "loss": 1.1239,
      "step": 773
    },
    {
      "epoch": 0.10881484605651624,
      "grad_norm": 1.6663458347320557,
      "learning_rate": 0.00019912337049440927,
      "loss": 1.1463,
      "step": 774
    },
    {
      "epoch": 0.10895543371292,
      "grad_norm": 1.6017011404037476,
      "learning_rate": 0.00019909238876990285,
      "loss": 1.2326,
      "step": 775
    },
    {
      "epoch": 0.10909602136932377,
      "grad_norm": 1.6592146158218384,
      "learning_rate": 0.00019906087152992986,
      "loss": 1.0684,
      "step": 776
    },
    {
      "epoch": 0.10923660902572754,
      "grad_norm": 1.8617225885391235,
      "learning_rate": 0.00019902881894481587,
      "loss": 1.1333,
      "step": 777
    },
    {
      "epoch": 0.1093771966821313,
      "grad_norm": 1.6567895412445068,
      "learning_rate": 0.00019899623118777962,
      "loss": 1.2425,
      "step": 778
    },
    {
      "epoch": 0.10951778433853508,
      "grad_norm": 1.6943678855895996,
      "learning_rate": 0.00019896310843493199,
      "loss": 1.1357,
      "step": 779
    },
    {
      "epoch": 0.10965837199493884,
      "grad_norm": 1.730482578277588,
      "learning_rate": 0.00019892945086527508,
      "loss": 1.0053,
      "step": 780
    },
    {
      "epoch": 0.1097989596513426,
      "grad_norm": 1.4923803806304932,
      "learning_rate": 0.0001988952586607013,
      "loss": 1.1075,
      "step": 781
    },
    {
      "epoch": 0.10993954730774638,
      "grad_norm": 1.544702410697937,
      "learning_rate": 0.0001988605320059922,
      "loss": 1.2568,
      "step": 782
    },
    {
      "epoch": 0.11008013496415014,
      "grad_norm": 1.6441439390182495,
      "learning_rate": 0.00019882527108881778,
      "loss": 1.1359,
      "step": 783
    },
    {
      "epoch": 0.11022072262055392,
      "grad_norm": 1.9902136325836182,
      "learning_rate": 0.0001987894760997352,
      "loss": 0.9854,
      "step": 784
    },
    {
      "epoch": 0.11036131027695768,
      "grad_norm": 1.69729483127594,
      "learning_rate": 0.00019875314723218787,
      "loss": 1.3543,
      "step": 785
    },
    {
      "epoch": 0.11050189793336145,
      "grad_norm": 1.7221208810806274,
      "learning_rate": 0.00019871628468250442,
      "loss": 1.2423,
      "step": 786
    },
    {
      "epoch": 0.11064248558976522,
      "grad_norm": 1.855310320854187,
      "learning_rate": 0.00019867888864989752,
      "loss": 1.1866,
      "step": 787
    },
    {
      "epoch": 0.11078307324616898,
      "grad_norm": 1.6051859855651855,
      "learning_rate": 0.000198640959336463,
      "loss": 1.1526,
      "step": 788
    },
    {
      "epoch": 0.11092366090257276,
      "grad_norm": 1.9342533349990845,
      "learning_rate": 0.00019860249694717862,
      "loss": 1.1782,
      "step": 789
    },
    {
      "epoch": 0.11106424855897652,
      "grad_norm": 1.8113036155700684,
      "learning_rate": 0.00019856350168990293,
      "loss": 1.2724,
      "step": 790
    },
    {
      "epoch": 0.11120483621538028,
      "grad_norm": 1.6168088912963867,
      "learning_rate": 0.00019852397377537428,
      "loss": 1.1832,
      "step": 791
    },
    {
      "epoch": 0.11134542387178406,
      "grad_norm": 1.5755318403244019,
      "learning_rate": 0.00019848391341720952,
      "loss": 1.1026,
      "step": 792
    },
    {
      "epoch": 0.11148601152818782,
      "grad_norm": 1.5602980852127075,
      "learning_rate": 0.0001984433208319031,
      "loss": 1.3621,
      "step": 793
    },
    {
      "epoch": 0.1116265991845916,
      "grad_norm": 1.7920963764190674,
      "learning_rate": 0.0001984021962388255,
      "loss": 1.0684,
      "step": 794
    },
    {
      "epoch": 0.11176718684099536,
      "grad_norm": 1.6979635953903198,
      "learning_rate": 0.0001983605398602225,
      "loss": 0.986,
      "step": 795
    },
    {
      "epoch": 0.11190777449739912,
      "grad_norm": 1.6279672384262085,
      "learning_rate": 0.0001983183519212136,
      "loss": 0.9931,
      "step": 796
    },
    {
      "epoch": 0.1120483621538029,
      "grad_norm": 1.5883936882019043,
      "learning_rate": 0.00019827563264979104,
      "loss": 1.1241,
      "step": 797
    },
    {
      "epoch": 0.11218894981020666,
      "grad_norm": 1.6425966024398804,
      "learning_rate": 0.00019823238227681847,
      "loss": 1.0232,
      "step": 798
    },
    {
      "epoch": 0.11232953746661044,
      "grad_norm": 1.7365361452102661,
      "learning_rate": 0.00019818860103602975,
      "loss": 1.2351,
      "step": 799
    },
    {
      "epoch": 0.1124701251230142,
      "grad_norm": 1.7571967840194702,
      "learning_rate": 0.00019814428916402755,
      "loss": 1.0893,
      "step": 800
    },
    {
      "epoch": 0.11261071277941796,
      "grad_norm": 1.8802690505981445,
      "learning_rate": 0.00019809944690028238,
      "loss": 1.1532,
      "step": 801
    },
    {
      "epoch": 0.11275130043582174,
      "grad_norm": 1.868325114250183,
      "learning_rate": 0.00019805407448713084,
      "loss": 1.2235,
      "step": 802
    },
    {
      "epoch": 0.1128918880922255,
      "grad_norm": 1.8131580352783203,
      "learning_rate": 0.0001980081721697748,
      "loss": 1.2256,
      "step": 803
    },
    {
      "epoch": 0.11303247574862928,
      "grad_norm": 1.8067150115966797,
      "learning_rate": 0.0001979617401962797,
      "loss": 1.1348,
      "step": 804
    },
    {
      "epoch": 0.11317306340503304,
      "grad_norm": 1.958418846130371,
      "learning_rate": 0.00019791477881757338,
      "loss": 1.0459,
      "step": 805
    },
    {
      "epoch": 0.1133136510614368,
      "grad_norm": 1.7763800621032715,
      "learning_rate": 0.0001978672882874447,
      "loss": 1.2021,
      "step": 806
    },
    {
      "epoch": 0.11345423871784058,
      "grad_norm": 1.674647331237793,
      "learning_rate": 0.0001978192688625422,
      "loss": 1.1365,
      "step": 807
    },
    {
      "epoch": 0.11359482637424434,
      "grad_norm": 1.7318278551101685,
      "learning_rate": 0.00019777072080237262,
      "loss": 1.022,
      "step": 808
    },
    {
      "epoch": 0.11373541403064812,
      "grad_norm": 1.497750997543335,
      "learning_rate": 0.00019772164436929958,
      "loss": 1.2131,
      "step": 809
    },
    {
      "epoch": 0.11387600168705188,
      "grad_norm": 1.7818105220794678,
      "learning_rate": 0.0001976720398285421,
      "loss": 1.0894,
      "step": 810
    },
    {
      "epoch": 0.11401658934345564,
      "grad_norm": 1.5615174770355225,
      "learning_rate": 0.00019762190744817323,
      "loss": 1.1634,
      "step": 811
    },
    {
      "epoch": 0.11415717699985942,
      "grad_norm": 1.9910293817520142,
      "learning_rate": 0.00019757124749911862,
      "loss": 1.1923,
      "step": 812
    },
    {
      "epoch": 0.11429776465626318,
      "grad_norm": 1.4571202993392944,
      "learning_rate": 0.00019752006025515492,
      "loss": 1.2356,
      "step": 813
    },
    {
      "epoch": 0.11443835231266694,
      "grad_norm": 1.4373924732208252,
      "learning_rate": 0.0001974683459929084,
      "loss": 1.2263,
      "step": 814
    },
    {
      "epoch": 0.11457893996907072,
      "grad_norm": 1.8476452827453613,
      "learning_rate": 0.00019741610499185357,
      "loss": 1.0835,
      "step": 815
    },
    {
      "epoch": 0.11471952762547448,
      "grad_norm": 1.623260498046875,
      "learning_rate": 0.0001973633375343114,
      "loss": 0.9936,
      "step": 816
    },
    {
      "epoch": 0.11486011528187826,
      "grad_norm": 1.5206754207611084,
      "learning_rate": 0.00019731004390544793,
      "loss": 1.1707,
      "step": 817
    },
    {
      "epoch": 0.11500070293828202,
      "grad_norm": 1.871897578239441,
      "learning_rate": 0.00019725622439327283,
      "loss": 1.1204,
      "step": 818
    },
    {
      "epoch": 0.11514129059468578,
      "grad_norm": 1.4997847080230713,
      "learning_rate": 0.00019720187928863776,
      "loss": 1.125,
      "step": 819
    },
    {
      "epoch": 0.11528187825108956,
      "grad_norm": 1.746768832206726,
      "learning_rate": 0.0001971470088852347,
      "loss": 1.0402,
      "step": 820
    },
    {
      "epoch": 0.11542246590749332,
      "grad_norm": 1.8480232954025269,
      "learning_rate": 0.00019709161347959448,
      "loss": 1.209,
      "step": 821
    },
    {
      "epoch": 0.1155630535638971,
      "grad_norm": 1.9023429155349731,
      "learning_rate": 0.0001970356933710852,
      "loss": 0.8656,
      "step": 822
    },
    {
      "epoch": 0.11570364122030086,
      "grad_norm": 2.008901357650757,
      "learning_rate": 0.0001969792488619105,
      "loss": 0.9642,
      "step": 823
    },
    {
      "epoch": 0.11584422887670462,
      "grad_norm": 1.8698229789733887,
      "learning_rate": 0.000196922280257108,
      "loss": 1.2692,
      "step": 824
    },
    {
      "epoch": 0.1159848165331084,
      "grad_norm": 1.8681187629699707,
      "learning_rate": 0.0001968647878645477,
      "loss": 1.0934,
      "step": 825
    },
    {
      "epoch": 0.11612540418951216,
      "grad_norm": 1.7884559631347656,
      "learning_rate": 0.0001968067719949302,
      "loss": 1.0187,
      "step": 826
    },
    {
      "epoch": 0.11626599184591593,
      "grad_norm": 1.6061149835586548,
      "learning_rate": 0.00019674823296178504,
      "loss": 1.157,
      "step": 827
    },
    {
      "epoch": 0.1164065795023197,
      "grad_norm": 2.1427001953125,
      "learning_rate": 0.0001966891710814691,
      "loss": 1.0998,
      "step": 828
    },
    {
      "epoch": 0.11654716715872346,
      "grad_norm": 1.782666563987732,
      "learning_rate": 0.00019662958667316483,
      "loss": 1.1009,
      "step": 829
    },
    {
      "epoch": 0.11668775481512723,
      "grad_norm": 1.4928667545318604,
      "learning_rate": 0.0001965694800588785,
      "loss": 1.0806,
      "step": 830
    },
    {
      "epoch": 0.116828342471531,
      "grad_norm": 1.6990327835083008,
      "learning_rate": 0.00019650885156343858,
      "loss": 1.0635,
      "step": 831
    },
    {
      "epoch": 0.11696893012793477,
      "grad_norm": 1.7601444721221924,
      "learning_rate": 0.00019644770151449372,
      "loss": 1.0377,
      "step": 832
    },
    {
      "epoch": 0.11710951778433853,
      "grad_norm": 1.626050353050232,
      "learning_rate": 0.0001963860302425113,
      "loss": 1.2453,
      "step": 833
    },
    {
      "epoch": 0.1172501054407423,
      "grad_norm": 1.6260331869125366,
      "learning_rate": 0.00019632383808077544,
      "loss": 1.0988,
      "step": 834
    },
    {
      "epoch": 0.11739069309714607,
      "grad_norm": 1.7578555345535278,
      "learning_rate": 0.00019626112536538524,
      "loss": 1.2342,
      "step": 835
    },
    {
      "epoch": 0.11753128075354984,
      "grad_norm": 1.7177125215530396,
      "learning_rate": 0.00019619789243525298,
      "loss": 1.1355,
      "step": 836
    },
    {
      "epoch": 0.11767186840995361,
      "grad_norm": 1.7187654972076416,
      "learning_rate": 0.00019613413963210237,
      "loss": 1.312,
      "step": 837
    },
    {
      "epoch": 0.11781245606635737,
      "grad_norm": 1.6125352382659912,
      "learning_rate": 0.0001960698673004665,
      "loss": 1.2712,
      "step": 838
    },
    {
      "epoch": 0.11795304372276114,
      "grad_norm": 1.8964959383010864,
      "learning_rate": 0.00019600507578768618,
      "loss": 1.0047,
      "step": 839
    },
    {
      "epoch": 0.11809363137916491,
      "grad_norm": 1.6479511260986328,
      "learning_rate": 0.00019593976544390792,
      "loss": 1.1585,
      "step": 840
    },
    {
      "epoch": 0.11823421903556867,
      "grad_norm": 1.8979135751724243,
      "learning_rate": 0.00019587393662208214,
      "loss": 1.1298,
      "step": 841
    },
    {
      "epoch": 0.11837480669197245,
      "grad_norm": 1.5763911008834839,
      "learning_rate": 0.0001958075896779612,
      "loss": 1.2022,
      "step": 842
    },
    {
      "epoch": 0.11851539434837621,
      "grad_norm": 1.7359899282455444,
      "learning_rate": 0.00019574072497009754,
      "loss": 1.1492,
      "step": 843
    },
    {
      "epoch": 0.11865598200477998,
      "grad_norm": 1.7291985750198364,
      "learning_rate": 0.00019567334285984167,
      "loss": 1.2491,
      "step": 844
    },
    {
      "epoch": 0.11879656966118375,
      "grad_norm": 1.6721079349517822,
      "learning_rate": 0.00019560544371134022,
      "loss": 1.1245,
      "step": 845
    },
    {
      "epoch": 0.11893715731758751,
      "grad_norm": 1.6582481861114502,
      "learning_rate": 0.00019553702789153405,
      "loss": 1.1848,
      "step": 846
    },
    {
      "epoch": 0.11907774497399129,
      "grad_norm": 2.071378231048584,
      "learning_rate": 0.0001954680957701562,
      "loss": 1.0362,
      "step": 847
    },
    {
      "epoch": 0.11921833263039505,
      "grad_norm": 1.802564263343811,
      "learning_rate": 0.00019539864771972985,
      "loss": 1.1561,
      "step": 848
    },
    {
      "epoch": 0.11935892028679881,
      "grad_norm": 1.877838134765625,
      "learning_rate": 0.00019532868411556645,
      "loss": 1.0868,
      "step": 849
    },
    {
      "epoch": 0.11949950794320259,
      "grad_norm": 1.7935469150543213,
      "learning_rate": 0.00019525820533576363,
      "loss": 1.1655,
      "step": 850
    },
    {
      "epoch": 0.11964009559960635,
      "grad_norm": 1.7193894386291504,
      "learning_rate": 0.00019518721176120301,
      "loss": 1.0828,
      "step": 851
    },
    {
      "epoch": 0.11978068325601013,
      "grad_norm": 1.633362889289856,
      "learning_rate": 0.0001951157037755484,
      "loss": 1.1482,
      "step": 852
    },
    {
      "epoch": 0.11992127091241389,
      "grad_norm": 1.6131726503372192,
      "learning_rate": 0.00019504368176524349,
      "loss": 1.1502,
      "step": 853
    },
    {
      "epoch": 0.12006185856881765,
      "grad_norm": 1.5067028999328613,
      "learning_rate": 0.00019497114611950988,
      "loss": 1.2548,
      "step": 854
    },
    {
      "epoch": 0.12020244622522143,
      "grad_norm": 1.469435691833496,
      "learning_rate": 0.00019489809723034505,
      "loss": 1.1741,
      "step": 855
    },
    {
      "epoch": 0.12034303388162519,
      "grad_norm": 1.5044822692871094,
      "learning_rate": 0.00019482453549252003,
      "loss": 1.031,
      "step": 856
    },
    {
      "epoch": 0.12048362153802897,
      "grad_norm": 1.7028956413269043,
      "learning_rate": 0.00019475046130357747,
      "loss": 1.1577,
      "step": 857
    },
    {
      "epoch": 0.12062420919443273,
      "grad_norm": 1.6984260082244873,
      "learning_rate": 0.00019467587506382937,
      "loss": 1.0575,
      "step": 858
    },
    {
      "epoch": 0.12076479685083649,
      "grad_norm": 1.8840218782424927,
      "learning_rate": 0.000194600777176355,
      "loss": 0.9721,
      "step": 859
    },
    {
      "epoch": 0.12090538450724027,
      "grad_norm": 1.6320379972457886,
      "learning_rate": 0.0001945251680469986,
      "loss": 1.1231,
      "step": 860
    },
    {
      "epoch": 0.12104597216364403,
      "grad_norm": 1.59637451171875,
      "learning_rate": 0.00019444904808436737,
      "loss": 1.1682,
      "step": 861
    },
    {
      "epoch": 0.1211865598200478,
      "grad_norm": 2.23915958404541,
      "learning_rate": 0.00019437241769982907,
      "loss": 0.9961,
      "step": 862
    },
    {
      "epoch": 0.12132714747645157,
      "grad_norm": 1.8534209728240967,
      "learning_rate": 0.00019429527730750997,
      "loss": 0.9723,
      "step": 863
    },
    {
      "epoch": 0.12146773513285533,
      "grad_norm": 2.257800817489624,
      "learning_rate": 0.0001942176273242924,
      "loss": 1.1227,
      "step": 864
    },
    {
      "epoch": 0.12160832278925911,
      "grad_norm": 1.6702897548675537,
      "learning_rate": 0.00019413946816981275,
      "loss": 1.1769,
      "step": 865
    },
    {
      "epoch": 0.12174891044566287,
      "grad_norm": 1.6733086109161377,
      "learning_rate": 0.000194060800266459,
      "loss": 1.154,
      "step": 866
    },
    {
      "epoch": 0.12188949810206663,
      "grad_norm": 1.6607953310012817,
      "learning_rate": 0.0001939816240393685,
      "loss": 1.1568,
      "step": 867
    },
    {
      "epoch": 0.12203008575847041,
      "grad_norm": 1.886461615562439,
      "learning_rate": 0.0001939019399164258,
      "loss": 1.1869,
      "step": 868
    },
    {
      "epoch": 0.12217067341487417,
      "grad_norm": 1.61186945438385,
      "learning_rate": 0.0001938217483282601,
      "loss": 1.2545,
      "step": 869
    },
    {
      "epoch": 0.12231126107127795,
      "grad_norm": 1.6861602067947388,
      "learning_rate": 0.00019374104970824302,
      "loss": 1.1052,
      "step": 870
    },
    {
      "epoch": 0.12245184872768171,
      "grad_norm": 1.6776971817016602,
      "learning_rate": 0.0001936598444924865,
      "loss": 1.1183,
      "step": 871
    },
    {
      "epoch": 0.12259243638408547,
      "grad_norm": 1.751807689666748,
      "learning_rate": 0.00019357813311983994,
      "loss": 1.0871,
      "step": 872
    },
    {
      "epoch": 0.12273302404048925,
      "grad_norm": 1.6255249977111816,
      "learning_rate": 0.00019349591603188833,
      "loss": 1.2106,
      "step": 873
    },
    {
      "epoch": 0.12287361169689301,
      "grad_norm": 1.804284930229187,
      "learning_rate": 0.00019341319367294953,
      "loss": 1.0673,
      "step": 874
    },
    {
      "epoch": 0.12301419935329679,
      "grad_norm": 1.7132014036178589,
      "learning_rate": 0.00019332996649007206,
      "loss": 1.194,
      "step": 875
    },
    {
      "epoch": 0.12315478700970055,
      "grad_norm": 1.8012069463729858,
      "learning_rate": 0.00019324623493303258,
      "loss": 1.1137,
      "step": 876
    },
    {
      "epoch": 0.12329537466610431,
      "grad_norm": 1.5478216409683228,
      "learning_rate": 0.00019316199945433344,
      "loss": 1.1363,
      "step": 877
    },
    {
      "epoch": 0.12343596232250809,
      "grad_norm": 1.6551463603973389,
      "learning_rate": 0.00019307726050920042,
      "loss": 1.2638,
      "step": 878
    },
    {
      "epoch": 0.12357654997891185,
      "grad_norm": 1.486127495765686,
      "learning_rate": 0.00019299201855558,
      "loss": 1.165,
      "step": 879
    },
    {
      "epoch": 0.12371713763531562,
      "grad_norm": 1.5192784070968628,
      "learning_rate": 0.00019290627405413704,
      "loss": 0.9008,
      "step": 880
    },
    {
      "epoch": 0.12385772529171939,
      "grad_norm": 1.466822624206543,
      "learning_rate": 0.00019282002746825235,
      "loss": 1.1012,
      "step": 881
    },
    {
      "epoch": 0.12399831294812315,
      "grad_norm": 1.637532353401184,
      "learning_rate": 0.00019273327926402008,
      "loss": 0.9136,
      "step": 882
    },
    {
      "epoch": 0.12413890060452692,
      "grad_norm": 1.503531575202942,
      "learning_rate": 0.00019264602991024514,
      "loss": 1.3157,
      "step": 883
    },
    {
      "epoch": 0.12427948826093069,
      "grad_norm": 1.6211224794387817,
      "learning_rate": 0.00019255827987844088,
      "loss": 1.0263,
      "step": 884
    },
    {
      "epoch": 0.12442007591733446,
      "grad_norm": 1.617193341255188,
      "learning_rate": 0.00019247002964282634,
      "loss": 1.2933,
      "step": 885
    },
    {
      "epoch": 0.12456066357373823,
      "grad_norm": 1.527893304824829,
      "learning_rate": 0.00019238127968032378,
      "loss": 1.1301,
      "step": 886
    },
    {
      "epoch": 0.12470125123014199,
      "grad_norm": 1.9666965007781982,
      "learning_rate": 0.00019229203047055609,
      "loss": 1.0104,
      "step": 887
    },
    {
      "epoch": 0.12484183888654576,
      "grad_norm": 1.6190413236618042,
      "learning_rate": 0.00019220228249584414,
      "loss": 1.3377,
      "step": 888
    },
    {
      "epoch": 0.12498242654294953,
      "grad_norm": 1.5455448627471924,
      "learning_rate": 0.00019211203624120438,
      "loss": 1.0578,
      "step": 889
    },
    {
      "epoch": 0.1251230141993533,
      "grad_norm": 1.5654734373092651,
      "learning_rate": 0.00019202129219434588,
      "loss": 1.0732,
      "step": 890
    },
    {
      "epoch": 0.12526360185575705,
      "grad_norm": 1.7902004718780518,
      "learning_rate": 0.00019193005084566797,
      "loss": 1.1163,
      "step": 891
    },
    {
      "epoch": 0.12540418951216084,
      "grad_norm": 1.504434585571289,
      "learning_rate": 0.00019183831268825756,
      "loss": 1.2872,
      "step": 892
    },
    {
      "epoch": 0.1255447771685646,
      "grad_norm": 1.8008671998977661,
      "learning_rate": 0.0001917460782178863,
      "loss": 1.1406,
      "step": 893
    },
    {
      "epoch": 0.12568536482496837,
      "grad_norm": 1.9844818115234375,
      "learning_rate": 0.00019165334793300808,
      "loss": 1.2337,
      "step": 894
    },
    {
      "epoch": 0.12582595248137213,
      "grad_norm": 1.9781428575515747,
      "learning_rate": 0.00019156012233475625,
      "loss": 1.2473,
      "step": 895
    },
    {
      "epoch": 0.1259665401377759,
      "grad_norm": 1.814671277999878,
      "learning_rate": 0.00019146640192694095,
      "loss": 1.105,
      "step": 896
    },
    {
      "epoch": 0.12610712779417968,
      "grad_norm": 1.8396697044372559,
      "learning_rate": 0.00019137218721604636,
      "loss": 1.2127,
      "step": 897
    },
    {
      "epoch": 0.12624771545058344,
      "grad_norm": 1.6790088415145874,
      "learning_rate": 0.000191277478711228,
      "loss": 1.0319,
      "step": 898
    },
    {
      "epoch": 0.1263883031069872,
      "grad_norm": 1.5947331190109253,
      "learning_rate": 0.00019118227692430995,
      "loss": 1.094,
      "step": 899
    },
    {
      "epoch": 0.12652889076339097,
      "grad_norm": 1.7621901035308838,
      "learning_rate": 0.00019108658236978204,
      "loss": 1.1414,
      "step": 900
    },
    {
      "epoch": 0.12666947841979473,
      "grad_norm": 1.664345622062683,
      "learning_rate": 0.00019099039556479713,
      "loss": 1.2783,
      "step": 901
    },
    {
      "epoch": 0.12681006607619852,
      "grad_norm": 1.945640206336975,
      "learning_rate": 0.00019089371702916836,
      "loss": 0.9035,
      "step": 902
    },
    {
      "epoch": 0.12695065373260228,
      "grad_norm": 2.161634683609009,
      "learning_rate": 0.00019079654728536622,
      "loss": 1.1437,
      "step": 903
    },
    {
      "epoch": 0.12709124138900604,
      "grad_norm": 1.7287232875823975,
      "learning_rate": 0.00019069888685851582,
      "loss": 1.0271,
      "step": 904
    },
    {
      "epoch": 0.1272318290454098,
      "grad_norm": 1.3658607006072998,
      "learning_rate": 0.00019060073627639402,
      "loss": 1.1608,
      "step": 905
    },
    {
      "epoch": 0.12737241670181357,
      "grad_norm": 1.637449026107788,
      "learning_rate": 0.00019050209606942653,
      "loss": 1.0364,
      "step": 906
    },
    {
      "epoch": 0.12751300435821736,
      "grad_norm": 1.5898149013519287,
      "learning_rate": 0.00019040296677068518,
      "loss": 1.0067,
      "step": 907
    },
    {
      "epoch": 0.12765359201462112,
      "grad_norm": 1.7914780378341675,
      "learning_rate": 0.0001903033489158849,
      "loss": 1.1886,
      "step": 908
    },
    {
      "epoch": 0.12779417967102488,
      "grad_norm": 1.9881715774536133,
      "learning_rate": 0.00019020324304338087,
      "loss": 1.0153,
      "step": 909
    },
    {
      "epoch": 0.12793476732742864,
      "grad_norm": 1.625450611114502,
      "learning_rate": 0.00019010264969416562,
      "loss": 1.1228,
      "step": 910
    },
    {
      "epoch": 0.1280753549838324,
      "grad_norm": 1.493229866027832,
      "learning_rate": 0.00019000156941186613,
      "loss": 1.01,
      "step": 911
    },
    {
      "epoch": 0.1282159426402362,
      "grad_norm": 1.6298959255218506,
      "learning_rate": 0.00018990000274274086,
      "loss": 1.2154,
      "step": 912
    },
    {
      "epoch": 0.12835653029663996,
      "grad_norm": 1.6940168142318726,
      "learning_rate": 0.00018979795023567675,
      "loss": 0.9648,
      "step": 913
    },
    {
      "epoch": 0.12849711795304372,
      "grad_norm": 1.5592472553253174,
      "learning_rate": 0.0001896954124421864,
      "loss": 1.0738,
      "step": 914
    },
    {
      "epoch": 0.12863770560944748,
      "grad_norm": 1.7209265232086182,
      "learning_rate": 0.0001895923899164049,
      "loss": 1.2261,
      "step": 915
    },
    {
      "epoch": 0.12877829326585125,
      "grad_norm": 1.6379361152648926,
      "learning_rate": 0.000189488883215087,
      "loss": 1.1198,
      "step": 916
    },
    {
      "epoch": 0.12891888092225504,
      "grad_norm": 1.7611297369003296,
      "learning_rate": 0.000189384892897604,
      "loss": 1.3228,
      "step": 917
    },
    {
      "epoch": 0.1290594685786588,
      "grad_norm": 1.6604681015014648,
      "learning_rate": 0.00018928041952594076,
      "loss": 1.1154,
      "step": 918
    },
    {
      "epoch": 0.12920005623506256,
      "grad_norm": 1.5681349039077759,
      "learning_rate": 0.00018917546366469274,
      "loss": 1.0363,
      "step": 919
    },
    {
      "epoch": 0.12934064389146632,
      "grad_norm": 1.6858689785003662,
      "learning_rate": 0.00018907002588106276,
      "loss": 1.1757,
      "step": 920
    },
    {
      "epoch": 0.12948123154787008,
      "grad_norm": 1.7433886528015137,
      "learning_rate": 0.00018896410674485809,
      "loss": 1.17,
      "step": 921
    },
    {
      "epoch": 0.12962181920427387,
      "grad_norm": 1.6968393325805664,
      "learning_rate": 0.00018885770682848733,
      "loss": 1.2699,
      "step": 922
    },
    {
      "epoch": 0.12976240686067764,
      "grad_norm": 1.9381352663040161,
      "learning_rate": 0.00018875082670695732,
      "loss": 1.1441,
      "step": 923
    },
    {
      "epoch": 0.1299029945170814,
      "grad_norm": 2.0201773643493652,
      "learning_rate": 0.00018864346695787005,
      "loss": 0.9518,
      "step": 924
    },
    {
      "epoch": 0.13004358217348516,
      "grad_norm": 1.5604139566421509,
      "learning_rate": 0.00018853562816141943,
      "loss": 1.2644,
      "step": 925
    },
    {
      "epoch": 0.13018416982988892,
      "grad_norm": 2.0192368030548096,
      "learning_rate": 0.00018842731090038835,
      "loss": 1.1635,
      "step": 926
    },
    {
      "epoch": 0.1303247574862927,
      "grad_norm": 1.6466028690338135,
      "learning_rate": 0.00018831851576014535,
      "loss": 1.1925,
      "step": 927
    },
    {
      "epoch": 0.13046534514269648,
      "grad_norm": 2.528287410736084,
      "learning_rate": 0.0001882092433286415,
      "loss": 1.0843,
      "step": 928
    },
    {
      "epoch": 0.13060593279910024,
      "grad_norm": 1.4437416791915894,
      "learning_rate": 0.00018809949419640735,
      "loss": 1.1253,
      "step": 929
    },
    {
      "epoch": 0.130746520455504,
      "grad_norm": 1.61188542842865,
      "learning_rate": 0.00018798926895654958,
      "loss": 1.0088,
      "step": 930
    },
    {
      "epoch": 0.13088710811190776,
      "grad_norm": 1.7455470561981201,
      "learning_rate": 0.00018787856820474779,
      "loss": 1.1054,
      "step": 931
    },
    {
      "epoch": 0.13102769576831155,
      "grad_norm": 1.8056749105453491,
      "learning_rate": 0.0001877673925392515,
      "loss": 1.1488,
      "step": 932
    },
    {
      "epoch": 0.13116828342471532,
      "grad_norm": 1.7331604957580566,
      "learning_rate": 0.00018765574256087656,
      "loss": 1.271,
      "step": 933
    },
    {
      "epoch": 0.13130887108111908,
      "grad_norm": 1.66908860206604,
      "learning_rate": 0.0001875436188730023,
      "loss": 1.1986,
      "step": 934
    },
    {
      "epoch": 0.13144945873752284,
      "grad_norm": 1.5953601598739624,
      "learning_rate": 0.00018743102208156794,
      "loss": 1.0438,
      "step": 935
    },
    {
      "epoch": 0.1315900463939266,
      "grad_norm": 1.675052523612976,
      "learning_rate": 0.00018731795279506954,
      "loss": 0.9654,
      "step": 936
    },
    {
      "epoch": 0.1317306340503304,
      "grad_norm": 1.8025383949279785,
      "learning_rate": 0.00018720441162455652,
      "loss": 1.0812,
      "step": 937
    },
    {
      "epoch": 0.13187122170673415,
      "grad_norm": 1.9278911352157593,
      "learning_rate": 0.0001870903991836285,
      "loss": 1.1794,
      "step": 938
    },
    {
      "epoch": 0.13201180936313792,
      "grad_norm": 1.733852505683899,
      "learning_rate": 0.00018697591608843195,
      "loss": 1.2703,
      "step": 939
    },
    {
      "epoch": 0.13215239701954168,
      "grad_norm": 1.5474668741226196,
      "learning_rate": 0.0001868609629576569,
      "loss": 1.1347,
      "step": 940
    },
    {
      "epoch": 0.13229298467594544,
      "grad_norm": 1.7299009561538696,
      "learning_rate": 0.00018674554041253345,
      "loss": 1.2138,
      "step": 941
    },
    {
      "epoch": 0.13243357233234923,
      "grad_norm": 1.7482271194458008,
      "learning_rate": 0.00018662964907682859,
      "loss": 1.2078,
      "step": 942
    },
    {
      "epoch": 0.132574159988753,
      "grad_norm": 1.604552984237671,
      "learning_rate": 0.0001865132895768427,
      "loss": 1.3709,
      "step": 943
    },
    {
      "epoch": 0.13271474764515676,
      "grad_norm": 1.692020058631897,
      "learning_rate": 0.00018639646254140627,
      "loss": 1.2802,
      "step": 944
    },
    {
      "epoch": 0.13285533530156052,
      "grad_norm": 1.678795576095581,
      "learning_rate": 0.00018627916860187638,
      "loss": 0.997,
      "step": 945
    },
    {
      "epoch": 0.13299592295796428,
      "grad_norm": 1.5958257913589478,
      "learning_rate": 0.00018616140839213338,
      "loss": 1.18,
      "step": 946
    },
    {
      "epoch": 0.13313651061436807,
      "grad_norm": 1.6167479753494263,
      "learning_rate": 0.00018604318254857746,
      "loss": 1.1346,
      "step": 947
    },
    {
      "epoch": 0.13327709827077183,
      "grad_norm": 1.5460007190704346,
      "learning_rate": 0.0001859244917101252,
      "loss": 1.0497,
      "step": 948
    },
    {
      "epoch": 0.1334176859271756,
      "grad_norm": 1.7104015350341797,
      "learning_rate": 0.00018580533651820603,
      "loss": 1.285,
      "step": 949
    },
    {
      "epoch": 0.13355827358357936,
      "grad_norm": 2.072925329208374,
      "learning_rate": 0.00018568571761675893,
      "loss": 1.0927,
      "step": 950
    },
    {
      "epoch": 0.13369886123998312,
      "grad_norm": 1.6212879419326782,
      "learning_rate": 0.00018556563565222885,
      "loss": 1.1707,
      "step": 951
    },
    {
      "epoch": 0.1338394488963869,
      "grad_norm": 1.8161336183547974,
      "learning_rate": 0.0001854450912735631,
      "loss": 1.1267,
      "step": 952
    },
    {
      "epoch": 0.13398003655279067,
      "grad_norm": 1.6966172456741333,
      "learning_rate": 0.0001853240851322082,
      "loss": 1.057,
      "step": 953
    },
    {
      "epoch": 0.13412062420919443,
      "grad_norm": 1.9814555644989014,
      "learning_rate": 0.00018520261788210595,
      "loss": 1.0283,
      "step": 954
    },
    {
      "epoch": 0.1342612118655982,
      "grad_norm": 1.8035095930099487,
      "learning_rate": 0.00018508069017969008,
      "loss": 1.0,
      "step": 955
    },
    {
      "epoch": 0.13440179952200196,
      "grad_norm": 1.4374579191207886,
      "learning_rate": 0.0001849583026838828,
      "loss": 1.0989,
      "step": 956
    },
    {
      "epoch": 0.13454238717840575,
      "grad_norm": 1.8490337133407593,
      "learning_rate": 0.0001848354560560911,
      "loss": 0.9351,
      "step": 957
    },
    {
      "epoch": 0.1346829748348095,
      "grad_norm": 1.6642589569091797,
      "learning_rate": 0.00018471215096020315,
      "loss": 1.0083,
      "step": 958
    },
    {
      "epoch": 0.13482356249121327,
      "grad_norm": 1.6418250799179077,
      "learning_rate": 0.00018458838806258485,
      "loss": 1.0447,
      "step": 959
    },
    {
      "epoch": 0.13496415014761703,
      "grad_norm": 1.6546956300735474,
      "learning_rate": 0.0001844641680320761,
      "loss": 1.123,
      "step": 960
    },
    {
      "epoch": 0.1351047378040208,
      "grad_norm": 1.6175512075424194,
      "learning_rate": 0.00018433949153998729,
      "loss": 1.0804,
      "step": 961
    },
    {
      "epoch": 0.1352453254604246,
      "grad_norm": 1.7042362689971924,
      "learning_rate": 0.00018421435926009556,
      "loss": 1.0515,
      "step": 962
    },
    {
      "epoch": 0.13538591311682835,
      "grad_norm": 1.3553853034973145,
      "learning_rate": 0.0001840887718686413,
      "loss": 1.1896,
      "step": 963
    },
    {
      "epoch": 0.1355265007732321,
      "grad_norm": 1.5464295148849487,
      "learning_rate": 0.00018396273004432433,
      "loss": 1.1303,
      "step": 964
    },
    {
      "epoch": 0.13566708842963587,
      "grad_norm": 1.8611130714416504,
      "learning_rate": 0.0001838362344683004,
      "loss": 1.1515,
      "step": 965
    },
    {
      "epoch": 0.13580767608603964,
      "grad_norm": 1.7415069341659546,
      "learning_rate": 0.00018370928582417734,
      "loss": 1.126,
      "step": 966
    },
    {
      "epoch": 0.13594826374244343,
      "grad_norm": 1.5930153131484985,
      "learning_rate": 0.00018358188479801157,
      "loss": 1.3169,
      "step": 967
    },
    {
      "epoch": 0.1360888513988472,
      "grad_norm": 1.8581985235214233,
      "learning_rate": 0.00018345403207830412,
      "loss": 1.0928,
      "step": 968
    },
    {
      "epoch": 0.13622943905525095,
      "grad_norm": 1.7309702634811401,
      "learning_rate": 0.0001833257283559972,
      "loss": 1.1546,
      "step": 969
    },
    {
      "epoch": 0.1363700267116547,
      "grad_norm": 1.7902735471725464,
      "learning_rate": 0.00018319697432447025,
      "loss": 1.0919,
      "step": 970
    },
    {
      "epoch": 0.13651061436805847,
      "grad_norm": 1.5937029123306274,
      "learning_rate": 0.00018306777067953633,
      "loss": 1.2703,
      "step": 971
    },
    {
      "epoch": 0.13665120202446226,
      "grad_norm": 1.6938567161560059,
      "learning_rate": 0.00018293811811943825,
      "loss": 1.1316,
      "step": 972
    },
    {
      "epoch": 0.13679178968086603,
      "grad_norm": 1.8981819152832031,
      "learning_rate": 0.00018280801734484484,
      "loss": 1.1425,
      "step": 973
    },
    {
      "epoch": 0.1369323773372698,
      "grad_norm": 1.5287666320800781,
      "learning_rate": 0.00018267746905884725,
      "loss": 1.1709,
      "step": 974
    },
    {
      "epoch": 0.13707296499367355,
      "grad_norm": 1.5672818422317505,
      "learning_rate": 0.00018254647396695505,
      "loss": 1.2134,
      "step": 975
    },
    {
      "epoch": 0.13721355265007731,
      "grad_norm": 1.5814554691314697,
      "learning_rate": 0.00018241503277709238,
      "loss": 1.0446,
      "step": 976
    },
    {
      "epoch": 0.1373541403064811,
      "grad_norm": 1.6790499687194824,
      "learning_rate": 0.0001822831461995942,
      "loss": 1.0445,
      "step": 977
    },
    {
      "epoch": 0.13749472796288487,
      "grad_norm": 1.7274872064590454,
      "learning_rate": 0.00018215081494720248,
      "loss": 1.16,
      "step": 978
    },
    {
      "epoch": 0.13763531561928863,
      "grad_norm": 1.8139612674713135,
      "learning_rate": 0.0001820180397350623,
      "loss": 1.0569,
      "step": 979
    },
    {
      "epoch": 0.1377759032756924,
      "grad_norm": 1.64694082736969,
      "learning_rate": 0.0001818848212807179,
      "loss": 1.0438,
      "step": 980
    },
    {
      "epoch": 0.13791649093209615,
      "grad_norm": 1.706474781036377,
      "learning_rate": 0.00018175116030410906,
      "loss": 1.2435,
      "step": 981
    },
    {
      "epoch": 0.13805707858849994,
      "grad_norm": 1.8787643909454346,
      "learning_rate": 0.0001816170575275669,
      "loss": 1.0368,
      "step": 982
    },
    {
      "epoch": 0.1381976662449037,
      "grad_norm": 1.771226167678833,
      "learning_rate": 0.00018148251367581012,
      "loss": 1.1704,
      "step": 983
    },
    {
      "epoch": 0.13833825390130747,
      "grad_norm": 1.623578429222107,
      "learning_rate": 0.00018134752947594117,
      "loss": 1.0215,
      "step": 984
    },
    {
      "epoch": 0.13847884155771123,
      "grad_norm": 2.0968096256256104,
      "learning_rate": 0.00018121210565744215,
      "loss": 0.9641,
      "step": 985
    },
    {
      "epoch": 0.138619429214115,
      "grad_norm": 1.71736478805542,
      "learning_rate": 0.00018107624295217098,
      "loss": 1.0783,
      "step": 986
    },
    {
      "epoch": 0.13876001687051878,
      "grad_norm": 1.7923246622085571,
      "learning_rate": 0.00018093994209435744,
      "loss": 1.1529,
      "step": 987
    },
    {
      "epoch": 0.13890060452692254,
      "grad_norm": 1.4781681299209595,
      "learning_rate": 0.00018080320382059907,
      "loss": 1.1552,
      "step": 988
    },
    {
      "epoch": 0.1390411921833263,
      "grad_norm": 1.6792172193527222,
      "learning_rate": 0.0001806660288698575,
      "loss": 1.1742,
      "step": 989
    },
    {
      "epoch": 0.13918177983973007,
      "grad_norm": 1.6657360792160034,
      "learning_rate": 0.000180528417983454,
      "loss": 1.1085,
      "step": 990
    },
    {
      "epoch": 0.13932236749613383,
      "grad_norm": 1.8550443649291992,
      "learning_rate": 0.0001803903719050659,
      "loss": 1.1547,
      "step": 991
    },
    {
      "epoch": 0.13946295515253762,
      "grad_norm": 1.6780256032943726,
      "learning_rate": 0.0001802518913807224,
      "loss": 1.2343,
      "step": 992
    },
    {
      "epoch": 0.13960354280894138,
      "grad_norm": 1.613356590270996,
      "learning_rate": 0.00018011297715880037,
      "loss": 1.1944,
      "step": 993
    },
    {
      "epoch": 0.13974413046534515,
      "grad_norm": 1.6818861961364746,
      "learning_rate": 0.00017997362999002067,
      "loss": 1.2028,
      "step": 994
    },
    {
      "epoch": 0.1398847181217489,
      "grad_norm": 1.651341438293457,
      "learning_rate": 0.00017983385062744374,
      "loss": 1.1549,
      "step": 995
    },
    {
      "epoch": 0.14002530577815267,
      "grad_norm": 1.5224683284759521,
      "learning_rate": 0.0001796936398264658,
      "loss": 1.2329,
      "step": 996
    },
    {
      "epoch": 0.14016589343455643,
      "grad_norm": 1.7024986743927002,
      "learning_rate": 0.00017955299834481454,
      "loss": 1.0476,
      "step": 997
    },
    {
      "epoch": 0.14030648109096022,
      "grad_norm": 1.5648019313812256,
      "learning_rate": 0.00017941192694254522,
      "loss": 1.0987,
      "step": 998
    },
    {
      "epoch": 0.14044706874736398,
      "grad_norm": 1.974932312965393,
      "learning_rate": 0.00017927042638203649,
      "loss": 1.1048,
      "step": 999
    },
    {
      "epoch": 0.14058765640376775,
      "grad_norm": 1.4856538772583008,
      "learning_rate": 0.00017912849742798617,
      "loss": 1.0872,
      "step": 1000
    },
    {
      "epoch": 0.14058765640376775,
      "eval_loss": 1.1844638586044312,
      "eval_runtime": 772.6922,
      "eval_samples_per_second": 16.366,
      "eval_steps_per_second": 8.183,
      "step": 1000
    },
    {
      "epoch": 0.1407282440601715,
      "grad_norm": 1.4653431177139282,
      "learning_rate": 0.00017898614084740726,
      "loss": 1.2353,
      "step": 1001
    },
    {
      "epoch": 0.14086883171657527,
      "grad_norm": 1.6938291788101196,
      "learning_rate": 0.00017884335740962377,
      "loss": 1.1338,
      "step": 1002
    },
    {
      "epoch": 0.14100941937297906,
      "grad_norm": 1.9971638917922974,
      "learning_rate": 0.00017870014788626647,
      "loss": 0.9759,
      "step": 1003
    },
    {
      "epoch": 0.14115000702938282,
      "grad_norm": 1.690354585647583,
      "learning_rate": 0.00017855651305126883,
      "loss": 1.1758,
      "step": 1004
    },
    {
      "epoch": 0.14129059468578659,
      "grad_norm": 1.8836556673049927,
      "learning_rate": 0.00017841245368086276,
      "loss": 1.2231,
      "step": 1005
    },
    {
      "epoch": 0.14143118234219035,
      "grad_norm": 1.8976904153823853,
      "learning_rate": 0.00017826797055357449,
      "loss": 1.1497,
      "step": 1006
    },
    {
      "epoch": 0.1415717699985941,
      "grad_norm": 1.6494817733764648,
      "learning_rate": 0.00017812306445022025,
      "loss": 1.0998,
      "step": 1007
    },
    {
      "epoch": 0.1417123576549979,
      "grad_norm": 1.685690999031067,
      "learning_rate": 0.00017797773615390222,
      "loss": 1.0268,
      "step": 1008
    },
    {
      "epoch": 0.14185294531140166,
      "grad_norm": 1.9979948997497559,
      "learning_rate": 0.00017783198645000402,
      "loss": 1.0126,
      "step": 1009
    },
    {
      "epoch": 0.14199353296780542,
      "grad_norm": 1.6119532585144043,
      "learning_rate": 0.0001776858161261869,
      "loss": 1.006,
      "step": 1010
    },
    {
      "epoch": 0.1421341206242092,
      "grad_norm": 1.629258394241333,
      "learning_rate": 0.00017753922597238498,
      "loss": 1.13,
      "step": 1011
    },
    {
      "epoch": 0.14227470828061295,
      "grad_norm": 1.7351263761520386,
      "learning_rate": 0.00017739221678080136,
      "loss": 1.0525,
      "step": 1012
    },
    {
      "epoch": 0.14241529593701674,
      "grad_norm": 1.448646068572998,
      "learning_rate": 0.00017724478934590373,
      "loss": 1.0499,
      "step": 1013
    },
    {
      "epoch": 0.1425558835934205,
      "grad_norm": 1.949147343635559,
      "learning_rate": 0.00017709694446441988,
      "loss": 1.0475,
      "step": 1014
    },
    {
      "epoch": 0.14269647124982426,
      "grad_norm": 1.5671868324279785,
      "learning_rate": 0.0001769486829353338,
      "loss": 1.0901,
      "step": 1015
    },
    {
      "epoch": 0.14283705890622803,
      "grad_norm": 1.5275238752365112,
      "learning_rate": 0.00017680000555988094,
      "loss": 1.2857,
      "step": 1016
    },
    {
      "epoch": 0.1429776465626318,
      "grad_norm": 1.558746099472046,
      "learning_rate": 0.00017665091314154413,
      "loss": 1.1042,
      "step": 1017
    },
    {
      "epoch": 0.14311823421903558,
      "grad_norm": 1.721778392791748,
      "learning_rate": 0.0001765014064860493,
      "loss": 1.0334,
      "step": 1018
    },
    {
      "epoch": 0.14325882187543934,
      "grad_norm": 1.903743028640747,
      "learning_rate": 0.00017635148640136078,
      "loss": 1.0948,
      "step": 1019
    },
    {
      "epoch": 0.1433994095318431,
      "grad_norm": 1.9835774898529053,
      "learning_rate": 0.0001762011536976773,
      "loss": 1.116,
      "step": 1020
    },
    {
      "epoch": 0.14353999718824687,
      "grad_norm": 1.7087197303771973,
      "learning_rate": 0.0001760504091874274,
      "loss": 1.2273,
      "step": 1021
    },
    {
      "epoch": 0.14368058484465063,
      "grad_norm": 1.5870301723480225,
      "learning_rate": 0.00017589925368526524,
      "loss": 1.1477,
      "step": 1022
    },
    {
      "epoch": 0.14382117250105442,
      "grad_norm": 1.7587378025054932,
      "learning_rate": 0.00017574768800806583,
      "loss": 1.138,
      "step": 1023
    },
    {
      "epoch": 0.14396176015745818,
      "grad_norm": 1.647207260131836,
      "learning_rate": 0.0001755957129749211,
      "loss": 1.1336,
      "step": 1024
    },
    {
      "epoch": 0.14410234781386194,
      "grad_norm": 1.5192877054214478,
      "learning_rate": 0.00017544332940713499,
      "loss": 1.2482,
      "step": 1025
    },
    {
      "epoch": 0.1442429354702657,
      "grad_norm": 1.7180424928665161,
      "learning_rate": 0.00017529053812821946,
      "loss": 0.9861,
      "step": 1026
    },
    {
      "epoch": 0.14438352312666947,
      "grad_norm": 1.821111798286438,
      "learning_rate": 0.00017513733996388968,
      "loss": 1.3363,
      "step": 1027
    },
    {
      "epoch": 0.14452411078307326,
      "grad_norm": 1.5975958108901978,
      "learning_rate": 0.00017498373574205975,
      "loss": 0.9352,
      "step": 1028
    },
    {
      "epoch": 0.14466469843947702,
      "grad_norm": 1.9245553016662598,
      "learning_rate": 0.00017482972629283822,
      "loss": 1.3045,
      "step": 1029
    },
    {
      "epoch": 0.14480528609588078,
      "grad_norm": 1.3543527126312256,
      "learning_rate": 0.00017467531244852352,
      "loss": 1.1293,
      "step": 1030
    },
    {
      "epoch": 0.14494587375228454,
      "grad_norm": 1.6222951412200928,
      "learning_rate": 0.00017452049504359954,
      "loss": 1.3402,
      "step": 1031
    },
    {
      "epoch": 0.1450864614086883,
      "grad_norm": 1.3777356147766113,
      "learning_rate": 0.00017436527491473115,
      "loss": 1.2337,
      "step": 1032
    },
    {
      "epoch": 0.1452270490650921,
      "grad_norm": 1.3549246788024902,
      "learning_rate": 0.00017420965290075948,
      "loss": 1.1743,
      "step": 1033
    },
    {
      "epoch": 0.14536763672149586,
      "grad_norm": 1.5738165378570557,
      "learning_rate": 0.00017405362984269763,
      "loss": 1.0522,
      "step": 1034
    },
    {
      "epoch": 0.14550822437789962,
      "grad_norm": 1.5752593278884888,
      "learning_rate": 0.00017389720658372607,
      "loss": 1.0736,
      "step": 1035
    },
    {
      "epoch": 0.14564881203430338,
      "grad_norm": 1.717543125152588,
      "learning_rate": 0.00017374038396918788,
      "loss": 1.1973,
      "step": 1036
    },
    {
      "epoch": 0.14578939969070714,
      "grad_norm": 1.512918472290039,
      "learning_rate": 0.00017358316284658452,
      "loss": 1.159,
      "step": 1037
    },
    {
      "epoch": 0.14592998734711093,
      "grad_norm": 1.5849283933639526,
      "learning_rate": 0.00017342554406557094,
      "loss": 1.0506,
      "step": 1038
    },
    {
      "epoch": 0.1460705750035147,
      "grad_norm": 1.6444880962371826,
      "learning_rate": 0.00017326752847795116,
      "loss": 1.1642,
      "step": 1039
    },
    {
      "epoch": 0.14621116265991846,
      "grad_norm": 1.7328587770462036,
      "learning_rate": 0.00017310911693767361,
      "loss": 1.0294,
      "step": 1040
    },
    {
      "epoch": 0.14635175031632222,
      "grad_norm": 1.7611771821975708,
      "learning_rate": 0.00017295031030082657,
      "loss": 1.1775,
      "step": 1041
    },
    {
      "epoch": 0.14649233797272598,
      "grad_norm": 1.7040449380874634,
      "learning_rate": 0.0001727911094256335,
      "loss": 1.2337,
      "step": 1042
    },
    {
      "epoch": 0.14663292562912977,
      "grad_norm": 1.5207765102386475,
      "learning_rate": 0.00017263151517244832,
      "loss": 1.0865,
      "step": 1043
    },
    {
      "epoch": 0.14677351328553354,
      "grad_norm": 1.690390944480896,
      "learning_rate": 0.00017247152840375093,
      "loss": 1.1811,
      "step": 1044
    },
    {
      "epoch": 0.1469141009419373,
      "grad_norm": 1.6098686456680298,
      "learning_rate": 0.00017231114998414243,
      "loss": 1.2069,
      "step": 1045
    },
    {
      "epoch": 0.14705468859834106,
      "grad_norm": 1.6763060092926025,
      "learning_rate": 0.00017215038078034048,
      "loss": 1.1971,
      "step": 1046
    },
    {
      "epoch": 0.14719527625474482,
      "grad_norm": 1.9448530673980713,
      "learning_rate": 0.00017198922166117468,
      "loss": 0.9447,
      "step": 1047
    },
    {
      "epoch": 0.1473358639111486,
      "grad_norm": 1.594589114189148,
      "learning_rate": 0.0001718276734975817,
      "loss": 1.2447,
      "step": 1048
    },
    {
      "epoch": 0.14747645156755237,
      "grad_norm": 1.520838737487793,
      "learning_rate": 0.00017166573716260075,
      "loss": 1.1248,
      "step": 1049
    },
    {
      "epoch": 0.14761703922395614,
      "grad_norm": 1.426365613937378,
      "learning_rate": 0.00017150341353136883,
      "loss": 1.2571,
      "step": 1050
    },
    {
      "epoch": 0.1477576268803599,
      "grad_norm": 1.5624771118164062,
      "learning_rate": 0.00017134070348111589,
      "loss": 1.0025,
      "step": 1051
    },
    {
      "epoch": 0.14789821453676366,
      "grad_norm": 1.7122548818588257,
      "learning_rate": 0.0001711776078911602,
      "loss": 1.0879,
      "step": 1052
    },
    {
      "epoch": 0.14803880219316745,
      "grad_norm": 1.6780216693878174,
      "learning_rate": 0.00017101412764290364,
      "loss": 1.1705,
      "step": 1053
    },
    {
      "epoch": 0.1481793898495712,
      "grad_norm": 1.6072584390640259,
      "learning_rate": 0.0001708502636198267,
      "loss": 1.0866,
      "step": 1054
    },
    {
      "epoch": 0.14831997750597498,
      "grad_norm": 1.4053975343704224,
      "learning_rate": 0.000170686016707484,
      "loss": 1.3563,
      "step": 1055
    },
    {
      "epoch": 0.14846056516237874,
      "grad_norm": 2.2360880374908447,
      "learning_rate": 0.00017052138779349938,
      "loss": 0.9693,
      "step": 1056
    },
    {
      "epoch": 0.1486011528187825,
      "grad_norm": 1.5913816690444946,
      "learning_rate": 0.00017035637776756097,
      "loss": 1.1838,
      "step": 1057
    },
    {
      "epoch": 0.1487417404751863,
      "grad_norm": 1.5517102479934692,
      "learning_rate": 0.00017019098752141663,
      "loss": 1.2439,
      "step": 1058
    },
    {
      "epoch": 0.14888232813159005,
      "grad_norm": 1.688496708869934,
      "learning_rate": 0.00017002521794886897,
      "loss": 1.2124,
      "step": 1059
    },
    {
      "epoch": 0.14902291578799381,
      "grad_norm": 1.5988236665725708,
      "learning_rate": 0.0001698590699457705,
      "loss": 1.0438,
      "step": 1060
    },
    {
      "epoch": 0.14916350344439758,
      "grad_norm": 1.9951744079589844,
      "learning_rate": 0.0001696925444100189,
      "loss": 1.1374,
      "step": 1061
    },
    {
      "epoch": 0.14930409110080134,
      "grad_norm": 1.7398035526275635,
      "learning_rate": 0.00016952564224155214,
      "loss": 0.9458,
      "step": 1062
    },
    {
      "epoch": 0.14944467875720513,
      "grad_norm": 1.6933164596557617,
      "learning_rate": 0.00016935836434234358,
      "loss": 1.1297,
      "step": 1063
    },
    {
      "epoch": 0.1495852664136089,
      "grad_norm": 1.6245601177215576,
      "learning_rate": 0.00016919071161639702,
      "loss": 1.0529,
      "step": 1064
    },
    {
      "epoch": 0.14972585407001265,
      "grad_norm": 1.6169276237487793,
      "learning_rate": 0.00016902268496974201,
      "loss": 0.9082,
      "step": 1065
    },
    {
      "epoch": 0.14986644172641642,
      "grad_norm": 1.6225723028182983,
      "learning_rate": 0.00016885428531042876,
      "loss": 1.2522,
      "step": 1066
    },
    {
      "epoch": 0.15000702938282018,
      "grad_norm": 1.8111348152160645,
      "learning_rate": 0.00016868551354852338,
      "loss": 1.0121,
      "step": 1067
    },
    {
      "epoch": 0.15014761703922397,
      "grad_norm": 1.5175821781158447,
      "learning_rate": 0.00016851637059610286,
      "loss": 1.1695,
      "step": 1068
    },
    {
      "epoch": 0.15028820469562773,
      "grad_norm": 1.5991849899291992,
      "learning_rate": 0.0001683468573672502,
      "loss": 1.2418,
      "step": 1069
    },
    {
      "epoch": 0.1504287923520315,
      "grad_norm": 1.9400492906570435,
      "learning_rate": 0.0001681769747780494,
      "loss": 1.0747,
      "step": 1070
    },
    {
      "epoch": 0.15056938000843526,
      "grad_norm": 2.0559115409851074,
      "learning_rate": 0.00016800672374658068,
      "loss": 1.1583,
      "step": 1071
    },
    {
      "epoch": 0.15070996766483902,
      "grad_norm": 1.8014408349990845,
      "learning_rate": 0.00016783610519291523,
      "loss": 1.1659,
      "step": 1072
    },
    {
      "epoch": 0.1508505553212428,
      "grad_norm": 1.8288286924362183,
      "learning_rate": 0.00016766512003911057,
      "loss": 1.1827,
      "step": 1073
    },
    {
      "epoch": 0.15099114297764657,
      "grad_norm": 1.6986114978790283,
      "learning_rate": 0.00016749376920920526,
      "loss": 1.1024,
      "step": 1074
    },
    {
      "epoch": 0.15113173063405033,
      "grad_norm": 1.709017038345337,
      "learning_rate": 0.0001673220536292141,
      "loss": 1.3148,
      "step": 1075
    },
    {
      "epoch": 0.1512723182904541,
      "grad_norm": 1.8994615077972412,
      "learning_rate": 0.00016714997422712316,
      "loss": 0.9449,
      "step": 1076
    },
    {
      "epoch": 0.15141290594685786,
      "grad_norm": 1.7733842134475708,
      "learning_rate": 0.00016697753193288455,
      "loss": 1.1377,
      "step": 1077
    },
    {
      "epoch": 0.15155349360326165,
      "grad_norm": 1.4710599184036255,
      "learning_rate": 0.00016680472767841158,
      "loss": 1.1781,
      "step": 1078
    },
    {
      "epoch": 0.1516940812596654,
      "grad_norm": 1.5988775491714478,
      "learning_rate": 0.00016663156239757367,
      "loss": 1.1423,
      "step": 1079
    },
    {
      "epoch": 0.15183466891606917,
      "grad_norm": 1.608449935913086,
      "learning_rate": 0.00016645803702619127,
      "loss": 1.2623,
      "step": 1080
    },
    {
      "epoch": 0.15197525657247293,
      "grad_norm": 1.667382836341858,
      "learning_rate": 0.00016628415250203094,
      "loss": 1.0726,
      "step": 1081
    },
    {
      "epoch": 0.1521158442288767,
      "grad_norm": 1.7795186042785645,
      "learning_rate": 0.00016610990976479996,
      "loss": 1.1201,
      "step": 1082
    },
    {
      "epoch": 0.15225643188528049,
      "grad_norm": 1.4416404962539673,
      "learning_rate": 0.00016593530975614172,
      "loss": 1.1491,
      "step": 1083
    },
    {
      "epoch": 0.15239701954168425,
      "grad_norm": 1.4938645362854004,
      "learning_rate": 0.0001657603534196302,
      "loss": 1.1161,
      "step": 1084
    },
    {
      "epoch": 0.152537607198088,
      "grad_norm": 1.6270085573196411,
      "learning_rate": 0.00016558504170076504,
      "loss": 1.1529,
      "step": 1085
    },
    {
      "epoch": 0.15267819485449177,
      "grad_norm": 1.4684978723526,
      "learning_rate": 0.0001654093755469666,
      "loss": 1.1583,
      "step": 1086
    },
    {
      "epoch": 0.15281878251089553,
      "grad_norm": 1.8414126634597778,
      "learning_rate": 0.0001652333559075706,
      "loss": 1.211,
      "step": 1087
    },
    {
      "epoch": 0.15295937016729932,
      "grad_norm": 1.5331807136535645,
      "learning_rate": 0.00016505698373382296,
      "loss": 1.1336,
      "step": 1088
    },
    {
      "epoch": 0.1530999578237031,
      "grad_norm": 1.5899224281311035,
      "learning_rate": 0.00016488025997887483,
      "loss": 1.2014,
      "step": 1089
    },
    {
      "epoch": 0.15324054548010685,
      "grad_norm": 1.4688620567321777,
      "learning_rate": 0.00016470318559777756,
      "loss": 1.1044,
      "step": 1090
    },
    {
      "epoch": 0.1533811331365106,
      "grad_norm": 1.551069974899292,
      "learning_rate": 0.00016452576154747702,
      "loss": 0.9358,
      "step": 1091
    },
    {
      "epoch": 0.15352172079291437,
      "grad_norm": 1.5020296573638916,
      "learning_rate": 0.0001643479887868091,
      "loss": 1.1172,
      "step": 1092
    },
    {
      "epoch": 0.15366230844931816,
      "grad_norm": 1.8346796035766602,
      "learning_rate": 0.00016416986827649392,
      "loss": 1.3032,
      "step": 1093
    },
    {
      "epoch": 0.15380289610572193,
      "grad_norm": 1.7420330047607422,
      "learning_rate": 0.00016399140097913105,
      "loss": 1.2877,
      "step": 1094
    },
    {
      "epoch": 0.1539434837621257,
      "grad_norm": 1.5457777976989746,
      "learning_rate": 0.00016381258785919415,
      "loss": 1.1206,
      "step": 1095
    },
    {
      "epoch": 0.15408407141852945,
      "grad_norm": 1.6148498058319092,
      "learning_rate": 0.0001636334298830258,
      "loss": 1.2383,
      "step": 1096
    },
    {
      "epoch": 0.1542246590749332,
      "grad_norm": 1.5052673816680908,
      "learning_rate": 0.00016345392801883217,
      "loss": 1.1735,
      "step": 1097
    },
    {
      "epoch": 0.154365246731337,
      "grad_norm": 1.8841081857681274,
      "learning_rate": 0.00016327408323667795,
      "loss": 1.0366,
      "step": 1098
    },
    {
      "epoch": 0.15450583438774076,
      "grad_norm": 1.7392793893814087,
      "learning_rate": 0.00016309389650848098,
      "loss": 1.2043,
      "step": 1099
    },
    {
      "epoch": 0.15464642204414453,
      "grad_norm": 1.6795672178268433,
      "learning_rate": 0.00016291336880800707,
      "loss": 0.9804,
      "step": 1100
    },
    {
      "epoch": 0.1547870097005483,
      "grad_norm": 1.6420036554336548,
      "learning_rate": 0.00016273250111086473,
      "loss": 0.9697,
      "step": 1101
    },
    {
      "epoch": 0.15492759735695205,
      "grad_norm": 1.6834732294082642,
      "learning_rate": 0.00016255129439449986,
      "loss": 1.169,
      "step": 1102
    },
    {
      "epoch": 0.1550681850133558,
      "grad_norm": 1.7680649757385254,
      "learning_rate": 0.00016236974963819046,
      "loss": 1.0393,
      "step": 1103
    },
    {
      "epoch": 0.1552087726697596,
      "grad_norm": 1.8938243389129639,
      "learning_rate": 0.00016218786782304143,
      "loss": 1.0337,
      "step": 1104
    },
    {
      "epoch": 0.15534936032616337,
      "grad_norm": 1.7788703441619873,
      "learning_rate": 0.00016200564993197914,
      "loss": 1.1102,
      "step": 1105
    },
    {
      "epoch": 0.15548994798256713,
      "grad_norm": 1.7918554544448853,
      "learning_rate": 0.00016182309694974623,
      "loss": 1.0357,
      "step": 1106
    },
    {
      "epoch": 0.1556305356389709,
      "grad_norm": 1.7530405521392822,
      "learning_rate": 0.00016164020986289624,
      "loss": 1.2663,
      "step": 1107
    },
    {
      "epoch": 0.15577112329537465,
      "grad_norm": 1.4205873012542725,
      "learning_rate": 0.0001614569896597882,
      "loss": 1.0402,
      "step": 1108
    },
    {
      "epoch": 0.15591171095177844,
      "grad_norm": 1.599515676498413,
      "learning_rate": 0.00016127343733058152,
      "loss": 0.9739,
      "step": 1109
    },
    {
      "epoch": 0.1560522986081822,
      "grad_norm": 1.8438628911972046,
      "learning_rate": 0.00016108955386723037,
      "loss": 1.0795,
      "step": 1110
    },
    {
      "epoch": 0.15619288626458597,
      "grad_norm": 1.4634827375411987,
      "learning_rate": 0.00016090534026347845,
      "loss": 1.1873,
      "step": 1111
    },
    {
      "epoch": 0.15633347392098973,
      "grad_norm": 1.7450485229492188,
      "learning_rate": 0.00016072079751485366,
      "loss": 1.0869,
      "step": 1112
    },
    {
      "epoch": 0.1564740615773935,
      "grad_norm": 1.5935403108596802,
      "learning_rate": 0.00016053592661866253,
      "loss": 1.1678,
      "step": 1113
    },
    {
      "epoch": 0.15661464923379728,
      "grad_norm": 2.031660318374634,
      "learning_rate": 0.00016035072857398515,
      "loss": 1.0324,
      "step": 1114
    },
    {
      "epoch": 0.15675523689020104,
      "grad_norm": 1.6534945964813232,
      "learning_rate": 0.00016016520438166953,
      "loss": 1.1367,
      "step": 1115
    },
    {
      "epoch": 0.1568958245466048,
      "grad_norm": 1.7433925867080688,
      "learning_rate": 0.00015997935504432614,
      "loss": 1.1403,
      "step": 1116
    },
    {
      "epoch": 0.15703641220300857,
      "grad_norm": 1.7363579273223877,
      "learning_rate": 0.00015979318156632272,
      "loss": 1.0351,
      "step": 1117
    },
    {
      "epoch": 0.15717699985941233,
      "grad_norm": 1.6750723123550415,
      "learning_rate": 0.0001596066849537787,
      "loss": 1.1363,
      "step": 1118
    },
    {
      "epoch": 0.15731758751581612,
      "grad_norm": 1.7257009744644165,
      "learning_rate": 0.00015941986621455974,
      "loss": 1.0569,
      "step": 1119
    },
    {
      "epoch": 0.15745817517221988,
      "grad_norm": 2.103485107421875,
      "learning_rate": 0.00015923272635827248,
      "loss": 0.9484,
      "step": 1120
    },
    {
      "epoch": 0.15759876282862365,
      "grad_norm": 1.6324942111968994,
      "learning_rate": 0.00015904526639625872,
      "loss": 1.1169,
      "step": 1121
    },
    {
      "epoch": 0.1577393504850274,
      "grad_norm": 1.9000846147537231,
      "learning_rate": 0.0001588574873415904,
      "loss": 1.1394,
      "step": 1122
    },
    {
      "epoch": 0.15787993814143117,
      "grad_norm": 1.6071754693984985,
      "learning_rate": 0.0001586693902090638,
      "loss": 1.0975,
      "step": 1123
    },
    {
      "epoch": 0.15802052579783496,
      "grad_norm": 1.4703553915023804,
      "learning_rate": 0.00015848097601519408,
      "loss": 1.0902,
      "step": 1124
    },
    {
      "epoch": 0.15816111345423872,
      "grad_norm": 1.3413752317428589,
      "learning_rate": 0.00015829224577821008,
      "loss": 1.1801,
      "step": 1125
    },
    {
      "epoch": 0.15830170111064248,
      "grad_norm": 1.5939141511917114,
      "learning_rate": 0.00015810320051804836,
      "loss": 1.0596,
      "step": 1126
    },
    {
      "epoch": 0.15844228876704625,
      "grad_norm": 1.4229445457458496,
      "learning_rate": 0.00015791384125634806,
      "loss": 1.1374,
      "step": 1127
    },
    {
      "epoch": 0.15858287642345,
      "grad_norm": 1.6670811176300049,
      "learning_rate": 0.00015772416901644527,
      "loss": 1.1288,
      "step": 1128
    },
    {
      "epoch": 0.1587234640798538,
      "grad_norm": 1.4844356775283813,
      "learning_rate": 0.00015753418482336739,
      "loss": 1.2545,
      "step": 1129
    },
    {
      "epoch": 0.15886405173625756,
      "grad_norm": 1.6271159648895264,
      "learning_rate": 0.00015734388970382773,
      "loss": 1.0123,
      "step": 1130
    },
    {
      "epoch": 0.15900463939266132,
      "grad_norm": 1.6552551984786987,
      "learning_rate": 0.00015715328468621996,
      "loss": 1.3156,
      "step": 1131
    },
    {
      "epoch": 0.15914522704906509,
      "grad_norm": 1.664570927619934,
      "learning_rate": 0.00015696237080061235,
      "loss": 1.2914,
      "step": 1132
    },
    {
      "epoch": 0.15928581470546885,
      "grad_norm": 1.8602396249771118,
      "learning_rate": 0.0001567711490787425,
      "loss": 1.1545,
      "step": 1133
    },
    {
      "epoch": 0.15942640236187264,
      "grad_norm": 1.6165016889572144,
      "learning_rate": 0.00015657962055401158,
      "loss": 1.1555,
      "step": 1134
    },
    {
      "epoch": 0.1595669900182764,
      "grad_norm": 1.7556298971176147,
      "learning_rate": 0.00015638778626147877,
      "loss": 1.125,
      "step": 1135
    },
    {
      "epoch": 0.15970757767468016,
      "grad_norm": 1.4579581022262573,
      "learning_rate": 0.00015619564723785568,
      "loss": 1.2311,
      "step": 1136
    },
    {
      "epoch": 0.15984816533108392,
      "grad_norm": 1.8416825532913208,
      "learning_rate": 0.00015600320452150075,
      "loss": 1.23,
      "step": 1137
    },
    {
      "epoch": 0.1599887529874877,
      "grad_norm": 1.5021090507507324,
      "learning_rate": 0.00015581045915241367,
      "loss": 1.2702,
      "step": 1138
    },
    {
      "epoch": 0.16012934064389148,
      "grad_norm": 1.762222170829773,
      "learning_rate": 0.00015561741217222968,
      "loss": 1.1255,
      "step": 1139
    },
    {
      "epoch": 0.16026992830029524,
      "grad_norm": 1.8725061416625977,
      "learning_rate": 0.00015542406462421403,
      "loss": 1.264,
      "step": 1140
    },
    {
      "epoch": 0.160410515956699,
      "grad_norm": 1.6377803087234497,
      "learning_rate": 0.00015523041755325623,
      "loss": 1.266,
      "step": 1141
    },
    {
      "epoch": 0.16055110361310276,
      "grad_norm": 1.620621681213379,
      "learning_rate": 0.00015503647200586457,
      "loss": 1.1323,
      "step": 1142
    },
    {
      "epoch": 0.16069169126950653,
      "grad_norm": 1.8762290477752686,
      "learning_rate": 0.00015484222903016032,
      "loss": 1.1598,
      "step": 1143
    },
    {
      "epoch": 0.16083227892591032,
      "grad_norm": 1.5318349599838257,
      "learning_rate": 0.00015464768967587207,
      "loss": 1.1294,
      "step": 1144
    },
    {
      "epoch": 0.16097286658231408,
      "grad_norm": 1.5075684785842896,
      "learning_rate": 0.0001544528549943302,
      "loss": 1.0009,
      "step": 1145
    },
    {
      "epoch": 0.16111345423871784,
      "grad_norm": 1.4730637073516846,
      "learning_rate": 0.000154257726038461,
      "loss": 1.1174,
      "step": 1146
    },
    {
      "epoch": 0.1612540418951216,
      "grad_norm": 1.4208699464797974,
      "learning_rate": 0.00015406230386278113,
      "loss": 1.1688,
      "step": 1147
    },
    {
      "epoch": 0.16139462955152536,
      "grad_norm": 1.427156686782837,
      "learning_rate": 0.00015386658952339186,
      "loss": 1.1978,
      "step": 1148
    },
    {
      "epoch": 0.16153521720792915,
      "grad_norm": 1.5315887928009033,
      "learning_rate": 0.0001536705840779734,
      "loss": 1.0988,
      "step": 1149
    },
    {
      "epoch": 0.16167580486433292,
      "grad_norm": 1.7283567190170288,
      "learning_rate": 0.00015347428858577904,
      "loss": 1.192,
      "step": 1150
    },
    {
      "epoch": 0.16181639252073668,
      "grad_norm": 1.4939123392105103,
      "learning_rate": 0.0001532777041076297,
      "loss": 1.2926,
      "step": 1151
    },
    {
      "epoch": 0.16195698017714044,
      "grad_norm": 1.7236601114273071,
      "learning_rate": 0.000153080831705908,
      "loss": 1.1002,
      "step": 1152
    },
    {
      "epoch": 0.1620975678335442,
      "grad_norm": 1.5009121894836426,
      "learning_rate": 0.00015288367244455247,
      "loss": 1.1093,
      "step": 1153
    },
    {
      "epoch": 0.162238155489948,
      "grad_norm": 1.6926287412643433,
      "learning_rate": 0.00015268622738905197,
      "loss": 1.2539,
      "step": 1154
    },
    {
      "epoch": 0.16237874314635176,
      "grad_norm": 1.7517296075820923,
      "learning_rate": 0.00015248849760643983,
      "loss": 1.0578,
      "step": 1155
    },
    {
      "epoch": 0.16251933080275552,
      "grad_norm": 1.8113586902618408,
      "learning_rate": 0.00015229048416528804,
      "loss": 0.9115,
      "step": 1156
    },
    {
      "epoch": 0.16265991845915928,
      "grad_norm": 1.5501978397369385,
      "learning_rate": 0.00015209218813570167,
      "loss": 1.1284,
      "step": 1157
    },
    {
      "epoch": 0.16280050611556304,
      "grad_norm": 1.5686930418014526,
      "learning_rate": 0.00015189361058931284,
      "loss": 0.9794,
      "step": 1158
    },
    {
      "epoch": 0.16294109377196683,
      "grad_norm": 1.6491615772247314,
      "learning_rate": 0.00015169475259927512,
      "loss": 0.8952,
      "step": 1159
    },
    {
      "epoch": 0.1630816814283706,
      "grad_norm": 1.5278033018112183,
      "learning_rate": 0.00015149561524025756,
      "loss": 1.2329,
      "step": 1160
    },
    {
      "epoch": 0.16322226908477436,
      "grad_norm": 1.7710986137390137,
      "learning_rate": 0.00015129619958843907,
      "loss": 1.2735,
      "step": 1161
    },
    {
      "epoch": 0.16336285674117812,
      "grad_norm": 1.3923383951187134,
      "learning_rate": 0.00015109650672150252,
      "loss": 1.2091,
      "step": 1162
    },
    {
      "epoch": 0.16350344439758188,
      "grad_norm": 1.5282748937606812,
      "learning_rate": 0.00015089653771862882,
      "loss": 1.1982,
      "step": 1163
    },
    {
      "epoch": 0.16364403205398567,
      "grad_norm": 1.7950266599655151,
      "learning_rate": 0.00015069629366049124,
      "loss": 1.0834,
      "step": 1164
    },
    {
      "epoch": 0.16378461971038943,
      "grad_norm": 1.6748510599136353,
      "learning_rate": 0.00015049577562924947,
      "loss": 1.0728,
      "step": 1165
    },
    {
      "epoch": 0.1639252073667932,
      "grad_norm": 1.6119117736816406,
      "learning_rate": 0.00015029498470854383,
      "loss": 0.9656,
      "step": 1166
    },
    {
      "epoch": 0.16406579502319696,
      "grad_norm": 1.493622064590454,
      "learning_rate": 0.00015009392198348938,
      "loss": 1.4991,
      "step": 1167
    },
    {
      "epoch": 0.16420638267960072,
      "grad_norm": 1.5013182163238525,
      "learning_rate": 0.00014989258854067005,
      "loss": 1.1298,
      "step": 1168
    },
    {
      "epoch": 0.1643469703360045,
      "grad_norm": 1.812945008277893,
      "learning_rate": 0.00014969098546813284,
      "loss": 1.3134,
      "step": 1169
    },
    {
      "epoch": 0.16448755799240827,
      "grad_norm": 1.696524977684021,
      "learning_rate": 0.0001494891138553818,
      "loss": 0.9993,
      "step": 1170
    },
    {
      "epoch": 0.16462814564881204,
      "grad_norm": 1.6282621622085571,
      "learning_rate": 0.0001492869747933723,
      "loss": 1.2707,
      "step": 1171
    },
    {
      "epoch": 0.1647687333052158,
      "grad_norm": 1.515710711479187,
      "learning_rate": 0.00014908456937450505,
      "loss": 1.0481,
      "step": 1172
    },
    {
      "epoch": 0.16490932096161956,
      "grad_norm": 1.555892825126648,
      "learning_rate": 0.00014888189869262017,
      "loss": 1.0063,
      "step": 1173
    },
    {
      "epoch": 0.16504990861802335,
      "grad_norm": 1.7203866243362427,
      "learning_rate": 0.00014867896384299133,
      "loss": 1.0275,
      "step": 1174
    },
    {
      "epoch": 0.1651904962744271,
      "grad_norm": 1.4863207340240479,
      "learning_rate": 0.0001484757659223198,
      "loss": 1.1541,
      "step": 1175
    },
    {
      "epoch": 0.16533108393083087,
      "grad_norm": 1.4583079814910889,
      "learning_rate": 0.00014827230602872864,
      "loss": 1.1395,
      "step": 1176
    },
    {
      "epoch": 0.16547167158723464,
      "grad_norm": 1.6193957328796387,
      "learning_rate": 0.0001480685852617565,
      "loss": 1.2919,
      "step": 1177
    },
    {
      "epoch": 0.1656122592436384,
      "grad_norm": 1.676221489906311,
      "learning_rate": 0.00014786460472235197,
      "loss": 1.2986,
      "step": 1178
    },
    {
      "epoch": 0.1657528469000422,
      "grad_norm": 1.7942696809768677,
      "learning_rate": 0.00014766036551286743,
      "loss": 1.257,
      "step": 1179
    },
    {
      "epoch": 0.16589343455644595,
      "grad_norm": 1.618816614151001,
      "learning_rate": 0.00014745586873705322,
      "loss": 1.1083,
      "step": 1180
    },
    {
      "epoch": 0.1660340222128497,
      "grad_norm": 1.5589632987976074,
      "learning_rate": 0.0001472511155000516,
      "loss": 1.1353,
      "step": 1181
    },
    {
      "epoch": 0.16617460986925348,
      "grad_norm": 1.721117377281189,
      "learning_rate": 0.0001470461069083908,
      "loss": 1.2002,
      "step": 1182
    },
    {
      "epoch": 0.16631519752565724,
      "grad_norm": 1.5311018228530884,
      "learning_rate": 0.00014684084406997903,
      "loss": 1.1617,
      "step": 1183
    },
    {
      "epoch": 0.16645578518206103,
      "grad_norm": 1.685840129852295,
      "learning_rate": 0.0001466353280940985,
      "loss": 1.0656,
      "step": 1184
    },
    {
      "epoch": 0.1665963728384648,
      "grad_norm": 1.718794345855713,
      "learning_rate": 0.00014642956009139942,
      "loss": 1.069,
      "step": 1185
    },
    {
      "epoch": 0.16673696049486855,
      "grad_norm": 1.3564704656600952,
      "learning_rate": 0.00014622354117389407,
      "loss": 1.1476,
      "step": 1186
    },
    {
      "epoch": 0.16687754815127231,
      "grad_norm": 1.583802342414856,
      "learning_rate": 0.00014601727245495063,
      "loss": 1.1093,
      "step": 1187
    },
    {
      "epoch": 0.16701813580767608,
      "grad_norm": 1.5046725273132324,
      "learning_rate": 0.00014581075504928732,
      "loss": 1.0348,
      "step": 1188
    },
    {
      "epoch": 0.16715872346407987,
      "grad_norm": 1.6267790794372559,
      "learning_rate": 0.00014560399007296625,
      "loss": 1.0416,
      "step": 1189
    },
    {
      "epoch": 0.16729931112048363,
      "grad_norm": 1.5688893795013428,
      "learning_rate": 0.00014539697864338752,
      "loss": 1.2062,
      "step": 1190
    },
    {
      "epoch": 0.1674398987768874,
      "grad_norm": 1.4886304140090942,
      "learning_rate": 0.00014518972187928312,
      "loss": 1.176,
      "step": 1191
    },
    {
      "epoch": 0.16758048643329115,
      "grad_norm": 1.617308497428894,
      "learning_rate": 0.00014498222090071085,
      "loss": 0.9904,
      "step": 1192
    },
    {
      "epoch": 0.16772107408969492,
      "grad_norm": 1.545033574104309,
      "learning_rate": 0.00014477447682904824,
      "loss": 1.0335,
      "step": 1193
    },
    {
      "epoch": 0.1678616617460987,
      "grad_norm": 1.639428734779358,
      "learning_rate": 0.00014456649078698662,
      "loss": 0.9852,
      "step": 1194
    },
    {
      "epoch": 0.16800224940250247,
      "grad_norm": 1.5077521800994873,
      "learning_rate": 0.00014435826389852497,
      "loss": 1.0831,
      "step": 1195
    },
    {
      "epoch": 0.16814283705890623,
      "grad_norm": 1.5754035711288452,
      "learning_rate": 0.00014414979728896385,
      "loss": 1.1572,
      "step": 1196
    },
    {
      "epoch": 0.16828342471531,
      "grad_norm": 1.573744773864746,
      "learning_rate": 0.00014394109208489927,
      "loss": 1.2173,
      "step": 1197
    },
    {
      "epoch": 0.16842401237171375,
      "grad_norm": 1.4838769435882568,
      "learning_rate": 0.00014373214941421672,
      "loss": 1.137,
      "step": 1198
    },
    {
      "epoch": 0.16856460002811754,
      "grad_norm": 1.4547994136810303,
      "learning_rate": 0.00014352297040608493,
      "loss": 1.0167,
      "step": 1199
    },
    {
      "epoch": 0.1687051876845213,
      "grad_norm": 1.5064082145690918,
      "learning_rate": 0.00014331355619094996,
      "loss": 1.1612,
      "step": 1200
    },
    {
      "epoch": 0.16884577534092507,
      "grad_norm": 1.6504435539245605,
      "learning_rate": 0.00014310390790052887,
      "loss": 1.1476,
      "step": 1201
    },
    {
      "epoch": 0.16898636299732883,
      "grad_norm": 1.603424072265625,
      "learning_rate": 0.00014289402666780378,
      "loss": 1.1337,
      "step": 1202
    },
    {
      "epoch": 0.1691269506537326,
      "grad_norm": 1.5893702507019043,
      "learning_rate": 0.00014268391362701568,
      "loss": 1.2875,
      "step": 1203
    },
    {
      "epoch": 0.16926753831013636,
      "grad_norm": 1.380562663078308,
      "learning_rate": 0.00014247356991365818,
      "loss": 1.1297,
      "step": 1204
    },
    {
      "epoch": 0.16940812596654015,
      "grad_norm": 1.8207881450653076,
      "learning_rate": 0.00014226299666447167,
      "loss": 0.9739,
      "step": 1205
    },
    {
      "epoch": 0.1695487136229439,
      "grad_norm": 1.4543616771697998,
      "learning_rate": 0.00014205219501743684,
      "loss": 1.1145,
      "step": 1206
    },
    {
      "epoch": 0.16968930127934767,
      "grad_norm": 1.4963948726654053,
      "learning_rate": 0.00014184116611176884,
      "loss": 1.246,
      "step": 1207
    },
    {
      "epoch": 0.16982988893575143,
      "grad_norm": 1.6215029954910278,
      "learning_rate": 0.00014162991108791079,
      "loss": 1.1795,
      "step": 1208
    },
    {
      "epoch": 0.1699704765921552,
      "grad_norm": 1.6002600193023682,
      "learning_rate": 0.00014141843108752796,
      "loss": 1.2587,
      "step": 1209
    },
    {
      "epoch": 0.17011106424855899,
      "grad_norm": 1.685610294342041,
      "learning_rate": 0.00014120672725350135,
      "loss": 1.0208,
      "step": 1210
    },
    {
      "epoch": 0.17025165190496275,
      "grad_norm": 1.4858587980270386,
      "learning_rate": 0.00014099480072992166,
      "loss": 1.1563,
      "step": 1211
    },
    {
      "epoch": 0.1703922395613665,
      "grad_norm": 1.5352383852005005,
      "learning_rate": 0.00014078265266208298,
      "loss": 1.242,
      "step": 1212
    },
    {
      "epoch": 0.17053282721777027,
      "grad_norm": 2.1248230934143066,
      "learning_rate": 0.00014057028419647678,
      "loss": 1.1326,
      "step": 1213
    },
    {
      "epoch": 0.17067341487417403,
      "grad_norm": 1.6415833234786987,
      "learning_rate": 0.00014035769648078545,
      "loss": 1.0923,
      "step": 1214
    },
    {
      "epoch": 0.17081400253057782,
      "grad_norm": 1.8061373233795166,
      "learning_rate": 0.0001401448906638764,
      "loss": 1.0783,
      "step": 1215
    },
    {
      "epoch": 0.1709545901869816,
      "grad_norm": 1.5099493265151978,
      "learning_rate": 0.00013993186789579558,
      "loss": 1.0227,
      "step": 1216
    },
    {
      "epoch": 0.17109517784338535,
      "grad_norm": 1.5410233736038208,
      "learning_rate": 0.00013971862932776153,
      "loss": 0.9975,
      "step": 1217
    },
    {
      "epoch": 0.1712357654997891,
      "grad_norm": 1.7846852540969849,
      "learning_rate": 0.00013950517611215882,
      "loss": 1.0035,
      "step": 1218
    },
    {
      "epoch": 0.17137635315619287,
      "grad_norm": 1.6204864978790283,
      "learning_rate": 0.00013929150940253225,
      "loss": 1.1605,
      "step": 1219
    },
    {
      "epoch": 0.17151694081259666,
      "grad_norm": 1.7248669862747192,
      "learning_rate": 0.00013907763035358022,
      "loss": 1.0879,
      "step": 1220
    },
    {
      "epoch": 0.17165752846900043,
      "grad_norm": 2.0563011169433594,
      "learning_rate": 0.00013886354012114868,
      "loss": 1.0881,
      "step": 1221
    },
    {
      "epoch": 0.1717981161254042,
      "grad_norm": 1.5618643760681152,
      "learning_rate": 0.00013864923986222494,
      "loss": 1.0681,
      "step": 1222
    },
    {
      "epoch": 0.17193870378180795,
      "grad_norm": 1.4689831733703613,
      "learning_rate": 0.00013843473073493122,
      "loss": 0.9956,
      "step": 1223
    },
    {
      "epoch": 0.1720792914382117,
      "grad_norm": 1.5191527605056763,
      "learning_rate": 0.0001382200138985186,
      "loss": 1.103,
      "step": 1224
    },
    {
      "epoch": 0.1722198790946155,
      "grad_norm": 1.5009140968322754,
      "learning_rate": 0.00013800509051336066,
      "loss": 1.0746,
      "step": 1225
    },
    {
      "epoch": 0.17236046675101926,
      "grad_norm": 1.7424370050430298,
      "learning_rate": 0.00013778996174094714,
      "loss": 1.0526,
      "step": 1226
    },
    {
      "epoch": 0.17250105440742303,
      "grad_norm": 1.7765791416168213,
      "learning_rate": 0.00013757462874387777,
      "loss": 1.3221,
      "step": 1227
    },
    {
      "epoch": 0.1726416420638268,
      "grad_norm": 1.5429083108901978,
      "learning_rate": 0.00013735909268585596,
      "loss": 1.1067,
      "step": 1228
    },
    {
      "epoch": 0.17278222972023055,
      "grad_norm": 1.735177993774414,
      "learning_rate": 0.0001371433547316825,
      "loss": 1.137,
      "step": 1229
    },
    {
      "epoch": 0.17292281737663434,
      "grad_norm": 1.5234744548797607,
      "learning_rate": 0.00013692741604724926,
      "loss": 1.1555,
      "step": 1230
    },
    {
      "epoch": 0.1730634050330381,
      "grad_norm": 1.7986416816711426,
      "learning_rate": 0.00013671127779953294,
      "loss": 1.1391,
      "step": 1231
    },
    {
      "epoch": 0.17320399268944187,
      "grad_norm": 1.5526134967803955,
      "learning_rate": 0.00013649494115658867,
      "loss": 1.0346,
      "step": 1232
    },
    {
      "epoch": 0.17334458034584563,
      "grad_norm": 1.5493760108947754,
      "learning_rate": 0.00013627840728754376,
      "loss": 1.2556,
      "step": 1233
    },
    {
      "epoch": 0.1734851680022494,
      "grad_norm": 1.7535312175750732,
      "learning_rate": 0.00013606167736259137,
      "loss": 1.0871,
      "step": 1234
    },
    {
      "epoch": 0.17362575565865318,
      "grad_norm": 1.5342096090316772,
      "learning_rate": 0.00013584475255298419,
      "loss": 1.0869,
      "step": 1235
    },
    {
      "epoch": 0.17376634331505694,
      "grad_norm": 1.6589802503585815,
      "learning_rate": 0.0001356276340310281,
      "loss": 1.3657,
      "step": 1236
    },
    {
      "epoch": 0.1739069309714607,
      "grad_norm": 1.9831687211990356,
      "learning_rate": 0.00013541032297007585,
      "loss": 1.1042,
      "step": 1237
    },
    {
      "epoch": 0.17404751862786447,
      "grad_norm": 2.1309776306152344,
      "learning_rate": 0.0001351928205445207,
      "loss": 1.2484,
      "step": 1238
    },
    {
      "epoch": 0.17418810628426823,
      "grad_norm": 1.6874473094940186,
      "learning_rate": 0.00013497512792979007,
      "loss": 1.0093,
      "step": 1239
    },
    {
      "epoch": 0.17432869394067202,
      "grad_norm": 1.5356870889663696,
      "learning_rate": 0.00013475724630233933,
      "loss": 1.0726,
      "step": 1240
    },
    {
      "epoch": 0.17446928159707578,
      "grad_norm": 1.7879536151885986,
      "learning_rate": 0.0001345391768396451,
      "loss": 1.136,
      "step": 1241
    },
    {
      "epoch": 0.17460986925347954,
      "grad_norm": 1.7186193466186523,
      "learning_rate": 0.00013432092072019925,
      "loss": 1.2223,
      "step": 1242
    },
    {
      "epoch": 0.1747504569098833,
      "grad_norm": 1.8174771070480347,
      "learning_rate": 0.00013410247912350228,
      "loss": 1.1058,
      "step": 1243
    },
    {
      "epoch": 0.17489104456628707,
      "grad_norm": 1.5616546869277954,
      "learning_rate": 0.00013388385323005717,
      "loss": 1.0496,
      "step": 1244
    },
    {
      "epoch": 0.17503163222269086,
      "grad_norm": 1.7404396533966064,
      "learning_rate": 0.00013366504422136278,
      "loss": 1.1168,
      "step": 1245
    },
    {
      "epoch": 0.17517221987909462,
      "grad_norm": 1.4800913333892822,
      "learning_rate": 0.00013344605327990757,
      "loss": 1.1511,
      "step": 1246
    },
    {
      "epoch": 0.17531280753549838,
      "grad_norm": 1.711107611656189,
      "learning_rate": 0.00013322688158916326,
      "loss": 1.1268,
      "step": 1247
    },
    {
      "epoch": 0.17545339519190215,
      "grad_norm": 2.1455037593841553,
      "learning_rate": 0.0001330075303335783,
      "loss": 1.0663,
      "step": 1248
    },
    {
      "epoch": 0.1755939828483059,
      "grad_norm": 1.6677528619766235,
      "learning_rate": 0.00013278800069857166,
      "loss": 1.0846,
      "step": 1249
    },
    {
      "epoch": 0.1757345705047097,
      "grad_norm": 1.597369909286499,
      "learning_rate": 0.00013256829387052615,
      "loss": 0.9002,
      "step": 1250
    },
    {
      "epoch": 0.17587515816111346,
      "grad_norm": 1.6138945817947388,
      "learning_rate": 0.00013234841103678232,
      "loss": 1.1126,
      "step": 1251
    },
    {
      "epoch": 0.17601574581751722,
      "grad_norm": 1.4623186588287354,
      "learning_rate": 0.00013212835338563176,
      "loss": 1.1943,
      "step": 1252
    },
    {
      "epoch": 0.17615633347392098,
      "grad_norm": 1.5830717086791992,
      "learning_rate": 0.00013190812210631093,
      "loss": 1.1907,
      "step": 1253
    },
    {
      "epoch": 0.17629692113032475,
      "grad_norm": 1.4829117059707642,
      "learning_rate": 0.00013168771838899447,
      "loss": 1.1644,
      "step": 1254
    },
    {
      "epoch": 0.17643750878672854,
      "grad_norm": 1.547098159790039,
      "learning_rate": 0.00013146714342478907,
      "loss": 1.0827,
      "step": 1255
    },
    {
      "epoch": 0.1765780964431323,
      "grad_norm": 1.6194298267364502,
      "learning_rate": 0.0001312463984057268,
      "loss": 1.1379,
      "step": 1256
    },
    {
      "epoch": 0.17671868409953606,
      "grad_norm": 1.568115234375,
      "learning_rate": 0.00013102548452475864,
      "loss": 1.1859,
      "step": 1257
    },
    {
      "epoch": 0.17685927175593982,
      "grad_norm": 1.833924412727356,
      "learning_rate": 0.00013080440297574833,
      "loss": 1.082,
      "step": 1258
    },
    {
      "epoch": 0.17699985941234359,
      "grad_norm": 1.5304882526397705,
      "learning_rate": 0.0001305831549534656,
      "loss": 1.1773,
      "step": 1259
    },
    {
      "epoch": 0.17714044706874738,
      "grad_norm": 1.4984384775161743,
      "learning_rate": 0.00013036174165357978,
      "loss": 1.2305,
      "step": 1260
    },
    {
      "epoch": 0.17728103472515114,
      "grad_norm": 1.3323620557785034,
      "learning_rate": 0.00013014016427265364,
      "loss": 1.2288,
      "step": 1261
    },
    {
      "epoch": 0.1774216223815549,
      "grad_norm": 1.5911346673965454,
      "learning_rate": 0.00012991842400813635,
      "loss": 1.2362,
      "step": 1262
    },
    {
      "epoch": 0.17756221003795866,
      "grad_norm": 1.8944346904754639,
      "learning_rate": 0.00012969652205835756,
      "loss": 1.1027,
      "step": 1263
    },
    {
      "epoch": 0.17770279769436242,
      "grad_norm": 1.513683557510376,
      "learning_rate": 0.00012947445962252065,
      "loss": 1.062,
      "step": 1264
    },
    {
      "epoch": 0.17784338535076621,
      "grad_norm": 1.4962800741195679,
      "learning_rate": 0.00012925223790069624,
      "loss": 1.2281,
      "step": 1265
    },
    {
      "epoch": 0.17798397300716998,
      "grad_norm": 1.5849968194961548,
      "learning_rate": 0.00012902985809381588,
      "loss": 1.0707,
      "step": 1266
    },
    {
      "epoch": 0.17812456066357374,
      "grad_norm": 1.5764967203140259,
      "learning_rate": 0.00012880732140366528,
      "loss": 1.1237,
      "step": 1267
    },
    {
      "epoch": 0.1782651483199775,
      "grad_norm": 1.4192633628845215,
      "learning_rate": 0.00012858462903287814,
      "loss": 1.1249,
      "step": 1268
    },
    {
      "epoch": 0.17840573597638126,
      "grad_norm": 3.358327865600586,
      "learning_rate": 0.00012836178218492942,
      "loss": 1.2096,
      "step": 1269
    },
    {
      "epoch": 0.17854632363278505,
      "grad_norm": 1.4648364782333374,
      "learning_rate": 0.00012813878206412887,
      "loss": 1.137,
      "step": 1270
    },
    {
      "epoch": 0.17868691128918882,
      "grad_norm": 1.8063987493515015,
      "learning_rate": 0.00012791562987561462,
      "loss": 1.1124,
      "step": 1271
    },
    {
      "epoch": 0.17882749894559258,
      "grad_norm": 1.4417414665222168,
      "learning_rate": 0.0001276923268253466,
      "loss": 1.0889,
      "step": 1272
    },
    {
      "epoch": 0.17896808660199634,
      "grad_norm": 1.4105467796325684,
      "learning_rate": 0.00012746887412009997,
      "loss": 1.0915,
      "step": 1273
    },
    {
      "epoch": 0.1791086742584001,
      "grad_norm": 1.7349767684936523,
      "learning_rate": 0.00012724527296745873,
      "loss": 1.0046,
      "step": 1274
    },
    {
      "epoch": 0.1792492619148039,
      "grad_norm": 1.6212068796157837,
      "learning_rate": 0.00012702152457580906,
      "loss": 1.0946,
      "step": 1275
    },
    {
      "epoch": 0.17938984957120765,
      "grad_norm": 1.8262444734573364,
      "learning_rate": 0.0001267976301543329,
      "loss": 1.073,
      "step": 1276
    },
    {
      "epoch": 0.17953043722761142,
      "grad_norm": 1.565361738204956,
      "learning_rate": 0.00012657359091300125,
      "loss": 1.1366,
      "step": 1277
    },
    {
      "epoch": 0.17967102488401518,
      "grad_norm": 1.9158523082733154,
      "learning_rate": 0.00012634940806256797,
      "loss": 1.0816,
      "step": 1278
    },
    {
      "epoch": 0.17981161254041894,
      "grad_norm": 1.5099929571151733,
      "learning_rate": 0.00012612508281456281,
      "loss": 1.1825,
      "step": 1279
    },
    {
      "epoch": 0.17995220019682273,
      "grad_norm": 1.53780198097229,
      "learning_rate": 0.00012590061638128512,
      "loss": 1.1161,
      "step": 1280
    },
    {
      "epoch": 0.1800927878532265,
      "grad_norm": 1.6130229234695435,
      "learning_rate": 0.00012567600997579728,
      "loss": 1.0057,
      "step": 1281
    },
    {
      "epoch": 0.18023337550963026,
      "grad_norm": 1.6890398263931274,
      "learning_rate": 0.0001254512648119181,
      "loss": 1.2831,
      "step": 1282
    },
    {
      "epoch": 0.18037396316603402,
      "grad_norm": 1.6563471555709839,
      "learning_rate": 0.00012522638210421625,
      "loss": 0.8955,
      "step": 1283
    },
    {
      "epoch": 0.18051455082243778,
      "grad_norm": 2.2905988693237305,
      "learning_rate": 0.00012500136306800369,
      "loss": 1.0198,
      "step": 1284
    },
    {
      "epoch": 0.18065513847884157,
      "grad_norm": 1.3179066181182861,
      "learning_rate": 0.00012477620891932918,
      "loss": 1.3198,
      "step": 1285
    },
    {
      "epoch": 0.18079572613524533,
      "grad_norm": 1.7086939811706543,
      "learning_rate": 0.0001245509208749716,
      "loss": 0.8963,
      "step": 1286
    },
    {
      "epoch": 0.1809363137916491,
      "grad_norm": 1.8725422620773315,
      "learning_rate": 0.0001243255001524335,
      "loss": 1.1303,
      "step": 1287
    },
    {
      "epoch": 0.18107690144805286,
      "grad_norm": 1.6194939613342285,
      "learning_rate": 0.0001240999479699344,
      "loss": 1.1249,
      "step": 1288
    },
    {
      "epoch": 0.18121748910445662,
      "grad_norm": 1.5600429773330688,
      "learning_rate": 0.00012387426554640426,
      "loss": 1.0468,
      "step": 1289
    },
    {
      "epoch": 0.1813580767608604,
      "grad_norm": 1.4525588750839233,
      "learning_rate": 0.0001236484541014769,
      "loss": 0.9897,
      "step": 1290
    },
    {
      "epoch": 0.18149866441726417,
      "grad_norm": 1.6308281421661377,
      "learning_rate": 0.0001234225148554834,
      "loss": 1.1793,
      "step": 1291
    },
    {
      "epoch": 0.18163925207366793,
      "grad_norm": 1.4816070795059204,
      "learning_rate": 0.0001231964490294455,
      "loss": 1.2532,
      "step": 1292
    },
    {
      "epoch": 0.1817798397300717,
      "grad_norm": 1.430775761604309,
      "learning_rate": 0.00012297025784506897,
      "loss": 1.3044,
      "step": 1293
    },
    {
      "epoch": 0.18192042738647546,
      "grad_norm": 1.6915491819381714,
      "learning_rate": 0.00012274394252473713,
      "loss": 1.0323,
      "step": 1294
    },
    {
      "epoch": 0.18206101504287925,
      "grad_norm": 1.6300327777862549,
      "learning_rate": 0.00012251750429150406,
      "loss": 0.9453,
      "step": 1295
    },
    {
      "epoch": 0.182201602699283,
      "grad_norm": 1.6402223110198975,
      "learning_rate": 0.00012229094436908813,
      "loss": 1.0659,
      "step": 1296
    },
    {
      "epoch": 0.18234219035568677,
      "grad_norm": 1.405741572380066,
      "learning_rate": 0.00012206426398186534,
      "loss": 1.231,
      "step": 1297
    },
    {
      "epoch": 0.18248277801209054,
      "grad_norm": 1.4749175310134888,
      "learning_rate": 0.00012183746435486274,
      "loss": 1.0851,
      "step": 1298
    },
    {
      "epoch": 0.1826233656684943,
      "grad_norm": 1.671188235282898,
      "learning_rate": 0.0001216105467137517,
      "loss": 0.9777,
      "step": 1299
    },
    {
      "epoch": 0.1827639533248981,
      "grad_norm": 1.5334643125534058,
      "learning_rate": 0.00012138351228484142,
      "loss": 1.1589,
      "step": 1300
    },
    {
      "epoch": 0.18290454098130185,
      "grad_norm": 1.3781603574752808,
      "learning_rate": 0.00012115636229507223,
      "loss": 0.9816,
      "step": 1301
    },
    {
      "epoch": 0.1830451286377056,
      "grad_norm": 1.6746119260787964,
      "learning_rate": 0.00012092909797200897,
      "loss": 1.1228,
      "step": 1302
    },
    {
      "epoch": 0.18318571629410937,
      "grad_norm": 1.8171316385269165,
      "learning_rate": 0.00012070172054383438,
      "loss": 1.0219,
      "step": 1303
    },
    {
      "epoch": 0.18332630395051314,
      "grad_norm": 1.5420523881912231,
      "learning_rate": 0.00012047423123934241,
      "loss": 1.0635,
      "step": 1304
    },
    {
      "epoch": 0.18346689160691693,
      "grad_norm": 1.3950395584106445,
      "learning_rate": 0.00012024663128793164,
      "loss": 1.1086,
      "step": 1305
    },
    {
      "epoch": 0.1836074792633207,
      "grad_norm": 1.4450749158859253,
      "learning_rate": 0.00012001892191959857,
      "loss": 1.249,
      "step": 1306
    },
    {
      "epoch": 0.18374806691972445,
      "grad_norm": 1.4706854820251465,
      "learning_rate": 0.00011979110436493104,
      "loss": 0.964,
      "step": 1307
    },
    {
      "epoch": 0.1838886545761282,
      "grad_norm": 1.8329100608825684,
      "learning_rate": 0.00011956317985510162,
      "loss": 1.0893,
      "step": 1308
    },
    {
      "epoch": 0.18402924223253198,
      "grad_norm": 1.520957589149475,
      "learning_rate": 0.00011933514962186074,
      "loss": 1.1175,
      "step": 1309
    },
    {
      "epoch": 0.18416982988893574,
      "grad_norm": 1.702605128288269,
      "learning_rate": 0.00011910701489753031,
      "loss": 0.9617,
      "step": 1310
    },
    {
      "epoch": 0.18431041754533953,
      "grad_norm": 1.5094081163406372,
      "learning_rate": 0.00011887877691499686,
      "loss": 1.0702,
      "step": 1311
    },
    {
      "epoch": 0.1844510052017433,
      "grad_norm": 1.400079369544983,
      "learning_rate": 0.00011865043690770496,
      "loss": 1.0654,
      "step": 1312
    },
    {
      "epoch": 0.18459159285814705,
      "grad_norm": 1.9245373010635376,
      "learning_rate": 0.00011842199610965057,
      "loss": 1.1123,
      "step": 1313
    },
    {
      "epoch": 0.18473218051455081,
      "grad_norm": 1.4354326725006104,
      "learning_rate": 0.0001181934557553743,
      "loss": 1.1156,
      "step": 1314
    },
    {
      "epoch": 0.18487276817095458,
      "grad_norm": 1.7361444234848022,
      "learning_rate": 0.00011796481707995484,
      "loss": 1.1486,
      "step": 1315
    },
    {
      "epoch": 0.18501335582735837,
      "grad_norm": 1.7279713153839111,
      "learning_rate": 0.00011773608131900211,
      "loss": 1.1953,
      "step": 1316
    },
    {
      "epoch": 0.18515394348376213,
      "grad_norm": 1.4192051887512207,
      "learning_rate": 0.0001175072497086509,
      "loss": 1.1249,
      "step": 1317
    },
    {
      "epoch": 0.1852945311401659,
      "grad_norm": 1.815427303314209,
      "learning_rate": 0.00011727832348555383,
      "loss": 0.9903,
      "step": 1318
    },
    {
      "epoch": 0.18543511879656965,
      "grad_norm": 1.6502643823623657,
      "learning_rate": 0.00011704930388687488,
      "loss": 1.141,
      "step": 1319
    },
    {
      "epoch": 0.18557570645297342,
      "grad_norm": 1.464217185974121,
      "learning_rate": 0.00011682019215028262,
      "loss": 1.1862,
      "step": 1320
    },
    {
      "epoch": 0.1857162941093772,
      "grad_norm": 1.5675817728042603,
      "learning_rate": 0.00011659098951394356,
      "loss": 1.1354,
      "step": 1321
    },
    {
      "epoch": 0.18585688176578097,
      "grad_norm": 1.4344136714935303,
      "learning_rate": 0.00011636169721651548,
      "loss": 1.1758,
      "step": 1322
    },
    {
      "epoch": 0.18599746942218473,
      "grad_norm": 1.5571012496948242,
      "learning_rate": 0.00011613231649714073,
      "loss": 1.0417,
      "step": 1323
    },
    {
      "epoch": 0.1861380570785885,
      "grad_norm": 1.7190297842025757,
      "learning_rate": 0.00011590284859543943,
      "loss": 1.1797,
      "step": 1324
    },
    {
      "epoch": 0.18627864473499225,
      "grad_norm": 1.6015808582305908,
      "learning_rate": 0.00011567329475150288,
      "loss": 1.1435,
      "step": 1325
    },
    {
      "epoch": 0.18641923239139604,
      "grad_norm": 1.7364178895950317,
      "learning_rate": 0.00011544365620588687,
      "loss": 1.0509,
      "step": 1326
    },
    {
      "epoch": 0.1865598200477998,
      "grad_norm": 1.5401281118392944,
      "learning_rate": 0.00011521393419960489,
      "loss": 1.0546,
      "step": 1327
    },
    {
      "epoch": 0.18670040770420357,
      "grad_norm": 1.7021801471710205,
      "learning_rate": 0.00011498412997412151,
      "loss": 1.1009,
      "step": 1328
    },
    {
      "epoch": 0.18684099536060733,
      "grad_norm": 1.7119665145874023,
      "learning_rate": 0.00011475424477134557,
      "loss": 0.8585,
      "step": 1329
    },
    {
      "epoch": 0.1869815830170111,
      "grad_norm": 1.5866222381591797,
      "learning_rate": 0.00011452427983362362,
      "loss": 1.0964,
      "step": 1330
    },
    {
      "epoch": 0.18712217067341488,
      "grad_norm": 1.646464228630066,
      "learning_rate": 0.00011429423640373297,
      "loss": 1.1932,
      "step": 1331
    },
    {
      "epoch": 0.18726275832981865,
      "grad_norm": 1.5609792470932007,
      "learning_rate": 0.0001140641157248753,
      "loss": 1.0946,
      "step": 1332
    },
    {
      "epoch": 0.1874033459862224,
      "grad_norm": 1.5953153371810913,
      "learning_rate": 0.00011383391904066958,
      "loss": 1.0399,
      "step": 1333
    },
    {
      "epoch": 0.18754393364262617,
      "grad_norm": 1.5688011646270752,
      "learning_rate": 0.00011360364759514566,
      "loss": 1.1094,
      "step": 1334
    },
    {
      "epoch": 0.18768452129902993,
      "grad_norm": 1.4564937353134155,
      "learning_rate": 0.00011337330263273735,
      "loss": 1.1111,
      "step": 1335
    },
    {
      "epoch": 0.18782510895543372,
      "grad_norm": 1.6002782583236694,
      "learning_rate": 0.00011314288539827576,
      "loss": 0.9529,
      "step": 1336
    },
    {
      "epoch": 0.18796569661183749,
      "grad_norm": 1.491550087928772,
      "learning_rate": 0.00011291239713698263,
      "loss": 1.0693,
      "step": 1337
    },
    {
      "epoch": 0.18810628426824125,
      "grad_norm": 1.4589107036590576,
      "learning_rate": 0.00011268183909446348,
      "loss": 1.1449,
      "step": 1338
    },
    {
      "epoch": 0.188246871924645,
      "grad_norm": 1.3745561838150024,
      "learning_rate": 0.00011245121251670099,
      "loss": 1.224,
      "step": 1339
    },
    {
      "epoch": 0.18838745958104877,
      "grad_norm": 1.3769426345825195,
      "learning_rate": 0.00011222051865004814,
      "loss": 1.2929,
      "step": 1340
    },
    {
      "epoch": 0.18852804723745256,
      "grad_norm": 1.698346734046936,
      "learning_rate": 0.00011198975874122167,
      "loss": 1.1087,
      "step": 1341
    },
    {
      "epoch": 0.18866863489385632,
      "grad_norm": 1.5066137313842773,
      "learning_rate": 0.00011175893403729511,
      "loss": 1.2172,
      "step": 1342
    },
    {
      "epoch": 0.1888092225502601,
      "grad_norm": 1.5342847108840942,
      "learning_rate": 0.00011152804578569225,
      "loss": 1.1171,
      "step": 1343
    },
    {
      "epoch": 0.18894981020666385,
      "grad_norm": 1.5742909908294678,
      "learning_rate": 0.00011129709523418024,
      "loss": 1.0647,
      "step": 1344
    },
    {
      "epoch": 0.1890903978630676,
      "grad_norm": 1.591515302658081,
      "learning_rate": 0.00011106608363086293,
      "loss": 1.2038,
      "step": 1345
    },
    {
      "epoch": 0.1892309855194714,
      "grad_norm": 1.5007787942886353,
      "learning_rate": 0.00011083501222417411,
      "loss": 1.2787,
      "step": 1346
    },
    {
      "epoch": 0.18937157317587516,
      "grad_norm": 1.6680328845977783,
      "learning_rate": 0.00011060388226287079,
      "loss": 1.0636,
      "step": 1347
    },
    {
      "epoch": 0.18951216083227893,
      "grad_norm": 1.3917148113250732,
      "learning_rate": 0.00011037269499602637,
      "loss": 1.1025,
      "step": 1348
    },
    {
      "epoch": 0.1896527484886827,
      "grad_norm": 1.5578041076660156,
      "learning_rate": 0.00011014145167302396,
      "loss": 1.3563,
      "step": 1349
    },
    {
      "epoch": 0.18979333614508645,
      "grad_norm": 1.5441862344741821,
      "learning_rate": 0.00010991015354354962,
      "loss": 1.0881,
      "step": 1350
    },
    {
      "epoch": 0.18993392380149024,
      "grad_norm": 1.6318796873092651,
      "learning_rate": 0.00010967880185758557,
      "loss": 1.076,
      "step": 1351
    },
    {
      "epoch": 0.190074511457894,
      "grad_norm": 1.6025941371917725,
      "learning_rate": 0.00010944739786540348,
      "loss": 1.1541,
      "step": 1352
    },
    {
      "epoch": 0.19021509911429776,
      "grad_norm": 1.658018946647644,
      "learning_rate": 0.0001092159428175577,
      "loss": 1.0439,
      "step": 1353
    },
    {
      "epoch": 0.19035568677070153,
      "grad_norm": 1.637986183166504,
      "learning_rate": 0.0001089844379648785,
      "loss": 0.9705,
      "step": 1354
    },
    {
      "epoch": 0.1904962744271053,
      "grad_norm": 2.000197410583496,
      "learning_rate": 0.00010875288455846519,
      "loss": 1.0934,
      "step": 1355
    },
    {
      "epoch": 0.19063686208350908,
      "grad_norm": 1.6505094766616821,
      "learning_rate": 0.00010852128384967975,
      "loss": 1.3775,
      "step": 1356
    },
    {
      "epoch": 0.19077744973991284,
      "grad_norm": 1.5769799947738647,
      "learning_rate": 0.00010828963709013948,
      "loss": 1.135,
      "step": 1357
    },
    {
      "epoch": 0.1909180373963166,
      "grad_norm": 1.4312957525253296,
      "learning_rate": 0.00010805794553171076,
      "loss": 1.2252,
      "step": 1358
    },
    {
      "epoch": 0.19105862505272037,
      "grad_norm": 1.4554307460784912,
      "learning_rate": 0.00010782621042650194,
      "loss": 1.2334,
      "step": 1359
    },
    {
      "epoch": 0.19119921270912413,
      "grad_norm": 1.6789618730545044,
      "learning_rate": 0.00010759443302685679,
      "loss": 1.0231,
      "step": 1360
    },
    {
      "epoch": 0.19133980036552792,
      "grad_norm": 1.475894808769226,
      "learning_rate": 0.00010736261458534764,
      "loss": 1.1066,
      "step": 1361
    },
    {
      "epoch": 0.19148038802193168,
      "grad_norm": 1.6469441652297974,
      "learning_rate": 0.00010713075635476854,
      "loss": 1.1275,
      "step": 1362
    },
    {
      "epoch": 0.19162097567833544,
      "grad_norm": 1.6057630777359009,
      "learning_rate": 0.00010689885958812871,
      "loss": 1.0155,
      "step": 1363
    },
    {
      "epoch": 0.1917615633347392,
      "grad_norm": 1.4159632921218872,
      "learning_rate": 0.00010666692553864545,
      "loss": 1.0388,
      "step": 1364
    },
    {
      "epoch": 0.19190215099114297,
      "grad_norm": 1.377211332321167,
      "learning_rate": 0.0001064349554597377,
      "loss": 1.1202,
      "step": 1365
    },
    {
      "epoch": 0.19204273864754676,
      "grad_norm": 1.4382482767105103,
      "learning_rate": 0.00010620295060501901,
      "loss": 0.9757,
      "step": 1366
    },
    {
      "epoch": 0.19218332630395052,
      "grad_norm": 1.9213197231292725,
      "learning_rate": 0.00010597091222829097,
      "loss": 1.1281,
      "step": 1367
    },
    {
      "epoch": 0.19232391396035428,
      "grad_norm": 1.5118399858474731,
      "learning_rate": 0.0001057388415835362,
      "loss": 1.1234,
      "step": 1368
    },
    {
      "epoch": 0.19246450161675804,
      "grad_norm": 1.5118408203125,
      "learning_rate": 0.00010550673992491178,
      "loss": 1.0254,
      "step": 1369
    },
    {
      "epoch": 0.1926050892731618,
      "grad_norm": 1.9071398973464966,
      "learning_rate": 0.00010527460850674237,
      "loss": 1.0211,
      "step": 1370
    },
    {
      "epoch": 0.1927456769295656,
      "grad_norm": 1.5908268690109253,
      "learning_rate": 0.00010504244858351353,
      "loss": 1.1597,
      "step": 1371
    },
    {
      "epoch": 0.19288626458596936,
      "grad_norm": 1.800240159034729,
      "learning_rate": 0.00010481026140986474,
      "loss": 0.8931,
      "step": 1372
    },
    {
      "epoch": 0.19302685224237312,
      "grad_norm": 1.702511191368103,
      "learning_rate": 0.00010457804824058282,
      "loss": 1.0382,
      "step": 1373
    },
    {
      "epoch": 0.19316743989877688,
      "grad_norm": 1.5173929929733276,
      "learning_rate": 0.0001043458103305951,
      "loss": 1.2864,
      "step": 1374
    },
    {
      "epoch": 0.19330802755518064,
      "grad_norm": 1.554733395576477,
      "learning_rate": 0.00010411354893496252,
      "loss": 1.1777,
      "step": 1375
    },
    {
      "epoch": 0.19344861521158443,
      "grad_norm": 1.7179187536239624,
      "learning_rate": 0.00010388126530887307,
      "loss": 1.1325,
      "step": 1376
    },
    {
      "epoch": 0.1935892028679882,
      "grad_norm": 1.6883199214935303,
      "learning_rate": 0.00010364896070763482,
      "loss": 1.1589,
      "step": 1377
    },
    {
      "epoch": 0.19372979052439196,
      "grad_norm": 1.8239678144454956,
      "learning_rate": 0.00010341663638666912,
      "loss": 1.0661,
      "step": 1378
    },
    {
      "epoch": 0.19387037818079572,
      "grad_norm": 1.7536067962646484,
      "learning_rate": 0.000103184293601504,
      "loss": 1.1754,
      "step": 1379
    },
    {
      "epoch": 0.19401096583719948,
      "grad_norm": 1.6542768478393555,
      "learning_rate": 0.00010295193360776721,
      "loss": 1.2543,
      "step": 1380
    },
    {
      "epoch": 0.19415155349360327,
      "grad_norm": 1.7918944358825684,
      "learning_rate": 0.00010271955766117951,
      "loss": 1.2192,
      "step": 1381
    },
    {
      "epoch": 0.19429214115000704,
      "grad_norm": 1.6903663873672485,
      "learning_rate": 0.0001024871670175479,
      "loss": 1.1681,
      "step": 1382
    },
    {
      "epoch": 0.1944327288064108,
      "grad_norm": 1.529369592666626,
      "learning_rate": 0.00010225476293275877,
      "loss": 1.1418,
      "step": 1383
    },
    {
      "epoch": 0.19457331646281456,
      "grad_norm": 1.425983190536499,
      "learning_rate": 0.00010202234666277115,
      "loss": 1.0477,
      "step": 1384
    },
    {
      "epoch": 0.19471390411921832,
      "grad_norm": 1.654103398323059,
      "learning_rate": 0.00010178991946360998,
      "loss": 1.164,
      "step": 1385
    },
    {
      "epoch": 0.1948544917756221,
      "grad_norm": 1.5258557796478271,
      "learning_rate": 0.0001015574825913592,
      "loss": 1.0823,
      "step": 1386
    },
    {
      "epoch": 0.19499507943202588,
      "grad_norm": 1.6584981679916382,
      "learning_rate": 0.00010132503730215505,
      "loss": 1.1922,
      "step": 1387
    },
    {
      "epoch": 0.19513566708842964,
      "grad_norm": 1.5751091241836548,
      "learning_rate": 0.00010109258485217923,
      "loss": 1.0277,
      "step": 1388
    },
    {
      "epoch": 0.1952762547448334,
      "grad_norm": 1.5051928758621216,
      "learning_rate": 0.00010086012649765218,
      "loss": 1.2561,
      "step": 1389
    },
    {
      "epoch": 0.19541684240123716,
      "grad_norm": 1.5144636631011963,
      "learning_rate": 0.00010062766349482622,
      "loss": 1.1918,
      "step": 1390
    },
    {
      "epoch": 0.19555743005764095,
      "grad_norm": 1.8928519487380981,
      "learning_rate": 0.00010039519709997877,
      "loss": 1.0145,
      "step": 1391
    },
    {
      "epoch": 0.19569801771404471,
      "grad_norm": 1.6500763893127441,
      "learning_rate": 0.00010016272856940567,
      "loss": 1.1139,
      "step": 1392
    },
    {
      "epoch": 0.19583860537044848,
      "grad_norm": 1.5982919931411743,
      "learning_rate": 9.993025915941418e-05,
      "loss": 1.1678,
      "step": 1393
    },
    {
      "epoch": 0.19597919302685224,
      "grad_norm": 1.514268398284912,
      "learning_rate": 9.969779012631643e-05,
      "loss": 1.162,
      "step": 1394
    },
    {
      "epoch": 0.196119780683256,
      "grad_norm": 1.5842721462249756,
      "learning_rate": 9.946532272642243e-05,
      "loss": 0.9992,
      "step": 1395
    },
    {
      "epoch": 0.1962603683396598,
      "grad_norm": 1.8290197849273682,
      "learning_rate": 9.923285821603341e-05,
      "loss": 1.1016,
      "step": 1396
    },
    {
      "epoch": 0.19640095599606355,
      "grad_norm": 1.3618017435073853,
      "learning_rate": 9.900039785143495e-05,
      "loss": 0.9754,
      "step": 1397
    },
    {
      "epoch": 0.19654154365246732,
      "grad_norm": 1.7173397541046143,
      "learning_rate": 9.876794288889024e-05,
      "loss": 1.0833,
      "step": 1398
    },
    {
      "epoch": 0.19668213130887108,
      "grad_norm": 1.6006220579147339,
      "learning_rate": 9.853549458463329e-05,
      "loss": 1.1778,
      "step": 1399
    },
    {
      "epoch": 0.19682271896527484,
      "grad_norm": 1.4291013479232788,
      "learning_rate": 9.830305419486212e-05,
      "loss": 1.0909,
      "step": 1400
    },
    {
      "epoch": 0.19696330662167863,
      "grad_norm": 1.3172802925109863,
      "learning_rate": 9.807062297573196e-05,
      "loss": 1.2324,
      "step": 1401
    },
    {
      "epoch": 0.1971038942780824,
      "grad_norm": 1.4713881015777588,
      "learning_rate": 9.783820218334848e-05,
      "loss": 1.1855,
      "step": 1402
    },
    {
      "epoch": 0.19724448193448615,
      "grad_norm": 1.5639885663986206,
      "learning_rate": 9.760579307376104e-05,
      "loss": 1.074,
      "step": 1403
    },
    {
      "epoch": 0.19738506959088992,
      "grad_norm": 1.7781959772109985,
      "learning_rate": 9.737339690295583e-05,
      "loss": 0.9695,
      "step": 1404
    },
    {
      "epoch": 0.19752565724729368,
      "grad_norm": 1.857828974723816,
      "learning_rate": 9.714101492684913e-05,
      "loss": 1.0935,
      "step": 1405
    },
    {
      "epoch": 0.19766624490369747,
      "grad_norm": 1.5970592498779297,
      "learning_rate": 9.69086484012805e-05,
      "loss": 1.1547,
      "step": 1406
    },
    {
      "epoch": 0.19780683256010123,
      "grad_norm": 1.5884848833084106,
      "learning_rate": 9.667629858200602e-05,
      "loss": 1.2728,
      "step": 1407
    },
    {
      "epoch": 0.197947420216505,
      "grad_norm": 1.6224325895309448,
      "learning_rate": 9.644396672469145e-05,
      "loss": 1.1774,
      "step": 1408
    },
    {
      "epoch": 0.19808800787290876,
      "grad_norm": 1.6667064428329468,
      "learning_rate": 9.621165408490552e-05,
      "loss": 1.2981,
      "step": 1409
    },
    {
      "epoch": 0.19822859552931252,
      "grad_norm": 1.56403386592865,
      "learning_rate": 9.597936191811307e-05,
      "loss": 1.0784,
      "step": 1410
    },
    {
      "epoch": 0.1983691831857163,
      "grad_norm": 1.404476523399353,
      "learning_rate": 9.574709147966834e-05,
      "loss": 1.0627,
      "step": 1411
    },
    {
      "epoch": 0.19850977084212007,
      "grad_norm": 1.4382264614105225,
      "learning_rate": 9.55148440248081e-05,
      "loss": 1.1818,
      "step": 1412
    },
    {
      "epoch": 0.19865035849852383,
      "grad_norm": 1.2532752752304077,
      "learning_rate": 9.528262080864496e-05,
      "loss": 1.2242,
      "step": 1413
    },
    {
      "epoch": 0.1987909461549276,
      "grad_norm": 1.4197824001312256,
      "learning_rate": 9.505042308616046e-05,
      "loss": 1.1909,
      "step": 1414
    },
    {
      "epoch": 0.19893153381133136,
      "grad_norm": 1.6274960041046143,
      "learning_rate": 9.481825211219847e-05,
      "loss": 1.1785,
      "step": 1415
    },
    {
      "epoch": 0.19907212146773512,
      "grad_norm": 1.521952748298645,
      "learning_rate": 9.458610914145826e-05,
      "loss": 1.0457,
      "step": 1416
    },
    {
      "epoch": 0.1992127091241389,
      "grad_norm": 1.5183908939361572,
      "learning_rate": 9.435399542848773e-05,
      "loss": 1.141,
      "step": 1417
    },
    {
      "epoch": 0.19935329678054267,
      "grad_norm": 1.4806602001190186,
      "learning_rate": 9.412191222767673e-05,
      "loss": 1.0853,
      "step": 1418
    },
    {
      "epoch": 0.19949388443694643,
      "grad_norm": 1.4998570680618286,
      "learning_rate": 9.388986079325012e-05,
      "loss": 1.0522,
      "step": 1419
    },
    {
      "epoch": 0.1996344720933502,
      "grad_norm": 1.5656377077102661,
      "learning_rate": 9.365784237926122e-05,
      "loss": 1.2075,
      "step": 1420
    },
    {
      "epoch": 0.19977505974975396,
      "grad_norm": 1.370351791381836,
      "learning_rate": 9.342585823958481e-05,
      "loss": 1.2778,
      "step": 1421
    },
    {
      "epoch": 0.19991564740615775,
      "grad_norm": 1.576250672340393,
      "learning_rate": 9.319390962791044e-05,
      "loss": 1.1349,
      "step": 1422
    },
    {
      "epoch": 0.2000562350625615,
      "grad_norm": 1.7526202201843262,
      "learning_rate": 9.296199779773569e-05,
      "loss": 0.9806,
      "step": 1423
    },
    {
      "epoch": 0.20019682271896527,
      "grad_norm": 1.5378177165985107,
      "learning_rate": 9.273012400235938e-05,
      "loss": 1.3376,
      "step": 1424
    },
    {
      "epoch": 0.20033741037536903,
      "grad_norm": 1.6682908535003662,
      "learning_rate": 9.249828949487478e-05,
      "loss": 1.0819,
      "step": 1425
    },
    {
      "epoch": 0.2004779980317728,
      "grad_norm": 1.3697816133499146,
      "learning_rate": 9.226649552816277e-05,
      "loss": 1.2011,
      "step": 1426
    },
    {
      "epoch": 0.2006185856881766,
      "grad_norm": 1.4844970703125,
      "learning_rate": 9.203474335488524e-05,
      "loss": 1.1425,
      "step": 1427
    },
    {
      "epoch": 0.20075917334458035,
      "grad_norm": 1.4531599283218384,
      "learning_rate": 9.180303422747814e-05,
      "loss": 1.0955,
      "step": 1428
    },
    {
      "epoch": 0.2008997610009841,
      "grad_norm": 2.0588364601135254,
      "learning_rate": 9.157136939814481e-05,
      "loss": 1.1304,
      "step": 1429
    },
    {
      "epoch": 0.20104034865738787,
      "grad_norm": 1.7277637720108032,
      "learning_rate": 9.133975011884925e-05,
      "loss": 1.1107,
      "step": 1430
    },
    {
      "epoch": 0.20118093631379164,
      "grad_norm": 1.5379058122634888,
      "learning_rate": 9.110817764130925e-05,
      "loss": 1.0152,
      "step": 1431
    },
    {
      "epoch": 0.20132152397019543,
      "grad_norm": 1.8112249374389648,
      "learning_rate": 9.087665321698962e-05,
      "loss": 1.1344,
      "step": 1432
    },
    {
      "epoch": 0.2014621116265992,
      "grad_norm": 1.565848708152771,
      "learning_rate": 9.06451780970956e-05,
      "loss": 1.0578,
      "step": 1433
    },
    {
      "epoch": 0.20160269928300295,
      "grad_norm": 1.643454909324646,
      "learning_rate": 9.041375353256591e-05,
      "loss": 1.1706,
      "step": 1434
    },
    {
      "epoch": 0.2017432869394067,
      "grad_norm": 1.6742842197418213,
      "learning_rate": 9.018238077406607e-05,
      "loss": 1.2154,
      "step": 1435
    },
    {
      "epoch": 0.20188387459581048,
      "grad_norm": 1.474034070968628,
      "learning_rate": 8.995106107198161e-05,
      "loss": 1.1003,
      "step": 1436
    },
    {
      "epoch": 0.20202446225221427,
      "grad_norm": 1.6321419477462769,
      "learning_rate": 8.971979567641135e-05,
      "loss": 1.1107,
      "step": 1437
    },
    {
      "epoch": 0.20216504990861803,
      "grad_norm": 1.3588474988937378,
      "learning_rate": 8.948858583716065e-05,
      "loss": 1.2893,
      "step": 1438
    },
    {
      "epoch": 0.2023056375650218,
      "grad_norm": 1.5399224758148193,
      "learning_rate": 8.925743280373459e-05,
      "loss": 1.1653,
      "step": 1439
    },
    {
      "epoch": 0.20244622522142555,
      "grad_norm": 1.5717051029205322,
      "learning_rate": 8.902633782533126e-05,
      "loss": 1.192,
      "step": 1440
    },
    {
      "epoch": 0.20258681287782931,
      "grad_norm": 1.6661121845245361,
      "learning_rate": 8.879530215083505e-05,
      "loss": 1.1465,
      "step": 1441
    },
    {
      "epoch": 0.2027274005342331,
      "grad_norm": 1.6912463903427124,
      "learning_rate": 8.856432702880984e-05,
      "loss": 1.1298,
      "step": 1442
    },
    {
      "epoch": 0.20286798819063687,
      "grad_norm": 1.5537707805633545,
      "learning_rate": 8.833341370749222e-05,
      "loss": 1.0928,
      "step": 1443
    },
    {
      "epoch": 0.20300857584704063,
      "grad_norm": 1.369154453277588,
      "learning_rate": 8.810256343478492e-05,
      "loss": 1.1032,
      "step": 1444
    },
    {
      "epoch": 0.2031491635034444,
      "grad_norm": 1.3402235507965088,
      "learning_rate": 8.787177745824982e-05,
      "loss": 1.1507,
      "step": 1445
    },
    {
      "epoch": 0.20328975115984815,
      "grad_norm": 1.307456135749817,
      "learning_rate": 8.76410570251014e-05,
      "loss": 1.1453,
      "step": 1446
    },
    {
      "epoch": 0.20343033881625194,
      "grad_norm": 1.370740532875061,
      "learning_rate": 8.741040338219988e-05,
      "loss": 1.1765,
      "step": 1447
    },
    {
      "epoch": 0.2035709264726557,
      "grad_norm": 1.6243501901626587,
      "learning_rate": 8.717981777604458e-05,
      "loss": 1.1154,
      "step": 1448
    },
    {
      "epoch": 0.20371151412905947,
      "grad_norm": 1.480530023574829,
      "learning_rate": 8.694930145276712e-05,
      "loss": 1.1484,
      "step": 1449
    },
    {
      "epoch": 0.20385210178546323,
      "grad_norm": 1.628445029258728,
      "learning_rate": 8.671885565812467e-05,
      "loss": 1.1773,
      "step": 1450
    },
    {
      "epoch": 0.203992689441867,
      "grad_norm": 1.4507479667663574,
      "learning_rate": 8.64884816374933e-05,
      "loss": 1.0243,
      "step": 1451
    },
    {
      "epoch": 0.20413327709827078,
      "grad_norm": 1.5000957250595093,
      "learning_rate": 8.625818063586117e-05,
      "loss": 1.1115,
      "step": 1452
    },
    {
      "epoch": 0.20427386475467454,
      "grad_norm": 1.6769925355911255,
      "learning_rate": 8.602795389782178e-05,
      "loss": 1.136,
      "step": 1453
    },
    {
      "epoch": 0.2044144524110783,
      "grad_norm": 1.603663444519043,
      "learning_rate": 8.579780266756745e-05,
      "loss": 1.1332,
      "step": 1454
    },
    {
      "epoch": 0.20455504006748207,
      "grad_norm": 1.7009077072143555,
      "learning_rate": 8.556772818888228e-05,
      "loss": 1.1123,
      "step": 1455
    },
    {
      "epoch": 0.20469562772388583,
      "grad_norm": 1.4416491985321045,
      "learning_rate": 8.533773170513565e-05,
      "loss": 1.1668,
      "step": 1456
    },
    {
      "epoch": 0.20483621538028962,
      "grad_norm": 1.7653838396072388,
      "learning_rate": 8.510781445927545e-05,
      "loss": 1.032,
      "step": 1457
    },
    {
      "epoch": 0.20497680303669338,
      "grad_norm": 1.6930487155914307,
      "learning_rate": 8.487797769382134e-05,
      "loss": 1.1271,
      "step": 1458
    },
    {
      "epoch": 0.20511739069309715,
      "grad_norm": 1.3983714580535889,
      "learning_rate": 8.464822265085802e-05,
      "loss": 1.0839,
      "step": 1459
    },
    {
      "epoch": 0.2052579783495009,
      "grad_norm": 1.7862848043441772,
      "learning_rate": 8.441855057202859e-05,
      "loss": 1.0208,
      "step": 1460
    },
    {
      "epoch": 0.20539856600590467,
      "grad_norm": 1.6046888828277588,
      "learning_rate": 8.418896269852779e-05,
      "loss": 1.2313,
      "step": 1461
    },
    {
      "epoch": 0.20553915366230846,
      "grad_norm": 1.8656378984451294,
      "learning_rate": 8.395946027109524e-05,
      "loss": 1.2311,
      "step": 1462
    },
    {
      "epoch": 0.20567974131871222,
      "grad_norm": 1.6971181631088257,
      "learning_rate": 8.373004453000893e-05,
      "loss": 1.2033,
      "step": 1463
    },
    {
      "epoch": 0.20582032897511598,
      "grad_norm": 2.239182472229004,
      "learning_rate": 8.350071671507819e-05,
      "loss": 1.0428,
      "step": 1464
    },
    {
      "epoch": 0.20596091663151975,
      "grad_norm": 1.635581374168396,
      "learning_rate": 8.327147806563733e-05,
      "loss": 0.923,
      "step": 1465
    },
    {
      "epoch": 0.2061015042879235,
      "grad_norm": 1.7282065153121948,
      "learning_rate": 8.30423298205387e-05,
      "loss": 1.3557,
      "step": 1466
    },
    {
      "epoch": 0.2062420919443273,
      "grad_norm": 1.5362571477890015,
      "learning_rate": 8.281327321814615e-05,
      "loss": 1.0538,
      "step": 1467
    },
    {
      "epoch": 0.20638267960073106,
      "grad_norm": 1.5600779056549072,
      "learning_rate": 8.258430949632823e-05,
      "loss": 1.1058,
      "step": 1468
    },
    {
      "epoch": 0.20652326725713482,
      "grad_norm": 1.4205633401870728,
      "learning_rate": 8.235543989245159e-05,
      "loss": 1.1746,
      "step": 1469
    },
    {
      "epoch": 0.20666385491353859,
      "grad_norm": 1.5221099853515625,
      "learning_rate": 8.212666564337415e-05,
      "loss": 1.0727,
      "step": 1470
    },
    {
      "epoch": 0.20680444256994235,
      "grad_norm": 1.4003214836120605,
      "learning_rate": 8.189798798543868e-05,
      "loss": 1.2705,
      "step": 1471
    },
    {
      "epoch": 0.20694503022634614,
      "grad_norm": 1.9143023490905762,
      "learning_rate": 8.166940815446576e-05,
      "loss": 1.0249,
      "step": 1472
    },
    {
      "epoch": 0.2070856178827499,
      "grad_norm": 1.6624321937561035,
      "learning_rate": 8.144092738574745e-05,
      "loss": 1.0697,
      "step": 1473
    },
    {
      "epoch": 0.20722620553915366,
      "grad_norm": 1.740128517150879,
      "learning_rate": 8.12125469140404e-05,
      "loss": 1.1407,
      "step": 1474
    },
    {
      "epoch": 0.20736679319555743,
      "grad_norm": 1.4182095527648926,
      "learning_rate": 8.098426797355919e-05,
      "loss": 1.2012,
      "step": 1475
    },
    {
      "epoch": 0.2075073808519612,
      "grad_norm": 1.513156533241272,
      "learning_rate": 8.075609179796976e-05,
      "loss": 1.0681,
      "step": 1476
    },
    {
      "epoch": 0.20764796850836498,
      "grad_norm": 1.4541505575180054,
      "learning_rate": 8.052801962038268e-05,
      "loss": 1.0625,
      "step": 1477
    },
    {
      "epoch": 0.20778855616476874,
      "grad_norm": 1.6394994258880615,
      "learning_rate": 8.030005267334647e-05,
      "loss": 1.0094,
      "step": 1478
    },
    {
      "epoch": 0.2079291438211725,
      "grad_norm": 1.4724233150482178,
      "learning_rate": 8.007219218884096e-05,
      "loss": 1.1438,
      "step": 1479
    },
    {
      "epoch": 0.20806973147757626,
      "grad_norm": 1.6223320960998535,
      "learning_rate": 7.984443939827072e-05,
      "loss": 1.14,
      "step": 1480
    },
    {
      "epoch": 0.20821031913398003,
      "grad_norm": 1.6267187595367432,
      "learning_rate": 7.961679553245816e-05,
      "loss": 1.0042,
      "step": 1481
    },
    {
      "epoch": 0.20835090679038382,
      "grad_norm": 1.7666763067245483,
      "learning_rate": 7.938926182163719e-05,
      "loss": 0.9712,
      "step": 1482
    },
    {
      "epoch": 0.20849149444678758,
      "grad_norm": 1.5993558168411255,
      "learning_rate": 7.916183949544635e-05,
      "loss": 1.1248,
      "step": 1483
    },
    {
      "epoch": 0.20863208210319134,
      "grad_norm": 1.487294316291809,
      "learning_rate": 7.893452978292226e-05,
      "loss": 1.1371,
      "step": 1484
    },
    {
      "epoch": 0.2087726697595951,
      "grad_norm": 1.38978910446167,
      "learning_rate": 7.870733391249292e-05,
      "loss": 1.1734,
      "step": 1485
    },
    {
      "epoch": 0.20891325741599887,
      "grad_norm": 1.5067280530929565,
      "learning_rate": 7.84802531119711e-05,
      "loss": 1.1935,
      "step": 1486
    },
    {
      "epoch": 0.20905384507240266,
      "grad_norm": 1.6772255897521973,
      "learning_rate": 7.825328860854778e-05,
      "loss": 1.0135,
      "step": 1487
    },
    {
      "epoch": 0.20919443272880642,
      "grad_norm": 1.6285632848739624,
      "learning_rate": 7.802644162878537e-05,
      "loss": 1.059,
      "step": 1488
    },
    {
      "epoch": 0.20933502038521018,
      "grad_norm": 1.5710265636444092,
      "learning_rate": 7.779971339861119e-05,
      "loss": 0.9314,
      "step": 1489
    },
    {
      "epoch": 0.20947560804161394,
      "grad_norm": 1.6923407316207886,
      "learning_rate": 7.75731051433108e-05,
      "loss": 1.0963,
      "step": 1490
    },
    {
      "epoch": 0.2096161956980177,
      "grad_norm": 1.6393663883209229,
      "learning_rate": 7.73466180875214e-05,
      "loss": 1.2781,
      "step": 1491
    },
    {
      "epoch": 0.2097567833544215,
      "grad_norm": 1.5296030044555664,
      "learning_rate": 7.712025345522521e-05,
      "loss": 1.1399,
      "step": 1492
    },
    {
      "epoch": 0.20989737101082526,
      "grad_norm": 1.5098450183868408,
      "learning_rate": 7.689401246974286e-05,
      "loss": 1.1963,
      "step": 1493
    },
    {
      "epoch": 0.21003795866722902,
      "grad_norm": 1.5548814535140991,
      "learning_rate": 7.666789635372673e-05,
      "loss": 1.1042,
      "step": 1494
    },
    {
      "epoch": 0.21017854632363278,
      "grad_norm": 1.5408406257629395,
      "learning_rate": 7.64419063291544e-05,
      "loss": 1.0141,
      "step": 1495
    },
    {
      "epoch": 0.21031913398003654,
      "grad_norm": 1.4169336557388306,
      "learning_rate": 7.621604361732204e-05,
      "loss": 0.9347,
      "step": 1496
    },
    {
      "epoch": 0.21045972163644033,
      "grad_norm": 1.4732944965362549,
      "learning_rate": 7.599030943883776e-05,
      "loss": 1.1267,
      "step": 1497
    },
    {
      "epoch": 0.2106003092928441,
      "grad_norm": 1.4074366092681885,
      "learning_rate": 7.576470501361509e-05,
      "loss": 1.123,
      "step": 1498
    },
    {
      "epoch": 0.21074089694924786,
      "grad_norm": 1.6032438278198242,
      "learning_rate": 7.55392315608663e-05,
      "loss": 1.0817,
      "step": 1499
    },
    {
      "epoch": 0.21088148460565162,
      "grad_norm": 1.828813910484314,
      "learning_rate": 7.531389029909594e-05,
      "loss": 1.1348,
      "step": 1500
    },
    {
      "epoch": 0.21088148460565162,
      "eval_loss": 1.160723328590393,
      "eval_runtime": 771.7664,
      "eval_samples_per_second": 16.386,
      "eval_steps_per_second": 8.193,
      "step": 1500
    },
    {
      "epoch": 0.21102207226205538,
      "grad_norm": 1.3949626684188843,
      "learning_rate": 7.508868244609404e-05,
      "loss": 1.1978,
      "step": 1501
    },
    {
      "epoch": 0.21116265991845917,
      "grad_norm": 1.5324956178665161,
      "learning_rate": 7.486360921892981e-05,
      "loss": 1.3277,
      "step": 1502
    },
    {
      "epoch": 0.21130324757486293,
      "grad_norm": 1.564859390258789,
      "learning_rate": 7.463867183394481e-05,
      "loss": 1.2096,
      "step": 1503
    },
    {
      "epoch": 0.2114438352312667,
      "grad_norm": 1.4992648363113403,
      "learning_rate": 7.441387150674655e-05,
      "loss": 1.0759,
      "step": 1504
    },
    {
      "epoch": 0.21158442288767046,
      "grad_norm": 1.35817289352417,
      "learning_rate": 7.418920945220178e-05,
      "loss": 1.0182,
      "step": 1505
    },
    {
      "epoch": 0.21172501054407422,
      "grad_norm": 1.4540961980819702,
      "learning_rate": 7.396468688443006e-05,
      "loss": 1.1367,
      "step": 1506
    },
    {
      "epoch": 0.211865598200478,
      "grad_norm": 1.890587329864502,
      "learning_rate": 7.37403050167971e-05,
      "loss": 1.2135,
      "step": 1507
    },
    {
      "epoch": 0.21200618585688177,
      "grad_norm": 1.4686682224273682,
      "learning_rate": 7.351606506190823e-05,
      "loss": 1.0165,
      "step": 1508
    },
    {
      "epoch": 0.21214677351328554,
      "grad_norm": 1.4976205825805664,
      "learning_rate": 7.32919682316019e-05,
      "loss": 1.0227,
      "step": 1509
    },
    {
      "epoch": 0.2122873611696893,
      "grad_norm": 1.6460814476013184,
      "learning_rate": 7.306801573694304e-05,
      "loss": 1.1969,
      "step": 1510
    },
    {
      "epoch": 0.21242794882609306,
      "grad_norm": 1.4040446281433105,
      "learning_rate": 7.284420878821657e-05,
      "loss": 1.143,
      "step": 1511
    },
    {
      "epoch": 0.21256853648249685,
      "grad_norm": 1.6532279253005981,
      "learning_rate": 7.262054859492091e-05,
      "loss": 1.058,
      "step": 1512
    },
    {
      "epoch": 0.2127091241389006,
      "grad_norm": 1.4413868188858032,
      "learning_rate": 7.239703636576129e-05,
      "loss": 1.0859,
      "step": 1513
    },
    {
      "epoch": 0.21284971179530437,
      "grad_norm": 1.8252434730529785,
      "learning_rate": 7.217367330864337e-05,
      "loss": 1.0662,
      "step": 1514
    },
    {
      "epoch": 0.21299029945170814,
      "grad_norm": 1.4108558893203735,
      "learning_rate": 7.195046063066661e-05,
      "loss": 1.1807,
      "step": 1515
    },
    {
      "epoch": 0.2131308871081119,
      "grad_norm": 1.4603064060211182,
      "learning_rate": 7.172739953811787e-05,
      "loss": 1.1337,
      "step": 1516
    },
    {
      "epoch": 0.21327147476451566,
      "grad_norm": 1.3226951360702515,
      "learning_rate": 7.15044912364647e-05,
      "loss": 1.1019,
      "step": 1517
    },
    {
      "epoch": 0.21341206242091945,
      "grad_norm": 1.5486570596694946,
      "learning_rate": 7.128173693034904e-05,
      "loss": 1.0859,
      "step": 1518
    },
    {
      "epoch": 0.21355265007732321,
      "grad_norm": 1.597848892211914,
      "learning_rate": 7.105913782358055e-05,
      "loss": 1.2847,
      "step": 1519
    },
    {
      "epoch": 0.21369323773372698,
      "grad_norm": 1.462699294090271,
      "learning_rate": 7.083669511913015e-05,
      "loss": 1.0937,
      "step": 1520
    },
    {
      "epoch": 0.21383382539013074,
      "grad_norm": 1.7216817140579224,
      "learning_rate": 7.061441001912359e-05,
      "loss": 1.0491,
      "step": 1521
    },
    {
      "epoch": 0.2139744130465345,
      "grad_norm": 1.6553010940551758,
      "learning_rate": 7.039228372483485e-05,
      "loss": 1.0338,
      "step": 1522
    },
    {
      "epoch": 0.2141150007029383,
      "grad_norm": 1.47225821018219,
      "learning_rate": 7.017031743667969e-05,
      "loss": 1.1387,
      "step": 1523
    },
    {
      "epoch": 0.21425558835934205,
      "grad_norm": 1.6392923593521118,
      "learning_rate": 6.994851235420918e-05,
      "loss": 1.1056,
      "step": 1524
    },
    {
      "epoch": 0.21439617601574582,
      "grad_norm": 1.4510540962219238,
      "learning_rate": 6.97268696761032e-05,
      "loss": 1.0782,
      "step": 1525
    },
    {
      "epoch": 0.21453676367214958,
      "grad_norm": 1.5914250612258911,
      "learning_rate": 6.950539060016395e-05,
      "loss": 1.1665,
      "step": 1526
    },
    {
      "epoch": 0.21467735132855334,
      "grad_norm": 1.6850095987319946,
      "learning_rate": 6.928407632330951e-05,
      "loss": 1.0402,
      "step": 1527
    },
    {
      "epoch": 0.21481793898495713,
      "grad_norm": 1.605427861213684,
      "learning_rate": 6.906292804156734e-05,
      "loss": 1.0249,
      "step": 1528
    },
    {
      "epoch": 0.2149585266413609,
      "grad_norm": 1.5581754446029663,
      "learning_rate": 6.88419469500678e-05,
      "loss": 1.1331,
      "step": 1529
    },
    {
      "epoch": 0.21509911429776465,
      "grad_norm": 1.5524507761001587,
      "learning_rate": 6.862113424303778e-05,
      "loss": 1.1539,
      "step": 1530
    },
    {
      "epoch": 0.21523970195416842,
      "grad_norm": 1.4787278175354004,
      "learning_rate": 6.840049111379416e-05,
      "loss": 1.1762,
      "step": 1531
    },
    {
      "epoch": 0.21538028961057218,
      "grad_norm": 1.5650027990341187,
      "learning_rate": 6.818001875473737e-05,
      "loss": 1.1503,
      "step": 1532
    },
    {
      "epoch": 0.21552087726697597,
      "grad_norm": 1.7275055646896362,
      "learning_rate": 6.7959718357345e-05,
      "loss": 1.1753,
      "step": 1533
    },
    {
      "epoch": 0.21566146492337973,
      "grad_norm": 1.7634620666503906,
      "learning_rate": 6.773959111216526e-05,
      "loss": 1.1478,
      "step": 1534
    },
    {
      "epoch": 0.2158020525797835,
      "grad_norm": 1.4024220705032349,
      "learning_rate": 6.751963820881068e-05,
      "loss": 1.1634,
      "step": 1535
    },
    {
      "epoch": 0.21594264023618726,
      "grad_norm": 1.4842162132263184,
      "learning_rate": 6.729986083595159e-05,
      "loss": 1.0393,
      "step": 1536
    },
    {
      "epoch": 0.21608322789259102,
      "grad_norm": 1.3746347427368164,
      "learning_rate": 6.708026018130969e-05,
      "loss": 1.2163,
      "step": 1537
    },
    {
      "epoch": 0.2162238155489948,
      "grad_norm": 1.506125807762146,
      "learning_rate": 6.686083743165166e-05,
      "loss": 1.0322,
      "step": 1538
    },
    {
      "epoch": 0.21636440320539857,
      "grad_norm": 1.4059820175170898,
      "learning_rate": 6.66415937727828e-05,
      "loss": 1.0576,
      "step": 1539
    },
    {
      "epoch": 0.21650499086180233,
      "grad_norm": 1.605995535850525,
      "learning_rate": 6.64225303895405e-05,
      "loss": 1.0361,
      "step": 1540
    },
    {
      "epoch": 0.2166455785182061,
      "grad_norm": 1.6578928232192993,
      "learning_rate": 6.620364846578796e-05,
      "loss": 0.9763,
      "step": 1541
    },
    {
      "epoch": 0.21678616617460986,
      "grad_norm": 1.353137493133545,
      "learning_rate": 6.598494918440769e-05,
      "loss": 1.0852,
      "step": 1542
    },
    {
      "epoch": 0.21692675383101365,
      "grad_norm": 1.4518400430679321,
      "learning_rate": 6.576643372729522e-05,
      "loss": 1.0375,
      "step": 1543
    },
    {
      "epoch": 0.2170673414874174,
      "grad_norm": 1.3996210098266602,
      "learning_rate": 6.554810327535257e-05,
      "loss": 1.1523,
      "step": 1544
    },
    {
      "epoch": 0.21720792914382117,
      "grad_norm": 1.4472051858901978,
      "learning_rate": 6.532995900848204e-05,
      "loss": 1.0209,
      "step": 1545
    },
    {
      "epoch": 0.21734851680022493,
      "grad_norm": 1.3614650964736938,
      "learning_rate": 6.511200210557969e-05,
      "loss": 1.1438,
      "step": 1546
    },
    {
      "epoch": 0.2174891044566287,
      "grad_norm": 1.7506767511367798,
      "learning_rate": 6.489423374452907e-05,
      "loss": 1.1916,
      "step": 1547
    },
    {
      "epoch": 0.21762969211303249,
      "grad_norm": 1.5632866621017456,
      "learning_rate": 6.467665510219474e-05,
      "loss": 1.1437,
      "step": 1548
    },
    {
      "epoch": 0.21777027976943625,
      "grad_norm": 1.4109556674957275,
      "learning_rate": 6.44592673544161e-05,
      "loss": 1.0679,
      "step": 1549
    },
    {
      "epoch": 0.21791086742584,
      "grad_norm": 1.3414338827133179,
      "learning_rate": 6.424207167600079e-05,
      "loss": 1.2132,
      "step": 1550
    },
    {
      "epoch": 0.21805145508224377,
      "grad_norm": 1.6753263473510742,
      "learning_rate": 6.402506924071856e-05,
      "loss": 1.2247,
      "step": 1551
    },
    {
      "epoch": 0.21819204273864753,
      "grad_norm": 1.5974037647247314,
      "learning_rate": 6.380826122129481e-05,
      "loss": 1.1316,
      "step": 1552
    },
    {
      "epoch": 0.21833263039505132,
      "grad_norm": 1.4769797325134277,
      "learning_rate": 6.359164878940425e-05,
      "loss": 1.2662,
      "step": 1553
    },
    {
      "epoch": 0.2184732180514551,
      "grad_norm": 1.483426570892334,
      "learning_rate": 6.33752331156646e-05,
      "loss": 1.0962,
      "step": 1554
    },
    {
      "epoch": 0.21861380570785885,
      "grad_norm": 1.4033695459365845,
      "learning_rate": 6.31590153696303e-05,
      "loss": 1.2437,
      "step": 1555
    },
    {
      "epoch": 0.2187543933642626,
      "grad_norm": 1.568790078163147,
      "learning_rate": 6.29429967197861e-05,
      "loss": 1.1156,
      "step": 1556
    },
    {
      "epoch": 0.21889498102066637,
      "grad_norm": 1.326011061668396,
      "learning_rate": 6.272717833354083e-05,
      "loss": 1.2154,
      "step": 1557
    },
    {
      "epoch": 0.21903556867707016,
      "grad_norm": 1.6700502634048462,
      "learning_rate": 6.251156137722103e-05,
      "loss": 1.1141,
      "step": 1558
    },
    {
      "epoch": 0.21917615633347393,
      "grad_norm": 1.4517412185668945,
      "learning_rate": 6.229614701606469e-05,
      "loss": 1.1952,
      "step": 1559
    },
    {
      "epoch": 0.2193167439898777,
      "grad_norm": 1.4636574983596802,
      "learning_rate": 6.208093641421489e-05,
      "loss": 1.0615,
      "step": 1560
    },
    {
      "epoch": 0.21945733164628145,
      "grad_norm": 1.4950652122497559,
      "learning_rate": 6.186593073471362e-05,
      "loss": 1.0599,
      "step": 1561
    },
    {
      "epoch": 0.2195979193026852,
      "grad_norm": 1.3167519569396973,
      "learning_rate": 6.16511311394954e-05,
      "loss": 1.1273,
      "step": 1562
    },
    {
      "epoch": 0.219738506959089,
      "grad_norm": 1.6112315654754639,
      "learning_rate": 6.1436538789381e-05,
      "loss": 1.0132,
      "step": 1563
    },
    {
      "epoch": 0.21987909461549277,
      "grad_norm": 1.4798012971878052,
      "learning_rate": 6.122215484407127e-05,
      "loss": 1.1377,
      "step": 1564
    },
    {
      "epoch": 0.22001968227189653,
      "grad_norm": 1.5212615728378296,
      "learning_rate": 6.100798046214067e-05,
      "loss": 1.1284,
      "step": 1565
    },
    {
      "epoch": 0.2201602699283003,
      "grad_norm": 1.316117525100708,
      "learning_rate": 6.07940168010313e-05,
      "loss": 1.0603,
      "step": 1566
    },
    {
      "epoch": 0.22030085758470405,
      "grad_norm": 1.8961280584335327,
      "learning_rate": 6.05802650170463e-05,
      "loss": 1.152,
      "step": 1567
    },
    {
      "epoch": 0.22044144524110784,
      "grad_norm": 1.4871350526809692,
      "learning_rate": 6.036672626534393e-05,
      "loss": 1.1659,
      "step": 1568
    },
    {
      "epoch": 0.2205820328975116,
      "grad_norm": 1.439705491065979,
      "learning_rate": 6.0153401699931156e-05,
      "loss": 1.0741,
      "step": 1569
    },
    {
      "epoch": 0.22072262055391537,
      "grad_norm": 1.422853708267212,
      "learning_rate": 5.994029247365736e-05,
      "loss": 1.1448,
      "step": 1570
    },
    {
      "epoch": 0.22086320821031913,
      "grad_norm": 1.5027722120285034,
      "learning_rate": 5.97273997382083e-05,
      "loss": 1.0057,
      "step": 1571
    },
    {
      "epoch": 0.2210037958667229,
      "grad_norm": 1.7878069877624512,
      "learning_rate": 5.951472464409964e-05,
      "loss": 1.1572,
      "step": 1572
    },
    {
      "epoch": 0.22114438352312668,
      "grad_norm": 1.7572718858718872,
      "learning_rate": 5.930226834067101e-05,
      "loss": 1.1072,
      "step": 1573
    },
    {
      "epoch": 0.22128497117953044,
      "grad_norm": 1.642195224761963,
      "learning_rate": 5.90900319760795e-05,
      "loss": 1.1669,
      "step": 1574
    },
    {
      "epoch": 0.2214255588359342,
      "grad_norm": 1.551194667816162,
      "learning_rate": 5.8878016697293756e-05,
      "loss": 1.0534,
      "step": 1575
    },
    {
      "epoch": 0.22156614649233797,
      "grad_norm": 1.3627210855484009,
      "learning_rate": 5.8666223650087494e-05,
      "loss": 1.2546,
      "step": 1576
    },
    {
      "epoch": 0.22170673414874173,
      "grad_norm": 1.4836134910583496,
      "learning_rate": 5.845465397903357e-05,
      "loss": 1.0173,
      "step": 1577
    },
    {
      "epoch": 0.22184732180514552,
      "grad_norm": 1.7789275646209717,
      "learning_rate": 5.8243308827497556e-05,
      "loss": 1.0423,
      "step": 1578
    },
    {
      "epoch": 0.22198790946154928,
      "grad_norm": 1.5005711317062378,
      "learning_rate": 5.8032189337631784e-05,
      "loss": 1.1835,
      "step": 1579
    },
    {
      "epoch": 0.22212849711795304,
      "grad_norm": 1.6039910316467285,
      "learning_rate": 5.782129665036893e-05,
      "loss": 1.2141,
      "step": 1580
    },
    {
      "epoch": 0.2222690847743568,
      "grad_norm": 1.565221905708313,
      "learning_rate": 5.761063190541617e-05,
      "loss": 1.086,
      "step": 1581
    },
    {
      "epoch": 0.22240967243076057,
      "grad_norm": 1.5367350578308105,
      "learning_rate": 5.740019624124862e-05,
      "loss": 1.0657,
      "step": 1582
    },
    {
      "epoch": 0.22255026008716436,
      "grad_norm": 1.509369969367981,
      "learning_rate": 5.718999079510358e-05,
      "loss": 0.9666,
      "step": 1583
    },
    {
      "epoch": 0.22269084774356812,
      "grad_norm": 1.755771279335022,
      "learning_rate": 5.698001670297401e-05,
      "loss": 1.1918,
      "step": 1584
    },
    {
      "epoch": 0.22283143539997188,
      "grad_norm": 1.4309196472167969,
      "learning_rate": 5.6770275099602874e-05,
      "loss": 1.1538,
      "step": 1585
    },
    {
      "epoch": 0.22297202305637565,
      "grad_norm": 1.5069326162338257,
      "learning_rate": 5.656076711847642e-05,
      "loss": 1.0827,
      "step": 1586
    },
    {
      "epoch": 0.2231126107127794,
      "grad_norm": 1.6586819887161255,
      "learning_rate": 5.635149389181855e-05,
      "loss": 1.0173,
      "step": 1587
    },
    {
      "epoch": 0.2232531983691832,
      "grad_norm": 1.5179388523101807,
      "learning_rate": 5.614245655058438e-05,
      "loss": 0.8962,
      "step": 1588
    },
    {
      "epoch": 0.22339378602558696,
      "grad_norm": 1.4269832372665405,
      "learning_rate": 5.593365622445437e-05,
      "loss": 1.0767,
      "step": 1589
    },
    {
      "epoch": 0.22353437368199072,
      "grad_norm": 1.4739983081817627,
      "learning_rate": 5.572509404182796e-05,
      "loss": 1.0932,
      "step": 1590
    },
    {
      "epoch": 0.22367496133839448,
      "grad_norm": 1.6911303997039795,
      "learning_rate": 5.551677112981779e-05,
      "loss": 1.038,
      "step": 1591
    },
    {
      "epoch": 0.22381554899479825,
      "grad_norm": 1.571319818496704,
      "learning_rate": 5.530868861424326e-05,
      "loss": 1.0677,
      "step": 1592
    },
    {
      "epoch": 0.22395613665120204,
      "grad_norm": 1.640388011932373,
      "learning_rate": 5.510084761962475e-05,
      "loss": 1.0034,
      "step": 1593
    },
    {
      "epoch": 0.2240967243076058,
      "grad_norm": 1.507590651512146,
      "learning_rate": 5.489324926917729e-05,
      "loss": 1.0588,
      "step": 1594
    },
    {
      "epoch": 0.22423731196400956,
      "grad_norm": 1.4195894002914429,
      "learning_rate": 5.468589468480478e-05,
      "loss": 1.0516,
      "step": 1595
    },
    {
      "epoch": 0.22437789962041332,
      "grad_norm": 1.4931979179382324,
      "learning_rate": 5.4478784987093566e-05,
      "loss": 1.0081,
      "step": 1596
    },
    {
      "epoch": 0.22451848727681709,
      "grad_norm": 1.6873202323913574,
      "learning_rate": 5.4271921295306736e-05,
      "loss": 1.0578,
      "step": 1597
    },
    {
      "epoch": 0.22465907493322088,
      "grad_norm": 1.4505760669708252,
      "learning_rate": 5.406530472737778e-05,
      "loss": 1.0172,
      "step": 1598
    },
    {
      "epoch": 0.22479966258962464,
      "grad_norm": 1.6177663803100586,
      "learning_rate": 5.385893639990483e-05,
      "loss": 1.0905,
      "step": 1599
    },
    {
      "epoch": 0.2249402502460284,
      "grad_norm": 1.373227834701538,
      "learning_rate": 5.36528174281443e-05,
      "loss": 1.0999,
      "step": 1600
    },
    {
      "epoch": 0.22508083790243216,
      "grad_norm": 1.530234932899475,
      "learning_rate": 5.34469489260052e-05,
      "loss": 1.136,
      "step": 1601
    },
    {
      "epoch": 0.22522142555883592,
      "grad_norm": 1.4088066816329956,
      "learning_rate": 5.3241332006042796e-05,
      "loss": 1.327,
      "step": 1602
    },
    {
      "epoch": 0.22536201321523971,
      "grad_norm": 1.4699517488479614,
      "learning_rate": 5.30359677794529e-05,
      "loss": 1.1842,
      "step": 1603
    },
    {
      "epoch": 0.22550260087164348,
      "grad_norm": 1.540465235710144,
      "learning_rate": 5.283085735606563e-05,
      "loss": 1.1753,
      "step": 1604
    },
    {
      "epoch": 0.22564318852804724,
      "grad_norm": 1.719286561012268,
      "learning_rate": 5.262600184433957e-05,
      "loss": 1.016,
      "step": 1605
    },
    {
      "epoch": 0.225783776184451,
      "grad_norm": 1.485685110092163,
      "learning_rate": 5.242140235135555e-05,
      "loss": 1.047,
      "step": 1606
    },
    {
      "epoch": 0.22592436384085476,
      "grad_norm": 1.4514281749725342,
      "learning_rate": 5.2217059982811014e-05,
      "loss": 1.1226,
      "step": 1607
    },
    {
      "epoch": 0.22606495149725855,
      "grad_norm": 1.4790730476379395,
      "learning_rate": 5.2012975843013786e-05,
      "loss": 1.223,
      "step": 1608
    },
    {
      "epoch": 0.22620553915366232,
      "grad_norm": 1.6410856246948242,
      "learning_rate": 5.1809151034876105e-05,
      "loss": 1.1419,
      "step": 1609
    },
    {
      "epoch": 0.22634612681006608,
      "grad_norm": 1.734657883644104,
      "learning_rate": 5.160558665990882e-05,
      "loss": 0.8445,
      "step": 1610
    },
    {
      "epoch": 0.22648671446646984,
      "grad_norm": 1.5317045450210571,
      "learning_rate": 5.140228381821526e-05,
      "loss": 1.1435,
      "step": 1611
    },
    {
      "epoch": 0.2266273021228736,
      "grad_norm": 1.4909271001815796,
      "learning_rate": 5.119924360848547e-05,
      "loss": 1.1352,
      "step": 1612
    },
    {
      "epoch": 0.2267678897792774,
      "grad_norm": 1.5143691301345825,
      "learning_rate": 5.0996467127990055e-05,
      "loss": 1.0999,
      "step": 1613
    },
    {
      "epoch": 0.22690847743568116,
      "grad_norm": 1.3751873970031738,
      "learning_rate": 5.079395547257455e-05,
      "loss": 1.1684,
      "step": 1614
    },
    {
      "epoch": 0.22704906509208492,
      "grad_norm": 1.5503463745117188,
      "learning_rate": 5.0591709736653106e-05,
      "loss": 1.1137,
      "step": 1615
    },
    {
      "epoch": 0.22718965274848868,
      "grad_norm": 1.7047022581100464,
      "learning_rate": 5.038973101320301e-05,
      "loss": 1.0292,
      "step": 1616
    },
    {
      "epoch": 0.22733024040489244,
      "grad_norm": 1.6617045402526855,
      "learning_rate": 5.018802039375836e-05,
      "loss": 1.0268,
      "step": 1617
    },
    {
      "epoch": 0.22747082806129623,
      "grad_norm": 1.70285165309906,
      "learning_rate": 4.998657896840454e-05,
      "loss": 1.095,
      "step": 1618
    },
    {
      "epoch": 0.2276114157177,
      "grad_norm": 1.449121117591858,
      "learning_rate": 4.978540782577199e-05,
      "loss": 1.0944,
      "step": 1619
    },
    {
      "epoch": 0.22775200337410376,
      "grad_norm": 1.6305643320083618,
      "learning_rate": 4.958450805303064e-05,
      "loss": 0.9266,
      "step": 1620
    },
    {
      "epoch": 0.22789259103050752,
      "grad_norm": 1.5039994716644287,
      "learning_rate": 4.9383880735883744e-05,
      "loss": 1.181,
      "step": 1621
    },
    {
      "epoch": 0.22803317868691128,
      "grad_norm": 1.6399520635604858,
      "learning_rate": 4.9183526958562296e-05,
      "loss": 1.0995,
      "step": 1622
    },
    {
      "epoch": 0.22817376634331504,
      "grad_norm": 1.3426909446716309,
      "learning_rate": 4.898344780381883e-05,
      "loss": 1.0154,
      "step": 1623
    },
    {
      "epoch": 0.22831435399971883,
      "grad_norm": 1.6068493127822876,
      "learning_rate": 4.878364435292202e-05,
      "loss": 1.0091,
      "step": 1624
    },
    {
      "epoch": 0.2284549416561226,
      "grad_norm": 1.601866364479065,
      "learning_rate": 4.858411768565033e-05,
      "loss": 1.0066,
      "step": 1625
    },
    {
      "epoch": 0.22859552931252636,
      "grad_norm": 2.0379581451416016,
      "learning_rate": 4.8384868880286625e-05,
      "loss": 1.1716,
      "step": 1626
    },
    {
      "epoch": 0.22873611696893012,
      "grad_norm": 1.6558034420013428,
      "learning_rate": 4.818589901361198e-05,
      "loss": 1.0903,
      "step": 1627
    },
    {
      "epoch": 0.22887670462533388,
      "grad_norm": 1.5233924388885498,
      "learning_rate": 4.798720916090018e-05,
      "loss": 0.8826,
      "step": 1628
    },
    {
      "epoch": 0.22901729228173767,
      "grad_norm": 2.1154236793518066,
      "learning_rate": 4.778880039591163e-05,
      "loss": 1.1461,
      "step": 1629
    },
    {
      "epoch": 0.22915787993814143,
      "grad_norm": 1.6643195152282715,
      "learning_rate": 4.7590673790887786e-05,
      "loss": 0.993,
      "step": 1630
    },
    {
      "epoch": 0.2292984675945452,
      "grad_norm": 1.433677315711975,
      "learning_rate": 4.739283041654514e-05,
      "loss": 1.0339,
      "step": 1631
    },
    {
      "epoch": 0.22943905525094896,
      "grad_norm": 1.4378267526626587,
      "learning_rate": 4.7195271342069695e-05,
      "loss": 1.0463,
      "step": 1632
    },
    {
      "epoch": 0.22957964290735272,
      "grad_norm": 1.4305810928344727,
      "learning_rate": 4.699799763511088e-05,
      "loss": 1.0145,
      "step": 1633
    },
    {
      "epoch": 0.2297202305637565,
      "grad_norm": 1.5089523792266846,
      "learning_rate": 4.680101036177609e-05,
      "loss": 1.002,
      "step": 1634
    },
    {
      "epoch": 0.22986081822016027,
      "grad_norm": 1.5013151168823242,
      "learning_rate": 4.660431058662461e-05,
      "loss": 1.2455,
      "step": 1635
    },
    {
      "epoch": 0.23000140587656404,
      "grad_norm": 1.4314576387405396,
      "learning_rate": 4.6407899372662225e-05,
      "loss": 0.9868,
      "step": 1636
    },
    {
      "epoch": 0.2301419935329678,
      "grad_norm": 1.9747835397720337,
      "learning_rate": 4.6211777781335074e-05,
      "loss": 0.9634,
      "step": 1637
    },
    {
      "epoch": 0.23028258118937156,
      "grad_norm": 1.4394334554672241,
      "learning_rate": 4.601594687252428e-05,
      "loss": 1.185,
      "step": 1638
    },
    {
      "epoch": 0.23042316884577535,
      "grad_norm": 1.6547013521194458,
      "learning_rate": 4.582040770453995e-05,
      "loss": 1.0897,
      "step": 1639
    },
    {
      "epoch": 0.2305637565021791,
      "grad_norm": 1.6926923990249634,
      "learning_rate": 4.562516133411563e-05,
      "loss": 0.8919,
      "step": 1640
    },
    {
      "epoch": 0.23070434415858287,
      "grad_norm": 1.547868013381958,
      "learning_rate": 4.543020881640245e-05,
      "loss": 1.0504,
      "step": 1641
    },
    {
      "epoch": 0.23084493181498664,
      "grad_norm": 1.439857840538025,
      "learning_rate": 4.52355512049636e-05,
      "loss": 1.1421,
      "step": 1642
    },
    {
      "epoch": 0.2309855194713904,
      "grad_norm": 1.5634474754333496,
      "learning_rate": 4.504118955176848e-05,
      "loss": 1.0783,
      "step": 1643
    },
    {
      "epoch": 0.2311261071277942,
      "grad_norm": 1.3428460359573364,
      "learning_rate": 4.484712490718708e-05,
      "loss": 1.1371,
      "step": 1644
    },
    {
      "epoch": 0.23126669478419795,
      "grad_norm": 1.8187869787216187,
      "learning_rate": 4.4653358319984314e-05,
      "loss": 1.3255,
      "step": 1645
    },
    {
      "epoch": 0.2314072824406017,
      "grad_norm": 2.1802961826324463,
      "learning_rate": 4.4459890837314264e-05,
      "loss": 1.0381,
      "step": 1646
    },
    {
      "epoch": 0.23154787009700548,
      "grad_norm": 1.4268701076507568,
      "learning_rate": 4.4266723504714716e-05,
      "loss": 1.0784,
      "step": 1647
    },
    {
      "epoch": 0.23168845775340924,
      "grad_norm": 1.51885187625885,
      "learning_rate": 4.407385736610121e-05,
      "loss": 1.2295,
      "step": 1648
    },
    {
      "epoch": 0.23182904540981303,
      "grad_norm": 1.4749325513839722,
      "learning_rate": 4.388129346376178e-05,
      "loss": 1.0939,
      "step": 1649
    },
    {
      "epoch": 0.2319696330662168,
      "grad_norm": 1.3162802457809448,
      "learning_rate": 4.368903283835088e-05,
      "loss": 1.1072,
      "step": 1650
    },
    {
      "epoch": 0.23211022072262055,
      "grad_norm": 1.315631628036499,
      "learning_rate": 4.3497076528884237e-05,
      "loss": 1.0922,
      "step": 1651
    },
    {
      "epoch": 0.23225080837902431,
      "grad_norm": 1.4175939559936523,
      "learning_rate": 4.330542557273278e-05,
      "loss": 1.1588,
      "step": 1652
    },
    {
      "epoch": 0.23239139603542808,
      "grad_norm": 1.511784315109253,
      "learning_rate": 4.311408100561741e-05,
      "loss": 1.0346,
      "step": 1653
    },
    {
      "epoch": 0.23253198369183187,
      "grad_norm": 1.5814801454544067,
      "learning_rate": 4.2923043861603106e-05,
      "loss": 1.07,
      "step": 1654
    },
    {
      "epoch": 0.23267257134823563,
      "grad_norm": 1.4917038679122925,
      "learning_rate": 4.273231517309361e-05,
      "loss": 1.1968,
      "step": 1655
    },
    {
      "epoch": 0.2328131590046394,
      "grad_norm": 1.6237690448760986,
      "learning_rate": 4.254189597082553e-05,
      "loss": 1.1568,
      "step": 1656
    },
    {
      "epoch": 0.23295374666104315,
      "grad_norm": 1.5046626329421997,
      "learning_rate": 4.235178728386315e-05,
      "loss": 1.1921,
      "step": 1657
    },
    {
      "epoch": 0.23309433431744692,
      "grad_norm": 1.9366413354873657,
      "learning_rate": 4.216199013959248e-05,
      "loss": 1.0157,
      "step": 1658
    },
    {
      "epoch": 0.2332349219738507,
      "grad_norm": 1.4819986820220947,
      "learning_rate": 4.197250556371604e-05,
      "loss": 0.9773,
      "step": 1659
    },
    {
      "epoch": 0.23337550963025447,
      "grad_norm": 1.5076452493667603,
      "learning_rate": 4.178333458024702e-05,
      "loss": 0.9949,
      "step": 1660
    },
    {
      "epoch": 0.23351609728665823,
      "grad_norm": 1.6471083164215088,
      "learning_rate": 4.159447821150408e-05,
      "loss": 1.1295,
      "step": 1661
    },
    {
      "epoch": 0.233656684943062,
      "grad_norm": 1.622885823249817,
      "learning_rate": 4.140593747810539e-05,
      "loss": 1.2203,
      "step": 1662
    },
    {
      "epoch": 0.23379727259946576,
      "grad_norm": 1.5501737594604492,
      "learning_rate": 4.121771339896365e-05,
      "loss": 1.05,
      "step": 1663
    },
    {
      "epoch": 0.23393786025586955,
      "grad_norm": 1.490450382232666,
      "learning_rate": 4.1029806991280064e-05,
      "loss": 1.2918,
      "step": 1664
    },
    {
      "epoch": 0.2340784479122733,
      "grad_norm": 1.5874296426773071,
      "learning_rate": 4.0842219270539205e-05,
      "loss": 1.0051,
      "step": 1665
    },
    {
      "epoch": 0.23421903556867707,
      "grad_norm": 1.597718596458435,
      "learning_rate": 4.0654951250503295e-05,
      "loss": 1.0586,
      "step": 1666
    },
    {
      "epoch": 0.23435962322508083,
      "grad_norm": 1.509056806564331,
      "learning_rate": 4.0468003943206946e-05,
      "loss": 1.1778,
      "step": 1667
    },
    {
      "epoch": 0.2345002108814846,
      "grad_norm": 1.3332123756408691,
      "learning_rate": 4.028137835895144e-05,
      "loss": 1.2028,
      "step": 1668
    },
    {
      "epoch": 0.23464079853788838,
      "grad_norm": 1.6622511148452759,
      "learning_rate": 4.009507550629955e-05,
      "loss": 1.1938,
      "step": 1669
    },
    {
      "epoch": 0.23478138619429215,
      "grad_norm": 1.5839459896087646,
      "learning_rate": 3.99090963920698e-05,
      "loss": 0.8822,
      "step": 1670
    },
    {
      "epoch": 0.2349219738506959,
      "grad_norm": 1.4915165901184082,
      "learning_rate": 3.972344202133129e-05,
      "loss": 1.0498,
      "step": 1671
    },
    {
      "epoch": 0.23506256150709967,
      "grad_norm": 1.4802489280700684,
      "learning_rate": 3.953811339739801e-05,
      "loss": 0.8337,
      "step": 1672
    },
    {
      "epoch": 0.23520314916350343,
      "grad_norm": 1.3509665727615356,
      "learning_rate": 3.9353111521823705e-05,
      "loss": 1.1655,
      "step": 1673
    },
    {
      "epoch": 0.23534373681990722,
      "grad_norm": 1.6669831275939941,
      "learning_rate": 3.916843739439614e-05,
      "loss": 0.8992,
      "step": 1674
    },
    {
      "epoch": 0.23548432447631099,
      "grad_norm": 1.2977465391159058,
      "learning_rate": 3.8984092013132e-05,
      "loss": 1.1515,
      "step": 1675
    },
    {
      "epoch": 0.23562491213271475,
      "grad_norm": 1.9068199396133423,
      "learning_rate": 3.8800076374271234e-05,
      "loss": 1.1516,
      "step": 1676
    },
    {
      "epoch": 0.2357654997891185,
      "grad_norm": 1.5420235395431519,
      "learning_rate": 3.861639147227195e-05,
      "loss": 1.0674,
      "step": 1677
    },
    {
      "epoch": 0.23590608744552227,
      "grad_norm": 1.5696589946746826,
      "learning_rate": 3.84330382998047e-05,
      "loss": 0.9411,
      "step": 1678
    },
    {
      "epoch": 0.23604667510192606,
      "grad_norm": 1.5182961225509644,
      "learning_rate": 3.82500178477475e-05,
      "loss": 1.1216,
      "step": 1679
    },
    {
      "epoch": 0.23618726275832982,
      "grad_norm": 1.3932242393493652,
      "learning_rate": 3.8067331105180085e-05,
      "loss": 1.0398,
      "step": 1680
    },
    {
      "epoch": 0.2363278504147336,
      "grad_norm": 1.4212230443954468,
      "learning_rate": 3.788497905937889e-05,
      "loss": 1.1285,
      "step": 1681
    },
    {
      "epoch": 0.23646843807113735,
      "grad_norm": 1.4621944427490234,
      "learning_rate": 3.770296269581153e-05,
      "loss": 1.0404,
      "step": 1682
    },
    {
      "epoch": 0.2366090257275411,
      "grad_norm": 1.5057135820388794,
      "learning_rate": 3.752128299813156e-05,
      "loss": 1.1097,
      "step": 1683
    },
    {
      "epoch": 0.2367496133839449,
      "grad_norm": 1.4208428859710693,
      "learning_rate": 3.73399409481731e-05,
      "loss": 1.0467,
      "step": 1684
    },
    {
      "epoch": 0.23689020104034866,
      "grad_norm": 1.283410906791687,
      "learning_rate": 3.71589375259455e-05,
      "loss": 1.265,
      "step": 1685
    },
    {
      "epoch": 0.23703078869675243,
      "grad_norm": 1.6303077936172485,
      "learning_rate": 3.697827370962821e-05,
      "loss": 0.9224,
      "step": 1686
    },
    {
      "epoch": 0.2371713763531562,
      "grad_norm": 1.4535539150238037,
      "learning_rate": 3.679795047556526e-05,
      "loss": 0.9732,
      "step": 1687
    },
    {
      "epoch": 0.23731196400955995,
      "grad_norm": 1.6086899042129517,
      "learning_rate": 3.661796879826021e-05,
      "loss": 0.9764,
      "step": 1688
    },
    {
      "epoch": 0.23745255166596374,
      "grad_norm": 1.3350915908813477,
      "learning_rate": 3.643832965037067e-05,
      "loss": 1.1078,
      "step": 1689
    },
    {
      "epoch": 0.2375931393223675,
      "grad_norm": 1.5258105993270874,
      "learning_rate": 3.625903400270329e-05,
      "loss": 1.2247,
      "step": 1690
    },
    {
      "epoch": 0.23773372697877126,
      "grad_norm": 1.5381146669387817,
      "learning_rate": 3.6080082824208206e-05,
      "loss": 0.9427,
      "step": 1691
    },
    {
      "epoch": 0.23787431463517503,
      "grad_norm": 1.3370639085769653,
      "learning_rate": 3.5901477081974147e-05,
      "loss": 1.1372,
      "step": 1692
    },
    {
      "epoch": 0.2380149022915788,
      "grad_norm": 1.5081748962402344,
      "learning_rate": 3.572321774122287e-05,
      "loss": 0.8799,
      "step": 1693
    },
    {
      "epoch": 0.23815548994798258,
      "grad_norm": 1.4730405807495117,
      "learning_rate": 3.554530576530425e-05,
      "loss": 1.3006,
      "step": 1694
    },
    {
      "epoch": 0.23829607760438634,
      "grad_norm": 1.3877323865890503,
      "learning_rate": 3.5367742115690814e-05,
      "loss": 1.074,
      "step": 1695
    },
    {
      "epoch": 0.2384366652607901,
      "grad_norm": 1.5649393796920776,
      "learning_rate": 3.519052775197279e-05,
      "loss": 1.1418,
      "step": 1696
    },
    {
      "epoch": 0.23857725291719387,
      "grad_norm": 1.601166844367981,
      "learning_rate": 3.5013663631852635e-05,
      "loss": 1.1208,
      "step": 1697
    },
    {
      "epoch": 0.23871784057359763,
      "grad_norm": 1.695304036140442,
      "learning_rate": 3.4837150711140174e-05,
      "loss": 0.9538,
      "step": 1698
    },
    {
      "epoch": 0.23885842823000142,
      "grad_norm": 1.8457608222961426,
      "learning_rate": 3.4660989943747144e-05,
      "loss": 1.0511,
      "step": 1699
    },
    {
      "epoch": 0.23899901588640518,
      "grad_norm": 1.421188473701477,
      "learning_rate": 3.4485182281682315e-05,
      "loss": 1.0203,
      "step": 1700
    },
    {
      "epoch": 0.23913960354280894,
      "grad_norm": 1.5967388153076172,
      "learning_rate": 3.4309728675046015e-05,
      "loss": 1.0787,
      "step": 1701
    },
    {
      "epoch": 0.2392801911992127,
      "grad_norm": 1.4043141603469849,
      "learning_rate": 3.413463007202543e-05,
      "loss": 1.1153,
      "step": 1702
    },
    {
      "epoch": 0.23942077885561647,
      "grad_norm": 1.6206440925598145,
      "learning_rate": 3.3959887418889e-05,
      "loss": 1.0456,
      "step": 1703
    },
    {
      "epoch": 0.23956136651202026,
      "grad_norm": 1.4887861013412476,
      "learning_rate": 3.378550165998172e-05,
      "loss": 1.1909,
      "step": 1704
    },
    {
      "epoch": 0.23970195416842402,
      "grad_norm": 1.7711763381958008,
      "learning_rate": 3.361147373771969e-05,
      "loss": 1.2139,
      "step": 1705
    },
    {
      "epoch": 0.23984254182482778,
      "grad_norm": 1.3990225791931152,
      "learning_rate": 3.343780459258535e-05,
      "loss": 1.0434,
      "step": 1706
    },
    {
      "epoch": 0.23998312948123154,
      "grad_norm": 1.4249845743179321,
      "learning_rate": 3.32644951631221e-05,
      "loss": 0.9736,
      "step": 1707
    },
    {
      "epoch": 0.2401237171376353,
      "grad_norm": 1.719207525253296,
      "learning_rate": 3.3091546385929474e-05,
      "loss": 1.1863,
      "step": 1708
    },
    {
      "epoch": 0.2402643047940391,
      "grad_norm": 1.4775670766830444,
      "learning_rate": 3.291895919565785e-05,
      "loss": 1.0718,
      "step": 1709
    },
    {
      "epoch": 0.24040489245044286,
      "grad_norm": 1.4064357280731201,
      "learning_rate": 3.2746734525003654e-05,
      "loss": 1.1656,
      "step": 1710
    },
    {
      "epoch": 0.24054548010684662,
      "grad_norm": 1.3772027492523193,
      "learning_rate": 3.2574873304704034e-05,
      "loss": 1.0772,
      "step": 1711
    },
    {
      "epoch": 0.24068606776325038,
      "grad_norm": 1.2557820081710815,
      "learning_rate": 3.240337646353214e-05,
      "loss": 1.13,
      "step": 1712
    },
    {
      "epoch": 0.24082665541965415,
      "grad_norm": 1.4893845319747925,
      "learning_rate": 3.223224492829178e-05,
      "loss": 1.1034,
      "step": 1713
    },
    {
      "epoch": 0.24096724307605794,
      "grad_norm": 1.4625838994979858,
      "learning_rate": 3.2061479623812715e-05,
      "loss": 1.05,
      "step": 1714
    },
    {
      "epoch": 0.2411078307324617,
      "grad_norm": 1.6931140422821045,
      "learning_rate": 3.189108147294539e-05,
      "loss": 1.0828,
      "step": 1715
    },
    {
      "epoch": 0.24124841838886546,
      "grad_norm": 1.558081865310669,
      "learning_rate": 3.1721051396556224e-05,
      "loss": 1.1279,
      "step": 1716
    },
    {
      "epoch": 0.24138900604526922,
      "grad_norm": 1.7570971250534058,
      "learning_rate": 3.1551390313522324e-05,
      "loss": 1.0684,
      "step": 1717
    },
    {
      "epoch": 0.24152959370167298,
      "grad_norm": 1.368325114250183,
      "learning_rate": 3.1382099140726836e-05,
      "loss": 1.2035,
      "step": 1718
    },
    {
      "epoch": 0.24167018135807677,
      "grad_norm": 1.5745594501495361,
      "learning_rate": 3.12131787930537e-05,
      "loss": 1.0438,
      "step": 1719
    },
    {
      "epoch": 0.24181076901448054,
      "grad_norm": 1.3207236528396606,
      "learning_rate": 3.104463018338293e-05,
      "loss": 0.9884,
      "step": 1720
    },
    {
      "epoch": 0.2419513566708843,
      "grad_norm": 1.6032792329788208,
      "learning_rate": 3.0876454222585584e-05,
      "loss": 1.1565,
      "step": 1721
    },
    {
      "epoch": 0.24209194432728806,
      "grad_norm": 1.7653945684432983,
      "learning_rate": 3.070865181951881e-05,
      "loss": 1.0585,
      "step": 1722
    },
    {
      "epoch": 0.24223253198369182,
      "grad_norm": 1.467819094657898,
      "learning_rate": 3.0541223881021053e-05,
      "loss": 1.1306,
      "step": 1723
    },
    {
      "epoch": 0.2423731196400956,
      "grad_norm": 1.4245917797088623,
      "learning_rate": 3.037417131190693e-05,
      "loss": 1.1954,
      "step": 1724
    },
    {
      "epoch": 0.24251370729649938,
      "grad_norm": 1.530781865119934,
      "learning_rate": 3.0207495014962662e-05,
      "loss": 1.2622,
      "step": 1725
    },
    {
      "epoch": 0.24265429495290314,
      "grad_norm": 1.5095685720443726,
      "learning_rate": 3.0041195890940854e-05,
      "loss": 1.1242,
      "step": 1726
    },
    {
      "epoch": 0.2427948826093069,
      "grad_norm": 1.336655855178833,
      "learning_rate": 2.9875274838555934e-05,
      "loss": 1.1351,
      "step": 1727
    },
    {
      "epoch": 0.24293547026571066,
      "grad_norm": 1.5856486558914185,
      "learning_rate": 2.9709732754479014e-05,
      "loss": 1.125,
      "step": 1728
    },
    {
      "epoch": 0.24307605792211442,
      "grad_norm": 1.5276812314987183,
      "learning_rate": 2.954457053333335e-05,
      "loss": 1.0831,
      "step": 1729
    },
    {
      "epoch": 0.24321664557851821,
      "grad_norm": 1.633736491203308,
      "learning_rate": 2.9379789067689157e-05,
      "loss": 1.0253,
      "step": 1730
    },
    {
      "epoch": 0.24335723323492198,
      "grad_norm": 1.6453917026519775,
      "learning_rate": 2.9215389248059133e-05,
      "loss": 1.1121,
      "step": 1731
    },
    {
      "epoch": 0.24349782089132574,
      "grad_norm": 1.317400336265564,
      "learning_rate": 2.9051371962893358e-05,
      "loss": 1.1974,
      "step": 1732
    },
    {
      "epoch": 0.2436384085477295,
      "grad_norm": 1.5056180953979492,
      "learning_rate": 2.8887738098574735e-05,
      "loss": 1.1148,
      "step": 1733
    },
    {
      "epoch": 0.24377899620413326,
      "grad_norm": 1.5619537830352783,
      "learning_rate": 2.8724488539413952e-05,
      "loss": 1.1276,
      "step": 1734
    },
    {
      "epoch": 0.24391958386053705,
      "grad_norm": 1.4676203727722168,
      "learning_rate": 2.856162416764496e-05,
      "loss": 1.1697,
      "step": 1735
    },
    {
      "epoch": 0.24406017151694082,
      "grad_norm": 1.4346168041229248,
      "learning_rate": 2.8399145863419962e-05,
      "loss": 1.1298,
      "step": 1736
    },
    {
      "epoch": 0.24420075917334458,
      "grad_norm": 1.5107377767562866,
      "learning_rate": 2.8237054504804893e-05,
      "loss": 1.1385,
      "step": 1737
    },
    {
      "epoch": 0.24434134682974834,
      "grad_norm": 1.5214862823486328,
      "learning_rate": 2.8075350967774426e-05,
      "loss": 0.963,
      "step": 1738
    },
    {
      "epoch": 0.2444819344861521,
      "grad_norm": 1.5271496772766113,
      "learning_rate": 2.791403612620751e-05,
      "loss": 1.0683,
      "step": 1739
    },
    {
      "epoch": 0.2446225221425559,
      "grad_norm": 1.5044151544570923,
      "learning_rate": 2.7753110851882324e-05,
      "loss": 1.2006,
      "step": 1740
    },
    {
      "epoch": 0.24476310979895965,
      "grad_norm": 2.0753235816955566,
      "learning_rate": 2.7592576014471983e-05,
      "loss": 1.1234,
      "step": 1741
    },
    {
      "epoch": 0.24490369745536342,
      "grad_norm": 2.1195249557495117,
      "learning_rate": 2.743243248153937e-05,
      "loss": 1.0563,
      "step": 1742
    },
    {
      "epoch": 0.24504428511176718,
      "grad_norm": 1.644473671913147,
      "learning_rate": 2.727268111853285e-05,
      "loss": 1.1172,
      "step": 1743
    },
    {
      "epoch": 0.24518487276817094,
      "grad_norm": 1.4608649015426636,
      "learning_rate": 2.711332278878127e-05,
      "loss": 1.0679,
      "step": 1744
    },
    {
      "epoch": 0.24532546042457473,
      "grad_norm": 1.3658154010772705,
      "learning_rate": 2.6954358353489606e-05,
      "loss": 1.1593,
      "step": 1745
    },
    {
      "epoch": 0.2454660480809785,
      "grad_norm": 1.4200356006622314,
      "learning_rate": 2.6795788671734003e-05,
      "loss": 1.057,
      "step": 1746
    },
    {
      "epoch": 0.24560663573738226,
      "grad_norm": 1.4890217781066895,
      "learning_rate": 2.6637614600457393e-05,
      "loss": 1.3531,
      "step": 1747
    },
    {
      "epoch": 0.24574722339378602,
      "grad_norm": 1.4032765626907349,
      "learning_rate": 2.6479836994464634e-05,
      "loss": 0.9356,
      "step": 1748
    },
    {
      "epoch": 0.24588781105018978,
      "grad_norm": 1.3872250318527222,
      "learning_rate": 2.6322456706418153e-05,
      "loss": 1.2236,
      "step": 1749
    },
    {
      "epoch": 0.24602839870659357,
      "grad_norm": 2.0748956203460693,
      "learning_rate": 2.6165474586833016e-05,
      "loss": 1.0308,
      "step": 1750
    },
    {
      "epoch": 0.24616898636299733,
      "grad_norm": 1.4895635843276978,
      "learning_rate": 2.600889148407267e-05,
      "loss": 0.9538,
      "step": 1751
    },
    {
      "epoch": 0.2463095740194011,
      "grad_norm": 1.4005964994430542,
      "learning_rate": 2.585270824434407e-05,
      "loss": 1.1905,
      "step": 1752
    },
    {
      "epoch": 0.24645016167580486,
      "grad_norm": 1.4222372770309448,
      "learning_rate": 2.5696925711693308e-05,
      "loss": 1.188,
      "step": 1753
    },
    {
      "epoch": 0.24659074933220862,
      "grad_norm": 1.3975499868392944,
      "learning_rate": 2.5541544728000898e-05,
      "loss": 1.0438,
      "step": 1754
    },
    {
      "epoch": 0.2467313369886124,
      "grad_norm": 1.5491129159927368,
      "learning_rate": 2.538656613297741e-05,
      "loss": 1.1743,
      "step": 1755
    },
    {
      "epoch": 0.24687192464501617,
      "grad_norm": 1.3603718280792236,
      "learning_rate": 2.5231990764158696e-05,
      "loss": 1.0597,
      "step": 1756
    },
    {
      "epoch": 0.24701251230141993,
      "grad_norm": 1.5148791074752808,
      "learning_rate": 2.5077819456901618e-05,
      "loss": 1.124,
      "step": 1757
    },
    {
      "epoch": 0.2471530999578237,
      "grad_norm": 1.760805606842041,
      "learning_rate": 2.492405304437928e-05,
      "loss": 1.1008,
      "step": 1758
    },
    {
      "epoch": 0.24729368761422746,
      "grad_norm": 1.5086336135864258,
      "learning_rate": 2.4770692357576742e-05,
      "loss": 1.0834,
      "step": 1759
    },
    {
      "epoch": 0.24743427527063125,
      "grad_norm": 1.519292950630188,
      "learning_rate": 2.4617738225286434e-05,
      "loss": 1.1452,
      "step": 1760
    },
    {
      "epoch": 0.247574862927035,
      "grad_norm": 1.468177080154419,
      "learning_rate": 2.4465191474103656e-05,
      "loss": 1.0417,
      "step": 1761
    },
    {
      "epoch": 0.24771545058343877,
      "grad_norm": 1.6986628770828247,
      "learning_rate": 2.4313052928422152e-05,
      "loss": 1.1877,
      "step": 1762
    },
    {
      "epoch": 0.24785603823984254,
      "grad_norm": 1.4548770189285278,
      "learning_rate": 2.4161323410429603e-05,
      "loss": 1.1647,
      "step": 1763
    },
    {
      "epoch": 0.2479966258962463,
      "grad_norm": 1.7443151473999023,
      "learning_rate": 2.401000374010329e-05,
      "loss": 1.0471,
      "step": 1764
    },
    {
      "epoch": 0.2481372135526501,
      "grad_norm": 1.7998054027557373,
      "learning_rate": 2.3859094735205513e-05,
      "loss": 1.027,
      "step": 1765
    },
    {
      "epoch": 0.24827780120905385,
      "grad_norm": 1.4809361696243286,
      "learning_rate": 2.370859721127934e-05,
      "loss": 1.2947,
      "step": 1766
    },
    {
      "epoch": 0.2484183888654576,
      "grad_norm": 1.2960363626480103,
      "learning_rate": 2.3558511981644004e-05,
      "loss": 1.1231,
      "step": 1767
    },
    {
      "epoch": 0.24855897652186137,
      "grad_norm": 1.558902382850647,
      "learning_rate": 2.340883985739074e-05,
      "loss": 1.1365,
      "step": 1768
    },
    {
      "epoch": 0.24869956417826514,
      "grad_norm": 1.4088468551635742,
      "learning_rate": 2.325958164737817e-05,
      "loss": 1.0559,
      "step": 1769
    },
    {
      "epoch": 0.24884015183466893,
      "grad_norm": 1.3945866823196411,
      "learning_rate": 2.311073815822812e-05,
      "loss": 1.0298,
      "step": 1770
    },
    {
      "epoch": 0.2489807394910727,
      "grad_norm": 1.4041543006896973,
      "learning_rate": 2.296231019432109e-05,
      "loss": 1.0752,
      "step": 1771
    },
    {
      "epoch": 0.24912132714747645,
      "grad_norm": 1.4116077423095703,
      "learning_rate": 2.2814298557792125e-05,
      "loss": 0.9228,
      "step": 1772
    },
    {
      "epoch": 0.2492619148038802,
      "grad_norm": 1.4872779846191406,
      "learning_rate": 2.26667040485262e-05,
      "loss": 1.088,
      "step": 1773
    },
    {
      "epoch": 0.24940250246028398,
      "grad_norm": 1.5760232210159302,
      "learning_rate": 2.25195274641542e-05,
      "loss": 1.1378,
      "step": 1774
    },
    {
      "epoch": 0.24954309011668777,
      "grad_norm": 1.5277187824249268,
      "learning_rate": 2.2372769600048317e-05,
      "loss": 1.1439,
      "step": 1775
    },
    {
      "epoch": 0.24968367777309153,
      "grad_norm": 1.4748845100402832,
      "learning_rate": 2.2226431249318015e-05,
      "loss": 1.2607,
      "step": 1776
    },
    {
      "epoch": 0.2498242654294953,
      "grad_norm": 1.2931876182556152,
      "learning_rate": 2.208051320280553e-05,
      "loss": 0.9953,
      "step": 1777
    },
    {
      "epoch": 0.24996485308589905,
      "grad_norm": 1.4739898443222046,
      "learning_rate": 2.1935016249081764e-05,
      "loss": 0.955,
      "step": 1778
    },
    {
      "epoch": 0.25010544074230284,
      "grad_norm": 1.5585743188858032,
      "learning_rate": 2.1789941174441843e-05,
      "loss": 1.1286,
      "step": 1779
    },
    {
      "epoch": 0.2502460283987066,
      "grad_norm": 1.384255051612854,
      "learning_rate": 2.164528876290114e-05,
      "loss": 1.1764,
      "step": 1780
    },
    {
      "epoch": 0.25038661605511037,
      "grad_norm": 1.360705018043518,
      "learning_rate": 2.1501059796190704e-05,
      "loss": 1.1269,
      "step": 1781
    },
    {
      "epoch": 0.2505272037115141,
      "grad_norm": 1.2704311609268188,
      "learning_rate": 2.13572550537533e-05,
      "loss": 1.1102,
      "step": 1782
    },
    {
      "epoch": 0.2506677913679179,
      "grad_norm": 1.397024393081665,
      "learning_rate": 2.121387531273903e-05,
      "loss": 1.2945,
      "step": 1783
    },
    {
      "epoch": 0.2508083790243217,
      "grad_norm": 1.5766592025756836,
      "learning_rate": 2.1070921348001293e-05,
      "loss": 1.0811,
      "step": 1784
    },
    {
      "epoch": 0.2509489666807254,
      "grad_norm": 1.3137331008911133,
      "learning_rate": 2.0928393932092394e-05,
      "loss": 1.0985,
      "step": 1785
    },
    {
      "epoch": 0.2510895543371292,
      "grad_norm": 1.583182692527771,
      "learning_rate": 2.0786293835259584e-05,
      "loss": 1.1992,
      "step": 1786
    },
    {
      "epoch": 0.25123014199353294,
      "grad_norm": 1.4840478897094727,
      "learning_rate": 2.064462182544071e-05,
      "loss": 1.1726,
      "step": 1787
    },
    {
      "epoch": 0.25137072964993673,
      "grad_norm": 1.4505600929260254,
      "learning_rate": 2.050337866826024e-05,
      "loss": 1.1221,
      "step": 1788
    },
    {
      "epoch": 0.2515113173063405,
      "grad_norm": 1.6483738422393799,
      "learning_rate": 2.0362565127024935e-05,
      "loss": 1.1328,
      "step": 1789
    },
    {
      "epoch": 0.25165190496274426,
      "grad_norm": 1.4563419818878174,
      "learning_rate": 2.022218196271992e-05,
      "loss": 1.1336,
      "step": 1790
    },
    {
      "epoch": 0.25179249261914805,
      "grad_norm": 1.3592698574066162,
      "learning_rate": 2.008222993400437e-05,
      "loss": 1.1693,
      "step": 1791
    },
    {
      "epoch": 0.2519330802755518,
      "grad_norm": 1.490612506866455,
      "learning_rate": 1.9942709797207636e-05,
      "loss": 1.0968,
      "step": 1792
    },
    {
      "epoch": 0.25207366793195557,
      "grad_norm": 1.3177897930145264,
      "learning_rate": 1.980362230632492e-05,
      "loss": 0.9714,
      "step": 1793
    },
    {
      "epoch": 0.25221425558835936,
      "grad_norm": 1.616123914718628,
      "learning_rate": 1.9664968213013436e-05,
      "loss": 1.2541,
      "step": 1794
    },
    {
      "epoch": 0.2523548432447631,
      "grad_norm": 1.3685609102249146,
      "learning_rate": 1.952674826658809e-05,
      "loss": 1.1202,
      "step": 1795
    },
    {
      "epoch": 0.2524954309011669,
      "grad_norm": 1.4508850574493408,
      "learning_rate": 1.938896321401772e-05,
      "loss": 1.1193,
      "step": 1796
    },
    {
      "epoch": 0.2526360185575706,
      "grad_norm": 1.4589567184448242,
      "learning_rate": 1.9251613799920764e-05,
      "loss": 1.0898,
      "step": 1797
    },
    {
      "epoch": 0.2527766062139744,
      "grad_norm": 1.6872867345809937,
      "learning_rate": 1.911470076656149e-05,
      "loss": 1.0175,
      "step": 1798
    },
    {
      "epoch": 0.2529171938703782,
      "grad_norm": 1.343597173690796,
      "learning_rate": 1.8978224853845826e-05,
      "loss": 1.1286,
      "step": 1799
    },
    {
      "epoch": 0.25305778152678193,
      "grad_norm": 1.3983393907546997,
      "learning_rate": 1.884218679931744e-05,
      "loss": 1.1296,
      "step": 1800
    },
    {
      "epoch": 0.2531983691831857,
      "grad_norm": 1.3995368480682373,
      "learning_rate": 1.8706587338153712e-05,
      "loss": 1.0079,
      "step": 1801
    },
    {
      "epoch": 0.25333895683958946,
      "grad_norm": 1.55381178855896,
      "learning_rate": 1.857142720316173e-05,
      "loss": 1.1234,
      "step": 1802
    },
    {
      "epoch": 0.25347954449599325,
      "grad_norm": 1.4618490934371948,
      "learning_rate": 1.8436707124774453e-05,
      "loss": 1.0462,
      "step": 1803
    },
    {
      "epoch": 0.25362013215239704,
      "grad_norm": 1.4780384302139282,
      "learning_rate": 1.830242783104661e-05,
      "loss": 0.9683,
      "step": 1804
    },
    {
      "epoch": 0.25376071980880077,
      "grad_norm": 1.5080485343933105,
      "learning_rate": 1.8168590047650913e-05,
      "loss": 1.145,
      "step": 1805
    },
    {
      "epoch": 0.25390130746520456,
      "grad_norm": 1.68174409866333,
      "learning_rate": 1.8035194497873966e-05,
      "loss": 1.1645,
      "step": 1806
    },
    {
      "epoch": 0.2540418951216083,
      "grad_norm": 1.3909717798233032,
      "learning_rate": 1.790224190261258e-05,
      "loss": 1.2145,
      "step": 1807
    },
    {
      "epoch": 0.2541824827780121,
      "grad_norm": 1.36217200756073,
      "learning_rate": 1.7769732980369612e-05,
      "loss": 1.1244,
      "step": 1808
    },
    {
      "epoch": 0.2543230704344159,
      "grad_norm": 1.435378074645996,
      "learning_rate": 1.7637668447250345e-05,
      "loss": 1.1516,
      "step": 1809
    },
    {
      "epoch": 0.2544636580908196,
      "grad_norm": 1.3950716257095337,
      "learning_rate": 1.750604901695837e-05,
      "loss": 1.1152,
      "step": 1810
    },
    {
      "epoch": 0.2546042457472234,
      "grad_norm": 1.3276385068893433,
      "learning_rate": 1.737487540079199e-05,
      "loss": 1.1566,
      "step": 1811
    },
    {
      "epoch": 0.25474483340362714,
      "grad_norm": 1.4438841342926025,
      "learning_rate": 1.7244148307640094e-05,
      "loss": 0.9031,
      "step": 1812
    },
    {
      "epoch": 0.2548854210600309,
      "grad_norm": 1.5685797929763794,
      "learning_rate": 1.71138684439786e-05,
      "loss": 0.9709,
      "step": 1813
    },
    {
      "epoch": 0.2550260087164347,
      "grad_norm": 1.3810696601867676,
      "learning_rate": 1.6984036513866385e-05,
      "loss": 1.1376,
      "step": 1814
    },
    {
      "epoch": 0.25516659637283845,
      "grad_norm": 1.5165419578552246,
      "learning_rate": 1.6854653218941718e-05,
      "loss": 1.25,
      "step": 1815
    },
    {
      "epoch": 0.25530718402924224,
      "grad_norm": 1.638188123703003,
      "learning_rate": 1.672571925841826e-05,
      "loss": 1.0126,
      "step": 1816
    },
    {
      "epoch": 0.255447771685646,
      "grad_norm": 1.4189975261688232,
      "learning_rate": 1.659723532908144e-05,
      "loss": 1.1698,
      "step": 1817
    },
    {
      "epoch": 0.25558835934204976,
      "grad_norm": 1.365018367767334,
      "learning_rate": 1.6469202125284532e-05,
      "loss": 1.2404,
      "step": 1818
    },
    {
      "epoch": 0.25572894699845355,
      "grad_norm": 1.5084635019302368,
      "learning_rate": 1.6341620338945185e-05,
      "loss": 1.2495,
      "step": 1819
    },
    {
      "epoch": 0.2558695346548573,
      "grad_norm": 1.2946443557739258,
      "learning_rate": 1.621449065954128e-05,
      "loss": 1.1152,
      "step": 1820
    },
    {
      "epoch": 0.2560101223112611,
      "grad_norm": 1.3421155214309692,
      "learning_rate": 1.6087813774107575e-05,
      "loss": 1.085,
      "step": 1821
    },
    {
      "epoch": 0.2561507099676648,
      "grad_norm": 1.7274938821792603,
      "learning_rate": 1.5961590367231726e-05,
      "loss": 0.9594,
      "step": 1822
    },
    {
      "epoch": 0.2562912976240686,
      "grad_norm": 1.5032212734222412,
      "learning_rate": 1.58358211210508e-05,
      "loss": 0.9701,
      "step": 1823
    },
    {
      "epoch": 0.2564318852804724,
      "grad_norm": 1.7710243463516235,
      "learning_rate": 1.57105067152474e-05,
      "loss": 1.0183,
      "step": 1824
    },
    {
      "epoch": 0.25657247293687613,
      "grad_norm": 1.4716575145721436,
      "learning_rate": 1.558564782704616e-05,
      "loss": 1.0826,
      "step": 1825
    },
    {
      "epoch": 0.2567130605932799,
      "grad_norm": 1.5251933336257935,
      "learning_rate": 1.54612451312099e-05,
      "loss": 1.0551,
      "step": 1826
    },
    {
      "epoch": 0.25685364824968365,
      "grad_norm": 1.5954726934432983,
      "learning_rate": 1.533729930003621e-05,
      "loss": 1.1242,
      "step": 1827
    },
    {
      "epoch": 0.25699423590608744,
      "grad_norm": 1.2775871753692627,
      "learning_rate": 1.5213811003353573e-05,
      "loss": 1.1193,
      "step": 1828
    },
    {
      "epoch": 0.25713482356249123,
      "grad_norm": 1.6025640964508057,
      "learning_rate": 1.5090780908517966e-05,
      "loss": 1.1527,
      "step": 1829
    },
    {
      "epoch": 0.25727541121889497,
      "grad_norm": 1.5332729816436768,
      "learning_rate": 1.496820968040904e-05,
      "loss": 1.1635,
      "step": 1830
    },
    {
      "epoch": 0.25741599887529876,
      "grad_norm": 1.5487300157546997,
      "learning_rate": 1.4846097981426744e-05,
      "loss": 1.0024,
      "step": 1831
    },
    {
      "epoch": 0.2575565865317025,
      "grad_norm": 1.6980130672454834,
      "learning_rate": 1.4724446471487551e-05,
      "loss": 1.2066,
      "step": 1832
    },
    {
      "epoch": 0.2576971741881063,
      "grad_norm": 1.6114153861999512,
      "learning_rate": 1.4603255808021054e-05,
      "loss": 1.2287,
      "step": 1833
    },
    {
      "epoch": 0.25783776184451007,
      "grad_norm": 1.859785795211792,
      "learning_rate": 1.4482526645966265e-05,
      "loss": 1.1445,
      "step": 1834
    },
    {
      "epoch": 0.2579783495009138,
      "grad_norm": 1.695548415184021,
      "learning_rate": 1.4362259637768215e-05,
      "loss": 1.456,
      "step": 1835
    },
    {
      "epoch": 0.2581189371573176,
      "grad_norm": 1.4714604616165161,
      "learning_rate": 1.4242455433374302e-05,
      "loss": 1.0026,
      "step": 1836
    },
    {
      "epoch": 0.25825952481372133,
      "grad_norm": 2.027747631072998,
      "learning_rate": 1.4123114680230854e-05,
      "loss": 0.9555,
      "step": 1837
    },
    {
      "epoch": 0.2584001124701251,
      "grad_norm": 1.5596187114715576,
      "learning_rate": 1.4004238023279682e-05,
      "loss": 1.134,
      "step": 1838
    },
    {
      "epoch": 0.2585407001265289,
      "grad_norm": 1.5754565000534058,
      "learning_rate": 1.3885826104954424e-05,
      "loss": 1.0461,
      "step": 1839
    },
    {
      "epoch": 0.25868128778293265,
      "grad_norm": 1.3882852792739868,
      "learning_rate": 1.3767879565177266e-05,
      "loss": 1.1517,
      "step": 1840
    },
    {
      "epoch": 0.25882187543933644,
      "grad_norm": 1.6804015636444092,
      "learning_rate": 1.3650399041355289e-05,
      "loss": 1.0366,
      "step": 1841
    },
    {
      "epoch": 0.25896246309574017,
      "grad_norm": 1.4737428426742554,
      "learning_rate": 1.3533385168377243e-05,
      "loss": 1.0292,
      "step": 1842
    },
    {
      "epoch": 0.25910305075214396,
      "grad_norm": 1.4385682344436646,
      "learning_rate": 1.3416838578609903e-05,
      "loss": 1.0684,
      "step": 1843
    },
    {
      "epoch": 0.25924363840854775,
      "grad_norm": 1.521018147468567,
      "learning_rate": 1.3300759901894844e-05,
      "loss": 1.1303,
      "step": 1844
    },
    {
      "epoch": 0.2593842260649515,
      "grad_norm": 1.4479395151138306,
      "learning_rate": 1.3185149765544847e-05,
      "loss": 1.0798,
      "step": 1845
    },
    {
      "epoch": 0.2595248137213553,
      "grad_norm": 1.4463353157043457,
      "learning_rate": 1.3070008794340693e-05,
      "loss": 1.0382,
      "step": 1846
    },
    {
      "epoch": 0.259665401377759,
      "grad_norm": 1.446382999420166,
      "learning_rate": 1.2955337610527619e-05,
      "loss": 1.0901,
      "step": 1847
    },
    {
      "epoch": 0.2598059890341628,
      "grad_norm": 1.5259506702423096,
      "learning_rate": 1.2841136833812118e-05,
      "loss": 1.1609,
      "step": 1848
    },
    {
      "epoch": 0.2599465766905666,
      "grad_norm": 1.5755846500396729,
      "learning_rate": 1.2727407081358433e-05,
      "loss": 1.0517,
      "step": 1849
    },
    {
      "epoch": 0.2600871643469703,
      "grad_norm": 1.6455343961715698,
      "learning_rate": 1.2614148967785344e-05,
      "loss": 0.9904,
      "step": 1850
    },
    {
      "epoch": 0.2602277520033741,
      "grad_norm": 1.4804887771606445,
      "learning_rate": 1.2501363105162766e-05,
      "loss": 0.8976,
      "step": 1851
    },
    {
      "epoch": 0.26036833965977785,
      "grad_norm": 1.3886507749557495,
      "learning_rate": 1.2389050103008515e-05,
      "loss": 1.0536,
      "step": 1852
    },
    {
      "epoch": 0.26050892731618164,
      "grad_norm": 1.5821760892868042,
      "learning_rate": 1.2277210568284913e-05,
      "loss": 1.0274,
      "step": 1853
    },
    {
      "epoch": 0.2606495149725854,
      "grad_norm": 1.6401944160461426,
      "learning_rate": 1.2165845105395645e-05,
      "loss": 1.0151,
      "step": 1854
    },
    {
      "epoch": 0.26079010262898916,
      "grad_norm": 1.2929768562316895,
      "learning_rate": 1.205495431618232e-05,
      "loss": 1.234,
      "step": 1855
    },
    {
      "epoch": 0.26093069028539295,
      "grad_norm": 1.2931005954742432,
      "learning_rate": 1.1944538799921411e-05,
      "loss": 1.1195,
      "step": 1856
    },
    {
      "epoch": 0.2610712779417967,
      "grad_norm": 1.5733193159103394,
      "learning_rate": 1.1834599153320825e-05,
      "loss": 1.0708,
      "step": 1857
    },
    {
      "epoch": 0.2612118655982005,
      "grad_norm": 1.6031277179718018,
      "learning_rate": 1.1725135970516876e-05,
      "loss": 1.0238,
      "step": 1858
    },
    {
      "epoch": 0.26135245325460427,
      "grad_norm": 1.386975884437561,
      "learning_rate": 1.1616149843070879e-05,
      "loss": 1.1222,
      "step": 1859
    },
    {
      "epoch": 0.261493040911008,
      "grad_norm": 1.4303991794586182,
      "learning_rate": 1.1507641359966114e-05,
      "loss": 1.0557,
      "step": 1860
    },
    {
      "epoch": 0.2616336285674118,
      "grad_norm": 1.4830299615859985,
      "learning_rate": 1.139961110760449e-05,
      "loss": 1.0217,
      "step": 1861
    },
    {
      "epoch": 0.2617742162238155,
      "grad_norm": 1.4652960300445557,
      "learning_rate": 1.1292059669803567e-05,
      "loss": 1.1043,
      "step": 1862
    },
    {
      "epoch": 0.2619148038802193,
      "grad_norm": 1.5590900182724,
      "learning_rate": 1.1184987627793175e-05,
      "loss": 1.1403,
      "step": 1863
    },
    {
      "epoch": 0.2620553915366231,
      "grad_norm": 1.7432669401168823,
      "learning_rate": 1.1078395560212518e-05,
      "loss": 0.969,
      "step": 1864
    },
    {
      "epoch": 0.26219597919302684,
      "grad_norm": 1.6110299825668335,
      "learning_rate": 1.0972284043106795e-05,
      "loss": 1.1986,
      "step": 1865
    },
    {
      "epoch": 0.26233656684943063,
      "grad_norm": 1.4141126871109009,
      "learning_rate": 1.086665364992433e-05,
      "loss": 1.0626,
      "step": 1866
    },
    {
      "epoch": 0.26247715450583436,
      "grad_norm": 1.4098074436187744,
      "learning_rate": 1.0761504951513246e-05,
      "loss": 1.1479,
      "step": 1867
    },
    {
      "epoch": 0.26261774216223815,
      "grad_norm": 1.4020349979400635,
      "learning_rate": 1.0656838516118584e-05,
      "loss": 1.2057,
      "step": 1868
    },
    {
      "epoch": 0.26275832981864194,
      "grad_norm": 1.659718632698059,
      "learning_rate": 1.0552654909379055e-05,
      "loss": 1.1311,
      "step": 1869
    },
    {
      "epoch": 0.2628989174750457,
      "grad_norm": 1.5277718305587769,
      "learning_rate": 1.0448954694324142e-05,
      "loss": 1.0182,
      "step": 1870
    },
    {
      "epoch": 0.26303950513144947,
      "grad_norm": 1.3547879457473755,
      "learning_rate": 1.0345738431370921e-05,
      "loss": 1.272,
      "step": 1871
    },
    {
      "epoch": 0.2631800927878532,
      "grad_norm": 1.383103847503662,
      "learning_rate": 1.0243006678321131e-05,
      "loss": 0.9634,
      "step": 1872
    },
    {
      "epoch": 0.263320680444257,
      "grad_norm": 1.5093876123428345,
      "learning_rate": 1.0140759990358073e-05,
      "loss": 0.8988,
      "step": 1873
    },
    {
      "epoch": 0.2634612681006608,
      "grad_norm": 1.3800232410430908,
      "learning_rate": 1.0038998920043741e-05,
      "loss": 1.1945,
      "step": 1874
    },
    {
      "epoch": 0.2636018557570645,
      "grad_norm": 1.4043328762054443,
      "learning_rate": 9.93772401731564e-06,
      "loss": 1.1256,
      "step": 1875
    },
    {
      "epoch": 0.2637424434134683,
      "grad_norm": 1.33708918094635,
      "learning_rate": 9.836935829484017e-06,
      "loss": 1.2925,
      "step": 1876
    },
    {
      "epoch": 0.26388303106987204,
      "grad_norm": 1.3147861957550049,
      "learning_rate": 9.736634901228814e-06,
      "loss": 1.0152,
      "step": 1877
    },
    {
      "epoch": 0.26402361872627583,
      "grad_norm": 1.538306713104248,
      "learning_rate": 9.636821774596638e-06,
      "loss": 1.0696,
      "step": 1878
    },
    {
      "epoch": 0.2641642063826796,
      "grad_norm": 1.5832607746124268,
      "learning_rate": 9.537496988998008e-06,
      "loss": 1.1029,
      "step": 1879
    },
    {
      "epoch": 0.26430479403908336,
      "grad_norm": 1.6709685325622559,
      "learning_rate": 9.438661081204281e-06,
      "loss": 1.0965,
      "step": 1880
    },
    {
      "epoch": 0.26444538169548715,
      "grad_norm": 1.5122888088226318,
      "learning_rate": 9.340314585344889e-06,
      "loss": 0.9282,
      "step": 1881
    },
    {
      "epoch": 0.2645859693518909,
      "grad_norm": 1.4539381265640259,
      "learning_rate": 9.24245803290429e-06,
      "loss": 1.0949,
      "step": 1882
    },
    {
      "epoch": 0.26472655700829467,
      "grad_norm": 1.4905940294265747,
      "learning_rate": 9.145091952719276e-06,
      "loss": 1.1354,
      "step": 1883
    },
    {
      "epoch": 0.26486714466469846,
      "grad_norm": 1.6049697399139404,
      "learning_rate": 9.048216870975968e-06,
      "loss": 1.2563,
      "step": 1884
    },
    {
      "epoch": 0.2650077323211022,
      "grad_norm": 1.3222384452819824,
      "learning_rate": 8.95183331120708e-06,
      "loss": 1.0591,
      "step": 1885
    },
    {
      "epoch": 0.265148319977506,
      "grad_norm": 1.526872158050537,
      "learning_rate": 8.85594179428898e-06,
      "loss": 1.2579,
      "step": 1886
    },
    {
      "epoch": 0.2652889076339097,
      "grad_norm": 1.4312899112701416,
      "learning_rate": 8.760542838439012e-06,
      "loss": 1.0564,
      "step": 1887
    },
    {
      "epoch": 0.2654294952903135,
      "grad_norm": 1.3813385963439941,
      "learning_rate": 8.66563695921252e-06,
      "loss": 1.061,
      "step": 1888
    },
    {
      "epoch": 0.2655700829467173,
      "grad_norm": 1.5017585754394531,
      "learning_rate": 8.571224669500289e-06,
      "loss": 1.18,
      "step": 1889
    },
    {
      "epoch": 0.26571067060312104,
      "grad_norm": 1.4125667810440063,
      "learning_rate": 8.477306479525515e-06,
      "loss": 1.0516,
      "step": 1890
    },
    {
      "epoch": 0.2658512582595248,
      "grad_norm": 1.436672329902649,
      "learning_rate": 8.383882896841288e-06,
      "loss": 1.1281,
      "step": 1891
    },
    {
      "epoch": 0.26599184591592856,
      "grad_norm": 1.3451474905014038,
      "learning_rate": 8.290954426327645e-06,
      "loss": 1.1069,
      "step": 1892
    },
    {
      "epoch": 0.26613243357233235,
      "grad_norm": 1.5095102787017822,
      "learning_rate": 8.198521570189033e-06,
      "loss": 1.1108,
      "step": 1893
    },
    {
      "epoch": 0.26627302122873614,
      "grad_norm": 1.439680814743042,
      "learning_rate": 8.106584827951402e-06,
      "loss": 1.0806,
      "step": 1894
    },
    {
      "epoch": 0.2664136088851399,
      "grad_norm": 1.6336694955825806,
      "learning_rate": 8.015144696459676e-06,
      "loss": 1.2163,
      "step": 1895
    },
    {
      "epoch": 0.26655419654154366,
      "grad_norm": 1.6116710901260376,
      "learning_rate": 7.92420166987492e-06,
      "loss": 1.0395,
      "step": 1896
    },
    {
      "epoch": 0.2666947841979474,
      "grad_norm": 1.3883085250854492,
      "learning_rate": 7.833756239671853e-06,
      "loss": 1.1377,
      "step": 1897
    },
    {
      "epoch": 0.2668353718543512,
      "grad_norm": 1.6591278314590454,
      "learning_rate": 7.743808894635962e-06,
      "loss": 1.1655,
      "step": 1898
    },
    {
      "epoch": 0.266975959510755,
      "grad_norm": 1.5283209085464478,
      "learning_rate": 7.654360120861071e-06,
      "loss": 1.1255,
      "step": 1899
    },
    {
      "epoch": 0.2671165471671587,
      "grad_norm": 1.3381026983261108,
      "learning_rate": 7.565410401746542e-06,
      "loss": 1.1394,
      "step": 1900
    },
    {
      "epoch": 0.2672571348235625,
      "grad_norm": 1.508339762687683,
      "learning_rate": 7.476960217994833e-06,
      "loss": 1.2876,
      "step": 1901
    },
    {
      "epoch": 0.26739772247996624,
      "grad_norm": 1.3482521772384644,
      "learning_rate": 7.389010047608724e-06,
      "loss": 1.0031,
      "step": 1902
    },
    {
      "epoch": 0.26753831013637003,
      "grad_norm": 1.4809972047805786,
      "learning_rate": 7.301560365888904e-06,
      "loss": 1.2275,
      "step": 1903
    },
    {
      "epoch": 0.2676788977927738,
      "grad_norm": 1.3467729091644287,
      "learning_rate": 7.214611645431235e-06,
      "loss": 0.9912,
      "step": 1904
    },
    {
      "epoch": 0.26781948544917755,
      "grad_norm": 1.6964353322982788,
      "learning_rate": 7.128164356124367e-06,
      "loss": 1.004,
      "step": 1905
    },
    {
      "epoch": 0.26796007310558134,
      "grad_norm": 1.4413080215454102,
      "learning_rate": 7.042218965147029e-06,
      "loss": 1.0731,
      "step": 1906
    },
    {
      "epoch": 0.2681006607619851,
      "grad_norm": 1.5642316341400146,
      "learning_rate": 6.956775936965665e-06,
      "loss": 1.0709,
      "step": 1907
    },
    {
      "epoch": 0.26824124841838887,
      "grad_norm": 1.4315415620803833,
      "learning_rate": 6.871835733331789e-06,
      "loss": 0.9987,
      "step": 1908
    },
    {
      "epoch": 0.26838183607479266,
      "grad_norm": 1.2421461343765259,
      "learning_rate": 6.787398813279611e-06,
      "loss": 1.0807,
      "step": 1909
    },
    {
      "epoch": 0.2685224237311964,
      "grad_norm": 1.5406197309494019,
      "learning_rate": 6.703465633123407e-06,
      "loss": 1.0189,
      "step": 1910
    },
    {
      "epoch": 0.2686630113876002,
      "grad_norm": 1.517959713935852,
      "learning_rate": 6.620036646455241e-06,
      "loss": 1.2547,
      "step": 1911
    },
    {
      "epoch": 0.2688035990440039,
      "grad_norm": 1.586104154586792,
      "learning_rate": 6.537112304142323e-06,
      "loss": 0.9596,
      "step": 1912
    },
    {
      "epoch": 0.2689441867004077,
      "grad_norm": 1.5606715679168701,
      "learning_rate": 6.4546930543247344e-06,
      "loss": 1.139,
      "step": 1913
    },
    {
      "epoch": 0.2690847743568115,
      "grad_norm": 1.4823366403579712,
      "learning_rate": 6.37277934241286e-06,
      "loss": 1.0259,
      "step": 1914
    },
    {
      "epoch": 0.26922536201321523,
      "grad_norm": 1.3722431659698486,
      "learning_rate": 6.2913716110851065e-06,
      "loss": 1.1036,
      "step": 1915
    },
    {
      "epoch": 0.269365949669619,
      "grad_norm": 1.6043699979782104,
      "learning_rate": 6.210470300285498e-06,
      "loss": 1.1526,
      "step": 1916
    },
    {
      "epoch": 0.26950653732602275,
      "grad_norm": 1.5225815773010254,
      "learning_rate": 6.130075847221139e-06,
      "loss": 0.9833,
      "step": 1917
    },
    {
      "epoch": 0.26964712498242654,
      "grad_norm": 1.3248240947723389,
      "learning_rate": 6.050188686360092e-06,
      "loss": 0.9865,
      "step": 1918
    },
    {
      "epoch": 0.26978771263883033,
      "grad_norm": 1.4484517574310303,
      "learning_rate": 5.970809249428821e-06,
      "loss": 1.0434,
      "step": 1919
    },
    {
      "epoch": 0.26992830029523407,
      "grad_norm": 1.5160471200942993,
      "learning_rate": 5.89193796541001e-06,
      "loss": 1.1612,
      "step": 1920
    },
    {
      "epoch": 0.27006888795163786,
      "grad_norm": 1.5475759506225586,
      "learning_rate": 5.813575260540128e-06,
      "loss": 1.117,
      "step": 1921
    },
    {
      "epoch": 0.2702094756080416,
      "grad_norm": 1.5457673072814941,
      "learning_rate": 5.735721558307228e-06,
      "loss": 1.1594,
      "step": 1922
    },
    {
      "epoch": 0.2703500632644454,
      "grad_norm": 1.4323554039001465,
      "learning_rate": 5.658377279448568e-06,
      "loss": 1.1538,
      "step": 1923
    },
    {
      "epoch": 0.2704906509208492,
      "grad_norm": 1.6739590167999268,
      "learning_rate": 5.581542841948417e-06,
      "loss": 1.1329,
      "step": 1924
    },
    {
      "epoch": 0.2706312385772529,
      "grad_norm": 1.577501654624939,
      "learning_rate": 5.505218661035705e-06,
      "loss": 1.1615,
      "step": 1925
    },
    {
      "epoch": 0.2707718262336567,
      "grad_norm": 1.6715196371078491,
      "learning_rate": 5.429405149181888e-06,
      "loss": 1.0045,
      "step": 1926
    },
    {
      "epoch": 0.27091241389006043,
      "grad_norm": 1.5919413566589355,
      "learning_rate": 5.3541027160986194e-06,
      "loss": 1.011,
      "step": 1927
    },
    {
      "epoch": 0.2710530015464642,
      "grad_norm": 1.6479839086532593,
      "learning_rate": 5.2793117687356175e-06,
      "loss": 1.0924,
      "step": 1928
    },
    {
      "epoch": 0.271193589202868,
      "grad_norm": 1.5627145767211914,
      "learning_rate": 5.205032711278379e-06,
      "loss": 1.0556,
      "step": 1929
    },
    {
      "epoch": 0.27133417685927175,
      "grad_norm": 2.0898404121398926,
      "learning_rate": 5.131265945146102e-06,
      "loss": 0.9866,
      "step": 1930
    },
    {
      "epoch": 0.27147476451567554,
      "grad_norm": 1.4196271896362305,
      "learning_rate": 5.058011868989388e-06,
      "loss": 1.1149,
      "step": 1931
    },
    {
      "epoch": 0.27161535217207927,
      "grad_norm": 1.3628184795379639,
      "learning_rate": 4.985270878688242e-06,
      "loss": 0.9059,
      "step": 1932
    },
    {
      "epoch": 0.27175593982848306,
      "grad_norm": 1.4847116470336914,
      "learning_rate": 4.9130433673497565e-06,
      "loss": 1.0147,
      "step": 1933
    },
    {
      "epoch": 0.27189652748488685,
      "grad_norm": 1.4532300233840942,
      "learning_rate": 4.841329725306143e-06,
      "loss": 1.2336,
      "step": 1934
    },
    {
      "epoch": 0.2720371151412906,
      "grad_norm": 1.974456548690796,
      "learning_rate": 4.770130340112555e-06,
      "loss": 1.014,
      "step": 1935
    },
    {
      "epoch": 0.2721777027976944,
      "grad_norm": 1.6286461353302002,
      "learning_rate": 4.699445596544971e-06,
      "loss": 1.0719,
      "step": 1936
    },
    {
      "epoch": 0.2723182904540981,
      "grad_norm": 1.5130809545516968,
      "learning_rate": 4.629275876598149e-06,
      "loss": 1.1562,
      "step": 1937
    },
    {
      "epoch": 0.2724588781105019,
      "grad_norm": 1.4163974523544312,
      "learning_rate": 4.559621559483562e-06,
      "loss": 1.1298,
      "step": 1938
    },
    {
      "epoch": 0.2725994657669057,
      "grad_norm": 1.4110751152038574,
      "learning_rate": 4.490483021627334e-06,
      "loss": 1.1904,
      "step": 1939
    },
    {
      "epoch": 0.2727400534233094,
      "grad_norm": 1.3694005012512207,
      "learning_rate": 4.421860636668218e-06,
      "loss": 1.0109,
      "step": 1940
    },
    {
      "epoch": 0.2728806410797132,
      "grad_norm": 1.5687670707702637,
      "learning_rate": 4.353754775455565e-06,
      "loss": 1.2247,
      "step": 1941
    },
    {
      "epoch": 0.27302122873611695,
      "grad_norm": 1.475361943244934,
      "learning_rate": 4.286165806047349e-06,
      "loss": 1.0641,
      "step": 1942
    },
    {
      "epoch": 0.27316181639252074,
      "grad_norm": 1.4667123556137085,
      "learning_rate": 4.219094093708109e-06,
      "loss": 1.1343,
      "step": 1943
    },
    {
      "epoch": 0.27330240404892453,
      "grad_norm": 1.3814541101455688,
      "learning_rate": 4.152540000907068e-06,
      "loss": 0.8916,
      "step": 1944
    },
    {
      "epoch": 0.27344299170532826,
      "grad_norm": 1.487830638885498,
      "learning_rate": 4.086503887316107e-06,
      "loss": 1.1881,
      "step": 1945
    },
    {
      "epoch": 0.27358357936173205,
      "grad_norm": 1.6003402471542358,
      "learning_rate": 4.0209861098078446e-06,
      "loss": 1.05,
      "step": 1946
    },
    {
      "epoch": 0.2737241670181358,
      "grad_norm": 1.540339708328247,
      "learning_rate": 3.955987022453689e-06,
      "loss": 1.1566,
      "step": 1947
    },
    {
      "epoch": 0.2738647546745396,
      "grad_norm": 1.6754003763198853,
      "learning_rate": 3.8915069765219746e-06,
      "loss": 1.091,
      "step": 1948
    },
    {
      "epoch": 0.27400534233094337,
      "grad_norm": 1.572129487991333,
      "learning_rate": 3.827546320476005e-06,
      "loss": 1.2666,
      "step": 1949
    },
    {
      "epoch": 0.2741459299873471,
      "grad_norm": 1.4516735076904297,
      "learning_rate": 3.7641053999722065e-06,
      "loss": 1.134,
      "step": 1950
    },
    {
      "epoch": 0.2742865176437509,
      "grad_norm": 1.7531778812408447,
      "learning_rate": 3.7011845578582392e-06,
      "loss": 1.0692,
      "step": 1951
    },
    {
      "epoch": 0.27442710530015463,
      "grad_norm": 1.7572572231292725,
      "learning_rate": 3.6387841341711693e-06,
      "loss": 1.0884,
      "step": 1952
    },
    {
      "epoch": 0.2745676929565584,
      "grad_norm": 1.5778717994689941,
      "learning_rate": 3.5769044661355887e-06,
      "loss": 1.1148,
      "step": 1953
    },
    {
      "epoch": 0.2747082806129622,
      "grad_norm": 1.7985799312591553,
      "learning_rate": 3.515545888161831e-06,
      "loss": 1.0171,
      "step": 1954
    },
    {
      "epoch": 0.27484886826936594,
      "grad_norm": 1.1958235502243042,
      "learning_rate": 3.4547087318442027e-06,
      "loss": 1.2906,
      "step": 1955
    },
    {
      "epoch": 0.27498945592576973,
      "grad_norm": 1.551285743713379,
      "learning_rate": 3.3943933259590445e-06,
      "loss": 1.0306,
      "step": 1956
    },
    {
      "epoch": 0.27513004358217347,
      "grad_norm": 1.5118900537490845,
      "learning_rate": 3.334599996463139e-06,
      "loss": 1.1275,
      "step": 1957
    },
    {
      "epoch": 0.27527063123857726,
      "grad_norm": 1.6719685792922974,
      "learning_rate": 3.2753290664918055e-06,
      "loss": 0.9402,
      "step": 1958
    },
    {
      "epoch": 0.27541121889498105,
      "grad_norm": 1.5568578243255615,
      "learning_rate": 3.216580856357243e-06,
      "loss": 0.93,
      "step": 1959
    },
    {
      "epoch": 0.2755518065513848,
      "grad_norm": 1.6808805465698242,
      "learning_rate": 3.1583556835467432e-06,
      "loss": 1.1385,
      "step": 1960
    },
    {
      "epoch": 0.27569239420778857,
      "grad_norm": 1.4610626697540283,
      "learning_rate": 3.1006538627210147e-06,
      "loss": 1.0016,
      "step": 1961
    },
    {
      "epoch": 0.2758329818641923,
      "grad_norm": 1.5331602096557617,
      "learning_rate": 3.043475705712462e-06,
      "loss": 1.101,
      "step": 1962
    },
    {
      "epoch": 0.2759735695205961,
      "grad_norm": 1.438628911972046,
      "learning_rate": 2.9868215215234862e-06,
      "loss": 1.1284,
      "step": 1963
    },
    {
      "epoch": 0.2761141571769999,
      "grad_norm": 1.472866415977478,
      "learning_rate": 2.9306916163248543e-06,
      "loss": 1.0798,
      "step": 1964
    },
    {
      "epoch": 0.2762547448334036,
      "grad_norm": 1.4375911951065063,
      "learning_rate": 2.875086293454021e-06,
      "loss": 1.1194,
      "step": 1965
    },
    {
      "epoch": 0.2763953324898074,
      "grad_norm": 1.4760693311691284,
      "learning_rate": 2.8200058534134766e-06,
      "loss": 1.0127,
      "step": 1966
    },
    {
      "epoch": 0.27653592014621114,
      "grad_norm": 1.408478021621704,
      "learning_rate": 2.7654505938691565e-06,
      "loss": 1.1113,
      "step": 1967
    },
    {
      "epoch": 0.27667650780261493,
      "grad_norm": 1.4464017152786255,
      "learning_rate": 2.7114208096487794e-06,
      "loss": 0.9501,
      "step": 1968
    },
    {
      "epoch": 0.2768170954590187,
      "grad_norm": 1.5480992794036865,
      "learning_rate": 2.657916792740334e-06,
      "loss": 1.0867,
      "step": 1969
    },
    {
      "epoch": 0.27695768311542246,
      "grad_norm": 1.4368162155151367,
      "learning_rate": 2.6049388322904265e-06,
      "loss": 1.1848,
      "step": 1970
    },
    {
      "epoch": 0.27709827077182625,
      "grad_norm": 1.3112915754318237,
      "learning_rate": 2.552487214602761e-06,
      "loss": 1.2264,
      "step": 1971
    },
    {
      "epoch": 0.27723885842823,
      "grad_norm": 1.4848741292953491,
      "learning_rate": 2.500562223136549e-06,
      "loss": 0.9093,
      "step": 1972
    },
    {
      "epoch": 0.2773794460846338,
      "grad_norm": 1.5114210844039917,
      "learning_rate": 2.4491641385050446e-06,
      "loss": 1.0165,
      "step": 1973
    },
    {
      "epoch": 0.27752003374103756,
      "grad_norm": 1.5234625339508057,
      "learning_rate": 2.39829323847397e-06,
      "loss": 1.0196,
      "step": 1974
    },
    {
      "epoch": 0.2776606213974413,
      "grad_norm": 1.5341310501098633,
      "learning_rate": 2.347949797960047e-06,
      "loss": 1.1115,
      "step": 1975
    },
    {
      "epoch": 0.2778012090538451,
      "grad_norm": 1.3445364236831665,
      "learning_rate": 2.29813408902948e-06,
      "loss": 1.0818,
      "step": 1976
    },
    {
      "epoch": 0.2779417967102488,
      "grad_norm": 1.488478422164917,
      "learning_rate": 2.248846380896519e-06,
      "loss": 1.025,
      "step": 1977
    },
    {
      "epoch": 0.2780823843666526,
      "grad_norm": 1.491041898727417,
      "learning_rate": 2.2000869399219637e-06,
      "loss": 0.9147,
      "step": 1978
    },
    {
      "epoch": 0.2782229720230564,
      "grad_norm": 1.62614107131958,
      "learning_rate": 2.1518560296118095e-06,
      "loss": 1.0665,
      "step": 1979
    },
    {
      "epoch": 0.27836355967946014,
      "grad_norm": 1.306634783744812,
      "learning_rate": 2.1041539106156917e-06,
      "loss": 1.1068,
      "step": 1980
    },
    {
      "epoch": 0.2785041473358639,
      "grad_norm": 1.3617976903915405,
      "learning_rate": 2.0569808407256086e-06,
      "loss": 1.0436,
      "step": 1981
    },
    {
      "epoch": 0.27864473499226766,
      "grad_norm": 1.3583171367645264,
      "learning_rate": 2.0103370748744244e-06,
      "loss": 1.0511,
      "step": 1982
    },
    {
      "epoch": 0.27878532264867145,
      "grad_norm": 1.4760396480560303,
      "learning_rate": 1.9642228651345796e-06,
      "loss": 0.9011,
      "step": 1983
    },
    {
      "epoch": 0.27892591030507524,
      "grad_norm": 1.5890097618103027,
      "learning_rate": 1.9186384607166376e-06,
      "loss": 1.2381,
      "step": 1984
    },
    {
      "epoch": 0.279066497961479,
      "grad_norm": 1.8947502374649048,
      "learning_rate": 1.8735841079680295e-06,
      "loss": 1.0892,
      "step": 1985
    },
    {
      "epoch": 0.27920708561788277,
      "grad_norm": 1.5266427993774414,
      "learning_rate": 1.829060050371656e-06,
      "loss": 0.9831,
      "step": 1986
    },
    {
      "epoch": 0.2793476732742865,
      "grad_norm": 1.468382477760315,
      "learning_rate": 1.785066528544599e-06,
      "loss": 1.13,
      "step": 1987
    },
    {
      "epoch": 0.2794882609306903,
      "grad_norm": 1.2977696657180786,
      "learning_rate": 1.7416037802368113e-06,
      "loss": 1.1986,
      "step": 1988
    },
    {
      "epoch": 0.279628848587094,
      "grad_norm": 1.498037338256836,
      "learning_rate": 1.6986720403298627e-06,
      "loss": 0.9628,
      "step": 1989
    },
    {
      "epoch": 0.2797694362434978,
      "grad_norm": 2.107191562652588,
      "learning_rate": 1.6562715408356078e-06,
      "loss": 1.0499,
      "step": 1990
    },
    {
      "epoch": 0.2799100238999016,
      "grad_norm": 1.478473424911499,
      "learning_rate": 1.6144025108949968e-06,
      "loss": 1.0049,
      "step": 1991
    },
    {
      "epoch": 0.28005061155630534,
      "grad_norm": 1.5224151611328125,
      "learning_rate": 1.573065176776789e-06,
      "loss": 1.1197,
      "step": 1992
    },
    {
      "epoch": 0.28019119921270913,
      "grad_norm": 1.508825659751892,
      "learning_rate": 1.5322597618763646e-06,
      "loss": 1.0493,
      "step": 1993
    },
    {
      "epoch": 0.28033178686911286,
      "grad_norm": 1.7198872566223145,
      "learning_rate": 1.4919864867145362e-06,
      "loss": 1.0902,
      "step": 1994
    },
    {
      "epoch": 0.28047237452551665,
      "grad_norm": 1.8344477415084839,
      "learning_rate": 1.4522455689362503e-06,
      "loss": 1.1235,
      "step": 1995
    },
    {
      "epoch": 0.28061296218192044,
      "grad_norm": 1.4096039533615112,
      "learning_rate": 1.4130372233095546e-06,
      "loss": 1.0658,
      "step": 1996
    },
    {
      "epoch": 0.2807535498383242,
      "grad_norm": 1.5403516292572021,
      "learning_rate": 1.3743616617243217e-06,
      "loss": 1.0565,
      "step": 1997
    },
    {
      "epoch": 0.28089413749472797,
      "grad_norm": 1.2957475185394287,
      "learning_rate": 1.3362190931911822e-06,
      "loss": 1.17,
      "step": 1998
    },
    {
      "epoch": 0.2810347251511317,
      "grad_norm": 1.6968591213226318,
      "learning_rate": 1.2986097238403384e-06,
      "loss": 1.0771,
      "step": 1999
    },
    {
      "epoch": 0.2811753128075355,
      "grad_norm": 1.4175798892974854,
      "learning_rate": 1.2615337569205077e-06,
      "loss": 1.1591,
      "step": 2000
    },
    {
      "epoch": 0.2811753128075355,
      "eval_loss": 1.1476349830627441,
      "eval_runtime": 771.8518,
      "eval_samples_per_second": 16.384,
      "eval_steps_per_second": 8.192,
      "step": 2000
    },
    {
      "epoch": 0.2813159004639393,
      "grad_norm": 1.5031172037124634,
      "learning_rate": 1.2249913927977475e-06,
      "loss": 1.0557,
      "step": 2001
    },
    {
      "epoch": 0.281456488120343,
      "grad_norm": 1.3757354021072388,
      "learning_rate": 1.1889828289544659e-06,
      "loss": 1.2578,
      "step": 2002
    },
    {
      "epoch": 0.2815970757767468,
      "grad_norm": 1.5433942079544067,
      "learning_rate": 1.1535082599882675e-06,
      "loss": 0.9502,
      "step": 2003
    },
    {
      "epoch": 0.28173766343315054,
      "grad_norm": 1.3650704622268677,
      "learning_rate": 1.1185678776109543e-06,
      "loss": 1.1037,
      "step": 2004
    },
    {
      "epoch": 0.28187825108955433,
      "grad_norm": 1.4916571378707886,
      "learning_rate": 1.084161870647471e-06,
      "loss": 1.0845,
      "step": 2005
    },
    {
      "epoch": 0.2820188387459581,
      "grad_norm": 1.615993618965149,
      "learning_rate": 1.0502904250349054e-06,
      "loss": 1.0139,
      "step": 2006
    },
    {
      "epoch": 0.28215942640236186,
      "grad_norm": 1.560158133506775,
      "learning_rate": 1.0169537238214455e-06,
      "loss": 0.9798,
      "step": 2007
    },
    {
      "epoch": 0.28230001405876565,
      "grad_norm": 1.4725921154022217,
      "learning_rate": 9.841519471654125e-07,
      "loss": 0.9966,
      "step": 2008
    },
    {
      "epoch": 0.2824406017151694,
      "grad_norm": 1.2675812244415283,
      "learning_rate": 9.518852723343075e-07,
      "loss": 1.1809,
      "step": 2009
    },
    {
      "epoch": 0.28258118937157317,
      "grad_norm": 1.3845819234848022,
      "learning_rate": 9.201538737038106e-07,
      "loss": 1.145,
      "step": 2010
    },
    {
      "epoch": 0.28272177702797696,
      "grad_norm": 1.5576871633529663,
      "learning_rate": 8.889579227568612e-07,
      "loss": 1.1835,
      "step": 2011
    },
    {
      "epoch": 0.2828623646843807,
      "grad_norm": 1.5273463726043701,
      "learning_rate": 8.58297588082746e-07,
      "loss": 1.195,
      "step": 2012
    },
    {
      "epoch": 0.2830029523407845,
      "grad_norm": 1.570312738418579,
      "learning_rate": 8.281730353761563e-07,
      "loss": 1.1027,
      "step": 2013
    },
    {
      "epoch": 0.2831435399971882,
      "grad_norm": 1.4842768907546997,
      "learning_rate": 7.985844274363329e-07,
      "loss": 1.0519,
      "step": 2014
    },
    {
      "epoch": 0.283284127653592,
      "grad_norm": 1.5416396856307983,
      "learning_rate": 7.695319241661114e-07,
      "loss": 1.0558,
      "step": 2015
    },
    {
      "epoch": 0.2834247153099958,
      "grad_norm": 1.5355985164642334,
      "learning_rate": 7.410156825711778e-07,
      "loss": 1.1238,
      "step": 2016
    },
    {
      "epoch": 0.28356530296639954,
      "grad_norm": 1.567523717880249,
      "learning_rate": 7.130358567590922e-07,
      "loss": 0.9577,
      "step": 2017
    },
    {
      "epoch": 0.2837058906228033,
      "grad_norm": 1.4636176824569702,
      "learning_rate": 6.855925979385447e-07,
      "loss": 1.2166,
      "step": 2018
    },
    {
      "epoch": 0.28384647827920706,
      "grad_norm": 1.3728286027908325,
      "learning_rate": 6.586860544184781e-07,
      "loss": 1.1496,
      "step": 2019
    },
    {
      "epoch": 0.28398706593561085,
      "grad_norm": 1.2497819662094116,
      "learning_rate": 6.323163716073776e-07,
      "loss": 1.151,
      "step": 2020
    },
    {
      "epoch": 0.28412765359201464,
      "grad_norm": 1.436539649963379,
      "learning_rate": 6.064836920123496e-07,
      "loss": 1.0769,
      "step": 2021
    },
    {
      "epoch": 0.2842682412484184,
      "grad_norm": 1.4800208806991577,
      "learning_rate": 5.811881552384768e-07,
      "loss": 1.1106,
      "step": 2022
    },
    {
      "epoch": 0.28440882890482216,
      "grad_norm": 1.5180922746658325,
      "learning_rate": 5.564298979879868e-07,
      "loss": 1.2679,
      "step": 2023
    },
    {
      "epoch": 0.2845494165612259,
      "grad_norm": 1.3939441442489624,
      "learning_rate": 5.322090540595515e-07,
      "loss": 1.2431,
      "step": 2024
    },
    {
      "epoch": 0.2846900042176297,
      "grad_norm": 1.5369542837142944,
      "learning_rate": 5.085257543475552e-07,
      "loss": 1.1129,
      "step": 2025
    },
    {
      "epoch": 0.2848305918740335,
      "grad_norm": 1.4204654693603516,
      "learning_rate": 4.853801268413616e-07,
      "loss": 1.1646,
      "step": 2026
    },
    {
      "epoch": 0.2849711795304372,
      "grad_norm": 1.5011147260665894,
      "learning_rate": 4.627722966246806e-07,
      "loss": 1.0921,
      "step": 2027
    },
    {
      "epoch": 0.285111767186841,
      "grad_norm": 1.4494242668151855,
      "learning_rate": 4.407023858748138e-07,
      "loss": 1.1376,
      "step": 2028
    },
    {
      "epoch": 0.28525235484324474,
      "grad_norm": 1.4798598289489746,
      "learning_rate": 4.1917051386206606e-07,
      "loss": 1.0422,
      "step": 2029
    },
    {
      "epoch": 0.2853929424996485,
      "grad_norm": 1.4072519540786743,
      "learning_rate": 3.9817679694906797e-07,
      "loss": 1.0959,
      "step": 2030
    },
    {
      "epoch": 0.2855335301560523,
      "grad_norm": 1.5167269706726074,
      "learning_rate": 3.777213485901432e-07,
      "loss": 1.1847,
      "step": 2031
    },
    {
      "epoch": 0.28567411781245605,
      "grad_norm": 1.7162890434265137,
      "learning_rate": 3.5780427933068685e-07,
      "loss": 1.1903,
      "step": 2032
    },
    {
      "epoch": 0.28581470546885984,
      "grad_norm": 1.1971735954284668,
      "learning_rate": 3.3842569680662127e-07,
      "loss": 1.0789,
      "step": 2033
    },
    {
      "epoch": 0.2859552931252636,
      "grad_norm": 1.4415709972381592,
      "learning_rate": 3.1958570574375237e-07,
      "loss": 1.0285,
      "step": 2034
    },
    {
      "epoch": 0.28609588078166737,
      "grad_norm": 1.690149188041687,
      "learning_rate": 3.0128440795723633e-07,
      "loss": 1.1027,
      "step": 2035
    },
    {
      "epoch": 0.28623646843807116,
      "grad_norm": 1.4007121324539185,
      "learning_rate": 2.835219023509916e-07,
      "loss": 1.141,
      "step": 2036
    },
    {
      "epoch": 0.2863770560944749,
      "grad_norm": 1.3839969635009766,
      "learning_rate": 2.662982849172546e-07,
      "loss": 1.0412,
      "step": 2037
    },
    {
      "epoch": 0.2865176437508787,
      "grad_norm": 1.5808452367782593,
      "learning_rate": 2.4961364873593576e-07,
      "loss": 1.02,
      "step": 2038
    },
    {
      "epoch": 0.2866582314072824,
      "grad_norm": 1.3689910173416138,
      "learning_rate": 2.3346808397423093e-07,
      "loss": 0.9294,
      "step": 2039
    },
    {
      "epoch": 0.2867988190636862,
      "grad_norm": 1.5193403959274292,
      "learning_rate": 2.1786167788604428e-07,
      "loss": 1.0765,
      "step": 2040
    },
    {
      "epoch": 0.28693940672009,
      "grad_norm": 1.277463436126709,
      "learning_rate": 2.027945148115884e-07,
      "loss": 1.12,
      "step": 2041
    },
    {
      "epoch": 0.28707999437649373,
      "grad_norm": 1.7244422435760498,
      "learning_rate": 1.8826667617687365e-07,
      "loss": 1.0167,
      "step": 2042
    },
    {
      "epoch": 0.2872205820328975,
      "grad_norm": 1.7544076442718506,
      "learning_rate": 1.7427824049330854e-07,
      "loss": 1.1316,
      "step": 2043
    },
    {
      "epoch": 0.28736116968930125,
      "grad_norm": 1.3945910930633545,
      "learning_rate": 1.6082928335724444e-07,
      "loss": 1.1873,
      "step": 2044
    },
    {
      "epoch": 0.28750175734570504,
      "grad_norm": 1.6343894004821777,
      "learning_rate": 1.4791987744959823e-07,
      "loss": 1.1147,
      "step": 2045
    },
    {
      "epoch": 0.28764234500210883,
      "grad_norm": 1.3554530143737793,
      "learning_rate": 1.3555009253543026e-07,
      "loss": 1.1975,
      "step": 2046
    },
    {
      "epoch": 0.28778293265851257,
      "grad_norm": 1.4255458116531372,
      "learning_rate": 1.2371999546355594e-07,
      "loss": 1.1673,
      "step": 2047
    },
    {
      "epoch": 0.28792352031491636,
      "grad_norm": 1.6745336055755615,
      "learning_rate": 1.1242965016625695e-07,
      "loss": 1.451,
      "step": 2048
    },
    {
      "epoch": 0.2880641079713201,
      "grad_norm": 1.5666884183883667,
      "learning_rate": 1.016791176588594e-07,
      "loss": 1.0282,
      "step": 2049
    },
    {
      "epoch": 0.2882046956277239,
      "grad_norm": 1.633862853050232,
      "learning_rate": 9.14684560394119e-08,
      "loss": 1.1106,
      "step": 2050
    },
    {
      "epoch": 0.2883452832841277,
      "grad_norm": 1.5345650911331177,
      "learning_rate": 8.179772048843015e-08,
      "loss": 1.0642,
      "step": 2051
    },
    {
      "epoch": 0.2884858709405314,
      "grad_norm": 1.4833000898361206,
      "learning_rate": 7.266696326853062e-08,
      "loss": 1.1598,
      "step": 2052
    },
    {
      "epoch": 0.2886264585969352,
      "grad_norm": 1.4100396633148193,
      "learning_rate": 6.407623372419735e-08,
      "loss": 1.0768,
      "step": 2053
    },
    {
      "epoch": 0.28876704625333893,
      "grad_norm": 1.4840251207351685,
      "learning_rate": 5.6025578281471145e-08,
      "loss": 1.0988,
      "step": 2054
    },
    {
      "epoch": 0.2889076339097427,
      "grad_norm": 1.553389549255371,
      "learning_rate": 4.851504044773858e-08,
      "loss": 1.1208,
      "step": 2055
    },
    {
      "epoch": 0.2890482215661465,
      "grad_norm": 1.300073266029358,
      "learning_rate": 4.154466081147668e-08,
      "loss": 0.9492,
      "step": 2056
    },
    {
      "epoch": 0.28918880922255025,
      "grad_norm": 1.3478394746780396,
      "learning_rate": 3.511447704204196e-08,
      "loss": 1.19,
      "step": 2057
    },
    {
      "epoch": 0.28932939687895404,
      "grad_norm": 1.4302232265472412,
      "learning_rate": 2.922452388945951e-08,
      "loss": 1.1609,
      "step": 2058
    },
    {
      "epoch": 0.28946998453535777,
      "grad_norm": 1.4906209707260132,
      "learning_rate": 2.3874833184234226e-08,
      "loss": 1.2879,
      "step": 2059
    },
    {
      "epoch": 0.28961057219176156,
      "grad_norm": 1.836055040359497,
      "learning_rate": 1.9065433837173184e-08,
      "loss": 1.1922,
      "step": 2060
    },
    {
      "epoch": 0.28975115984816535,
      "grad_norm": 1.511201024055481,
      "learning_rate": 1.4796351839263534e-08,
      "loss": 1.1624,
      "step": 2061
    },
    {
      "epoch": 0.2898917475045691,
      "grad_norm": 1.8918993473052979,
      "learning_rate": 1.1067610261494832e-08,
      "loss": 1.0161,
      "step": 2062
    },
    {
      "epoch": 0.2900323351609729,
      "grad_norm": 1.2767348289489746,
      "learning_rate": 7.879229254748043e-09,
      "loss": 1.2256,
      "step": 2063
    },
    {
      "epoch": 0.2901729228173766,
      "grad_norm": 1.5776616334915161,
      "learning_rate": 5.23122604967341e-09,
      "loss": 1.0756,
      "step": 2064
    },
    {
      "epoch": 0.2903135104737804,
      "grad_norm": 1.5285292863845825,
      "learning_rate": 3.123614956634935e-09,
      "loss": 1.052,
      "step": 2065
    },
    {
      "epoch": 0.2904540981301842,
      "grad_norm": 1.3718680143356323,
      "learning_rate": 1.5564073655771615e-09,
      "loss": 1.1153,
      "step": 2066
    },
    {
      "epoch": 0.2905946857865879,
      "grad_norm": 1.5747724771499634,
      "learning_rate": 5.296117460140693e-10,
      "loss": 1.0307,
      "step": 2067
    },
    {
      "epoch": 0.2907352734429917,
      "grad_norm": 1.3631147146224976,
      "learning_rate": 4.323364696245946e-11,
      "loss": 1.1694,
      "step": 2068
    },
    {
      "epoch": 0.29087586109939545,
      "grad_norm": 1.3556069135665894,
      "learning_rate": 9.727569690864968e-11,
      "loss": 1.2536,
      "step": 2069
    },
    {
      "epoch": 0.29101644875579924,
      "grad_norm": 1.371313452720642,
      "learning_rate": 6.917376037973711e-10,
      "loss": 1.1537,
      "step": 2070
    },
    {
      "epoch": 0.29115703641220303,
      "grad_norm": 1.554242730140686,
      "learning_rate": 1.8266161550317685e-09,
      "loss": 1.0196,
      "step": 2071
    },
    {
      "epoch": 0.29129762406860676,
      "grad_norm": 1.6822043657302856,
      "learning_rate": 3.501905217506707e-09,
      "loss": 1.0525,
      "step": 2072
    },
    {
      "epoch": 0.29143821172501055,
      "grad_norm": 1.3837276697158813,
      "learning_rate": 5.717595737608772e-09,
      "loss": 1.2177,
      "step": 2073
    },
    {
      "epoch": 0.2915787993814143,
      "grad_norm": 1.7001011371612549,
      "learning_rate": 8.473675741293986e-09,
      "loss": 1.0091,
      "step": 2074
    },
    {
      "epoch": 0.2917193870378181,
      "grad_norm": 1.5086917877197266,
      "learning_rate": 1.1770130334154417e-08,
      "loss": 1.1586,
      "step": 2075
    },
    {
      "epoch": 0.29185997469422187,
      "grad_norm": 1.5363924503326416,
      "learning_rate": 1.5606941701462596e-08,
      "loss": 0.9835,
      "step": 2076
    },
    {
      "epoch": 0.2920005623506256,
      "grad_norm": 1.3545851707458496,
      "learning_rate": 1.998408910831584e-08,
      "loss": 1.1644,
      "step": 2077
    },
    {
      "epoch": 0.2921411500070294,
      "grad_norm": 1.4151930809020996,
      "learning_rate": 2.490154889972507e-08,
      "loss": 1.2109,
      "step": 2078
    },
    {
      "epoch": 0.2922817376634331,
      "grad_norm": 1.4708632230758667,
      "learning_rate": 3.035929450072583e-08,
      "loss": 1.1935,
      "step": 2079
    },
    {
      "epoch": 0.2924223253198369,
      "grad_norm": 1.5734426975250244,
      "learning_rate": 3.635729641654484e-08,
      "loss": 1.0489,
      "step": 2080
    },
    {
      "epoch": 0.2925629129762407,
      "grad_norm": 1.4322646856307983,
      "learning_rate": 4.2895522232766496e-08,
      "loss": 1.0513,
      "step": 2081
    },
    {
      "epoch": 0.29270350063264444,
      "grad_norm": 1.4104416370391846,
      "learning_rate": 4.9973936615488326e-08,
      "loss": 1.272,
      "step": 2082
    },
    {
      "epoch": 0.29284408828904823,
      "grad_norm": 1.3405824899673462,
      "learning_rate": 5.7592501311498624e-08,
      "loss": 0.9622,
      "step": 2083
    },
    {
      "epoch": 0.29298467594545197,
      "grad_norm": 1.534777283668518,
      "learning_rate": 6.57511751485429e-08,
      "loss": 1.0869,
      "step": 2084
    },
    {
      "epoch": 0.29312526360185576,
      "grad_norm": 1.4600536823272705,
      "learning_rate": 7.44499140354682e-08,
      "loss": 1.0722,
      "step": 2085
    },
    {
      "epoch": 0.29326585125825955,
      "grad_norm": 1.5318377017974854,
      "learning_rate": 8.368867096252287e-08,
      "loss": 1.2274,
      "step": 2086
    },
    {
      "epoch": 0.2934064389146633,
      "grad_norm": 1.5747817754745483,
      "learning_rate": 9.346739600158972e-08,
      "loss": 1.0011,
      "step": 2087
    },
    {
      "epoch": 0.29354702657106707,
      "grad_norm": 1.5902436971664429,
      "learning_rate": 1.0378603630643025e-07,
      "loss": 1.0914,
      "step": 2088
    },
    {
      "epoch": 0.2936876142274708,
      "grad_norm": 1.395960807800293,
      "learning_rate": 1.1464453611300663e-07,
      "loss": 1.0747,
      "step": 2089
    },
    {
      "epoch": 0.2938282018838746,
      "grad_norm": 1.407586932182312,
      "learning_rate": 1.2604283673979257e-07,
      "loss": 1.0879,
      "step": 2090
    },
    {
      "epoch": 0.2939687895402784,
      "grad_norm": 1.4049508571624756,
      "learning_rate": 1.379808765880397e-07,
      "loss": 1.1831,
      "step": 2091
    },
    {
      "epoch": 0.2941093771966821,
      "grad_norm": 1.748708724975586,
      "learning_rate": 1.504585911421441e-07,
      "loss": 1.1425,
      "step": 2092
    },
    {
      "epoch": 0.2942499648530859,
      "grad_norm": 1.7278168201446533,
      "learning_rate": 1.6347591296999031e-07,
      "loss": 0.8781,
      "step": 2093
    },
    {
      "epoch": 0.29439055250948964,
      "grad_norm": 1.3054282665252686,
      "learning_rate": 1.770327717233178e-07,
      "loss": 1.1222,
      "step": 2094
    },
    {
      "epoch": 0.29453114016589343,
      "grad_norm": 1.5526026487350464,
      "learning_rate": 1.9112909413810942e-07,
      "loss": 1.1301,
      "step": 2095
    },
    {
      "epoch": 0.2946717278222972,
      "grad_norm": 1.634819746017456,
      "learning_rate": 2.0576480403495802e-07,
      "loss": 1.0431,
      "step": 2096
    },
    {
      "epoch": 0.29481231547870096,
      "grad_norm": 1.3928965330123901,
      "learning_rate": 2.2093982231951026e-07,
      "loss": 1.123,
      "step": 2097
    },
    {
      "epoch": 0.29495290313510475,
      "grad_norm": 1.507536768913269,
      "learning_rate": 2.3665406698285542e-07,
      "loss": 1.1259,
      "step": 2098
    },
    {
      "epoch": 0.2950934907915085,
      "grad_norm": 1.4346659183502197,
      "learning_rate": 2.529074531020359e-07,
      "loss": 1.213,
      "step": 2099
    },
    {
      "epoch": 0.2952340784479123,
      "grad_norm": 1.5032918453216553,
      "learning_rate": 2.6969989284042484e-07,
      "loss": 1.1541,
      "step": 2100
    },
    {
      "epoch": 0.29537466610431606,
      "grad_norm": 1.2891212701797485,
      "learning_rate": 2.8703129544825903e-07,
      "loss": 1.0075,
      "step": 2101
    },
    {
      "epoch": 0.2955152537607198,
      "grad_norm": 1.4989335536956787,
      "learning_rate": 3.049015672631161e-07,
      "loss": 1.0168,
      "step": 2102
    },
    {
      "epoch": 0.2956558414171236,
      "grad_norm": 1.6083587408065796,
      "learning_rate": 3.2331061171039234e-07,
      "loss": 1.0247,
      "step": 2103
    },
    {
      "epoch": 0.2957964290735273,
      "grad_norm": 1.4017013311386108,
      "learning_rate": 3.4225832930385726e-07,
      "loss": 1.1819,
      "step": 2104
    },
    {
      "epoch": 0.2959370167299311,
      "grad_norm": 1.3218934535980225,
      "learning_rate": 3.61744617646198e-07,
      "loss": 1.0511,
      "step": 2105
    },
    {
      "epoch": 0.2960776043863349,
      "grad_norm": 1.6402549743652344,
      "learning_rate": 3.81769371429519e-07,
      "loss": 1.1921,
      "step": 2106
    },
    {
      "epoch": 0.29621819204273864,
      "grad_norm": 1.6788759231567383,
      "learning_rate": 4.023324824359964e-07,
      "loss": 1.2626,
      "step": 2107
    },
    {
      "epoch": 0.2963587796991424,
      "grad_norm": 1.287074089050293,
      "learning_rate": 4.2343383953836745e-07,
      "loss": 1.0942,
      "step": 2108
    },
    {
      "epoch": 0.29649936735554616,
      "grad_norm": 1.7501200437545776,
      "learning_rate": 4.4507332870059594e-07,
      "loss": 1.1764,
      "step": 2109
    },
    {
      "epoch": 0.29663995501194995,
      "grad_norm": 1.4297988414764404,
      "learning_rate": 4.6725083297848304e-07,
      "loss": 0.9255,
      "step": 2110
    },
    {
      "epoch": 0.29678054266835374,
      "grad_norm": 1.4229438304901123,
      "learning_rate": 4.89966232520267e-07,
      "loss": 1.0835,
      "step": 2111
    },
    {
      "epoch": 0.2969211303247575,
      "grad_norm": 1.6475692987442017,
      "learning_rate": 5.132194045673111e-07,
      "loss": 1.1707,
      "step": 2112
    },
    {
      "epoch": 0.29706171798116127,
      "grad_norm": 1.6095492839813232,
      "learning_rate": 5.370102234547147e-07,
      "loss": 1.0918,
      "step": 2113
    },
    {
      "epoch": 0.297202305637565,
      "grad_norm": 1.4704030752182007,
      "learning_rate": 5.613385606120569e-07,
      "loss": 1.028,
      "step": 2114
    },
    {
      "epoch": 0.2973428932939688,
      "grad_norm": 1.6137768030166626,
      "learning_rate": 5.862042845640403e-07,
      "loss": 0.9637,
      "step": 2115
    },
    {
      "epoch": 0.2974834809503726,
      "grad_norm": 1.6161149740219116,
      "learning_rate": 6.11607260931224e-07,
      "loss": 1.1781,
      "step": 2116
    },
    {
      "epoch": 0.2976240686067763,
      "grad_norm": 1.6209287643432617,
      "learning_rate": 6.375473524307451e-07,
      "loss": 1.0292,
      "step": 2117
    },
    {
      "epoch": 0.2977646562631801,
      "grad_norm": 1.401870608329773,
      "learning_rate": 6.640244188770739e-07,
      "loss": 1.1445,
      "step": 2118
    },
    {
      "epoch": 0.29790524391958384,
      "grad_norm": 1.4407286643981934,
      "learning_rate": 6.910383171827351e-07,
      "loss": 1.1499,
      "step": 2119
    },
    {
      "epoch": 0.29804583157598763,
      "grad_norm": 1.5600831508636475,
      "learning_rate": 7.185889013591074e-07,
      "loss": 0.9308,
      "step": 2120
    },
    {
      "epoch": 0.2981864192323914,
      "grad_norm": 1.452297568321228,
      "learning_rate": 7.466760225172342e-07,
      "loss": 0.9862,
      "step": 2121
    },
    {
      "epoch": 0.29832700688879515,
      "grad_norm": 1.6887307167053223,
      "learning_rate": 7.752995288685894e-07,
      "loss": 1.0035,
      "step": 2122
    },
    {
      "epoch": 0.29846759454519894,
      "grad_norm": 1.4682317972183228,
      "learning_rate": 8.044592657258987e-07,
      "loss": 1.2522,
      "step": 2123
    },
    {
      "epoch": 0.2986081822016027,
      "grad_norm": 1.4903210401535034,
      "learning_rate": 8.341550755040062e-07,
      "loss": 0.9441,
      "step": 2124
    },
    {
      "epoch": 0.29874876985800647,
      "grad_norm": 1.2723902463912964,
      "learning_rate": 8.643867977206954e-07,
      "loss": 1.0111,
      "step": 2125
    },
    {
      "epoch": 0.29888935751441026,
      "grad_norm": 1.5484318733215332,
      "learning_rate": 8.951542689976e-07,
      "loss": 1.0249,
      "step": 2126
    },
    {
      "epoch": 0.299029945170814,
      "grad_norm": 1.4897651672363281,
      "learning_rate": 9.264573230610029e-07,
      "loss": 1.0199,
      "step": 2127
    },
    {
      "epoch": 0.2991705328272178,
      "grad_norm": 1.6007806062698364,
      "learning_rate": 9.58295790742847e-07,
      "loss": 1.1984,
      "step": 2128
    },
    {
      "epoch": 0.2993111204836215,
      "grad_norm": 1.7752472162246704,
      "learning_rate": 9.906694999815446e-07,
      "loss": 1.0633,
      "step": 2129
    },
    {
      "epoch": 0.2994517081400253,
      "grad_norm": 1.6050792932510376,
      "learning_rate": 1.0235782758229894e-06,
      "loss": 1.0824,
      "step": 2130
    },
    {
      "epoch": 0.2995922957964291,
      "grad_norm": 1.3362979888916016,
      "learning_rate": 1.057021940421421e-06,
      "loss": 1.0395,
      "step": 2131
    },
    {
      "epoch": 0.29973288345283283,
      "grad_norm": 1.3762074708938599,
      "learning_rate": 1.0910003130404911e-06,
      "loss": 1.2769,
      "step": 2132
    },
    {
      "epoch": 0.2998734711092366,
      "grad_norm": 1.5603691339492798,
      "learning_rate": 1.12551321005413e-06,
      "loss": 1.2457,
      "step": 2133
    },
    {
      "epoch": 0.30001405876564036,
      "grad_norm": 1.6741743087768555,
      "learning_rate": 1.1605604449476116e-06,
      "loss": 1.1638,
      "step": 2134
    },
    {
      "epoch": 0.30015464642204415,
      "grad_norm": 1.4574658870697021,
      "learning_rate": 1.196141828318531e-06,
      "loss": 1.0279,
      "step": 2135
    },
    {
      "epoch": 0.30029523407844794,
      "grad_norm": 1.3236932754516602,
      "learning_rate": 1.2322571678778482e-06,
      "loss": 1.0507,
      "step": 2136
    },
    {
      "epoch": 0.30043582173485167,
      "grad_norm": 1.717746615409851,
      "learning_rate": 1.2689062684508978e-06,
      "loss": 1.0859,
      "step": 2137
    },
    {
      "epoch": 0.30057640939125546,
      "grad_norm": 1.4784644842147827,
      "learning_rate": 1.3060889319784885e-06,
      "loss": 1.2348,
      "step": 2138
    },
    {
      "epoch": 0.3007169970476592,
      "grad_norm": 1.3315507173538208,
      "learning_rate": 1.3438049575179025e-06,
      "loss": 1.0709,
      "step": 2139
    },
    {
      "epoch": 0.300857584704063,
      "grad_norm": 1.584871768951416,
      "learning_rate": 1.3820541412440713e-06,
      "loss": 0.9577,
      "step": 2140
    },
    {
      "epoch": 0.3009981723604668,
      "grad_norm": 1.3968477249145508,
      "learning_rate": 1.420836276450599e-06,
      "loss": 1.1058,
      "step": 2141
    },
    {
      "epoch": 0.3011387600168705,
      "grad_norm": 1.5823681354522705,
      "learning_rate": 1.460151153550926e-06,
      "loss": 1.0269,
      "step": 2142
    },
    {
      "epoch": 0.3012793476732743,
      "grad_norm": 1.30938720703125,
      "learning_rate": 1.4999985600794298e-06,
      "loss": 1.0224,
      "step": 2143
    },
    {
      "epoch": 0.30141993532967803,
      "grad_norm": 1.4543120861053467,
      "learning_rate": 1.5403782806926225e-06,
      "loss": 1.1904,
      "step": 2144
    },
    {
      "epoch": 0.3015605229860818,
      "grad_norm": 1.7214415073394775,
      "learning_rate": 1.5812900971702627e-06,
      "loss": 1.1023,
      "step": 2145
    },
    {
      "epoch": 0.3017011106424856,
      "grad_norm": 1.532670021057129,
      "learning_rate": 1.6227337884165417e-06,
      "loss": 1.031,
      "step": 2146
    },
    {
      "epoch": 0.30184169829888935,
      "grad_norm": 1.3816742897033691,
      "learning_rate": 1.6647091304613172e-06,
      "loss": 0.9485,
      "step": 2147
    },
    {
      "epoch": 0.30198228595529314,
      "grad_norm": 1.3201311826705933,
      "learning_rate": 1.7072158964612672e-06,
      "loss": 0.9842,
      "step": 2148
    },
    {
      "epoch": 0.3021228736116969,
      "grad_norm": 1.3898518085479736,
      "learning_rate": 1.7502538567012007e-06,
      "loss": 1.1951,
      "step": 2149
    },
    {
      "epoch": 0.30226346126810066,
      "grad_norm": 1.3466874361038208,
      "learning_rate": 1.7938227785951668e-06,
      "loss": 1.2461,
      "step": 2150
    },
    {
      "epoch": 0.30240404892450445,
      "grad_norm": 1.5167697668075562,
      "learning_rate": 1.8379224266878548e-06,
      "loss": 1.1909,
      "step": 2151
    },
    {
      "epoch": 0.3025446365809082,
      "grad_norm": 1.4345914125442505,
      "learning_rate": 1.8825525626557594e-06,
      "loss": 1.171,
      "step": 2152
    },
    {
      "epoch": 0.302685224237312,
      "grad_norm": 1.429538369178772,
      "learning_rate": 1.927712945308557e-06,
      "loss": 1.114,
      "step": 2153
    },
    {
      "epoch": 0.3028258118937157,
      "grad_norm": 1.6275800466537476,
      "learning_rate": 1.9734033305903066e-06,
      "loss": 0.98,
      "step": 2154
    },
    {
      "epoch": 0.3029663995501195,
      "grad_norm": 1.8187564611434937,
      "learning_rate": 2.019623471580867e-06,
      "loss": 1.2625,
      "step": 2155
    },
    {
      "epoch": 0.3031069872065233,
      "grad_norm": 1.4205243587493896,
      "learning_rate": 2.0663731184971778e-06,
      "loss": 1.196,
      "step": 2156
    },
    {
      "epoch": 0.303247574862927,
      "grad_norm": 1.6312562227249146,
      "learning_rate": 2.113652018694612e-06,
      "loss": 1.2388,
      "step": 2157
    },
    {
      "epoch": 0.3033881625193308,
      "grad_norm": 1.3889849185943604,
      "learning_rate": 2.161459916668351e-06,
      "loss": 1.1208,
      "step": 2158
    },
    {
      "epoch": 0.30352875017573455,
      "grad_norm": 1.4144657850265503,
      "learning_rate": 2.2097965540547992e-06,
      "loss": 0.9841,
      "step": 2159
    },
    {
      "epoch": 0.30366933783213834,
      "grad_norm": 1.436781644821167,
      "learning_rate": 2.2586616696328664e-06,
      "loss": 1.0658,
      "step": 2160
    },
    {
      "epoch": 0.30380992548854213,
      "grad_norm": 1.5081862211227417,
      "learning_rate": 2.3080549993255595e-06,
      "loss": 1.0379,
      "step": 2161
    },
    {
      "epoch": 0.30395051314494587,
      "grad_norm": 1.4369542598724365,
      "learning_rate": 2.3579762762012127e-06,
      "loss": 1.2564,
      "step": 2162
    },
    {
      "epoch": 0.30409110080134966,
      "grad_norm": 1.6051793098449707,
      "learning_rate": 2.4084252304751088e-06,
      "loss": 1.0226,
      "step": 2163
    },
    {
      "epoch": 0.3042316884577534,
      "grad_norm": 1.6071279048919678,
      "learning_rate": 2.4594015895107904e-06,
      "loss": 1.0587,
      "step": 2164
    },
    {
      "epoch": 0.3043722761141572,
      "grad_norm": 1.4366730451583862,
      "learning_rate": 2.5109050778216347e-06,
      "loss": 1.1372,
      "step": 2165
    },
    {
      "epoch": 0.30451286377056097,
      "grad_norm": 1.665662407875061,
      "learning_rate": 2.562935417072276e-06,
      "loss": 1.1471,
      "step": 2166
    },
    {
      "epoch": 0.3046534514269647,
      "grad_norm": 1.3750959634780884,
      "learning_rate": 2.6154923260801824e-06,
      "loss": 0.9287,
      "step": 2167
    },
    {
      "epoch": 0.3047940390833685,
      "grad_norm": 1.663771629333496,
      "learning_rate": 2.668575520817096e-06,
      "loss": 1.0453,
      "step": 2168
    },
    {
      "epoch": 0.30493462673977223,
      "grad_norm": 1.258080005645752,
      "learning_rate": 2.722184714410614e-06,
      "loss": 1.1675,
      "step": 2169
    },
    {
      "epoch": 0.305075214396176,
      "grad_norm": 1.754284143447876,
      "learning_rate": 2.776319617145695e-06,
      "loss": 0.9575,
      "step": 2170
    },
    {
      "epoch": 0.3052158020525798,
      "grad_norm": 1.496250867843628,
      "learning_rate": 2.830979936466338e-06,
      "loss": 1.0309,
      "step": 2171
    },
    {
      "epoch": 0.30535638970898354,
      "grad_norm": 1.5736641883850098,
      "learning_rate": 2.8861653769770014e-06,
      "loss": 1.2617,
      "step": 2172
    },
    {
      "epoch": 0.30549697736538733,
      "grad_norm": 1.7080246210098267,
      "learning_rate": 2.941875640444347e-06,
      "loss": 0.9908,
      "step": 2173
    },
    {
      "epoch": 0.30563756502179107,
      "grad_norm": 1.525061845779419,
      "learning_rate": 2.998110425798717e-06,
      "loss": 0.9873,
      "step": 2174
    },
    {
      "epoch": 0.30577815267819486,
      "grad_norm": 1.3414701223373413,
      "learning_rate": 3.05486942913592e-06,
      "loss": 1.1748,
      "step": 2175
    },
    {
      "epoch": 0.30591874033459865,
      "grad_norm": 1.5051696300506592,
      "learning_rate": 3.1121523437186774e-06,
      "loss": 1.0573,
      "step": 2176
    },
    {
      "epoch": 0.3060593279910024,
      "grad_norm": 1.4089961051940918,
      "learning_rate": 3.1699588599784837e-06,
      "loss": 1.095,
      "step": 2177
    },
    {
      "epoch": 0.3061999156474062,
      "grad_norm": 1.6059106588363647,
      "learning_rate": 3.2282886655171207e-06,
      "loss": 1.0347,
      "step": 2178
    },
    {
      "epoch": 0.3063405033038099,
      "grad_norm": 1.4730181694030762,
      "learning_rate": 3.2871414451084547e-06,
      "loss": 1.2297,
      "step": 2179
    },
    {
      "epoch": 0.3064810909602137,
      "grad_norm": 1.4801195859909058,
      "learning_rate": 3.3465168807000345e-06,
      "loss": 0.9763,
      "step": 2180
    },
    {
      "epoch": 0.3066216786166175,
      "grad_norm": 1.6411943435668945,
      "learning_rate": 3.406414651414935e-06,
      "loss": 0.9714,
      "step": 2181
    },
    {
      "epoch": 0.3067622662730212,
      "grad_norm": 1.5691460371017456,
      "learning_rate": 3.4668344335533678e-06,
      "loss": 1.1237,
      "step": 2182
    },
    {
      "epoch": 0.306902853929425,
      "grad_norm": 1.4515266418457031,
      "learning_rate": 3.527775900594543e-06,
      "loss": 1.2826,
      "step": 2183
    },
    {
      "epoch": 0.30704344158582875,
      "grad_norm": 1.3572288751602173,
      "learning_rate": 3.5892387231983292e-06,
      "loss": 1.1915,
      "step": 2184
    },
    {
      "epoch": 0.30718402924223254,
      "grad_norm": 1.6041160821914673,
      "learning_rate": 3.6512225692071244e-06,
      "loss": 1.077,
      "step": 2185
    },
    {
      "epoch": 0.3073246168986363,
      "grad_norm": 1.52364981174469,
      "learning_rate": 3.7137271036475684e-06,
      "loss": 1.2156,
      "step": 2186
    },
    {
      "epoch": 0.30746520455504006,
      "grad_norm": 1.6818699836730957,
      "learning_rate": 3.7767519887324297e-06,
      "loss": 1.1872,
      "step": 2187
    },
    {
      "epoch": 0.30760579221144385,
      "grad_norm": 1.4065322875976562,
      "learning_rate": 3.840296883862393e-06,
      "loss": 1.0961,
      "step": 2188
    },
    {
      "epoch": 0.3077463798678476,
      "grad_norm": 1.4919540882110596,
      "learning_rate": 3.904361445627869e-06,
      "loss": 1.0487,
      "step": 2189
    },
    {
      "epoch": 0.3078869675242514,
      "grad_norm": 1.3514857292175293,
      "learning_rate": 3.968945327810925e-06,
      "loss": 0.9679,
      "step": 2190
    },
    {
      "epoch": 0.30802755518065517,
      "grad_norm": 1.5102182626724243,
      "learning_rate": 4.034048181387085e-06,
      "loss": 1.2892,
      "step": 2191
    },
    {
      "epoch": 0.3081681428370589,
      "grad_norm": 1.368980050086975,
      "learning_rate": 4.099669654527282e-06,
      "loss": 1.167,
      "step": 2192
    },
    {
      "epoch": 0.3083087304934627,
      "grad_norm": 1.3581781387329102,
      "learning_rate": 4.165809392599662e-06,
      "loss": 1.0965,
      "step": 2193
    },
    {
      "epoch": 0.3084493181498664,
      "grad_norm": 1.467124342918396,
      "learning_rate": 4.23246703817165e-06,
      "loss": 1.1199,
      "step": 2194
    },
    {
      "epoch": 0.3085899058062702,
      "grad_norm": 1.6823662519454956,
      "learning_rate": 4.2996422310116915e-06,
      "loss": 1.1668,
      "step": 2195
    },
    {
      "epoch": 0.308730493462674,
      "grad_norm": 1.6194134950637817,
      "learning_rate": 4.367334608091389e-06,
      "loss": 1.0588,
      "step": 2196
    },
    {
      "epoch": 0.30887108111907774,
      "grad_norm": 1.4680997133255005,
      "learning_rate": 4.4355438035873164e-06,
      "loss": 0.9803,
      "step": 2197
    },
    {
      "epoch": 0.30901166877548153,
      "grad_norm": 1.46083402633667,
      "learning_rate": 4.5042694488830915e-06,
      "loss": 1.1874,
      "step": 2198
    },
    {
      "epoch": 0.30915225643188526,
      "grad_norm": 1.4046075344085693,
      "learning_rate": 4.57351117257131e-06,
      "loss": 1.0454,
      "step": 2199
    },
    {
      "epoch": 0.30929284408828905,
      "grad_norm": 1.4879835844039917,
      "learning_rate": 4.643268600455608e-06,
      "loss": 1.0913,
      "step": 2200
    },
    {
      "epoch": 0.3094334317446928,
      "grad_norm": 1.466220498085022,
      "learning_rate": 4.7135413555525866e-06,
      "loss": 1.095,
      "step": 2201
    },
    {
      "epoch": 0.3095740194010966,
      "grad_norm": 1.361093521118164,
      "learning_rate": 4.784329058093984e-06,
      "loss": 1.1388,
      "step": 2202
    },
    {
      "epoch": 0.30971460705750037,
      "grad_norm": 1.3678358793258667,
      "learning_rate": 4.855631325528609e-06,
      "loss": 1.1929,
      "step": 2203
    },
    {
      "epoch": 0.3098551947139041,
      "grad_norm": 1.4158788919448853,
      "learning_rate": 4.927447772524507e-06,
      "loss": 1.0103,
      "step": 2204
    },
    {
      "epoch": 0.3099957823703079,
      "grad_norm": 1.8388233184814453,
      "learning_rate": 4.999778010970913e-06,
      "loss": 1.1256,
      "step": 2205
    },
    {
      "epoch": 0.3101363700267116,
      "grad_norm": 1.5419753789901733,
      "learning_rate": 5.072621649980525e-06,
      "loss": 1.0468,
      "step": 2206
    },
    {
      "epoch": 0.3102769576831154,
      "grad_norm": 1.5483195781707764,
      "learning_rate": 5.145978295891429e-06,
      "loss": 0.8499,
      "step": 2207
    },
    {
      "epoch": 0.3104175453395192,
      "grad_norm": 1.4003221988677979,
      "learning_rate": 5.219847552269385e-06,
      "loss": 1.0648,
      "step": 2208
    },
    {
      "epoch": 0.31055813299592294,
      "grad_norm": 1.6232738494873047,
      "learning_rate": 5.294229019909858e-06,
      "loss": 1.2759,
      "step": 2209
    },
    {
      "epoch": 0.31069872065232673,
      "grad_norm": 1.5645356178283691,
      "learning_rate": 5.369122296840256e-06,
      "loss": 0.9874,
      "step": 2210
    },
    {
      "epoch": 0.31083930830873047,
      "grad_norm": 1.313323974609375,
      "learning_rate": 5.444526978322007e-06,
      "loss": 1.1395,
      "step": 2211
    },
    {
      "epoch": 0.31097989596513426,
      "grad_norm": 1.44762122631073,
      "learning_rate": 5.520442656852887e-06,
      "loss": 1.3427,
      "step": 2212
    },
    {
      "epoch": 0.31112048362153805,
      "grad_norm": 1.3975509405136108,
      "learning_rate": 5.596868922169074e-06,
      "loss": 1.1523,
      "step": 2213
    },
    {
      "epoch": 0.3112610712779418,
      "grad_norm": 1.3087579011917114,
      "learning_rate": 5.673805361247453e-06,
      "loss": 1.043,
      "step": 2214
    },
    {
      "epoch": 0.31140165893434557,
      "grad_norm": 1.5022962093353271,
      "learning_rate": 5.751251558307824e-06,
      "loss": 1.1068,
      "step": 2215
    },
    {
      "epoch": 0.3115422465907493,
      "grad_norm": 1.5275300741195679,
      "learning_rate": 5.82920709481517e-06,
      "loss": 1.0497,
      "step": 2216
    },
    {
      "epoch": 0.3116828342471531,
      "grad_norm": 1.4267473220825195,
      "learning_rate": 5.907671549481853e-06,
      "loss": 1.2288,
      "step": 2217
    },
    {
      "epoch": 0.3118234219035569,
      "grad_norm": 1.5731414556503296,
      "learning_rate": 5.98664449827e-06,
      "loss": 1.1973,
      "step": 2218
    },
    {
      "epoch": 0.3119640095599606,
      "grad_norm": 1.4305567741394043,
      "learning_rate": 6.066125514393661e-06,
      "loss": 1.0401,
      "step": 2219
    },
    {
      "epoch": 0.3121045972163644,
      "grad_norm": 1.6521694660186768,
      "learning_rate": 6.14611416832126e-06,
      "loss": 1.1512,
      "step": 2220
    },
    {
      "epoch": 0.31224518487276814,
      "grad_norm": 1.3683497905731201,
      "learning_rate": 6.226610027777779e-06,
      "loss": 1.0566,
      "step": 2221
    },
    {
      "epoch": 0.31238577252917193,
      "grad_norm": 1.334836721420288,
      "learning_rate": 6.3076126577472195e-06,
      "loss": 0.9241,
      "step": 2222
    },
    {
      "epoch": 0.3125263601855757,
      "grad_norm": 1.3984036445617676,
      "learning_rate": 6.389121620474836e-06,
      "loss": 0.9636,
      "step": 2223
    },
    {
      "epoch": 0.31266694784197946,
      "grad_norm": 1.69764244556427,
      "learning_rate": 6.471136475469575e-06,
      "loss": 1.158,
      "step": 2224
    },
    {
      "epoch": 0.31280753549838325,
      "grad_norm": 1.3825101852416992,
      "learning_rate": 6.553656779506456e-06,
      "loss": 1.0799,
      "step": 2225
    },
    {
      "epoch": 0.312948123154787,
      "grad_norm": 1.546836495399475,
      "learning_rate": 6.63668208662892e-06,
      "loss": 1.0916,
      "step": 2226
    },
    {
      "epoch": 0.3130887108111908,
      "grad_norm": 1.504961371421814,
      "learning_rate": 6.720211948151334e-06,
      "loss": 0.9813,
      "step": 2227
    },
    {
      "epoch": 0.31322929846759456,
      "grad_norm": 1.5054129362106323,
      "learning_rate": 6.804245912661222e-06,
      "loss": 1.23,
      "step": 2228
    },
    {
      "epoch": 0.3133698861239983,
      "grad_norm": 1.4661927223205566,
      "learning_rate": 6.88878352602198e-06,
      "loss": 1.0572,
      "step": 2229
    },
    {
      "epoch": 0.3135104737804021,
      "grad_norm": 1.5271692276000977,
      "learning_rate": 6.973824331375056e-06,
      "loss": 1.1045,
      "step": 2230
    },
    {
      "epoch": 0.3136510614368058,
      "grad_norm": 1.2110599279403687,
      "learning_rate": 7.059367869142663e-06,
      "loss": 1.2416,
      "step": 2231
    },
    {
      "epoch": 0.3137916490932096,
      "grad_norm": 1.4872448444366455,
      "learning_rate": 7.145413677030044e-06,
      "loss": 1.1224,
      "step": 2232
    },
    {
      "epoch": 0.3139322367496134,
      "grad_norm": 1.5572465658187866,
      "learning_rate": 7.231961290028133e-06,
      "loss": 1.1053,
      "step": 2233
    },
    {
      "epoch": 0.31407282440601714,
      "grad_norm": 1.5224570035934448,
      "learning_rate": 7.31901024041598e-06,
      "loss": 1.022,
      "step": 2234
    },
    {
      "epoch": 0.3142134120624209,
      "grad_norm": 1.6546130180358887,
      "learning_rate": 7.406560057763334e-06,
      "loss": 1.1547,
      "step": 2235
    },
    {
      "epoch": 0.31435399971882466,
      "grad_norm": 1.5701643228530884,
      "learning_rate": 7.494610268933111e-06,
      "loss": 1.1964,
      "step": 2236
    },
    {
      "epoch": 0.31449458737522845,
      "grad_norm": 1.497252345085144,
      "learning_rate": 7.583160398084044e-06,
      "loss": 1.1127,
      "step": 2237
    },
    {
      "epoch": 0.31463517503163224,
      "grad_norm": 1.5550978183746338,
      "learning_rate": 7.67220996667316e-06,
      "loss": 1.2186,
      "step": 2238
    },
    {
      "epoch": 0.314775762688036,
      "grad_norm": 1.9865341186523438,
      "learning_rate": 7.761758493458482e-06,
      "loss": 0.9927,
      "step": 2239
    },
    {
      "epoch": 0.31491635034443977,
      "grad_norm": 1.5144331455230713,
      "learning_rate": 7.851805494501474e-06,
      "loss": 1.1337,
      "step": 2240
    },
    {
      "epoch": 0.3150569380008435,
      "grad_norm": 1.4463096857070923,
      "learning_rate": 7.942350483169814e-06,
      "loss": 1.0404,
      "step": 2241
    },
    {
      "epoch": 0.3151975256572473,
      "grad_norm": 1.5359266996383667,
      "learning_rate": 8.03339297013992e-06,
      "loss": 1.1162,
      "step": 2242
    },
    {
      "epoch": 0.3153381133136511,
      "grad_norm": 1.7842427492141724,
      "learning_rate": 8.124932463399648e-06,
      "loss": 0.9842,
      "step": 2243
    },
    {
      "epoch": 0.3154787009700548,
      "grad_norm": 1.7317911386489868,
      "learning_rate": 8.216968468250886e-06,
      "loss": 1.0105,
      "step": 2244
    },
    {
      "epoch": 0.3156192886264586,
      "grad_norm": 1.679977297782898,
      "learning_rate": 8.30950048731235e-06,
      "loss": 1.2097,
      "step": 2245
    },
    {
      "epoch": 0.31575987628286234,
      "grad_norm": 1.634305715560913,
      "learning_rate": 8.402528020522127e-06,
      "loss": 1.1803,
      "step": 2246
    },
    {
      "epoch": 0.31590046393926613,
      "grad_norm": 1.8168134689331055,
      "learning_rate": 8.496050565140457e-06,
      "loss": 0.978,
      "step": 2247
    },
    {
      "epoch": 0.3160410515956699,
      "grad_norm": 1.5968728065490723,
      "learning_rate": 8.590067615752428e-06,
      "loss": 1.0797,
      "step": 2248
    },
    {
      "epoch": 0.31618163925207365,
      "grad_norm": 1.4383652210235596,
      "learning_rate": 8.684578664270793e-06,
      "loss": 1.0999,
      "step": 2249
    },
    {
      "epoch": 0.31632222690847744,
      "grad_norm": 1.65225350856781,
      "learning_rate": 8.779583199938524e-06,
      "loss": 1.0242,
      "step": 2250
    },
    {
      "epoch": 0.3164628145648812,
      "grad_norm": 1.4342926740646362,
      "learning_rate": 8.875080709331795e-06,
      "loss": 1.2168,
      "step": 2251
    },
    {
      "epoch": 0.31660340222128497,
      "grad_norm": 1.8665882349014282,
      "learning_rate": 8.971070676362558e-06,
      "loss": 1.0841,
      "step": 2252
    },
    {
      "epoch": 0.31674398987768876,
      "grad_norm": 1.506164312362671,
      "learning_rate": 9.067552582281491e-06,
      "loss": 1.1334,
      "step": 2253
    },
    {
      "epoch": 0.3168845775340925,
      "grad_norm": 1.5472074747085571,
      "learning_rate": 9.164525905680699e-06,
      "loss": 0.9976,
      "step": 2254
    },
    {
      "epoch": 0.3170251651904963,
      "grad_norm": 1.3759838342666626,
      "learning_rate": 9.261990122496588e-06,
      "loss": 1.1492,
      "step": 2255
    },
    {
      "epoch": 0.3171657528469,
      "grad_norm": 1.594873070716858,
      "learning_rate": 9.359944706012646e-06,
      "loss": 1.2596,
      "step": 2256
    },
    {
      "epoch": 0.3173063405033038,
      "grad_norm": 1.480833888053894,
      "learning_rate": 9.458389126862355e-06,
      "loss": 1.2391,
      "step": 2257
    },
    {
      "epoch": 0.3174469281597076,
      "grad_norm": 1.4435936212539673,
      "learning_rate": 9.557322853031991e-06,
      "loss": 1.1073,
      "step": 2258
    },
    {
      "epoch": 0.31758751581611133,
      "grad_norm": 1.5617893934249878,
      "learning_rate": 9.656745349863539e-06,
      "loss": 0.9653,
      "step": 2259
    },
    {
      "epoch": 0.3177281034725151,
      "grad_norm": 1.4536830186843872,
      "learning_rate": 9.756656080057547e-06,
      "loss": 1.1221,
      "step": 2260
    },
    {
      "epoch": 0.31786869112891886,
      "grad_norm": 1.5346980094909668,
      "learning_rate": 9.857054503676077e-06,
      "loss": 0.985,
      "step": 2261
    },
    {
      "epoch": 0.31800927878532265,
      "grad_norm": 1.4085376262664795,
      "learning_rate": 9.9579400781456e-06,
      "loss": 1.2497,
      "step": 2262
    },
    {
      "epoch": 0.31814986644172644,
      "grad_norm": 1.7065951824188232,
      "learning_rate": 1.0059312258259856e-05,
      "loss": 1.1867,
      "step": 2263
    },
    {
      "epoch": 0.31829045409813017,
      "grad_norm": 1.4536925554275513,
      "learning_rate": 1.0161170496182981e-05,
      "loss": 1.17,
      "step": 2264
    },
    {
      "epoch": 0.31843104175453396,
      "grad_norm": 1.506919264793396,
      "learning_rate": 1.0263514241452255e-05,
      "loss": 1.09,
      "step": 2265
    },
    {
      "epoch": 0.3185716294109377,
      "grad_norm": 1.260092854499817,
      "learning_rate": 1.0366342940981255e-05,
      "loss": 0.9686,
      "step": 2266
    },
    {
      "epoch": 0.3187122170673415,
      "grad_norm": 1.380090355873108,
      "learning_rate": 1.0469656039062681e-05,
      "loss": 1.1334,
      "step": 2267
    },
    {
      "epoch": 0.3188528047237453,
      "grad_norm": 1.5744314193725586,
      "learning_rate": 1.0573452977371535e-05,
      "loss": 1.159,
      "step": 2268
    },
    {
      "epoch": 0.318993392380149,
      "grad_norm": 1.8518835306167603,
      "learning_rate": 1.0677733194967977e-05,
      "loss": 1.0681,
      "step": 2269
    },
    {
      "epoch": 0.3191339800365528,
      "grad_norm": 1.2564624547958374,
      "learning_rate": 1.0782496128300478e-05,
      "loss": 1.0016,
      "step": 2270
    },
    {
      "epoch": 0.31927456769295653,
      "grad_norm": 1.5382548570632935,
      "learning_rate": 1.0887741211208768e-05,
      "loss": 1.2086,
      "step": 2271
    },
    {
      "epoch": 0.3194151553493603,
      "grad_norm": 1.546035647392273,
      "learning_rate": 1.099346787492701e-05,
      "loss": 1.2433,
      "step": 2272
    },
    {
      "epoch": 0.3195557430057641,
      "grad_norm": 1.4775725603103638,
      "learning_rate": 1.1099675548086707e-05,
      "loss": 1.2518,
      "step": 2273
    },
    {
      "epoch": 0.31969633066216785,
      "grad_norm": 1.5403450727462769,
      "learning_rate": 1.1206363656719998e-05,
      "loss": 0.9665,
      "step": 2274
    },
    {
      "epoch": 0.31983691831857164,
      "grad_norm": 1.2333321571350098,
      "learning_rate": 1.1313531624262553e-05,
      "loss": 1.146,
      "step": 2275
    },
    {
      "epoch": 0.3199775059749754,
      "grad_norm": 1.353036642074585,
      "learning_rate": 1.1421178871556882e-05,
      "loss": 1.1944,
      "step": 2276
    },
    {
      "epoch": 0.32011809363137916,
      "grad_norm": 1.3629494905471802,
      "learning_rate": 1.1529304816855258e-05,
      "loss": 1.1527,
      "step": 2277
    },
    {
      "epoch": 0.32025868128778295,
      "grad_norm": 1.4694194793701172,
      "learning_rate": 1.16379088758231e-05,
      "loss": 0.959,
      "step": 2278
    },
    {
      "epoch": 0.3203992689441867,
      "grad_norm": 1.4279195070266724,
      "learning_rate": 1.1746990461541874e-05,
      "loss": 1.1624,
      "step": 2279
    },
    {
      "epoch": 0.3205398566005905,
      "grad_norm": 1.370681643486023,
      "learning_rate": 1.1856548984512517e-05,
      "loss": 1.2225,
      "step": 2280
    },
    {
      "epoch": 0.3206804442569942,
      "grad_norm": 1.769364833831787,
      "learning_rate": 1.1966583852658374e-05,
      "loss": 1.0911,
      "step": 2281
    },
    {
      "epoch": 0.320821031913398,
      "grad_norm": 1.6444621086120605,
      "learning_rate": 1.2077094471328655e-05,
      "loss": 1.1902,
      "step": 2282
    },
    {
      "epoch": 0.3209616195698018,
      "grad_norm": 1.3369948863983154,
      "learning_rate": 1.2188080243301403e-05,
      "loss": 0.9263,
      "step": 2283
    },
    {
      "epoch": 0.3211022072262055,
      "grad_norm": 1.5498321056365967,
      "learning_rate": 1.2299540568786916e-05,
      "loss": 1.0339,
      "step": 2284
    },
    {
      "epoch": 0.3212427948826093,
      "grad_norm": 1.7421201467514038,
      "learning_rate": 1.2411474845430871e-05,
      "loss": 1.0401,
      "step": 2285
    },
    {
      "epoch": 0.32138338253901305,
      "grad_norm": 1.7879258394241333,
      "learning_rate": 1.2523882468317616e-05,
      "loss": 0.9899,
      "step": 2286
    },
    {
      "epoch": 0.32152397019541684,
      "grad_norm": 1.4350789785385132,
      "learning_rate": 1.2636762829973414e-05,
      "loss": 1.0442,
      "step": 2287
    },
    {
      "epoch": 0.32166455785182063,
      "grad_norm": 1.5984934568405151,
      "learning_rate": 1.2750115320369837e-05,
      "loss": 1.088,
      "step": 2288
    },
    {
      "epoch": 0.32180514550822437,
      "grad_norm": 1.5120837688446045,
      "learning_rate": 1.2863939326926876e-05,
      "loss": 1.1737,
      "step": 2289
    },
    {
      "epoch": 0.32194573316462816,
      "grad_norm": 1.7485439777374268,
      "learning_rate": 1.297823423451645e-05,
      "loss": 1.115,
      "step": 2290
    },
    {
      "epoch": 0.3220863208210319,
      "grad_norm": 1.398285984992981,
      "learning_rate": 1.3092999425465536e-05,
      "loss": 1.1581,
      "step": 2291
    },
    {
      "epoch": 0.3222269084774357,
      "grad_norm": 1.6197527647018433,
      "learning_rate": 1.3208234279559673e-05,
      "loss": 1.1371,
      "step": 2292
    },
    {
      "epoch": 0.32236749613383947,
      "grad_norm": 1.4913413524627686,
      "learning_rate": 1.3323938174046202e-05,
      "loss": 1.112,
      "step": 2293
    },
    {
      "epoch": 0.3225080837902432,
      "grad_norm": 1.3870015144348145,
      "learning_rate": 1.3440110483637735e-05,
      "loss": 1.1318,
      "step": 2294
    },
    {
      "epoch": 0.322648671446647,
      "grad_norm": 1.5744941234588623,
      "learning_rate": 1.3556750580515376e-05,
      "loss": 0.9505,
      "step": 2295
    },
    {
      "epoch": 0.32278925910305073,
      "grad_norm": 1.3954246044158936,
      "learning_rate": 1.3673857834332315e-05,
      "loss": 0.9199,
      "step": 2296
    },
    {
      "epoch": 0.3229298467594545,
      "grad_norm": 1.6637446880340576,
      "learning_rate": 1.3791431612217043e-05,
      "loss": 1.2027,
      "step": 2297
    },
    {
      "epoch": 0.3230704344158583,
      "grad_norm": 1.768285870552063,
      "learning_rate": 1.3909471278776953e-05,
      "loss": 1.1354,
      "step": 2298
    },
    {
      "epoch": 0.32321102207226204,
      "grad_norm": 1.7372947931289673,
      "learning_rate": 1.4027976196101556e-05,
      "loss": 0.9705,
      "step": 2299
    },
    {
      "epoch": 0.32335160972866583,
      "grad_norm": 1.5063644647598267,
      "learning_rate": 1.4146945723766202e-05,
      "loss": 0.9833,
      "step": 2300
    },
    {
      "epoch": 0.32349219738506957,
      "grad_norm": 1.5225610733032227,
      "learning_rate": 1.4266379218835269e-05,
      "loss": 0.984,
      "step": 2301
    },
    {
      "epoch": 0.32363278504147336,
      "grad_norm": 1.6111737489700317,
      "learning_rate": 1.4386276035865786e-05,
      "loss": 1.0411,
      "step": 2302
    },
    {
      "epoch": 0.32377337269787715,
      "grad_norm": 2.144036054611206,
      "learning_rate": 1.4506635526910962e-05,
      "loss": 1.0549,
      "step": 2303
    },
    {
      "epoch": 0.3239139603542809,
      "grad_norm": 1.3386446237564087,
      "learning_rate": 1.462745704152354e-05,
      "loss": 1.1851,
      "step": 2304
    },
    {
      "epoch": 0.3240545480106847,
      "grad_norm": 1.3200876712799072,
      "learning_rate": 1.4748739926759481e-05,
      "loss": 1.0901,
      "step": 2305
    },
    {
      "epoch": 0.3241951356670884,
      "grad_norm": 1.4012330770492554,
      "learning_rate": 1.4870483527181278e-05,
      "loss": 1.1412,
      "step": 2306
    },
    {
      "epoch": 0.3243357233234922,
      "grad_norm": 1.6272919178009033,
      "learning_rate": 1.4992687184861753e-05,
      "loss": 1.0915,
      "step": 2307
    },
    {
      "epoch": 0.324476310979896,
      "grad_norm": 1.491696834564209,
      "learning_rate": 1.5115350239387394e-05,
      "loss": 1.1212,
      "step": 2308
    },
    {
      "epoch": 0.3246168986362997,
      "grad_norm": 1.5090833902359009,
      "learning_rate": 1.5238472027862093e-05,
      "loss": 1.2478,
      "step": 2309
    },
    {
      "epoch": 0.3247574862927035,
      "grad_norm": 1.4606928825378418,
      "learning_rate": 1.536205188491058e-05,
      "loss": 0.9366,
      "step": 2310
    },
    {
      "epoch": 0.32489807394910725,
      "grad_norm": 1.5856558084487915,
      "learning_rate": 1.5486089142682135e-05,
      "loss": 1.0018,
      "step": 2311
    },
    {
      "epoch": 0.32503866160551104,
      "grad_norm": 1.6098039150238037,
      "learning_rate": 1.5610583130854118e-05,
      "loss": 1.0025,
      "step": 2312
    },
    {
      "epoch": 0.3251792492619148,
      "grad_norm": 1.4660744667053223,
      "learning_rate": 1.5735533176635666e-05,
      "loss": 0.9527,
      "step": 2313
    },
    {
      "epoch": 0.32531983691831856,
      "grad_norm": 1.7182506322860718,
      "learning_rate": 1.5860938604771234e-05,
      "loss": 0.8222,
      "step": 2314
    },
    {
      "epoch": 0.32546042457472235,
      "grad_norm": 1.6362828016281128,
      "learning_rate": 1.5986798737544363e-05,
      "loss": 1.0384,
      "step": 2315
    },
    {
      "epoch": 0.3256010122311261,
      "grad_norm": 1.2885829210281372,
      "learning_rate": 1.61131128947812e-05,
      "loss": 1.1797,
      "step": 2316
    },
    {
      "epoch": 0.3257415998875299,
      "grad_norm": 1.4275461435317993,
      "learning_rate": 1.6239880393854344e-05,
      "loss": 1.276,
      "step": 2317
    },
    {
      "epoch": 0.32588218754393367,
      "grad_norm": 1.4545388221740723,
      "learning_rate": 1.6367100549686332e-05,
      "loss": 1.1703,
      "step": 2318
    },
    {
      "epoch": 0.3260227752003374,
      "grad_norm": 1.463460087776184,
      "learning_rate": 1.6494772674753568e-05,
      "loss": 1.0728,
      "step": 2319
    },
    {
      "epoch": 0.3261633628567412,
      "grad_norm": 1.6955108642578125,
      "learning_rate": 1.6622896079089813e-05,
      "loss": 0.9828,
      "step": 2320
    },
    {
      "epoch": 0.3263039505131449,
      "grad_norm": 1.5239710807800293,
      "learning_rate": 1.6751470070290133e-05,
      "loss": 0.9955,
      "step": 2321
    },
    {
      "epoch": 0.3264445381695487,
      "grad_norm": 1.2959400415420532,
      "learning_rate": 1.688049395351441e-05,
      "loss": 1.3148,
      "step": 2322
    },
    {
      "epoch": 0.3265851258259525,
      "grad_norm": 1.4270635843276978,
      "learning_rate": 1.7009967031491338e-05,
      "loss": 1.1632,
      "step": 2323
    },
    {
      "epoch": 0.32672571348235624,
      "grad_norm": 1.5348272323608398,
      "learning_rate": 1.7139888604521982e-05,
      "loss": 1.0418,
      "step": 2324
    },
    {
      "epoch": 0.32686630113876003,
      "grad_norm": 1.352413535118103,
      "learning_rate": 1.7270257970483695e-05,
      "loss": 1.3059,
      "step": 2325
    },
    {
      "epoch": 0.32700688879516376,
      "grad_norm": 1.5301002264022827,
      "learning_rate": 1.7401074424833807e-05,
      "loss": 0.9793,
      "step": 2326
    },
    {
      "epoch": 0.32714747645156755,
      "grad_norm": 1.4340777397155762,
      "learning_rate": 1.7532337260613596e-05,
      "loss": 1.1446,
      "step": 2327
    },
    {
      "epoch": 0.32728806410797134,
      "grad_norm": 1.5998631715774536,
      "learning_rate": 1.7664045768451887e-05,
      "loss": 1.1759,
      "step": 2328
    },
    {
      "epoch": 0.3274286517643751,
      "grad_norm": 1.340337872505188,
      "learning_rate": 1.779619923656912e-05,
      "loss": 1.0637,
      "step": 2329
    },
    {
      "epoch": 0.32756923942077887,
      "grad_norm": 1.6466315984725952,
      "learning_rate": 1.7928796950780957e-05,
      "loss": 1.0186,
      "step": 2330
    },
    {
      "epoch": 0.3277098270771826,
      "grad_norm": 1.523271918296814,
      "learning_rate": 1.8061838194502367e-05,
      "loss": 1.0571,
      "step": 2331
    },
    {
      "epoch": 0.3278504147335864,
      "grad_norm": 1.4927911758422852,
      "learning_rate": 1.8195322248751312e-05,
      "loss": 0.983,
      "step": 2332
    },
    {
      "epoch": 0.3279910023899902,
      "grad_norm": 1.4217561483383179,
      "learning_rate": 1.8329248392152797e-05,
      "loss": 1.0647,
      "step": 2333
    },
    {
      "epoch": 0.3281315900463939,
      "grad_norm": 1.316707730293274,
      "learning_rate": 1.846361590094261e-05,
      "loss": 1.1459,
      "step": 2334
    },
    {
      "epoch": 0.3282721777027977,
      "grad_norm": 1.3970845937728882,
      "learning_rate": 1.8598424048971386e-05,
      "loss": 1.1635,
      "step": 2335
    },
    {
      "epoch": 0.32841276535920144,
      "grad_norm": 1.411522388458252,
      "learning_rate": 1.8733672107708354e-05,
      "loss": 1.0915,
      "step": 2336
    },
    {
      "epoch": 0.32855335301560523,
      "grad_norm": 1.575118899345398,
      "learning_rate": 1.8869359346245517e-05,
      "loss": 1.1043,
      "step": 2337
    },
    {
      "epoch": 0.328693940672009,
      "grad_norm": 1.6396105289459229,
      "learning_rate": 1.9005485031301308e-05,
      "loss": 1.0091,
      "step": 2338
    },
    {
      "epoch": 0.32883452832841276,
      "grad_norm": 1.5332680940628052,
      "learning_rate": 1.9142048427224823e-05,
      "loss": 1.0859,
      "step": 2339
    },
    {
      "epoch": 0.32897511598481655,
      "grad_norm": 1.64375638961792,
      "learning_rate": 1.9279048795999633e-05,
      "loss": 1.1704,
      "step": 2340
    },
    {
      "epoch": 0.3291157036412203,
      "grad_norm": 1.4292243719100952,
      "learning_rate": 1.9416485397247785e-05,
      "loss": 1.2622,
      "step": 2341
    },
    {
      "epoch": 0.32925629129762407,
      "grad_norm": 1.9127782583236694,
      "learning_rate": 1.955435748823392e-05,
      "loss": 0.9249,
      "step": 2342
    },
    {
      "epoch": 0.32939687895402786,
      "grad_norm": 1.4954016208648682,
      "learning_rate": 1.9692664323869136e-05,
      "loss": 0.8843,
      "step": 2343
    },
    {
      "epoch": 0.3295374666104316,
      "grad_norm": 1.6087110042572021,
      "learning_rate": 1.98314051567151e-05,
      "loss": 1.059,
      "step": 2344
    },
    {
      "epoch": 0.3296780542668354,
      "grad_norm": 1.4285305738449097,
      "learning_rate": 1.9970579236988052e-05,
      "loss": 1.0439,
      "step": 2345
    },
    {
      "epoch": 0.3298186419232391,
      "grad_norm": 1.5899680852890015,
      "learning_rate": 2.0110185812562933e-05,
      "loss": 1.114,
      "step": 2346
    },
    {
      "epoch": 0.3299592295796429,
      "grad_norm": 1.2223150730133057,
      "learning_rate": 2.0250224128977314e-05,
      "loss": 1.361,
      "step": 2347
    },
    {
      "epoch": 0.3300998172360467,
      "grad_norm": 1.495377540588379,
      "learning_rate": 2.0390693429435637e-05,
      "loss": 1.0271,
      "step": 2348
    },
    {
      "epoch": 0.33024040489245043,
      "grad_norm": 1.550046443939209,
      "learning_rate": 2.0531592954813116e-05,
      "loss": 0.938,
      "step": 2349
    },
    {
      "epoch": 0.3303809925488542,
      "grad_norm": 1.3953087329864502,
      "learning_rate": 2.067292194366005e-05,
      "loss": 1.2211,
      "step": 2350
    },
    {
      "epoch": 0.33052158020525796,
      "grad_norm": 1.355339765548706,
      "learning_rate": 2.0814679632205736e-05,
      "loss": 1.1583,
      "step": 2351
    },
    {
      "epoch": 0.33066216786166175,
      "grad_norm": 1.518764853477478,
      "learning_rate": 2.0956865254362768e-05,
      "loss": 1.1266,
      "step": 2352
    },
    {
      "epoch": 0.33080275551806554,
      "grad_norm": 1.4083670377731323,
      "learning_rate": 2.1099478041731023e-05,
      "loss": 1.2368,
      "step": 2353
    },
    {
      "epoch": 0.3309433431744693,
      "grad_norm": 1.5342488288879395,
      "learning_rate": 2.1242517223601965e-05,
      "loss": 1.05,
      "step": 2354
    },
    {
      "epoch": 0.33108393083087306,
      "grad_norm": 1.6187372207641602,
      "learning_rate": 2.138598202696267e-05,
      "loss": 1.0528,
      "step": 2355
    },
    {
      "epoch": 0.3312245184872768,
      "grad_norm": 1.4941320419311523,
      "learning_rate": 2.1529871676500135e-05,
      "loss": 1.1158,
      "step": 2356
    },
    {
      "epoch": 0.3313651061436806,
      "grad_norm": 1.9820646047592163,
      "learning_rate": 2.1674185394605307e-05,
      "loss": 1.1017,
      "step": 2357
    },
    {
      "epoch": 0.3315056938000844,
      "grad_norm": 1.4911271333694458,
      "learning_rate": 2.1818922401377518e-05,
      "loss": 1.2104,
      "step": 2358
    },
    {
      "epoch": 0.3316462814564881,
      "grad_norm": 1.4940606355667114,
      "learning_rate": 2.19640819146284e-05,
      "loss": 1.0405,
      "step": 2359
    },
    {
      "epoch": 0.3317868691128919,
      "grad_norm": 1.719232201576233,
      "learning_rate": 2.210966314988643e-05,
      "loss": 0.9645,
      "step": 2360
    },
    {
      "epoch": 0.33192745676929564,
      "grad_norm": 1.2886637449264526,
      "learning_rate": 2.2255665320400877e-05,
      "loss": 0.9683,
      "step": 2361
    },
    {
      "epoch": 0.3320680444256994,
      "grad_norm": 1.5149534940719604,
      "learning_rate": 2.2402087637146295e-05,
      "loss": 0.9823,
      "step": 2362
    },
    {
      "epoch": 0.3322086320821032,
      "grad_norm": 1.6270469427108765,
      "learning_rate": 2.254892930882664e-05,
      "loss": 1.103,
      "step": 2363
    },
    {
      "epoch": 0.33234921973850695,
      "grad_norm": 1.7192723751068115,
      "learning_rate": 2.2696189541879565e-05,
      "loss": 1.1886,
      "step": 2364
    },
    {
      "epoch": 0.33248980739491074,
      "grad_norm": 1.5245076417922974,
      "learning_rate": 2.2843867540480736e-05,
      "loss": 0.9797,
      "step": 2365
    },
    {
      "epoch": 0.3326303950513145,
      "grad_norm": 1.47898268699646,
      "learning_rate": 2.2991962506548205e-05,
      "loss": 1.1021,
      "step": 2366
    },
    {
      "epoch": 0.33277098270771827,
      "grad_norm": 1.6667265892028809,
      "learning_rate": 2.314047363974653e-05,
      "loss": 1.2312,
      "step": 2367
    },
    {
      "epoch": 0.33291157036412206,
      "grad_norm": 1.6212310791015625,
      "learning_rate": 2.3289400137491334e-05,
      "loss": 1.0239,
      "step": 2368
    },
    {
      "epoch": 0.3330521580205258,
      "grad_norm": 1.4724760055541992,
      "learning_rate": 2.3438741194953407e-05,
      "loss": 0.9975,
      "step": 2369
    },
    {
      "epoch": 0.3331927456769296,
      "grad_norm": 1.474480390548706,
      "learning_rate": 2.358849600506331e-05,
      "loss": 1.0734,
      "step": 2370
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 1.5894185304641724,
      "learning_rate": 2.3738663758515445e-05,
      "loss": 1.0154,
      "step": 2371
    },
    {
      "epoch": 0.3334739209897371,
      "grad_norm": 1.4743986129760742,
      "learning_rate": 2.388924364377273e-05,
      "loss": 1.1581,
      "step": 2372
    },
    {
      "epoch": 0.3336145086461409,
      "grad_norm": 1.7137017250061035,
      "learning_rate": 2.404023484707072e-05,
      "loss": 1.1426,
      "step": 2373
    },
    {
      "epoch": 0.33375509630254463,
      "grad_norm": 1.2868447303771973,
      "learning_rate": 2.4191636552422226e-05,
      "loss": 1.1152,
      "step": 2374
    },
    {
      "epoch": 0.3338956839589484,
      "grad_norm": 1.3844937086105347,
      "learning_rate": 2.4343447941621523e-05,
      "loss": 1.0863,
      "step": 2375
    },
    {
      "epoch": 0.33403627161535215,
      "grad_norm": 1.459953784942627,
      "learning_rate": 2.4495668194248967e-05,
      "loss": 1.1047,
      "step": 2376
    },
    {
      "epoch": 0.33417685927175594,
      "grad_norm": 1.5539829730987549,
      "learning_rate": 2.464829648767525e-05,
      "loss": 1.1633,
      "step": 2377
    },
    {
      "epoch": 0.33431744692815973,
      "grad_norm": 1.3612070083618164,
      "learning_rate": 2.480133199706599e-05,
      "loss": 1.1886,
      "step": 2378
    },
    {
      "epoch": 0.33445803458456347,
      "grad_norm": 1.5586650371551514,
      "learning_rate": 2.4954773895386107e-05,
      "loss": 1.0055,
      "step": 2379
    },
    {
      "epoch": 0.33459862224096726,
      "grad_norm": 1.541959285736084,
      "learning_rate": 2.510862135340426e-05,
      "loss": 1.0048,
      "step": 2380
    },
    {
      "epoch": 0.334739209897371,
      "grad_norm": 1.6173774003982544,
      "learning_rate": 2.5262873539697486e-05,
      "loss": 1.0387,
      "step": 2381
    },
    {
      "epoch": 0.3348797975537748,
      "grad_norm": 1.6522225141525269,
      "learning_rate": 2.54175296206555e-05,
      "loss": 1.1124,
      "step": 2382
    },
    {
      "epoch": 0.3350203852101786,
      "grad_norm": 1.5080829858779907,
      "learning_rate": 2.5572588760485316e-05,
      "loss": 1.0256,
      "step": 2383
    },
    {
      "epoch": 0.3351609728665823,
      "grad_norm": 1.3646751642227173,
      "learning_rate": 2.572805012121572e-05,
      "loss": 1.0433,
      "step": 2384
    },
    {
      "epoch": 0.3353015605229861,
      "grad_norm": 1.4882711172103882,
      "learning_rate": 2.5883912862701885e-05,
      "loss": 1.0007,
      "step": 2385
    },
    {
      "epoch": 0.33544214817938983,
      "grad_norm": 1.7795170545578003,
      "learning_rate": 2.6040176142629723e-05,
      "loss": 1.0095,
      "step": 2386
    },
    {
      "epoch": 0.3355827358357936,
      "grad_norm": 1.59416663646698,
      "learning_rate": 2.6196839116520678e-05,
      "loss": 1.1357,
      "step": 2387
    },
    {
      "epoch": 0.3357233234921974,
      "grad_norm": 1.5449644327163696,
      "learning_rate": 2.6353900937736043e-05,
      "loss": 1.1156,
      "step": 2388
    },
    {
      "epoch": 0.33586391114860115,
      "grad_norm": 1.4335663318634033,
      "learning_rate": 2.6511360757481797e-05,
      "loss": 1.1743,
      "step": 2389
    },
    {
      "epoch": 0.33600449880500494,
      "grad_norm": 1.673878788948059,
      "learning_rate": 2.6669217724812912e-05,
      "loss": 1.0483,
      "step": 2390
    },
    {
      "epoch": 0.33614508646140867,
      "grad_norm": 2.0321521759033203,
      "learning_rate": 2.6827470986638215e-05,
      "loss": 1.152,
      "step": 2391
    },
    {
      "epoch": 0.33628567411781246,
      "grad_norm": 1.4057587385177612,
      "learning_rate": 2.6986119687724777e-05,
      "loss": 1.0251,
      "step": 2392
    },
    {
      "epoch": 0.33642626177421625,
      "grad_norm": 1.562355637550354,
      "learning_rate": 2.714516297070273e-05,
      "loss": 1.0936,
      "step": 2393
    },
    {
      "epoch": 0.33656684943062,
      "grad_norm": 1.4584121704101562,
      "learning_rate": 2.73045999760697e-05,
      "loss": 1.0935,
      "step": 2394
    },
    {
      "epoch": 0.3367074370870238,
      "grad_norm": 2.0481815338134766,
      "learning_rate": 2.746442984219567e-05,
      "loss": 1.2407,
      "step": 2395
    },
    {
      "epoch": 0.3368480247434275,
      "grad_norm": 1.4508947134017944,
      "learning_rate": 2.7624651705327397e-05,
      "loss": 1.123,
      "step": 2396
    },
    {
      "epoch": 0.3369886123998313,
      "grad_norm": 1.3487021923065186,
      "learning_rate": 2.7785264699593362e-05,
      "loss": 1.0399,
      "step": 2397
    },
    {
      "epoch": 0.3371292000562351,
      "grad_norm": 1.5952852964401245,
      "learning_rate": 2.794626795700811e-05,
      "loss": 0.9817,
      "step": 2398
    },
    {
      "epoch": 0.3372697877126388,
      "grad_norm": 1.5393096208572388,
      "learning_rate": 2.8107660607477314e-05,
      "loss": 1.0895,
      "step": 2399
    },
    {
      "epoch": 0.3374103753690426,
      "grad_norm": 1.5500117540359497,
      "learning_rate": 2.826944177880212e-05,
      "loss": 1.2071,
      "step": 2400
    },
    {
      "epoch": 0.33755096302544635,
      "grad_norm": 1.564423680305481,
      "learning_rate": 2.8431610596684165e-05,
      "loss": 1.0656,
      "step": 2401
    },
    {
      "epoch": 0.33769155068185014,
      "grad_norm": 1.3665696382522583,
      "learning_rate": 2.8594166184730064e-05,
      "loss": 1.1883,
      "step": 2402
    },
    {
      "epoch": 0.33783213833825393,
      "grad_norm": 1.4029409885406494,
      "learning_rate": 2.8757107664456305e-05,
      "loss": 1.1318,
      "step": 2403
    },
    {
      "epoch": 0.33797272599465766,
      "grad_norm": 1.503305196762085,
      "learning_rate": 2.8920434155293873e-05,
      "loss": 1.0945,
      "step": 2404
    },
    {
      "epoch": 0.33811331365106145,
      "grad_norm": 1.417162299156189,
      "learning_rate": 2.908414477459319e-05,
      "loss": 1.3645,
      "step": 2405
    },
    {
      "epoch": 0.3382539013074652,
      "grad_norm": 1.540455937385559,
      "learning_rate": 2.9248238637628645e-05,
      "loss": 1.092,
      "step": 2406
    },
    {
      "epoch": 0.338394488963869,
      "grad_norm": 1.7909809350967407,
      "learning_rate": 2.941271485760356e-05,
      "loss": 1.2518,
      "step": 2407
    },
    {
      "epoch": 0.3385350766202727,
      "grad_norm": 1.4971238374710083,
      "learning_rate": 2.957757254565502e-05,
      "loss": 1.0308,
      "step": 2408
    },
    {
      "epoch": 0.3386756642766765,
      "grad_norm": 1.826697587966919,
      "learning_rate": 2.974281081085831e-05,
      "loss": 1.1983,
      "step": 2409
    },
    {
      "epoch": 0.3388162519330803,
      "grad_norm": 1.530930757522583,
      "learning_rate": 2.9908428760232267e-05,
      "loss": 1.073,
      "step": 2410
    },
    {
      "epoch": 0.338956839589484,
      "grad_norm": 1.4239702224731445,
      "learning_rate": 3.0074425498743695e-05,
      "loss": 1.14,
      "step": 2411
    },
    {
      "epoch": 0.3390974272458878,
      "grad_norm": 1.4549081325531006,
      "learning_rate": 3.0240800129312485e-05,
      "loss": 0.9462,
      "step": 2412
    },
    {
      "epoch": 0.33923801490229155,
      "grad_norm": 1.49067223072052,
      "learning_rate": 3.0407551752816076e-05,
      "loss": 1.0657,
      "step": 2413
    },
    {
      "epoch": 0.33937860255869534,
      "grad_norm": 1.874159574508667,
      "learning_rate": 3.057467946809477e-05,
      "loss": 0.9128,
      "step": 2414
    },
    {
      "epoch": 0.33951919021509913,
      "grad_norm": 1.6167099475860596,
      "learning_rate": 3.074218237195633e-05,
      "loss": 0.9067,
      "step": 2415
    },
    {
      "epoch": 0.33965977787150287,
      "grad_norm": 1.4912806749343872,
      "learning_rate": 3.091005955918099e-05,
      "loss": 1.2298,
      "step": 2416
    },
    {
      "epoch": 0.33980036552790666,
      "grad_norm": 1.5594364404678345,
      "learning_rate": 3.107831012252609e-05,
      "loss": 1.0199,
      "step": 2417
    },
    {
      "epoch": 0.3399409531843104,
      "grad_norm": 1.7069963216781616,
      "learning_rate": 3.124693315273133e-05,
      "loss": 1.0515,
      "step": 2418
    },
    {
      "epoch": 0.3400815408407142,
      "grad_norm": 1.4183577299118042,
      "learning_rate": 3.1415927738523574e-05,
      "loss": 1.05,
      "step": 2419
    },
    {
      "epoch": 0.34022212849711797,
      "grad_norm": 1.4474772214889526,
      "learning_rate": 3.1585292966621506e-05,
      "loss": 1.0321,
      "step": 2420
    },
    {
      "epoch": 0.3403627161535217,
      "grad_norm": 1.5113989114761353,
      "learning_rate": 3.1755027921741e-05,
      "loss": 1.0553,
      "step": 2421
    },
    {
      "epoch": 0.3405033038099255,
      "grad_norm": 1.7249720096588135,
      "learning_rate": 3.1925131686599684e-05,
      "loss": 1.1238,
      "step": 2422
    },
    {
      "epoch": 0.34064389146632923,
      "grad_norm": 1.8996272087097168,
      "learning_rate": 3.209560334192228e-05,
      "loss": 1.177,
      "step": 2423
    },
    {
      "epoch": 0.340784479122733,
      "grad_norm": 1.3720005750656128,
      "learning_rate": 3.2266441966445035e-05,
      "loss": 1.1131,
      "step": 2424
    },
    {
      "epoch": 0.3409250667791368,
      "grad_norm": 1.4634134769439697,
      "learning_rate": 3.2437646636921304e-05,
      "loss": 1.1161,
      "step": 2425
    },
    {
      "epoch": 0.34106565443554054,
      "grad_norm": 1.4345488548278809,
      "learning_rate": 3.260921642812613e-05,
      "loss": 0.9998,
      "step": 2426
    },
    {
      "epoch": 0.34120624209194433,
      "grad_norm": 1.3437786102294922,
      "learning_rate": 3.278115041286147e-05,
      "loss": 1.1513,
      "step": 2427
    },
    {
      "epoch": 0.34134682974834807,
      "grad_norm": 1.5653249025344849,
      "learning_rate": 3.295344766196088e-05,
      "loss": 1.0626,
      "step": 2428
    },
    {
      "epoch": 0.34148741740475186,
      "grad_norm": 1.4749289751052856,
      "learning_rate": 3.312610724429499e-05,
      "loss": 1.0383,
      "step": 2429
    },
    {
      "epoch": 0.34162800506115565,
      "grad_norm": 1.614280104637146,
      "learning_rate": 3.3299128226776223e-05,
      "loss": 1.0549,
      "step": 2430
    },
    {
      "epoch": 0.3417685927175594,
      "grad_norm": 1.4573554992675781,
      "learning_rate": 3.347250967436397e-05,
      "loss": 0.9804,
      "step": 2431
    },
    {
      "epoch": 0.3419091803739632,
      "grad_norm": 1.6599959135055542,
      "learning_rate": 3.3646250650069435e-05,
      "loss": 1.1836,
      "step": 2432
    },
    {
      "epoch": 0.3420497680303669,
      "grad_norm": 1.562712550163269,
      "learning_rate": 3.3820350214961026e-05,
      "loss": 1.0066,
      "step": 2433
    },
    {
      "epoch": 0.3421903556867707,
      "grad_norm": 1.5676110982894897,
      "learning_rate": 3.399480742816923e-05,
      "loss": 1.0804,
      "step": 2434
    },
    {
      "epoch": 0.3423309433431745,
      "grad_norm": 1.3571357727050781,
      "learning_rate": 3.416962134689174e-05,
      "loss": 1.1824,
      "step": 2435
    },
    {
      "epoch": 0.3424715309995782,
      "grad_norm": 1.715049386024475,
      "learning_rate": 3.4344791026398404e-05,
      "loss": 1.2131,
      "step": 2436
    },
    {
      "epoch": 0.342612118655982,
      "grad_norm": 1.4518100023269653,
      "learning_rate": 3.4520315520036627e-05,
      "loss": 1.2068,
      "step": 2437
    },
    {
      "epoch": 0.34275270631238575,
      "grad_norm": 1.6421300172805786,
      "learning_rate": 3.46961938792363e-05,
      "loss": 1.0397,
      "step": 2438
    },
    {
      "epoch": 0.34289329396878954,
      "grad_norm": 1.3055039644241333,
      "learning_rate": 3.4872425153514886e-05,
      "loss": 1.3051,
      "step": 2439
    },
    {
      "epoch": 0.3430338816251933,
      "grad_norm": 1.4621165990829468,
      "learning_rate": 3.504900839048266e-05,
      "loss": 1.131,
      "step": 2440
    },
    {
      "epoch": 0.34317446928159706,
      "grad_norm": 1.4566413164138794,
      "learning_rate": 3.5225942635847765e-05,
      "loss": 1.1083,
      "step": 2441
    },
    {
      "epoch": 0.34331505693800085,
      "grad_norm": 1.5678629875183105,
      "learning_rate": 3.5403226933421585e-05,
      "loss": 1.1331,
      "step": 2442
    },
    {
      "epoch": 0.3434556445944046,
      "grad_norm": 1.6727914810180664,
      "learning_rate": 3.5580860325123575e-05,
      "loss": 1.1419,
      "step": 2443
    },
    {
      "epoch": 0.3435962322508084,
      "grad_norm": 1.6576087474822998,
      "learning_rate": 3.57588418509867e-05,
      "loss": 0.9238,
      "step": 2444
    },
    {
      "epoch": 0.34373681990721217,
      "grad_norm": 1.5231664180755615,
      "learning_rate": 3.5937170549162444e-05,
      "loss": 1.0182,
      "step": 2445
    },
    {
      "epoch": 0.3438774075636159,
      "grad_norm": 1.315199375152588,
      "learning_rate": 3.611584545592629e-05,
      "loss": 1.203,
      "step": 2446
    },
    {
      "epoch": 0.3440179952200197,
      "grad_norm": 1.7076830863952637,
      "learning_rate": 3.629486560568258e-05,
      "loss": 0.8852,
      "step": 2447
    },
    {
      "epoch": 0.3441585828764234,
      "grad_norm": 1.4460480213165283,
      "learning_rate": 3.647423003096992e-05,
      "loss": 1.0391,
      "step": 2448
    },
    {
      "epoch": 0.3442991705328272,
      "grad_norm": 1.7489732503890991,
      "learning_rate": 3.665393776246634e-05,
      "loss": 0.9945,
      "step": 2449
    },
    {
      "epoch": 0.344439758189231,
      "grad_norm": 1.4256293773651123,
      "learning_rate": 3.683398782899474e-05,
      "loss": 0.9465,
      "step": 2450
    },
    {
      "epoch": 0.34458034584563474,
      "grad_norm": 1.4857397079467773,
      "learning_rate": 3.70143792575279e-05,
      "loss": 1.088,
      "step": 2451
    },
    {
      "epoch": 0.34472093350203853,
      "grad_norm": 1.4502534866333008,
      "learning_rate": 3.719511107319354e-05,
      "loss": 1.2101,
      "step": 2452
    },
    {
      "epoch": 0.34486152115844226,
      "grad_norm": 1.4294525384902954,
      "learning_rate": 3.73761822992803e-05,
      "loss": 1.0677,
      "step": 2453
    },
    {
      "epoch": 0.34500210881484605,
      "grad_norm": 1.2757153511047363,
      "learning_rate": 3.7557591957242244e-05,
      "loss": 1.0271,
      "step": 2454
    },
    {
      "epoch": 0.34514269647124984,
      "grad_norm": 1.4106216430664062,
      "learning_rate": 3.773933906670479e-05,
      "loss": 1.2325,
      "step": 2455
    },
    {
      "epoch": 0.3452832841276536,
      "grad_norm": 1.3381438255310059,
      "learning_rate": 3.792142264546926e-05,
      "loss": 1.1584,
      "step": 2456
    },
    {
      "epoch": 0.34542387178405737,
      "grad_norm": 1.6820634603500366,
      "learning_rate": 3.8103841709519064e-05,
      "loss": 1.1718,
      "step": 2457
    },
    {
      "epoch": 0.3455644594404611,
      "grad_norm": 1.962124228477478,
      "learning_rate": 3.828659527302434e-05,
      "loss": 1.0573,
      "step": 2458
    },
    {
      "epoch": 0.3457050470968649,
      "grad_norm": 1.5643898248672485,
      "learning_rate": 3.846968234834759e-05,
      "loss": 1.1439,
      "step": 2459
    },
    {
      "epoch": 0.3458456347532687,
      "grad_norm": 1.3456926345825195,
      "learning_rate": 3.8653101946048856e-05,
      "loss": 1.0125,
      "step": 2460
    },
    {
      "epoch": 0.3459862224096724,
      "grad_norm": 1.4497411251068115,
      "learning_rate": 3.88368530748914e-05,
      "loss": 1.0025,
      "step": 2461
    },
    {
      "epoch": 0.3461268100660762,
      "grad_norm": 1.6087408065795898,
      "learning_rate": 3.9020934741846584e-05,
      "loss": 0.9139,
      "step": 2462
    },
    {
      "epoch": 0.34626739772247994,
      "grad_norm": 1.6611628532409668,
      "learning_rate": 3.9205345952099556e-05,
      "loss": 0.9596,
      "step": 2463
    },
    {
      "epoch": 0.34640798537888373,
      "grad_norm": 1.97322416305542,
      "learning_rate": 3.9390085709054505e-05,
      "loss": 1.0712,
      "step": 2464
    },
    {
      "epoch": 0.3465485730352875,
      "grad_norm": 1.2985522747039795,
      "learning_rate": 3.957515301434025e-05,
      "loss": 1.0423,
      "step": 2465
    },
    {
      "epoch": 0.34668916069169126,
      "grad_norm": 1.4826892614364624,
      "learning_rate": 3.976054686781527e-05,
      "loss": 1.1902,
      "step": 2466
    },
    {
      "epoch": 0.34682974834809505,
      "grad_norm": 1.5473721027374268,
      "learning_rate": 3.9946266267573426e-05,
      "loss": 1.044,
      "step": 2467
    },
    {
      "epoch": 0.3469703360044988,
      "grad_norm": 1.716592788696289,
      "learning_rate": 4.013231020994913e-05,
      "loss": 1.2394,
      "step": 2468
    },
    {
      "epoch": 0.34711092366090257,
      "grad_norm": 1.406256914138794,
      "learning_rate": 4.031867768952314e-05,
      "loss": 1.0849,
      "step": 2469
    },
    {
      "epoch": 0.34725151131730636,
      "grad_norm": 1.6054375171661377,
      "learning_rate": 4.050536769912763e-05,
      "loss": 1.1029,
      "step": 2470
    },
    {
      "epoch": 0.3473920989737101,
      "grad_norm": 1.3714641332626343,
      "learning_rate": 4.06923792298515e-05,
      "loss": 1.1757,
      "step": 2471
    },
    {
      "epoch": 0.3475326866301139,
      "grad_norm": 1.6408666372299194,
      "learning_rate": 4.0879711271046536e-05,
      "loss": 1.0641,
      "step": 2472
    },
    {
      "epoch": 0.3476732742865176,
      "grad_norm": 1.429000735282898,
      "learning_rate": 4.106736281033207e-05,
      "loss": 1.1391,
      "step": 2473
    },
    {
      "epoch": 0.3478138619429214,
      "grad_norm": 1.4571688175201416,
      "learning_rate": 4.1255332833601145e-05,
      "loss": 1.0126,
      "step": 2474
    },
    {
      "epoch": 0.3479544495993252,
      "grad_norm": 1.569838047027588,
      "learning_rate": 4.144362032502517e-05,
      "loss": 1.1348,
      "step": 2475
    },
    {
      "epoch": 0.34809503725572893,
      "grad_norm": 1.365267038345337,
      "learning_rate": 4.163222426706035e-05,
      "loss": 1.1204,
      "step": 2476
    },
    {
      "epoch": 0.3482356249121327,
      "grad_norm": 1.639432668685913,
      "learning_rate": 4.182114364045242e-05,
      "loss": 1.2048,
      "step": 2477
    },
    {
      "epoch": 0.34837621256853646,
      "grad_norm": 1.3537776470184326,
      "learning_rate": 4.20103774242428e-05,
      "loss": 1.0773,
      "step": 2478
    },
    {
      "epoch": 0.34851680022494025,
      "grad_norm": 1.1863987445831299,
      "learning_rate": 4.2199924595773235e-05,
      "loss": 1.0435,
      "step": 2479
    },
    {
      "epoch": 0.34865738788134404,
      "grad_norm": 1.5406614542007446,
      "learning_rate": 4.2389784130692415e-05,
      "loss": 1.2784,
      "step": 2480
    },
    {
      "epoch": 0.3487979755377478,
      "grad_norm": 1.5271189212799072,
      "learning_rate": 4.257995500296055e-05,
      "loss": 1.0205,
      "step": 2481
    },
    {
      "epoch": 0.34893856319415156,
      "grad_norm": 1.5101876258850098,
      "learning_rate": 4.277043618485572e-05,
      "loss": 1.0384,
      "step": 2482
    },
    {
      "epoch": 0.3490791508505553,
      "grad_norm": 1.7611315250396729,
      "learning_rate": 4.2961226646978506e-05,
      "loss": 0.9717,
      "step": 2483
    },
    {
      "epoch": 0.3492197385069591,
      "grad_norm": 1.319441318511963,
      "learning_rate": 4.315232535825856e-05,
      "loss": 1.1234,
      "step": 2484
    },
    {
      "epoch": 0.3493603261633629,
      "grad_norm": 1.6227055788040161,
      "learning_rate": 4.3343731285959375e-05,
      "loss": 1.077,
      "step": 2485
    },
    {
      "epoch": 0.3495009138197666,
      "grad_norm": 1.5977728366851807,
      "learning_rate": 4.35354433956845e-05,
      "loss": 1.1639,
      "step": 2486
    },
    {
      "epoch": 0.3496415014761704,
      "grad_norm": 1.3975216150283813,
      "learning_rate": 4.3727460651382345e-05,
      "loss": 1.2332,
      "step": 2487
    },
    {
      "epoch": 0.34978208913257414,
      "grad_norm": 1.2953253984451294,
      "learning_rate": 4.3919782015352726e-05,
      "loss": 1.1566,
      "step": 2488
    },
    {
      "epoch": 0.3499226767889779,
      "grad_norm": 1.6825931072235107,
      "learning_rate": 4.411240644825172e-05,
      "loss": 1.1227,
      "step": 2489
    },
    {
      "epoch": 0.3500632644453817,
      "grad_norm": 1.8936030864715576,
      "learning_rate": 4.430533290909764e-05,
      "loss": 1.0739,
      "step": 2490
    },
    {
      "epoch": 0.35020385210178545,
      "grad_norm": 1.578672170639038,
      "learning_rate": 4.4498560355276564e-05,
      "loss": 1.0969,
      "step": 2491
    },
    {
      "epoch": 0.35034443975818924,
      "grad_norm": 1.4653433561325073,
      "learning_rate": 4.4692087742547906e-05,
      "loss": 1.257,
      "step": 2492
    },
    {
      "epoch": 0.350485027414593,
      "grad_norm": 1.4573891162872314,
      "learning_rate": 4.488591402505038e-05,
      "loss": 1.1871,
      "step": 2493
    },
    {
      "epoch": 0.35062561507099677,
      "grad_norm": 1.4794780015945435,
      "learning_rate": 4.508003815530717e-05,
      "loss": 1.0954,
      "step": 2494
    },
    {
      "epoch": 0.35076620272740056,
      "grad_norm": 1.570761799812317,
      "learning_rate": 4.52744590842319e-05,
      "loss": 1.107,
      "step": 2495
    },
    {
      "epoch": 0.3509067903838043,
      "grad_norm": 1.5772807598114014,
      "learning_rate": 4.546917576113418e-05,
      "loss": 1.1497,
      "step": 2496
    },
    {
      "epoch": 0.3510473780402081,
      "grad_norm": 1.4165607690811157,
      "learning_rate": 4.566418713372561e-05,
      "loss": 1.0598,
      "step": 2497
    },
    {
      "epoch": 0.3511879656966118,
      "grad_norm": 1.5616035461425781,
      "learning_rate": 4.5859492148124705e-05,
      "loss": 1.2319,
      "step": 2498
    },
    {
      "epoch": 0.3513285533530156,
      "grad_norm": 1.5287199020385742,
      "learning_rate": 4.605508974886356e-05,
      "loss": 1.1435,
      "step": 2499
    },
    {
      "epoch": 0.3514691410094194,
      "grad_norm": 1.4001505374908447,
      "learning_rate": 4.625097887889274e-05,
      "loss": 1.0983,
      "step": 2500
    },
    {
      "epoch": 0.3514691410094194,
      "eval_loss": 1.1457109451293945,
      "eval_runtime": 771.3387,
      "eval_samples_per_second": 16.395,
      "eval_steps_per_second": 8.197,
      "step": 2500
    },
    {
      "epoch": 0.35160972866582313,
      "grad_norm": 1.4507442712783813,
      "learning_rate": 4.644715847958767e-05,
      "loss": 1.2703,
      "step": 2501
    },
    {
      "epoch": 0.3517503163222269,
      "grad_norm": 1.6655983924865723,
      "learning_rate": 4.664362749075352e-05,
      "loss": 0.9379,
      "step": 2502
    },
    {
      "epoch": 0.35189090397863065,
      "grad_norm": 1.8190733194351196,
      "learning_rate": 4.684038485063191e-05,
      "loss": 1.0496,
      "step": 2503
    },
    {
      "epoch": 0.35203149163503444,
      "grad_norm": 1.385292649269104,
      "learning_rate": 4.7037429495905824e-05,
      "loss": 1.1269,
      "step": 2504
    },
    {
      "epoch": 0.35217207929143823,
      "grad_norm": 1.763066053390503,
      "learning_rate": 4.7234760361706075e-05,
      "loss": 1.0584,
      "step": 2505
    },
    {
      "epoch": 0.35231266694784197,
      "grad_norm": 1.4243578910827637,
      "learning_rate": 4.7432376381616116e-05,
      "loss": 1.1571,
      "step": 2506
    },
    {
      "epoch": 0.35245325460424576,
      "grad_norm": 1.418994426727295,
      "learning_rate": 4.7630276487678946e-05,
      "loss": 1.0781,
      "step": 2507
    },
    {
      "epoch": 0.3525938422606495,
      "grad_norm": 1.7240078449249268,
      "learning_rate": 4.7828459610401885e-05,
      "loss": 1.0707,
      "step": 2508
    },
    {
      "epoch": 0.3527344299170533,
      "grad_norm": 1.6017993688583374,
      "learning_rate": 4.802692467876318e-05,
      "loss": 1.136,
      "step": 2509
    },
    {
      "epoch": 0.3528750175734571,
      "grad_norm": 1.4929568767547607,
      "learning_rate": 4.822567062021694e-05,
      "loss": 1.0192,
      "step": 2510
    },
    {
      "epoch": 0.3530156052298608,
      "grad_norm": 1.700010061264038,
      "learning_rate": 4.842469636069954e-05,
      "loss": 1.0165,
      "step": 2511
    },
    {
      "epoch": 0.3531561928862646,
      "grad_norm": 1.5188117027282715,
      "learning_rate": 4.86240008246355e-05,
      "loss": 0.9929,
      "step": 2512
    },
    {
      "epoch": 0.35329678054266833,
      "grad_norm": 1.5344212055206299,
      "learning_rate": 4.882358293494279e-05,
      "loss": 1.0763,
      "step": 2513
    },
    {
      "epoch": 0.3534373681990721,
      "grad_norm": 1.3881734609603882,
      "learning_rate": 4.9023441613039e-05,
      "loss": 1.1745,
      "step": 2514
    },
    {
      "epoch": 0.3535779558554759,
      "grad_norm": 1.5886605978012085,
      "learning_rate": 4.922357577884703e-05,
      "loss": 1.118,
      "step": 2515
    },
    {
      "epoch": 0.35371854351187965,
      "grad_norm": 1.7893551588058472,
      "learning_rate": 4.94239843508012e-05,
      "loss": 1.2688,
      "step": 2516
    },
    {
      "epoch": 0.35385913116828344,
      "grad_norm": 1.5427148342132568,
      "learning_rate": 4.962466624585267e-05,
      "loss": 1.2487,
      "step": 2517
    },
    {
      "epoch": 0.35399971882468717,
      "grad_norm": 1.4825206995010376,
      "learning_rate": 4.982562037947556e-05,
      "loss": 1.222,
      "step": 2518
    },
    {
      "epoch": 0.35414030648109096,
      "grad_norm": 1.342725396156311,
      "learning_rate": 5.002684566567271e-05,
      "loss": 1.0453,
      "step": 2519
    },
    {
      "epoch": 0.35428089413749475,
      "grad_norm": 1.421018362045288,
      "learning_rate": 5.022834101698182e-05,
      "loss": 1.1635,
      "step": 2520
    },
    {
      "epoch": 0.3544214817938985,
      "grad_norm": 1.3579864501953125,
      "learning_rate": 5.043010534448083e-05,
      "loss": 1.017,
      "step": 2521
    },
    {
      "epoch": 0.3545620694503023,
      "grad_norm": 1.5154105424880981,
      "learning_rate": 5.063213755779422e-05,
      "loss": 1.0561,
      "step": 2522
    },
    {
      "epoch": 0.354702657106706,
      "grad_norm": 1.3992061614990234,
      "learning_rate": 5.0834436565098655e-05,
      "loss": 1.0064,
      "step": 2523
    },
    {
      "epoch": 0.3548432447631098,
      "grad_norm": 1.6290247440338135,
      "learning_rate": 5.1037001273129205e-05,
      "loss": 1.0353,
      "step": 2524
    },
    {
      "epoch": 0.3549838324195136,
      "grad_norm": 1.655885934829712,
      "learning_rate": 5.1239830587184814e-05,
      "loss": 0.8956,
      "step": 2525
    },
    {
      "epoch": 0.3551244200759173,
      "grad_norm": 1.6506013870239258,
      "learning_rate": 5.1442923411134544e-05,
      "loss": 1.0831,
      "step": 2526
    },
    {
      "epoch": 0.3552650077323211,
      "grad_norm": 1.4425996541976929,
      "learning_rate": 5.1646278647423305e-05,
      "loss": 1.1718,
      "step": 2527
    },
    {
      "epoch": 0.35540559538872485,
      "grad_norm": 1.447643518447876,
      "learning_rate": 5.184989519707809e-05,
      "loss": 1.1533,
      "step": 2528
    },
    {
      "epoch": 0.35554618304512864,
      "grad_norm": 1.5244598388671875,
      "learning_rate": 5.205377195971357e-05,
      "loss": 1.1362,
      "step": 2529
    },
    {
      "epoch": 0.35568677070153243,
      "grad_norm": 1.4219790697097778,
      "learning_rate": 5.225790783353794e-05,
      "loss": 1.0479,
      "step": 2530
    },
    {
      "epoch": 0.35582735835793616,
      "grad_norm": 1.4828978776931763,
      "learning_rate": 5.246230171535955e-05,
      "loss": 0.9388,
      "step": 2531
    },
    {
      "epoch": 0.35596794601433995,
      "grad_norm": 1.354168176651001,
      "learning_rate": 5.2666952500592126e-05,
      "loss": 1.1174,
      "step": 2532
    },
    {
      "epoch": 0.3561085336707437,
      "grad_norm": 1.295822024345398,
      "learning_rate": 5.287185908326111e-05,
      "loss": 1.0898,
      "step": 2533
    },
    {
      "epoch": 0.3562491213271475,
      "grad_norm": 1.3160591125488281,
      "learning_rate": 5.307702035600949e-05,
      "loss": 1.0749,
      "step": 2534
    },
    {
      "epoch": 0.35638970898355127,
      "grad_norm": 1.7009356021881104,
      "learning_rate": 5.328243521010409e-05,
      "loss": 1.0313,
      "step": 2535
    },
    {
      "epoch": 0.356530296639955,
      "grad_norm": 1.4157969951629639,
      "learning_rate": 5.3488102535441075e-05,
      "loss": 1.0985,
      "step": 2536
    },
    {
      "epoch": 0.3566708842963588,
      "grad_norm": 1.6956347227096558,
      "learning_rate": 5.3694021220552325e-05,
      "loss": 1.0255,
      "step": 2537
    },
    {
      "epoch": 0.3568114719527625,
      "grad_norm": 1.5358480215072632,
      "learning_rate": 5.390019015261121e-05,
      "loss": 1.1684,
      "step": 2538
    },
    {
      "epoch": 0.3569520596091663,
      "grad_norm": 1.3121274709701538,
      "learning_rate": 5.4106608217438924e-05,
      "loss": 1.0868,
      "step": 2539
    },
    {
      "epoch": 0.3570926472655701,
      "grad_norm": 1.280051589012146,
      "learning_rate": 5.431327429951012e-05,
      "loss": 1.1875,
      "step": 2540
    },
    {
      "epoch": 0.35723323492197384,
      "grad_norm": 1.2072852849960327,
      "learning_rate": 5.452018728195918e-05,
      "loss": 1.2558,
      "step": 2541
    },
    {
      "epoch": 0.35737382257837763,
      "grad_norm": 1.6805657148361206,
      "learning_rate": 5.472734604658606e-05,
      "loss": 1.0867,
      "step": 2542
    },
    {
      "epoch": 0.35751441023478137,
      "grad_norm": 1.4239544868469238,
      "learning_rate": 5.4934749473862745e-05,
      "loss": 1.0881,
      "step": 2543
    },
    {
      "epoch": 0.35765499789118516,
      "grad_norm": 1.235000729560852,
      "learning_rate": 5.514239644293877e-05,
      "loss": 1.168,
      "step": 2544
    },
    {
      "epoch": 0.35779558554758895,
      "grad_norm": 1.6497955322265625,
      "learning_rate": 5.535028583164757e-05,
      "loss": 1.0009,
      "step": 2545
    },
    {
      "epoch": 0.3579361732039927,
      "grad_norm": 1.6011333465576172,
      "learning_rate": 5.5558416516512455e-05,
      "loss": 1.0191,
      "step": 2546
    },
    {
      "epoch": 0.35807676086039647,
      "grad_norm": 1.6763039827346802,
      "learning_rate": 5.5766787372752936e-05,
      "loss": 1.1318,
      "step": 2547
    },
    {
      "epoch": 0.3582173485168002,
      "grad_norm": 1.4215991497039795,
      "learning_rate": 5.597539727429042e-05,
      "loss": 1.1381,
      "step": 2548
    },
    {
      "epoch": 0.358357936173204,
      "grad_norm": 1.4474365711212158,
      "learning_rate": 5.618424509375425e-05,
      "loss": 0.9034,
      "step": 2549
    },
    {
      "epoch": 0.3584985238296078,
      "grad_norm": 1.3405014276504517,
      "learning_rate": 5.6393329702488496e-05,
      "loss": 1.0997,
      "step": 2550
    },
    {
      "epoch": 0.3586391114860115,
      "grad_norm": 1.4513859748840332,
      "learning_rate": 5.660264997055712e-05,
      "loss": 0.9969,
      "step": 2551
    },
    {
      "epoch": 0.3587796991424153,
      "grad_norm": 1.6310783624649048,
      "learning_rate": 5.6812204766751e-05,
      "loss": 1.0109,
      "step": 2552
    },
    {
      "epoch": 0.35892028679881904,
      "grad_norm": 1.3873109817504883,
      "learning_rate": 5.702199295859296e-05,
      "loss": 1.2235,
      "step": 2553
    },
    {
      "epoch": 0.35906087445522283,
      "grad_norm": 1.6274387836456299,
      "learning_rate": 5.723201341234512e-05,
      "loss": 1.0997,
      "step": 2554
    },
    {
      "epoch": 0.3592014621116266,
      "grad_norm": 1.6434168815612793,
      "learning_rate": 5.744226499301396e-05,
      "loss": 0.9769,
      "step": 2555
    },
    {
      "epoch": 0.35934204976803036,
      "grad_norm": 1.416793704032898,
      "learning_rate": 5.765274656435733e-05,
      "loss": 1.1818,
      "step": 2556
    },
    {
      "epoch": 0.35948263742443415,
      "grad_norm": 1.506474494934082,
      "learning_rate": 5.78634569888896e-05,
      "loss": 1.0603,
      "step": 2557
    },
    {
      "epoch": 0.3596232250808379,
      "grad_norm": 1.1603097915649414,
      "learning_rate": 5.8074395127888904e-05,
      "loss": 1.0457,
      "step": 2558
    },
    {
      "epoch": 0.3597638127372417,
      "grad_norm": 1.4657173156738281,
      "learning_rate": 5.8285559841402405e-05,
      "loss": 1.2711,
      "step": 2559
    },
    {
      "epoch": 0.35990440039364546,
      "grad_norm": 1.6975390911102295,
      "learning_rate": 5.8496949988253194e-05,
      "loss": 1.0443,
      "step": 2560
    },
    {
      "epoch": 0.3600449880500492,
      "grad_norm": 1.5168673992156982,
      "learning_rate": 5.870856442604551e-05,
      "loss": 1.0382,
      "step": 2561
    },
    {
      "epoch": 0.360185575706453,
      "grad_norm": 1.4871639013290405,
      "learning_rate": 5.892040201117202e-05,
      "loss": 1.191,
      "step": 2562
    },
    {
      "epoch": 0.3603261633628567,
      "grad_norm": 1.7900187969207764,
      "learning_rate": 5.913246159881909e-05,
      "loss": 1.0239,
      "step": 2563
    },
    {
      "epoch": 0.3604667510192605,
      "grad_norm": 1.4008746147155762,
      "learning_rate": 5.9344742042973725e-05,
      "loss": 0.9696,
      "step": 2564
    },
    {
      "epoch": 0.3606073386756643,
      "grad_norm": 1.6451793909072876,
      "learning_rate": 5.955724219642882e-05,
      "loss": 1.153,
      "step": 2565
    },
    {
      "epoch": 0.36074792633206804,
      "grad_norm": 1.4492735862731934,
      "learning_rate": 5.9769960910790455e-05,
      "loss": 1.1517,
      "step": 2566
    },
    {
      "epoch": 0.3608885139884718,
      "grad_norm": 1.4754942655563354,
      "learning_rate": 5.998289703648328e-05,
      "loss": 0.9439,
      "step": 2567
    },
    {
      "epoch": 0.36102910164487556,
      "grad_norm": 1.3372230529785156,
      "learning_rate": 6.019604942275706e-05,
      "loss": 1.1324,
      "step": 2568
    },
    {
      "epoch": 0.36116968930127935,
      "grad_norm": 1.449591040611267,
      "learning_rate": 6.0409416917692864e-05,
      "loss": 1.1285,
      "step": 2569
    },
    {
      "epoch": 0.36131027695768314,
      "grad_norm": 1.5005736351013184,
      "learning_rate": 6.062299836820915e-05,
      "loss": 1.0655,
      "step": 2570
    },
    {
      "epoch": 0.3614508646140869,
      "grad_norm": 1.5008031129837036,
      "learning_rate": 6.0836792620068495e-05,
      "loss": 0.9737,
      "step": 2571
    },
    {
      "epoch": 0.36159145227049067,
      "grad_norm": 1.577317237854004,
      "learning_rate": 6.105079851788287e-05,
      "loss": 1.0613,
      "step": 2572
    },
    {
      "epoch": 0.3617320399268944,
      "grad_norm": 1.5837997198104858,
      "learning_rate": 6.1265014905121e-05,
      "loss": 0.9307,
      "step": 2573
    },
    {
      "epoch": 0.3618726275832982,
      "grad_norm": 1.474953293800354,
      "learning_rate": 6.14794406241137e-05,
      "loss": 1.2642,
      "step": 2574
    },
    {
      "epoch": 0.362013215239702,
      "grad_norm": 1.5723570585250854,
      "learning_rate": 6.169407451606096e-05,
      "loss": 0.9335,
      "step": 2575
    },
    {
      "epoch": 0.3621538028961057,
      "grad_norm": 1.4281582832336426,
      "learning_rate": 6.190891542103711e-05,
      "loss": 1.0571,
      "step": 2576
    },
    {
      "epoch": 0.3622943905525095,
      "grad_norm": 1.525734543800354,
      "learning_rate": 6.21239621779984e-05,
      "loss": 0.9591,
      "step": 2577
    },
    {
      "epoch": 0.36243497820891324,
      "grad_norm": 2.071716785430908,
      "learning_rate": 6.233921362478812e-05,
      "loss": 0.9591,
      "step": 2578
    },
    {
      "epoch": 0.36257556586531703,
      "grad_norm": 1.732648253440857,
      "learning_rate": 6.255466859814383e-05,
      "loss": 1.2994,
      "step": 2579
    },
    {
      "epoch": 0.3627161535217208,
      "grad_norm": 1.3992712497711182,
      "learning_rate": 6.277032593370263e-05,
      "loss": 1.1042,
      "step": 2580
    },
    {
      "epoch": 0.36285674117812455,
      "grad_norm": 1.5451161861419678,
      "learning_rate": 6.298618446600856e-05,
      "loss": 0.9664,
      "step": 2581
    },
    {
      "epoch": 0.36299732883452834,
      "grad_norm": 1.4634068012237549,
      "learning_rate": 6.320224302851793e-05,
      "loss": 1.0748,
      "step": 2582
    },
    {
      "epoch": 0.3631379164909321,
      "grad_norm": 1.5656347274780273,
      "learning_rate": 6.341850045360652e-05,
      "loss": 1.0959,
      "step": 2583
    },
    {
      "epoch": 0.36327850414733587,
      "grad_norm": 1.6228914260864258,
      "learning_rate": 6.363495557257474e-05,
      "loss": 1.0709,
      "step": 2584
    },
    {
      "epoch": 0.36341909180373966,
      "grad_norm": 1.5089365243911743,
      "learning_rate": 6.385160721565529e-05,
      "loss": 1.1139,
      "step": 2585
    },
    {
      "epoch": 0.3635596794601434,
      "grad_norm": 1.390669584274292,
      "learning_rate": 6.406845421201836e-05,
      "loss": 1.1497,
      "step": 2586
    },
    {
      "epoch": 0.3637002671165472,
      "grad_norm": 1.4136580228805542,
      "learning_rate": 6.428549538977885e-05,
      "loss": 1.1325,
      "step": 2587
    },
    {
      "epoch": 0.3638408547729509,
      "grad_norm": 1.3321926593780518,
      "learning_rate": 6.450272957600172e-05,
      "loss": 1.0825,
      "step": 2588
    },
    {
      "epoch": 0.3639814424293547,
      "grad_norm": 1.3990774154663086,
      "learning_rate": 6.472015559670916e-05,
      "loss": 1.1876,
      "step": 2589
    },
    {
      "epoch": 0.3641220300857585,
      "grad_norm": 1.4466227293014526,
      "learning_rate": 6.49377722768868e-05,
      "loss": 1.0425,
      "step": 2590
    },
    {
      "epoch": 0.36426261774216223,
      "grad_norm": 1.3683024644851685,
      "learning_rate": 6.515557844048965e-05,
      "loss": 1.1391,
      "step": 2591
    },
    {
      "epoch": 0.364403205398566,
      "grad_norm": 1.305620551109314,
      "learning_rate": 6.537357291044883e-05,
      "loss": 1.0415,
      "step": 2592
    },
    {
      "epoch": 0.36454379305496976,
      "grad_norm": 1.520009160041809,
      "learning_rate": 6.559175450867771e-05,
      "loss": 0.9182,
      "step": 2593
    },
    {
      "epoch": 0.36468438071137355,
      "grad_norm": 1.4348278045654297,
      "learning_rate": 6.58101220560786e-05,
      "loss": 1.0984,
      "step": 2594
    },
    {
      "epoch": 0.36482496836777734,
      "grad_norm": 1.627060055732727,
      "learning_rate": 6.602867437254871e-05,
      "loss": 1.0077,
      "step": 2595
    },
    {
      "epoch": 0.36496555602418107,
      "grad_norm": 1.508205533027649,
      "learning_rate": 6.624741027698676e-05,
      "loss": 1.0647,
      "step": 2596
    },
    {
      "epoch": 0.36510614368058486,
      "grad_norm": 1.674864411354065,
      "learning_rate": 6.646632858729925e-05,
      "loss": 1.27,
      "step": 2597
    },
    {
      "epoch": 0.3652467313369886,
      "grad_norm": 1.5784465074539185,
      "learning_rate": 6.66854281204072e-05,
      "loss": 1.1944,
      "step": 2598
    },
    {
      "epoch": 0.3653873189933924,
      "grad_norm": 1.4915945529937744,
      "learning_rate": 6.690470769225196e-05,
      "loss": 1.1218,
      "step": 2599
    },
    {
      "epoch": 0.3655279066497962,
      "grad_norm": 1.5710467100143433,
      "learning_rate": 6.712416611780205e-05,
      "loss": 1.1205,
      "step": 2600
    },
    {
      "epoch": 0.3656684943061999,
      "grad_norm": 1.226946234703064,
      "learning_rate": 6.734380221105934e-05,
      "loss": 1.2748,
      "step": 2601
    },
    {
      "epoch": 0.3658090819626037,
      "grad_norm": 1.5513732433319092,
      "learning_rate": 6.756361478506574e-05,
      "loss": 1.0398,
      "step": 2602
    },
    {
      "epoch": 0.36594966961900743,
      "grad_norm": 1.424202799797058,
      "learning_rate": 6.778360265190924e-05,
      "loss": 1.1393,
      "step": 2603
    },
    {
      "epoch": 0.3660902572754112,
      "grad_norm": 1.2974752187728882,
      "learning_rate": 6.800376462273055e-05,
      "loss": 1.1833,
      "step": 2604
    },
    {
      "epoch": 0.366230844931815,
      "grad_norm": 1.5628727674484253,
      "learning_rate": 6.822409950772944e-05,
      "loss": 1.0884,
      "step": 2605
    },
    {
      "epoch": 0.36637143258821875,
      "grad_norm": 1.383550763130188,
      "learning_rate": 6.844460611617144e-05,
      "loss": 1.2088,
      "step": 2606
    },
    {
      "epoch": 0.36651202024462254,
      "grad_norm": 1.475934624671936,
      "learning_rate": 6.866528325639389e-05,
      "loss": 1.1163,
      "step": 2607
    },
    {
      "epoch": 0.3666526079010263,
      "grad_norm": 1.3810166120529175,
      "learning_rate": 6.888612973581231e-05,
      "loss": 1.2121,
      "step": 2608
    },
    {
      "epoch": 0.36679319555743006,
      "grad_norm": 1.6095088720321655,
      "learning_rate": 6.910714436092754e-05,
      "loss": 1.157,
      "step": 2609
    },
    {
      "epoch": 0.36693378321383385,
      "grad_norm": 1.6501710414886475,
      "learning_rate": 6.932832593733143e-05,
      "loss": 1.1965,
      "step": 2610
    },
    {
      "epoch": 0.3670743708702376,
      "grad_norm": 1.4720014333724976,
      "learning_rate": 6.954967326971364e-05,
      "loss": 1.124,
      "step": 2611
    },
    {
      "epoch": 0.3672149585266414,
      "grad_norm": 1.2395762205123901,
      "learning_rate": 6.977118516186801e-05,
      "loss": 1.0864,
      "step": 2612
    },
    {
      "epoch": 0.3673555461830451,
      "grad_norm": 1.6225031614303589,
      "learning_rate": 6.999286041669926e-05,
      "loss": 1.0279,
      "step": 2613
    },
    {
      "epoch": 0.3674961338394489,
      "grad_norm": 1.502915620803833,
      "learning_rate": 7.02146978362291e-05,
      "loss": 1.1785,
      "step": 2614
    },
    {
      "epoch": 0.3676367214958527,
      "grad_norm": 1.8486300706863403,
      "learning_rate": 7.043669622160283e-05,
      "loss": 0.9501,
      "step": 2615
    },
    {
      "epoch": 0.3677773091522564,
      "grad_norm": 1.4935492277145386,
      "learning_rate": 7.065885437309588e-05,
      "loss": 0.9412,
      "step": 2616
    },
    {
      "epoch": 0.3679178968086602,
      "grad_norm": 1.434739351272583,
      "learning_rate": 7.088117109012049e-05,
      "loss": 1.0935,
      "step": 2617
    },
    {
      "epoch": 0.36805848446506395,
      "grad_norm": 1.3069562911987305,
      "learning_rate": 7.110364517123172e-05,
      "loss": 1.2642,
      "step": 2618
    },
    {
      "epoch": 0.36819907212146774,
      "grad_norm": 1.882298231124878,
      "learning_rate": 7.132627541413428e-05,
      "loss": 1.0376,
      "step": 2619
    },
    {
      "epoch": 0.3683396597778715,
      "grad_norm": 1.5285580158233643,
      "learning_rate": 7.154906061568888e-05,
      "loss": 1.0073,
      "step": 2620
    },
    {
      "epoch": 0.36848024743427527,
      "grad_norm": 1.6254830360412598,
      "learning_rate": 7.177199957191905e-05,
      "loss": 1.1373,
      "step": 2621
    },
    {
      "epoch": 0.36862083509067906,
      "grad_norm": 1.4782743453979492,
      "learning_rate": 7.199509107801714e-05,
      "loss": 1.2047,
      "step": 2622
    },
    {
      "epoch": 0.3687614227470828,
      "grad_norm": 1.575298547744751,
      "learning_rate": 7.221833392835118e-05,
      "loss": 0.9939,
      "step": 2623
    },
    {
      "epoch": 0.3689020104034866,
      "grad_norm": 1.4251362085342407,
      "learning_rate": 7.244172691647124e-05,
      "loss": 1.0561,
      "step": 2624
    },
    {
      "epoch": 0.3690425980598903,
      "grad_norm": 1.6391470432281494,
      "learning_rate": 7.266526883511622e-05,
      "loss": 1.1494,
      "step": 2625
    },
    {
      "epoch": 0.3691831857162941,
      "grad_norm": 1.6320112943649292,
      "learning_rate": 7.288895847622003e-05,
      "loss": 1.0801,
      "step": 2626
    },
    {
      "epoch": 0.3693237733726979,
      "grad_norm": 1.526487946510315,
      "learning_rate": 7.311279463091802e-05,
      "loss": 1.0234,
      "step": 2627
    },
    {
      "epoch": 0.36946436102910163,
      "grad_norm": 1.487818956375122,
      "learning_rate": 7.333677608955425e-05,
      "loss": 0.8973,
      "step": 2628
    },
    {
      "epoch": 0.3696049486855054,
      "grad_norm": 1.5781453847885132,
      "learning_rate": 7.356090164168705e-05,
      "loss": 1.0543,
      "step": 2629
    },
    {
      "epoch": 0.36974553634190915,
      "grad_norm": 1.676884412765503,
      "learning_rate": 7.378517007609656e-05,
      "loss": 1.0374,
      "step": 2630
    },
    {
      "epoch": 0.36988612399831294,
      "grad_norm": 1.5493888854980469,
      "learning_rate": 7.400958018079005e-05,
      "loss": 1.0512,
      "step": 2631
    },
    {
      "epoch": 0.37002671165471673,
      "grad_norm": 1.7915970087051392,
      "learning_rate": 7.423413074300989e-05,
      "loss": 1.0287,
      "step": 2632
    },
    {
      "epoch": 0.37016729931112047,
      "grad_norm": 1.9011952877044678,
      "learning_rate": 7.445882054923886e-05,
      "loss": 1.1368,
      "step": 2633
    },
    {
      "epoch": 0.37030788696752426,
      "grad_norm": 1.9742546081542969,
      "learning_rate": 7.468364838520771e-05,
      "loss": 1.1437,
      "step": 2634
    },
    {
      "epoch": 0.370448474623928,
      "grad_norm": 1.7076935768127441,
      "learning_rate": 7.490861303590067e-05,
      "loss": 1.2344,
      "step": 2635
    },
    {
      "epoch": 0.3705890622803318,
      "grad_norm": 1.4989367723464966,
      "learning_rate": 7.513371328556316e-05,
      "loss": 1.0794,
      "step": 2636
    },
    {
      "epoch": 0.3707296499367356,
      "grad_norm": 1.3882806301116943,
      "learning_rate": 7.535894791770737e-05,
      "loss": 1.058,
      "step": 2637
    },
    {
      "epoch": 0.3708702375931393,
      "grad_norm": 1.4206311702728271,
      "learning_rate": 7.55843157151197e-05,
      "loss": 1.2033,
      "step": 2638
    },
    {
      "epoch": 0.3710108252495431,
      "grad_norm": 1.5078250169754028,
      "learning_rate": 7.580981545986629e-05,
      "loss": 1.1026,
      "step": 2639
    },
    {
      "epoch": 0.37115141290594683,
      "grad_norm": 1.6225999593734741,
      "learning_rate": 7.603544593330082e-05,
      "loss": 1.0038,
      "step": 2640
    },
    {
      "epoch": 0.3712920005623506,
      "grad_norm": 1.5948246717453003,
      "learning_rate": 7.626120591607006e-05,
      "loss": 1.0925,
      "step": 2641
    },
    {
      "epoch": 0.3714325882187544,
      "grad_norm": 1.5151712894439697,
      "learning_rate": 7.648709418812129e-05,
      "loss": 0.9591,
      "step": 2642
    },
    {
      "epoch": 0.37157317587515815,
      "grad_norm": 1.1876274347305298,
      "learning_rate": 7.67131095287079e-05,
      "loss": 1.3516,
      "step": 2643
    },
    {
      "epoch": 0.37171376353156194,
      "grad_norm": 1.527751088142395,
      "learning_rate": 7.693925071639719e-05,
      "loss": 1.1546,
      "step": 2644
    },
    {
      "epoch": 0.37185435118796567,
      "grad_norm": 1.6868016719818115,
      "learning_rate": 7.716551652907599e-05,
      "loss": 1.1371,
      "step": 2645
    },
    {
      "epoch": 0.37199493884436946,
      "grad_norm": 1.6738375425338745,
      "learning_rate": 7.739190574395776e-05,
      "loss": 1.0368,
      "step": 2646
    },
    {
      "epoch": 0.37213552650077325,
      "grad_norm": 1.4637868404388428,
      "learning_rate": 7.7618417137589e-05,
      "loss": 1.0858,
      "step": 2647
    },
    {
      "epoch": 0.372276114157177,
      "grad_norm": 1.3813661336898804,
      "learning_rate": 7.78450494858559e-05,
      "loss": 1.0966,
      "step": 2648
    },
    {
      "epoch": 0.3724167018135808,
      "grad_norm": 1.4209660291671753,
      "learning_rate": 7.807180156399132e-05,
      "loss": 1.0702,
      "step": 2649
    },
    {
      "epoch": 0.3725572894699845,
      "grad_norm": 1.4125865697860718,
      "learning_rate": 7.829867214658046e-05,
      "loss": 1.1159,
      "step": 2650
    },
    {
      "epoch": 0.3726978771263883,
      "grad_norm": 2.247926950454712,
      "learning_rate": 7.85256600075687e-05,
      "loss": 1.2132,
      "step": 2651
    },
    {
      "epoch": 0.3728384647827921,
      "grad_norm": 2.015129327774048,
      "learning_rate": 7.875276392026722e-05,
      "loss": 1.0757,
      "step": 2652
    },
    {
      "epoch": 0.3729790524391958,
      "grad_norm": 1.4920425415039062,
      "learning_rate": 7.897998265736042e-05,
      "loss": 1.1351,
      "step": 2653
    },
    {
      "epoch": 0.3731196400955996,
      "grad_norm": 1.3900187015533447,
      "learning_rate": 7.920731499091163e-05,
      "loss": 1.0663,
      "step": 2654
    },
    {
      "epoch": 0.37326022775200335,
      "grad_norm": 1.562975525856018,
      "learning_rate": 7.943475969237079e-05,
      "loss": 1.1458,
      "step": 2655
    },
    {
      "epoch": 0.37340081540840714,
      "grad_norm": 1.3884087800979614,
      "learning_rate": 7.966231553258022e-05,
      "loss": 1.0547,
      "step": 2656
    },
    {
      "epoch": 0.37354140306481093,
      "grad_norm": 1.4637973308563232,
      "learning_rate": 7.988998128178206e-05,
      "loss": 1.2117,
      "step": 2657
    },
    {
      "epoch": 0.37368199072121466,
      "grad_norm": 1.2843408584594727,
      "learning_rate": 8.011775570962382e-05,
      "loss": 1.1835,
      "step": 2658
    },
    {
      "epoch": 0.37382257837761845,
      "grad_norm": 1.451019048690796,
      "learning_rate": 8.034563758516635e-05,
      "loss": 1.0205,
      "step": 2659
    },
    {
      "epoch": 0.3739631660340222,
      "grad_norm": 1.2583070993423462,
      "learning_rate": 8.057362567688936e-05,
      "loss": 1.1231,
      "step": 2660
    },
    {
      "epoch": 0.374103753690426,
      "grad_norm": 1.4738574028015137,
      "learning_rate": 8.080171875269897e-05,
      "loss": 1.1987,
      "step": 2661
    },
    {
      "epoch": 0.37424434134682977,
      "grad_norm": 1.3363100290298462,
      "learning_rate": 8.10299155799334e-05,
      "loss": 1.0375,
      "step": 2662
    },
    {
      "epoch": 0.3743849290032335,
      "grad_norm": 1.4111905097961426,
      "learning_rate": 8.125821492537074e-05,
      "loss": 1.0124,
      "step": 2663
    },
    {
      "epoch": 0.3745255166596373,
      "grad_norm": 1.8315365314483643,
      "learning_rate": 8.148661555523459e-05,
      "loss": 1.0559,
      "step": 2664
    },
    {
      "epoch": 0.374666104316041,
      "grad_norm": 1.328649640083313,
      "learning_rate": 8.171511623520167e-05,
      "loss": 1.2059,
      "step": 2665
    },
    {
      "epoch": 0.3748066919724448,
      "grad_norm": 1.4611704349517822,
      "learning_rate": 8.194371573040747e-05,
      "loss": 1.1782,
      "step": 2666
    },
    {
      "epoch": 0.3749472796288486,
      "grad_norm": 1.4868890047073364,
      "learning_rate": 8.217241280545373e-05,
      "loss": 1.1153,
      "step": 2667
    },
    {
      "epoch": 0.37508786728525234,
      "grad_norm": 1.5664066076278687,
      "learning_rate": 8.240120622441501e-05,
      "loss": 1.0843,
      "step": 2668
    },
    {
      "epoch": 0.37522845494165613,
      "grad_norm": 1.4771925210952759,
      "learning_rate": 8.263009475084496e-05,
      "loss": 1.161,
      "step": 2669
    },
    {
      "epoch": 0.37536904259805987,
      "grad_norm": 1.7180688381195068,
      "learning_rate": 8.28590771477833e-05,
      "loss": 1.0963,
      "step": 2670
    },
    {
      "epoch": 0.37550963025446366,
      "grad_norm": 1.533623218536377,
      "learning_rate": 8.308815217776242e-05,
      "loss": 1.2222,
      "step": 2671
    },
    {
      "epoch": 0.37565021791086745,
      "grad_norm": 1.5842229127883911,
      "learning_rate": 8.331731860281435e-05,
      "loss": 1.0997,
      "step": 2672
    },
    {
      "epoch": 0.3757908055672712,
      "grad_norm": 1.4248261451721191,
      "learning_rate": 8.354657518447693e-05,
      "loss": 1.2097,
      "step": 2673
    },
    {
      "epoch": 0.37593139322367497,
      "grad_norm": 1.7966675758361816,
      "learning_rate": 8.377592068380085e-05,
      "loss": 1.2809,
      "step": 2674
    },
    {
      "epoch": 0.3760719808800787,
      "grad_norm": 1.501310110092163,
      "learning_rate": 8.40053538613562e-05,
      "loss": 0.8947,
      "step": 2675
    },
    {
      "epoch": 0.3762125685364825,
      "grad_norm": 1.587172031402588,
      "learning_rate": 8.423487347723949e-05,
      "loss": 1.1126,
      "step": 2676
    },
    {
      "epoch": 0.3763531561928863,
      "grad_norm": 1.351994276046753,
      "learning_rate": 8.446447829107987e-05,
      "loss": 1.12,
      "step": 2677
    },
    {
      "epoch": 0.37649374384929,
      "grad_norm": 1.196803331375122,
      "learning_rate": 8.469416706204611e-05,
      "loss": 1.3552,
      "step": 2678
    },
    {
      "epoch": 0.3766343315056938,
      "grad_norm": 1.454355001449585,
      "learning_rate": 8.49239385488532e-05,
      "loss": 1.1174,
      "step": 2679
    },
    {
      "epoch": 0.37677491916209754,
      "grad_norm": 1.5828267335891724,
      "learning_rate": 8.515379150976935e-05,
      "loss": 1.0268,
      "step": 2680
    },
    {
      "epoch": 0.37691550681850133,
      "grad_norm": 1.5164055824279785,
      "learning_rate": 8.538372470262223e-05,
      "loss": 1.0891,
      "step": 2681
    },
    {
      "epoch": 0.3770560944749051,
      "grad_norm": 1.2222626209259033,
      "learning_rate": 8.5613736884806e-05,
      "loss": 1.1558,
      "step": 2682
    },
    {
      "epoch": 0.37719668213130886,
      "grad_norm": 1.506642460823059,
      "learning_rate": 8.584382681328786e-05,
      "loss": 0.8551,
      "step": 2683
    },
    {
      "epoch": 0.37733726978771265,
      "grad_norm": 1.3947241306304932,
      "learning_rate": 8.607399324461511e-05,
      "loss": 1.0572,
      "step": 2684
    },
    {
      "epoch": 0.3774778574441164,
      "grad_norm": 1.797595739364624,
      "learning_rate": 8.630423493492144e-05,
      "loss": 1.0902,
      "step": 2685
    },
    {
      "epoch": 0.3776184451005202,
      "grad_norm": 1.4838796854019165,
      "learning_rate": 8.653455063993357e-05,
      "loss": 0.9942,
      "step": 2686
    },
    {
      "epoch": 0.37775903275692396,
      "grad_norm": 1.5957480669021606,
      "learning_rate": 8.676493911497875e-05,
      "loss": 0.9835,
      "step": 2687
    },
    {
      "epoch": 0.3778996204133277,
      "grad_norm": 1.377171277999878,
      "learning_rate": 8.69953991149906e-05,
      "loss": 1.0389,
      "step": 2688
    },
    {
      "epoch": 0.3780402080697315,
      "grad_norm": 1.6809266805648804,
      "learning_rate": 8.722592939451631e-05,
      "loss": 0.9829,
      "step": 2689
    },
    {
      "epoch": 0.3781807957261352,
      "grad_norm": 1.4696694612503052,
      "learning_rate": 8.745652870772318e-05,
      "loss": 1.1058,
      "step": 2690
    },
    {
      "epoch": 0.378321383382539,
      "grad_norm": 1.4977242946624756,
      "learning_rate": 8.768719580840569e-05,
      "loss": 0.9151,
      "step": 2691
    },
    {
      "epoch": 0.3784619710389428,
      "grad_norm": 1.6743556261062622,
      "learning_rate": 8.791792944999172e-05,
      "loss": 1.164,
      "step": 2692
    },
    {
      "epoch": 0.37860255869534654,
      "grad_norm": 1.6559395790100098,
      "learning_rate": 8.814872838554965e-05,
      "loss": 0.9558,
      "step": 2693
    },
    {
      "epoch": 0.3787431463517503,
      "grad_norm": 1.50855553150177,
      "learning_rate": 8.83795913677949e-05,
      "loss": 1.1404,
      "step": 2694
    },
    {
      "epoch": 0.37888373400815406,
      "grad_norm": 1.320807933807373,
      "learning_rate": 8.861051714909704e-05,
      "loss": 1.2877,
      "step": 2695
    },
    {
      "epoch": 0.37902432166455785,
      "grad_norm": 1.4517478942871094,
      "learning_rate": 8.884150448148598e-05,
      "loss": 1.0979,
      "step": 2696
    },
    {
      "epoch": 0.37916490932096164,
      "grad_norm": 1.4228923320770264,
      "learning_rate": 8.907255211665909e-05,
      "loss": 1.0249,
      "step": 2697
    },
    {
      "epoch": 0.3793054969773654,
      "grad_norm": 1.3634127378463745,
      "learning_rate": 8.930365880598777e-05,
      "loss": 1.1662,
      "step": 2698
    },
    {
      "epoch": 0.37944608463376917,
      "grad_norm": 1.51992666721344,
      "learning_rate": 8.953482330052457e-05,
      "loss": 1.0875,
      "step": 2699
    },
    {
      "epoch": 0.3795866722901729,
      "grad_norm": 1.3075053691864014,
      "learning_rate": 8.976604435100932e-05,
      "loss": 0.9798,
      "step": 2700
    },
    {
      "epoch": 0.3797272599465767,
      "grad_norm": 1.3148120641708374,
      "learning_rate": 8.999732070787635e-05,
      "loss": 0.997,
      "step": 2701
    },
    {
      "epoch": 0.3798678476029805,
      "grad_norm": 1.5131416320800781,
      "learning_rate": 9.0228651121261e-05,
      "loss": 1.1875,
      "step": 2702
    },
    {
      "epoch": 0.3800084352593842,
      "grad_norm": 1.3038175106048584,
      "learning_rate": 9.046003434100672e-05,
      "loss": 0.9805,
      "step": 2703
    },
    {
      "epoch": 0.380149022915788,
      "grad_norm": 1.537192463874817,
      "learning_rate": 9.069146911667147e-05,
      "loss": 1.0776,
      "step": 2704
    },
    {
      "epoch": 0.38028961057219174,
      "grad_norm": 1.3715260028839111,
      "learning_rate": 9.092295419753425e-05,
      "loss": 1.1686,
      "step": 2705
    },
    {
      "epoch": 0.38043019822859553,
      "grad_norm": 1.635269284248352,
      "learning_rate": 9.115448833260278e-05,
      "loss": 1.0898,
      "step": 2706
    },
    {
      "epoch": 0.3805707858849993,
      "grad_norm": 1.558831810951233,
      "learning_rate": 9.138607027061922e-05,
      "loss": 1.2921,
      "step": 2707
    },
    {
      "epoch": 0.38071137354140305,
      "grad_norm": 1.5317187309265137,
      "learning_rate": 9.161769876006783e-05,
      "loss": 1.0991,
      "step": 2708
    },
    {
      "epoch": 0.38085196119780684,
      "grad_norm": 1.5453031063079834,
      "learning_rate": 9.184937254918071e-05,
      "loss": 1.1204,
      "step": 2709
    },
    {
      "epoch": 0.3809925488542106,
      "grad_norm": 1.50357985496521,
      "learning_rate": 9.208109038594573e-05,
      "loss": 1.2385,
      "step": 2710
    },
    {
      "epoch": 0.38113313651061437,
      "grad_norm": 1.5680991411209106,
      "learning_rate": 9.23128510181123e-05,
      "loss": 1.0524,
      "step": 2711
    },
    {
      "epoch": 0.38127372416701816,
      "grad_norm": 1.5807344913482666,
      "learning_rate": 9.254465319319899e-05,
      "loss": 1.0231,
      "step": 2712
    },
    {
      "epoch": 0.3814143118234219,
      "grad_norm": 1.4988694190979004,
      "learning_rate": 9.277649565849925e-05,
      "loss": 0.9874,
      "step": 2713
    },
    {
      "epoch": 0.3815548994798257,
      "grad_norm": 1.587576150894165,
      "learning_rate": 9.300837716108939e-05,
      "loss": 1.1526,
      "step": 2714
    },
    {
      "epoch": 0.3816954871362294,
      "grad_norm": 1.4451206922531128,
      "learning_rate": 9.324029644783435e-05,
      "loss": 1.0644,
      "step": 2715
    },
    {
      "epoch": 0.3818360747926332,
      "grad_norm": 1.4432424306869507,
      "learning_rate": 9.347225226539528e-05,
      "loss": 1.1492,
      "step": 2716
    },
    {
      "epoch": 0.381976662449037,
      "grad_norm": 1.6694376468658447,
      "learning_rate": 9.370424336023532e-05,
      "loss": 1.2004,
      "step": 2717
    },
    {
      "epoch": 0.38211725010544073,
      "grad_norm": 1.247651219367981,
      "learning_rate": 9.393626847862757e-05,
      "loss": 1.1492,
      "step": 2718
    },
    {
      "epoch": 0.3822578377618445,
      "grad_norm": 1.8106296062469482,
      "learning_rate": 9.416832636666083e-05,
      "loss": 1.0977,
      "step": 2719
    },
    {
      "epoch": 0.38239842541824826,
      "grad_norm": 1.403602123260498,
      "learning_rate": 9.440041577024723e-05,
      "loss": 1.2037,
      "step": 2720
    },
    {
      "epoch": 0.38253901307465205,
      "grad_norm": 1.4064615964889526,
      "learning_rate": 9.463253543512797e-05,
      "loss": 1.0844,
      "step": 2721
    },
    {
      "epoch": 0.38267960073105584,
      "grad_norm": 1.544590950012207,
      "learning_rate": 9.486468410688132e-05,
      "loss": 1.0185,
      "step": 2722
    },
    {
      "epoch": 0.38282018838745957,
      "grad_norm": 1.2627629041671753,
      "learning_rate": 9.50968605309285e-05,
      "loss": 1.3227,
      "step": 2723
    },
    {
      "epoch": 0.38296077604386336,
      "grad_norm": 1.394309639930725,
      "learning_rate": 9.532906345254074e-05,
      "loss": 1.1162,
      "step": 2724
    },
    {
      "epoch": 0.3831013637002671,
      "grad_norm": 1.5603388547897339,
      "learning_rate": 9.556129161684615e-05,
      "loss": 1.3021,
      "step": 2725
    },
    {
      "epoch": 0.3832419513566709,
      "grad_norm": 1.5538884401321411,
      "learning_rate": 9.57935437688363e-05,
      "loss": 0.9972,
      "step": 2726
    },
    {
      "epoch": 0.3833825390130747,
      "grad_norm": 1.4970897436141968,
      "learning_rate": 9.602581865337349e-05,
      "loss": 1.3111,
      "step": 2727
    },
    {
      "epoch": 0.3835231266694784,
      "grad_norm": 1.3637696504592896,
      "learning_rate": 9.625811501519654e-05,
      "loss": 1.0333,
      "step": 2728
    },
    {
      "epoch": 0.3836637143258822,
      "grad_norm": 1.4109879732131958,
      "learning_rate": 9.649043159892882e-05,
      "loss": 1.261,
      "step": 2729
    },
    {
      "epoch": 0.38380430198228593,
      "grad_norm": 1.8951594829559326,
      "learning_rate": 9.6722767149084e-05,
      "loss": 1.1764,
      "step": 2730
    },
    {
      "epoch": 0.3839448896386897,
      "grad_norm": 1.321424126625061,
      "learning_rate": 9.695512041007369e-05,
      "loss": 1.0012,
      "step": 2731
    },
    {
      "epoch": 0.3840854772950935,
      "grad_norm": 1.6736654043197632,
      "learning_rate": 9.718749012621311e-05,
      "loss": 1.112,
      "step": 2732
    },
    {
      "epoch": 0.38422606495149725,
      "grad_norm": 1.4725463390350342,
      "learning_rate": 9.741987504172924e-05,
      "loss": 1.1515,
      "step": 2733
    },
    {
      "epoch": 0.38436665260790104,
      "grad_norm": 1.4629011154174805,
      "learning_rate": 9.765227390076647e-05,
      "loss": 1.0395,
      "step": 2734
    },
    {
      "epoch": 0.3845072402643048,
      "grad_norm": 1.3974485397338867,
      "learning_rate": 9.788468544739422e-05,
      "loss": 1.1495,
      "step": 2735
    },
    {
      "epoch": 0.38464782792070856,
      "grad_norm": 1.3937411308288574,
      "learning_rate": 9.811710842561283e-05,
      "loss": 1.1027,
      "step": 2736
    },
    {
      "epoch": 0.38478841557711235,
      "grad_norm": 1.537476897239685,
      "learning_rate": 9.834954157936136e-05,
      "loss": 1.3229,
      "step": 2737
    },
    {
      "epoch": 0.3849290032335161,
      "grad_norm": 1.6366387605667114,
      "learning_rate": 9.85819836525235e-05,
      "loss": 1.1379,
      "step": 2738
    },
    {
      "epoch": 0.3850695908899199,
      "grad_norm": 1.5734015703201294,
      "learning_rate": 9.881443338893522e-05,
      "loss": 1.1432,
      "step": 2739
    },
    {
      "epoch": 0.3852101785463236,
      "grad_norm": 1.4866102933883667,
      "learning_rate": 9.904688953239035e-05,
      "loss": 1.1908,
      "step": 2740
    },
    {
      "epoch": 0.3853507662027274,
      "grad_norm": 1.5062594413757324,
      "learning_rate": 9.927935082664881e-05,
      "loss": 1.0613,
      "step": 2741
    },
    {
      "epoch": 0.3854913538591312,
      "grad_norm": 1.4154350757598877,
      "learning_rate": 9.95118160154423e-05,
      "loss": 1.2732,
      "step": 2742
    },
    {
      "epoch": 0.3856319415155349,
      "grad_norm": 1.6992545127868652,
      "learning_rate": 9.974428384248153e-05,
      "loss": 1.0493,
      "step": 2743
    },
    {
      "epoch": 0.3857725291719387,
      "grad_norm": 1.6510138511657715,
      "learning_rate": 9.997675305146302e-05,
      "loss": 0.9758,
      "step": 2744
    },
    {
      "epoch": 0.38591311682834245,
      "grad_norm": 1.379258632659912,
      "learning_rate": 0.00010020922238607563,
      "loss": 1.3721,
      "step": 2745
    },
    {
      "epoch": 0.38605370448474624,
      "grad_norm": 1.594556450843811,
      "learning_rate": 0.00010044169059000796,
      "loss": 1.1424,
      "step": 2746
    },
    {
      "epoch": 0.38619429214115003,
      "grad_norm": 1.5866600275039673,
      "learning_rate": 0.00010067415640695427,
      "loss": 1.0464,
      "step": 2747
    },
    {
      "epoch": 0.38633487979755377,
      "grad_norm": 1.398728609085083,
      "learning_rate": 0.00010090661858062199,
      "loss": 1.2069,
      "step": 2748
    },
    {
      "epoch": 0.38647546745395756,
      "grad_norm": 1.880760669708252,
      "learning_rate": 0.00010113907585473799,
      "loss": 0.9429,
      "step": 2749
    },
    {
      "epoch": 0.3866160551103613,
      "grad_norm": 1.5909810066223145,
      "learning_rate": 0.00010137152697305599,
      "loss": 1.2509,
      "step": 2750
    },
    {
      "epoch": 0.3867566427667651,
      "grad_norm": 1.5487297773361206,
      "learning_rate": 0.00010160397067936268,
      "loss": 1.0246,
      "step": 2751
    },
    {
      "epoch": 0.38689723042316887,
      "grad_norm": 1.7061704397201538,
      "learning_rate": 0.00010183640571748487,
      "loss": 1.1875,
      "step": 2752
    },
    {
      "epoch": 0.3870378180795726,
      "grad_norm": 1.6238174438476562,
      "learning_rate": 0.00010206883083129619,
      "loss": 1.0919,
      "step": 2753
    },
    {
      "epoch": 0.3871784057359764,
      "grad_norm": 1.4496607780456543,
      "learning_rate": 0.0001023012447647241,
      "loss": 1.1428,
      "step": 2754
    },
    {
      "epoch": 0.38731899339238013,
      "grad_norm": 1.5121047496795654,
      "learning_rate": 0.00010253364626175623,
      "loss": 1.0819,
      "step": 2755
    },
    {
      "epoch": 0.3874595810487839,
      "grad_norm": 1.8277976512908936,
      "learning_rate": 0.00010276603406644756,
      "loss": 1.2466,
      "step": 2756
    },
    {
      "epoch": 0.3876001687051877,
      "grad_norm": 1.4639886617660522,
      "learning_rate": 0.00010299840692292688,
      "loss": 1.2695,
      "step": 2757
    },
    {
      "epoch": 0.38774075636159144,
      "grad_norm": 1.416172981262207,
      "learning_rate": 0.00010323076357540416,
      "loss": 1.1507,
      "step": 2758
    },
    {
      "epoch": 0.38788134401799523,
      "grad_norm": 1.4498172998428345,
      "learning_rate": 0.00010346310276817655,
      "loss": 1.083,
      "step": 2759
    },
    {
      "epoch": 0.38802193167439897,
      "grad_norm": 1.6471742391586304,
      "learning_rate": 0.0001036954232456357,
      "loss": 1.0357,
      "step": 2760
    },
    {
      "epoch": 0.38816251933080276,
      "grad_norm": 1.3745568990707397,
      "learning_rate": 0.00010392772375227434,
      "loss": 1.1784,
      "step": 2761
    },
    {
      "epoch": 0.38830310698720655,
      "grad_norm": 1.585745096206665,
      "learning_rate": 0.00010416000303269331,
      "loss": 1.1654,
      "step": 2762
    },
    {
      "epoch": 0.3884436946436103,
      "grad_norm": 1.4576352834701538,
      "learning_rate": 0.00010439225983160808,
      "loss": 1.1116,
      "step": 2763
    },
    {
      "epoch": 0.38858428230001407,
      "grad_norm": 1.5870800018310547,
      "learning_rate": 0.00010462449289385528,
      "loss": 1.0153,
      "step": 2764
    },
    {
      "epoch": 0.3887248699564178,
      "grad_norm": 1.4250578880310059,
      "learning_rate": 0.00010485670096440036,
      "loss": 0.9867,
      "step": 2765
    },
    {
      "epoch": 0.3888654576128216,
      "grad_norm": 1.625108242034912,
      "learning_rate": 0.00010508888278834352,
      "loss": 1.1796,
      "step": 2766
    },
    {
      "epoch": 0.3890060452692254,
      "grad_norm": 1.385621190071106,
      "learning_rate": 0.00010532103711092684,
      "loss": 0.9232,
      "step": 2767
    },
    {
      "epoch": 0.3891466329256291,
      "grad_norm": 1.6536011695861816,
      "learning_rate": 0.00010555316267754096,
      "loss": 1.2118,
      "step": 2768
    },
    {
      "epoch": 0.3892872205820329,
      "grad_norm": 1.4167743921279907,
      "learning_rate": 0.00010578525823373217,
      "loss": 1.2118,
      "step": 2769
    },
    {
      "epoch": 0.38942780823843665,
      "grad_norm": 1.7858283519744873,
      "learning_rate": 0.00010601732252520873,
      "loss": 0.9912,
      "step": 2770
    },
    {
      "epoch": 0.38956839589484044,
      "grad_norm": 1.4761207103729248,
      "learning_rate": 0.00010624935429784783,
      "loss": 1.1684,
      "step": 2771
    },
    {
      "epoch": 0.3897089835512442,
      "grad_norm": 1.7772603034973145,
      "learning_rate": 0.00010648135229770246,
      "loss": 1.0862,
      "step": 2772
    },
    {
      "epoch": 0.38984957120764796,
      "grad_norm": 1.8215521574020386,
      "learning_rate": 0.00010671331527100827,
      "loss": 0.9245,
      "step": 2773
    },
    {
      "epoch": 0.38999015886405175,
      "grad_norm": 1.546992540359497,
      "learning_rate": 0.00010694524196419002,
      "loss": 1.2089,
      "step": 2774
    },
    {
      "epoch": 0.3901307465204555,
      "grad_norm": 1.312246561050415,
      "learning_rate": 0.00010717713112386858,
      "loss": 1.1017,
      "step": 2775
    },
    {
      "epoch": 0.3902713341768593,
      "grad_norm": 1.9738069772720337,
      "learning_rate": 0.00010740898149686756,
      "loss": 1.0586,
      "step": 2776
    },
    {
      "epoch": 0.39041192183326306,
      "grad_norm": 1.5985878705978394,
      "learning_rate": 0.00010764079183022047,
      "loss": 1.1733,
      "step": 2777
    },
    {
      "epoch": 0.3905525094896668,
      "grad_norm": 1.4716832637786865,
      "learning_rate": 0.00010787256087117692,
      "loss": 1.1007,
      "step": 2778
    },
    {
      "epoch": 0.3906930971460706,
      "grad_norm": 1.7397269010543823,
      "learning_rate": 0.00010810428736720981,
      "loss": 1.1342,
      "step": 2779
    },
    {
      "epoch": 0.3908336848024743,
      "grad_norm": 1.5310194492340088,
      "learning_rate": 0.00010833597006602181,
      "loss": 1.0981,
      "step": 2780
    },
    {
      "epoch": 0.3909742724588781,
      "grad_norm": 1.5887361764907837,
      "learning_rate": 0.00010856760771555257,
      "loss": 1.1661,
      "step": 2781
    },
    {
      "epoch": 0.3911148601152819,
      "grad_norm": 2.104175090789795,
      "learning_rate": 0.000108799199063985,
      "loss": 1.0713,
      "step": 2782
    },
    {
      "epoch": 0.39125544777168564,
      "grad_norm": 1.4610835313796997,
      "learning_rate": 0.00010903074285975202,
      "loss": 1.1687,
      "step": 2783
    },
    {
      "epoch": 0.39139603542808943,
      "grad_norm": 1.3477734327316284,
      "learning_rate": 0.00010926223785154401,
      "loss": 1.0126,
      "step": 2784
    },
    {
      "epoch": 0.39153662308449316,
      "grad_norm": 1.6047677993774414,
      "learning_rate": 0.00010949368278831466,
      "loss": 1.0636,
      "step": 2785
    },
    {
      "epoch": 0.39167721074089695,
      "grad_norm": 1.5342696905136108,
      "learning_rate": 0.00010972507641928865,
      "loss": 1.0178,
      "step": 2786
    },
    {
      "epoch": 0.39181779839730074,
      "grad_norm": 1.555497407913208,
      "learning_rate": 0.00010995641749396726,
      "loss": 1.242,
      "step": 2787
    },
    {
      "epoch": 0.3919583860537045,
      "grad_norm": 1.417351245880127,
      "learning_rate": 0.00011018770476213645,
      "loss": 1.0619,
      "step": 2788
    },
    {
      "epoch": 0.39209897371010827,
      "grad_norm": 1.3778603076934814,
      "learning_rate": 0.00011041893697387247,
      "loss": 1.1434,
      "step": 2789
    },
    {
      "epoch": 0.392239561366512,
      "grad_norm": 1.5204204320907593,
      "learning_rate": 0.00011065011287954963,
      "loss": 1.1391,
      "step": 2790
    },
    {
      "epoch": 0.3923801490229158,
      "grad_norm": 1.6635582447052002,
      "learning_rate": 0.00011088123122984582,
      "loss": 1.1301,
      "step": 2791
    },
    {
      "epoch": 0.3925207366793196,
      "grad_norm": 1.425424575805664,
      "learning_rate": 0.00011111229077575068,
      "loss": 1.0611,
      "step": 2792
    },
    {
      "epoch": 0.3926613243357233,
      "grad_norm": 1.5582612752914429,
      "learning_rate": 0.00011134329026857108,
      "loss": 1.2223,
      "step": 2793
    },
    {
      "epoch": 0.3928019119921271,
      "grad_norm": 1.5527281761169434,
      "learning_rate": 0.00011157422845993901,
      "loss": 1.0499,
      "step": 2794
    },
    {
      "epoch": 0.39294249964853084,
      "grad_norm": 1.488529920578003,
      "learning_rate": 0.00011180510410181701,
      "loss": 1.1541,
      "step": 2795
    },
    {
      "epoch": 0.39308308730493463,
      "grad_norm": 1.495755672454834,
      "learning_rate": 0.00011203591594650638,
      "loss": 0.9875,
      "step": 2796
    },
    {
      "epoch": 0.3932236749613384,
      "grad_norm": 1.6893550157546997,
      "learning_rate": 0.0001122666627466526,
      "loss": 1.153,
      "step": 2797
    },
    {
      "epoch": 0.39336426261774216,
      "grad_norm": 1.4782142639160156,
      "learning_rate": 0.0001124973432552533,
      "loss": 1.0843,
      "step": 2798
    },
    {
      "epoch": 0.39350485027414595,
      "grad_norm": 1.3603196144104004,
      "learning_rate": 0.00011272795622566354,
      "loss": 1.2425,
      "step": 2799
    },
    {
      "epoch": 0.3936454379305497,
      "grad_norm": 1.375249981880188,
      "learning_rate": 0.00011295850041160413,
      "loss": 1.0328,
      "step": 2800
    },
    {
      "epoch": 0.39378602558695347,
      "grad_norm": 1.6747864484786987,
      "learning_rate": 0.00011318897456716724,
      "loss": 1.1218,
      "step": 2801
    },
    {
      "epoch": 0.39392661324335726,
      "grad_norm": 1.708353877067566,
      "learning_rate": 0.00011341937744682352,
      "loss": 1.1157,
      "step": 2802
    },
    {
      "epoch": 0.394067200899761,
      "grad_norm": 1.5765055418014526,
      "learning_rate": 0.00011364970780542885,
      "loss": 1.1037,
      "step": 2803
    },
    {
      "epoch": 0.3942077885561648,
      "grad_norm": 1.6798044443130493,
      "learning_rate": 0.00011387996439823094,
      "loss": 1.0998,
      "step": 2804
    },
    {
      "epoch": 0.3943483762125685,
      "grad_norm": 1.4637730121612549,
      "learning_rate": 0.00011411014598087648,
      "loss": 1.1002,
      "step": 2805
    },
    {
      "epoch": 0.3944889638689723,
      "grad_norm": 1.451733112335205,
      "learning_rate": 0.00011434025130941697,
      "loss": 0.9097,
      "step": 2806
    },
    {
      "epoch": 0.3946295515253761,
      "grad_norm": 1.389222502708435,
      "learning_rate": 0.00011457027914031653,
      "loss": 1.1002,
      "step": 2807
    },
    {
      "epoch": 0.39477013918177983,
      "grad_norm": 1.5107685327529907,
      "learning_rate": 0.00011480022823045772,
      "loss": 1.1398,
      "step": 2808
    },
    {
      "epoch": 0.3949107268381836,
      "grad_norm": 1.5314667224884033,
      "learning_rate": 0.00011503009733714907,
      "loss": 0.9945,
      "step": 2809
    },
    {
      "epoch": 0.39505131449458736,
      "grad_norm": 1.6043691635131836,
      "learning_rate": 0.00011525988521813069,
      "loss": 1.1565,
      "step": 2810
    },
    {
      "epoch": 0.39519190215099115,
      "grad_norm": 1.3084238767623901,
      "learning_rate": 0.00011548959063158232,
      "loss": 1.2511,
      "step": 2811
    },
    {
      "epoch": 0.39533248980739494,
      "grad_norm": 1.4939817190170288,
      "learning_rate": 0.0001157192123361289,
      "loss": 1.0819,
      "step": 2812
    },
    {
      "epoch": 0.3954730774637987,
      "grad_norm": 1.4628314971923828,
      "learning_rate": 0.00011594874909084818,
      "loss": 1.1061,
      "step": 2813
    },
    {
      "epoch": 0.39561366512020246,
      "grad_norm": 1.4906792640686035,
      "learning_rate": 0.00011617819965527645,
      "loss": 1.1486,
      "step": 2814
    },
    {
      "epoch": 0.3957542527766062,
      "grad_norm": 1.513870120048523,
      "learning_rate": 0.00011640756278941632,
      "loss": 0.87,
      "step": 2815
    },
    {
      "epoch": 0.39589484043301,
      "grad_norm": 1.7726430892944336,
      "learning_rate": 0.00011663683725374248,
      "loss": 1.0281,
      "step": 2816
    },
    {
      "epoch": 0.3960354280894138,
      "grad_norm": 1.6841741800308228,
      "learning_rate": 0.00011686602180920927,
      "loss": 1.0919,
      "step": 2817
    },
    {
      "epoch": 0.3961760157458175,
      "grad_norm": 1.2849020957946777,
      "learning_rate": 0.00011709511521725631,
      "loss": 1.0851,
      "step": 2818
    },
    {
      "epoch": 0.3963166034022213,
      "grad_norm": 1.6976521015167236,
      "learning_rate": 0.00011732411623981635,
      "loss": 1.2005,
      "step": 2819
    },
    {
      "epoch": 0.39645719105862504,
      "grad_norm": 1.5611851215362549,
      "learning_rate": 0.00011755302363932111,
      "loss": 0.984,
      "step": 2820
    },
    {
      "epoch": 0.3965977787150288,
      "grad_norm": 1.326809287071228,
      "learning_rate": 0.00011778183617870838,
      "loss": 1.0749,
      "step": 2821
    },
    {
      "epoch": 0.3967383663714326,
      "grad_norm": 1.806330680847168,
      "learning_rate": 0.00011801055262142851,
      "loss": 1.1152,
      "step": 2822
    },
    {
      "epoch": 0.39687895402783635,
      "grad_norm": 1.7327831983566284,
      "learning_rate": 0.00011823917173145115,
      "loss": 1.1218,
      "step": 2823
    },
    {
      "epoch": 0.39701954168424014,
      "grad_norm": 1.6013824939727783,
      "learning_rate": 0.00011846769227327224,
      "loss": 1.0205,
      "step": 2824
    },
    {
      "epoch": 0.3971601293406439,
      "grad_norm": 1.423599362373352,
      "learning_rate": 0.00011869611301192007,
      "loss": 1.262,
      "step": 2825
    },
    {
      "epoch": 0.39730071699704766,
      "grad_norm": 1.236615777015686,
      "learning_rate": 0.00011892443271296243,
      "loss": 1.0432,
      "step": 2826
    },
    {
      "epoch": 0.3974413046534514,
      "grad_norm": 1.702130913734436,
      "learning_rate": 0.00011915265014251303,
      "loss": 1.147,
      "step": 2827
    },
    {
      "epoch": 0.3975818923098552,
      "grad_norm": 1.7817691564559937,
      "learning_rate": 0.00011938076406723853,
      "loss": 1.0417,
      "step": 2828
    },
    {
      "epoch": 0.397722479966259,
      "grad_norm": 1.3875641822814941,
      "learning_rate": 0.00011960877325436468,
      "loss": 0.9951,
      "step": 2829
    },
    {
      "epoch": 0.3978630676226627,
      "grad_norm": 1.4442225694656372,
      "learning_rate": 0.0001198366764716834,
      "loss": 1.012,
      "step": 2830
    },
    {
      "epoch": 0.3980036552790665,
      "grad_norm": 1.666203498840332,
      "learning_rate": 0.00012006447248755912,
      "loss": 1.0681,
      "step": 2831
    },
    {
      "epoch": 0.39814424293547024,
      "grad_norm": 1.6569268703460693,
      "learning_rate": 0.00012029216007093595,
      "loss": 1.1869,
      "step": 2832
    },
    {
      "epoch": 0.39828483059187403,
      "grad_norm": 1.4847407341003418,
      "learning_rate": 0.00012051973799134368,
      "loss": 1.072,
      "step": 2833
    },
    {
      "epoch": 0.3984254182482778,
      "grad_norm": 1.573314905166626,
      "learning_rate": 0.00012074720501890484,
      "loss": 1.0942,
      "step": 2834
    },
    {
      "epoch": 0.39856600590468155,
      "grad_norm": 1.7595103979110718,
      "learning_rate": 0.00012097455992434114,
      "loss": 1.0387,
      "step": 2835
    },
    {
      "epoch": 0.39870659356108534,
      "grad_norm": 1.5664292573928833,
      "learning_rate": 0.00012120180147898055,
      "loss": 1.1238,
      "step": 2836
    },
    {
      "epoch": 0.3988471812174891,
      "grad_norm": 1.731202483177185,
      "learning_rate": 0.00012142892845476328,
      "loss": 1.0974,
      "step": 2837
    },
    {
      "epoch": 0.39898776887389287,
      "grad_norm": 1.5997743606567383,
      "learning_rate": 0.00012165593962424886,
      "loss": 1.2444,
      "step": 2838
    },
    {
      "epoch": 0.39912835653029666,
      "grad_norm": 1.5531119108200073,
      "learning_rate": 0.00012188283376062258,
      "loss": 1.1331,
      "step": 2839
    },
    {
      "epoch": 0.3992689441867004,
      "grad_norm": 2.7208261489868164,
      "learning_rate": 0.00012210960963770243,
      "loss": 1.0357,
      "step": 2840
    },
    {
      "epoch": 0.3994095318431042,
      "grad_norm": 1.6400601863861084,
      "learning_rate": 0.00012233626602994535,
      "loss": 1.1569,
      "step": 2841
    },
    {
      "epoch": 0.3995501194995079,
      "grad_norm": 1.5690133571624756,
      "learning_rate": 0.00012256280171245377,
      "loss": 0.9446,
      "step": 2842
    },
    {
      "epoch": 0.3996907071559117,
      "grad_norm": 1.7000865936279297,
      "learning_rate": 0.00012278921546098292,
      "loss": 1.1804,
      "step": 2843
    },
    {
      "epoch": 0.3998312948123155,
      "grad_norm": 1.9064589738845825,
      "learning_rate": 0.00012301550605194666,
      "loss": 1.148,
      "step": 2844
    },
    {
      "epoch": 0.39997188246871923,
      "grad_norm": 2.013197660446167,
      "learning_rate": 0.00012324167226242456,
      "loss": 1.2414,
      "step": 2845
    },
    {
      "epoch": 0.400112470125123,
      "grad_norm": 1.5411672592163086,
      "learning_rate": 0.00012346771287016818,
      "loss": 1.102,
      "step": 2846
    },
    {
      "epoch": 0.40025305778152676,
      "grad_norm": 1.4576671123504639,
      "learning_rate": 0.0001236936266536082,
      "loss": 1.1733,
      "step": 2847
    },
    {
      "epoch": 0.40039364543793055,
      "grad_norm": 1.3971939086914062,
      "learning_rate": 0.0001239194123918604,
      "loss": 1.0915,
      "step": 2848
    },
    {
      "epoch": 0.40053423309433434,
      "grad_norm": 1.804437279701233,
      "learning_rate": 0.00012414506886473264,
      "loss": 1.1236,
      "step": 2849
    },
    {
      "epoch": 0.40067482075073807,
      "grad_norm": 1.6832995414733887,
      "learning_rate": 0.00012437059485273123,
      "loss": 1.1256,
      "step": 2850
    },
    {
      "epoch": 0.40081540840714186,
      "grad_norm": 1.412095308303833,
      "learning_rate": 0.00012459598913706796,
      "loss": 1.091,
      "step": 2851
    },
    {
      "epoch": 0.4009559960635456,
      "grad_norm": 1.6633363962173462,
      "learning_rate": 0.0001248212504996661,
      "loss": 1.0493,
      "step": 2852
    },
    {
      "epoch": 0.4010965837199494,
      "grad_norm": 1.4391154050827026,
      "learning_rate": 0.00012504637772316727,
      "loss": 1.0843,
      "step": 2853
    },
    {
      "epoch": 0.4012371713763532,
      "grad_norm": 1.5341330766677856,
      "learning_rate": 0.00012527136959093805,
      "loss": 1.0699,
      "step": 2854
    },
    {
      "epoch": 0.4013777590327569,
      "grad_norm": 1.4618231058120728,
      "learning_rate": 0.00012549622488707666,
      "loss": 1.0547,
      "step": 2855
    },
    {
      "epoch": 0.4015183466891607,
      "grad_norm": 1.6272594928741455,
      "learning_rate": 0.0001257209423964192,
      "loss": 1.0922,
      "step": 2856
    },
    {
      "epoch": 0.40165893434556443,
      "grad_norm": 1.420209527015686,
      "learning_rate": 0.0001259455209045464,
      "loss": 1.0182,
      "step": 2857
    },
    {
      "epoch": 0.4017995220019682,
      "grad_norm": 1.3456954956054688,
      "learning_rate": 0.0001261699591977902,
      "loss": 1.0612,
      "step": 2858
    },
    {
      "epoch": 0.401940109658372,
      "grad_norm": 1.637459635734558,
      "learning_rate": 0.00012639425606324043,
      "loss": 1.0711,
      "step": 2859
    },
    {
      "epoch": 0.40208069731477575,
      "grad_norm": 1.6655423641204834,
      "learning_rate": 0.0001266184102887511,
      "loss": 1.1509,
      "step": 2860
    },
    {
      "epoch": 0.40222128497117954,
      "grad_norm": 1.3559671640396118,
      "learning_rate": 0.00012684242066294692,
      "loss": 1.1669,
      "step": 2861
    },
    {
      "epoch": 0.4023618726275833,
      "grad_norm": 1.420451283454895,
      "learning_rate": 0.00012706628597523035,
      "loss": 1.0183,
      "step": 2862
    },
    {
      "epoch": 0.40250246028398706,
      "grad_norm": 1.342098593711853,
      "learning_rate": 0.00012729000501578744,
      "loss": 1.3323,
      "step": 2863
    },
    {
      "epoch": 0.40264304794039085,
      "grad_norm": 1.6255697011947632,
      "learning_rate": 0.00012751357657559518,
      "loss": 1.209,
      "step": 2864
    },
    {
      "epoch": 0.4027836355967946,
      "grad_norm": 1.3923523426055908,
      "learning_rate": 0.00012773699944642694,
      "loss": 0.9507,
      "step": 2865
    },
    {
      "epoch": 0.4029242232531984,
      "grad_norm": 1.8647217750549316,
      "learning_rate": 0.00012796027242086027,
      "loss": 1.0069,
      "step": 2866
    },
    {
      "epoch": 0.4030648109096021,
      "grad_norm": 1.5726319551467896,
      "learning_rate": 0.00012818339429228228,
      "loss": 1.0048,
      "step": 2867
    },
    {
      "epoch": 0.4032053985660059,
      "grad_norm": 1.6928491592407227,
      "learning_rate": 0.0001284063638548972,
      "loss": 1.0711,
      "step": 2868
    },
    {
      "epoch": 0.4033459862224097,
      "grad_norm": 1.3873882293701172,
      "learning_rate": 0.0001286291799037317,
      "loss": 1.0502,
      "step": 2869
    },
    {
      "epoch": 0.4034865738788134,
      "grad_norm": 1.609147310256958,
      "learning_rate": 0.0001288518412346427,
      "loss": 1.1799,
      "step": 2870
    },
    {
      "epoch": 0.4036271615352172,
      "grad_norm": 2.014526128768921,
      "learning_rate": 0.00012907434664432286,
      "loss": 1.1634,
      "step": 2871
    },
    {
      "epoch": 0.40376774919162095,
      "grad_norm": 1.588838815689087,
      "learning_rate": 0.00012929669493030776,
      "loss": 1.1438,
      "step": 2872
    },
    {
      "epoch": 0.40390833684802474,
      "grad_norm": 1.5564947128295898,
      "learning_rate": 0.00012951888489098169,
      "loss": 1.1072,
      "step": 2873
    },
    {
      "epoch": 0.40404892450442853,
      "grad_norm": 1.3501005172729492,
      "learning_rate": 0.00012974091532558495,
      "loss": 1.0458,
      "step": 2874
    },
    {
      "epoch": 0.40418951216083226,
      "grad_norm": 1.6562769412994385,
      "learning_rate": 0.0001299627850342198,
      "loss": 1.099,
      "step": 2875
    },
    {
      "epoch": 0.40433009981723605,
      "grad_norm": 1.8822752237319946,
      "learning_rate": 0.00013018449281785725,
      "loss": 0.9899,
      "step": 2876
    },
    {
      "epoch": 0.4044706874736398,
      "grad_norm": 1.6780750751495361,
      "learning_rate": 0.00013040603747834295,
      "loss": 1.0572,
      "step": 2877
    },
    {
      "epoch": 0.4046112751300436,
      "grad_norm": 1.299143671989441,
      "learning_rate": 0.00013062741781840465,
      "loss": 1.3015,
      "step": 2878
    },
    {
      "epoch": 0.40475186278644737,
      "grad_norm": 1.4591505527496338,
      "learning_rate": 0.0001308486326416578,
      "loss": 1.0704,
      "step": 2879
    },
    {
      "epoch": 0.4048924504428511,
      "grad_norm": 1.6327036619186401,
      "learning_rate": 0.00013106968075261236,
      "loss": 1.2162,
      "step": 2880
    },
    {
      "epoch": 0.4050330380992549,
      "grad_norm": 1.654765009880066,
      "learning_rate": 0.00013129056095667933,
      "loss": 1.0945,
      "step": 2881
    },
    {
      "epoch": 0.40517362575565863,
      "grad_norm": 1.3370532989501953,
      "learning_rate": 0.00013151127206017694,
      "loss": 1.1783,
      "step": 2882
    },
    {
      "epoch": 0.4053142134120624,
      "grad_norm": 1.607530951499939,
      "learning_rate": 0.00013173181287033772,
      "loss": 1.074,
      "step": 2883
    },
    {
      "epoch": 0.4054548010684662,
      "grad_norm": 1.4095925092697144,
      "learning_rate": 0.00013195218219531375,
      "loss": 1.2326,
      "step": 2884
    },
    {
      "epoch": 0.40559538872486994,
      "grad_norm": 1.7238799333572388,
      "learning_rate": 0.00013217237884418463,
      "loss": 1.0588,
      "step": 2885
    },
    {
      "epoch": 0.40573597638127373,
      "grad_norm": 1.558576226234436,
      "learning_rate": 0.00013239240162696254,
      "loss": 1.1278,
      "step": 2886
    },
    {
      "epoch": 0.40587656403767747,
      "grad_norm": 1.8359451293945312,
      "learning_rate": 0.00013261224935459982,
      "loss": 1.0794,
      "step": 2887
    },
    {
      "epoch": 0.40601715169408126,
      "grad_norm": 1.313831090927124,
      "learning_rate": 0.00013283192083899417,
      "loss": 1.2111,
      "step": 2888
    },
    {
      "epoch": 0.40615773935048505,
      "grad_norm": 1.7834773063659668,
      "learning_rate": 0.00013305141489299637,
      "loss": 0.9836,
      "step": 2889
    },
    {
      "epoch": 0.4062983270068888,
      "grad_norm": 1.4945993423461914,
      "learning_rate": 0.00013327073033041567,
      "loss": 1.1248,
      "step": 2890
    },
    {
      "epoch": 0.40643891466329257,
      "grad_norm": 1.5489299297332764,
      "learning_rate": 0.00013348986596602698,
      "loss": 1.1626,
      "step": 2891
    },
    {
      "epoch": 0.4065795023196963,
      "grad_norm": 1.428887963294983,
      "learning_rate": 0.0001337088206155763,
      "loss": 1.2761,
      "step": 2892
    },
    {
      "epoch": 0.4067200899761001,
      "grad_norm": 1.4626760482788086,
      "learning_rate": 0.00013392759309578838,
      "loss": 1.2295,
      "step": 2893
    },
    {
      "epoch": 0.4068606776325039,
      "grad_norm": 1.5018242597579956,
      "learning_rate": 0.00013414618222437193,
      "loss": 0.9411,
      "step": 2894
    },
    {
      "epoch": 0.4070012652889076,
      "grad_norm": 1.4080866575241089,
      "learning_rate": 0.000134364586820027,
      "loss": 1.0266,
      "step": 2895
    },
    {
      "epoch": 0.4071418529453114,
      "grad_norm": 1.7509691715240479,
      "learning_rate": 0.00013458280570245035,
      "loss": 0.9685,
      "step": 2896
    },
    {
      "epoch": 0.40728244060171515,
      "grad_norm": 1.6060086488723755,
      "learning_rate": 0.00013480083769234287,
      "loss": 1.0531,
      "step": 2897
    },
    {
      "epoch": 0.40742302825811894,
      "grad_norm": 1.5436334609985352,
      "learning_rate": 0.00013501868161141514,
      "loss": 1.0758,
      "step": 2898
    },
    {
      "epoch": 0.4075636159145227,
      "grad_norm": 1.502274513244629,
      "learning_rate": 0.0001352363362823943,
      "loss": 1.1289,
      "step": 2899
    },
    {
      "epoch": 0.40770420357092646,
      "grad_norm": 1.5212819576263428,
      "learning_rate": 0.00013545380052903006,
      "loss": 1.1647,
      "step": 2900
    },
    {
      "epoch": 0.40784479122733025,
      "grad_norm": 1.7440624237060547,
      "learning_rate": 0.00013567107317610127,
      "loss": 1.1144,
      "step": 2901
    },
    {
      "epoch": 0.407985378883734,
      "grad_norm": 1.4481451511383057,
      "learning_rate": 0.0001358881530494224,
      "loss": 1.1747,
      "step": 2902
    },
    {
      "epoch": 0.4081259665401378,
      "grad_norm": 1.5458532571792603,
      "learning_rate": 0.00013610503897584948,
      "loss": 1.1451,
      "step": 2903
    },
    {
      "epoch": 0.40826655419654156,
      "grad_norm": 1.6322726011276245,
      "learning_rate": 0.00013632172978328685,
      "loss": 1.0914,
      "step": 2904
    },
    {
      "epoch": 0.4084071418529453,
      "grad_norm": 1.4892418384552002,
      "learning_rate": 0.000136538224300693,
      "loss": 0.9638,
      "step": 2905
    },
    {
      "epoch": 0.4085477295093491,
      "grad_norm": 1.4691433906555176,
      "learning_rate": 0.00013675452135808767,
      "loss": 1.0652,
      "step": 2906
    },
    {
      "epoch": 0.4086883171657528,
      "grad_norm": 1.6278977394104004,
      "learning_rate": 0.00013697061978655737,
      "loss": 0.9694,
      "step": 2907
    },
    {
      "epoch": 0.4088289048221566,
      "grad_norm": 1.4346688985824585,
      "learning_rate": 0.00013718651841826215,
      "loss": 1.0892,
      "step": 2908
    },
    {
      "epoch": 0.4089694924785604,
      "grad_norm": 1.728352665901184,
      "learning_rate": 0.00013740221608644175,
      "loss": 1.065,
      "step": 2909
    },
    {
      "epoch": 0.40911008013496414,
      "grad_norm": 1.300907850265503,
      "learning_rate": 0.00013761771162542212,
      "loss": 1.1333,
      "step": 2910
    },
    {
      "epoch": 0.40925066779136793,
      "grad_norm": 1.6274948120117188,
      "learning_rate": 0.0001378330038706214,
      "loss": 1.0331,
      "step": 2911
    },
    {
      "epoch": 0.40939125544777166,
      "grad_norm": 1.4976552724838257,
      "learning_rate": 0.00013804809165855639,
      "loss": 1.2377,
      "step": 2912
    },
    {
      "epoch": 0.40953184310417545,
      "grad_norm": 1.7361762523651123,
      "learning_rate": 0.00013826297382684878,
      "loss": 1.1758,
      "step": 2913
    },
    {
      "epoch": 0.40967243076057924,
      "grad_norm": 1.5712218284606934,
      "learning_rate": 0.00013847764921423169,
      "loss": 1.0705,
      "step": 2914
    },
    {
      "epoch": 0.409813018416983,
      "grad_norm": 1.412294626235962,
      "learning_rate": 0.00013869211666055545,
      "loss": 1.0501,
      "step": 2915
    },
    {
      "epoch": 0.40995360607338677,
      "grad_norm": 1.6149150133132935,
      "learning_rate": 0.00013890637500679434,
      "loss": 1.1581,
      "step": 2916
    },
    {
      "epoch": 0.4100941937297905,
      "grad_norm": 1.4340790510177612,
      "learning_rate": 0.00013912042309505244,
      "loss": 1.011,
      "step": 2917
    },
    {
      "epoch": 0.4102347813861943,
      "grad_norm": 1.5137197971343994,
      "learning_rate": 0.00013933425976857052,
      "loss": 1.1836,
      "step": 2918
    },
    {
      "epoch": 0.4103753690425981,
      "grad_norm": 1.4428815841674805,
      "learning_rate": 0.00013954788387173138,
      "loss": 1.0942,
      "step": 2919
    },
    {
      "epoch": 0.4105159566990018,
      "grad_norm": 1.600741982460022,
      "learning_rate": 0.00013976129425006682,
      "loss": 1.0992,
      "step": 2920
    },
    {
      "epoch": 0.4106565443554056,
      "grad_norm": 1.9680043458938599,
      "learning_rate": 0.00013997448975026382,
      "loss": 1.1282,
      "step": 2921
    },
    {
      "epoch": 0.41079713201180934,
      "grad_norm": 1.424501657485962,
      "learning_rate": 0.00014018746922017035,
      "loss": 1.0927,
      "step": 2922
    },
    {
      "epoch": 0.41093771966821313,
      "grad_norm": 1.5407642126083374,
      "learning_rate": 0.00014040023150880193,
      "loss": 1.1138,
      "step": 2923
    },
    {
      "epoch": 0.4110783073246169,
      "grad_norm": 1.5971639156341553,
      "learning_rate": 0.0001406127754663477,
      "loss": 1.2358,
      "step": 2924
    },
    {
      "epoch": 0.41121889498102066,
      "grad_norm": 1.3560789823532104,
      "learning_rate": 0.00014082509994417703,
      "loss": 1.1417,
      "step": 2925
    },
    {
      "epoch": 0.41135948263742445,
      "grad_norm": 1.566992998123169,
      "learning_rate": 0.00014103720379484498,
      "loss": 1.1302,
      "step": 2926
    },
    {
      "epoch": 0.4115000702938282,
      "grad_norm": 1.4654635190963745,
      "learning_rate": 0.00014124908587209923,
      "loss": 0.9713,
      "step": 2927
    },
    {
      "epoch": 0.41164065795023197,
      "grad_norm": 1.574826717376709,
      "learning_rate": 0.0001414607450308856,
      "loss": 1.1673,
      "step": 2928
    },
    {
      "epoch": 0.41178124560663576,
      "grad_norm": 1.8358408212661743,
      "learning_rate": 0.0001416721801273552,
      "loss": 1.1911,
      "step": 2929
    },
    {
      "epoch": 0.4119218332630395,
      "grad_norm": 1.839680552482605,
      "learning_rate": 0.00014188339001886942,
      "loss": 1.079,
      "step": 2930
    },
    {
      "epoch": 0.4120624209194433,
      "grad_norm": 1.4203612804412842,
      "learning_rate": 0.0001420943735640071,
      "loss": 1.1397,
      "step": 2931
    },
    {
      "epoch": 0.412203008575847,
      "grad_norm": 1.441125750541687,
      "learning_rate": 0.00014230512962257,
      "loss": 0.9713,
      "step": 2932
    },
    {
      "epoch": 0.4123435962322508,
      "grad_norm": 1.3836162090301514,
      "learning_rate": 0.00014251565705558958,
      "loss": 1.1895,
      "step": 2933
    },
    {
      "epoch": 0.4124841838886546,
      "grad_norm": 1.5409700870513916,
      "learning_rate": 0.0001427259547253326,
      "loss": 0.8422,
      "step": 2934
    },
    {
      "epoch": 0.41262477154505833,
      "grad_norm": 1.5604654550552368,
      "learning_rate": 0.00014293602149530765,
      "loss": 0.9874,
      "step": 2935
    },
    {
      "epoch": 0.4127653592014621,
      "grad_norm": 1.4251446723937988,
      "learning_rate": 0.00014314585623027094,
      "loss": 1.1511,
      "step": 2936
    },
    {
      "epoch": 0.41290594685786586,
      "grad_norm": 1.5880742073059082,
      "learning_rate": 0.000143355457796233,
      "loss": 1.1298,
      "step": 2937
    },
    {
      "epoch": 0.41304653451426965,
      "grad_norm": 1.6004259586334229,
      "learning_rate": 0.0001435648250604642,
      "loss": 1.0671,
      "step": 2938
    },
    {
      "epoch": 0.41318712217067344,
      "grad_norm": 1.5575200319290161,
      "learning_rate": 0.00014377395689150094,
      "loss": 1.1502,
      "step": 2939
    },
    {
      "epoch": 0.41332770982707717,
      "grad_norm": 1.8404197692871094,
      "learning_rate": 0.00014398285215915245,
      "loss": 1.0864,
      "step": 2940
    },
    {
      "epoch": 0.41346829748348096,
      "grad_norm": 1.7299280166625977,
      "learning_rate": 0.00014419150973450596,
      "loss": 1.0222,
      "step": 2941
    },
    {
      "epoch": 0.4136088851398847,
      "grad_norm": 1.6255651712417603,
      "learning_rate": 0.00014439992848993362,
      "loss": 1.1334,
      "step": 2942
    },
    {
      "epoch": 0.4137494727962885,
      "grad_norm": 1.5738003253936768,
      "learning_rate": 0.00014460810729909768,
      "loss": 1.2508,
      "step": 2943
    },
    {
      "epoch": 0.4138900604526923,
      "grad_norm": 1.6456865072250366,
      "learning_rate": 0.00014481604503695765,
      "loss": 1.0012,
      "step": 2944
    },
    {
      "epoch": 0.414030648109096,
      "grad_norm": 1.5446182489395142,
      "learning_rate": 0.0001450237405797755,
      "loss": 1.0496,
      "step": 2945
    },
    {
      "epoch": 0.4141712357654998,
      "grad_norm": 1.515960931777954,
      "learning_rate": 0.00014523119280512235,
      "loss": 1.0741,
      "step": 2946
    },
    {
      "epoch": 0.41431182342190354,
      "grad_norm": 1.477173089981079,
      "learning_rate": 0.00014543840059188384,
      "loss": 1.0405,
      "step": 2947
    },
    {
      "epoch": 0.4144524110783073,
      "grad_norm": 1.5881997346878052,
      "learning_rate": 0.00014564536282026707,
      "loss": 1.1181,
      "step": 2948
    },
    {
      "epoch": 0.4145929987347111,
      "grad_norm": 1.6715365648269653,
      "learning_rate": 0.0001458520783718058,
      "loss": 1.0741,
      "step": 2949
    },
    {
      "epoch": 0.41473358639111485,
      "grad_norm": 1.6618218421936035,
      "learning_rate": 0.00014605854612936728,
      "loss": 1.0791,
      "step": 2950
    },
    {
      "epoch": 0.41487417404751864,
      "grad_norm": 1.33306884765625,
      "learning_rate": 0.0001462647649771574,
      "loss": 1.127,
      "step": 2951
    },
    {
      "epoch": 0.4150147617039224,
      "grad_norm": 1.7019565105438232,
      "learning_rate": 0.00014647073380072767,
      "loss": 1.0308,
      "step": 2952
    },
    {
      "epoch": 0.41515534936032616,
      "grad_norm": 1.4814045429229736,
      "learning_rate": 0.00014667645148698047,
      "loss": 1.1589,
      "step": 2953
    },
    {
      "epoch": 0.41529593701672995,
      "grad_norm": 1.4768359661102295,
      "learning_rate": 0.00014688191692417566,
      "loss": 1.0956,
      "step": 2954
    },
    {
      "epoch": 0.4154365246731337,
      "grad_norm": 1.5776194334030151,
      "learning_rate": 0.00014708712900193585,
      "loss": 1.0253,
      "step": 2955
    },
    {
      "epoch": 0.4155771123295375,
      "grad_norm": 1.6728999614715576,
      "learning_rate": 0.00014729208661125345,
      "loss": 1.1096,
      "step": 2956
    },
    {
      "epoch": 0.4157176999859412,
      "grad_norm": 1.5672919750213623,
      "learning_rate": 0.00014749678864449565,
      "loss": 1.1028,
      "step": 2957
    },
    {
      "epoch": 0.415858287642345,
      "grad_norm": 1.6628248691558838,
      "learning_rate": 0.00014770123399541078,
      "loss": 1.0709,
      "step": 2958
    },
    {
      "epoch": 0.4159988752987488,
      "grad_norm": 1.4324936866760254,
      "learning_rate": 0.00014790542155913473,
      "loss": 1.0271,
      "step": 2959
    },
    {
      "epoch": 0.41613946295515253,
      "grad_norm": 1.852441430091858,
      "learning_rate": 0.00014810935023219608,
      "loss": 1.0764,
      "step": 2960
    },
    {
      "epoch": 0.4162800506115563,
      "grad_norm": 1.4862960577011108,
      "learning_rate": 0.00014831301891252298,
      "loss": 1.0403,
      "step": 2961
    },
    {
      "epoch": 0.41642063826796005,
      "grad_norm": 1.65050208568573,
      "learning_rate": 0.00014851642649944805,
      "loss": 0.9827,
      "step": 2962
    },
    {
      "epoch": 0.41656122592436384,
      "grad_norm": 1.494929552078247,
      "learning_rate": 0.00014871957189371545,
      "loss": 1.2136,
      "step": 2963
    },
    {
      "epoch": 0.41670181358076763,
      "grad_norm": 1.961538314819336,
      "learning_rate": 0.00014892245399748596,
      "loss": 1.0617,
      "step": 2964
    },
    {
      "epoch": 0.41684240123717137,
      "grad_norm": 1.4828922748565674,
      "learning_rate": 0.00014912507171434354,
      "loss": 1.2917,
      "step": 2965
    },
    {
      "epoch": 0.41698298889357516,
      "grad_norm": 1.38355553150177,
      "learning_rate": 0.0001493274239493004,
      "loss": 1.239,
      "step": 2966
    },
    {
      "epoch": 0.4171235765499789,
      "grad_norm": 1.7497987747192383,
      "learning_rate": 0.0001495295096088041,
      "loss": 0.9614,
      "step": 2967
    },
    {
      "epoch": 0.4172641642063827,
      "grad_norm": 1.6070762872695923,
      "learning_rate": 0.00014973132760074238,
      "loss": 1.1154,
      "step": 2968
    },
    {
      "epoch": 0.41740475186278647,
      "grad_norm": 1.5242149829864502,
      "learning_rate": 0.0001499328768344499,
      "loss": 0.9579,
      "step": 2969
    },
    {
      "epoch": 0.4175453395191902,
      "grad_norm": 1.4175968170166016,
      "learning_rate": 0.00015013415622071321,
      "loss": 1.0251,
      "step": 2970
    },
    {
      "epoch": 0.417685927175594,
      "grad_norm": 1.5839974880218506,
      "learning_rate": 0.0001503351646717777,
      "loss": 1.0758,
      "step": 2971
    },
    {
      "epoch": 0.41782651483199773,
      "grad_norm": 1.546884536743164,
      "learning_rate": 0.00015053590110135261,
      "loss": 1.0589,
      "step": 2972
    },
    {
      "epoch": 0.4179671024884015,
      "grad_norm": 1.4209792613983154,
      "learning_rate": 0.00015073636442461757,
      "loss": 0.9951,
      "step": 2973
    },
    {
      "epoch": 0.4181076901448053,
      "grad_norm": 1.4988601207733154,
      "learning_rate": 0.0001509365535582276,
      "loss": 1.072,
      "step": 2974
    },
    {
      "epoch": 0.41824827780120905,
      "grad_norm": 1.4333910942077637,
      "learning_rate": 0.00015113646742032007,
      "loss": 1.258,
      "step": 2975
    },
    {
      "epoch": 0.41838886545761284,
      "grad_norm": 1.5245790481567383,
      "learning_rate": 0.00015133610493051967,
      "loss": 1.0192,
      "step": 2976
    },
    {
      "epoch": 0.41852945311401657,
      "grad_norm": 1.6075663566589355,
      "learning_rate": 0.00015153546500994456,
      "loss": 1.209,
      "step": 2977
    },
    {
      "epoch": 0.41867004077042036,
      "grad_norm": 1.5810633897781372,
      "learning_rate": 0.00015173454658121225,
      "loss": 1.0476,
      "step": 2978
    },
    {
      "epoch": 0.41881062842682415,
      "grad_norm": 1.565464973449707,
      "learning_rate": 0.00015193334856844522,
      "loss": 1.0102,
      "step": 2979
    },
    {
      "epoch": 0.4189512160832279,
      "grad_norm": 1.355522871017456,
      "learning_rate": 0.00015213186989727727,
      "loss": 1.0545,
      "step": 2980
    },
    {
      "epoch": 0.4190918037396317,
      "grad_norm": 1.5387210845947266,
      "learning_rate": 0.00015233010949485855,
      "loss": 1.1499,
      "step": 2981
    },
    {
      "epoch": 0.4192323913960354,
      "grad_norm": 1.5735410451889038,
      "learning_rate": 0.00015252806628986184,
      "loss": 1.0572,
      "step": 2982
    },
    {
      "epoch": 0.4193729790524392,
      "grad_norm": 1.4129856824874878,
      "learning_rate": 0.00015272573921248822,
      "loss": 1.0532,
      "step": 2983
    },
    {
      "epoch": 0.419513566708843,
      "grad_norm": 1.6190142631530762,
      "learning_rate": 0.00015292312719447307,
      "loss": 1.1817,
      "step": 2984
    },
    {
      "epoch": 0.4196541543652467,
      "grad_norm": 1.4294264316558838,
      "learning_rate": 0.00015312022916909144,
      "loss": 1.1697,
      "step": 2985
    },
    {
      "epoch": 0.4197947420216505,
      "grad_norm": 1.5456002950668335,
      "learning_rate": 0.00015331704407116407,
      "loss": 1.0864,
      "step": 2986
    },
    {
      "epoch": 0.41993532967805425,
      "grad_norm": 2.660444974899292,
      "learning_rate": 0.00015351357083706304,
      "loss": 1.1426,
      "step": 2987
    },
    {
      "epoch": 0.42007591733445804,
      "grad_norm": 1.4615892171859741,
      "learning_rate": 0.00015370980840471782,
      "loss": 1.0267,
      "step": 2988
    },
    {
      "epoch": 0.4202165049908618,
      "grad_norm": 1.3637720346450806,
      "learning_rate": 0.00015390575571362048,
      "loss": 1.117,
      "step": 2989
    },
    {
      "epoch": 0.42035709264726556,
      "grad_norm": 1.4376306533813477,
      "learning_rate": 0.00015410141170483187,
      "loss": 1.2871,
      "step": 2990
    },
    {
      "epoch": 0.42049768030366935,
      "grad_norm": 1.4531259536743164,
      "learning_rate": 0.00015429677532098702,
      "loss": 1.043,
      "step": 2991
    },
    {
      "epoch": 0.4206382679600731,
      "grad_norm": 1.671582579612732,
      "learning_rate": 0.0001544918455063013,
      "loss": 1.168,
      "step": 2992
    },
    {
      "epoch": 0.4207788556164769,
      "grad_norm": 1.9900532960891724,
      "learning_rate": 0.0001546866212065756,
      "loss": 1.0291,
      "step": 2993
    },
    {
      "epoch": 0.42091944327288067,
      "grad_norm": 1.4720362424850464,
      "learning_rate": 0.00015488110136920228,
      "loss": 1.144,
      "step": 2994
    },
    {
      "epoch": 0.4210600309292844,
      "grad_norm": 1.6281218528747559,
      "learning_rate": 0.0001550752849431709,
      "loss": 1.1344,
      "step": 2995
    },
    {
      "epoch": 0.4212006185856882,
      "grad_norm": 1.6025186777114868,
      "learning_rate": 0.00015526917087907405,
      "loss": 1.0855,
      "step": 2996
    },
    {
      "epoch": 0.4213412062420919,
      "grad_norm": 1.5312020778656006,
      "learning_rate": 0.00015546275812911237,
      "loss": 1.0624,
      "step": 2997
    },
    {
      "epoch": 0.4214817938984957,
      "grad_norm": 1.6187868118286133,
      "learning_rate": 0.0001556560456471009,
      "loss": 1.1644,
      "step": 2998
    },
    {
      "epoch": 0.4216223815548995,
      "grad_norm": 1.5114648342132568,
      "learning_rate": 0.00015584903238847467,
      "loss": 1.0112,
      "step": 2999
    },
    {
      "epoch": 0.42176296921130324,
      "grad_norm": 1.5982730388641357,
      "learning_rate": 0.0001560417173102939,
      "loss": 1.0994,
      "step": 3000
    },
    {
      "epoch": 0.42176296921130324,
      "eval_loss": 1.1572028398513794,
      "eval_runtime": 771.8801,
      "eval_samples_per_second": 16.383,
      "eval_steps_per_second": 8.192,
      "step": 3000
    },
    {
      "epoch": 0.42190355686770703,
      "grad_norm": 1.5199434757232666,
      "learning_rate": 0.00015623409937125,
      "loss": 1.1533,
      "step": 3001
    },
    {
      "epoch": 0.42204414452411076,
      "grad_norm": 1.4094321727752686,
      "learning_rate": 0.00015642617753167104,
      "loss": 1.0087,
      "step": 3002
    },
    {
      "epoch": 0.42218473218051455,
      "grad_norm": 1.625590443611145,
      "learning_rate": 0.00015661795075352762,
      "loss": 1.0713,
      "step": 3003
    },
    {
      "epoch": 0.42232531983691834,
      "grad_norm": 1.6807918548583984,
      "learning_rate": 0.0001568094180004381,
      "loss": 0.9579,
      "step": 3004
    },
    {
      "epoch": 0.4224659074933221,
      "grad_norm": 1.6047941446304321,
      "learning_rate": 0.00015700057823767445,
      "loss": 1.2268,
      "step": 3005
    },
    {
      "epoch": 0.42260649514972587,
      "grad_norm": 1.5363056659698486,
      "learning_rate": 0.00015719143043216768,
      "loss": 0.9944,
      "step": 3006
    },
    {
      "epoch": 0.4227470828061296,
      "grad_norm": 1.4162005186080933,
      "learning_rate": 0.00015738197355251386,
      "loss": 1.0533,
      "step": 3007
    },
    {
      "epoch": 0.4228876704625334,
      "grad_norm": 1.8770489692687988,
      "learning_rate": 0.00015757220656897896,
      "loss": 1.0291,
      "step": 3008
    },
    {
      "epoch": 0.4230282581189372,
      "grad_norm": 1.4308953285217285,
      "learning_rate": 0.00015776212845350503,
      "loss": 1.1729,
      "step": 3009
    },
    {
      "epoch": 0.4231688457753409,
      "grad_norm": 1.8837121725082397,
      "learning_rate": 0.0001579517381797154,
      "loss": 0.9711,
      "step": 3010
    },
    {
      "epoch": 0.4233094334317447,
      "grad_norm": 1.2511197328567505,
      "learning_rate": 0.00015814103472292068,
      "loss": 1.1842,
      "step": 3011
    },
    {
      "epoch": 0.42345002108814844,
      "grad_norm": 1.6152673959732056,
      "learning_rate": 0.00015833001706012362,
      "loss": 1.2408,
      "step": 3012
    },
    {
      "epoch": 0.42359060874455223,
      "grad_norm": 1.347214698791504,
      "learning_rate": 0.00015851868417002512,
      "loss": 1.2525,
      "step": 3013
    },
    {
      "epoch": 0.423731196400956,
      "grad_norm": 1.5896176099777222,
      "learning_rate": 0.0001587070350330297,
      "loss": 0.9658,
      "step": 3014
    },
    {
      "epoch": 0.42387178405735976,
      "grad_norm": 1.7637876272201538,
      "learning_rate": 0.00015889506863125095,
      "loss": 1.0555,
      "step": 3015
    },
    {
      "epoch": 0.42401237171376355,
      "grad_norm": 1.6514075994491577,
      "learning_rate": 0.00015908278394851706,
      "loss": 1.1692,
      "step": 3016
    },
    {
      "epoch": 0.4241529593701673,
      "grad_norm": 1.486755132675171,
      "learning_rate": 0.00015927017997037593,
      "loss": 1.0373,
      "step": 3017
    },
    {
      "epoch": 0.42429354702657107,
      "grad_norm": 1.7084097862243652,
      "learning_rate": 0.00015945725568410147,
      "loss": 1.0924,
      "step": 3018
    },
    {
      "epoch": 0.42443413468297486,
      "grad_norm": 1.3989841938018799,
      "learning_rate": 0.00015964401007869826,
      "loss": 1.2461,
      "step": 3019
    },
    {
      "epoch": 0.4245747223393786,
      "grad_norm": 1.4139690399169922,
      "learning_rate": 0.00015983044214490767,
      "loss": 1.1871,
      "step": 3020
    },
    {
      "epoch": 0.4247153099957824,
      "grad_norm": 1.5244067907333374,
      "learning_rate": 0.00016001655087521263,
      "loss": 1.0598,
      "step": 3021
    },
    {
      "epoch": 0.4248558976521861,
      "grad_norm": 1.736275553703308,
      "learning_rate": 0.00016020233526384373,
      "loss": 0.9674,
      "step": 3022
    },
    {
      "epoch": 0.4249964853085899,
      "grad_norm": 1.4941325187683105,
      "learning_rate": 0.0001603877943067842,
      "loss": 1.03,
      "step": 3023
    },
    {
      "epoch": 0.4251370729649937,
      "grad_norm": 1.5721123218536377,
      "learning_rate": 0.00016057292700177574,
      "loss": 0.958,
      "step": 3024
    },
    {
      "epoch": 0.42527766062139744,
      "grad_norm": 1.823833703994751,
      "learning_rate": 0.00016075773234832324,
      "loss": 1.2836,
      "step": 3025
    },
    {
      "epoch": 0.4254182482778012,
      "grad_norm": 1.4654356241226196,
      "learning_rate": 0.0001609422093477012,
      "loss": 1.0804,
      "step": 3026
    },
    {
      "epoch": 0.42555883593420496,
      "grad_norm": 1.8365566730499268,
      "learning_rate": 0.0001611263570029581,
      "loss": 1.0859,
      "step": 3027
    },
    {
      "epoch": 0.42569942359060875,
      "grad_norm": 1.6284703016281128,
      "learning_rate": 0.00016131017431892278,
      "loss": 1.1184,
      "step": 3028
    },
    {
      "epoch": 0.42584001124701254,
      "grad_norm": 1.611696720123291,
      "learning_rate": 0.00016149366030220866,
      "loss": 1.1583,
      "step": 3029
    },
    {
      "epoch": 0.4259805989034163,
      "grad_norm": 1.577688217163086,
      "learning_rate": 0.00016167681396122029,
      "loss": 1.2137,
      "step": 3030
    },
    {
      "epoch": 0.42612118655982006,
      "grad_norm": 1.4833835363388062,
      "learning_rate": 0.00016185963430615785,
      "loss": 1.0918,
      "step": 3031
    },
    {
      "epoch": 0.4262617742162238,
      "grad_norm": 1.4100712537765503,
      "learning_rate": 0.00016204212034902314,
      "loss": 1.1324,
      "step": 3032
    },
    {
      "epoch": 0.4264023618726276,
      "grad_norm": 1.4558285474777222,
      "learning_rate": 0.00016222427110362407,
      "loss": 1.1077,
      "step": 3033
    },
    {
      "epoch": 0.4265429495290313,
      "grad_norm": 1.5927988290786743,
      "learning_rate": 0.0001624060855855811,
      "loss": 1.2149,
      "step": 3034
    },
    {
      "epoch": 0.4266835371854351,
      "grad_norm": 1.3322768211364746,
      "learning_rate": 0.00016258756281233169,
      "loss": 1.2117,
      "step": 3035
    },
    {
      "epoch": 0.4268241248418389,
      "grad_norm": 1.5151448249816895,
      "learning_rate": 0.0001627687018031357,
      "loss": 1.1246,
      "step": 3036
    },
    {
      "epoch": 0.42696471249824264,
      "grad_norm": 1.7026020288467407,
      "learning_rate": 0.00016294950157908132,
      "loss": 1.063,
      "step": 3037
    },
    {
      "epoch": 0.42710530015464643,
      "grad_norm": 1.6033897399902344,
      "learning_rate": 0.00016312996116308955,
      "loss": 1.0555,
      "step": 3038
    },
    {
      "epoch": 0.42724588781105016,
      "grad_norm": 1.4728339910507202,
      "learning_rate": 0.00016331007957992027,
      "loss": 1.0439,
      "step": 3039
    },
    {
      "epoch": 0.42738647546745395,
      "grad_norm": 1.6386981010437012,
      "learning_rate": 0.00016348985585617652,
      "loss": 1.3243,
      "step": 3040
    },
    {
      "epoch": 0.42752706312385774,
      "grad_norm": 1.3627372980117798,
      "learning_rate": 0.00016366928902031088,
      "loss": 1.2796,
      "step": 3041
    },
    {
      "epoch": 0.4276676507802615,
      "grad_norm": 1.6082005500793457,
      "learning_rate": 0.00016384837810262982,
      "loss": 1.1428,
      "step": 3042
    },
    {
      "epoch": 0.42780823843666527,
      "grad_norm": 1.4834266901016235,
      "learning_rate": 0.00016402712213529967,
      "loss": 1.0866,
      "step": 3043
    },
    {
      "epoch": 0.427948826093069,
      "grad_norm": 1.4153090715408325,
      "learning_rate": 0.00016420552015235096,
      "loss": 1.1137,
      "step": 3044
    },
    {
      "epoch": 0.4280894137494728,
      "grad_norm": 1.8289520740509033,
      "learning_rate": 0.00016438357118968457,
      "loss": 1.2164,
      "step": 3045
    },
    {
      "epoch": 0.4282300014058766,
      "grad_norm": 1.584618091583252,
      "learning_rate": 0.0001645612742850764,
      "loss": 1.1602,
      "step": 3046
    },
    {
      "epoch": 0.4283705890622803,
      "grad_norm": 1.5300061702728271,
      "learning_rate": 0.0001647386284781828,
      "loss": 1.3412,
      "step": 3047
    },
    {
      "epoch": 0.4285111767186841,
      "grad_norm": 1.4399769306182861,
      "learning_rate": 0.00016491563281054533,
      "loss": 1.2017,
      "step": 3048
    },
    {
      "epoch": 0.42865176437508784,
      "grad_norm": 1.8793187141418457,
      "learning_rate": 0.0001650922863255967,
      "loss": 1.2046,
      "step": 3049
    },
    {
      "epoch": 0.42879235203149163,
      "grad_norm": 1.8270939588546753,
      "learning_rate": 0.00016526858806866518,
      "loss": 0.992,
      "step": 3050
    },
    {
      "epoch": 0.4289329396878954,
      "grad_norm": 1.207284927368164,
      "learning_rate": 0.0001654445370869804,
      "loss": 1.1456,
      "step": 3051
    },
    {
      "epoch": 0.42907352734429915,
      "grad_norm": 1.6767165660858154,
      "learning_rate": 0.00016562013242967777,
      "loss": 1.1756,
      "step": 3052
    },
    {
      "epoch": 0.42921411500070294,
      "grad_norm": 1.6849076747894287,
      "learning_rate": 0.0001657953731478044,
      "loss": 1.3141,
      "step": 3053
    },
    {
      "epoch": 0.4293547026571067,
      "grad_norm": 1.5969510078430176,
      "learning_rate": 0.00016597025829432377,
      "loss": 1.2258,
      "step": 3054
    },
    {
      "epoch": 0.42949529031351047,
      "grad_norm": 1.5226308107376099,
      "learning_rate": 0.0001661447869241208,
      "loss": 1.0344,
      "step": 3055
    },
    {
      "epoch": 0.42963587796991426,
      "grad_norm": 1.609743595123291,
      "learning_rate": 0.00016631895809400722,
      "loss": 1.0461,
      "step": 3056
    },
    {
      "epoch": 0.429776465626318,
      "grad_norm": 1.9165412187576294,
      "learning_rate": 0.00016649277086272648,
      "loss": 1.0449,
      "step": 3057
    },
    {
      "epoch": 0.4299170532827218,
      "grad_norm": 1.478758454322815,
      "learning_rate": 0.0001666662242909591,
      "loss": 1.0117,
      "step": 3058
    },
    {
      "epoch": 0.4300576409391255,
      "grad_norm": 1.7024550437927246,
      "learning_rate": 0.00016683931744132733,
      "loss": 1.08,
      "step": 3059
    },
    {
      "epoch": 0.4301982285955293,
      "grad_norm": 2.077120780944824,
      "learning_rate": 0.00016701204937840048,
      "loss": 1.2719,
      "step": 3060
    },
    {
      "epoch": 0.4303388162519331,
      "grad_norm": 1.4326013326644897,
      "learning_rate": 0.00016718441916869988,
      "loss": 1.1236,
      "step": 3061
    },
    {
      "epoch": 0.43047940390833683,
      "grad_norm": 1.5313353538513184,
      "learning_rate": 0.0001673564258807042,
      "loss": 0.9819,
      "step": 3062
    },
    {
      "epoch": 0.4306199915647406,
      "grad_norm": 1.468106985092163,
      "learning_rate": 0.00016752806858485406,
      "loss": 1.2153,
      "step": 3063
    },
    {
      "epoch": 0.43076057922114436,
      "grad_norm": 1.6533043384552002,
      "learning_rate": 0.00016769934635355727,
      "loss": 1.0984,
      "step": 3064
    },
    {
      "epoch": 0.43090116687754815,
      "grad_norm": 1.5463413000106812,
      "learning_rate": 0.00016787025826119382,
      "loss": 1.0949,
      "step": 3065
    },
    {
      "epoch": 0.43104175453395194,
      "grad_norm": 1.5521867275238037,
      "learning_rate": 0.00016804080338412103,
      "loss": 1.069,
      "step": 3066
    },
    {
      "epoch": 0.43118234219035567,
      "grad_norm": 1.5262343883514404,
      "learning_rate": 0.00016821098080067824,
      "loss": 1.1698,
      "step": 3067
    },
    {
      "epoch": 0.43132292984675946,
      "grad_norm": 1.6990517377853394,
      "learning_rate": 0.000168380789591192,
      "loss": 1.0846,
      "step": 3068
    },
    {
      "epoch": 0.4314635175031632,
      "grad_norm": 1.554581642150879,
      "learning_rate": 0.00016855022883798099,
      "loss": 1.0727,
      "step": 3069
    },
    {
      "epoch": 0.431604105159567,
      "grad_norm": 1.6806628704071045,
      "learning_rate": 0.000168719297625361,
      "loss": 1.1985,
      "step": 3070
    },
    {
      "epoch": 0.4317446928159708,
      "grad_norm": 1.4954893589019775,
      "learning_rate": 0.00016888799503964985,
      "loss": 1.0592,
      "step": 3071
    },
    {
      "epoch": 0.4318852804723745,
      "grad_norm": 1.845212459564209,
      "learning_rate": 0.0001690563201691723,
      "loss": 1.1563,
      "step": 3072
    },
    {
      "epoch": 0.4320258681287783,
      "grad_norm": 1.6395840644836426,
      "learning_rate": 0.000169224272104265,
      "loss": 1.0874,
      "step": 3073
    },
    {
      "epoch": 0.43216645578518204,
      "grad_norm": 1.4616835117340088,
      "learning_rate": 0.00016939184993728165,
      "loss": 1.1315,
      "step": 3074
    },
    {
      "epoch": 0.4323070434415858,
      "grad_norm": 1.9215165376663208,
      "learning_rate": 0.0001695590527625973,
      "loss": 1.0752,
      "step": 3075
    },
    {
      "epoch": 0.4324476310979896,
      "grad_norm": 1.4891427755355835,
      "learning_rate": 0.00016972587967661377,
      "loss": 1.2469,
      "step": 3076
    },
    {
      "epoch": 0.43258821875439335,
      "grad_norm": 1.7168625593185425,
      "learning_rate": 0.00016989232977776455,
      "loss": 1.1921,
      "step": 3077
    },
    {
      "epoch": 0.43272880641079714,
      "grad_norm": 1.513603687286377,
      "learning_rate": 0.0001700584021665193,
      "loss": 1.0417,
      "step": 3078
    },
    {
      "epoch": 0.4328693940672009,
      "grad_norm": 1.7891192436218262,
      "learning_rate": 0.00017022409594538905,
      "loss": 1.0575,
      "step": 3079
    },
    {
      "epoch": 0.43300998172360466,
      "grad_norm": 1.579747200012207,
      "learning_rate": 0.00017038941021893064,
      "loss": 1.1787,
      "step": 3080
    },
    {
      "epoch": 0.43315056938000845,
      "grad_norm": 1.6555976867675781,
      "learning_rate": 0.0001705543440937523,
      "loss": 1.2629,
      "step": 3081
    },
    {
      "epoch": 0.4332911570364122,
      "grad_norm": 1.6423802375793457,
      "learning_rate": 0.00017071889667851764,
      "loss": 1.0974,
      "step": 3082
    },
    {
      "epoch": 0.433431744692816,
      "grad_norm": 1.4550368785858154,
      "learning_rate": 0.00017088306708395093,
      "loss": 1.3165,
      "step": 3083
    },
    {
      "epoch": 0.4335723323492197,
      "grad_norm": 1.3182145357131958,
      "learning_rate": 0.0001710468544228418,
      "loss": 1.2327,
      "step": 3084
    },
    {
      "epoch": 0.4337129200056235,
      "grad_norm": 1.4700767993927002,
      "learning_rate": 0.00017121025781005023,
      "loss": 1.1725,
      "step": 3085
    },
    {
      "epoch": 0.4338535076620273,
      "grad_norm": 1.4697145223617554,
      "learning_rate": 0.0001713732763625109,
      "loss": 0.8295,
      "step": 3086
    },
    {
      "epoch": 0.43399409531843103,
      "grad_norm": 1.741134762763977,
      "learning_rate": 0.00017153590919923833,
      "loss": 1.156,
      "step": 3087
    },
    {
      "epoch": 0.4341346829748348,
      "grad_norm": 1.8644394874572754,
      "learning_rate": 0.00017169815544133154,
      "loss": 1.2204,
      "step": 3088
    },
    {
      "epoch": 0.43427527063123855,
      "grad_norm": 1.5213323831558228,
      "learning_rate": 0.0001718600142119788,
      "loss": 1.0984,
      "step": 3089
    },
    {
      "epoch": 0.43441585828764234,
      "grad_norm": 1.7008649110794067,
      "learning_rate": 0.0001720214846364623,
      "loss": 0.9691,
      "step": 3090
    },
    {
      "epoch": 0.43455644594404613,
      "grad_norm": 1.485600233078003,
      "learning_rate": 0.00017218256584216292,
      "loss": 1.2285,
      "step": 3091
    },
    {
      "epoch": 0.43469703360044987,
      "grad_norm": 1.4808274507522583,
      "learning_rate": 0.00017234325695856498,
      "loss": 1.2879,
      "step": 3092
    },
    {
      "epoch": 0.43483762125685366,
      "grad_norm": 1.5319359302520752,
      "learning_rate": 0.00017250355711726097,
      "loss": 1.261,
      "step": 3093
    },
    {
      "epoch": 0.4349782089132574,
      "grad_norm": 1.2687233686447144,
      "learning_rate": 0.00017266346545195628,
      "loss": 1.2744,
      "step": 3094
    },
    {
      "epoch": 0.4351187965696612,
      "grad_norm": 1.510211706161499,
      "learning_rate": 0.00017282298109847342,
      "loss": 0.9513,
      "step": 3095
    },
    {
      "epoch": 0.43525938422606497,
      "grad_norm": 1.6213937997817993,
      "learning_rate": 0.00017298210319475753,
      "loss": 1.0499,
      "step": 3096
    },
    {
      "epoch": 0.4353999718824687,
      "grad_norm": 1.5843737125396729,
      "learning_rate": 0.00017314083088088022,
      "loss": 1.0328,
      "step": 3097
    },
    {
      "epoch": 0.4355405595388725,
      "grad_norm": 1.7623302936553955,
      "learning_rate": 0.00017329916329904494,
      "loss": 1.0789,
      "step": 3098
    },
    {
      "epoch": 0.43568114719527623,
      "grad_norm": 1.5139172077178955,
      "learning_rate": 0.00017345709959359074,
      "loss": 1.1196,
      "step": 3099
    },
    {
      "epoch": 0.43582173485168,
      "grad_norm": 1.6108108758926392,
      "learning_rate": 0.00017361463891099791,
      "loss": 1.3057,
      "step": 3100
    },
    {
      "epoch": 0.4359623225080838,
      "grad_norm": 1.5227322578430176,
      "learning_rate": 0.00017377178039989176,
      "loss": 1.0845,
      "step": 3101
    },
    {
      "epoch": 0.43610291016448754,
      "grad_norm": 1.794675350189209,
      "learning_rate": 0.0001739285232110478,
      "loss": 1.1376,
      "step": 3102
    },
    {
      "epoch": 0.43624349782089133,
      "grad_norm": 1.4200152158737183,
      "learning_rate": 0.0001740848664973957,
      "loss": 1.1565,
      "step": 3103
    },
    {
      "epoch": 0.43638408547729507,
      "grad_norm": 1.6973844766616821,
      "learning_rate": 0.00017424080941402464,
      "loss": 1.0875,
      "step": 3104
    },
    {
      "epoch": 0.43652467313369886,
      "grad_norm": 1.5803894996643066,
      "learning_rate": 0.00017439635111818723,
      "loss": 1.1276,
      "step": 3105
    },
    {
      "epoch": 0.43666526079010265,
      "grad_norm": 1.4979039430618286,
      "learning_rate": 0.00017455149076930457,
      "loss": 1.1304,
      "step": 3106
    },
    {
      "epoch": 0.4368058484465064,
      "grad_norm": 1.7445011138916016,
      "learning_rate": 0.0001747062275289701,
      "loss": 1.1516,
      "step": 3107
    },
    {
      "epoch": 0.4369464361029102,
      "grad_norm": 1.5882351398468018,
      "learning_rate": 0.000174860560560955,
      "loss": 1.1913,
      "step": 3108
    },
    {
      "epoch": 0.4370870237593139,
      "grad_norm": 1.5771902799606323,
      "learning_rate": 0.00017501448903121204,
      "loss": 1.0647,
      "step": 3109
    },
    {
      "epoch": 0.4372276114157177,
      "grad_norm": 1.4274848699569702,
      "learning_rate": 0.00017516801210788052,
      "loss": 1.1781,
      "step": 3110
    },
    {
      "epoch": 0.4373681990721215,
      "grad_norm": 1.5162875652313232,
      "learning_rate": 0.00017532112896129025,
      "loss": 1.1423,
      "step": 3111
    },
    {
      "epoch": 0.4375087867285252,
      "grad_norm": 1.2680790424346924,
      "learning_rate": 0.0001754738387639667,
      "loss": 1.2876,
      "step": 3112
    },
    {
      "epoch": 0.437649374384929,
      "grad_norm": 1.5769133567810059,
      "learning_rate": 0.0001756261406906349,
      "loss": 1.1895,
      "step": 3113
    },
    {
      "epoch": 0.43778996204133275,
      "grad_norm": 1.437627911567688,
      "learning_rate": 0.00017577803391822416,
      "loss": 1.1296,
      "step": 3114
    },
    {
      "epoch": 0.43793054969773654,
      "grad_norm": 1.3613845109939575,
      "learning_rate": 0.0001759295176258726,
      "loss": 1.3032,
      "step": 3115
    },
    {
      "epoch": 0.4380711373541403,
      "grad_norm": 1.362497091293335,
      "learning_rate": 0.00017608059099493128,
      "loss": 1.1505,
      "step": 3116
    },
    {
      "epoch": 0.43821172501054406,
      "grad_norm": 1.558774709701538,
      "learning_rate": 0.00017623125320896912,
      "loss": 1.0451,
      "step": 3117
    },
    {
      "epoch": 0.43835231266694785,
      "grad_norm": 1.4125198125839233,
      "learning_rate": 0.00017638150345377658,
      "loss": 1.0924,
      "step": 3118
    },
    {
      "epoch": 0.4384929003233516,
      "grad_norm": 1.4377515316009521,
      "learning_rate": 0.0001765313409173708,
      "loss": 1.1572,
      "step": 3119
    },
    {
      "epoch": 0.4386334879797554,
      "grad_norm": 1.607118844985962,
      "learning_rate": 0.0001766807647899996,
      "loss": 1.0829,
      "step": 3120
    },
    {
      "epoch": 0.43877407563615917,
      "grad_norm": 1.4993189573287964,
      "learning_rate": 0.00017682977426414603,
      "loss": 1.0262,
      "step": 3121
    },
    {
      "epoch": 0.4389146632925629,
      "grad_norm": 1.4892578125,
      "learning_rate": 0.00017697836853453224,
      "loss": 1.1052,
      "step": 3122
    },
    {
      "epoch": 0.4390552509489667,
      "grad_norm": 1.6505454778671265,
      "learning_rate": 0.00017712654679812476,
      "loss": 1.1106,
      "step": 3123
    },
    {
      "epoch": 0.4391958386053704,
      "grad_norm": 1.6752421855926514,
      "learning_rate": 0.0001772743082541379,
      "loss": 1.193,
      "step": 3124
    },
    {
      "epoch": 0.4393364262617742,
      "grad_norm": 1.7045884132385254,
      "learning_rate": 0.00017742165210403877,
      "loss": 1.1279,
      "step": 3125
    },
    {
      "epoch": 0.439477013918178,
      "grad_norm": 1.400654673576355,
      "learning_rate": 0.000177568577551551,
      "loss": 1.1325,
      "step": 3126
    },
    {
      "epoch": 0.43961760157458174,
      "grad_norm": 1.6140096187591553,
      "learning_rate": 0.0001777150838026597,
      "loss": 1.1179,
      "step": 3127
    },
    {
      "epoch": 0.43975818923098553,
      "grad_norm": 1.3939998149871826,
      "learning_rate": 0.00017786117006561504,
      "loss": 1.2008,
      "step": 3128
    },
    {
      "epoch": 0.43989877688738926,
      "grad_norm": 1.8683339357376099,
      "learning_rate": 0.00017800683555093733,
      "loss": 0.9515,
      "step": 3129
    },
    {
      "epoch": 0.44003936454379305,
      "grad_norm": 1.616634488105774,
      "learning_rate": 0.00017815207947142033,
      "loss": 1.163,
      "step": 3130
    },
    {
      "epoch": 0.44017995220019684,
      "grad_norm": 1.490604281425476,
      "learning_rate": 0.00017829690104213648,
      "loss": 1.1135,
      "step": 3131
    },
    {
      "epoch": 0.4403205398566006,
      "grad_norm": 1.7612413167953491,
      "learning_rate": 0.0001784412994804404,
      "loss": 1.0438,
      "step": 3132
    },
    {
      "epoch": 0.44046112751300437,
      "grad_norm": 1.5427476167678833,
      "learning_rate": 0.00017858527400597352,
      "loss": 1.1609,
      "step": 3133
    },
    {
      "epoch": 0.4406017151694081,
      "grad_norm": 1.5783510208129883,
      "learning_rate": 0.00017872882384066817,
      "loss": 1.0844,
      "step": 3134
    },
    {
      "epoch": 0.4407423028258119,
      "grad_norm": 1.3439441919326782,
      "learning_rate": 0.0001788719482087517,
      "loss": 1.2233,
      "step": 3135
    },
    {
      "epoch": 0.4408828904822157,
      "grad_norm": 1.73897385597229,
      "learning_rate": 0.000179014646336751,
      "loss": 1.0857,
      "step": 3136
    },
    {
      "epoch": 0.4410234781386194,
      "grad_norm": 1.4559074640274048,
      "learning_rate": 0.00017915691745349623,
      "loss": 1.1644,
      "step": 3137
    },
    {
      "epoch": 0.4411640657950232,
      "grad_norm": 1.6197588443756104,
      "learning_rate": 0.00017929876079012525,
      "loss": 1.0571,
      "step": 3138
    },
    {
      "epoch": 0.44130465345142694,
      "grad_norm": 1.8033051490783691,
      "learning_rate": 0.00017944017558008777,
      "loss": 1.1248,
      "step": 3139
    },
    {
      "epoch": 0.44144524110783073,
      "grad_norm": 1.418507695198059,
      "learning_rate": 0.00017958116105914947,
      "loss": 1.0852,
      "step": 3140
    },
    {
      "epoch": 0.4415858287642345,
      "grad_norm": 1.5166536569595337,
      "learning_rate": 0.00017972171646539605,
      "loss": 1.2762,
      "step": 3141
    },
    {
      "epoch": 0.44172641642063826,
      "grad_norm": 1.3859754800796509,
      "learning_rate": 0.00017986184103923749,
      "loss": 1.0951,
      "step": 3142
    },
    {
      "epoch": 0.44186700407704205,
      "grad_norm": 1.6361781358718872,
      "learning_rate": 0.00018000153402341194,
      "loss": 1.1077,
      "step": 3143
    },
    {
      "epoch": 0.4420075917334458,
      "grad_norm": 1.547247290611267,
      "learning_rate": 0.00018014079466299014,
      "loss": 1.1124,
      "step": 3144
    },
    {
      "epoch": 0.44214817938984957,
      "grad_norm": 1.6467697620391846,
      "learning_rate": 0.0001802796222053792,
      "loss": 1.0968,
      "step": 3145
    },
    {
      "epoch": 0.44228876704625336,
      "grad_norm": 1.768883466720581,
      "learning_rate": 0.00018041801590032674,
      "loss": 1.1389,
      "step": 3146
    },
    {
      "epoch": 0.4424293547026571,
      "grad_norm": 1.5373023748397827,
      "learning_rate": 0.000180555974999925,
      "loss": 0.9806,
      "step": 3147
    },
    {
      "epoch": 0.4425699423590609,
      "grad_norm": 1.6406632661819458,
      "learning_rate": 0.00018069349875861497,
      "loss": 0.9474,
      "step": 3148
    },
    {
      "epoch": 0.4427105300154646,
      "grad_norm": 1.4863826036453247,
      "learning_rate": 0.00018083058643319016,
      "loss": 1.1377,
      "step": 3149
    },
    {
      "epoch": 0.4428511176718684,
      "grad_norm": 1.6950080394744873,
      "learning_rate": 0.0001809672372828009,
      "loss": 0.9573,
      "step": 3150
    },
    {
      "epoch": 0.4429917053282722,
      "grad_norm": 1.5309185981750488,
      "learning_rate": 0.00018110345056895807,
      "loss": 1.221,
      "step": 3151
    },
    {
      "epoch": 0.44313229298467594,
      "grad_norm": 1.3764957189559937,
      "learning_rate": 0.00018123922555553736,
      "loss": 1.0282,
      "step": 3152
    },
    {
      "epoch": 0.4432728806410797,
      "grad_norm": 1.8881162405014038,
      "learning_rate": 0.00018137456150878303,
      "loss": 1.1413,
      "step": 3153
    },
    {
      "epoch": 0.44341346829748346,
      "grad_norm": 1.823763132095337,
      "learning_rate": 0.000181509457697312,
      "loss": 1.1625,
      "step": 3154
    },
    {
      "epoch": 0.44355405595388725,
      "grad_norm": 1.7852914333343506,
      "learning_rate": 0.00018164391339211785,
      "loss": 1.0166,
      "step": 3155
    },
    {
      "epoch": 0.44369464361029104,
      "grad_norm": 1.6736303567886353,
      "learning_rate": 0.0001817779278665745,
      "loss": 1.0317,
      "step": 3156
    },
    {
      "epoch": 0.4438352312666948,
      "grad_norm": 1.7487742900848389,
      "learning_rate": 0.0001819115003964405,
      "loss": 1.0743,
      "step": 3157
    },
    {
      "epoch": 0.44397581892309856,
      "grad_norm": 1.8748531341552734,
      "learning_rate": 0.00018204463025986258,
      "loss": 0.9309,
      "step": 3158
    },
    {
      "epoch": 0.4441164065795023,
      "grad_norm": 1.6438868045806885,
      "learning_rate": 0.0001821773167373799,
      "loss": 0.9782,
      "step": 3159
    },
    {
      "epoch": 0.4442569942359061,
      "grad_norm": 1.8003995418548584,
      "learning_rate": 0.00018230955911192767,
      "loss": 1.1098,
      "step": 3160
    },
    {
      "epoch": 0.4443975818923099,
      "grad_norm": 1.7574056386947632,
      "learning_rate": 0.00018244135666884117,
      "loss": 1.0497,
      "step": 3161
    },
    {
      "epoch": 0.4445381695487136,
      "grad_norm": 1.724685788154602,
      "learning_rate": 0.00018257270869585942,
      "loss": 1.0861,
      "step": 3162
    },
    {
      "epoch": 0.4446787572051174,
      "grad_norm": 1.6409759521484375,
      "learning_rate": 0.00018270361448312943,
      "loss": 0.9698,
      "step": 3163
    },
    {
      "epoch": 0.44481934486152114,
      "grad_norm": 1.5991286039352417,
      "learning_rate": 0.00018283407332320964,
      "loss": 1.1503,
      "step": 3164
    },
    {
      "epoch": 0.4449599325179249,
      "grad_norm": 1.7041174173355103,
      "learning_rate": 0.0001829640845110738,
      "loss": 1.182,
      "step": 3165
    },
    {
      "epoch": 0.4451005201743287,
      "grad_norm": 1.5778220891952515,
      "learning_rate": 0.00018309364734411495,
      "loss": 1.0526,
      "step": 3166
    },
    {
      "epoch": 0.44524110783073245,
      "grad_norm": 1.4940539598464966,
      "learning_rate": 0.0001832227611221492,
      "loss": 1.2699,
      "step": 3167
    },
    {
      "epoch": 0.44538169548713624,
      "grad_norm": 1.4692035913467407,
      "learning_rate": 0.00018335142514741936,
      "loss": 1.1065,
      "step": 3168
    },
    {
      "epoch": 0.44552228314354,
      "grad_norm": 1.7302888631820679,
      "learning_rate": 0.00018347963872459883,
      "loss": 1.0809,
      "step": 3169
    },
    {
      "epoch": 0.44566287079994377,
      "grad_norm": 1.297942042350769,
      "learning_rate": 0.00018360740116079522,
      "loss": 1.1226,
      "step": 3170
    },
    {
      "epoch": 0.44580345845634756,
      "grad_norm": 1.5318225622177124,
      "learning_rate": 0.00018373471176555439,
      "loss": 1.2389,
      "step": 3171
    },
    {
      "epoch": 0.4459440461127513,
      "grad_norm": 1.512954831123352,
      "learning_rate": 0.00018386156985086388,
      "loss": 1.0093,
      "step": 3172
    },
    {
      "epoch": 0.4460846337691551,
      "grad_norm": 1.490644931793213,
      "learning_rate": 0.0001839879747311566,
      "loss": 1.2256,
      "step": 3173
    },
    {
      "epoch": 0.4462252214255588,
      "grad_norm": 1.6254115104675293,
      "learning_rate": 0.00018411392572331494,
      "loss": 1.1278,
      "step": 3174
    },
    {
      "epoch": 0.4463658090819626,
      "grad_norm": 1.8583766222000122,
      "learning_rate": 0.000184239422146674,
      "loss": 0.9626,
      "step": 3175
    },
    {
      "epoch": 0.4465063967383664,
      "grad_norm": 1.541916847229004,
      "learning_rate": 0.00018436446332302566,
      "loss": 1.1256,
      "step": 3176
    },
    {
      "epoch": 0.44664698439477013,
      "grad_norm": 1.5708916187286377,
      "learning_rate": 0.0001844890485766217,
      "loss": 1.0938,
      "step": 3177
    },
    {
      "epoch": 0.4467875720511739,
      "grad_norm": 1.436668038368225,
      "learning_rate": 0.00018461317723417818,
      "loss": 1.2472,
      "step": 3178
    },
    {
      "epoch": 0.44692815970757765,
      "grad_norm": 1.648244023323059,
      "learning_rate": 0.00018473684862487847,
      "loss": 1.1366,
      "step": 3179
    },
    {
      "epoch": 0.44706874736398144,
      "grad_norm": 1.9605716466903687,
      "learning_rate": 0.00018486006208037726,
      "loss": 1.105,
      "step": 3180
    },
    {
      "epoch": 0.44720933502038523,
      "grad_norm": 1.8758617639541626,
      "learning_rate": 0.0001849828169348038,
      "loss": 1.0359,
      "step": 3181
    },
    {
      "epoch": 0.44734992267678897,
      "grad_norm": 1.5094432830810547,
      "learning_rate": 0.00018510511252476584,
      "loss": 1.1292,
      "step": 3182
    },
    {
      "epoch": 0.44749051033319276,
      "grad_norm": 1.493501901626587,
      "learning_rate": 0.00018522694818935316,
      "loss": 1.2736,
      "step": 3183
    },
    {
      "epoch": 0.4476310979895965,
      "grad_norm": 1.835216999053955,
      "learning_rate": 0.00018534832327014104,
      "loss": 1.1216,
      "step": 3184
    },
    {
      "epoch": 0.4477716856460003,
      "grad_norm": 1.773746132850647,
      "learning_rate": 0.0001854692371111936,
      "loss": 1.1062,
      "step": 3185
    },
    {
      "epoch": 0.4479122733024041,
      "grad_norm": 1.5245671272277832,
      "learning_rate": 0.000185589689059068,
      "loss": 1.056,
      "step": 3186
    },
    {
      "epoch": 0.4480528609588078,
      "grad_norm": 1.5524650812149048,
      "learning_rate": 0.00018570967846281723,
      "loss": 1.0005,
      "step": 3187
    },
    {
      "epoch": 0.4481934486152116,
      "grad_norm": 1.5923912525177002,
      "learning_rate": 0.00018582920467399424,
      "loss": 1.1461,
      "step": 3188
    },
    {
      "epoch": 0.44833403627161533,
      "grad_norm": 1.5916200876235962,
      "learning_rate": 0.00018594826704665488,
      "loss": 1.0975,
      "step": 3189
    },
    {
      "epoch": 0.4484746239280191,
      "grad_norm": 1.7979869842529297,
      "learning_rate": 0.00018606686493736186,
      "loss": 1.2152,
      "step": 3190
    },
    {
      "epoch": 0.4486152115844229,
      "grad_norm": 1.5986148118972778,
      "learning_rate": 0.00018618499770518812,
      "loss": 1.099,
      "step": 3191
    },
    {
      "epoch": 0.44875579924082665,
      "grad_norm": 1.5275638103485107,
      "learning_rate": 0.00018630266471171986,
      "loss": 0.9873,
      "step": 3192
    },
    {
      "epoch": 0.44889638689723044,
      "grad_norm": 1.644615650177002,
      "learning_rate": 0.00018641986532106085,
      "loss": 1.1044,
      "step": 3193
    },
    {
      "epoch": 0.44903697455363417,
      "grad_norm": 1.4990376234054565,
      "learning_rate": 0.00018653659889983487,
      "loss": 1.0023,
      "step": 3194
    },
    {
      "epoch": 0.44917756221003796,
      "grad_norm": 1.587220549583435,
      "learning_rate": 0.00018665286481719012,
      "loss": 1.0466,
      "step": 3195
    },
    {
      "epoch": 0.44931814986644175,
      "grad_norm": 2.2069644927978516,
      "learning_rate": 0.0001867686624448017,
      "loss": 0.8871,
      "step": 3196
    },
    {
      "epoch": 0.4494587375228455,
      "grad_norm": 1.339219093322754,
      "learning_rate": 0.0001868839911568757,
      "loss": 1.1782,
      "step": 3197
    },
    {
      "epoch": 0.4495993251792493,
      "grad_norm": 1.5183438062667847,
      "learning_rate": 0.0001869988503301522,
      "loss": 1.0702,
      "step": 3198
    },
    {
      "epoch": 0.449739912835653,
      "grad_norm": 2.235534906387329,
      "learning_rate": 0.00018711323934390893,
      "loss": 1.0879,
      "step": 3199
    },
    {
      "epoch": 0.4498805004920568,
      "grad_norm": 1.6622740030288696,
      "learning_rate": 0.00018722715757996417,
      "loss": 1.0724,
      "step": 3200
    },
    {
      "epoch": 0.4500210881484606,
      "grad_norm": 1.6693710088729858,
      "learning_rate": 0.00018734060442268072,
      "loss": 1.1912,
      "step": 3201
    },
    {
      "epoch": 0.4501616758048643,
      "grad_norm": 1.5303916931152344,
      "learning_rate": 0.0001874535792589686,
      "loss": 1.0853,
      "step": 3202
    },
    {
      "epoch": 0.4503022634612681,
      "grad_norm": 1.6946380138397217,
      "learning_rate": 0.00018756608147828886,
      "loss": 1.0903,
      "step": 3203
    },
    {
      "epoch": 0.45044285111767185,
      "grad_norm": 1.9255988597869873,
      "learning_rate": 0.0001876781104726565,
      "loss": 1.1121,
      "step": 3204
    },
    {
      "epoch": 0.45058343877407564,
      "grad_norm": 1.5013493299484253,
      "learning_rate": 0.00018778966563664406,
      "loss": 1.1031,
      "step": 3205
    },
    {
      "epoch": 0.45072402643047943,
      "grad_norm": 1.6668682098388672,
      "learning_rate": 0.0001879007463673846,
      "loss": 1.1178,
      "step": 3206
    },
    {
      "epoch": 0.45086461408688316,
      "grad_norm": 1.3627811670303345,
      "learning_rate": 0.00018801135206457544,
      "loss": 1.0869,
      "step": 3207
    },
    {
      "epoch": 0.45100520174328695,
      "grad_norm": 1.5147308111190796,
      "learning_rate": 0.00018812148213048053,
      "loss": 1.1673,
      "step": 3208
    },
    {
      "epoch": 0.4511457893996907,
      "grad_norm": 1.482494592666626,
      "learning_rate": 0.00018823113596993477,
      "loss": 1.1447,
      "step": 3209
    },
    {
      "epoch": 0.4512863770560945,
      "grad_norm": 1.5497442483901978,
      "learning_rate": 0.0001883403129903464,
      "loss": 1.1314,
      "step": 3210
    },
    {
      "epoch": 0.45142696471249827,
      "grad_norm": 1.8608015775680542,
      "learning_rate": 0.00018844901260170054,
      "loss": 1.2097,
      "step": 3211
    },
    {
      "epoch": 0.451567552368902,
      "grad_norm": 1.6804125308990479,
      "learning_rate": 0.0001885572342165623,
      "loss": 1.1551,
      "step": 3212
    },
    {
      "epoch": 0.4517081400253058,
      "grad_norm": 1.8375053405761719,
      "learning_rate": 0.00018866497725008005,
      "loss": 1.0948,
      "step": 3213
    },
    {
      "epoch": 0.4518487276817095,
      "grad_norm": 1.8146287202835083,
      "learning_rate": 0.0001887722411199885,
      "loss": 1.0983,
      "step": 3214
    },
    {
      "epoch": 0.4519893153381133,
      "grad_norm": 1.48105788230896,
      "learning_rate": 0.00018887902524661183,
      "loss": 1.1592,
      "step": 3215
    },
    {
      "epoch": 0.4521299029945171,
      "grad_norm": 1.6806939840316772,
      "learning_rate": 0.00018898532905286684,
      "loss": 1.0345,
      "step": 3216
    },
    {
      "epoch": 0.45227049065092084,
      "grad_norm": 1.4273369312286377,
      "learning_rate": 0.00018909115196426602,
      "loss": 1.2144,
      "step": 3217
    },
    {
      "epoch": 0.45241107830732463,
      "grad_norm": 1.704586148262024,
      "learning_rate": 0.0001891964934089209,
      "loss": 1.168,
      "step": 3218
    },
    {
      "epoch": 0.45255166596372837,
      "grad_norm": 1.3643442392349243,
      "learning_rate": 0.00018930135281754483,
      "loss": 1.196,
      "step": 3219
    },
    {
      "epoch": 0.45269225362013216,
      "grad_norm": 1.6370229721069336,
      "learning_rate": 0.00018940572962345616,
      "loss": 1.1543,
      "step": 3220
    },
    {
      "epoch": 0.45283284127653595,
      "grad_norm": 1.5378804206848145,
      "learning_rate": 0.00018950962326258128,
      "loss": 1.0578,
      "step": 3221
    },
    {
      "epoch": 0.4529734289329397,
      "grad_norm": 2.111809253692627,
      "learning_rate": 0.00018961303317345792,
      "loss": 1.0438,
      "step": 3222
    },
    {
      "epoch": 0.45311401658934347,
      "grad_norm": 1.8930079936981201,
      "learning_rate": 0.0001897159587972378,
      "loss": 1.0885,
      "step": 3223
    },
    {
      "epoch": 0.4532546042457472,
      "grad_norm": 1.7445110082626343,
      "learning_rate": 0.00018981839957768986,
      "loss": 1.1814,
      "step": 3224
    },
    {
      "epoch": 0.453395191902151,
      "grad_norm": 1.521631121635437,
      "learning_rate": 0.0001899203549612032,
      "loss": 1.1128,
      "step": 3225
    },
    {
      "epoch": 0.4535357795585548,
      "grad_norm": 1.6246145963668823,
      "learning_rate": 0.00019002182439679022,
      "loss": 1.0561,
      "step": 3226
    },
    {
      "epoch": 0.4536763672149585,
      "grad_norm": 1.6023118495941162,
      "learning_rate": 0.00019012280733608935,
      "loss": 1.119,
      "step": 3227
    },
    {
      "epoch": 0.4538169548713623,
      "grad_norm": 1.6071927547454834,
      "learning_rate": 0.0001902233032333683,
      "loss": 1.0498,
      "step": 3228
    },
    {
      "epoch": 0.45395754252776604,
      "grad_norm": 1.6124399900436401,
      "learning_rate": 0.0001903233115455266,
      "loss": 1.1889,
      "step": 3229
    },
    {
      "epoch": 0.45409813018416983,
      "grad_norm": 1.5719568729400635,
      "learning_rate": 0.0001904228317320991,
      "loss": 1.0175,
      "step": 3230
    },
    {
      "epoch": 0.4542387178405736,
      "grad_norm": 1.4730674028396606,
      "learning_rate": 0.00019052186325525835,
      "loss": 1.2442,
      "step": 3231
    },
    {
      "epoch": 0.45437930549697736,
      "grad_norm": 1.5172780752182007,
      "learning_rate": 0.00019062040557981774,
      "loss": 1.0966,
      "step": 3232
    },
    {
      "epoch": 0.45451989315338115,
      "grad_norm": 1.6164716482162476,
      "learning_rate": 0.00019071845817323463,
      "loss": 1.1928,
      "step": 3233
    },
    {
      "epoch": 0.4546604808097849,
      "grad_norm": 1.7357474565505981,
      "learning_rate": 0.0001908160205056127,
      "loss": 0.9199,
      "step": 3234
    },
    {
      "epoch": 0.4548010684661887,
      "grad_norm": 1.487215280532837,
      "learning_rate": 0.00019091309204970528,
      "loss": 1.0954,
      "step": 3235
    },
    {
      "epoch": 0.45494165612259246,
      "grad_norm": 1.4316610097885132,
      "learning_rate": 0.0001910096722809179,
      "loss": 1.1652,
      "step": 3236
    },
    {
      "epoch": 0.4550822437789962,
      "grad_norm": 1.4441157579421997,
      "learning_rate": 0.00019110576067731138,
      "loss": 1.1571,
      "step": 3237
    },
    {
      "epoch": 0.4552228314354,
      "grad_norm": 1.4759703874588013,
      "learning_rate": 0.0001912013567196044,
      "loss": 1.0268,
      "step": 3238
    },
    {
      "epoch": 0.4553634190918037,
      "grad_norm": 1.5219768285751343,
      "learning_rate": 0.00019129645989117644,
      "loss": 1.0422,
      "step": 3239
    },
    {
      "epoch": 0.4555040067482075,
      "grad_norm": 1.5629980564117432,
      "learning_rate": 0.0001913910696780706,
      "loss": 1.1921,
      "step": 3240
    },
    {
      "epoch": 0.4556445944046113,
      "grad_norm": 1.787369728088379,
      "learning_rate": 0.0001914851855689963,
      "loss": 1.1295,
      "step": 3241
    },
    {
      "epoch": 0.45578518206101504,
      "grad_norm": 1.648879051208496,
      "learning_rate": 0.00019157880705533212,
      "loss": 1.0267,
      "step": 3242
    },
    {
      "epoch": 0.4559257697174188,
      "grad_norm": 1.4288392066955566,
      "learning_rate": 0.00019167193363112844,
      "loss": 1.2123,
      "step": 3243
    },
    {
      "epoch": 0.45606635737382256,
      "grad_norm": 1.550159215927124,
      "learning_rate": 0.0001917645647931102,
      "loss": 1.1076,
      "step": 3244
    },
    {
      "epoch": 0.45620694503022635,
      "grad_norm": 1.8001580238342285,
      "learning_rate": 0.00019185670004067982,
      "loss": 1.0703,
      "step": 3245
    },
    {
      "epoch": 0.4563475326866301,
      "grad_norm": 1.5358669757843018,
      "learning_rate": 0.00019194833887591962,
      "loss": 1.0572,
      "step": 3246
    },
    {
      "epoch": 0.4564881203430339,
      "grad_norm": 1.853590488433838,
      "learning_rate": 0.0001920394808035946,
      "loss": 1.2643,
      "step": 3247
    },
    {
      "epoch": 0.45662870799943767,
      "grad_norm": 1.5857511758804321,
      "learning_rate": 0.00019213012533115525,
      "loss": 1.0509,
      "step": 3248
    },
    {
      "epoch": 0.4567692956558414,
      "grad_norm": 1.7067980766296387,
      "learning_rate": 0.00019222027196874006,
      "loss": 1.1287,
      "step": 3249
    },
    {
      "epoch": 0.4569098833122452,
      "grad_norm": 1.4682480096817017,
      "learning_rate": 0.00019230992022917828,
      "loss": 1.2435,
      "step": 3250
    },
    {
      "epoch": 0.4570504709686489,
      "grad_norm": 1.621389389038086,
      "learning_rate": 0.0001923990696279923,
      "loss": 1.1028,
      "step": 3251
    },
    {
      "epoch": 0.4571910586250527,
      "grad_norm": 1.4565461874008179,
      "learning_rate": 0.0001924877196834007,
      "loss": 1.1096,
      "step": 3252
    },
    {
      "epoch": 0.4573316462814565,
      "grad_norm": 1.8926925659179688,
      "learning_rate": 0.0001925758699163205,
      "loss": 1.0667,
      "step": 3253
    },
    {
      "epoch": 0.45747223393786024,
      "grad_norm": 1.7135066986083984,
      "learning_rate": 0.0001926635198503699,
      "loss": 1.2322,
      "step": 3254
    },
    {
      "epoch": 0.45761282159426403,
      "grad_norm": 1.778018832206726,
      "learning_rate": 0.0001927506690118707,
      "loss": 1.0635,
      "step": 3255
    },
    {
      "epoch": 0.45775340925066776,
      "grad_norm": 1.9006184339523315,
      "learning_rate": 0.00019283731692985116,
      "loss": 1.1052,
      "step": 3256
    },
    {
      "epoch": 0.45789399690707155,
      "grad_norm": 1.7689692974090576,
      "learning_rate": 0.0001929234631360482,
      "loss": 1.1002,
      "step": 3257
    },
    {
      "epoch": 0.45803458456347534,
      "grad_norm": 1.8175965547561646,
      "learning_rate": 0.00019300910716491028,
      "loss": 1.0062,
      "step": 3258
    },
    {
      "epoch": 0.4581751722198791,
      "grad_norm": 1.686124324798584,
      "learning_rate": 0.00019309424855359946,
      "loss": 1.1502,
      "step": 3259
    },
    {
      "epoch": 0.45831575987628287,
      "grad_norm": 1.4275610446929932,
      "learning_rate": 0.0001931788868419944,
      "loss": 1.0208,
      "step": 3260
    },
    {
      "epoch": 0.4584563475326866,
      "grad_norm": 1.71065092086792,
      "learning_rate": 0.00019326302157269252,
      "loss": 1.162,
      "step": 3261
    },
    {
      "epoch": 0.4585969351890904,
      "grad_norm": 1.79793119430542,
      "learning_rate": 0.00019334665229101266,
      "loss": 1.0939,
      "step": 3262
    },
    {
      "epoch": 0.4587375228454942,
      "grad_norm": 1.6609662771224976,
      "learning_rate": 0.00019342977854499722,
      "loss": 1.1467,
      "step": 3263
    },
    {
      "epoch": 0.4588781105018979,
      "grad_norm": 1.4693876504898071,
      "learning_rate": 0.00019351239988541513,
      "loss": 1.1271,
      "step": 3264
    },
    {
      "epoch": 0.4590186981583017,
      "grad_norm": 1.364440679550171,
      "learning_rate": 0.0001935945158657637,
      "loss": 0.957,
      "step": 3265
    },
    {
      "epoch": 0.45915928581470544,
      "grad_norm": 1.5585960149765015,
      "learning_rate": 0.00019367612604227158,
      "loss": 1.2777,
      "step": 3266
    },
    {
      "epoch": 0.45929987347110923,
      "grad_norm": 1.707665205001831,
      "learning_rate": 0.0001937572299739006,
      "loss": 1.0127,
      "step": 3267
    },
    {
      "epoch": 0.459440461127513,
      "grad_norm": 1.6099849939346313,
      "learning_rate": 0.00019383782722234868,
      "loss": 1.0456,
      "step": 3268
    },
    {
      "epoch": 0.45958104878391676,
      "grad_norm": 1.5259515047073364,
      "learning_rate": 0.00019391791735205182,
      "loss": 1.1028,
      "step": 3269
    },
    {
      "epoch": 0.45972163644032055,
      "grad_norm": 1.4775805473327637,
      "learning_rate": 0.0001939974999301866,
      "loss": 1.1509,
      "step": 3270
    },
    {
      "epoch": 0.4598622240967243,
      "grad_norm": 1.508345603942871,
      "learning_rate": 0.00019407657452667266,
      "loss": 1.0823,
      "step": 3271
    },
    {
      "epoch": 0.46000281175312807,
      "grad_norm": 1.668908715248108,
      "learning_rate": 0.0001941551407141746,
      "loss": 1.0316,
      "step": 3272
    },
    {
      "epoch": 0.46014339940953186,
      "grad_norm": 1.753269910812378,
      "learning_rate": 0.00019423319806810492,
      "loss": 1.1348,
      "step": 3273
    },
    {
      "epoch": 0.4602839870659356,
      "grad_norm": 1.9424091577529907,
      "learning_rate": 0.00019431074616662556,
      "loss": 1.0163,
      "step": 3274
    },
    {
      "epoch": 0.4604245747223394,
      "grad_norm": 1.740915060043335,
      "learning_rate": 0.00019438778459065093,
      "loss": 1.0535,
      "step": 3275
    },
    {
      "epoch": 0.4605651623787431,
      "grad_norm": 1.5911599397659302,
      "learning_rate": 0.00019446431292384966,
      "loss": 1.0331,
      "step": 3276
    },
    {
      "epoch": 0.4607057500351469,
      "grad_norm": 1.8266139030456543,
      "learning_rate": 0.00019454033075264708,
      "loss": 1.2023,
      "step": 3277
    },
    {
      "epoch": 0.4608463376915507,
      "grad_norm": 1.6196670532226562,
      "learning_rate": 0.00019461583766622721,
      "loss": 1.2538,
      "step": 3278
    },
    {
      "epoch": 0.46098692534795443,
      "grad_norm": 1.5268537998199463,
      "learning_rate": 0.00019469083325653546,
      "loss": 1.3717,
      "step": 3279
    },
    {
      "epoch": 0.4611275130043582,
      "grad_norm": 1.8436176776885986,
      "learning_rate": 0.00019476531711828024,
      "loss": 0.9883,
      "step": 3280
    },
    {
      "epoch": 0.46126810066076196,
      "grad_norm": 1.7099618911743164,
      "learning_rate": 0.0001948392888489357,
      "loss": 1.259,
      "step": 3281
    },
    {
      "epoch": 0.46140868831716575,
      "grad_norm": 1.6978356838226318,
      "learning_rate": 0.0001949127480487434,
      "loss": 1.2455,
      "step": 3282
    },
    {
      "epoch": 0.46154927597356954,
      "grad_norm": 1.585282564163208,
      "learning_rate": 0.000194985694320715,
      "loss": 1.1067,
      "step": 3283
    },
    {
      "epoch": 0.4616898636299733,
      "grad_norm": 2.46761155128479,
      "learning_rate": 0.00019505812727063383,
      "loss": 0.9913,
      "step": 3284
    },
    {
      "epoch": 0.46183045128637706,
      "grad_norm": 1.6439449787139893,
      "learning_rate": 0.00019513004650705757,
      "loss": 1.2007,
      "step": 3285
    },
    {
      "epoch": 0.4619710389427808,
      "grad_norm": 1.6804428100585938,
      "learning_rate": 0.00019520145164131995,
      "loss": 1.1194,
      "step": 3286
    },
    {
      "epoch": 0.4621116265991846,
      "grad_norm": 1.465842843055725,
      "learning_rate": 0.0001952723422875331,
      "loss": 1.1938,
      "step": 3287
    },
    {
      "epoch": 0.4622522142555884,
      "grad_norm": 1.48807692527771,
      "learning_rate": 0.00019534271806258952,
      "loss": 1.0302,
      "step": 3288
    },
    {
      "epoch": 0.4623928019119921,
      "grad_norm": 1.6693642139434814,
      "learning_rate": 0.00019541257858616417,
      "loss": 1.1879,
      "step": 3289
    },
    {
      "epoch": 0.4625333895683959,
      "grad_norm": 1.7700566053390503,
      "learning_rate": 0.00019548192348071655,
      "loss": 0.9621,
      "step": 3290
    },
    {
      "epoch": 0.46267397722479964,
      "grad_norm": 1.6643636226654053,
      "learning_rate": 0.00019555075237149265,
      "loss": 1.1579,
      "step": 3291
    },
    {
      "epoch": 0.4628145648812034,
      "grad_norm": 1.480509877204895,
      "learning_rate": 0.0001956190648865272,
      "loss": 1.1954,
      "step": 3292
    },
    {
      "epoch": 0.4629551525376072,
      "grad_norm": 1.5868602991104126,
      "learning_rate": 0.00019568686065664544,
      "loss": 0.976,
      "step": 3293
    },
    {
      "epoch": 0.46309574019401095,
      "grad_norm": 1.495326042175293,
      "learning_rate": 0.00019575413931546517,
      "loss": 1.1031,
      "step": 3294
    },
    {
      "epoch": 0.46323632785041474,
      "grad_norm": 1.5598524808883667,
      "learning_rate": 0.00019582090049939877,
      "loss": 1.0029,
      "step": 3295
    },
    {
      "epoch": 0.4633769155068185,
      "grad_norm": 1.406623363494873,
      "learning_rate": 0.0001958871438476553,
      "loss": 1.2575,
      "step": 3296
    },
    {
      "epoch": 0.46351750316322227,
      "grad_norm": 1.5317660570144653,
      "learning_rate": 0.00019595286900224212,
      "loss": 1.333,
      "step": 3297
    },
    {
      "epoch": 0.46365809081962606,
      "grad_norm": 1.6207188367843628,
      "learning_rate": 0.00019601807560796713,
      "loss": 1.1224,
      "step": 3298
    },
    {
      "epoch": 0.4637986784760298,
      "grad_norm": 1.5104868412017822,
      "learning_rate": 0.00019608276331244053,
      "loss": 1.0109,
      "step": 3299
    },
    {
      "epoch": 0.4639392661324336,
      "grad_norm": 1.3576815128326416,
      "learning_rate": 0.00019614693176607683,
      "loss": 1.1908,
      "step": 3300
    },
    {
      "epoch": 0.4640798537888373,
      "grad_norm": 1.4787133932113647,
      "learning_rate": 0.00019621058062209654,
      "loss": 1.139,
      "step": 3301
    },
    {
      "epoch": 0.4642204414452411,
      "grad_norm": 1.6530110836029053,
      "learning_rate": 0.0001962737095365283,
      "loss": 1.0969,
      "step": 3302
    },
    {
      "epoch": 0.4643610291016449,
      "grad_norm": 1.628546118736267,
      "learning_rate": 0.0001963363181682106,
      "loss": 1.1805,
      "step": 3303
    },
    {
      "epoch": 0.46450161675804863,
      "grad_norm": 1.8981812000274658,
      "learning_rate": 0.0001963984061787937,
      "loss": 1.2018,
      "step": 3304
    },
    {
      "epoch": 0.4646422044144524,
      "grad_norm": 1.9972995519638062,
      "learning_rate": 0.0001964599732327412,
      "loss": 1.1625,
      "step": 3305
    },
    {
      "epoch": 0.46478279207085615,
      "grad_norm": 1.6538752317428589,
      "learning_rate": 0.0001965210189973323,
      "loss": 1.2088,
      "step": 3306
    },
    {
      "epoch": 0.46492337972725994,
      "grad_norm": 1.7131551504135132,
      "learning_rate": 0.0001965815431426632,
      "loss": 1.1188,
      "step": 3307
    },
    {
      "epoch": 0.46506396738366373,
      "grad_norm": 1.598732352256775,
      "learning_rate": 0.00019664154534164912,
      "loss": 1.0579,
      "step": 3308
    },
    {
      "epoch": 0.46520455504006747,
      "grad_norm": 2.1298539638519287,
      "learning_rate": 0.00019670102527002587,
      "loss": 1.265,
      "step": 3309
    },
    {
      "epoch": 0.46534514269647126,
      "grad_norm": 1.5860410928726196,
      "learning_rate": 0.00019675998260635183,
      "loss": 1.2048,
      "step": 3310
    },
    {
      "epoch": 0.465485730352875,
      "grad_norm": 1.5889828205108643,
      "learning_rate": 0.0001968184170320096,
      "loss": 1.1385,
      "step": 3311
    },
    {
      "epoch": 0.4656263180092788,
      "grad_norm": 1.518093466758728,
      "learning_rate": 0.00019687632823120753,
      "loss": 1.1532,
      "step": 3312
    },
    {
      "epoch": 0.4657669056656826,
      "grad_norm": 1.5684781074523926,
      "learning_rate": 0.00019693371589098177,
      "loss": 1.0994,
      "step": 3313
    },
    {
      "epoch": 0.4659074933220863,
      "grad_norm": 1.7670962810516357,
      "learning_rate": 0.00019699057970119766,
      "loss": 1.0397,
      "step": 3314
    },
    {
      "epoch": 0.4660480809784901,
      "grad_norm": 1.4377338886260986,
      "learning_rate": 0.00019704691935455162,
      "loss": 1.0747,
      "step": 3315
    },
    {
      "epoch": 0.46618866863489383,
      "grad_norm": 1.7336262464523315,
      "learning_rate": 0.00019710273454657265,
      "loss": 1.2266,
      "step": 3316
    },
    {
      "epoch": 0.4663292562912976,
      "grad_norm": 1.4929819107055664,
      "learning_rate": 0.00019715802497562415,
      "loss": 1.0479,
      "step": 3317
    },
    {
      "epoch": 0.4664698439477014,
      "grad_norm": 1.9454610347747803,
      "learning_rate": 0.00019721279034290524,
      "loss": 1.1156,
      "step": 3318
    },
    {
      "epoch": 0.46661043160410515,
      "grad_norm": 1.6175256967544556,
      "learning_rate": 0.00019726703035245283,
      "loss": 1.0831,
      "step": 3319
    },
    {
      "epoch": 0.46675101926050894,
      "grad_norm": 1.4404327869415283,
      "learning_rate": 0.00019732074471114278,
      "loss": 1.1626,
      "step": 3320
    },
    {
      "epoch": 0.46689160691691267,
      "grad_norm": 1.7768429517745972,
      "learning_rate": 0.00019737393312869178,
      "loss": 1.1174,
      "step": 3321
    },
    {
      "epoch": 0.46703219457331646,
      "grad_norm": 1.607519507408142,
      "learning_rate": 0.00019742659531765878,
      "loss": 1.1314,
      "step": 3322
    },
    {
      "epoch": 0.46717278222972025,
      "grad_norm": 1.6154793500900269,
      "learning_rate": 0.00019747873099344656,
      "loss": 1.2256,
      "step": 3323
    },
    {
      "epoch": 0.467313369886124,
      "grad_norm": 1.8441311120986938,
      "learning_rate": 0.0001975303398743033,
      "loss": 0.9434,
      "step": 3324
    },
    {
      "epoch": 0.4674539575425278,
      "grad_norm": 1.520572543144226,
      "learning_rate": 0.00019758142168132417,
      "loss": 1.0673,
      "step": 3325
    },
    {
      "epoch": 0.4675945451989315,
      "grad_norm": 1.595366358757019,
      "learning_rate": 0.00019763197613845255,
      "loss": 1.0884,
      "step": 3326
    },
    {
      "epoch": 0.4677351328553353,
      "grad_norm": 2.324237585067749,
      "learning_rate": 0.00019768200297248193,
      "loss": 0.9224,
      "step": 3327
    },
    {
      "epoch": 0.4678757205117391,
      "grad_norm": 1.6157039403915405,
      "learning_rate": 0.00019773150191305705,
      "loss": 1.1784,
      "step": 3328
    },
    {
      "epoch": 0.4680163081681428,
      "grad_norm": 1.874991536140442,
      "learning_rate": 0.00019778047269267558,
      "loss": 1.1103,
      "step": 3329
    },
    {
      "epoch": 0.4681568958245466,
      "grad_norm": 1.5767600536346436,
      "learning_rate": 0.00019782891504668943,
      "loss": 1.2781,
      "step": 3330
    },
    {
      "epoch": 0.46829748348095035,
      "grad_norm": 1.5732533931732178,
      "learning_rate": 0.00019787682871330627,
      "loss": 1.2103,
      "step": 3331
    },
    {
      "epoch": 0.46843807113735414,
      "grad_norm": 1.6820166110992432,
      "learning_rate": 0.00019792421343359085,
      "loss": 1.0244,
      "step": 3332
    },
    {
      "epoch": 0.46857865879375793,
      "grad_norm": 1.403633952140808,
      "learning_rate": 0.0001979710689514665,
      "loss": 1.2601,
      "step": 3333
    },
    {
      "epoch": 0.46871924645016166,
      "grad_norm": 1.613083004951477,
      "learning_rate": 0.00019801739501371646,
      "loss": 0.9558,
      "step": 3334
    },
    {
      "epoch": 0.46885983410656545,
      "grad_norm": 1.753831386566162,
      "learning_rate": 0.0001980631913699852,
      "loss": 1.0277,
      "step": 3335
    },
    {
      "epoch": 0.4690004217629692,
      "grad_norm": 1.5759246349334717,
      "learning_rate": 0.00019810845777277995,
      "loss": 1.0917,
      "step": 3336
    },
    {
      "epoch": 0.469141009419373,
      "grad_norm": 1.6166754961013794,
      "learning_rate": 0.00019815319397747177,
      "loss": 1.0711,
      "step": 3337
    },
    {
      "epoch": 0.46928159707577677,
      "grad_norm": 1.6785099506378174,
      "learning_rate": 0.00019819739974229715,
      "loss": 0.9907,
      "step": 3338
    },
    {
      "epoch": 0.4694221847321805,
      "grad_norm": 1.4258477687835693,
      "learning_rate": 0.00019824107482835905,
      "loss": 1.1573,
      "step": 3339
    },
    {
      "epoch": 0.4695627723885843,
      "grad_norm": 1.6837691068649292,
      "learning_rate": 0.00019828421899962853,
      "loss": 1.1816,
      "step": 3340
    },
    {
      "epoch": 0.469703360044988,
      "grad_norm": 1.6297104358673096,
      "learning_rate": 0.00019832683202294557,
      "loss": 0.954,
      "step": 3341
    },
    {
      "epoch": 0.4698439477013918,
      "grad_norm": 1.6349260807037354,
      "learning_rate": 0.00019836891366802078,
      "loss": 1.2856,
      "step": 3342
    },
    {
      "epoch": 0.4699845353577956,
      "grad_norm": 1.4602775573730469,
      "learning_rate": 0.0001984104637074363,
      "loss": 1.0721,
      "step": 3343
    },
    {
      "epoch": 0.47012512301419934,
      "grad_norm": 1.5848400592803955,
      "learning_rate": 0.00019845148191664734,
      "loss": 1.0537,
      "step": 3344
    },
    {
      "epoch": 0.47026571067060313,
      "grad_norm": 1.6370667219161987,
      "learning_rate": 0.00019849196807398306,
      "loss": 1.0044,
      "step": 3345
    },
    {
      "epoch": 0.47040629832700687,
      "grad_norm": 1.5269997119903564,
      "learning_rate": 0.00019853192196064807,
      "loss": 1.1678,
      "step": 3346
    },
    {
      "epoch": 0.47054688598341066,
      "grad_norm": 1.982067584991455,
      "learning_rate": 0.0001985713433607234,
      "loss": 1.1119,
      "step": 3347
    },
    {
      "epoch": 0.47068747363981445,
      "grad_norm": 1.565603494644165,
      "learning_rate": 0.00019861023206116775,
      "loss": 1.2034,
      "step": 3348
    },
    {
      "epoch": 0.4708280612962182,
      "grad_norm": 1.4361058473587036,
      "learning_rate": 0.00019864858785181868,
      "loss": 1.0927,
      "step": 3349
    },
    {
      "epoch": 0.47096864895262197,
      "grad_norm": 1.5688005685806274,
      "learning_rate": 0.00019868641052539368,
      "loss": 1.2383,
      "step": 3350
    },
    {
      "epoch": 0.4711092366090257,
      "grad_norm": 1.4878824949264526,
      "learning_rate": 0.00019872369987749132,
      "loss": 1.0869,
      "step": 3351
    },
    {
      "epoch": 0.4712498242654295,
      "grad_norm": 1.5821523666381836,
      "learning_rate": 0.0001987604557065923,
      "loss": 1.0749,
      "step": 3352
    },
    {
      "epoch": 0.4713904119218333,
      "grad_norm": 1.6060481071472168,
      "learning_rate": 0.00019879667781406063,
      "loss": 1.099,
      "step": 3353
    },
    {
      "epoch": 0.471530999578237,
      "grad_norm": 1.5122332572937012,
      "learning_rate": 0.0001988323660041447,
      "loss": 1.1725,
      "step": 3354
    },
    {
      "epoch": 0.4716715872346408,
      "grad_norm": 1.6417362689971924,
      "learning_rate": 0.00019886752008397828,
      "loss": 1.149,
      "step": 3355
    },
    {
      "epoch": 0.47181217489104454,
      "grad_norm": 1.414832353591919,
      "learning_rate": 0.00019890213986358148,
      "loss": 1.0392,
      "step": 3356
    },
    {
      "epoch": 0.47195276254744833,
      "grad_norm": 1.7045929431915283,
      "learning_rate": 0.00019893622515586198,
      "loss": 0.9764,
      "step": 3357
    },
    {
      "epoch": 0.4720933502038521,
      "grad_norm": 1.4909868240356445,
      "learning_rate": 0.00019896977577661592,
      "loss": 1.3567,
      "step": 3358
    },
    {
      "epoch": 0.47223393786025586,
      "grad_norm": 1.5450488328933716,
      "learning_rate": 0.00019900279154452897,
      "loss": 1.1675,
      "step": 3359
    },
    {
      "epoch": 0.47237452551665965,
      "grad_norm": 1.6774107217788696,
      "learning_rate": 0.00019903527228117706,
      "loss": 1.0206,
      "step": 3360
    },
    {
      "epoch": 0.4725151131730634,
      "grad_norm": 1.4228885173797607,
      "learning_rate": 0.0001990672178110278,
      "loss": 1.0393,
      "step": 3361
    },
    {
      "epoch": 0.4726557008294672,
      "grad_norm": 1.5823191404342651,
      "learning_rate": 0.00019909862796144094,
      "loss": 1.2527,
      "step": 3362
    },
    {
      "epoch": 0.47279628848587096,
      "grad_norm": 1.5499451160430908,
      "learning_rate": 0.00019912950256266966,
      "loss": 0.9385,
      "step": 3363
    },
    {
      "epoch": 0.4729368761422747,
      "grad_norm": 1.5034055709838867,
      "learning_rate": 0.00019915984144786133,
      "loss": 1.1385,
      "step": 3364
    },
    {
      "epoch": 0.4730774637986785,
      "grad_norm": 1.7582358121871948,
      "learning_rate": 0.00019918964445305842,
      "loss": 1.0718,
      "step": 3365
    },
    {
      "epoch": 0.4732180514550822,
      "grad_norm": 1.5986504554748535,
      "learning_rate": 0.00019921891141719945,
      "loss": 1.1471,
      "step": 3366
    },
    {
      "epoch": 0.473358639111486,
      "grad_norm": 1.540013074874878,
      "learning_rate": 0.0001992476421821197,
      "loss": 1.1587,
      "step": 3367
    },
    {
      "epoch": 0.4734992267678898,
      "grad_norm": 1.4725242853164673,
      "learning_rate": 0.00019927583659255235,
      "loss": 1.1817,
      "step": 3368
    },
    {
      "epoch": 0.47363981442429354,
      "grad_norm": 2.007878065109253,
      "learning_rate": 0.00019930349449612895,
      "loss": 1.1868,
      "step": 3369
    },
    {
      "epoch": 0.4737804020806973,
      "grad_norm": 1.5524427890777588,
      "learning_rate": 0.00019933061574338068,
      "loss": 1.1385,
      "step": 3370
    },
    {
      "epoch": 0.47392098973710106,
      "grad_norm": 1.7926955223083496,
      "learning_rate": 0.00019935720018773872,
      "loss": 1.0657,
      "step": 3371
    },
    {
      "epoch": 0.47406157739350485,
      "grad_norm": 1.639826774597168,
      "learning_rate": 0.00019938324768553533,
      "loss": 1.053,
      "step": 3372
    },
    {
      "epoch": 0.47420216504990864,
      "grad_norm": 1.911899447441101,
      "learning_rate": 0.0001994087580960045,
      "loss": 1.1328,
      "step": 3373
    },
    {
      "epoch": 0.4743427527063124,
      "grad_norm": 1.2169060707092285,
      "learning_rate": 0.0001994337312812828,
      "loss": 1.1943,
      "step": 3374
    },
    {
      "epoch": 0.47448334036271617,
      "grad_norm": 1.5074080228805542,
      "learning_rate": 0.00019945816710641002,
      "loss": 1.1349,
      "step": 3375
    },
    {
      "epoch": 0.4746239280191199,
      "grad_norm": 1.6493173837661743,
      "learning_rate": 0.00019948206543933005,
      "loss": 1.2151,
      "step": 3376
    },
    {
      "epoch": 0.4747645156755237,
      "grad_norm": 1.6262887716293335,
      "learning_rate": 0.00019950542615089132,
      "loss": 1.1828,
      "step": 3377
    },
    {
      "epoch": 0.4749051033319275,
      "grad_norm": 1.377705454826355,
      "learning_rate": 0.00019952824911484784,
      "loss": 1.0237,
      "step": 3378
    },
    {
      "epoch": 0.4750456909883312,
      "grad_norm": 1.5686708688735962,
      "learning_rate": 0.0001995505342078597,
      "loss": 1.0045,
      "step": 3379
    },
    {
      "epoch": 0.475186278644735,
      "grad_norm": 1.5970723628997803,
      "learning_rate": 0.00019957228130949365,
      "loss": 1.0428,
      "step": 3380
    },
    {
      "epoch": 0.47532686630113874,
      "grad_norm": 1.6592744588851929,
      "learning_rate": 0.00019959349030222395,
      "loss": 1.1687,
      "step": 3381
    },
    {
      "epoch": 0.47546745395754253,
      "grad_norm": 1.4892263412475586,
      "learning_rate": 0.00019961416107143286,
      "loss": 1.2041,
      "step": 3382
    },
    {
      "epoch": 0.4756080416139463,
      "grad_norm": 1.5162616968154907,
      "learning_rate": 0.00019963429350541137,
      "loss": 1.0068,
      "step": 3383
    },
    {
      "epoch": 0.47574862927035005,
      "grad_norm": 2.102450370788574,
      "learning_rate": 0.00019965388749535964,
      "loss": 1.2301,
      "step": 3384
    },
    {
      "epoch": 0.47588921692675384,
      "grad_norm": 1.6222028732299805,
      "learning_rate": 0.0001996729429353878,
      "loss": 0.9122,
      "step": 3385
    },
    {
      "epoch": 0.4760298045831576,
      "grad_norm": 1.7526503801345825,
      "learning_rate": 0.0001996914597225164,
      "loss": 1.2955,
      "step": 3386
    },
    {
      "epoch": 0.47617039223956137,
      "grad_norm": 1.7738004922866821,
      "learning_rate": 0.00019970943775667686,
      "loss": 1.4021,
      "step": 3387
    },
    {
      "epoch": 0.47631097989596516,
      "grad_norm": 1.603786587715149,
      "learning_rate": 0.0001997268769407123,
      "loss": 1.1692,
      "step": 3388
    },
    {
      "epoch": 0.4764515675523689,
      "grad_norm": 1.8415170907974243,
      "learning_rate": 0.0001997437771803778,
      "loss": 1.1525,
      "step": 3389
    },
    {
      "epoch": 0.4765921552087727,
      "grad_norm": 1.7206870317459106,
      "learning_rate": 0.00019976013838434096,
      "loss": 1.0135,
      "step": 3390
    },
    {
      "epoch": 0.4767327428651764,
      "grad_norm": 1.6455782651901245,
      "learning_rate": 0.00019977596046418257,
      "loss": 1.0813,
      "step": 3391
    },
    {
      "epoch": 0.4768733305215802,
      "grad_norm": 1.3065447807312012,
      "learning_rate": 0.00019979124333439682,
      "loss": 1.0208,
      "step": 3392
    },
    {
      "epoch": 0.477013918177984,
      "grad_norm": 1.605247139930725,
      "learning_rate": 0.00019980598691239206,
      "loss": 1.0776,
      "step": 3393
    },
    {
      "epoch": 0.47715450583438773,
      "grad_norm": 1.5926387310028076,
      "learning_rate": 0.00019982019111849087,
      "loss": 1.2742,
      "step": 3394
    },
    {
      "epoch": 0.4772950934907915,
      "grad_norm": 1.6697275638580322,
      "learning_rate": 0.00019983385587593093,
      "loss": 1.1789,
      "step": 3395
    },
    {
      "epoch": 0.47743568114719526,
      "grad_norm": 1.5768369436264038,
      "learning_rate": 0.00019984698111086505,
      "loss": 1.1894,
      "step": 3396
    },
    {
      "epoch": 0.47757626880359905,
      "grad_norm": 1.8101179599761963,
      "learning_rate": 0.00019985956675236177,
      "loss": 1.0172,
      "step": 3397
    },
    {
      "epoch": 0.47771685646000284,
      "grad_norm": 1.585906982421875,
      "learning_rate": 0.00019987161273240579,
      "loss": 1.1674,
      "step": 3398
    },
    {
      "epoch": 0.47785744411640657,
      "grad_norm": 1.5611917972564697,
      "learning_rate": 0.0001998831189858981,
      "loss": 1.1056,
      "step": 3399
    },
    {
      "epoch": 0.47799803177281036,
      "grad_norm": 1.4002258777618408,
      "learning_rate": 0.0001998940854506566,
      "loss": 1.2217,
      "step": 3400
    },
    {
      "epoch": 0.4781386194292141,
      "grad_norm": 1.6283776760101318,
      "learning_rate": 0.0001999045120674163,
      "loss": 1.0639,
      "step": 3401
    },
    {
      "epoch": 0.4782792070856179,
      "grad_norm": 1.602163314819336,
      "learning_rate": 0.0001999143987798296,
      "loss": 1.0796,
      "step": 3402
    },
    {
      "epoch": 0.4784197947420217,
      "grad_norm": 1.6284319162368774,
      "learning_rate": 0.00019992374553446667,
      "loss": 1.1798,
      "step": 3403
    },
    {
      "epoch": 0.4785603823984254,
      "grad_norm": 1.6477832794189453,
      "learning_rate": 0.0001999325522808158,
      "loss": 1.159,
      "step": 3404
    },
    {
      "epoch": 0.4787009700548292,
      "grad_norm": 1.3184211254119873,
      "learning_rate": 0.0001999408189712835,
      "loss": 1.0582,
      "step": 3405
    },
    {
      "epoch": 0.47884155771123293,
      "grad_norm": 1.6036186218261719,
      "learning_rate": 0.0001999485455611949,
      "loss": 1.1424,
      "step": 3406
    },
    {
      "epoch": 0.4789821453676367,
      "grad_norm": 1.478720784187317,
      "learning_rate": 0.00019995573200879397,
      "loss": 1.1076,
      "step": 3407
    },
    {
      "epoch": 0.4791227330240405,
      "grad_norm": 1.2861512899398804,
      "learning_rate": 0.0001999623782752436,
      "loss": 0.9757,
      "step": 3408
    },
    {
      "epoch": 0.47926332068044425,
      "grad_norm": 1.4720852375030518,
      "learning_rate": 0.00019996848432462608,
      "loss": 1.3591,
      "step": 3409
    },
    {
      "epoch": 0.47940390833684804,
      "grad_norm": 1.7304606437683105,
      "learning_rate": 0.00019997405012394306,
      "loss": 0.8975,
      "step": 3410
    },
    {
      "epoch": 0.4795444959932518,
      "grad_norm": 1.7161039113998413,
      "learning_rate": 0.00019997907564311583,
      "loss": 1.1041,
      "step": 3411
    },
    {
      "epoch": 0.47968508364965556,
      "grad_norm": 1.5034428834915161,
      "learning_rate": 0.0001999835608549854,
      "loss": 1.1375,
      "step": 3412
    },
    {
      "epoch": 0.47982567130605935,
      "grad_norm": 1.8445419073104858,
      "learning_rate": 0.0001999875057353129,
      "loss": 1.0335,
      "step": 3413
    },
    {
      "epoch": 0.4799662589624631,
      "grad_norm": 1.7106130123138428,
      "learning_rate": 0.00019999091026277928,
      "loss": 1.1542,
      "step": 3414
    },
    {
      "epoch": 0.4801068466188669,
      "grad_norm": 1.4896001815795898,
      "learning_rate": 0.0001999937744189858,
      "loss": 1.0498,
      "step": 3415
    },
    {
      "epoch": 0.4802474342752706,
      "grad_norm": 1.699540138244629,
      "learning_rate": 0.00019999609818845398,
      "loss": 1.1212,
      "step": 3416
    },
    {
      "epoch": 0.4803880219316744,
      "grad_norm": 1.411373257637024,
      "learning_rate": 0.00019999788155862573,
      "loss": 1.1356,
      "step": 3417
    },
    {
      "epoch": 0.4805286095880782,
      "grad_norm": 1.4942293167114258,
      "learning_rate": 0.0001999991245198633,
      "loss": 1.1498,
      "step": 3418
    },
    {
      "epoch": 0.4806691972444819,
      "grad_norm": 1.5200082063674927,
      "learning_rate": 0.00019999982706544954,
      "loss": 1.125,
      "step": 3419
    },
    {
      "epoch": 0.4808097849008857,
      "grad_norm": 1.6338897943496704,
      "learning_rate": 0.00019999998919158768,
      "loss": 1.1915,
      "step": 3420
    },
    {
      "epoch": 0.48095037255728945,
      "grad_norm": 1.7035242319107056,
      "learning_rate": 0.0001999996108974016,
      "loss": 1.1257,
      "step": 3421
    },
    {
      "epoch": 0.48109096021369324,
      "grad_norm": 1.7155303955078125,
      "learning_rate": 0.00019999869218493568,
      "loss": 1.0703,
      "step": 3422
    },
    {
      "epoch": 0.48123154787009703,
      "grad_norm": 1.9401447772979736,
      "learning_rate": 0.00019999723305915484,
      "loss": 0.9731,
      "step": 3423
    },
    {
      "epoch": 0.48137213552650077,
      "grad_norm": 1.555639624595642,
      "learning_rate": 0.00019999523352794442,
      "loss": 1.0495,
      "step": 3424
    },
    {
      "epoch": 0.48151272318290456,
      "grad_norm": 1.5827713012695312,
      "learning_rate": 0.0001999926936021104,
      "loss": 1.0696,
      "step": 3425
    },
    {
      "epoch": 0.4816533108393083,
      "grad_norm": 2.227073907852173,
      "learning_rate": 0.00019998961329537897,
      "loss": 1.192,
      "step": 3426
    },
    {
      "epoch": 0.4817938984957121,
      "grad_norm": 1.5356168746948242,
      "learning_rate": 0.00019998599262439679,
      "loss": 1.115,
      "step": 3427
    },
    {
      "epoch": 0.48193448615211587,
      "grad_norm": 1.6472971439361572,
      "learning_rate": 0.00019998183160873067,
      "loss": 1.0981,
      "step": 3428
    },
    {
      "epoch": 0.4820750738085196,
      "grad_norm": 1.6813911199569702,
      "learning_rate": 0.0001999771302708676,
      "loss": 1.135,
      "step": 3429
    },
    {
      "epoch": 0.4822156614649234,
      "grad_norm": 1.636351227760315,
      "learning_rate": 0.00019997188863621457,
      "loss": 1.0808,
      "step": 3430
    },
    {
      "epoch": 0.48235624912132713,
      "grad_norm": 1.7487175464630127,
      "learning_rate": 0.00019996610673309845,
      "loss": 1.2532,
      "step": 3431
    },
    {
      "epoch": 0.4824968367777309,
      "grad_norm": 1.7238794565200806,
      "learning_rate": 0.0001999597845927658,
      "loss": 1.0662,
      "step": 3432
    },
    {
      "epoch": 0.4826374244341347,
      "grad_norm": 1.5761266946792603,
      "learning_rate": 0.00019995292224938278,
      "loss": 1.0931,
      "step": 3433
    },
    {
      "epoch": 0.48277801209053844,
      "grad_norm": 1.469896912574768,
      "learning_rate": 0.00019994551974003488,
      "loss": 1.169,
      "step": 3434
    },
    {
      "epoch": 0.48291859974694223,
      "grad_norm": 1.6635735034942627,
      "learning_rate": 0.00019993757710472677,
      "loss": 1.1513,
      "step": 3435
    },
    {
      "epoch": 0.48305918740334597,
      "grad_norm": 1.376404047012329,
      "learning_rate": 0.00019992909438638204,
      "loss": 1.2194,
      "step": 3436
    },
    {
      "epoch": 0.48319977505974976,
      "grad_norm": 1.6121550798416138,
      "learning_rate": 0.0001999200716308431,
      "loss": 1.1513,
      "step": 3437
    },
    {
      "epoch": 0.48334036271615355,
      "grad_norm": 1.567616581916809,
      "learning_rate": 0.0001999105088868707,
      "loss": 1.2189,
      "step": 3438
    },
    {
      "epoch": 0.4834809503725573,
      "grad_norm": 1.4781194925308228,
      "learning_rate": 0.00019990040620614387,
      "loss": 1.2117,
      "step": 3439
    },
    {
      "epoch": 0.4836215380289611,
      "grad_norm": 1.7202019691467285,
      "learning_rate": 0.00019988976364325957,
      "loss": 1.0559,
      "step": 3440
    },
    {
      "epoch": 0.4837621256853648,
      "grad_norm": 1.6376553773880005,
      "learning_rate": 0.00019987858125573236,
      "loss": 0.8842,
      "step": 3441
    },
    {
      "epoch": 0.4839027133417686,
      "grad_norm": 1.376672387123108,
      "learning_rate": 0.00019986685910399418,
      "loss": 1.2571,
      "step": 3442
    },
    {
      "epoch": 0.4840433009981724,
      "grad_norm": 1.311773419380188,
      "learning_rate": 0.0001998545972513939,
      "loss": 0.973,
      "step": 3443
    },
    {
      "epoch": 0.4841838886545761,
      "grad_norm": 1.3390560150146484,
      "learning_rate": 0.00019984179576419705,
      "loss": 1.0776,
      "step": 3444
    },
    {
      "epoch": 0.4843244763109799,
      "grad_norm": 1.8996272087097168,
      "learning_rate": 0.00019982845471158548,
      "loss": 1.0858,
      "step": 3445
    },
    {
      "epoch": 0.48446506396738365,
      "grad_norm": 1.4217652082443237,
      "learning_rate": 0.000199814574165657,
      "loss": 1.3005,
      "step": 3446
    },
    {
      "epoch": 0.48460565162378744,
      "grad_norm": 1.466475486755371,
      "learning_rate": 0.0001998001542014249,
      "loss": 1.1148,
      "step": 3447
    },
    {
      "epoch": 0.4847462392801912,
      "grad_norm": 1.4954991340637207,
      "learning_rate": 0.00019978519489681756,
      "loss": 0.9326,
      "step": 3448
    },
    {
      "epoch": 0.48488682693659496,
      "grad_norm": 1.9856195449829102,
      "learning_rate": 0.00019976969633267815,
      "loss": 1.1484,
      "step": 3449
    },
    {
      "epoch": 0.48502741459299875,
      "grad_norm": 1.6031893491744995,
      "learning_rate": 0.00019975365859276406,
      "loss": 1.0996,
      "step": 3450
    },
    {
      "epoch": 0.4851680022494025,
      "grad_norm": 1.8179928064346313,
      "learning_rate": 0.0001997370817637465,
      "loss": 1.0882,
      "step": 3451
    },
    {
      "epoch": 0.4853085899058063,
      "grad_norm": 1.552886962890625,
      "learning_rate": 0.00019971996593521008,
      "loss": 1.2606,
      "step": 3452
    },
    {
      "epoch": 0.48544917756221,
      "grad_norm": 1.5957696437835693,
      "learning_rate": 0.00019970231119965215,
      "loss": 0.9991,
      "step": 3453
    },
    {
      "epoch": 0.4855897652186138,
      "grad_norm": 1.5699517726898193,
      "learning_rate": 0.0001996841176524825,
      "loss": 1.1576,
      "step": 3454
    },
    {
      "epoch": 0.4857303528750176,
      "grad_norm": 1.7023415565490723,
      "learning_rate": 0.00019966538539202287,
      "loss": 1.0385,
      "step": 3455
    },
    {
      "epoch": 0.4858709405314213,
      "grad_norm": 1.7766003608703613,
      "learning_rate": 0.0001996461145195061,
      "loss": 1.0329,
      "step": 3456
    },
    {
      "epoch": 0.4860115281878251,
      "grad_norm": 2.0209293365478516,
      "learning_rate": 0.00019962630513907596,
      "loss": 1.1715,
      "step": 3457
    },
    {
      "epoch": 0.48615211584422885,
      "grad_norm": 1.506122350692749,
      "learning_rate": 0.0001996059573577864,
      "loss": 1.1564,
      "step": 3458
    },
    {
      "epoch": 0.48629270350063264,
      "grad_norm": 1.3697240352630615,
      "learning_rate": 0.00019958507128560096,
      "loss": 1.1852,
      "step": 3459
    },
    {
      "epoch": 0.48643329115703643,
      "grad_norm": 1.672161340713501,
      "learning_rate": 0.00019956364703539218,
      "loss": 1.2014,
      "step": 3460
    },
    {
      "epoch": 0.48657387881344016,
      "grad_norm": 1.8319733142852783,
      "learning_rate": 0.00019954168472294118,
      "loss": 1.0503,
      "step": 3461
    },
    {
      "epoch": 0.48671446646984395,
      "grad_norm": 1.5017945766448975,
      "learning_rate": 0.00019951918446693668,
      "loss": 1.0806,
      "step": 3462
    },
    {
      "epoch": 0.4868550541262477,
      "grad_norm": 1.6148134469985962,
      "learning_rate": 0.0001994961463889747,
      "loss": 1.2438,
      "step": 3463
    },
    {
      "epoch": 0.4869956417826515,
      "grad_norm": 1.7978739738464355,
      "learning_rate": 0.00019947257061355767,
      "loss": 0.9002,
      "step": 3464
    },
    {
      "epoch": 0.48713622943905527,
      "grad_norm": 1.3880726099014282,
      "learning_rate": 0.0001994484572680939,
      "loss": 1.3301,
      "step": 3465
    },
    {
      "epoch": 0.487276817095459,
      "grad_norm": 1.6025991439819336,
      "learning_rate": 0.00019942380648289688,
      "loss": 0.9888,
      "step": 3466
    },
    {
      "epoch": 0.4874174047518628,
      "grad_norm": 1.3286842107772827,
      "learning_rate": 0.00019939861839118441,
      "loss": 1.181,
      "step": 3467
    },
    {
      "epoch": 0.4875579924082665,
      "grad_norm": 1.515671968460083,
      "learning_rate": 0.0001993728931290781,
      "loss": 1.2186,
      "step": 3468
    },
    {
      "epoch": 0.4876985800646703,
      "grad_norm": 1.512807011604309,
      "learning_rate": 0.0001993466308356025,
      "loss": 1.1193,
      "step": 3469
    },
    {
      "epoch": 0.4878391677210741,
      "grad_norm": 1.4050400257110596,
      "learning_rate": 0.00019931983165268438,
      "loss": 1.1505,
      "step": 3470
    },
    {
      "epoch": 0.48797975537747784,
      "grad_norm": 1.9276859760284424,
      "learning_rate": 0.000199292495725152,
      "loss": 1.0555,
      "step": 3471
    },
    {
      "epoch": 0.48812034303388163,
      "grad_norm": 1.5585317611694336,
      "learning_rate": 0.00019926462320073429,
      "loss": 1.181,
      "step": 3472
    },
    {
      "epoch": 0.48826093069028537,
      "grad_norm": 1.4719743728637695,
      "learning_rate": 0.00019923621423006006,
      "loss": 1.0434,
      "step": 3473
    },
    {
      "epoch": 0.48840151834668916,
      "grad_norm": 1.5573036670684814,
      "learning_rate": 0.00019920726896665723,
      "loss": 0.9643,
      "step": 3474
    },
    {
      "epoch": 0.48854210600309295,
      "grad_norm": 1.8796683549880981,
      "learning_rate": 0.00019917778756695177,
      "loss": 1.1414,
      "step": 3475
    },
    {
      "epoch": 0.4886826936594967,
      "grad_norm": 1.4987471103668213,
      "learning_rate": 0.00019914777019026727,
      "loss": 1.2388,
      "step": 3476
    },
    {
      "epoch": 0.48882328131590047,
      "grad_norm": 1.4048137664794922,
      "learning_rate": 0.0001991172169988237,
      "loss": 1.1397,
      "step": 3477
    },
    {
      "epoch": 0.4889638689723042,
      "grad_norm": 1.7440463304519653,
      "learning_rate": 0.00019908612815773682,
      "loss": 1.2548,
      "step": 3478
    },
    {
      "epoch": 0.489104456628708,
      "grad_norm": 1.610785722732544,
      "learning_rate": 0.00019905450383501697,
      "loss": 1.1152,
      "step": 3479
    },
    {
      "epoch": 0.4892450442851118,
      "grad_norm": 1.7589292526245117,
      "learning_rate": 0.00019902234420156849,
      "loss": 1.0287,
      "step": 3480
    },
    {
      "epoch": 0.4893856319415155,
      "grad_norm": 1.8279848098754883,
      "learning_rate": 0.00019898964943118857,
      "loss": 0.9825,
      "step": 3481
    },
    {
      "epoch": 0.4895262195979193,
      "grad_norm": 2.0163145065307617,
      "learning_rate": 0.00019895641970056643,
      "loss": 1.0598,
      "step": 3482
    },
    {
      "epoch": 0.48966680725432304,
      "grad_norm": 1.760863184928894,
      "learning_rate": 0.00019892265518928228,
      "loss": 1.041,
      "step": 3483
    },
    {
      "epoch": 0.48980739491072683,
      "grad_norm": 1.4557090997695923,
      "learning_rate": 0.00019888835607980644,
      "loss": 1.0896,
      "step": 3484
    },
    {
      "epoch": 0.4899479825671306,
      "grad_norm": 1.3904736042022705,
      "learning_rate": 0.0001988535225574983,
      "loss": 1.1218,
      "step": 3485
    },
    {
      "epoch": 0.49008857022353436,
      "grad_norm": 1.5134161710739136,
      "learning_rate": 0.0001988181548106053,
      "loss": 0.9855,
      "step": 3486
    },
    {
      "epoch": 0.49022915787993815,
      "grad_norm": 1.5753072500228882,
      "learning_rate": 0.00019878225303026197,
      "loss": 1.1725,
      "step": 3487
    },
    {
      "epoch": 0.4903697455363419,
      "grad_norm": 1.6741448640823364,
      "learning_rate": 0.00019874581741048878,
      "loss": 1.0701,
      "step": 3488
    },
    {
      "epoch": 0.4905103331927457,
      "grad_norm": 1.6311262845993042,
      "learning_rate": 0.00019870884814819135,
      "loss": 1.1096,
      "step": 3489
    },
    {
      "epoch": 0.49065092084914946,
      "grad_norm": 1.5495127439498901,
      "learning_rate": 0.00019867134544315903,
      "loss": 1.1031,
      "step": 3490
    },
    {
      "epoch": 0.4907915085055532,
      "grad_norm": 1.4549237489700317,
      "learning_rate": 0.00019863330949806415,
      "loss": 1.2845,
      "step": 3491
    },
    {
      "epoch": 0.490932096161957,
      "grad_norm": 1.4402852058410645,
      "learning_rate": 0.00019859474051846064,
      "loss": 1.0099,
      "step": 3492
    },
    {
      "epoch": 0.4910726838183607,
      "grad_norm": 1.6489686965942383,
      "learning_rate": 0.00019855563871278316,
      "loss": 1.1968,
      "step": 3493
    },
    {
      "epoch": 0.4912132714747645,
      "grad_norm": 1.7465388774871826,
      "learning_rate": 0.00019851600429234584,
      "loss": 1.1014,
      "step": 3494
    },
    {
      "epoch": 0.4913538591311683,
      "grad_norm": 1.3914461135864258,
      "learning_rate": 0.00019847583747134118,
      "loss": 1.1588,
      "step": 3495
    },
    {
      "epoch": 0.49149444678757204,
      "grad_norm": 1.6746124029159546,
      "learning_rate": 0.0001984351384668388,
      "loss": 1.0578,
      "step": 3496
    },
    {
      "epoch": 0.4916350344439758,
      "grad_norm": 1.4225691556930542,
      "learning_rate": 0.0001983939074987845,
      "loss": 1.0351,
      "step": 3497
    },
    {
      "epoch": 0.49177562210037956,
      "grad_norm": 1.4606013298034668,
      "learning_rate": 0.00019835214478999878,
      "loss": 1.0754,
      "step": 3498
    },
    {
      "epoch": 0.49191620975678335,
      "grad_norm": 1.2552516460418701,
      "learning_rate": 0.00019830985056617587,
      "loss": 1.2484,
      "step": 3499
    },
    {
      "epoch": 0.49205679741318714,
      "grad_norm": 1.3750171661376953,
      "learning_rate": 0.00019826702505588238,
      "loss": 1.0905,
      "step": 3500
    },
    {
      "epoch": 0.49205679741318714,
      "eval_loss": 1.1756529808044434,
      "eval_runtime": 771.4931,
      "eval_samples_per_second": 16.392,
      "eval_steps_per_second": 8.196,
      "step": 3500
    },
    {
      "epoch": 0.4921973850695909,
      "grad_norm": 1.7237358093261719,
      "learning_rate": 0.00019822366849055604,
      "loss": 1.0848,
      "step": 3501
    },
    {
      "epoch": 0.49233797272599467,
      "grad_norm": 1.8151134252548218,
      "learning_rate": 0.0001981797811045046,
      "loss": 1.1212,
      "step": 3502
    },
    {
      "epoch": 0.4924785603823984,
      "grad_norm": 1.531727910041809,
      "learning_rate": 0.00019813536313490447,
      "loss": 1.057,
      "step": 3503
    },
    {
      "epoch": 0.4926191480388022,
      "grad_norm": 1.536389946937561,
      "learning_rate": 0.00019809041482179936,
      "loss": 1.1396,
      "step": 3504
    },
    {
      "epoch": 0.492759735695206,
      "grad_norm": 1.9208904504776,
      "learning_rate": 0.00019804493640809912,
      "loss": 0.8549,
      "step": 3505
    },
    {
      "epoch": 0.4929003233516097,
      "grad_norm": 1.4396376609802246,
      "learning_rate": 0.00019799892813957844,
      "loss": 1.1724,
      "step": 3506
    },
    {
      "epoch": 0.4930409110080135,
      "grad_norm": 1.7213244438171387,
      "learning_rate": 0.00019795239026487526,
      "loss": 1.0804,
      "step": 3507
    },
    {
      "epoch": 0.49318149866441724,
      "grad_norm": 1.7996331453323364,
      "learning_rate": 0.00019790532303548986,
      "loss": 1.2397,
      "step": 3508
    },
    {
      "epoch": 0.49332208632082103,
      "grad_norm": 1.8622909784317017,
      "learning_rate": 0.00019785772670578307,
      "loss": 1.1637,
      "step": 3509
    },
    {
      "epoch": 0.4934626739772248,
      "grad_norm": 1.4775292873382568,
      "learning_rate": 0.00019780960153297517,
      "loss": 1.2897,
      "step": 3510
    },
    {
      "epoch": 0.49360326163362855,
      "grad_norm": 2.015002965927124,
      "learning_rate": 0.00019776094777714437,
      "loss": 1.1114,
      "step": 3511
    },
    {
      "epoch": 0.49374384929003234,
      "grad_norm": 1.4867494106292725,
      "learning_rate": 0.0001977117657012256,
      "loss": 1.0002,
      "step": 3512
    },
    {
      "epoch": 0.4938844369464361,
      "grad_norm": 1.809673547744751,
      "learning_rate": 0.0001976620555710087,
      "loss": 1.2189,
      "step": 3513
    },
    {
      "epoch": 0.49402502460283987,
      "grad_norm": 1.4428997039794922,
      "learning_rate": 0.00019761181765513738,
      "loss": 1.1148,
      "step": 3514
    },
    {
      "epoch": 0.49416561225924366,
      "grad_norm": 1.4905405044555664,
      "learning_rate": 0.00019756105222510758,
      "loss": 1.1767,
      "step": 3515
    },
    {
      "epoch": 0.4943061999156474,
      "grad_norm": 1.3486416339874268,
      "learning_rate": 0.00019750975955526608,
      "loss": 1.1153,
      "step": 3516
    },
    {
      "epoch": 0.4944467875720512,
      "grad_norm": 1.7866111993789673,
      "learning_rate": 0.00019745793992280883,
      "loss": 1.2051,
      "step": 3517
    },
    {
      "epoch": 0.4945873752284549,
      "grad_norm": 1.443369746208191,
      "learning_rate": 0.0001974055936077798,
      "loss": 1.036,
      "step": 3518
    },
    {
      "epoch": 0.4947279628848587,
      "grad_norm": 1.4128276109695435,
      "learning_rate": 0.000197352720893069,
      "loss": 1.2127,
      "step": 3519
    },
    {
      "epoch": 0.4948685505412625,
      "grad_norm": 1.3866122961044312,
      "learning_rate": 0.0001972993220644115,
      "loss": 1.1853,
      "step": 3520
    },
    {
      "epoch": 0.49500913819766623,
      "grad_norm": 1.383903980255127,
      "learning_rate": 0.0001972453974103854,
      "loss": 1.3033,
      "step": 3521
    },
    {
      "epoch": 0.49514972585407,
      "grad_norm": 1.4273645877838135,
      "learning_rate": 0.00019719094722241048,
      "loss": 1.0272,
      "step": 3522
    },
    {
      "epoch": 0.49529031351047376,
      "grad_norm": 1.469364881515503,
      "learning_rate": 0.0001971359717947467,
      "loss": 1.155,
      "step": 3523
    },
    {
      "epoch": 0.49543090116687755,
      "grad_norm": 1.6330296993255615,
      "learning_rate": 0.00019708047142449244,
      "loss": 1.2756,
      "step": 3524
    },
    {
      "epoch": 0.49557148882328134,
      "grad_norm": 1.8993473052978516,
      "learning_rate": 0.0001970244464115831,
      "loss": 1.1049,
      "step": 3525
    },
    {
      "epoch": 0.49571207647968507,
      "grad_norm": 1.6570795774459839,
      "learning_rate": 0.00019696789705878916,
      "loss": 1.2472,
      "step": 3526
    },
    {
      "epoch": 0.49585266413608886,
      "grad_norm": 1.444677472114563,
      "learning_rate": 0.0001969108236717149,
      "loss": 1.1325,
      "step": 3527
    },
    {
      "epoch": 0.4959932517924926,
      "grad_norm": 1.5004249811172485,
      "learning_rate": 0.00019685322655879658,
      "loss": 1.0813,
      "step": 3528
    },
    {
      "epoch": 0.4961338394488964,
      "grad_norm": 1.3776262998580933,
      "learning_rate": 0.00019679510603130064,
      "loss": 1.3089,
      "step": 3529
    },
    {
      "epoch": 0.4962744271053002,
      "grad_norm": 1.8351444005966187,
      "learning_rate": 0.00019673646240332235,
      "loss": 1.1045,
      "step": 3530
    },
    {
      "epoch": 0.4964150147617039,
      "grad_norm": 1.5332343578338623,
      "learning_rate": 0.00019667729599178374,
      "loss": 1.1828,
      "step": 3531
    },
    {
      "epoch": 0.4965556024181077,
      "grad_norm": 1.5084378719329834,
      "learning_rate": 0.00019661760711643225,
      "loss": 0.9543,
      "step": 3532
    },
    {
      "epoch": 0.49669619007451143,
      "grad_norm": 1.5066108703613281,
      "learning_rate": 0.00019655739609983867,
      "loss": 1.232,
      "step": 3533
    },
    {
      "epoch": 0.4968367777309152,
      "grad_norm": 1.3503634929656982,
      "learning_rate": 0.00019649666326739565,
      "loss": 1.113,
      "step": 3534
    },
    {
      "epoch": 0.496977365387319,
      "grad_norm": 1.854359745979309,
      "learning_rate": 0.00019643540894731572,
      "loss": 1.2639,
      "step": 3535
    },
    {
      "epoch": 0.49711795304372275,
      "grad_norm": 1.7596244812011719,
      "learning_rate": 0.00019637363347062975,
      "loss": 1.0485,
      "step": 3536
    },
    {
      "epoch": 0.49725854070012654,
      "grad_norm": 1.497044324874878,
      "learning_rate": 0.00019631133717118505,
      "loss": 1.1247,
      "step": 3537
    },
    {
      "epoch": 0.4973991283565303,
      "grad_norm": 1.6081379652023315,
      "learning_rate": 0.00019624852038564345,
      "loss": 1.152,
      "step": 3538
    },
    {
      "epoch": 0.49753971601293406,
      "grad_norm": 1.4462971687316895,
      "learning_rate": 0.00019618518345347974,
      "loss": 1.1622,
      "step": 3539
    },
    {
      "epoch": 0.49768030366933785,
      "grad_norm": 1.5896004438400269,
      "learning_rate": 0.00019612132671697954,
      "loss": 1.04,
      "step": 3540
    },
    {
      "epoch": 0.4978208913257416,
      "grad_norm": 1.5084689855575562,
      "learning_rate": 0.00019605695052123768,
      "loss": 1.1197,
      "step": 3541
    },
    {
      "epoch": 0.4979614789821454,
      "grad_norm": 1.7922512292861938,
      "learning_rate": 0.0001959920552141563,
      "loss": 1.2242,
      "step": 3542
    },
    {
      "epoch": 0.4981020666385491,
      "grad_norm": 1.847695231437683,
      "learning_rate": 0.0001959266411464428,
      "loss": 1.0199,
      "step": 3543
    },
    {
      "epoch": 0.4982426542949529,
      "grad_norm": 1.5519657135009766,
      "learning_rate": 0.0001958607086716082,
      "loss": 1.071,
      "step": 3544
    },
    {
      "epoch": 0.4983832419513567,
      "grad_norm": 2.008594512939453,
      "learning_rate": 0.00019579425814596497,
      "loss": 1.1112,
      "step": 3545
    },
    {
      "epoch": 0.4985238296077604,
      "grad_norm": 1.805629014968872,
      "learning_rate": 0.00019572728992862533,
      "loss": 1.1659,
      "step": 3546
    },
    {
      "epoch": 0.4986644172641642,
      "grad_norm": 1.6197986602783203,
      "learning_rate": 0.00019565980438149914,
      "loss": 1.1983,
      "step": 3547
    },
    {
      "epoch": 0.49880500492056795,
      "grad_norm": 1.4621025323867798,
      "learning_rate": 0.0001955918018692921,
      "loss": 1.1069,
      "step": 3548
    },
    {
      "epoch": 0.49894559257697174,
      "grad_norm": 1.6799734830856323,
      "learning_rate": 0.00019552328275950363,
      "loss": 1.0126,
      "step": 3549
    },
    {
      "epoch": 0.49908618023337553,
      "grad_norm": 1.523012638092041,
      "learning_rate": 0.00019545424742242499,
      "loss": 1.0866,
      "step": 3550
    },
    {
      "epoch": 0.49922676788977927,
      "grad_norm": 1.6338179111480713,
      "learning_rate": 0.0001953846962311371,
      "loss": 1.093,
      "step": 3551
    },
    {
      "epoch": 0.49936735554618306,
      "grad_norm": 1.3568872213363647,
      "learning_rate": 0.00019531462956150894,
      "loss": 1.0566,
      "step": 3552
    },
    {
      "epoch": 0.4995079432025868,
      "grad_norm": 1.4436982870101929,
      "learning_rate": 0.00019524404779219497,
      "loss": 1.2121,
      "step": 3553
    },
    {
      "epoch": 0.4996485308589906,
      "grad_norm": 1.3861697912216187,
      "learning_rate": 0.00019517295130463348,
      "loss": 1.2984,
      "step": 3554
    },
    {
      "epoch": 0.49978911851539437,
      "grad_norm": 1.5952372550964355,
      "learning_rate": 0.00019510134048304433,
      "loss": 0.9834,
      "step": 3555
    },
    {
      "epoch": 0.4999297061717981,
      "grad_norm": 1.620132327079773,
      "learning_rate": 0.0001950292157144271,
      "loss": 1.1332,
      "step": 3556
    },
    {
      "epoch": 0.5000702938282019,
      "grad_norm": 1.512007474899292,
      "learning_rate": 0.00019495657738855869,
      "loss": 1.0817,
      "step": 3557
    },
    {
      "epoch": 0.5002108814846057,
      "grad_norm": 1.6786788702011108,
      "learning_rate": 0.00019488342589799138,
      "loss": 1.2406,
      "step": 3558
    },
    {
      "epoch": 0.5003514691410094,
      "grad_norm": 1.6350194215774536,
      "learning_rate": 0.00019480976163805078,
      "loss": 0.9232,
      "step": 3559
    },
    {
      "epoch": 0.5004920567974132,
      "grad_norm": 1.6930019855499268,
      "learning_rate": 0.00019473558500683358,
      "loss": 1.2865,
      "step": 3560
    },
    {
      "epoch": 0.5006326444538169,
      "grad_norm": 1.4427614212036133,
      "learning_rate": 0.0001946608964052054,
      "loss": 1.2478,
      "step": 3561
    },
    {
      "epoch": 0.5007732321102207,
      "grad_norm": 1.7015951871871948,
      "learning_rate": 0.0001945856962367986,
      "loss": 1.1991,
      "step": 3562
    },
    {
      "epoch": 0.5009138197666245,
      "grad_norm": 2.0140883922576904,
      "learning_rate": 0.0001945099849080103,
      "loss": 0.9701,
      "step": 3563
    },
    {
      "epoch": 0.5010544074230282,
      "grad_norm": 1.9378361701965332,
      "learning_rate": 0.00019443376282799997,
      "loss": 1.1062,
      "step": 3564
    },
    {
      "epoch": 0.501194995079432,
      "grad_norm": 1.4531147480010986,
      "learning_rate": 0.00019435703040868722,
      "loss": 1.127,
      "step": 3565
    },
    {
      "epoch": 0.5013355827358358,
      "grad_norm": 1.6864938735961914,
      "learning_rate": 0.0001942797880647496,
      "loss": 1.2018,
      "step": 3566
    },
    {
      "epoch": 0.5014761703922396,
      "grad_norm": 1.5693116188049316,
      "learning_rate": 0.00019420203621362063,
      "loss": 1.2246,
      "step": 3567
    },
    {
      "epoch": 0.5016167580486434,
      "grad_norm": 2.065089464187622,
      "learning_rate": 0.0001941237752754871,
      "loss": 1.1075,
      "step": 3568
    },
    {
      "epoch": 0.501757345705047,
      "grad_norm": 1.5827796459197998,
      "learning_rate": 0.0001940450056732871,
      "loss": 1.3158,
      "step": 3569
    },
    {
      "epoch": 0.5018979333614508,
      "grad_norm": 1.71378493309021,
      "learning_rate": 0.00019396572783270753,
      "loss": 1.1185,
      "step": 3570
    },
    {
      "epoch": 0.5020385210178546,
      "grad_norm": 1.411409854888916,
      "learning_rate": 0.0001938859421821821,
      "loss": 1.0248,
      "step": 3571
    },
    {
      "epoch": 0.5021791086742584,
      "grad_norm": 1.702235460281372,
      "learning_rate": 0.00019380564915288866,
      "loss": 1.0443,
      "step": 3572
    },
    {
      "epoch": 0.5023196963306622,
      "grad_norm": 1.5470056533813477,
      "learning_rate": 0.00019372484917874717,
      "loss": 1.064,
      "step": 3573
    },
    {
      "epoch": 0.5024602839870659,
      "grad_norm": 1.5414520502090454,
      "learning_rate": 0.00019364354269641704,
      "loss": 1.2807,
      "step": 3574
    },
    {
      "epoch": 0.5026008716434697,
      "grad_norm": 1.5100198984146118,
      "learning_rate": 0.00019356173014529522,
      "loss": 1.308,
      "step": 3575
    },
    {
      "epoch": 0.5027414592998735,
      "grad_norm": 1.5485409498214722,
      "learning_rate": 0.0001934794119675133,
      "loss": 1.0455,
      "step": 3576
    },
    {
      "epoch": 0.5028820469562773,
      "grad_norm": 1.469976544380188,
      "learning_rate": 0.00019339658860793553,
      "loss": 1.1031,
      "step": 3577
    },
    {
      "epoch": 0.503022634612681,
      "grad_norm": 1.8500107526779175,
      "learning_rate": 0.00019331326051415628,
      "loss": 1.0567,
      "step": 3578
    },
    {
      "epoch": 0.5031632222690847,
      "grad_norm": 1.6106845140457153,
      "learning_rate": 0.0001932294281364975,
      "loss": 1.1954,
      "step": 3579
    },
    {
      "epoch": 0.5033038099254885,
      "grad_norm": 1.7810707092285156,
      "learning_rate": 0.00019314509192800645,
      "loss": 1.06,
      "step": 3580
    },
    {
      "epoch": 0.5034443975818923,
      "grad_norm": 1.5509262084960938,
      "learning_rate": 0.00019306025234445314,
      "loss": 1.1953,
      "step": 3581
    },
    {
      "epoch": 0.5035849852382961,
      "grad_norm": 1.5146313905715942,
      "learning_rate": 0.00019297490984432808,
      "loss": 1.1797,
      "step": 3582
    },
    {
      "epoch": 0.5037255728946999,
      "grad_norm": 1.679510235786438,
      "learning_rate": 0.0001928890648888395,
      "loss": 1.0985,
      "step": 3583
    },
    {
      "epoch": 0.5038661605511036,
      "grad_norm": 1.5291295051574707,
      "learning_rate": 0.000192802717941911,
      "loss": 1.3009,
      "step": 3584
    },
    {
      "epoch": 0.5040067482075073,
      "grad_norm": 1.523842215538025,
      "learning_rate": 0.00019271586947017903,
      "loss": 1.235,
      "step": 3585
    },
    {
      "epoch": 0.5041473358639111,
      "grad_norm": 1.4023487567901611,
      "learning_rate": 0.0001926285199429906,
      "loss": 1.1918,
      "step": 3586
    },
    {
      "epoch": 0.5042879235203149,
      "grad_norm": 1.9775969982147217,
      "learning_rate": 0.0001925406698324002,
      "loss": 1.0506,
      "step": 3587
    },
    {
      "epoch": 0.5044285111767187,
      "grad_norm": 1.2975975275039673,
      "learning_rate": 0.00019245231961316782,
      "loss": 1.1475,
      "step": 3588
    },
    {
      "epoch": 0.5045690988331224,
      "grad_norm": 1.9386390447616577,
      "learning_rate": 0.000192363469762756,
      "loss": 1.1351,
      "step": 3589
    },
    {
      "epoch": 0.5047096864895262,
      "grad_norm": 1.4826356172561646,
      "learning_rate": 0.00019227412076132752,
      "loss": 1.1056,
      "step": 3590
    },
    {
      "epoch": 0.50485027414593,
      "grad_norm": 1.5296432971954346,
      "learning_rate": 0.0001921842730917425,
      "loss": 1.1189,
      "step": 3591
    },
    {
      "epoch": 0.5049908618023338,
      "grad_norm": 1.8712908029556274,
      "learning_rate": 0.00019209392723955614,
      "loss": 1.0707,
      "step": 3592
    },
    {
      "epoch": 0.5051314494587376,
      "grad_norm": 1.9690479040145874,
      "learning_rate": 0.00019200308369301573,
      "loss": 1.0886,
      "step": 3593
    },
    {
      "epoch": 0.5052720371151412,
      "grad_norm": 1.47812020778656,
      "learning_rate": 0.00019191174294305846,
      "loss": 1.1917,
      "step": 3594
    },
    {
      "epoch": 0.505412624771545,
      "grad_norm": 1.6938480138778687,
      "learning_rate": 0.00019181990548330826,
      "loss": 1.0703,
      "step": 3595
    },
    {
      "epoch": 0.5055532124279488,
      "grad_norm": 1.5146780014038086,
      "learning_rate": 0.00019172757181007354,
      "loss": 1.2952,
      "step": 3596
    },
    {
      "epoch": 0.5056938000843526,
      "grad_norm": 1.4516061544418335,
      "learning_rate": 0.00019163474242234419,
      "loss": 1.1758,
      "step": 3597
    },
    {
      "epoch": 0.5058343877407564,
      "grad_norm": 1.773118019104004,
      "learning_rate": 0.00019154141782178928,
      "loss": 1.0024,
      "step": 3598
    },
    {
      "epoch": 0.5059749753971601,
      "grad_norm": 1.5232595205307007,
      "learning_rate": 0.00019144759851275387,
      "loss": 1.2407,
      "step": 3599
    },
    {
      "epoch": 0.5061155630535639,
      "grad_norm": 1.5919992923736572,
      "learning_rate": 0.00019135328500225667,
      "loss": 1.1946,
      "step": 3600
    },
    {
      "epoch": 0.5062561507099677,
      "grad_norm": 1.7809345722198486,
      "learning_rate": 0.00019125847779998708,
      "loss": 1.0593,
      "step": 3601
    },
    {
      "epoch": 0.5063967383663714,
      "grad_norm": 1.6356592178344727,
      "learning_rate": 0.0001911631774183026,
      "loss": 1.1708,
      "step": 3602
    },
    {
      "epoch": 0.5065373260227752,
      "grad_norm": 1.5724273920059204,
      "learning_rate": 0.0001910673843722259,
      "loss": 1.157,
      "step": 3603
    },
    {
      "epoch": 0.5066779136791789,
      "grad_norm": 1.6084489822387695,
      "learning_rate": 0.0001909710991794421,
      "loss": 1.1888,
      "step": 3604
    },
    {
      "epoch": 0.5068185013355827,
      "grad_norm": 1.4469398260116577,
      "learning_rate": 0.000190874322360296,
      "loss": 0.9837,
      "step": 3605
    },
    {
      "epoch": 0.5069590889919865,
      "grad_norm": 1.3576964139938354,
      "learning_rate": 0.0001907770544377893,
      "loss": 1.1889,
      "step": 3606
    },
    {
      "epoch": 0.5070996766483903,
      "grad_norm": 1.7188951969146729,
      "learning_rate": 0.0001906792959375777,
      "loss": 0.9657,
      "step": 3607
    },
    {
      "epoch": 0.5072402643047941,
      "grad_norm": 1.7029569149017334,
      "learning_rate": 0.00019058104738796798,
      "loss": 1.1444,
      "step": 3608
    },
    {
      "epoch": 0.5073808519611978,
      "grad_norm": 2.33640456199646,
      "learning_rate": 0.00019048230931991534,
      "loss": 1.2293,
      "step": 3609
    },
    {
      "epoch": 0.5075214396176015,
      "grad_norm": 1.683066725730896,
      "learning_rate": 0.00019038308226702048,
      "loss": 1.1839,
      "step": 3610
    },
    {
      "epoch": 0.5076620272740053,
      "grad_norm": 1.7087126970291138,
      "learning_rate": 0.00019028336676552663,
      "loss": 1.2019,
      "step": 3611
    },
    {
      "epoch": 0.5078026149304091,
      "grad_norm": 1.789767861366272,
      "learning_rate": 0.0001901831633543166,
      "loss": 1.0006,
      "step": 3612
    },
    {
      "epoch": 0.5079432025868129,
      "grad_norm": 1.5411628484725952,
      "learning_rate": 0.00019008247257491009,
      "loss": 1.0269,
      "step": 3613
    },
    {
      "epoch": 0.5080837902432166,
      "grad_norm": 1.6936371326446533,
      "learning_rate": 0.0001899812949714606,
      "loss": 1.0978,
      "step": 3614
    },
    {
      "epoch": 0.5082243778996204,
      "grad_norm": 1.6971980333328247,
      "learning_rate": 0.00018987963109075255,
      "loss": 1.1974,
      "step": 3615
    },
    {
      "epoch": 0.5083649655560242,
      "grad_norm": 1.5394200086593628,
      "learning_rate": 0.0001897774814821982,
      "loss": 1.3009,
      "step": 3616
    },
    {
      "epoch": 0.508505553212428,
      "grad_norm": 1.7504249811172485,
      "learning_rate": 0.00018967484669783495,
      "loss": 1.0563,
      "step": 3617
    },
    {
      "epoch": 0.5086461408688318,
      "grad_norm": 1.6234488487243652,
      "learning_rate": 0.000189571727292322,
      "loss": 1.1178,
      "step": 3618
    },
    {
      "epoch": 0.5087867285252354,
      "grad_norm": 1.6255872249603271,
      "learning_rate": 0.0001894681238229377,
      "loss": 1.01,
      "step": 3619
    },
    {
      "epoch": 0.5089273161816392,
      "grad_norm": 1.4783170223236084,
      "learning_rate": 0.00018936403684957629,
      "loss": 1.1807,
      "step": 3620
    },
    {
      "epoch": 0.509067903838043,
      "grad_norm": 1.5387217998504639,
      "learning_rate": 0.000189259466934745,
      "loss": 1.0924,
      "step": 3621
    },
    {
      "epoch": 0.5092084914944468,
      "grad_norm": 1.6021257638931274,
      "learning_rate": 0.000189154414643561,
      "loss": 1.2175,
      "step": 3622
    },
    {
      "epoch": 0.5093490791508506,
      "grad_norm": 1.6728668212890625,
      "learning_rate": 0.00018904888054374817,
      "loss": 1.0964,
      "step": 3623
    },
    {
      "epoch": 0.5094896668072543,
      "grad_norm": 1.4190289974212646,
      "learning_rate": 0.0001889428652056344,
      "loss": 1.2169,
      "step": 3624
    },
    {
      "epoch": 0.5096302544636581,
      "grad_norm": 1.6314561367034912,
      "learning_rate": 0.00018883636920214816,
      "loss": 1.1413,
      "step": 3625
    },
    {
      "epoch": 0.5097708421200619,
      "grad_norm": 1.6979494094848633,
      "learning_rate": 0.00018872939310881558,
      "loss": 1.0401,
      "step": 3626
    },
    {
      "epoch": 0.5099114297764656,
      "grad_norm": 1.96439790725708,
      "learning_rate": 0.0001886219375037572,
      "loss": 1.2187,
      "step": 3627
    },
    {
      "epoch": 0.5100520174328694,
      "grad_norm": 1.4975608587265015,
      "learning_rate": 0.00018851400296768508,
      "loss": 1.2921,
      "step": 3628
    },
    {
      "epoch": 0.5101926050892731,
      "grad_norm": 1.5817891359329224,
      "learning_rate": 0.0001884055900838994,
      "loss": 1.1487,
      "step": 3629
    },
    {
      "epoch": 0.5103331927456769,
      "grad_norm": 1.6842440366744995,
      "learning_rate": 0.0001882966994382856,
      "loss": 1.3805,
      "step": 3630
    },
    {
      "epoch": 0.5104737804020807,
      "grad_norm": 1.6529258489608765,
      "learning_rate": 0.0001881873316193108,
      "loss": 1.0708,
      "step": 3631
    },
    {
      "epoch": 0.5106143680584845,
      "grad_norm": 1.581998348236084,
      "learning_rate": 0.00018807748721802096,
      "loss": 1.168,
      "step": 3632
    },
    {
      "epoch": 0.5107549557148883,
      "grad_norm": 1.7644962072372437,
      "learning_rate": 0.00018796716682803775,
      "loss": 1.0663,
      "step": 3633
    },
    {
      "epoch": 0.510895543371292,
      "grad_norm": 1.568817138671875,
      "learning_rate": 0.00018785637104555496,
      "loss": 1.1909,
      "step": 3634
    },
    {
      "epoch": 0.5110361310276957,
      "grad_norm": 1.4183634519577026,
      "learning_rate": 0.00018774510046933558,
      "loss": 1.2442,
      "step": 3635
    },
    {
      "epoch": 0.5111767186840995,
      "grad_norm": 1.6716878414154053,
      "learning_rate": 0.00018763335570070848,
      "loss": 1.073,
      "step": 3636
    },
    {
      "epoch": 0.5113173063405033,
      "grad_norm": 1.737038254737854,
      "learning_rate": 0.0001875211373435652,
      "loss": 1.3673,
      "step": 3637
    },
    {
      "epoch": 0.5114578939969071,
      "grad_norm": 1.5697096586227417,
      "learning_rate": 0.00018740844600435657,
      "loss": 1.1554,
      "step": 3638
    },
    {
      "epoch": 0.5115984816533108,
      "grad_norm": 1.3224068880081177,
      "learning_rate": 0.00018729528229208964,
      "loss": 1.2692,
      "step": 3639
    },
    {
      "epoch": 0.5117390693097146,
      "grad_norm": 1.9877614974975586,
      "learning_rate": 0.00018718164681832405,
      "loss": 1.1216,
      "step": 3640
    },
    {
      "epoch": 0.5118796569661184,
      "grad_norm": 1.9939446449279785,
      "learning_rate": 0.00018706754019716919,
      "loss": 1.1583,
      "step": 3641
    },
    {
      "epoch": 0.5120202446225222,
      "grad_norm": 2.0314769744873047,
      "learning_rate": 0.00018695296304528043,
      "loss": 1.0589,
      "step": 3642
    },
    {
      "epoch": 0.512160832278926,
      "grad_norm": 1.5715000629425049,
      "learning_rate": 0.00018683791598185607,
      "loss": 1.1363,
      "step": 3643
    },
    {
      "epoch": 0.5123014199353296,
      "grad_norm": 1.4694231748580933,
      "learning_rate": 0.00018672239962863382,
      "loss": 1.2592,
      "step": 3644
    },
    {
      "epoch": 0.5124420075917334,
      "grad_norm": 1.603505253791809,
      "learning_rate": 0.00018660641460988776,
      "loss": 1.1111,
      "step": 3645
    },
    {
      "epoch": 0.5125825952481372,
      "grad_norm": 1.4830131530761719,
      "learning_rate": 0.00018648996155242443,
      "loss": 1.1194,
      "step": 3646
    },
    {
      "epoch": 0.512723182904541,
      "grad_norm": 1.3449971675872803,
      "learning_rate": 0.00018637304108557997,
      "loss": 1.0715,
      "step": 3647
    },
    {
      "epoch": 0.5128637705609448,
      "grad_norm": 2.0291547775268555,
      "learning_rate": 0.00018625565384121625,
      "loss": 1.0919,
      "step": 3648
    },
    {
      "epoch": 0.5130043582173485,
      "grad_norm": 2.139681100845337,
      "learning_rate": 0.00018613780045371813,
      "loss": 1.0865,
      "step": 3649
    },
    {
      "epoch": 0.5131449458737523,
      "grad_norm": 1.7162889242172241,
      "learning_rate": 0.00018601948155998917,
      "loss": 1.1391,
      "step": 3650
    },
    {
      "epoch": 0.513285533530156,
      "grad_norm": 1.5887316465377808,
      "learning_rate": 0.00018590069779944883,
      "loss": 1.0121,
      "step": 3651
    },
    {
      "epoch": 0.5134261211865598,
      "grad_norm": 1.5829851627349854,
      "learning_rate": 0.00018578144981402868,
      "loss": 1.1865,
      "step": 3652
    },
    {
      "epoch": 0.5135667088429636,
      "grad_norm": 1.59730064868927,
      "learning_rate": 0.00018566173824816927,
      "loss": 1.1183,
      "step": 3653
    },
    {
      "epoch": 0.5137072964993673,
      "grad_norm": 1.5172289609909058,
      "learning_rate": 0.00018554156374881622,
      "loss": 1.1443,
      "step": 3654
    },
    {
      "epoch": 0.5138478841557711,
      "grad_norm": 1.6195428371429443,
      "learning_rate": 0.00018542092696541707,
      "loss": 1.0919,
      "step": 3655
    },
    {
      "epoch": 0.5139884718121749,
      "grad_norm": 1.8147934675216675,
      "learning_rate": 0.00018529982854991752,
      "loss": 1.0918,
      "step": 3656
    },
    {
      "epoch": 0.5141290594685787,
      "grad_norm": 1.7218654155731201,
      "learning_rate": 0.0001851782691567582,
      "loss": 1.0603,
      "step": 3657
    },
    {
      "epoch": 0.5142696471249825,
      "grad_norm": 1.5269746780395508,
      "learning_rate": 0.0001850562494428707,
      "loss": 1.2032,
      "step": 3658
    },
    {
      "epoch": 0.5144102347813861,
      "grad_norm": 1.6778831481933594,
      "learning_rate": 0.0001849337700676745,
      "loss": 1.2608,
      "step": 3659
    },
    {
      "epoch": 0.5145508224377899,
      "grad_norm": 1.8773914575576782,
      "learning_rate": 0.00018481083169307315,
      "loss": 0.9957,
      "step": 3660
    },
    {
      "epoch": 0.5146914100941937,
      "grad_norm": 1.8511583805084229,
      "learning_rate": 0.00018468743498345066,
      "loss": 1.0964,
      "step": 3661
    },
    {
      "epoch": 0.5148319977505975,
      "grad_norm": 1.5718145370483398,
      "learning_rate": 0.00018456358060566798,
      "loss": 1.1992,
      "step": 3662
    },
    {
      "epoch": 0.5149725854070013,
      "grad_norm": 1.4890737533569336,
      "learning_rate": 0.00018443926922905935,
      "loss": 1.0996,
      "step": 3663
    },
    {
      "epoch": 0.515113173063405,
      "grad_norm": 1.593003273010254,
      "learning_rate": 0.00018431450152542892,
      "loss": 1.0941,
      "step": 3664
    },
    {
      "epoch": 0.5152537607198088,
      "grad_norm": 1.6489965915679932,
      "learning_rate": 0.0001841892781690467,
      "loss": 1.1548,
      "step": 3665
    },
    {
      "epoch": 0.5153943483762126,
      "grad_norm": 1.9510236978530884,
      "learning_rate": 0.00018406359983664532,
      "loss": 1.0404,
      "step": 3666
    },
    {
      "epoch": 0.5155349360326164,
      "grad_norm": 1.3901524543762207,
      "learning_rate": 0.00018393746720741594,
      "loss": 1.1374,
      "step": 3667
    },
    {
      "epoch": 0.5156755236890201,
      "grad_norm": 1.4264352321624756,
      "learning_rate": 0.00018381088096300517,
      "loss": 1.2286,
      "step": 3668
    },
    {
      "epoch": 0.5158161113454238,
      "grad_norm": 1.6209874153137207,
      "learning_rate": 0.00018368384178751076,
      "loss": 0.956,
      "step": 3669
    },
    {
      "epoch": 0.5159566990018276,
      "grad_norm": 1.5290089845657349,
      "learning_rate": 0.0001835563503674784,
      "loss": 1.1457,
      "step": 3670
    },
    {
      "epoch": 0.5160972866582314,
      "grad_norm": 1.6175745725631714,
      "learning_rate": 0.0001834284073918976,
      "loss": 1.0331,
      "step": 3671
    },
    {
      "epoch": 0.5162378743146352,
      "grad_norm": 1.7426071166992188,
      "learning_rate": 0.00018330001355219844,
      "loss": 1.148,
      "step": 3672
    },
    {
      "epoch": 0.516378461971039,
      "grad_norm": 1.5506643056869507,
      "learning_rate": 0.00018317116954224725,
      "loss": 1.1275,
      "step": 3673
    },
    {
      "epoch": 0.5165190496274427,
      "grad_norm": 1.831276774406433,
      "learning_rate": 0.0001830418760583434,
      "loss": 1.1967,
      "step": 3674
    },
    {
      "epoch": 0.5166596372838465,
      "grad_norm": 1.8346151113510132,
      "learning_rate": 0.00018291213379921512,
      "loss": 1.2469,
      "step": 3675
    },
    {
      "epoch": 0.5168002249402502,
      "grad_norm": 1.7666515111923218,
      "learning_rate": 0.0001827819434660162,
      "loss": 1.0932,
      "step": 3676
    },
    {
      "epoch": 0.516940812596654,
      "grad_norm": 1.5704768896102905,
      "learning_rate": 0.00018265130576232159,
      "loss": 1.1803,
      "step": 3677
    },
    {
      "epoch": 0.5170814002530578,
      "grad_norm": 1.6016086339950562,
      "learning_rate": 0.00018252022139412412,
      "loss": 1.2399,
      "step": 3678
    },
    {
      "epoch": 0.5172219879094615,
      "grad_norm": 1.5247554779052734,
      "learning_rate": 0.00018238869106983047,
      "loss": 1.3207,
      "step": 3679
    },
    {
      "epoch": 0.5173625755658653,
      "grad_norm": 1.4733017683029175,
      "learning_rate": 0.00018225671550025724,
      "loss": 1.1654,
      "step": 3680
    },
    {
      "epoch": 0.5175031632222691,
      "grad_norm": 1.467795968055725,
      "learning_rate": 0.00018212429539862744,
      "loss": 1.1118,
      "step": 3681
    },
    {
      "epoch": 0.5176437508786729,
      "grad_norm": 1.6559836864471436,
      "learning_rate": 0.00018199143148056617,
      "loss": 1.0065,
      "step": 3682
    },
    {
      "epoch": 0.5177843385350767,
      "grad_norm": 1.6089259386062622,
      "learning_rate": 0.00018185812446409714,
      "loss": 1.116,
      "step": 3683
    },
    {
      "epoch": 0.5179249261914803,
      "grad_norm": 1.6406182050704956,
      "learning_rate": 0.00018172437506963867,
      "loss": 1.2553,
      "step": 3684
    },
    {
      "epoch": 0.5180655138478841,
      "grad_norm": 1.5384609699249268,
      "learning_rate": 0.00018159018401999976,
      "loss": 1.1363,
      "step": 3685
    },
    {
      "epoch": 0.5182061015042879,
      "grad_norm": 1.3827362060546875,
      "learning_rate": 0.00018145555204037613,
      "loss": 1.2461,
      "step": 3686
    },
    {
      "epoch": 0.5183466891606917,
      "grad_norm": 1.5396732091903687,
      "learning_rate": 0.0001813204798583465,
      "loss": 0.9303,
      "step": 3687
    },
    {
      "epoch": 0.5184872768170955,
      "grad_norm": 1.610592007637024,
      "learning_rate": 0.00018118496820386848,
      "loss": 1.0675,
      "step": 3688
    },
    {
      "epoch": 0.5186278644734992,
      "grad_norm": 1.447862982749939,
      "learning_rate": 0.00018104901780927467,
      "loss": 1.1218,
      "step": 3689
    },
    {
      "epoch": 0.518768452129903,
      "grad_norm": 1.5255844593048096,
      "learning_rate": 0.00018091262940926863,
      "loss": 1.1819,
      "step": 3690
    },
    {
      "epoch": 0.5189090397863068,
      "grad_norm": 1.5965325832366943,
      "learning_rate": 0.0001807758037409211,
      "loss": 1.1483,
      "step": 3691
    },
    {
      "epoch": 0.5190496274427105,
      "grad_norm": 1.5772151947021484,
      "learning_rate": 0.000180638541543666,
      "loss": 1.1193,
      "step": 3692
    },
    {
      "epoch": 0.5191902150991143,
      "grad_norm": 1.511913537979126,
      "learning_rate": 0.00018050084355929614,
      "loss": 1.1919,
      "step": 3693
    },
    {
      "epoch": 0.519330802755518,
      "grad_norm": 1.2857669591903687,
      "learning_rate": 0.00018036271053195938,
      "loss": 1.2676,
      "step": 3694
    },
    {
      "epoch": 0.5194713904119218,
      "grad_norm": 1.6920479536056519,
      "learning_rate": 0.00018022414320815496,
      "loss": 0.9719,
      "step": 3695
    },
    {
      "epoch": 0.5196119780683256,
      "grad_norm": 1.5320823192596436,
      "learning_rate": 0.0001800851423367288,
      "loss": 1.146,
      "step": 3696
    },
    {
      "epoch": 0.5197525657247294,
      "grad_norm": 1.56381356716156,
      "learning_rate": 0.00017994570866886996,
      "loss": 1.0892,
      "step": 3697
    },
    {
      "epoch": 0.5198931533811332,
      "grad_norm": 1.542754054069519,
      "learning_rate": 0.00017980584295810648,
      "loss": 1.1128,
      "step": 3698
    },
    {
      "epoch": 0.5200337410375369,
      "grad_norm": 1.510730266571045,
      "learning_rate": 0.0001796655459603011,
      "loss": 1.1293,
      "step": 3699
    },
    {
      "epoch": 0.5201743286939406,
      "grad_norm": 1.4919191598892212,
      "learning_rate": 0.00017952481843364746,
      "loss": 1.1843,
      "step": 3700
    },
    {
      "epoch": 0.5203149163503444,
      "grad_norm": 1.5339435338974,
      "learning_rate": 0.0001793836611386657,
      "loss": 1.1307,
      "step": 3701
    },
    {
      "epoch": 0.5204555040067482,
      "grad_norm": 1.5453394651412964,
      "learning_rate": 0.00017924207483819866,
      "loss": 1.0485,
      "step": 3702
    },
    {
      "epoch": 0.520596091663152,
      "grad_norm": 1.5457124710083008,
      "learning_rate": 0.00017910006029740755,
      "loss": 1.2811,
      "step": 3703
    },
    {
      "epoch": 0.5207366793195557,
      "grad_norm": 1.6885244846343994,
      "learning_rate": 0.00017895761828376798,
      "loss": 1.0263,
      "step": 3704
    },
    {
      "epoch": 0.5208772669759595,
      "grad_norm": 2.411804676055908,
      "learning_rate": 0.0001788147495670655,
      "loss": 1.1201,
      "step": 3705
    },
    {
      "epoch": 0.5210178546323633,
      "grad_norm": 1.7698750495910645,
      "learning_rate": 0.00017867145491939183,
      "loss": 1.0149,
      "step": 3706
    },
    {
      "epoch": 0.5211584422887671,
      "grad_norm": 1.6677958965301514,
      "learning_rate": 0.00017852773511514047,
      "loss": 1.035,
      "step": 3707
    },
    {
      "epoch": 0.5212990299451709,
      "grad_norm": 1.5712441205978394,
      "learning_rate": 0.00017838359093100257,
      "loss": 1.0839,
      "step": 3708
    },
    {
      "epoch": 0.5214396176015745,
      "grad_norm": 1.6297316551208496,
      "learning_rate": 0.00017823902314596256,
      "loss": 0.8921,
      "step": 3709
    },
    {
      "epoch": 0.5215802052579783,
      "grad_norm": 1.6683827638626099,
      "learning_rate": 0.00017809403254129432,
      "loss": 1.017,
      "step": 3710
    },
    {
      "epoch": 0.5217207929143821,
      "grad_norm": 1.6705636978149414,
      "learning_rate": 0.00017794861990055657,
      "loss": 1.065,
      "step": 3711
    },
    {
      "epoch": 0.5218613805707859,
      "grad_norm": 1.6101211309432983,
      "learning_rate": 0.000177802786009589,
      "loss": 1.0109,
      "step": 3712
    },
    {
      "epoch": 0.5220019682271897,
      "grad_norm": 1.480465054512024,
      "learning_rate": 0.0001776565316565075,
      "loss": 1.1689,
      "step": 3713
    },
    {
      "epoch": 0.5221425558835934,
      "grad_norm": 1.8111977577209473,
      "learning_rate": 0.00017750985763170042,
      "loss": 1.1529,
      "step": 3714
    },
    {
      "epoch": 0.5222831435399972,
      "grad_norm": 1.8700212240219116,
      "learning_rate": 0.0001773627647278242,
      "loss": 1.0397,
      "step": 3715
    },
    {
      "epoch": 0.522423731196401,
      "grad_norm": 1.4079864025115967,
      "learning_rate": 0.0001772152537397988,
      "loss": 1.1157,
      "step": 3716
    },
    {
      "epoch": 0.5225643188528047,
      "grad_norm": 1.6956493854522705,
      "learning_rate": 0.00017706732546480373,
      "loss": 1.2418,
      "step": 3717
    },
    {
      "epoch": 0.5227049065092085,
      "grad_norm": 1.600470781326294,
      "learning_rate": 0.0001769189807022734,
      "loss": 1.2451,
      "step": 3718
    },
    {
      "epoch": 0.5228454941656122,
      "grad_norm": 1.7283469438552856,
      "learning_rate": 0.00017677022025389336,
      "loss": 1.0379,
      "step": 3719
    },
    {
      "epoch": 0.522986081822016,
      "grad_norm": 1.5760300159454346,
      "learning_rate": 0.0001766210449235952,
      "loss": 1.0231,
      "step": 3720
    },
    {
      "epoch": 0.5231266694784198,
      "grad_norm": 1.3661226034164429,
      "learning_rate": 0.00017647145551755294,
      "loss": 1.1109,
      "step": 3721
    },
    {
      "epoch": 0.5232672571348236,
      "grad_norm": 2.0329251289367676,
      "learning_rate": 0.00017632145284417802,
      "loss": 1.1614,
      "step": 3722
    },
    {
      "epoch": 0.5234078447912274,
      "grad_norm": 1.792861819267273,
      "learning_rate": 0.00017617103771411574,
      "loss": 1.1219,
      "step": 3723
    },
    {
      "epoch": 0.523548432447631,
      "grad_norm": 1.59397292137146,
      "learning_rate": 0.00017602021094023992,
      "loss": 1.0663,
      "step": 3724
    },
    {
      "epoch": 0.5236890201040348,
      "grad_norm": 1.5984725952148438,
      "learning_rate": 0.00017586897333764933,
      "loss": 0.9552,
      "step": 3725
    },
    {
      "epoch": 0.5238296077604386,
      "grad_norm": 1.7898902893066406,
      "learning_rate": 0.0001757173257236626,
      "loss": 1.0919,
      "step": 3726
    },
    {
      "epoch": 0.5239701954168424,
      "grad_norm": 1.7190055847167969,
      "learning_rate": 0.00017556526891781456,
      "loss": 1.2271,
      "step": 3727
    },
    {
      "epoch": 0.5241107830732462,
      "grad_norm": 1.5536561012268066,
      "learning_rate": 0.00017541280374185108,
      "loss": 1.0933,
      "step": 3728
    },
    {
      "epoch": 0.5242513707296499,
      "grad_norm": 1.3654345273971558,
      "learning_rate": 0.00017525993101972503,
      "loss": 1.1123,
      "step": 3729
    },
    {
      "epoch": 0.5243919583860537,
      "grad_norm": 1.278737187385559,
      "learning_rate": 0.00017510665157759178,
      "loss": 1.19,
      "step": 3730
    },
    {
      "epoch": 0.5245325460424575,
      "grad_norm": 1.7194167375564575,
      "learning_rate": 0.00017495296624380478,
      "loss": 1.1024,
      "step": 3731
    },
    {
      "epoch": 0.5246731336988613,
      "grad_norm": 1.5114794969558716,
      "learning_rate": 0.00017479887584891072,
      "loss": 1.2039,
      "step": 3732
    },
    {
      "epoch": 0.524813721355265,
      "grad_norm": 1.6135969161987305,
      "learning_rate": 0.0001746443812256456,
      "loss": 1.156,
      "step": 3733
    },
    {
      "epoch": 0.5249543090116687,
      "grad_norm": 1.4902197122573853,
      "learning_rate": 0.00017448948320892985,
      "loss": 0.9093,
      "step": 3734
    },
    {
      "epoch": 0.5250948966680725,
      "grad_norm": 1.6908851861953735,
      "learning_rate": 0.00017433418263586395,
      "loss": 1.2655,
      "step": 3735
    },
    {
      "epoch": 0.5252354843244763,
      "grad_norm": 1.8719606399536133,
      "learning_rate": 0.00017417848034572377,
      "loss": 0.9476,
      "step": 3736
    },
    {
      "epoch": 0.5253760719808801,
      "grad_norm": 1.3934838771820068,
      "learning_rate": 0.0001740223771799562,
      "loss": 1.1807,
      "step": 3737
    },
    {
      "epoch": 0.5255166596372839,
      "grad_norm": 1.8158665895462036,
      "learning_rate": 0.00017386587398217476,
      "loss": 1.0777,
      "step": 3738
    },
    {
      "epoch": 0.5256572472936876,
      "grad_norm": 1.6443907022476196,
      "learning_rate": 0.00017370897159815445,
      "loss": 1.1393,
      "step": 3739
    },
    {
      "epoch": 0.5257978349500914,
      "grad_norm": 1.4934128522872925,
      "learning_rate": 0.00017355167087582785,
      "loss": 1.1473,
      "step": 3740
    },
    {
      "epoch": 0.5259384226064951,
      "grad_norm": 1.484321117401123,
      "learning_rate": 0.00017339397266527993,
      "loss": 1.2714,
      "step": 3741
    },
    {
      "epoch": 0.5260790102628989,
      "grad_norm": 1.7539621591567993,
      "learning_rate": 0.00017323587781874425,
      "loss": 1.0408,
      "step": 3742
    },
    {
      "epoch": 0.5262195979193027,
      "grad_norm": 1.7769300937652588,
      "learning_rate": 0.00017307738719059734,
      "loss": 1.0113,
      "step": 3743
    },
    {
      "epoch": 0.5263601855757064,
      "grad_norm": 1.5248358249664307,
      "learning_rate": 0.00017291850163735504,
      "loss": 1.0663,
      "step": 3744
    },
    {
      "epoch": 0.5265007732321102,
      "grad_norm": 1.5893051624298096,
      "learning_rate": 0.00017275922201766703,
      "loss": 1.1187,
      "step": 3745
    },
    {
      "epoch": 0.526641360888514,
      "grad_norm": 1.5556881427764893,
      "learning_rate": 0.00017259954919231313,
      "loss": 1.1429,
      "step": 3746
    },
    {
      "epoch": 0.5267819485449178,
      "grad_norm": 1.7362316846847534,
      "learning_rate": 0.0001724394840241976,
      "loss": 0.8313,
      "step": 3747
    },
    {
      "epoch": 0.5269225362013216,
      "grad_norm": 1.4000568389892578,
      "learning_rate": 0.0001722790273783454,
      "loss": 1.2054,
      "step": 3748
    },
    {
      "epoch": 0.5270631238577252,
      "grad_norm": 1.6014084815979004,
      "learning_rate": 0.00017211818012189678,
      "loss": 1.1406,
      "step": 3749
    },
    {
      "epoch": 0.527203711514129,
      "grad_norm": 1.7928180694580078,
      "learning_rate": 0.00017195694312410327,
      "loss": 0.9521,
      "step": 3750
    },
    {
      "epoch": 0.5273442991705328,
      "grad_norm": 1.7550559043884277,
      "learning_rate": 0.00017179531725632237,
      "loss": 1.197,
      "step": 3751
    },
    {
      "epoch": 0.5274848868269366,
      "grad_norm": 1.5872974395751953,
      "learning_rate": 0.00017163330339201326,
      "loss": 0.9784,
      "step": 3752
    },
    {
      "epoch": 0.5276254744833404,
      "grad_norm": 1.5067297220230103,
      "learning_rate": 0.00017147090240673176,
      "loss": 1.2294,
      "step": 3753
    },
    {
      "epoch": 0.5277660621397441,
      "grad_norm": 1.994153618812561,
      "learning_rate": 0.00017130811517812613,
      "loss": 1.0431,
      "step": 3754
    },
    {
      "epoch": 0.5279066497961479,
      "grad_norm": 1.6700845956802368,
      "learning_rate": 0.00017114494258593153,
      "loss": 1.0173,
      "step": 3755
    },
    {
      "epoch": 0.5280472374525517,
      "grad_norm": 1.4686076641082764,
      "learning_rate": 0.00017098138551196597,
      "loss": 1.2655,
      "step": 3756
    },
    {
      "epoch": 0.5281878251089555,
      "grad_norm": 1.843608021736145,
      "learning_rate": 0.00017081744484012524,
      "loss": 1.1475,
      "step": 3757
    },
    {
      "epoch": 0.5283284127653592,
      "grad_norm": 1.4970977306365967,
      "learning_rate": 0.0001706531214563781,
      "loss": 1.2761,
      "step": 3758
    },
    {
      "epoch": 0.5284690004217629,
      "grad_norm": 1.4942389726638794,
      "learning_rate": 0.00017048841624876175,
      "loss": 1.0549,
      "step": 3759
    },
    {
      "epoch": 0.5286095880781667,
      "grad_norm": 1.9837214946746826,
      "learning_rate": 0.0001703233301073765,
      "loss": 1.1074,
      "step": 3760
    },
    {
      "epoch": 0.5287501757345705,
      "grad_norm": 1.4138529300689697,
      "learning_rate": 0.00017015786392438157,
      "loss": 1.0698,
      "step": 3761
    },
    {
      "epoch": 0.5288907633909743,
      "grad_norm": 1.7966243028640747,
      "learning_rate": 0.00016999201859399,
      "loss": 1.0821,
      "step": 3762
    },
    {
      "epoch": 0.5290313510473781,
      "grad_norm": 1.5200825929641724,
      "learning_rate": 0.00016982579501246381,
      "loss": 1.2001,
      "step": 3763
    },
    {
      "epoch": 0.5291719387037818,
      "grad_norm": 1.7835664749145508,
      "learning_rate": 0.00016965919407810893,
      "loss": 1.2699,
      "step": 3764
    },
    {
      "epoch": 0.5293125263601856,
      "grad_norm": 1.5412695407867432,
      "learning_rate": 0.0001694922166912709,
      "loss": 1.2011,
      "step": 3765
    },
    {
      "epoch": 0.5294531140165893,
      "grad_norm": 1.5364247560501099,
      "learning_rate": 0.0001693248637543295,
      "loss": 1.1738,
      "step": 3766
    },
    {
      "epoch": 0.5295937016729931,
      "grad_norm": 1.2982698678970337,
      "learning_rate": 0.0001691571361716942,
      "loss": 1.0911,
      "step": 3767
    },
    {
      "epoch": 0.5297342893293969,
      "grad_norm": 1.4120033979415894,
      "learning_rate": 0.00016898903484979886,
      "loss": 1.276,
      "step": 3768
    },
    {
      "epoch": 0.5298748769858006,
      "grad_norm": 1.358357310295105,
      "learning_rate": 0.00016882056069709753,
      "loss": 1.0731,
      "step": 3769
    },
    {
      "epoch": 0.5300154646422044,
      "grad_norm": 1.4677215814590454,
      "learning_rate": 0.00016865171462405868,
      "loss": 1.1043,
      "step": 3770
    },
    {
      "epoch": 0.5301560522986082,
      "grad_norm": 1.3956471681594849,
      "learning_rate": 0.00016848249754316107,
      "loss": 1.17,
      "step": 3771
    },
    {
      "epoch": 0.530296639955012,
      "grad_norm": 1.487593173980713,
      "learning_rate": 0.00016831291036888802,
      "loss": 0.9351,
      "step": 3772
    },
    {
      "epoch": 0.5304372276114158,
      "grad_norm": 1.3656667470932007,
      "learning_rate": 0.00016814295401772352,
      "loss": 1.134,
      "step": 3773
    },
    {
      "epoch": 0.5305778152678194,
      "grad_norm": 1.3995112180709839,
      "learning_rate": 0.00016797262940814606,
      "loss": 1.1797,
      "step": 3774
    },
    {
      "epoch": 0.5307184029242232,
      "grad_norm": 1.5755138397216797,
      "learning_rate": 0.00016780193746062473,
      "loss": 1.0099,
      "step": 3775
    },
    {
      "epoch": 0.530858990580627,
      "grad_norm": 1.6843140125274658,
      "learning_rate": 0.0001676308790976135,
      "loss": 1.1974,
      "step": 3776
    },
    {
      "epoch": 0.5309995782370308,
      "grad_norm": 1.6219100952148438,
      "learning_rate": 0.00016745945524354667,
      "loss": 1.1849,
      "step": 3777
    },
    {
      "epoch": 0.5311401658934346,
      "grad_norm": 1.6722092628479004,
      "learning_rate": 0.00016728766682483375,
      "loss": 1.1103,
      "step": 3778
    },
    {
      "epoch": 0.5312807535498383,
      "grad_norm": 1.6675746440887451,
      "learning_rate": 0.00016711551476985425,
      "loss": 1.0696,
      "step": 3779
    },
    {
      "epoch": 0.5314213412062421,
      "grad_norm": 1.5984926223754883,
      "learning_rate": 0.00016694300000895303,
      "loss": 1.1587,
      "step": 3780
    },
    {
      "epoch": 0.5315619288626459,
      "grad_norm": 1.3499037027359009,
      "learning_rate": 0.000166770123474435,
      "loss": 1.0533,
      "step": 3781
    },
    {
      "epoch": 0.5317025165190497,
      "grad_norm": 1.5308693647384644,
      "learning_rate": 0.00016659688610056025,
      "loss": 0.9687,
      "step": 3782
    },
    {
      "epoch": 0.5318431041754534,
      "grad_norm": 1.3356037139892578,
      "learning_rate": 0.00016642328882353878,
      "loss": 1.1602,
      "step": 3783
    },
    {
      "epoch": 0.5319836918318571,
      "grad_norm": 1.6446813344955444,
      "learning_rate": 0.00016624933258152567,
      "loss": 1.1751,
      "step": 3784
    },
    {
      "epoch": 0.5321242794882609,
      "grad_norm": 1.6401786804199219,
      "learning_rate": 0.0001660750183146159,
      "loss": 1.106,
      "step": 3785
    },
    {
      "epoch": 0.5322648671446647,
      "grad_norm": 1.5615326166152954,
      "learning_rate": 0.00016590034696483942,
      "loss": 1.1535,
      "step": 3786
    },
    {
      "epoch": 0.5324054548010685,
      "grad_norm": 1.5413111448287964,
      "learning_rate": 0.00016572531947615563,
      "loss": 1.0503,
      "step": 3787
    },
    {
      "epoch": 0.5325460424574723,
      "grad_norm": 1.7092329263687134,
      "learning_rate": 0.00016554993679444887,
      "loss": 1.0392,
      "step": 3788
    },
    {
      "epoch": 0.532686630113876,
      "grad_norm": 1.8237522840499878,
      "learning_rate": 0.00016537419986752282,
      "loss": 1.2031,
      "step": 3789
    },
    {
      "epoch": 0.5328272177702797,
      "grad_norm": 1.5319064855575562,
      "learning_rate": 0.00016519810964509582,
      "loss": 1.0445,
      "step": 3790
    },
    {
      "epoch": 0.5329678054266835,
      "grad_norm": 1.5245063304901123,
      "learning_rate": 0.00016502166707879507,
      "loss": 1.0524,
      "step": 3791
    },
    {
      "epoch": 0.5331083930830873,
      "grad_norm": 2.1862645149230957,
      "learning_rate": 0.00016484487312215233,
      "loss": 1.1641,
      "step": 3792
    },
    {
      "epoch": 0.5332489807394911,
      "grad_norm": 1.57451593875885,
      "learning_rate": 0.00016466772873059812,
      "loss": 1.169,
      "step": 3793
    },
    {
      "epoch": 0.5333895683958948,
      "grad_norm": 1.454856276512146,
      "learning_rate": 0.00016449023486145688,
      "loss": 1.1194,
      "step": 3794
    },
    {
      "epoch": 0.5335301560522986,
      "grad_norm": 1.5088508129119873,
      "learning_rate": 0.00016431239247394173,
      "loss": 1.3244,
      "step": 3795
    },
    {
      "epoch": 0.5336707437087024,
      "grad_norm": 1.4693565368652344,
      "learning_rate": 0.00016413420252914894,
      "loss": 1.1766,
      "step": 3796
    },
    {
      "epoch": 0.5338113313651062,
      "grad_norm": 1.4929934740066528,
      "learning_rate": 0.00016395566599005355,
      "loss": 1.1265,
      "step": 3797
    },
    {
      "epoch": 0.53395191902151,
      "grad_norm": 2.540339231491089,
      "learning_rate": 0.00016377678382150323,
      "loss": 1.1736,
      "step": 3798
    },
    {
      "epoch": 0.5340925066779136,
      "grad_norm": 1.635434627532959,
      "learning_rate": 0.00016359755699021382,
      "loss": 1.1947,
      "step": 3799
    },
    {
      "epoch": 0.5342330943343174,
      "grad_norm": 1.3419463634490967,
      "learning_rate": 0.00016341798646476346,
      "loss": 1.3158,
      "step": 3800
    },
    {
      "epoch": 0.5343736819907212,
      "grad_norm": 1.6953282356262207,
      "learning_rate": 0.0001632380732155881,
      "loss": 1.0691,
      "step": 3801
    },
    {
      "epoch": 0.534514269647125,
      "grad_norm": 1.6255332231521606,
      "learning_rate": 0.00016305781821497538,
      "loss": 1.1085,
      "step": 3802
    },
    {
      "epoch": 0.5346548573035288,
      "grad_norm": 1.321670651435852,
      "learning_rate": 0.00016287722243706033,
      "loss": 1.0756,
      "step": 3803
    },
    {
      "epoch": 0.5347954449599325,
      "grad_norm": 1.3633695840835571,
      "learning_rate": 0.000162696286857819,
      "loss": 1.1416,
      "step": 3804
    },
    {
      "epoch": 0.5349360326163363,
      "grad_norm": 1.5280017852783203,
      "learning_rate": 0.0001625150124550645,
      "loss": 1.1669,
      "step": 3805
    },
    {
      "epoch": 0.5350766202727401,
      "grad_norm": 1.5169228315353394,
      "learning_rate": 0.0001623334002084404,
      "loss": 1.3298,
      "step": 3806
    },
    {
      "epoch": 0.5352172079291438,
      "grad_norm": 1.666818380355835,
      "learning_rate": 0.0001621514510994164,
      "loss": 1.0442,
      "step": 3807
    },
    {
      "epoch": 0.5353577955855476,
      "grad_norm": 1.4644503593444824,
      "learning_rate": 0.00016196916611128253,
      "loss": 1.0691,
      "step": 3808
    },
    {
      "epoch": 0.5354983832419513,
      "grad_norm": 1.4461878538131714,
      "learning_rate": 0.00016178654622914414,
      "loss": 1.064,
      "step": 3809
    },
    {
      "epoch": 0.5356389708983551,
      "grad_norm": 1.706080675125122,
      "learning_rate": 0.0001616035924399161,
      "loss": 1.1756,
      "step": 3810
    },
    {
      "epoch": 0.5357795585547589,
      "grad_norm": 1.576417088508606,
      "learning_rate": 0.00016142030573231808,
      "loss": 1.2379,
      "step": 3811
    },
    {
      "epoch": 0.5359201462111627,
      "grad_norm": 1.7323365211486816,
      "learning_rate": 0.0001612366870968688,
      "loss": 1.2363,
      "step": 3812
    },
    {
      "epoch": 0.5360607338675665,
      "grad_norm": 1.6116310358047485,
      "learning_rate": 0.00016105273752588088,
      "loss": 1.1556,
      "step": 3813
    },
    {
      "epoch": 0.5362013215239702,
      "grad_norm": 1.5729904174804688,
      "learning_rate": 0.00016086845801345514,
      "loss": 1.1097,
      "step": 3814
    },
    {
      "epoch": 0.5363419091803739,
      "grad_norm": 1.6259440183639526,
      "learning_rate": 0.00016068384955547562,
      "loss": 1.2376,
      "step": 3815
    },
    {
      "epoch": 0.5364824968367777,
      "grad_norm": 2.170084238052368,
      "learning_rate": 0.0001604989131496043,
      "loss": 1.2361,
      "step": 3816
    },
    {
      "epoch": 0.5366230844931815,
      "grad_norm": 1.7361100912094116,
      "learning_rate": 0.0001603136497952749,
      "loss": 0.9918,
      "step": 3817
    },
    {
      "epoch": 0.5367636721495853,
      "grad_norm": 1.3526750802993774,
      "learning_rate": 0.0001601280604936886,
      "loss": 1.2948,
      "step": 3818
    },
    {
      "epoch": 0.536904259805989,
      "grad_norm": 1.7053451538085938,
      "learning_rate": 0.0001599421462478076,
      "loss": 1.139,
      "step": 3819
    },
    {
      "epoch": 0.5370448474623928,
      "grad_norm": 1.5709898471832275,
      "learning_rate": 0.0001597559080623506,
      "loss": 1.1139,
      "step": 3820
    },
    {
      "epoch": 0.5371854351187966,
      "grad_norm": 1.640308141708374,
      "learning_rate": 0.00015956934694378653,
      "loss": 1.1465,
      "step": 3821
    },
    {
      "epoch": 0.5373260227752004,
      "grad_norm": 1.461578369140625,
      "learning_rate": 0.00015938246390032984,
      "loss": 1.1153,
      "step": 3822
    },
    {
      "epoch": 0.5374666104316042,
      "grad_norm": 1.7849466800689697,
      "learning_rate": 0.00015919525994193437,
      "loss": 1.0736,
      "step": 3823
    },
    {
      "epoch": 0.5376071980880078,
      "grad_norm": 1.673755168914795,
      "learning_rate": 0.00015900773608028878,
      "loss": 1.0466,
      "step": 3824
    },
    {
      "epoch": 0.5377477857444116,
      "grad_norm": 1.698654294013977,
      "learning_rate": 0.00015881989332881,
      "loss": 1.2141,
      "step": 3825
    },
    {
      "epoch": 0.5378883734008154,
      "grad_norm": 1.4475127458572388,
      "learning_rate": 0.0001586317327026387,
      "loss": 1.0518,
      "step": 3826
    },
    {
      "epoch": 0.5380289610572192,
      "grad_norm": 1.4675617218017578,
      "learning_rate": 0.000158443255218633,
      "loss": 1.0976,
      "step": 3827
    },
    {
      "epoch": 0.538169548713623,
      "grad_norm": 1.7141369581222534,
      "learning_rate": 0.000158254461895364,
      "loss": 1.063,
      "step": 3828
    },
    {
      "epoch": 0.5383101363700267,
      "grad_norm": 1.5986168384552002,
      "learning_rate": 0.00015806535375310907,
      "loss": 1.0443,
      "step": 3829
    },
    {
      "epoch": 0.5384507240264305,
      "grad_norm": 2.0895659923553467,
      "learning_rate": 0.00015787593181384722,
      "loss": 1.0899,
      "step": 3830
    },
    {
      "epoch": 0.5385913116828343,
      "grad_norm": 1.99961256980896,
      "learning_rate": 0.0001576861971012531,
      "loss": 1.0945,
      "step": 3831
    },
    {
      "epoch": 0.538731899339238,
      "grad_norm": 2.195162057876587,
      "learning_rate": 0.000157496150640692,
      "loss": 1.1704,
      "step": 3832
    },
    {
      "epoch": 0.5388724869956418,
      "grad_norm": 1.7708661556243896,
      "learning_rate": 0.00015730579345921356,
      "loss": 0.9426,
      "step": 3833
    },
    {
      "epoch": 0.5390130746520455,
      "grad_norm": 1.459641933441162,
      "learning_rate": 0.0001571151265855468,
      "loss": 1.1044,
      "step": 3834
    },
    {
      "epoch": 0.5391536623084493,
      "grad_norm": 1.4998329877853394,
      "learning_rate": 0.00015692415105009443,
      "loss": 1.0277,
      "step": 3835
    },
    {
      "epoch": 0.5392942499648531,
      "grad_norm": 1.427986979484558,
      "learning_rate": 0.00015673286788492715,
      "loss": 1.2146,
      "step": 3836
    },
    {
      "epoch": 0.5394348376212569,
      "grad_norm": 1.5996015071868896,
      "learning_rate": 0.0001565412781237782,
      "loss": 0.9671,
      "step": 3837
    },
    {
      "epoch": 0.5395754252776607,
      "grad_norm": 1.6934775114059448,
      "learning_rate": 0.00015634938280203762,
      "loss": 1.0562,
      "step": 3838
    },
    {
      "epoch": 0.5397160129340643,
      "grad_norm": 1.7432687282562256,
      "learning_rate": 0.00015615718295674687,
      "loss": 1.0186,
      "step": 3839
    },
    {
      "epoch": 0.5398566005904681,
      "grad_norm": 1.9276608228683472,
      "learning_rate": 0.0001559646796265931,
      "loss": 1.2101,
      "step": 3840
    },
    {
      "epoch": 0.5399971882468719,
      "grad_norm": 1.5071669816970825,
      "learning_rate": 0.00015577187385190362,
      "loss": 1.2018,
      "step": 3841
    },
    {
      "epoch": 0.5401377759032757,
      "grad_norm": 1.6039589643478394,
      "learning_rate": 0.00015557876667464,
      "loss": 1.2119,
      "step": 3842
    },
    {
      "epoch": 0.5402783635596795,
      "grad_norm": 1.446941614151001,
      "learning_rate": 0.00015538535913839287,
      "loss": 1.3036,
      "step": 3843
    },
    {
      "epoch": 0.5404189512160832,
      "grad_norm": 1.304337739944458,
      "learning_rate": 0.00015519165228837593,
      "loss": 1.2947,
      "step": 3844
    },
    {
      "epoch": 0.540559538872487,
      "grad_norm": 1.6457767486572266,
      "learning_rate": 0.0001549976471714206,
      "loss": 1.0668,
      "step": 3845
    },
    {
      "epoch": 0.5407001265288908,
      "grad_norm": 1.6387271881103516,
      "learning_rate": 0.0001548033448359698,
      "loss": 1.0992,
      "step": 3846
    },
    {
      "epoch": 0.5408407141852946,
      "grad_norm": 1.7398921251296997,
      "learning_rate": 0.00015460874633207335,
      "loss": 0.9465,
      "step": 3847
    },
    {
      "epoch": 0.5409813018416983,
      "grad_norm": 1.522489070892334,
      "learning_rate": 0.0001544138527113809,
      "loss": 1.1254,
      "step": 3848
    },
    {
      "epoch": 0.541121889498102,
      "grad_norm": 1.6085925102233887,
      "learning_rate": 0.0001542186650271375,
      "loss": 1.1453,
      "step": 3849
    },
    {
      "epoch": 0.5412624771545058,
      "grad_norm": 1.5238244533538818,
      "learning_rate": 0.00015402318433417692,
      "loss": 1.1563,
      "step": 3850
    },
    {
      "epoch": 0.5414030648109096,
      "grad_norm": 1.7735174894332886,
      "learning_rate": 0.00015382741168891702,
      "loss": 1.0203,
      "step": 3851
    },
    {
      "epoch": 0.5415436524673134,
      "grad_norm": 1.3820545673370361,
      "learning_rate": 0.00015363134814935285,
      "loss": 0.9648,
      "step": 3852
    },
    {
      "epoch": 0.5416842401237172,
      "grad_norm": 1.6669459342956543,
      "learning_rate": 0.00015343499477505178,
      "loss": 1.1267,
      "step": 3853
    },
    {
      "epoch": 0.5418248277801209,
      "grad_norm": 1.4683483839035034,
      "learning_rate": 0.0001532383526271475,
      "loss": 1.2097,
      "step": 3854
    },
    {
      "epoch": 0.5419654154365247,
      "grad_norm": 1.6453440189361572,
      "learning_rate": 0.00015304142276833423,
      "loss": 1.2153,
      "step": 3855
    },
    {
      "epoch": 0.5421060030929284,
      "grad_norm": 1.7566879987716675,
      "learning_rate": 0.00015284420626286114,
      "loss": 1.1859,
      "step": 3856
    },
    {
      "epoch": 0.5422465907493322,
      "grad_norm": 1.6332367658615112,
      "learning_rate": 0.00015264670417652632,
      "loss": 1.0997,
      "step": 3857
    },
    {
      "epoch": 0.542387178405736,
      "grad_norm": 1.7005887031555176,
      "learning_rate": 0.0001524489175766713,
      "loss": 1.1071,
      "step": 3858
    },
    {
      "epoch": 0.5425277660621397,
      "grad_norm": 1.577656626701355,
      "learning_rate": 0.00015225084753217523,
      "loss": 1.0889,
      "step": 3859
    },
    {
      "epoch": 0.5426683537185435,
      "grad_norm": 1.64665687084198,
      "learning_rate": 0.0001520524951134491,
      "loss": 1.0065,
      "step": 3860
    },
    {
      "epoch": 0.5428089413749473,
      "grad_norm": 1.4895246028900146,
      "learning_rate": 0.00015185386139242958,
      "loss": 1.1516,
      "step": 3861
    },
    {
      "epoch": 0.5429495290313511,
      "grad_norm": 1.5270977020263672,
      "learning_rate": 0.00015165494744257395,
      "loss": 1.2059,
      "step": 3862
    },
    {
      "epoch": 0.5430901166877549,
      "grad_norm": 1.5819623470306396,
      "learning_rate": 0.0001514557543388537,
      "loss": 1.3285,
      "step": 3863
    },
    {
      "epoch": 0.5432307043441585,
      "grad_norm": 1.476211667060852,
      "learning_rate": 0.00015125628315774901,
      "loss": 0.9638,
      "step": 3864
    },
    {
      "epoch": 0.5433712920005623,
      "grad_norm": 1.8835612535476685,
      "learning_rate": 0.0001510565349772427,
      "loss": 0.9825,
      "step": 3865
    },
    {
      "epoch": 0.5435118796569661,
      "grad_norm": 1.47813081741333,
      "learning_rate": 0.0001508565108768147,
      "loss": 1.0895,
      "step": 3866
    },
    {
      "epoch": 0.5436524673133699,
      "grad_norm": 1.8078404664993286,
      "learning_rate": 0.00015065621193743601,
      "loss": 0.9923,
      "step": 3867
    },
    {
      "epoch": 0.5437930549697737,
      "grad_norm": 1.805268406867981,
      "learning_rate": 0.00015045563924156303,
      "loss": 0.9991,
      "step": 3868
    },
    {
      "epoch": 0.5439336426261774,
      "grad_norm": 1.8015936613082886,
      "learning_rate": 0.0001502547938731313,
      "loss": 1.0614,
      "step": 3869
    },
    {
      "epoch": 0.5440742302825812,
      "grad_norm": 1.578975796699524,
      "learning_rate": 0.00015005367691755023,
      "loss": 1.0953,
      "step": 3870
    },
    {
      "epoch": 0.544214817938985,
      "grad_norm": 1.7438080310821533,
      "learning_rate": 0.00014985228946169684,
      "loss": 0.975,
      "step": 3871
    },
    {
      "epoch": 0.5443554055953888,
      "grad_norm": 2.175262928009033,
      "learning_rate": 0.00014965063259391,
      "loss": 0.9503,
      "step": 3872
    },
    {
      "epoch": 0.5444959932517925,
      "grad_norm": 1.775667428970337,
      "learning_rate": 0.0001494487074039846,
      "loss": 1.222,
      "step": 3873
    },
    {
      "epoch": 0.5446365809081962,
      "grad_norm": 2.0063724517822266,
      "learning_rate": 0.00014924651498316526,
      "loss": 1.2206,
      "step": 3874
    },
    {
      "epoch": 0.5447771685646,
      "grad_norm": 1.8759170770645142,
      "learning_rate": 0.00014904405642414138,
      "loss": 1.1133,
      "step": 3875
    },
    {
      "epoch": 0.5449177562210038,
      "grad_norm": 1.7773388624191284,
      "learning_rate": 0.00014884133282104002,
      "loss": 1.1712,
      "step": 3876
    },
    {
      "epoch": 0.5450583438774076,
      "grad_norm": 1.4942837953567505,
      "learning_rate": 0.00014863834526942103,
      "loss": 0.9756,
      "step": 3877
    },
    {
      "epoch": 0.5451989315338114,
      "grad_norm": 1.6313179731369019,
      "learning_rate": 0.00014843509486627026,
      "loss": 1.0317,
      "step": 3878
    },
    {
      "epoch": 0.5453395191902151,
      "grad_norm": 1.5798286199569702,
      "learning_rate": 0.00014823158270999462,
      "loss": 1.1392,
      "step": 3879
    },
    {
      "epoch": 0.5454801068466189,
      "grad_norm": 1.5830914974212646,
      "learning_rate": 0.00014802780990041508,
      "loss": 0.9787,
      "step": 3880
    },
    {
      "epoch": 0.5456206945030226,
      "grad_norm": 1.6928977966308594,
      "learning_rate": 0.0001478237775387616,
      "loss": 1.1732,
      "step": 3881
    },
    {
      "epoch": 0.5457612821594264,
      "grad_norm": 1.8092249631881714,
      "learning_rate": 0.0001476194867276664,
      "loss": 1.1699,
      "step": 3882
    },
    {
      "epoch": 0.5459018698158302,
      "grad_norm": 1.8961039781570435,
      "learning_rate": 0.00014741493857115895,
      "loss": 1.0818,
      "step": 3883
    },
    {
      "epoch": 0.5460424574722339,
      "grad_norm": 1.442639708518982,
      "learning_rate": 0.00014721013417465895,
      "loss": 0.9733,
      "step": 3884
    },
    {
      "epoch": 0.5461830451286377,
      "grad_norm": 1.5416443347930908,
      "learning_rate": 0.00014700507464497124,
      "loss": 1.0759,
      "step": 3885
    },
    {
      "epoch": 0.5463236327850415,
      "grad_norm": 1.5705790519714355,
      "learning_rate": 0.00014679976109027926,
      "loss": 1.2461,
      "step": 3886
    },
    {
      "epoch": 0.5464642204414453,
      "grad_norm": 1.3455380201339722,
      "learning_rate": 0.00014659419462013942,
      "loss": 1.0694,
      "step": 3887
    },
    {
      "epoch": 0.5466048080978491,
      "grad_norm": 1.7763983011245728,
      "learning_rate": 0.00014638837634547462,
      "loss": 1.2408,
      "step": 3888
    },
    {
      "epoch": 0.5467453957542527,
      "grad_norm": 1.4022117853164673,
      "learning_rate": 0.00014618230737856885,
      "loss": 1.2195,
      "step": 3889
    },
    {
      "epoch": 0.5468859834106565,
      "grad_norm": 1.4808353185653687,
      "learning_rate": 0.00014597598883306088,
      "loss": 1.0538,
      "step": 3890
    },
    {
      "epoch": 0.5470265710670603,
      "grad_norm": 1.6522396802902222,
      "learning_rate": 0.00014576942182393819,
      "loss": 1.079,
      "step": 3891
    },
    {
      "epoch": 0.5471671587234641,
      "grad_norm": 1.50790536403656,
      "learning_rate": 0.00014556260746753088,
      "loss": 1.1547,
      "step": 3892
    },
    {
      "epoch": 0.5473077463798679,
      "grad_norm": 1.7406824827194214,
      "learning_rate": 0.00014535554688150585,
      "loss": 1.0822,
      "step": 3893
    },
    {
      "epoch": 0.5474483340362716,
      "grad_norm": 1.5230892896652222,
      "learning_rate": 0.000145148241184861,
      "loss": 1.1105,
      "step": 3894
    },
    {
      "epoch": 0.5475889216926754,
      "grad_norm": 1.4690616130828857,
      "learning_rate": 0.00014494069149791828,
      "loss": 1.1483,
      "step": 3895
    },
    {
      "epoch": 0.5477295093490792,
      "grad_norm": 1.6596863269805908,
      "learning_rate": 0.0001447328989423187,
      "loss": 1.2293,
      "step": 3896
    },
    {
      "epoch": 0.547870097005483,
      "grad_norm": 1.5453368425369263,
      "learning_rate": 0.00014452486464101535,
      "loss": 0.9686,
      "step": 3897
    },
    {
      "epoch": 0.5480106846618867,
      "grad_norm": 1.5397090911865234,
      "learning_rate": 0.0001443165897182683,
      "loss": 1.355,
      "step": 3898
    },
    {
      "epoch": 0.5481512723182904,
      "grad_norm": 1.4836153984069824,
      "learning_rate": 0.00014410807529963743,
      "loss": 1.0929,
      "step": 3899
    },
    {
      "epoch": 0.5482918599746942,
      "grad_norm": 1.4231551885604858,
      "learning_rate": 0.00014389932251197731,
      "loss": 1.2543,
      "step": 3900
    },
    {
      "epoch": 0.548432447631098,
      "grad_norm": 1.5932199954986572,
      "learning_rate": 0.00014369033248343033,
      "loss": 0.9643,
      "step": 3901
    },
    {
      "epoch": 0.5485730352875018,
      "grad_norm": 1.4862078428268433,
      "learning_rate": 0.00014348110634342155,
      "loss": 1.2286,
      "step": 3902
    },
    {
      "epoch": 0.5487136229439056,
      "grad_norm": 1.5863451957702637,
      "learning_rate": 0.0001432716452226514,
      "loss": 1.1582,
      "step": 3903
    },
    {
      "epoch": 0.5488542106003093,
      "grad_norm": 1.6338907480239868,
      "learning_rate": 0.00014306195025309064,
      "loss": 1.3017,
      "step": 3904
    },
    {
      "epoch": 0.548994798256713,
      "grad_norm": 1.5038765668869019,
      "learning_rate": 0.0001428520225679734,
      "loss": 1.04,
      "step": 3905
    },
    {
      "epoch": 0.5491353859131168,
      "grad_norm": 1.4763274192810059,
      "learning_rate": 0.000142641863301792,
      "loss": 1.1083,
      "step": 3906
    },
    {
      "epoch": 0.5492759735695206,
      "grad_norm": 1.6350760459899902,
      "learning_rate": 0.00014243147359028967,
      "loss": 1.1975,
      "step": 3907
    },
    {
      "epoch": 0.5494165612259244,
      "grad_norm": 1.7605444192886353,
      "learning_rate": 0.00014222085457045553,
      "loss": 1.059,
      "step": 3908
    },
    {
      "epoch": 0.5495571488823281,
      "grad_norm": 1.466586709022522,
      "learning_rate": 0.00014201000738051737,
      "loss": 1.1299,
      "step": 3909
    },
    {
      "epoch": 0.5496977365387319,
      "grad_norm": 1.7486028671264648,
      "learning_rate": 0.00014179893315993675,
      "loss": 1.0002,
      "step": 3910
    },
    {
      "epoch": 0.5498383241951357,
      "grad_norm": 1.4624099731445312,
      "learning_rate": 0.0001415876330494015,
      "loss": 1.0942,
      "step": 3911
    },
    {
      "epoch": 0.5499789118515395,
      "grad_norm": 1.619309425354004,
      "learning_rate": 0.00014137610819082064,
      "loss": 0.9939,
      "step": 3912
    },
    {
      "epoch": 0.5501194995079433,
      "grad_norm": 1.647270679473877,
      "learning_rate": 0.00014116435972731753,
      "loss": 1.1556,
      "step": 3913
    },
    {
      "epoch": 0.5502600871643469,
      "grad_norm": 1.4949188232421875,
      "learning_rate": 0.00014095238880322411,
      "loss": 1.2394,
      "step": 3914
    },
    {
      "epoch": 0.5504006748207507,
      "grad_norm": 1.807557225227356,
      "learning_rate": 0.0001407401965640745,
      "loss": 1.1138,
      "step": 3915
    },
    {
      "epoch": 0.5505412624771545,
      "grad_norm": 1.4974621534347534,
      "learning_rate": 0.00014052778415659861,
      "loss": 1.1778,
      "step": 3916
    },
    {
      "epoch": 0.5506818501335583,
      "grad_norm": 1.4404653310775757,
      "learning_rate": 0.00014031515272871654,
      "loss": 1.2562,
      "step": 3917
    },
    {
      "epoch": 0.5508224377899621,
      "grad_norm": 1.3669326305389404,
      "learning_rate": 0.00014010230342953182,
      "loss": 0.9315,
      "step": 3918
    },
    {
      "epoch": 0.5509630254463658,
      "grad_norm": 1.6525205373764038,
      "learning_rate": 0.00013988923740932554,
      "loss": 1.1936,
      "step": 3919
    },
    {
      "epoch": 0.5511036131027696,
      "grad_norm": 1.8875749111175537,
      "learning_rate": 0.00013967595581954972,
      "loss": 1.0422,
      "step": 3920
    },
    {
      "epoch": 0.5512442007591734,
      "grad_norm": 1.531981348991394,
      "learning_rate": 0.00013946245981282166,
      "loss": 1.2123,
      "step": 3921
    },
    {
      "epoch": 0.5513847884155771,
      "grad_norm": 1.55530846118927,
      "learning_rate": 0.00013924875054291728,
      "loss": 1.349,
      "step": 3922
    },
    {
      "epoch": 0.5515253760719809,
      "grad_norm": 1.7464920282363892,
      "learning_rate": 0.00013903482916476513,
      "loss": 1.0606,
      "step": 3923
    },
    {
      "epoch": 0.5516659637283846,
      "grad_norm": 1.5669474601745605,
      "learning_rate": 0.00013882069683443968,
      "loss": 1.0327,
      "step": 3924
    },
    {
      "epoch": 0.5518065513847884,
      "grad_norm": 1.4222705364227295,
      "learning_rate": 0.00013860635470915603,
      "loss": 1.0018,
      "step": 3925
    },
    {
      "epoch": 0.5519471390411922,
      "grad_norm": 1.3238657712936401,
      "learning_rate": 0.0001383918039472624,
      "loss": 1.2434,
      "step": 3926
    },
    {
      "epoch": 0.552087726697596,
      "grad_norm": 1.8229690790176392,
      "learning_rate": 0.00013817704570823512,
      "loss": 1.1299,
      "step": 3927
    },
    {
      "epoch": 0.5522283143539998,
      "grad_norm": 1.527869462966919,
      "learning_rate": 0.00013796208115267114,
      "loss": 0.953,
      "step": 3928
    },
    {
      "epoch": 0.5523689020104035,
      "grad_norm": 1.3534142971038818,
      "learning_rate": 0.00013774691144228312,
      "loss": 1.1512,
      "step": 3929
    },
    {
      "epoch": 0.5525094896668072,
      "grad_norm": 1.6637139320373535,
      "learning_rate": 0.00013753153773989177,
      "loss": 1.0237,
      "step": 3930
    },
    {
      "epoch": 0.552650077323211,
      "grad_norm": 1.475791335105896,
      "learning_rate": 0.0001373159612094205,
      "loss": 1.2538,
      "step": 3931
    },
    {
      "epoch": 0.5527906649796148,
      "grad_norm": 1.4248042106628418,
      "learning_rate": 0.00013710018301588893,
      "loss": 1.0571,
      "step": 3932
    },
    {
      "epoch": 0.5529312526360186,
      "grad_norm": 1.6820420026779175,
      "learning_rate": 0.00013688420432540635,
      "loss": 0.9716,
      "step": 3933
    },
    {
      "epoch": 0.5530718402924223,
      "grad_norm": 1.5674381256103516,
      "learning_rate": 0.00013666802630516567,
      "loss": 1.2184,
      "step": 3934
    },
    {
      "epoch": 0.5532124279488261,
      "grad_norm": 1.5818829536437988,
      "learning_rate": 0.00013645165012343688,
      "loss": 1.0481,
      "step": 3935
    },
    {
      "epoch": 0.5533530156052299,
      "grad_norm": 1.56309175491333,
      "learning_rate": 0.00013623507694956102,
      "loss": 1.0375,
      "step": 3936
    },
    {
      "epoch": 0.5534936032616337,
      "grad_norm": 1.7113338708877563,
      "learning_rate": 0.0001360183079539436,
      "loss": 1.1702,
      "step": 3937
    },
    {
      "epoch": 0.5536341909180374,
      "grad_norm": 1.703328013420105,
      "learning_rate": 0.00013580134430804866,
      "loss": 1.0707,
      "step": 3938
    },
    {
      "epoch": 0.5537747785744411,
      "grad_norm": 1.4956313371658325,
      "learning_rate": 0.0001355841871843917,
      "loss": 1.2238,
      "step": 3939
    },
    {
      "epoch": 0.5539153662308449,
      "grad_norm": 1.680105209350586,
      "learning_rate": 0.00013536683775653422,
      "loss": 1.2278,
      "step": 3940
    },
    {
      "epoch": 0.5540559538872487,
      "grad_norm": 1.55063796043396,
      "learning_rate": 0.00013514929719907678,
      "loss": 1.0527,
      "step": 3941
    },
    {
      "epoch": 0.5541965415436525,
      "grad_norm": 1.355248212814331,
      "learning_rate": 0.00013493156668765302,
      "loss": 1.0801,
      "step": 3942
    },
    {
      "epoch": 0.5543371292000563,
      "grad_norm": 1.7224987745285034,
      "learning_rate": 0.00013471364739892284,
      "loss": 1.1422,
      "step": 3943
    },
    {
      "epoch": 0.55447771685646,
      "grad_norm": 1.6128566265106201,
      "learning_rate": 0.00013449554051056655,
      "loss": 1.1973,
      "step": 3944
    },
    {
      "epoch": 0.5546183045128638,
      "grad_norm": 1.7153887748718262,
      "learning_rate": 0.00013427724720127822,
      "loss": 1.046,
      "step": 3945
    },
    {
      "epoch": 0.5547588921692675,
      "grad_norm": 1.62929368019104,
      "learning_rate": 0.00013405876865075952,
      "loss": 1.2837,
      "step": 3946
    },
    {
      "epoch": 0.5548994798256713,
      "grad_norm": 1.4206769466400146,
      "learning_rate": 0.00013384010603971288,
      "loss": 1.2518,
      "step": 3947
    },
    {
      "epoch": 0.5550400674820751,
      "grad_norm": 1.7167932987213135,
      "learning_rate": 0.00013362126054983567,
      "loss": 1.0929,
      "step": 3948
    },
    {
      "epoch": 0.5551806551384788,
      "grad_norm": 1.9676324129104614,
      "learning_rate": 0.0001334022333638135,
      "loss": 1.017,
      "step": 3949
    },
    {
      "epoch": 0.5553212427948826,
      "grad_norm": 1.7059996128082275,
      "learning_rate": 0.00013318302566531405,
      "loss": 1.2151,
      "step": 3950
    },
    {
      "epoch": 0.5554618304512864,
      "grad_norm": 1.6943539381027222,
      "learning_rate": 0.00013296363863898037,
      "loss": 1.1996,
      "step": 3951
    },
    {
      "epoch": 0.5556024181076902,
      "grad_norm": 1.588172197341919,
      "learning_rate": 0.00013274407347042448,
      "loss": 1.0493,
      "step": 3952
    },
    {
      "epoch": 0.555743005764094,
      "grad_norm": 1.566749930381775,
      "learning_rate": 0.0001325243313462216,
      "loss": 1.0695,
      "step": 3953
    },
    {
      "epoch": 0.5558835934204976,
      "grad_norm": 1.77114999294281,
      "learning_rate": 0.00013230441345390273,
      "loss": 1.141,
      "step": 3954
    },
    {
      "epoch": 0.5560241810769014,
      "grad_norm": 1.5325640439987183,
      "learning_rate": 0.0001320843209819492,
      "loss": 1.2529,
      "step": 3955
    },
    {
      "epoch": 0.5561647687333052,
      "grad_norm": 1.328894019126892,
      "learning_rate": 0.0001318640551197852,
      "loss": 1.1602,
      "step": 3956
    },
    {
      "epoch": 0.556305356389709,
      "grad_norm": 1.6925562620162964,
      "learning_rate": 0.00013164361705777283,
      "loss": 1.0591,
      "step": 3957
    },
    {
      "epoch": 0.5564459440461128,
      "grad_norm": 1.4138293266296387,
      "learning_rate": 0.00013142300798720395,
      "loss": 1.1272,
      "step": 3958
    },
    {
      "epoch": 0.5565865317025165,
      "grad_norm": 1.6659469604492188,
      "learning_rate": 0.00013120222910029515,
      "loss": 1.08,
      "step": 3959
    },
    {
      "epoch": 0.5567271193589203,
      "grad_norm": 1.79518461227417,
      "learning_rate": 0.0001309812815901803,
      "loss": 1.1633,
      "step": 3960
    },
    {
      "epoch": 0.5568677070153241,
      "grad_norm": 1.5644574165344238,
      "learning_rate": 0.00013076016665090513,
      "loss": 1.1582,
      "step": 3961
    },
    {
      "epoch": 0.5570082946717279,
      "grad_norm": 1.4507501125335693,
      "learning_rate": 0.00013053888547741953,
      "loss": 1.1284,
      "step": 3962
    },
    {
      "epoch": 0.5571488823281316,
      "grad_norm": 1.7567702531814575,
      "learning_rate": 0.00013031743926557215,
      "loss": 1.0889,
      "step": 3963
    },
    {
      "epoch": 0.5572894699845353,
      "grad_norm": 1.5538691282272339,
      "learning_rate": 0.00013009582921210354,
      "loss": 0.9984,
      "step": 3964
    },
    {
      "epoch": 0.5574300576409391,
      "grad_norm": 1.7582006454467773,
      "learning_rate": 0.0001298740565146396,
      "loss": 1.1015,
      "step": 3965
    },
    {
      "epoch": 0.5575706452973429,
      "grad_norm": 1.521117091178894,
      "learning_rate": 0.00012965212237168504,
      "loss": 1.1838,
      "step": 3966
    },
    {
      "epoch": 0.5577112329537467,
      "grad_norm": 2.1228201389312744,
      "learning_rate": 0.00012943002798261732,
      "loss": 1.1935,
      "step": 3967
    },
    {
      "epoch": 0.5578518206101505,
      "grad_norm": 1.7819856405258179,
      "learning_rate": 0.00012920777454767976,
      "loss": 0.9513,
      "step": 3968
    },
    {
      "epoch": 0.5579924082665542,
      "grad_norm": 1.4721482992172241,
      "learning_rate": 0.00012898536326797524,
      "loss": 0.9879,
      "step": 3969
    },
    {
      "epoch": 0.558132995922958,
      "grad_norm": 1.5510343313217163,
      "learning_rate": 0.0001287627953454597,
      "loss": 1.143,
      "step": 3970
    },
    {
      "epoch": 0.5582735835793617,
      "grad_norm": 1.7081395387649536,
      "learning_rate": 0.00012854007198293533,
      "loss": 1.1305,
      "step": 3971
    },
    {
      "epoch": 0.5584141712357655,
      "grad_norm": 1.5295957326889038,
      "learning_rate": 0.00012831719438404494,
      "loss": 1.1284,
      "step": 3972
    },
    {
      "epoch": 0.5585547588921693,
      "grad_norm": 1.4552247524261475,
      "learning_rate": 0.0001280941637532642,
      "loss": 1.1684,
      "step": 3973
    },
    {
      "epoch": 0.558695346548573,
      "grad_norm": 1.5246106386184692,
      "learning_rate": 0.00012787098129589627,
      "loss": 1.1401,
      "step": 3974
    },
    {
      "epoch": 0.5588359342049768,
      "grad_norm": 1.6140977144241333,
      "learning_rate": 0.00012764764821806442,
      "loss": 1.1534,
      "step": 3975
    },
    {
      "epoch": 0.5589765218613806,
      "grad_norm": 1.8850557804107666,
      "learning_rate": 0.00012742416572670648,
      "loss": 1.1321,
      "step": 3976
    },
    {
      "epoch": 0.5591171095177844,
      "grad_norm": 1.4362857341766357,
      "learning_rate": 0.00012720053502956707,
      "loss": 1.2769,
      "step": 3977
    },
    {
      "epoch": 0.559257697174188,
      "grad_norm": 2.1125741004943848,
      "learning_rate": 0.00012697675733519225,
      "loss": 0.9734,
      "step": 3978
    },
    {
      "epoch": 0.5593982848305918,
      "grad_norm": 1.383237600326538,
      "learning_rate": 0.00012675283385292206,
      "loss": 1.0679,
      "step": 3979
    },
    {
      "epoch": 0.5595388724869956,
      "grad_norm": 1.5852957963943481,
      "learning_rate": 0.00012652876579288496,
      "loss": 0.9735,
      "step": 3980
    },
    {
      "epoch": 0.5596794601433994,
      "grad_norm": 1.5995701551437378,
      "learning_rate": 0.00012630455436599017,
      "loss": 1.0902,
      "step": 3981
    },
    {
      "epoch": 0.5598200477998032,
      "grad_norm": 1.5200456380844116,
      "learning_rate": 0.00012608020078392213,
      "loss": 0.9442,
      "step": 3982
    },
    {
      "epoch": 0.5599606354562069,
      "grad_norm": 1.3112149238586426,
      "learning_rate": 0.000125855706259133,
      "loss": 1.2304,
      "step": 3983
    },
    {
      "epoch": 0.5601012231126107,
      "grad_norm": 1.7582054138183594,
      "learning_rate": 0.00012563107200483739,
      "loss": 1.0576,
      "step": 3984
    },
    {
      "epoch": 0.5602418107690145,
      "grad_norm": 1.3468613624572754,
      "learning_rate": 0.00012540629923500426,
      "loss": 0.9773,
      "step": 3985
    },
    {
      "epoch": 0.5603823984254183,
      "grad_norm": 1.548923134803772,
      "learning_rate": 0.00012518138916435168,
      "loss": 1.0694,
      "step": 3986
    },
    {
      "epoch": 0.560522986081822,
      "grad_norm": 1.6743748188018799,
      "learning_rate": 0.00012495634300833928,
      "loss": 0.9958,
      "step": 3987
    },
    {
      "epoch": 0.5606635737382257,
      "grad_norm": 1.6062607765197754,
      "learning_rate": 0.00012473116198316266,
      "loss": 1.1341,
      "step": 3988
    },
    {
      "epoch": 0.5608041613946295,
      "grad_norm": 1.6284127235412598,
      "learning_rate": 0.00012450584730574574,
      "loss": 1.1989,
      "step": 3989
    },
    {
      "epoch": 0.5609447490510333,
      "grad_norm": 1.6897327899932861,
      "learning_rate": 0.00012428040019373503,
      "loss": 1.127,
      "step": 3990
    },
    {
      "epoch": 0.5610853367074371,
      "grad_norm": 1.421090006828308,
      "learning_rate": 0.00012405482186549267,
      "loss": 1.0254,
      "step": 3991
    },
    {
      "epoch": 0.5612259243638409,
      "grad_norm": 1.6002284288406372,
      "learning_rate": 0.00012382911354008995,
      "loss": 0.9916,
      "step": 3992
    },
    {
      "epoch": 0.5613665120202446,
      "grad_norm": 1.5964062213897705,
      "learning_rate": 0.00012360327643730075,
      "loss": 1.2777,
      "step": 3993
    },
    {
      "epoch": 0.5615070996766484,
      "grad_norm": 1.5343259572982788,
      "learning_rate": 0.0001233773117775946,
      "loss": 1.1566,
      "step": 3994
    },
    {
      "epoch": 0.5616476873330521,
      "grad_norm": 1.9964089393615723,
      "learning_rate": 0.00012315122078213072,
      "loss": 1.0836,
      "step": 3995
    },
    {
      "epoch": 0.5617882749894559,
      "grad_norm": 1.665576696395874,
      "learning_rate": 0.00012292500467275092,
      "loss": 1.0852,
      "step": 3996
    },
    {
      "epoch": 0.5619288626458597,
      "grad_norm": 1.567152976989746,
      "learning_rate": 0.00012269866467197322,
      "loss": 1.1021,
      "step": 3997
    },
    {
      "epoch": 0.5620694503022634,
      "grad_norm": 1.8434057235717773,
      "learning_rate": 0.00012247220200298494,
      "loss": 0.9909,
      "step": 3998
    },
    {
      "epoch": 0.5622100379586672,
      "grad_norm": 1.3561681509017944,
      "learning_rate": 0.0001222456178896366,
      "loss": 1.1852,
      "step": 3999
    },
    {
      "epoch": 0.562350625615071,
      "grad_norm": 1.6375491619110107,
      "learning_rate": 0.00012201891355643498,
      "loss": 1.2658,
      "step": 4000
    },
    {
      "epoch": 0.562350625615071,
      "eval_loss": 1.1635297536849976,
      "eval_runtime": 771.6759,
      "eval_samples_per_second": 16.388,
      "eval_steps_per_second": 8.194,
      "step": 4000
    },
    {
      "epoch": 0.5624912132714748,
      "grad_norm": 1.624438762664795,
      "learning_rate": 0.00012179209022853654,
      "loss": 1.0555,
      "step": 4001
    },
    {
      "epoch": 0.5626318009278786,
      "grad_norm": 1.8508399724960327,
      "learning_rate": 0.0001215651491317405,
      "loss": 1.1328,
      "step": 4002
    },
    {
      "epoch": 0.5627723885842822,
      "grad_norm": 2.248283863067627,
      "learning_rate": 0.00012133809149248321,
      "loss": 1.139,
      "step": 4003
    },
    {
      "epoch": 0.562912976240686,
      "grad_norm": 2.1197385787963867,
      "learning_rate": 0.00012111091853783014,
      "loss": 1.0557,
      "step": 4004
    },
    {
      "epoch": 0.5630535638970898,
      "grad_norm": 1.6341224908828735,
      "learning_rate": 0.00012088363149547039,
      "loss": 1.1225,
      "step": 4005
    },
    {
      "epoch": 0.5631941515534936,
      "grad_norm": 1.6008939743041992,
      "learning_rate": 0.00012065623159370926,
      "loss": 1.1455,
      "step": 4006
    },
    {
      "epoch": 0.5633347392098974,
      "grad_norm": 1.8846298456192017,
      "learning_rate": 0.00012042872006146249,
      "loss": 1.0211,
      "step": 4007
    },
    {
      "epoch": 0.5634753268663011,
      "grad_norm": 1.8133467435836792,
      "learning_rate": 0.00012020109812824847,
      "loss": 1.0416,
      "step": 4008
    },
    {
      "epoch": 0.5636159145227049,
      "grad_norm": 1.6468199491500854,
      "learning_rate": 0.00011997336702418266,
      "loss": 1.2404,
      "step": 4009
    },
    {
      "epoch": 0.5637565021791087,
      "grad_norm": 1.9869403839111328,
      "learning_rate": 0.00011974552797997039,
      "loss": 1.0478,
      "step": 4010
    },
    {
      "epoch": 0.5638970898355125,
      "grad_norm": 1.3261923789978027,
      "learning_rate": 0.00011951758222690021,
      "loss": 1.1583,
      "step": 4011
    },
    {
      "epoch": 0.5640376774919162,
      "grad_norm": 1.650475263595581,
      "learning_rate": 0.0001192895309968376,
      "loss": 1.1306,
      "step": 4012
    },
    {
      "epoch": 0.5641782651483199,
      "grad_norm": 1.4865940809249878,
      "learning_rate": 0.00011906137552221766,
      "loss": 0.9445,
      "step": 4013
    },
    {
      "epoch": 0.5643188528047237,
      "grad_norm": 1.3799567222595215,
      "learning_rate": 0.00011883311703603919,
      "loss": 1.0271,
      "step": 4014
    },
    {
      "epoch": 0.5644594404611275,
      "grad_norm": 1.3713600635528564,
      "learning_rate": 0.00011860475677185756,
      "loss": 1.1148,
      "step": 4015
    },
    {
      "epoch": 0.5646000281175313,
      "grad_norm": 1.6336877346038818,
      "learning_rate": 0.00011837629596377829,
      "loss": 0.9557,
      "step": 4016
    },
    {
      "epoch": 0.5647406157739351,
      "grad_norm": 1.601557970046997,
      "learning_rate": 0.00011814773584644995,
      "loss": 1.1583,
      "step": 4017
    },
    {
      "epoch": 0.5648812034303388,
      "grad_norm": 1.618944525718689,
      "learning_rate": 0.00011791907765505815,
      "loss": 1.1211,
      "step": 4018
    },
    {
      "epoch": 0.5650217910867426,
      "grad_norm": 1.4385606050491333,
      "learning_rate": 0.0001176903226253183,
      "loss": 1.1319,
      "step": 4019
    },
    {
      "epoch": 0.5651623787431463,
      "grad_norm": 1.8210099935531616,
      "learning_rate": 0.00011746147199346931,
      "loss": 1.2691,
      "step": 4020
    },
    {
      "epoch": 0.5653029663995501,
      "grad_norm": 1.6339364051818848,
      "learning_rate": 0.00011723252699626648,
      "loss": 1.1182,
      "step": 4021
    },
    {
      "epoch": 0.5654435540559539,
      "grad_norm": 1.8090896606445312,
      "learning_rate": 0.00011700348887097532,
      "loss": 1.0659,
      "step": 4022
    },
    {
      "epoch": 0.5655841417123576,
      "grad_norm": 1.5306227207183838,
      "learning_rate": 0.00011677435885536454,
      "loss": 1.0094,
      "step": 4023
    },
    {
      "epoch": 0.5657247293687614,
      "grad_norm": 1.6284476518630981,
      "learning_rate": 0.00011654513818769954,
      "loss": 1.0256,
      "step": 4024
    },
    {
      "epoch": 0.5658653170251652,
      "grad_norm": 2.015366554260254,
      "learning_rate": 0.00011631582810673534,
      "loss": 1.1606,
      "step": 4025
    },
    {
      "epoch": 0.566005904681569,
      "grad_norm": 1.5720336437225342,
      "learning_rate": 0.00011608642985171046,
      "loss": 1.1766,
      "step": 4026
    },
    {
      "epoch": 0.5661464923379728,
      "grad_norm": 1.4904803037643433,
      "learning_rate": 0.0001158569446623398,
      "loss": 1.2244,
      "step": 4027
    },
    {
      "epoch": 0.5662870799943764,
      "grad_norm": 1.5877892971038818,
      "learning_rate": 0.00011562737377880814,
      "loss": 0.9824,
      "step": 4028
    },
    {
      "epoch": 0.5664276676507802,
      "grad_norm": 2.1488122940063477,
      "learning_rate": 0.00011539771844176342,
      "loss": 1.1136,
      "step": 4029
    },
    {
      "epoch": 0.566568255307184,
      "grad_norm": 1.5014896392822266,
      "learning_rate": 0.00011516797989230956,
      "loss": 1.2437,
      "step": 4030
    },
    {
      "epoch": 0.5667088429635878,
      "grad_norm": 1.3258026838302612,
      "learning_rate": 0.00011493815937200094,
      "loss": 1.112,
      "step": 4031
    },
    {
      "epoch": 0.5668494306199916,
      "grad_norm": 1.4623006582260132,
      "learning_rate": 0.00011470825812283417,
      "loss": 0.9097,
      "step": 4032
    },
    {
      "epoch": 0.5669900182763953,
      "grad_norm": 1.5426688194274902,
      "learning_rate": 0.00011447827738724269,
      "loss": 1.1655,
      "step": 4033
    },
    {
      "epoch": 0.5671306059327991,
      "grad_norm": 1.58692467212677,
      "learning_rate": 0.00011424821840808895,
      "loss": 0.9751,
      "step": 4034
    },
    {
      "epoch": 0.5672711935892029,
      "grad_norm": 1.3573929071426392,
      "learning_rate": 0.00011401808242865894,
      "loss": 1.2641,
      "step": 4035
    },
    {
      "epoch": 0.5674117812456067,
      "grad_norm": 1.460105299949646,
      "learning_rate": 0.00011378787069265415,
      "loss": 1.0974,
      "step": 4036
    },
    {
      "epoch": 0.5675523689020104,
      "grad_norm": 1.286098837852478,
      "learning_rate": 0.00011355758444418592,
      "loss": 1.0506,
      "step": 4037
    },
    {
      "epoch": 0.5676929565584141,
      "grad_norm": 1.4736844301223755,
      "learning_rate": 0.00011332722492776774,
      "loss": 1.1226,
      "step": 4038
    },
    {
      "epoch": 0.5678335442148179,
      "grad_norm": 1.400517225265503,
      "learning_rate": 0.00011309679338830983,
      "loss": 1.0403,
      "step": 4039
    },
    {
      "epoch": 0.5679741318712217,
      "grad_norm": 1.496596336364746,
      "learning_rate": 0.00011286629107111086,
      "loss": 1.2497,
      "step": 4040
    },
    {
      "epoch": 0.5681147195276255,
      "grad_norm": 1.4562294483184814,
      "learning_rate": 0.00011263571922185245,
      "loss": 1.2057,
      "step": 4041
    },
    {
      "epoch": 0.5682553071840293,
      "grad_norm": 1.5912797451019287,
      "learning_rate": 0.00011240507908659191,
      "loss": 1.1049,
      "step": 4042
    },
    {
      "epoch": 0.568395894840433,
      "grad_norm": 1.5452648401260376,
      "learning_rate": 0.00011217437191175554,
      "loss": 1.0454,
      "step": 4043
    },
    {
      "epoch": 0.5685364824968367,
      "grad_norm": 1.6715019941329956,
      "learning_rate": 0.00011194359894413185,
      "loss": 1.3876,
      "step": 4044
    },
    {
      "epoch": 0.5686770701532405,
      "grad_norm": 1.839421272277832,
      "learning_rate": 0.00011171276143086502,
      "loss": 1.2129,
      "step": 4045
    },
    {
      "epoch": 0.5688176578096443,
      "grad_norm": 1.5027565956115723,
      "learning_rate": 0.00011148186061944805,
      "loss": 1.0731,
      "step": 4046
    },
    {
      "epoch": 0.5689582454660481,
      "grad_norm": 1.7460906505584717,
      "learning_rate": 0.000111250897757716,
      "loss": 1.0549,
      "step": 4047
    },
    {
      "epoch": 0.5690988331224518,
      "grad_norm": 1.6669596433639526,
      "learning_rate": 0.00011101987409383934,
      "loss": 1.0306,
      "step": 4048
    },
    {
      "epoch": 0.5692394207788556,
      "grad_norm": 1.4240697622299194,
      "learning_rate": 0.00011078879087631673,
      "loss": 1.2094,
      "step": 4049
    },
    {
      "epoch": 0.5693800084352594,
      "grad_norm": 1.5358521938323975,
      "learning_rate": 0.00011055764935396938,
      "loss": 1.2325,
      "step": 4050
    },
    {
      "epoch": 0.5695205960916632,
      "grad_norm": 1.4270415306091309,
      "learning_rate": 0.00011032645077593288,
      "loss": 1.0587,
      "step": 4051
    },
    {
      "epoch": 0.569661183748067,
      "grad_norm": 2.9819393157958984,
      "learning_rate": 0.00011009519639165169,
      "loss": 1.188,
      "step": 4052
    },
    {
      "epoch": 0.5698017714044706,
      "grad_norm": 1.8607054948806763,
      "learning_rate": 0.00010986388745087135,
      "loss": 1.0192,
      "step": 4053
    },
    {
      "epoch": 0.5699423590608744,
      "grad_norm": 1.5495058298110962,
      "learning_rate": 0.0001096325252036329,
      "loss": 1.1255,
      "step": 4054
    },
    {
      "epoch": 0.5700829467172782,
      "grad_norm": 1.585081696510315,
      "learning_rate": 0.00010940111090026477,
      "loss": 1.0341,
      "step": 4055
    },
    {
      "epoch": 0.570223534373682,
      "grad_norm": 1.5388468503952026,
      "learning_rate": 0.00010916964579137725,
      "loss": 0.9729,
      "step": 4056
    },
    {
      "epoch": 0.5703641220300858,
      "grad_norm": 1.5102996826171875,
      "learning_rate": 0.00010893813112785467,
      "loss": 1.0614,
      "step": 4057
    },
    {
      "epoch": 0.5705047096864895,
      "grad_norm": 1.9239026308059692,
      "learning_rate": 0.00010870656816084983,
      "loss": 1.0869,
      "step": 4058
    },
    {
      "epoch": 0.5706452973428933,
      "grad_norm": 1.6632723808288574,
      "learning_rate": 0.00010847495814177593,
      "loss": 1.1031,
      "step": 4059
    },
    {
      "epoch": 0.570785884999297,
      "grad_norm": 1.9787684679031372,
      "learning_rate": 0.00010824330232230089,
      "loss": 1.0795,
      "step": 4060
    },
    {
      "epoch": 0.5709264726557008,
      "grad_norm": 1.3629584312438965,
      "learning_rate": 0.00010801160195433972,
      "loss": 1.2766,
      "step": 4061
    },
    {
      "epoch": 0.5710670603121046,
      "grad_norm": 1.5499821901321411,
      "learning_rate": 0.00010777985829004881,
      "loss": 0.9935,
      "step": 4062
    },
    {
      "epoch": 0.5712076479685083,
      "grad_norm": 1.7588714361190796,
      "learning_rate": 0.00010754807258181787,
      "loss": 1.0969,
      "step": 4063
    },
    {
      "epoch": 0.5713482356249121,
      "grad_norm": 1.5832879543304443,
      "learning_rate": 0.00010731624608226427,
      "loss": 1.3267,
      "step": 4064
    },
    {
      "epoch": 0.5714888232813159,
      "grad_norm": 1.6579564809799194,
      "learning_rate": 0.00010708438004422543,
      "loss": 1.0284,
      "step": 4065
    },
    {
      "epoch": 0.5716294109377197,
      "grad_norm": 1.4698227643966675,
      "learning_rate": 0.00010685247572075303,
      "loss": 1.1115,
      "step": 4066
    },
    {
      "epoch": 0.5717699985941235,
      "grad_norm": 1.4323146343231201,
      "learning_rate": 0.00010662053436510504,
      "loss": 1.0304,
      "step": 4067
    },
    {
      "epoch": 0.5719105862505272,
      "grad_norm": 1.3872884511947632,
      "learning_rate": 0.00010638855723073989,
      "loss": 0.9836,
      "step": 4068
    },
    {
      "epoch": 0.5720511739069309,
      "grad_norm": 1.7440104484558105,
      "learning_rate": 0.0001061565455713093,
      "loss": 1.3509,
      "step": 4069
    },
    {
      "epoch": 0.5721917615633347,
      "grad_norm": 1.6638835668563843,
      "learning_rate": 0.00010592450064065159,
      "loss": 1.0616,
      "step": 4070
    },
    {
      "epoch": 0.5723323492197385,
      "grad_norm": 1.874626874923706,
      "learning_rate": 0.00010569242369278491,
      "loss": 1.0749,
      "step": 4071
    },
    {
      "epoch": 0.5724729368761423,
      "grad_norm": 1.3097511529922485,
      "learning_rate": 0.00010546031598190027,
      "loss": 0.9199,
      "step": 4072
    },
    {
      "epoch": 0.572613524532546,
      "grad_norm": 1.534995198249817,
      "learning_rate": 0.00010522817876235509,
      "loss": 1.2265,
      "step": 4073
    },
    {
      "epoch": 0.5727541121889498,
      "grad_norm": 1.5412578582763672,
      "learning_rate": 0.00010499601328866626,
      "loss": 1.2338,
      "step": 4074
    },
    {
      "epoch": 0.5728946998453536,
      "grad_norm": 1.59591805934906,
      "learning_rate": 0.00010476382081550338,
      "loss": 1.1714,
      "step": 4075
    },
    {
      "epoch": 0.5730352875017574,
      "grad_norm": 1.4012467861175537,
      "learning_rate": 0.00010453160259768171,
      "loss": 0.918,
      "step": 4076
    },
    {
      "epoch": 0.5731758751581612,
      "grad_norm": 1.4918253421783447,
      "learning_rate": 0.00010429935989015595,
      "loss": 1.2636,
      "step": 4077
    },
    {
      "epoch": 0.5733164628145648,
      "grad_norm": 1.2853596210479736,
      "learning_rate": 0.00010406709394801304,
      "loss": 1.091,
      "step": 4078
    },
    {
      "epoch": 0.5734570504709686,
      "grad_norm": 1.900497555732727,
      "learning_rate": 0.00010383480602646551,
      "loss": 1.069,
      "step": 4079
    },
    {
      "epoch": 0.5735976381273724,
      "grad_norm": 1.5572820901870728,
      "learning_rate": 0.00010360249738084443,
      "loss": 1.1396,
      "step": 4080
    },
    {
      "epoch": 0.5737382257837762,
      "grad_norm": 1.4134215116500854,
      "learning_rate": 0.00010337016926659337,
      "loss": 1.1554,
      "step": 4081
    },
    {
      "epoch": 0.57387881344018,
      "grad_norm": 1.8869800567626953,
      "learning_rate": 0.0001031378229392606,
      "loss": 1.106,
      "step": 4082
    },
    {
      "epoch": 0.5740194010965837,
      "grad_norm": 1.4702309370040894,
      "learning_rate": 0.00010290545965449318,
      "loss": 1.0756,
      "step": 4083
    },
    {
      "epoch": 0.5741599887529875,
      "grad_norm": 1.518019676208496,
      "learning_rate": 0.00010267308066802941,
      "loss": 1.1607,
      "step": 4084
    },
    {
      "epoch": 0.5743005764093913,
      "grad_norm": 1.3203870058059692,
      "learning_rate": 0.00010244068723569302,
      "loss": 1.2398,
      "step": 4085
    },
    {
      "epoch": 0.574441164065795,
      "grad_norm": 1.4370421171188354,
      "learning_rate": 0.00010220828061338525,
      "loss": 1.1538,
      "step": 4086
    },
    {
      "epoch": 0.5745817517221988,
      "grad_norm": 1.8046213388442993,
      "learning_rate": 0.00010197586205707894,
      "loss": 1.0626,
      "step": 4087
    },
    {
      "epoch": 0.5747223393786025,
      "grad_norm": 1.2604687213897705,
      "learning_rate": 0.00010174343282281134,
      "loss": 1.0449,
      "step": 4088
    },
    {
      "epoch": 0.5748629270350063,
      "grad_norm": 1.3485549688339233,
      "learning_rate": 0.00010151099416667742,
      "loss": 1.169,
      "step": 4089
    },
    {
      "epoch": 0.5750035146914101,
      "grad_norm": 1.2802033424377441,
      "learning_rate": 0.00010127854734482311,
      "loss": 1.2495,
      "step": 4090
    },
    {
      "epoch": 0.5751441023478139,
      "grad_norm": 1.610844373703003,
      "learning_rate": 0.00010104609361343828,
      "loss": 0.8887,
      "step": 4091
    },
    {
      "epoch": 0.5752846900042177,
      "grad_norm": 1.995839238166809,
      "learning_rate": 0.00010081363422875031,
      "loss": 1.0287,
      "step": 4092
    },
    {
      "epoch": 0.5754252776606213,
      "grad_norm": 2.0530014038085938,
      "learning_rate": 0.00010058117044701716,
      "loss": 1.2103,
      "step": 4093
    },
    {
      "epoch": 0.5755658653170251,
      "grad_norm": 1.7005441188812256,
      "learning_rate": 0.00010034870352452055,
      "loss": 1.0509,
      "step": 4094
    },
    {
      "epoch": 0.5757064529734289,
      "grad_norm": 1.5908674001693726,
      "learning_rate": 0.00010011623471755891,
      "loss": 0.9264,
      "step": 4095
    },
    {
      "epoch": 0.5758470406298327,
      "grad_norm": 1.5426108837127686,
      "learning_rate": 9.988376528244117e-05,
      "loss": 1.3266,
      "step": 4096
    },
    {
      "epoch": 0.5759876282862365,
      "grad_norm": 1.809288740158081,
      "learning_rate": 9.965129647547962e-05,
      "loss": 1.0901,
      "step": 4097
    },
    {
      "epoch": 0.5761282159426402,
      "grad_norm": 1.5840275287628174,
      "learning_rate": 9.941882955298291e-05,
      "loss": 1.1056,
      "step": 4098
    },
    {
      "epoch": 0.576268803599044,
      "grad_norm": 1.5523747205734253,
      "learning_rate": 9.918636577124966e-05,
      "loss": 1.161,
      "step": 4099
    },
    {
      "epoch": 0.5764093912554478,
      "grad_norm": 1.6288014650344849,
      "learning_rate": 9.895390638656187e-05,
      "loss": 1.2052,
      "step": 4100
    },
    {
      "epoch": 0.5765499789118516,
      "grad_norm": 1.627454400062561,
      "learning_rate": 9.872145265517695e-05,
      "loss": 1.134,
      "step": 4101
    },
    {
      "epoch": 0.5766905665682553,
      "grad_norm": 1.3928477764129639,
      "learning_rate": 9.848900583332257e-05,
      "loss": 1.1645,
      "step": 4102
    },
    {
      "epoch": 0.576831154224659,
      "grad_norm": 1.5905396938323975,
      "learning_rate": 9.825656717718874e-05,
      "loss": 1.0821,
      "step": 4103
    },
    {
      "epoch": 0.5769717418810628,
      "grad_norm": 1.4843705892562866,
      "learning_rate": 9.802413794292112e-05,
      "loss": 1.0503,
      "step": 4104
    },
    {
      "epoch": 0.5771123295374666,
      "grad_norm": 1.5523444414138794,
      "learning_rate": 9.779171938661491e-05,
      "loss": 0.9415,
      "step": 4105
    },
    {
      "epoch": 0.5772529171938704,
      "grad_norm": 1.7932676076889038,
      "learning_rate": 9.755931276430705e-05,
      "loss": 1.1594,
      "step": 4106
    },
    {
      "epoch": 0.5773935048502742,
      "grad_norm": 1.5670524835586548,
      "learning_rate": 9.732691933197057e-05,
      "loss": 1.1582,
      "step": 4107
    },
    {
      "epoch": 0.5775340925066779,
      "grad_norm": 1.4571954011917114,
      "learning_rate": 9.709454034550697e-05,
      "loss": 1.2033,
      "step": 4108
    },
    {
      "epoch": 0.5776746801630817,
      "grad_norm": 1.6463483572006226,
      "learning_rate": 9.686217706073946e-05,
      "loss": 1.2277,
      "step": 4109
    },
    {
      "epoch": 0.5778152678194854,
      "grad_norm": 1.4486931562423706,
      "learning_rate": 9.662983073340658e-05,
      "loss": 0.9291,
      "step": 4110
    },
    {
      "epoch": 0.5779558554758892,
      "grad_norm": 1.3631370067596436,
      "learning_rate": 9.639750261915564e-05,
      "loss": 1.1951,
      "step": 4111
    },
    {
      "epoch": 0.578096443132293,
      "grad_norm": 1.9223905801773071,
      "learning_rate": 9.616519397353455e-05,
      "loss": 1.0936,
      "step": 4112
    },
    {
      "epoch": 0.5782370307886967,
      "grad_norm": 1.637665867805481,
      "learning_rate": 9.593290605198713e-05,
      "loss": 1.1062,
      "step": 4113
    },
    {
      "epoch": 0.5783776184451005,
      "grad_norm": 1.324052333831787,
      "learning_rate": 9.570064010984413e-05,
      "loss": 1.085,
      "step": 4114
    },
    {
      "epoch": 0.5785182061015043,
      "grad_norm": 1.7659406661987305,
      "learning_rate": 9.546839740231837e-05,
      "loss": 1.1038,
      "step": 4115
    },
    {
      "epoch": 0.5786587937579081,
      "grad_norm": 1.6049100160598755,
      "learning_rate": 9.523617918449678e-05,
      "loss": 1.0342,
      "step": 4116
    },
    {
      "epoch": 0.5787993814143119,
      "grad_norm": 1.6284385919570923,
      "learning_rate": 9.500398671133381e-05,
      "loss": 0.977,
      "step": 4117
    },
    {
      "epoch": 0.5789399690707155,
      "grad_norm": 1.3733383417129517,
      "learning_rate": 9.477182123764488e-05,
      "loss": 1.0997,
      "step": 4118
    },
    {
      "epoch": 0.5790805567271193,
      "grad_norm": 1.642594575881958,
      "learning_rate": 9.453968401809989e-05,
      "loss": 1.1188,
      "step": 4119
    },
    {
      "epoch": 0.5792211443835231,
      "grad_norm": 1.4414639472961426,
      "learning_rate": 9.430757630721516e-05,
      "loss": 1.0948,
      "step": 4120
    },
    {
      "epoch": 0.5793617320399269,
      "grad_norm": 1.6329948902130127,
      "learning_rate": 9.40754993593484e-05,
      "loss": 1.0896,
      "step": 4121
    },
    {
      "epoch": 0.5795023196963307,
      "grad_norm": 1.4446980953216553,
      "learning_rate": 9.384345442869077e-05,
      "loss": 1.2178,
      "step": 4122
    },
    {
      "epoch": 0.5796429073527344,
      "grad_norm": 1.5405209064483643,
      "learning_rate": 9.361144276926019e-05,
      "loss": 1.2474,
      "step": 4123
    },
    {
      "epoch": 0.5797834950091382,
      "grad_norm": 1.4495494365692139,
      "learning_rate": 9.337946563489512e-05,
      "loss": 1.1533,
      "step": 4124
    },
    {
      "epoch": 0.579924082665542,
      "grad_norm": 1.5312670469284058,
      "learning_rate": 9.314752427924703e-05,
      "loss": 1.0668,
      "step": 4125
    },
    {
      "epoch": 0.5800646703219458,
      "grad_norm": 1.2855480909347534,
      "learning_rate": 9.291561995577454e-05,
      "loss": 1.0934,
      "step": 4126
    },
    {
      "epoch": 0.5802052579783495,
      "grad_norm": 2.12532377243042,
      "learning_rate": 9.268375391773588e-05,
      "loss": 1.1042,
      "step": 4127
    },
    {
      "epoch": 0.5803458456347532,
      "grad_norm": 1.3586033582687378,
      "learning_rate": 9.245192741818221e-05,
      "loss": 1.163,
      "step": 4128
    },
    {
      "epoch": 0.580486433291157,
      "grad_norm": 1.3961461782455444,
      "learning_rate": 9.222014170995117e-05,
      "loss": 1.074,
      "step": 4129
    },
    {
      "epoch": 0.5806270209475608,
      "grad_norm": 1.438223123550415,
      "learning_rate": 9.198839804566034e-05,
      "loss": 1.2003,
      "step": 4130
    },
    {
      "epoch": 0.5807676086039646,
      "grad_norm": 1.408590316772461,
      "learning_rate": 9.175669767769918e-05,
      "loss": 1.1695,
      "step": 4131
    },
    {
      "epoch": 0.5809081962603684,
      "grad_norm": 1.4102329015731812,
      "learning_rate": 9.152504185822423e-05,
      "loss": 1.1762,
      "step": 4132
    },
    {
      "epoch": 0.5810487839167721,
      "grad_norm": 1.578930139541626,
      "learning_rate": 9.129343183915025e-05,
      "loss": 0.8909,
      "step": 4133
    },
    {
      "epoch": 0.5811893715731759,
      "grad_norm": 1.9491959810256958,
      "learning_rate": 9.106186887214532e-05,
      "loss": 1.0047,
      "step": 4134
    },
    {
      "epoch": 0.5813299592295796,
      "grad_norm": 1.7259621620178223,
      "learning_rate": 9.083035420862292e-05,
      "loss": 1.0757,
      "step": 4135
    },
    {
      "epoch": 0.5814705468859834,
      "grad_norm": 1.429060697555542,
      "learning_rate": 9.05988890997353e-05,
      "loss": 1.0324,
      "step": 4136
    },
    {
      "epoch": 0.5816111345423872,
      "grad_norm": 1.6342957019805908,
      "learning_rate": 9.03674747963671e-05,
      "loss": 1.2348,
      "step": 4137
    },
    {
      "epoch": 0.5817517221987909,
      "grad_norm": 1.7282531261444092,
      "learning_rate": 9.013611254912871e-05,
      "loss": 1.1168,
      "step": 4138
    },
    {
      "epoch": 0.5818923098551947,
      "grad_norm": 1.4712570905685425,
      "learning_rate": 8.990480360834838e-05,
      "loss": 1.1137,
      "step": 4139
    },
    {
      "epoch": 0.5820328975115985,
      "grad_norm": 1.3941638469696045,
      "learning_rate": 8.967354922406727e-05,
      "loss": 1.0554,
      "step": 4140
    },
    {
      "epoch": 0.5821734851680023,
      "grad_norm": 2.155773162841797,
      "learning_rate": 8.94423506460307e-05,
      "loss": 1.2755,
      "step": 4141
    },
    {
      "epoch": 0.5823140728244061,
      "grad_norm": 1.3591934442520142,
      "learning_rate": 8.921120912368326e-05,
      "loss": 1.0909,
      "step": 4142
    },
    {
      "epoch": 0.5824546604808097,
      "grad_norm": 1.3323320150375366,
      "learning_rate": 8.898012590616083e-05,
      "loss": 0.9737,
      "step": 4143
    },
    {
      "epoch": 0.5825952481372135,
      "grad_norm": 1.5060689449310303,
      "learning_rate": 8.874910224228407e-05,
      "loss": 0.9405,
      "step": 4144
    },
    {
      "epoch": 0.5827358357936173,
      "grad_norm": 1.3919918537139893,
      "learning_rate": 8.851813938055192e-05,
      "loss": 1.2147,
      "step": 4145
    },
    {
      "epoch": 0.5828764234500211,
      "grad_norm": 1.5048372745513916,
      "learning_rate": 8.828723856913505e-05,
      "loss": 1.3283,
      "step": 4146
    },
    {
      "epoch": 0.5830170111064249,
      "grad_norm": 1.612302303314209,
      "learning_rate": 8.805640105586823e-05,
      "loss": 0.9325,
      "step": 4147
    },
    {
      "epoch": 0.5831575987628286,
      "grad_norm": 1.6926614046096802,
      "learning_rate": 8.782562808824462e-05,
      "loss": 1.209,
      "step": 4148
    },
    {
      "epoch": 0.5832981864192324,
      "grad_norm": 1.5206537246704102,
      "learning_rate": 8.759492091340817e-05,
      "loss": 1.032,
      "step": 4149
    },
    {
      "epoch": 0.5834387740756362,
      "grad_norm": 1.4960602521896362,
      "learning_rate": 8.736428077814752e-05,
      "loss": 1.0302,
      "step": 4150
    },
    {
      "epoch": 0.58357936173204,
      "grad_norm": 1.4819774627685547,
      "learning_rate": 8.71337089288893e-05,
      "loss": 1.1802,
      "step": 4151
    },
    {
      "epoch": 0.5837199493884437,
      "grad_norm": 1.4674115180969238,
      "learning_rate": 8.690320661169024e-05,
      "loss": 0.9929,
      "step": 4152
    },
    {
      "epoch": 0.5838605370448474,
      "grad_norm": 1.745369791984558,
      "learning_rate": 8.667277507223222e-05,
      "loss": 1.0825,
      "step": 4153
    },
    {
      "epoch": 0.5840011247012512,
      "grad_norm": 1.622039794921875,
      "learning_rate": 8.644241555581424e-05,
      "loss": 1.1253,
      "step": 4154
    },
    {
      "epoch": 0.584141712357655,
      "grad_norm": 1.5848309993743896,
      "learning_rate": 8.621212930734591e-05,
      "loss": 1.1849,
      "step": 4155
    },
    {
      "epoch": 0.5842823000140588,
      "grad_norm": 1.486699104309082,
      "learning_rate": 8.598191757134104e-05,
      "loss": 1.173,
      "step": 4156
    },
    {
      "epoch": 0.5844228876704626,
      "grad_norm": 1.4568766355514526,
      "learning_rate": 8.575178159191113e-05,
      "loss": 1.019,
      "step": 4157
    },
    {
      "epoch": 0.5845634753268663,
      "grad_norm": 1.5060558319091797,
      "learning_rate": 8.552172261275739e-05,
      "loss": 1.1796,
      "step": 4158
    },
    {
      "epoch": 0.58470406298327,
      "grad_norm": 1.4735040664672852,
      "learning_rate": 8.52917418771659e-05,
      "loss": 1.181,
      "step": 4159
    },
    {
      "epoch": 0.5848446506396738,
      "grad_norm": 1.4132533073425293,
      "learning_rate": 8.506184062799914e-05,
      "loss": 1.1975,
      "step": 4160
    },
    {
      "epoch": 0.5849852382960776,
      "grad_norm": 1.7201894521713257,
      "learning_rate": 8.483202010769042e-05,
      "loss": 1.0339,
      "step": 4161
    },
    {
      "epoch": 0.5851258259524814,
      "grad_norm": 1.7111510038375854,
      "learning_rate": 8.460228155823684e-05,
      "loss": 1.1094,
      "step": 4162
    },
    {
      "epoch": 0.5852664136088851,
      "grad_norm": 1.7137900590896606,
      "learning_rate": 8.437262622119183e-05,
      "loss": 1.046,
      "step": 4163
    },
    {
      "epoch": 0.5854070012652889,
      "grad_norm": 1.4121006727218628,
      "learning_rate": 8.414305533766016e-05,
      "loss": 1.1409,
      "step": 4164
    },
    {
      "epoch": 0.5855475889216927,
      "grad_norm": 1.4923151731491089,
      "learning_rate": 8.391357014828971e-05,
      "loss": 1.2139,
      "step": 4165
    },
    {
      "epoch": 0.5856881765780965,
      "grad_norm": 1.6963143348693848,
      "learning_rate": 8.368417189326466e-05,
      "loss": 1.0116,
      "step": 4166
    },
    {
      "epoch": 0.5858287642345003,
      "grad_norm": 1.4766533374786377,
      "learning_rate": 8.345486181230062e-05,
      "loss": 1.3042,
      "step": 4167
    },
    {
      "epoch": 0.5859693518909039,
      "grad_norm": 1.6993334293365479,
      "learning_rate": 8.322564114463554e-05,
      "loss": 1.1781,
      "step": 4168
    },
    {
      "epoch": 0.5861099395473077,
      "grad_norm": 1.5286372900009155,
      "learning_rate": 8.299651112902476e-05,
      "loss": 1.0995,
      "step": 4169
    },
    {
      "epoch": 0.5862505272037115,
      "grad_norm": 1.6968472003936768,
      "learning_rate": 8.276747300373358e-05,
      "loss": 0.9882,
      "step": 4170
    },
    {
      "epoch": 0.5863911148601153,
      "grad_norm": 1.577603816986084,
      "learning_rate": 8.253852800653075e-05,
      "loss": 1.1235,
      "step": 4171
    },
    {
      "epoch": 0.5865317025165191,
      "grad_norm": 1.4103081226348877,
      "learning_rate": 8.230967737468168e-05,
      "loss": 1.0779,
      "step": 4172
    },
    {
      "epoch": 0.5866722901729228,
      "grad_norm": 1.3782298564910889,
      "learning_rate": 8.2080922344942e-05,
      "loss": 0.9131,
      "step": 4173
    },
    {
      "epoch": 0.5868128778293266,
      "grad_norm": 1.3994282484054565,
      "learning_rate": 8.185226415355e-05,
      "loss": 1.183,
      "step": 4174
    },
    {
      "epoch": 0.5869534654857304,
      "grad_norm": 1.3962469100952148,
      "learning_rate": 8.162370403622187e-05,
      "loss": 1.1618,
      "step": 4175
    },
    {
      "epoch": 0.5870940531421341,
      "grad_norm": 1.8983590602874756,
      "learning_rate": 8.139524322814252e-05,
      "loss": 1.2075,
      "step": 4176
    },
    {
      "epoch": 0.5872346407985379,
      "grad_norm": 1.3825422525405884,
      "learning_rate": 8.116688296396089e-05,
      "loss": 0.9856,
      "step": 4177
    },
    {
      "epoch": 0.5873752284549416,
      "grad_norm": 1.5633920431137085,
      "learning_rate": 8.093862447778242e-05,
      "loss": 0.9431,
      "step": 4178
    },
    {
      "epoch": 0.5875158161113454,
      "grad_norm": 1.4687788486480713,
      "learning_rate": 8.071046900316248e-05,
      "loss": 1.0768,
      "step": 4179
    },
    {
      "epoch": 0.5876564037677492,
      "grad_norm": 1.5420085191726685,
      "learning_rate": 8.048241777309977e-05,
      "loss": 1.0422,
      "step": 4180
    },
    {
      "epoch": 0.587796991424153,
      "grad_norm": 1.728670597076416,
      "learning_rate": 8.025447202002979e-05,
      "loss": 1.1709,
      "step": 4181
    },
    {
      "epoch": 0.5879375790805568,
      "grad_norm": 1.5436347723007202,
      "learning_rate": 8.00266329758173e-05,
      "loss": 1.1001,
      "step": 4182
    },
    {
      "epoch": 0.5880781667369605,
      "grad_norm": 1.4828145503997803,
      "learning_rate": 7.979890187175167e-05,
      "loss": 1.2927,
      "step": 4183
    },
    {
      "epoch": 0.5882187543933642,
      "grad_norm": 1.6836767196655273,
      "learning_rate": 7.957127993853768e-05,
      "loss": 1.15,
      "step": 4184
    },
    {
      "epoch": 0.588359342049768,
      "grad_norm": 1.7820947170257568,
      "learning_rate": 7.934376840629063e-05,
      "loss": 1.0307,
      "step": 4185
    },
    {
      "epoch": 0.5884999297061718,
      "grad_norm": 1.8901464939117432,
      "learning_rate": 7.911636850452975e-05,
      "loss": 1.098,
      "step": 4186
    },
    {
      "epoch": 0.5886405173625756,
      "grad_norm": 1.4528756141662598,
      "learning_rate": 7.888908146216993e-05,
      "loss": 1.2327,
      "step": 4187
    },
    {
      "epoch": 0.5887811050189793,
      "grad_norm": 1.5124706029891968,
      "learning_rate": 7.866190850751685e-05,
      "loss": 1.2133,
      "step": 4188
    },
    {
      "epoch": 0.5889216926753831,
      "grad_norm": 1.548366904258728,
      "learning_rate": 7.843485086825946e-05,
      "loss": 0.9843,
      "step": 4189
    },
    {
      "epoch": 0.5890622803317869,
      "grad_norm": 1.487217903137207,
      "learning_rate": 7.820790977146353e-05,
      "loss": 1.0401,
      "step": 4190
    },
    {
      "epoch": 0.5892028679881907,
      "grad_norm": 1.6446367502212524,
      "learning_rate": 7.798108644356517e-05,
      "loss": 1.1581,
      "step": 4191
    },
    {
      "epoch": 0.5893434556445944,
      "grad_norm": 1.5782257318496704,
      "learning_rate": 7.775438211036353e-05,
      "loss": 1.2773,
      "step": 4192
    },
    {
      "epoch": 0.5894840433009981,
      "grad_norm": 1.4850808382034302,
      "learning_rate": 7.752779799701504e-05,
      "loss": 1.2556,
      "step": 4193
    },
    {
      "epoch": 0.5896246309574019,
      "grad_norm": 1.48939847946167,
      "learning_rate": 7.730133532802695e-05,
      "loss": 1.0362,
      "step": 4194
    },
    {
      "epoch": 0.5897652186138057,
      "grad_norm": 1.4075407981872559,
      "learning_rate": 7.707499532724916e-05,
      "loss": 1.1842,
      "step": 4195
    },
    {
      "epoch": 0.5899058062702095,
      "grad_norm": 1.3069419860839844,
      "learning_rate": 7.684877921786933e-05,
      "loss": 1.0605,
      "step": 4196
    },
    {
      "epoch": 0.5900463939266133,
      "grad_norm": 1.3781691789627075,
      "learning_rate": 7.662268822240544e-05,
      "loss": 1.0563,
      "step": 4197
    },
    {
      "epoch": 0.590186981583017,
      "grad_norm": 1.4723843336105347,
      "learning_rate": 7.639672356269932e-05,
      "loss": 1.2222,
      "step": 4198
    },
    {
      "epoch": 0.5903275692394208,
      "grad_norm": 1.4538120031356812,
      "learning_rate": 7.617088645991004e-05,
      "loss": 1.2281,
      "step": 4199
    },
    {
      "epoch": 0.5904681568958245,
      "grad_norm": 1.5857374668121338,
      "learning_rate": 7.594517813450749e-05,
      "loss": 1.3078,
      "step": 4200
    },
    {
      "epoch": 0.5906087445522283,
      "grad_norm": 1.321958303451538,
      "learning_rate": 7.571959980626497e-05,
      "loss": 1.0851,
      "step": 4201
    },
    {
      "epoch": 0.5907493322086321,
      "grad_norm": 1.4515509605407715,
      "learning_rate": 7.549415269425444e-05,
      "loss": 1.2119,
      "step": 4202
    },
    {
      "epoch": 0.5908899198650358,
      "grad_norm": 1.2492576837539673,
      "learning_rate": 7.526883801683751e-05,
      "loss": 1.2103,
      "step": 4203
    },
    {
      "epoch": 0.5910305075214396,
      "grad_norm": 1.4944450855255127,
      "learning_rate": 7.504365699166061e-05,
      "loss": 1.2808,
      "step": 4204
    },
    {
      "epoch": 0.5911710951778434,
      "grad_norm": 1.5426299571990967,
      "learning_rate": 7.481861083564845e-05,
      "loss": 1.0778,
      "step": 4205
    },
    {
      "epoch": 0.5913116828342472,
      "grad_norm": 1.504692792892456,
      "learning_rate": 7.459370076499579e-05,
      "loss": 1.2755,
      "step": 4206
    },
    {
      "epoch": 0.591452270490651,
      "grad_norm": 1.4401805400848389,
      "learning_rate": 7.436892799516268e-05,
      "loss": 1.1253,
      "step": 4207
    },
    {
      "epoch": 0.5915928581470546,
      "grad_norm": 1.610812783241272,
      "learning_rate": 7.414429374086697e-05,
      "loss": 1.2199,
      "step": 4208
    },
    {
      "epoch": 0.5917334458034584,
      "grad_norm": 1.4524190425872803,
      "learning_rate": 7.391979921607794e-05,
      "loss": 1.0122,
      "step": 4209
    },
    {
      "epoch": 0.5918740334598622,
      "grad_norm": 1.5610370635986328,
      "learning_rate": 7.369544563400998e-05,
      "loss": 1.0609,
      "step": 4210
    },
    {
      "epoch": 0.592014621116266,
      "grad_norm": 1.5946533679962158,
      "learning_rate": 7.34712342071152e-05,
      "loss": 1.1239,
      "step": 4211
    },
    {
      "epoch": 0.5921552087726698,
      "grad_norm": 1.3448432683944702,
      "learning_rate": 7.324716614707782e-05,
      "loss": 1.0841,
      "step": 4212
    },
    {
      "epoch": 0.5922957964290735,
      "grad_norm": 1.3389289379119873,
      "learning_rate": 7.30232426648079e-05,
      "loss": 1.2661,
      "step": 4213
    },
    {
      "epoch": 0.5924363840854773,
      "grad_norm": 1.8335050344467163,
      "learning_rate": 7.279946497043301e-05,
      "loss": 1.2074,
      "step": 4214
    },
    {
      "epoch": 0.5925769717418811,
      "grad_norm": 1.3863948583602905,
      "learning_rate": 7.25758342732936e-05,
      "loss": 0.9329,
      "step": 4215
    },
    {
      "epoch": 0.5927175593982849,
      "grad_norm": 1.6306954622268677,
      "learning_rate": 7.235235178193556e-05,
      "loss": 1.0845,
      "step": 4216
    },
    {
      "epoch": 0.5928581470546886,
      "grad_norm": 1.680195689201355,
      "learning_rate": 7.21290187041038e-05,
      "loss": 1.0328,
      "step": 4217
    },
    {
      "epoch": 0.5929987347110923,
      "grad_norm": 1.634758710861206,
      "learning_rate": 7.190583624673597e-05,
      "loss": 1.0657,
      "step": 4218
    },
    {
      "epoch": 0.5931393223674961,
      "grad_norm": 1.4600855112075806,
      "learning_rate": 7.168280561595524e-05,
      "loss": 1.2085,
      "step": 4219
    },
    {
      "epoch": 0.5932799100238999,
      "grad_norm": 1.551268458366394,
      "learning_rate": 7.145992801706456e-05,
      "loss": 1.1168,
      "step": 4220
    },
    {
      "epoch": 0.5934204976803037,
      "grad_norm": 1.5092949867248535,
      "learning_rate": 7.123720465454046e-05,
      "loss": 1.0605,
      "step": 4221
    },
    {
      "epoch": 0.5935610853367075,
      "grad_norm": 1.298520565032959,
      "learning_rate": 7.101463673202485e-05,
      "loss": 1.2685,
      "step": 4222
    },
    {
      "epoch": 0.5937016729931112,
      "grad_norm": 1.4333882331848145,
      "learning_rate": 7.079222545232014e-05,
      "loss": 1.1297,
      "step": 4223
    },
    {
      "epoch": 0.593842260649515,
      "grad_norm": 1.3803248405456543,
      "learning_rate": 7.056997201738274e-05,
      "loss": 1.0207,
      "step": 4224
    },
    {
      "epoch": 0.5939828483059187,
      "grad_norm": 1.6230905055999756,
      "learning_rate": 7.034787762831501e-05,
      "loss": 1.2458,
      "step": 4225
    },
    {
      "epoch": 0.5941234359623225,
      "grad_norm": 1.5969089269638062,
      "learning_rate": 7.012594348536064e-05,
      "loss": 1.0949,
      "step": 4226
    },
    {
      "epoch": 0.5942640236187263,
      "grad_norm": 1.8327897787094116,
      "learning_rate": 6.990417078789645e-05,
      "loss": 1.1725,
      "step": 4227
    },
    {
      "epoch": 0.59440461127513,
      "grad_norm": 1.5417488813400269,
      "learning_rate": 6.968256073442782e-05,
      "loss": 1.0514,
      "step": 4228
    },
    {
      "epoch": 0.5945451989315338,
      "grad_norm": 1.3674736022949219,
      "learning_rate": 6.946111452258065e-05,
      "loss": 1.16,
      "step": 4229
    },
    {
      "epoch": 0.5946857865879376,
      "grad_norm": 1.4234743118286133,
      "learning_rate": 6.923983334909505e-05,
      "loss": 1.0813,
      "step": 4230
    },
    {
      "epoch": 0.5948263742443414,
      "grad_norm": 1.443596601486206,
      "learning_rate": 6.90187184098196e-05,
      "loss": 1.0994,
      "step": 4231
    },
    {
      "epoch": 0.5949669619007452,
      "grad_norm": 1.4384586811065674,
      "learning_rate": 6.879777089970499e-05,
      "loss": 1.1105,
      "step": 4232
    },
    {
      "epoch": 0.5951075495571488,
      "grad_norm": 1.5228984355926514,
      "learning_rate": 6.857699201279613e-05,
      "loss": 1.0453,
      "step": 4233
    },
    {
      "epoch": 0.5952481372135526,
      "grad_norm": 1.6803926229476929,
      "learning_rate": 6.835638294222727e-05,
      "loss": 1.1505,
      "step": 4234
    },
    {
      "epoch": 0.5953887248699564,
      "grad_norm": 1.5883737802505493,
      "learning_rate": 6.813594488021477e-05,
      "loss": 1.2517,
      "step": 4235
    },
    {
      "epoch": 0.5955293125263602,
      "grad_norm": 1.278574824333191,
      "learning_rate": 6.79156790180509e-05,
      "loss": 1.0957,
      "step": 4236
    },
    {
      "epoch": 0.595669900182764,
      "grad_norm": 1.339820146560669,
      "learning_rate": 6.76955865460974e-05,
      "loss": 1.0507,
      "step": 4237
    },
    {
      "epoch": 0.5958104878391677,
      "grad_norm": 1.5273261070251465,
      "learning_rate": 6.747566865377852e-05,
      "loss": 1.0611,
      "step": 4238
    },
    {
      "epoch": 0.5959510754955715,
      "grad_norm": 1.330202341079712,
      "learning_rate": 6.725592652957541e-05,
      "loss": 1.1236,
      "step": 4239
    },
    {
      "epoch": 0.5960916631519753,
      "grad_norm": 1.4519484043121338,
      "learning_rate": 6.703636136101976e-05,
      "loss": 1.163,
      "step": 4240
    },
    {
      "epoch": 0.596232250808379,
      "grad_norm": 1.3539050817489624,
      "learning_rate": 6.681697433468602e-05,
      "loss": 1.0255,
      "step": 4241
    },
    {
      "epoch": 0.5963728384647828,
      "grad_norm": 1.6199321746826172,
      "learning_rate": 6.659776663618654e-05,
      "loss": 1.0746,
      "step": 4242
    },
    {
      "epoch": 0.5965134261211865,
      "grad_norm": 1.912966251373291,
      "learning_rate": 6.637873945016442e-05,
      "loss": 1.1574,
      "step": 4243
    },
    {
      "epoch": 0.5966540137775903,
      "grad_norm": 1.4175331592559814,
      "learning_rate": 6.615989396028721e-05,
      "loss": 1.3179,
      "step": 4244
    },
    {
      "epoch": 0.5967946014339941,
      "grad_norm": 1.4059772491455078,
      "learning_rate": 6.59412313492407e-05,
      "loss": 1.0878,
      "step": 4245
    },
    {
      "epoch": 0.5969351890903979,
      "grad_norm": 1.4151118993759155,
      "learning_rate": 6.572275279872175e-05,
      "loss": 0.9676,
      "step": 4246
    },
    {
      "epoch": 0.5970757767468017,
      "grad_norm": 1.2971704006195068,
      "learning_rate": 6.550445948943343e-05,
      "loss": 1.2852,
      "step": 4247
    },
    {
      "epoch": 0.5972163644032054,
      "grad_norm": 1.711425542831421,
      "learning_rate": 6.52863526010773e-05,
      "loss": 1.2415,
      "step": 4248
    },
    {
      "epoch": 0.5973569520596091,
      "grad_norm": 1.4828789234161377,
      "learning_rate": 6.506843331234711e-05,
      "loss": 1.0048,
      "step": 4249
    },
    {
      "epoch": 0.5974975397160129,
      "grad_norm": 1.6645047664642334,
      "learning_rate": 6.485070280092311e-05,
      "loss": 1.1059,
      "step": 4250
    },
    {
      "epoch": 0.5976381273724167,
      "grad_norm": 1.4339182376861572,
      "learning_rate": 6.463316224346584e-05,
      "loss": 0.9151,
      "step": 4251
    },
    {
      "epoch": 0.5977787150288205,
      "grad_norm": 1.2656444311141968,
      "learning_rate": 6.441581281560836e-05,
      "loss": 1.2756,
      "step": 4252
    },
    {
      "epoch": 0.5979193026852242,
      "grad_norm": 1.4918078184127808,
      "learning_rate": 6.419865569195158e-05,
      "loss": 0.9547,
      "step": 4253
    },
    {
      "epoch": 0.598059890341628,
      "grad_norm": 1.5096545219421387,
      "learning_rate": 6.398169204605636e-05,
      "loss": 1.0977,
      "step": 4254
    },
    {
      "epoch": 0.5982004779980318,
      "grad_norm": 1.4800612926483154,
      "learning_rate": 6.376492305043898e-05,
      "loss": 0.9854,
      "step": 4255
    },
    {
      "epoch": 0.5983410656544356,
      "grad_norm": 1.5839406251907349,
      "learning_rate": 6.354834987656326e-05,
      "loss": 1.0785,
      "step": 4256
    },
    {
      "epoch": 0.5984816533108394,
      "grad_norm": 1.3532739877700806,
      "learning_rate": 6.333197369483446e-05,
      "loss": 1.1667,
      "step": 4257
    },
    {
      "epoch": 0.598622240967243,
      "grad_norm": 1.7125253677368164,
      "learning_rate": 6.311579567459356e-05,
      "loss": 1.1104,
      "step": 4258
    },
    {
      "epoch": 0.5987628286236468,
      "grad_norm": 1.6467697620391846,
      "learning_rate": 6.289981698411111e-05,
      "loss": 0.9622,
      "step": 4259
    },
    {
      "epoch": 0.5989034162800506,
      "grad_norm": 1.2817507982254028,
      "learning_rate": 6.268403879057955e-05,
      "loss": 1.1612,
      "step": 4260
    },
    {
      "epoch": 0.5990440039364544,
      "grad_norm": 1.6554582118988037,
      "learning_rate": 6.246846226010832e-05,
      "loss": 1.1409,
      "step": 4261
    },
    {
      "epoch": 0.5991845915928582,
      "grad_norm": 3.393735885620117,
      "learning_rate": 6.225308855771696e-05,
      "loss": 1.1174,
      "step": 4262
    },
    {
      "epoch": 0.5993251792492619,
      "grad_norm": 1.4198832511901855,
      "learning_rate": 6.203791884732885e-05,
      "loss": 0.9806,
      "step": 4263
    },
    {
      "epoch": 0.5994657669056657,
      "grad_norm": 1.4474982023239136,
      "learning_rate": 6.182295429176512e-05,
      "loss": 1.1521,
      "step": 4264
    },
    {
      "epoch": 0.5996063545620695,
      "grad_norm": 1.3669469356536865,
      "learning_rate": 6.160819605273757e-05,
      "loss": 1.1952,
      "step": 4265
    },
    {
      "epoch": 0.5997469422184732,
      "grad_norm": 1.734615445137024,
      "learning_rate": 6.139364529084397e-05,
      "loss": 1.2274,
      "step": 4266
    },
    {
      "epoch": 0.599887529874877,
      "grad_norm": 1.590633511543274,
      "learning_rate": 6.117930316556038e-05,
      "loss": 1.0488,
      "step": 4267
    },
    {
      "epoch": 0.6000281175312807,
      "grad_norm": 1.8271090984344482,
      "learning_rate": 6.0965170835235006e-05,
      "loss": 1.1787,
      "step": 4268
    },
    {
      "epoch": 0.6001687051876845,
      "grad_norm": 1.7865246534347534,
      "learning_rate": 6.075124945708277e-05,
      "loss": 0.9497,
      "step": 4269
    },
    {
      "epoch": 0.6003092928440883,
      "grad_norm": 1.5280213356018066,
      "learning_rate": 6.053754018717839e-05,
      "loss": 0.8985,
      "step": 4270
    },
    {
      "epoch": 0.6004498805004921,
      "grad_norm": 1.6126213073730469,
      "learning_rate": 6.0324044180450335e-05,
      "loss": 0.9155,
      "step": 4271
    },
    {
      "epoch": 0.6005904681568959,
      "grad_norm": 1.3436758518218994,
      "learning_rate": 6.0110762590674695e-05,
      "loss": 1.3437,
      "step": 4272
    },
    {
      "epoch": 0.6007310558132996,
      "grad_norm": 1.6500016450881958,
      "learning_rate": 5.9897696570468175e-05,
      "loss": 1.2177,
      "step": 4273
    },
    {
      "epoch": 0.6008716434697033,
      "grad_norm": 1.4086854457855225,
      "learning_rate": 5.9684847271283454e-05,
      "loss": 1.0449,
      "step": 4274
    },
    {
      "epoch": 0.6010122311261071,
      "grad_norm": 1.70358145236969,
      "learning_rate": 5.9472215843401524e-05,
      "loss": 1.065,
      "step": 4275
    },
    {
      "epoch": 0.6011528187825109,
      "grad_norm": 1.5281661748886108,
      "learning_rate": 5.925980343592566e-05,
      "loss": 1.0655,
      "step": 4276
    },
    {
      "epoch": 0.6012934064389147,
      "grad_norm": 1.5017807483673096,
      "learning_rate": 5.90476111967758e-05,
      "loss": 1.1654,
      "step": 4277
    },
    {
      "epoch": 0.6014339940953184,
      "grad_norm": 1.312555193901062,
      "learning_rate": 5.883564027268251e-05,
      "loss": 1.0035,
      "step": 4278
    },
    {
      "epoch": 0.6015745817517222,
      "grad_norm": 1.4056788682937622,
      "learning_rate": 5.862389180917942e-05,
      "loss": 1.1789,
      "step": 4279
    },
    {
      "epoch": 0.601715169408126,
      "grad_norm": 1.2841614484786987,
      "learning_rate": 5.841236695059855e-05,
      "loss": 1.1538,
      "step": 4280
    },
    {
      "epoch": 0.6018557570645298,
      "grad_norm": 1.3331403732299805,
      "learning_rate": 5.8201066840063324e-05,
      "loss": 1.0438,
      "step": 4281
    },
    {
      "epoch": 0.6019963447209336,
      "grad_norm": 1.2809027433395386,
      "learning_rate": 5.798999261948261e-05,
      "loss": 1.229,
      "step": 4282
    },
    {
      "epoch": 0.6021369323773372,
      "grad_norm": 1.3463876247406006,
      "learning_rate": 5.77791454295447e-05,
      "loss": 1.0916,
      "step": 4283
    },
    {
      "epoch": 0.602277520033741,
      "grad_norm": 1.333112120628357,
      "learning_rate": 5.75685264097103e-05,
      "loss": 1.0738,
      "step": 4284
    },
    {
      "epoch": 0.6024181076901448,
      "grad_norm": 1.573660135269165,
      "learning_rate": 5.7358136698207985e-05,
      "loss": 1.1672,
      "step": 4285
    },
    {
      "epoch": 0.6025586953465486,
      "grad_norm": 1.2619086503982544,
      "learning_rate": 5.7147977432026654e-05,
      "loss": 1.0527,
      "step": 4286
    },
    {
      "epoch": 0.6026992830029524,
      "grad_norm": 1.3653135299682617,
      "learning_rate": 5.6938049746909496e-05,
      "loss": 1.182,
      "step": 4287
    },
    {
      "epoch": 0.6028398706593561,
      "grad_norm": 1.4287258386611938,
      "learning_rate": 5.6728354777348656e-05,
      "loss": 1.2136,
      "step": 4288
    },
    {
      "epoch": 0.6029804583157599,
      "grad_norm": 1.5081720352172852,
      "learning_rate": 5.651889365657852e-05,
      "loss": 1.0803,
      "step": 4289
    },
    {
      "epoch": 0.6031210459721636,
      "grad_norm": 1.4072084426879883,
      "learning_rate": 5.6309667516569653e-05,
      "loss": 1.1081,
      "step": 4290
    },
    {
      "epoch": 0.6032616336285674,
      "grad_norm": 1.4997000694274902,
      "learning_rate": 5.6100677488022925e-05,
      "loss": 1.18,
      "step": 4291
    },
    {
      "epoch": 0.6034022212849712,
      "grad_norm": 1.32863450050354,
      "learning_rate": 5.589192470036258e-05,
      "loss": 1.1851,
      "step": 4292
    },
    {
      "epoch": 0.6035428089413749,
      "grad_norm": 1.9259086847305298,
      "learning_rate": 5.568341028173169e-05,
      "loss": 0.9879,
      "step": 4293
    },
    {
      "epoch": 0.6036833965977787,
      "grad_norm": 1.9060866832733154,
      "learning_rate": 5.547513535898471e-05,
      "loss": 1.2086,
      "step": 4294
    },
    {
      "epoch": 0.6038239842541825,
      "grad_norm": 1.3560576438903809,
      "learning_rate": 5.526710105768143e-05,
      "loss": 1.0508,
      "step": 4295
    },
    {
      "epoch": 0.6039645719105863,
      "grad_norm": 1.6633834838867188,
      "learning_rate": 5.5059308502081766e-05,
      "loss": 1.1788,
      "step": 4296
    },
    {
      "epoch": 0.6041051595669901,
      "grad_norm": 1.2527902126312256,
      "learning_rate": 5.485175881513907e-05,
      "loss": 1.1101,
      "step": 4297
    },
    {
      "epoch": 0.6042457472233937,
      "grad_norm": 1.5427803993225098,
      "learning_rate": 5.464445311849412e-05,
      "loss": 1.2126,
      "step": 4298
    },
    {
      "epoch": 0.6043863348797975,
      "grad_norm": 1.3695780038833618,
      "learning_rate": 5.4437392532469346e-05,
      "loss": 1.1707,
      "step": 4299
    },
    {
      "epoch": 0.6045269225362013,
      "grad_norm": 1.4725677967071533,
      "learning_rate": 5.423057817606187e-05,
      "loss": 1.0278,
      "step": 4300
    },
    {
      "epoch": 0.6046675101926051,
      "grad_norm": 1.4721102714538574,
      "learning_rate": 5.4024011166939115e-05,
      "loss": 1.2705,
      "step": 4301
    },
    {
      "epoch": 0.6048080978490089,
      "grad_norm": 1.4243910312652588,
      "learning_rate": 5.381769262143128e-05,
      "loss": 1.1247,
      "step": 4302
    },
    {
      "epoch": 0.6049486855054126,
      "grad_norm": 1.537965178489685,
      "learning_rate": 5.361162365452538e-05,
      "loss": 1.1777,
      "step": 4303
    },
    {
      "epoch": 0.6050892731618164,
      "grad_norm": 1.5104551315307617,
      "learning_rate": 5.340580537986074e-05,
      "loss": 1.1311,
      "step": 4304
    },
    {
      "epoch": 0.6052298608182202,
      "grad_norm": 1.5846277475357056,
      "learning_rate": 5.320023890972079e-05,
      "loss": 1.1517,
      "step": 4305
    },
    {
      "epoch": 0.605370448474624,
      "grad_norm": 1.3748670816421509,
      "learning_rate": 5.299492535502881e-05,
      "loss": 1.0802,
      "step": 4306
    },
    {
      "epoch": 0.6055110361310277,
      "grad_norm": 1.4671114683151245,
      "learning_rate": 5.278986582534109e-05,
      "loss": 1.0319,
      "step": 4307
    },
    {
      "epoch": 0.6056516237874314,
      "grad_norm": 1.4248442649841309,
      "learning_rate": 5.258506142884111e-05,
      "loss": 1.1191,
      "step": 4308
    },
    {
      "epoch": 0.6057922114438352,
      "grad_norm": 1.5168888568878174,
      "learning_rate": 5.2380513272333595e-05,
      "loss": 1.1237,
      "step": 4309
    },
    {
      "epoch": 0.605932799100239,
      "grad_norm": 1.2091078758239746,
      "learning_rate": 5.217622246123861e-05,
      "loss": 1.0993,
      "step": 4310
    },
    {
      "epoch": 0.6060733867566428,
      "grad_norm": 1.4522743225097656,
      "learning_rate": 5.197219009958488e-05,
      "loss": 0.9898,
      "step": 4311
    },
    {
      "epoch": 0.6062139744130466,
      "grad_norm": 1.5619193315505981,
      "learning_rate": 5.1768417290005355e-05,
      "loss": 1.0768,
      "step": 4312
    },
    {
      "epoch": 0.6063545620694503,
      "grad_norm": 1.407865047454834,
      "learning_rate": 5.156490513372981e-05,
      "loss": 1.1652,
      "step": 4313
    },
    {
      "epoch": 0.606495149725854,
      "grad_norm": 1.693955898284912,
      "learning_rate": 5.1361654730579125e-05,
      "loss": 1.1942,
      "step": 4314
    },
    {
      "epoch": 0.6066357373822578,
      "grad_norm": 1.4608654975891113,
      "learning_rate": 5.115866717896005e-05,
      "loss": 1.0096,
      "step": 4315
    },
    {
      "epoch": 0.6067763250386616,
      "grad_norm": 1.3502553701400757,
      "learning_rate": 5.095594357585869e-05,
      "loss": 1.1782,
      "step": 4316
    },
    {
      "epoch": 0.6069169126950654,
      "grad_norm": 1.8047771453857422,
      "learning_rate": 5.0753485016834726e-05,
      "loss": 1.0787,
      "step": 4317
    },
    {
      "epoch": 0.6070575003514691,
      "grad_norm": 1.4042876958847046,
      "learning_rate": 5.055129259601562e-05,
      "loss": 1.1699,
      "step": 4318
    },
    {
      "epoch": 0.6071980880078729,
      "grad_norm": 1.5897648334503174,
      "learning_rate": 5.0349367406089976e-05,
      "loss": 0.9878,
      "step": 4319
    },
    {
      "epoch": 0.6073386756642767,
      "grad_norm": 1.5452066659927368,
      "learning_rate": 5.0147710538303114e-05,
      "loss": 1.1491,
      "step": 4320
    },
    {
      "epoch": 0.6074792633206805,
      "grad_norm": 1.414280652999878,
      "learning_rate": 4.99463230824499e-05,
      "loss": 1.158,
      "step": 4321
    },
    {
      "epoch": 0.6076198509770843,
      "grad_norm": 1.6704879999160767,
      "learning_rate": 4.974520612686868e-05,
      "loss": 1.1028,
      "step": 4322
    },
    {
      "epoch": 0.6077604386334879,
      "grad_norm": 1.3957321643829346,
      "learning_rate": 4.954436075843711e-05,
      "loss": 1.2535,
      "step": 4323
    },
    {
      "epoch": 0.6079010262898917,
      "grad_norm": 1.5514394044876099,
      "learning_rate": 4.934378806256404e-05,
      "loss": 1.0344,
      "step": 4324
    },
    {
      "epoch": 0.6080416139462955,
      "grad_norm": 1.401624083518982,
      "learning_rate": 4.914348912318536e-05,
      "loss": 1.1864,
      "step": 4325
    },
    {
      "epoch": 0.6081822016026993,
      "grad_norm": 1.3931816816329956,
      "learning_rate": 4.8943465022757364e-05,
      "loss": 1.1757,
      "step": 4326
    },
    {
      "epoch": 0.6083227892591031,
      "grad_norm": 1.2539271116256714,
      "learning_rate": 4.874371684225104e-05,
      "loss": 1.2593,
      "step": 4327
    },
    {
      "epoch": 0.6084633769155068,
      "grad_norm": 1.4128599166870117,
      "learning_rate": 4.854424566114629e-05,
      "loss": 1.1155,
      "step": 4328
    },
    {
      "epoch": 0.6086039645719106,
      "grad_norm": 1.410922646522522,
      "learning_rate": 4.8345052557426177e-05,
      "loss": 1.2657,
      "step": 4329
    },
    {
      "epoch": 0.6087445522283144,
      "grad_norm": 1.2792892456054688,
      "learning_rate": 4.814613860757039e-05,
      "loss": 1.1646,
      "step": 4330
    },
    {
      "epoch": 0.6088851398847182,
      "grad_norm": 1.4834266901016235,
      "learning_rate": 4.7947504886551045e-05,
      "loss": 1.1957,
      "step": 4331
    },
    {
      "epoch": 0.6090257275411219,
      "grad_norm": 1.4080910682678223,
      "learning_rate": 4.7749152467824824e-05,
      "loss": 1.197,
      "step": 4332
    },
    {
      "epoch": 0.6091663151975256,
      "grad_norm": 1.384048581123352,
      "learning_rate": 4.755108242332876e-05,
      "loss": 1.1367,
      "step": 4333
    },
    {
      "epoch": 0.6093069028539294,
      "grad_norm": 1.4815808534622192,
      "learning_rate": 4.735329582347376e-05,
      "loss": 1.0719,
      "step": 4334
    },
    {
      "epoch": 0.6094474905103332,
      "grad_norm": 1.3955570459365845,
      "learning_rate": 4.715579373713892e-05,
      "loss": 1.2964,
      "step": 4335
    },
    {
      "epoch": 0.609588078166737,
      "grad_norm": 1.7314403057098389,
      "learning_rate": 4.6958577231665766e-05,
      "loss": 1.1602,
      "step": 4336
    },
    {
      "epoch": 0.6097286658231408,
      "grad_norm": 1.6436820030212402,
      "learning_rate": 4.676164737285263e-05,
      "loss": 0.9942,
      "step": 4337
    },
    {
      "epoch": 0.6098692534795445,
      "grad_norm": 1.6054106950759888,
      "learning_rate": 4.65650052249482e-05,
      "loss": 0.9992,
      "step": 4338
    },
    {
      "epoch": 0.6100098411359482,
      "grad_norm": 1.6610833406448364,
      "learning_rate": 4.636865185064728e-05,
      "loss": 0.9908,
      "step": 4339
    },
    {
      "epoch": 0.610150428792352,
      "grad_norm": 1.2309097051620483,
      "learning_rate": 4.6172588311083095e-05,
      "loss": 1.0814,
      "step": 4340
    },
    {
      "epoch": 0.6102910164487558,
      "grad_norm": 1.5919324159622192,
      "learning_rate": 4.597681566582298e-05,
      "loss": 1.2145,
      "step": 4341
    },
    {
      "epoch": 0.6104316041051596,
      "grad_norm": 2.1186587810516357,
      "learning_rate": 4.578133497286265e-05,
      "loss": 1.059,
      "step": 4342
    },
    {
      "epoch": 0.6105721917615633,
      "grad_norm": 1.7373801469802856,
      "learning_rate": 4.5586147288619174e-05,
      "loss": 0.9895,
      "step": 4343
    },
    {
      "epoch": 0.6107127794179671,
      "grad_norm": 1.206427812576294,
      "learning_rate": 4.5391253667926724e-05,
      "loss": 1.1094,
      "step": 4344
    },
    {
      "epoch": 0.6108533670743709,
      "grad_norm": 1.6961253881454468,
      "learning_rate": 4.5196655164030164e-05,
      "loss": 0.8964,
      "step": 4345
    },
    {
      "epoch": 0.6109939547307747,
      "grad_norm": 1.405066967010498,
      "learning_rate": 4.500235282857948e-05,
      "loss": 1.0301,
      "step": 4346
    },
    {
      "epoch": 0.6111345423871785,
      "grad_norm": 1.3108572959899902,
      "learning_rate": 4.4808347711624056e-05,
      "loss": 1.0792,
      "step": 4347
    },
    {
      "epoch": 0.6112751300435821,
      "grad_norm": 1.4207971096038818,
      "learning_rate": 4.461464086160728e-05,
      "loss": 1.0716,
      "step": 4348
    },
    {
      "epoch": 0.6114157176999859,
      "grad_norm": 1.5100897550582886,
      "learning_rate": 4.4421233325359976e-05,
      "loss": 1.1736,
      "step": 4349
    },
    {
      "epoch": 0.6115563053563897,
      "grad_norm": 1.2307566404342651,
      "learning_rate": 4.42281261480965e-05,
      "loss": 1.1635,
      "step": 4350
    },
    {
      "epoch": 0.6116968930127935,
      "grad_norm": 1.4345271587371826,
      "learning_rate": 4.403532037340696e-05,
      "loss": 0.9486,
      "step": 4351
    },
    {
      "epoch": 0.6118374806691973,
      "grad_norm": 1.508758306503296,
      "learning_rate": 4.38428170432532e-05,
      "loss": 1.0451,
      "step": 4352
    },
    {
      "epoch": 0.611978068325601,
      "grad_norm": 1.4468878507614136,
      "learning_rate": 4.3650617197962454e-05,
      "loss": 1.0542,
      "step": 4353
    },
    {
      "epoch": 0.6121186559820048,
      "grad_norm": 1.8240516185760498,
      "learning_rate": 4.345872187622187e-05,
      "loss": 0.8715,
      "step": 4354
    },
    {
      "epoch": 0.6122592436384086,
      "grad_norm": 1.5071755647659302,
      "learning_rate": 4.326713211507285e-05,
      "loss": 0.8966,
      "step": 4355
    },
    {
      "epoch": 0.6123998312948123,
      "grad_norm": 1.2899247407913208,
      "learning_rate": 4.30758489499057e-05,
      "loss": 0.9891,
      "step": 4356
    },
    {
      "epoch": 0.6125404189512161,
      "grad_norm": 1.4941260814666748,
      "learning_rate": 4.288487341445317e-05,
      "loss": 1.0794,
      "step": 4357
    },
    {
      "epoch": 0.6126810066076198,
      "grad_norm": 1.3990248441696167,
      "learning_rate": 4.269420654078658e-05,
      "loss": 1.2177,
      "step": 4358
    },
    {
      "epoch": 0.6128215942640236,
      "grad_norm": 1.5790759325027466,
      "learning_rate": 4.2503849359308115e-05,
      "loss": 1.1138,
      "step": 4359
    },
    {
      "epoch": 0.6129621819204274,
      "grad_norm": 1.5905839204788208,
      "learning_rate": 4.2313802898746816e-05,
      "loss": 1.0246,
      "step": 4360
    },
    {
      "epoch": 0.6131027695768312,
      "grad_norm": 1.678378701210022,
      "learning_rate": 4.2124068186152896e-05,
      "loss": 0.9704,
      "step": 4361
    },
    {
      "epoch": 0.613243357233235,
      "grad_norm": 1.3668862581253052,
      "learning_rate": 4.1934646246891004e-05,
      "loss": 1.0314,
      "step": 4362
    },
    {
      "epoch": 0.6133839448896387,
      "grad_norm": 1.6395385265350342,
      "learning_rate": 4.174553810463604e-05,
      "loss": 0.9342,
      "step": 4363
    },
    {
      "epoch": 0.6135245325460424,
      "grad_norm": 1.4189099073410034,
      "learning_rate": 4.1556744781366964e-05,
      "loss": 1.0732,
      "step": 4364
    },
    {
      "epoch": 0.6136651202024462,
      "grad_norm": 1.5765348672866821,
      "learning_rate": 4.136826729736136e-05,
      "loss": 1.0833,
      "step": 4365
    },
    {
      "epoch": 0.61380570785885,
      "grad_norm": 1.3064919710159302,
      "learning_rate": 4.118010667119013e-05,
      "loss": 1.0798,
      "step": 4366
    },
    {
      "epoch": 0.6139462955152538,
      "grad_norm": 1.5052297115325928,
      "learning_rate": 4.099226391971135e-05,
      "loss": 1.1089,
      "step": 4367
    },
    {
      "epoch": 0.6140868831716575,
      "grad_norm": 1.5134916305541992,
      "learning_rate": 4.080474005806553e-05,
      "loss": 1.0525,
      "step": 4368
    },
    {
      "epoch": 0.6142274708280613,
      "grad_norm": 1.6728014945983887,
      "learning_rate": 4.0617536099670286e-05,
      "loss": 1.219,
      "step": 4369
    },
    {
      "epoch": 0.6143680584844651,
      "grad_norm": 1.2799314260482788,
      "learning_rate": 4.043065305621353e-05,
      "loss": 1.1585,
      "step": 4370
    },
    {
      "epoch": 0.6145086461408689,
      "grad_norm": 1.5163085460662842,
      "learning_rate": 4.024409193764945e-05,
      "loss": 1.0563,
      "step": 4371
    },
    {
      "epoch": 0.6146492337972727,
      "grad_norm": 1.4219701290130615,
      "learning_rate": 4.005785375219239e-05,
      "loss": 1.3444,
      "step": 4372
    },
    {
      "epoch": 0.6147898214536763,
      "grad_norm": 1.4664735794067383,
      "learning_rate": 3.987193950631144e-05,
      "loss": 0.8697,
      "step": 4373
    },
    {
      "epoch": 0.6149304091100801,
      "grad_norm": 1.428406834602356,
      "learning_rate": 3.968635020472522e-05,
      "loss": 1.1891,
      "step": 4374
    },
    {
      "epoch": 0.6150709967664839,
      "grad_norm": 1.193264126777649,
      "learning_rate": 3.950108685039586e-05,
      "loss": 1.0278,
      "step": 4375
    },
    {
      "epoch": 0.6152115844228877,
      "grad_norm": 1.486804723739624,
      "learning_rate": 3.9316150444524305e-05,
      "loss": 0.9753,
      "step": 4376
    },
    {
      "epoch": 0.6153521720792915,
      "grad_norm": 1.509149193763733,
      "learning_rate": 3.9131541986544995e-05,
      "loss": 1.0236,
      "step": 4377
    },
    {
      "epoch": 0.6154927597356952,
      "grad_norm": 1.5526857376098633,
      "learning_rate": 3.8947262474119254e-05,
      "loss": 1.0404,
      "step": 4378
    },
    {
      "epoch": 0.615633347392099,
      "grad_norm": 1.4888659715652466,
      "learning_rate": 3.876331290313112e-05,
      "loss": 1.2512,
      "step": 4379
    },
    {
      "epoch": 0.6157739350485028,
      "grad_norm": 1.4944264888763428,
      "learning_rate": 3.8579694267681984e-05,
      "loss": 0.9484,
      "step": 4380
    },
    {
      "epoch": 0.6159145227049065,
      "grad_norm": 1.5568236112594604,
      "learning_rate": 3.839640756008397e-05,
      "loss": 0.9685,
      "step": 4381
    },
    {
      "epoch": 0.6160551103613103,
      "grad_norm": 1.7347071170806885,
      "learning_rate": 3.821345377085608e-05,
      "loss": 1.1157,
      "step": 4382
    },
    {
      "epoch": 0.616195698017714,
      "grad_norm": 1.4222285747528076,
      "learning_rate": 3.8030833888717463e-05,
      "loss": 1.1036,
      "step": 4383
    },
    {
      "epoch": 0.6163362856741178,
      "grad_norm": 1.3943883180618286,
      "learning_rate": 3.78485489005836e-05,
      "loss": 1.1418,
      "step": 4384
    },
    {
      "epoch": 0.6164768733305216,
      "grad_norm": 1.461656093597412,
      "learning_rate": 3.766659979155972e-05,
      "loss": 1.074,
      "step": 4385
    },
    {
      "epoch": 0.6166174609869254,
      "grad_norm": 1.9054063558578491,
      "learning_rate": 3.748498754493563e-05,
      "loss": 1.0386,
      "step": 4386
    },
    {
      "epoch": 0.6167580486433292,
      "grad_norm": 1.4297500848770142,
      "learning_rate": 3.7303713142180906e-05,
      "loss": 1.1283,
      "step": 4387
    },
    {
      "epoch": 0.6168986362997328,
      "grad_norm": 1.3975093364715576,
      "learning_rate": 3.7122777562939805e-05,
      "loss": 1.0754,
      "step": 4388
    },
    {
      "epoch": 0.6170392239561366,
      "grad_norm": 1.1918034553527832,
      "learning_rate": 3.6942181785024655e-05,
      "loss": 1.0718,
      "step": 4389
    },
    {
      "epoch": 0.6171798116125404,
      "grad_norm": 1.3748321533203125,
      "learning_rate": 3.676192678441198e-05,
      "loss": 1.1667,
      "step": 4390
    },
    {
      "epoch": 0.6173203992689442,
      "grad_norm": 1.579368233680725,
      "learning_rate": 3.658201353523656e-05,
      "loss": 1.2213,
      "step": 4391
    },
    {
      "epoch": 0.617460986925348,
      "grad_norm": 1.3240174055099487,
      "learning_rate": 3.6402443009786246e-05,
      "loss": 1.0808,
      "step": 4392
    },
    {
      "epoch": 0.6176015745817517,
      "grad_norm": 1.5125865936279297,
      "learning_rate": 3.62232161784969e-05,
      "loss": 1.0488,
      "step": 4393
    },
    {
      "epoch": 0.6177421622381555,
      "grad_norm": 1.395004153251648,
      "learning_rate": 3.604433400994659e-05,
      "loss": 1.1963,
      "step": 4394
    },
    {
      "epoch": 0.6178827498945593,
      "grad_norm": 1.421115756034851,
      "learning_rate": 3.5865797470851e-05,
      "loss": 1.2428,
      "step": 4395
    },
    {
      "epoch": 0.6180233375509631,
      "grad_norm": 1.8728317022323608,
      "learning_rate": 3.568760752605841e-05,
      "loss": 1.0845,
      "step": 4396
    },
    {
      "epoch": 0.6181639252073667,
      "grad_norm": 1.3712613582611084,
      "learning_rate": 3.550976513854316e-05,
      "loss": 1.1193,
      "step": 4397
    },
    {
      "epoch": 0.6183045128637705,
      "grad_norm": 1.6290863752365112,
      "learning_rate": 3.5332271269401785e-05,
      "loss": 1.1232,
      "step": 4398
    },
    {
      "epoch": 0.6184451005201743,
      "grad_norm": 1.431323766708374,
      "learning_rate": 3.5155126877847724e-05,
      "loss": 1.0867,
      "step": 4399
    },
    {
      "epoch": 0.6185856881765781,
      "grad_norm": 1.7410309314727783,
      "learning_rate": 3.4978332921204984e-05,
      "loss": 1.0688,
      "step": 4400
    },
    {
      "epoch": 0.6187262758329819,
      "grad_norm": 1.4703388214111328,
      "learning_rate": 3.48018903549044e-05,
      "loss": 1.1206,
      "step": 4401
    },
    {
      "epoch": 0.6188668634893856,
      "grad_norm": 1.3409868478775024,
      "learning_rate": 3.462580013247716e-05,
      "loss": 1.1791,
      "step": 4402
    },
    {
      "epoch": 0.6190074511457894,
      "grad_norm": 1.4749587774276733,
      "learning_rate": 3.445006320555113e-05,
      "loss": 1.018,
      "step": 4403
    },
    {
      "epoch": 0.6191480388021932,
      "grad_norm": 1.4152321815490723,
      "learning_rate": 3.427468052384448e-05,
      "loss": 1.1396,
      "step": 4404
    },
    {
      "epoch": 0.619288626458597,
      "grad_norm": 1.3408496379852295,
      "learning_rate": 3.40996530351607e-05,
      "loss": 1.0889,
      "step": 4405
    },
    {
      "epoch": 0.6194292141150007,
      "grad_norm": 1.318666934967041,
      "learning_rate": 3.392498168538401e-05,
      "loss": 1.0756,
      "step": 4406
    },
    {
      "epoch": 0.6195698017714044,
      "grad_norm": 1.5575569868087769,
      "learning_rate": 3.37506674184744e-05,
      "loss": 1.0809,
      "step": 4407
    },
    {
      "epoch": 0.6197103894278082,
      "grad_norm": 1.5042906999588013,
      "learning_rate": 3.357671117646129e-05,
      "loss": 1.1034,
      "step": 4408
    },
    {
      "epoch": 0.619850977084212,
      "grad_norm": 1.5212104320526123,
      "learning_rate": 3.3403113899439934e-05,
      "loss": 1.1415,
      "step": 4409
    },
    {
      "epoch": 0.6199915647406158,
      "grad_norm": 1.6616339683532715,
      "learning_rate": 3.3229876525565004e-05,
      "loss": 1.1151,
      "step": 4410
    },
    {
      "epoch": 0.6201321523970196,
      "grad_norm": 1.4756731986999512,
      "learning_rate": 3.305699999104697e-05,
      "loss": 1.08,
      "step": 4411
    },
    {
      "epoch": 0.6202727400534233,
      "grad_norm": 1.259914755821228,
      "learning_rate": 3.2884485230145864e-05,
      "loss": 1.0971,
      "step": 4412
    },
    {
      "epoch": 0.620413327709827,
      "grad_norm": 1.5841761827468872,
      "learning_rate": 3.2712333175166365e-05,
      "loss": 1.0999,
      "step": 4413
    },
    {
      "epoch": 0.6205539153662308,
      "grad_norm": 1.6461697816848755,
      "learning_rate": 3.254054475645326e-05,
      "loss": 1.2192,
      "step": 4414
    },
    {
      "epoch": 0.6206945030226346,
      "grad_norm": 1.424811601638794,
      "learning_rate": 3.236912090238656e-05,
      "loss": 1.1659,
      "step": 4415
    },
    {
      "epoch": 0.6208350906790384,
      "grad_norm": 1.339572548866272,
      "learning_rate": 3.219806253937534e-05,
      "loss": 1.2613,
      "step": 4416
    },
    {
      "epoch": 0.6209756783354421,
      "grad_norm": 1.477257490158081,
      "learning_rate": 3.202737059185398e-05,
      "loss": 0.974,
      "step": 4417
    },
    {
      "epoch": 0.6211162659918459,
      "grad_norm": 1.2759066820144653,
      "learning_rate": 3.1857045982276556e-05,
      "loss": 1.0652,
      "step": 4418
    },
    {
      "epoch": 0.6212568536482497,
      "grad_norm": 1.7240900993347168,
      "learning_rate": 3.168708963111198e-05,
      "loss": 1.079,
      "step": 4419
    },
    {
      "epoch": 0.6213974413046535,
      "grad_norm": 1.2336928844451904,
      "learning_rate": 3.1517502456839146e-05,
      "loss": 1.1758,
      "step": 4420
    },
    {
      "epoch": 0.6215380289610573,
      "grad_norm": 1.5001429319381714,
      "learning_rate": 3.13482853759413e-05,
      "loss": 1.1288,
      "step": 4421
    },
    {
      "epoch": 0.6216786166174609,
      "grad_norm": 1.4633482694625854,
      "learning_rate": 3.117943930290246e-05,
      "loss": 1.1878,
      "step": 4422
    },
    {
      "epoch": 0.6218192042738647,
      "grad_norm": 1.8321245908737183,
      "learning_rate": 3.101096515020119e-05,
      "loss": 1.0304,
      "step": 4423
    },
    {
      "epoch": 0.6219597919302685,
      "grad_norm": 1.5590680837631226,
      "learning_rate": 3.084286382830591e-05,
      "loss": 1.0027,
      "step": 4424
    },
    {
      "epoch": 0.6221003795866723,
      "grad_norm": 1.5889335870742798,
      "learning_rate": 3.067513624567042e-05,
      "loss": 1.1256,
      "step": 4425
    },
    {
      "epoch": 0.6222409672430761,
      "grad_norm": 1.6196380853652954,
      "learning_rate": 3.0507783308729167e-05,
      "loss": 1.1894,
      "step": 4426
    },
    {
      "epoch": 0.6223815548994798,
      "grad_norm": 1.3884514570236206,
      "learning_rate": 3.0340805921891113e-05,
      "loss": 1.0617,
      "step": 4427
    },
    {
      "epoch": 0.6225221425558836,
      "grad_norm": 1.5002044439315796,
      "learning_rate": 3.017420498753638e-05,
      "loss": 1.0097,
      "step": 4428
    },
    {
      "epoch": 0.6226627302122874,
      "grad_norm": 1.5755399465560913,
      "learning_rate": 3.0007981406009976e-05,
      "loss": 1.0326,
      "step": 4429
    },
    {
      "epoch": 0.6228033178686911,
      "grad_norm": 1.6131588220596313,
      "learning_rate": 2.984213607561841e-05,
      "loss": 0.9766,
      "step": 4430
    },
    {
      "epoch": 0.6229439055250949,
      "grad_norm": 1.8061624765396118,
      "learning_rate": 2.9676669892623634e-05,
      "loss": 1.1878,
      "step": 4431
    },
    {
      "epoch": 0.6230844931814986,
      "grad_norm": 1.4408483505249023,
      "learning_rate": 2.951158375123838e-05,
      "loss": 0.9999,
      "step": 4432
    },
    {
      "epoch": 0.6232250808379024,
      "grad_norm": 1.4647165536880493,
      "learning_rate": 2.9346878543621814e-05,
      "loss": 1.0783,
      "step": 4433
    },
    {
      "epoch": 0.6233656684943062,
      "grad_norm": 1.4452180862426758,
      "learning_rate": 2.9182555159874813e-05,
      "loss": 1.0876,
      "step": 4434
    },
    {
      "epoch": 0.62350625615071,
      "grad_norm": 1.4254707098007202,
      "learning_rate": 2.901861448803408e-05,
      "loss": 1.1282,
      "step": 4435
    },
    {
      "epoch": 0.6236468438071138,
      "grad_norm": 1.4154839515686035,
      "learning_rate": 2.885505741406852e-05,
      "loss": 1.0489,
      "step": 4436
    },
    {
      "epoch": 0.6237874314635174,
      "grad_norm": 1.6588928699493408,
      "learning_rate": 2.8691884821873926e-05,
      "loss": 1.0371,
      "step": 4437
    },
    {
      "epoch": 0.6239280191199212,
      "grad_norm": 1.3315945863723755,
      "learning_rate": 2.8529097593268218e-05,
      "loss": 1.1163,
      "step": 4438
    },
    {
      "epoch": 0.624068606776325,
      "grad_norm": 1.4353880882263184,
      "learning_rate": 2.836669660798691e-05,
      "loss": 1.127,
      "step": 4439
    },
    {
      "epoch": 0.6242091944327288,
      "grad_norm": 1.4644019603729248,
      "learning_rate": 2.8204682743677623e-05,
      "loss": 1.074,
      "step": 4440
    },
    {
      "epoch": 0.6243497820891326,
      "grad_norm": 1.5801353454589844,
      "learning_rate": 2.8043056875896712e-05,
      "loss": 1.0783,
      "step": 4441
    },
    {
      "epoch": 0.6244903697455363,
      "grad_norm": 1.2935967445373535,
      "learning_rate": 2.7881819878103288e-05,
      "loss": 1.1393,
      "step": 4442
    },
    {
      "epoch": 0.6246309574019401,
      "grad_norm": 1.3892353773117065,
      "learning_rate": 2.7720972621654696e-05,
      "loss": 1.134,
      "step": 4443
    },
    {
      "epoch": 0.6247715450583439,
      "grad_norm": 1.6180310249328613,
      "learning_rate": 2.7560515975802436e-05,
      "loss": 1.2264,
      "step": 4444
    },
    {
      "epoch": 0.6249121327147477,
      "grad_norm": 1.6356141567230225,
      "learning_rate": 2.7400450807686928e-05,
      "loss": 1.1083,
      "step": 4445
    },
    {
      "epoch": 0.6250527203711514,
      "grad_norm": 1.3472166061401367,
      "learning_rate": 2.7240777982332954e-05,
      "loss": 1.0145,
      "step": 4446
    },
    {
      "epoch": 0.6251933080275551,
      "grad_norm": 1.5314542055130005,
      "learning_rate": 2.708149836264514e-05,
      "loss": 1.1817,
      "step": 4447
    },
    {
      "epoch": 0.6253338956839589,
      "grad_norm": 1.6723191738128662,
      "learning_rate": 2.6922612809402646e-05,
      "loss": 1.0435,
      "step": 4448
    },
    {
      "epoch": 0.6254744833403627,
      "grad_norm": 1.5202618837356567,
      "learning_rate": 2.6764122181255747e-05,
      "loss": 1.1042,
      "step": 4449
    },
    {
      "epoch": 0.6256150709967665,
      "grad_norm": 1.5254162549972534,
      "learning_rate": 2.660602733472011e-05,
      "loss": 1.0888,
      "step": 4450
    },
    {
      "epoch": 0.6257556586531703,
      "grad_norm": 1.1581802368164062,
      "learning_rate": 2.6448329124172267e-05,
      "loss": 1.11,
      "step": 4451
    },
    {
      "epoch": 0.625896246309574,
      "grad_norm": 1.7343603372573853,
      "learning_rate": 2.6291028401845596e-05,
      "loss": 0.9883,
      "step": 4452
    },
    {
      "epoch": 0.6260368339659778,
      "grad_norm": 1.4901602268218994,
      "learning_rate": 2.613412601782529e-05,
      "loss": 1.0347,
      "step": 4453
    },
    {
      "epoch": 0.6261774216223815,
      "grad_norm": 1.299425721168518,
      "learning_rate": 2.597762282004379e-05,
      "loss": 1.1432,
      "step": 4454
    },
    {
      "epoch": 0.6263180092787853,
      "grad_norm": 1.2473987340927124,
      "learning_rate": 2.5821519654276406e-05,
      "loss": 1.0536,
      "step": 4455
    },
    {
      "epoch": 0.6264585969351891,
      "grad_norm": 1.4938387870788574,
      "learning_rate": 2.5665817364136114e-05,
      "loss": 1.1717,
      "step": 4456
    },
    {
      "epoch": 0.6265991845915928,
      "grad_norm": 1.4258654117584229,
      "learning_rate": 2.551051679107016e-05,
      "loss": 1.0852,
      "step": 4457
    },
    {
      "epoch": 0.6267397722479966,
      "grad_norm": 1.5060728788375854,
      "learning_rate": 2.5355618774354516e-05,
      "loss": 1.1235,
      "step": 4458
    },
    {
      "epoch": 0.6268803599044004,
      "grad_norm": 1.405777931213379,
      "learning_rate": 2.5201124151089272e-05,
      "loss": 1.1388,
      "step": 4459
    },
    {
      "epoch": 0.6270209475608042,
      "grad_norm": 1.2915644645690918,
      "learning_rate": 2.5047033756195338e-05,
      "loss": 1.0087,
      "step": 4460
    },
    {
      "epoch": 0.627161535217208,
      "grad_norm": 1.4761492013931274,
      "learning_rate": 2.489334842240826e-05,
      "loss": 0.9806,
      "step": 4461
    },
    {
      "epoch": 0.6273021228736116,
      "grad_norm": 1.554646611213684,
      "learning_rate": 2.474006898027502e-05,
      "loss": 1.1853,
      "step": 4462
    },
    {
      "epoch": 0.6274427105300154,
      "grad_norm": 1.4308911561965942,
      "learning_rate": 2.458719625814897e-05,
      "loss": 1.11,
      "step": 4463
    },
    {
      "epoch": 0.6275832981864192,
      "grad_norm": 1.541159987449646,
      "learning_rate": 2.443473108218547e-05,
      "loss": 1.0438,
      "step": 4464
    },
    {
      "epoch": 0.627723885842823,
      "grad_norm": 1.5582081079483032,
      "learning_rate": 2.428267427633739e-05,
      "loss": 1.1809,
      "step": 4465
    },
    {
      "epoch": 0.6278644734992268,
      "grad_norm": 1.399914026260376,
      "learning_rate": 2.413102666235083e-05,
      "loss": 1.0133,
      "step": 4466
    },
    {
      "epoch": 0.6280050611556305,
      "grad_norm": 1.46014404296875,
      "learning_rate": 2.3979789059760062e-05,
      "loss": 0.9893,
      "step": 4467
    },
    {
      "epoch": 0.6281456488120343,
      "grad_norm": 1.634110450744629,
      "learning_rate": 2.382896228588425e-05,
      "loss": 1.2178,
      "step": 4468
    },
    {
      "epoch": 0.6282862364684381,
      "grad_norm": 1.2450031042099,
      "learning_rate": 2.367854715582203e-05,
      "loss": 1.2112,
      "step": 4469
    },
    {
      "epoch": 0.6284268241248419,
      "grad_norm": 1.5941013097763062,
      "learning_rate": 2.352854448244719e-05,
      "loss": 1.1152,
      "step": 4470
    },
    {
      "epoch": 0.6285674117812456,
      "grad_norm": 1.389519453048706,
      "learning_rate": 2.3378955076404863e-05,
      "loss": 1.1643,
      "step": 4471
    },
    {
      "epoch": 0.6287079994376493,
      "grad_norm": 1.4938688278198242,
      "learning_rate": 2.3229779746106693e-05,
      "loss": 1.1388,
      "step": 4472
    },
    {
      "epoch": 0.6288485870940531,
      "grad_norm": 1.5804443359375,
      "learning_rate": 2.3081019297726582e-05,
      "loss": 1.0678,
      "step": 4473
    },
    {
      "epoch": 0.6289891747504569,
      "grad_norm": 1.3976081609725952,
      "learning_rate": 2.293267453519642e-05,
      "loss": 1.0578,
      "step": 4474
    },
    {
      "epoch": 0.6291297624068607,
      "grad_norm": 1.397852897644043,
      "learning_rate": 2.2784746260201184e-05,
      "loss": 0.9588,
      "step": 4475
    },
    {
      "epoch": 0.6292703500632645,
      "grad_norm": 1.3752132654190063,
      "learning_rate": 2.2637235272175796e-05,
      "loss": 1.1867,
      "step": 4476
    },
    {
      "epoch": 0.6294109377196682,
      "grad_norm": 1.6590983867645264,
      "learning_rate": 2.249014236829967e-05,
      "loss": 1.1895,
      "step": 4477
    },
    {
      "epoch": 0.629551525376072,
      "grad_norm": 1.4390580654144287,
      "learning_rate": 2.2343468343492524e-05,
      "loss": 1.0843,
      "step": 4478
    },
    {
      "epoch": 0.6296921130324757,
      "grad_norm": 1.3901548385620117,
      "learning_rate": 2.2197213990411116e-05,
      "loss": 1.0618,
      "step": 4479
    },
    {
      "epoch": 0.6298327006888795,
      "grad_norm": 1.5120019912719727,
      "learning_rate": 2.2051380099443452e-05,
      "loss": 1.0617,
      "step": 4480
    },
    {
      "epoch": 0.6299732883452833,
      "grad_norm": 1.3477047681808472,
      "learning_rate": 2.1905967458705722e-05,
      "loss": 1.1747,
      "step": 4481
    },
    {
      "epoch": 0.630113876001687,
      "grad_norm": 1.4019070863723755,
      "learning_rate": 2.1760976854037474e-05,
      "loss": 1.0612,
      "step": 4482
    },
    {
      "epoch": 0.6302544636580908,
      "grad_norm": 1.5615360736846924,
      "learning_rate": 2.161640906899749e-05,
      "loss": 1.255,
      "step": 4483
    },
    {
      "epoch": 0.6303950513144946,
      "grad_norm": 1.4424104690551758,
      "learning_rate": 2.1472264884859527e-05,
      "loss": 1.0222,
      "step": 4484
    },
    {
      "epoch": 0.6305356389708984,
      "grad_norm": 1.6282355785369873,
      "learning_rate": 2.1328545080608264e-05,
      "loss": 1.0052,
      "step": 4485
    },
    {
      "epoch": 0.6306762266273022,
      "grad_norm": 1.7765384912490845,
      "learning_rate": 2.11852504329345e-05,
      "loss": 1.0043,
      "step": 4486
    },
    {
      "epoch": 0.6308168142837058,
      "grad_norm": 1.6934775114059448,
      "learning_rate": 2.104238171623213e-05,
      "loss": 1.033,
      "step": 4487
    },
    {
      "epoch": 0.6309574019401096,
      "grad_norm": 1.3477107286453247,
      "learning_rate": 2.0899939702592498e-05,
      "loss": 0.9866,
      "step": 4488
    },
    {
      "epoch": 0.6310979895965134,
      "grad_norm": 1.3790810108184814,
      "learning_rate": 2.0757925161801394e-05,
      "loss": 1.0059,
      "step": 4489
    },
    {
      "epoch": 0.6312385772529172,
      "grad_norm": 1.6331619024276733,
      "learning_rate": 2.061633886133435e-05,
      "loss": 1.2265,
      "step": 4490
    },
    {
      "epoch": 0.631379164909321,
      "grad_norm": 1.5564173460006714,
      "learning_rate": 2.0475181566352586e-05,
      "loss": 1.0317,
      "step": 4491
    },
    {
      "epoch": 0.6315197525657247,
      "grad_norm": 1.4913239479064941,
      "learning_rate": 2.033445403969889e-05,
      "loss": 0.971,
      "step": 4492
    },
    {
      "epoch": 0.6316603402221285,
      "grad_norm": 1.848768949508667,
      "learning_rate": 2.0194157041893612e-05,
      "loss": 0.9984,
      "step": 4493
    },
    {
      "epoch": 0.6318009278785323,
      "grad_norm": 1.6827806234359741,
      "learning_rate": 2.0054291331130005e-05,
      "loss": 1.1791,
      "step": 4494
    },
    {
      "epoch": 0.631941515534936,
      "grad_norm": 1.4217039346694946,
      "learning_rate": 1.99148576632713e-05,
      "loss": 0.9814,
      "step": 4495
    },
    {
      "epoch": 0.6320821031913398,
      "grad_norm": 1.7841097116470337,
      "learning_rate": 1.977585679184515e-05,
      "loss": 1.0978,
      "step": 4496
    },
    {
      "epoch": 0.6322226908477435,
      "grad_norm": 1.4245531558990479,
      "learning_rate": 1.9637289468040577e-05,
      "loss": 1.2075,
      "step": 4497
    },
    {
      "epoch": 0.6323632785041473,
      "grad_norm": 1.5407109260559082,
      "learning_rate": 1.949915644070398e-05,
      "loss": 1.0965,
      "step": 4498
    },
    {
      "epoch": 0.6325038661605511,
      "grad_norm": 1.3557565212249756,
      "learning_rate": 1.936145845633407e-05,
      "loss": 1.1531,
      "step": 4499
    },
    {
      "epoch": 0.6326444538169549,
      "grad_norm": 1.6637272834777832,
      "learning_rate": 1.922419625907892e-05,
      "loss": 0.9441,
      "step": 4500
    },
    {
      "epoch": 0.6326444538169549,
      "eval_loss": 1.144382119178772,
      "eval_runtime": 771.578,
      "eval_samples_per_second": 16.39,
      "eval_steps_per_second": 8.195,
      "step": 4500
    },
    {
      "epoch": 0.6327850414733587,
      "grad_norm": 1.7313671112060547,
      "learning_rate": 1.9087370590731424e-05,
      "loss": 1.1638,
      "step": 4501
    },
    {
      "epoch": 0.6329256291297624,
      "grad_norm": 1.792948842048645,
      "learning_rate": 1.89509821907254e-05,
      "loss": 1.0081,
      "step": 4502
    },
    {
      "epoch": 0.6330662167861661,
      "grad_norm": 1.8421803712844849,
      "learning_rate": 1.8815031796131532e-05,
      "loss": 1.1638,
      "step": 4503
    },
    {
      "epoch": 0.6332068044425699,
      "grad_norm": 1.4263116121292114,
      "learning_rate": 1.8679520141653583e-05,
      "loss": 1.1547,
      "step": 4504
    },
    {
      "epoch": 0.6333473920989737,
      "grad_norm": 1.7271922826766968,
      "learning_rate": 1.8544447959623846e-05,
      "loss": 1.0875,
      "step": 4505
    },
    {
      "epoch": 0.6334879797553775,
      "grad_norm": 1.4782007932662964,
      "learning_rate": 1.8409815980000324e-05,
      "loss": 1.2143,
      "step": 4506
    },
    {
      "epoch": 0.6336285674117812,
      "grad_norm": 1.273679494857788,
      "learning_rate": 1.827562493036139e-05,
      "loss": 1.0547,
      "step": 4507
    },
    {
      "epoch": 0.633769155068185,
      "grad_norm": 1.466796636581421,
      "learning_rate": 1.8141875535902907e-05,
      "loss": 0.9773,
      "step": 4508
    },
    {
      "epoch": 0.6339097427245888,
      "grad_norm": 1.3641548156738281,
      "learning_rate": 1.8008568519433876e-05,
      "loss": 1.3103,
      "step": 4509
    },
    {
      "epoch": 0.6340503303809926,
      "grad_norm": 1.385964274406433,
      "learning_rate": 1.787570460137259e-05,
      "loss": 0.8734,
      "step": 4510
    },
    {
      "epoch": 0.6341909180373964,
      "grad_norm": 1.3315889835357666,
      "learning_rate": 1.774328449974273e-05,
      "loss": 1.1553,
      "step": 4511
    },
    {
      "epoch": 0.6343315056938,
      "grad_norm": 1.5484455823898315,
      "learning_rate": 1.7611308930169625e-05,
      "loss": 1.0502,
      "step": 4512
    },
    {
      "epoch": 0.6344720933502038,
      "grad_norm": 1.416001319885254,
      "learning_rate": 1.747977860587585e-05,
      "loss": 1.0369,
      "step": 4513
    },
    {
      "epoch": 0.6346126810066076,
      "grad_norm": 1.665939450263977,
      "learning_rate": 1.7348694237678498e-05,
      "loss": 1.0582,
      "step": 4514
    },
    {
      "epoch": 0.6347532686630114,
      "grad_norm": 1.1654421091079712,
      "learning_rate": 1.7218056533983895e-05,
      "loss": 1.1158,
      "step": 4515
    },
    {
      "epoch": 0.6348938563194152,
      "grad_norm": 1.3930370807647705,
      "learning_rate": 1.708786620078483e-05,
      "loss": 1.1427,
      "step": 4516
    },
    {
      "epoch": 0.6350344439758189,
      "grad_norm": 1.6332199573516846,
      "learning_rate": 1.695812394165669e-05,
      "loss": 1.1899,
      "step": 4517
    },
    {
      "epoch": 0.6351750316322227,
      "grad_norm": 1.2831356525421143,
      "learning_rate": 1.682883045775281e-05,
      "loss": 1.1198,
      "step": 4518
    },
    {
      "epoch": 0.6353156192886265,
      "grad_norm": 1.3898485898971558,
      "learning_rate": 1.6699986447801607e-05,
      "loss": 1.1126,
      "step": 4519
    },
    {
      "epoch": 0.6354562069450302,
      "grad_norm": 1.5956634283065796,
      "learning_rate": 1.6571592608102382e-05,
      "loss": 1.1473,
      "step": 4520
    },
    {
      "epoch": 0.635596794601434,
      "grad_norm": 1.5226705074310303,
      "learning_rate": 1.6443649632521606e-05,
      "loss": 0.8555,
      "step": 4521
    },
    {
      "epoch": 0.6357373822578377,
      "grad_norm": 1.470116138458252,
      "learning_rate": 1.6316158212489318e-05,
      "loss": 1.1396,
      "step": 4522
    },
    {
      "epoch": 0.6358779699142415,
      "grad_norm": 1.3038480281829834,
      "learning_rate": 1.6189119036994925e-05,
      "loss": 1.0424,
      "step": 4523
    },
    {
      "epoch": 0.6360185575706453,
      "grad_norm": 1.4289804697036743,
      "learning_rate": 1.6062532792584018e-05,
      "loss": 1.1232,
      "step": 4524
    },
    {
      "epoch": 0.6361591452270491,
      "grad_norm": 1.5107018947601318,
      "learning_rate": 1.5936400163354782e-05,
      "loss": 0.9122,
      "step": 4525
    },
    {
      "epoch": 0.6362997328834529,
      "grad_norm": 1.58344304561615,
      "learning_rate": 1.5810721830953333e-05,
      "loss": 1.1532,
      "step": 4526
    },
    {
      "epoch": 0.6364403205398566,
      "grad_norm": 1.6025232076644897,
      "learning_rate": 1.5685498474571116e-05,
      "loss": 1.104,
      "step": 4527
    },
    {
      "epoch": 0.6365809081962603,
      "grad_norm": 1.7390124797821045,
      "learning_rate": 1.5560730770940658e-05,
      "loss": 0.9467,
      "step": 4528
    },
    {
      "epoch": 0.6367214958526641,
      "grad_norm": 1.6282975673675537,
      "learning_rate": 1.5436419394332057e-05,
      "loss": 1.1049,
      "step": 4529
    },
    {
      "epoch": 0.6368620835090679,
      "grad_norm": 1.5621891021728516,
      "learning_rate": 1.5312565016549452e-05,
      "loss": 1.0868,
      "step": 4530
    },
    {
      "epoch": 0.6370026711654717,
      "grad_norm": 1.3707304000854492,
      "learning_rate": 1.5189168306926926e-05,
      "loss": 1.239,
      "step": 4531
    },
    {
      "epoch": 0.6371432588218754,
      "grad_norm": 1.301941990852356,
      "learning_rate": 1.506622993232546e-05,
      "loss": 1.1568,
      "step": 4532
    },
    {
      "epoch": 0.6372838464782792,
      "grad_norm": 1.4280186891555786,
      "learning_rate": 1.4943750557129378e-05,
      "loss": 1.1396,
      "step": 4533
    },
    {
      "epoch": 0.637424434134683,
      "grad_norm": 1.5440527200698853,
      "learning_rate": 1.4821730843241898e-05,
      "loss": 1.04,
      "step": 4534
    },
    {
      "epoch": 0.6375650217910868,
      "grad_norm": 1.3729318380355835,
      "learning_rate": 1.4700171450082433e-05,
      "loss": 1.1302,
      "step": 4535
    },
    {
      "epoch": 0.6377056094474906,
      "grad_norm": 1.4124313592910767,
      "learning_rate": 1.457907303458298e-05,
      "loss": 1.0684,
      "step": 4536
    },
    {
      "epoch": 0.6378461971038942,
      "grad_norm": 1.416968584060669,
      "learning_rate": 1.4458436251183804e-05,
      "loss": 1.1076,
      "step": 4537
    },
    {
      "epoch": 0.637986784760298,
      "grad_norm": 1.7226933240890503,
      "learning_rate": 1.433826175183085e-05,
      "loss": 1.0278,
      "step": 4538
    },
    {
      "epoch": 0.6381273724167018,
      "grad_norm": 1.4787588119506836,
      "learning_rate": 1.4218550185971325e-05,
      "loss": 1.2679,
      "step": 4539
    },
    {
      "epoch": 0.6382679600731056,
      "grad_norm": 1.3610107898712158,
      "learning_rate": 1.4099302200551213e-05,
      "loss": 0.9747,
      "step": 4540
    },
    {
      "epoch": 0.6384085477295094,
      "grad_norm": 1.4667015075683594,
      "learning_rate": 1.3980518440010926e-05,
      "loss": 1.1394,
      "step": 4541
    },
    {
      "epoch": 0.6385491353859131,
      "grad_norm": 1.4055155515670776,
      "learning_rate": 1.3862199546281939e-05,
      "loss": 1.0955,
      "step": 4542
    },
    {
      "epoch": 0.6386897230423169,
      "grad_norm": 1.345955729484558,
      "learning_rate": 1.3744346158783682e-05,
      "loss": 0.9937,
      "step": 4543
    },
    {
      "epoch": 0.6388303106987206,
      "grad_norm": 1.8019993305206299,
      "learning_rate": 1.362695891442013e-05,
      "loss": 1.027,
      "step": 4544
    },
    {
      "epoch": 0.6389708983551244,
      "grad_norm": 1.4690574407577515,
      "learning_rate": 1.3510038447575623e-05,
      "loss": 1.2568,
      "step": 4545
    },
    {
      "epoch": 0.6391114860115282,
      "grad_norm": 1.3037898540496826,
      "learning_rate": 1.3393585390112285e-05,
      "loss": 1.0033,
      "step": 4546
    },
    {
      "epoch": 0.6392520736679319,
      "grad_norm": 1.509144902229309,
      "learning_rate": 1.3277600371366173e-05,
      "loss": 1.2401,
      "step": 4547
    },
    {
      "epoch": 0.6393926613243357,
      "grad_norm": 1.4416965246200562,
      "learning_rate": 1.3162084018143961e-05,
      "loss": 1.1529,
      "step": 4548
    },
    {
      "epoch": 0.6395332489807395,
      "grad_norm": 1.3746495246887207,
      "learning_rate": 1.3047036954719672e-05,
      "loss": 0.9523,
      "step": 4549
    },
    {
      "epoch": 0.6396738366371433,
      "grad_norm": 1.6035786867141724,
      "learning_rate": 1.2932459802830887e-05,
      "loss": 0.941,
      "step": 4550
    },
    {
      "epoch": 0.6398144242935471,
      "grad_norm": 1.4460724592208862,
      "learning_rate": 1.2818353181675913e-05,
      "loss": 1.1436,
      "step": 4551
    },
    {
      "epoch": 0.6399550119499507,
      "grad_norm": 1.4307199716567993,
      "learning_rate": 1.270471770791044e-05,
      "loss": 1.104,
      "step": 4552
    },
    {
      "epoch": 0.6400955996063545,
      "grad_norm": 1.3028912544250488,
      "learning_rate": 1.2591553995643468e-05,
      "loss": 1.1008,
      "step": 4553
    },
    {
      "epoch": 0.6402361872627583,
      "grad_norm": 1.4883086681365967,
      "learning_rate": 1.2478862656434765e-05,
      "loss": 0.9387,
      "step": 4554
    },
    {
      "epoch": 0.6403767749191621,
      "grad_norm": 1.5039490461349487,
      "learning_rate": 1.2366644299291563e-05,
      "loss": 1.121,
      "step": 4555
    },
    {
      "epoch": 0.6405173625755659,
      "grad_norm": 1.7084461450576782,
      "learning_rate": 1.2254899530664465e-05,
      "loss": 1.128,
      "step": 4556
    },
    {
      "epoch": 0.6406579502319696,
      "grad_norm": 1.6071950197219849,
      "learning_rate": 1.2143628954445163e-05,
      "loss": 1.131,
      "step": 4557
    },
    {
      "epoch": 0.6407985378883734,
      "grad_norm": 1.3377387523651123,
      "learning_rate": 1.2032833171962265e-05,
      "loss": 1.1283,
      "step": 4558
    },
    {
      "epoch": 0.6409391255447772,
      "grad_norm": 1.3342350721359253,
      "learning_rate": 1.1922512781979022e-05,
      "loss": 1.1275,
      "step": 4559
    },
    {
      "epoch": 0.641079713201181,
      "grad_norm": 1.3437042236328125,
      "learning_rate": 1.1812668380689296e-05,
      "loss": 1.0719,
      "step": 4560
    },
    {
      "epoch": 0.6412203008575847,
      "grad_norm": 1.1669013500213623,
      "learning_rate": 1.1703300561714482e-05,
      "loss": 1.0583,
      "step": 4561
    },
    {
      "epoch": 0.6413608885139884,
      "grad_norm": 1.6965230703353882,
      "learning_rate": 1.1594409916100535e-05,
      "loss": 1.0856,
      "step": 4562
    },
    {
      "epoch": 0.6415014761703922,
      "grad_norm": 1.28836190700531,
      "learning_rate": 1.1485997032314954e-05,
      "loss": 1.1146,
      "step": 4563
    },
    {
      "epoch": 0.641642063826796,
      "grad_norm": 1.6541627645492554,
      "learning_rate": 1.1378062496242826e-05,
      "loss": 1.158,
      "step": 4564
    },
    {
      "epoch": 0.6417826514831998,
      "grad_norm": 1.5767271518707275,
      "learning_rate": 1.1270606891184543e-05,
      "loss": 1.1874,
      "step": 4565
    },
    {
      "epoch": 0.6419232391396036,
      "grad_norm": 1.4117753505706787,
      "learning_rate": 1.1163630797851842e-05,
      "loss": 1.1423,
      "step": 4566
    },
    {
      "epoch": 0.6420638267960073,
      "grad_norm": 1.603371262550354,
      "learning_rate": 1.1057134794365598e-05,
      "loss": 1.2485,
      "step": 4567
    },
    {
      "epoch": 0.642204414452411,
      "grad_norm": 1.601109266281128,
      "learning_rate": 1.0951119456251901e-05,
      "loss": 1.0469,
      "step": 4568
    },
    {
      "epoch": 0.6423450021088148,
      "grad_norm": 1.455797553062439,
      "learning_rate": 1.0845585356439092e-05,
      "loss": 0.9041,
      "step": 4569
    },
    {
      "epoch": 0.6424855897652186,
      "grad_norm": 1.7134190797805786,
      "learning_rate": 1.0740533065254964e-05,
      "loss": 0.9472,
      "step": 4570
    },
    {
      "epoch": 0.6426261774216224,
      "grad_norm": 1.4496278762817383,
      "learning_rate": 1.0635963150423744e-05,
      "loss": 1.1634,
      "step": 4571
    },
    {
      "epoch": 0.6427667650780261,
      "grad_norm": 1.4793556928634644,
      "learning_rate": 1.0531876177062338e-05,
      "loss": 1.192,
      "step": 4572
    },
    {
      "epoch": 0.6429073527344299,
      "grad_norm": 1.8215618133544922,
      "learning_rate": 1.0428272707678033e-05,
      "loss": 0.9175,
      "step": 4573
    },
    {
      "epoch": 0.6430479403908337,
      "grad_norm": 1.357853889465332,
      "learning_rate": 1.0325153302165091e-05,
      "loss": 1.1276,
      "step": 4574
    },
    {
      "epoch": 0.6431885280472375,
      "grad_norm": 1.4100955724716187,
      "learning_rate": 1.0222518517801805e-05,
      "loss": 1.2328,
      "step": 4575
    },
    {
      "epoch": 0.6433291157036413,
      "grad_norm": 1.7632883787155151,
      "learning_rate": 1.0120368909247558e-05,
      "loss": 0.9749,
      "step": 4576
    },
    {
      "epoch": 0.6434697033600449,
      "grad_norm": 1.3649660348892212,
      "learning_rate": 1.00187050285394e-05,
      "loss": 1.259,
      "step": 4577
    },
    {
      "epoch": 0.6436102910164487,
      "grad_norm": 1.4080471992492676,
      "learning_rate": 9.917527425089912e-06,
      "loss": 1.0207,
      "step": 4578
    },
    {
      "epoch": 0.6437508786728525,
      "grad_norm": 1.5512540340423584,
      "learning_rate": 9.81683664568348e-06,
      "loss": 1.1094,
      "step": 4579
    },
    {
      "epoch": 0.6438914663292563,
      "grad_norm": 1.2668583393096924,
      "learning_rate": 9.71663323447345e-06,
      "loss": 1.0997,
      "step": 4580
    },
    {
      "epoch": 0.6440320539856601,
      "grad_norm": 2.0123953819274902,
      "learning_rate": 9.61691773297948e-06,
      "loss": 1.0402,
      "step": 4581
    },
    {
      "epoch": 0.6441726416420638,
      "grad_norm": 1.5434449911117554,
      "learning_rate": 9.51769068008469e-06,
      "loss": 1.0305,
      "step": 4582
    },
    {
      "epoch": 0.6443132292984676,
      "grad_norm": 1.3106489181518555,
      "learning_rate": 9.418952612032061e-06,
      "loss": 1.1233,
      "step": 4583
    },
    {
      "epoch": 0.6444538169548714,
      "grad_norm": 1.4043315649032593,
      "learning_rate": 9.320704062422415e-06,
      "loss": 1.0474,
      "step": 4584
    },
    {
      "epoch": 0.6445944046112752,
      "grad_norm": 1.2796956300735474,
      "learning_rate": 9.222945562210695e-06,
      "loss": 1.1592,
      "step": 4585
    },
    {
      "epoch": 0.6447349922676789,
      "grad_norm": 1.4754557609558105,
      "learning_rate": 9.125677639703988e-06,
      "loss": 1.1417,
      "step": 4586
    },
    {
      "epoch": 0.6448755799240826,
      "grad_norm": 1.4679827690124512,
      "learning_rate": 9.028900820557973e-06,
      "loss": 1.2849,
      "step": 4587
    },
    {
      "epoch": 0.6450161675804864,
      "grad_norm": 2.0796046257019043,
      "learning_rate": 8.932615627774165e-06,
      "loss": 0.7722,
      "step": 4588
    },
    {
      "epoch": 0.6451567552368902,
      "grad_norm": 1.3975369930267334,
      "learning_rate": 8.83682258169738e-06,
      "loss": 1.1146,
      "step": 4589
    },
    {
      "epoch": 0.645297342893294,
      "grad_norm": 1.2121435403823853,
      "learning_rate": 8.741522200012952e-06,
      "loss": 1.0864,
      "step": 4590
    },
    {
      "epoch": 0.6454379305496978,
      "grad_norm": 1.8791464567184448,
      "learning_rate": 8.646714997743376e-06,
      "loss": 1.0714,
      "step": 4591
    },
    {
      "epoch": 0.6455785182061015,
      "grad_norm": 1.4890104532241821,
      "learning_rate": 8.552401487246164e-06,
      "loss": 0.9364,
      "step": 4592
    },
    {
      "epoch": 0.6457191058625052,
      "grad_norm": 1.4233893156051636,
      "learning_rate": 8.45858217821075e-06,
      "loss": 0.8702,
      "step": 4593
    },
    {
      "epoch": 0.645859693518909,
      "grad_norm": 1.4284602403640747,
      "learning_rate": 8.365257577655806e-06,
      "loss": 0.9725,
      "step": 4594
    },
    {
      "epoch": 0.6460002811753128,
      "grad_norm": 1.4677550792694092,
      "learning_rate": 8.272428189926563e-06,
      "loss": 1.1689,
      "step": 4595
    },
    {
      "epoch": 0.6461408688317166,
      "grad_norm": 1.4904371500015259,
      "learning_rate": 8.180094516691728e-06,
      "loss": 0.8672,
      "step": 4596
    },
    {
      "epoch": 0.6462814564881203,
      "grad_norm": 1.3578132390975952,
      "learning_rate": 8.088257056941528e-06,
      "loss": 1.1481,
      "step": 4597
    },
    {
      "epoch": 0.6464220441445241,
      "grad_norm": 1.5984734296798706,
      "learning_rate": 7.996916306984293e-06,
      "loss": 0.9219,
      "step": 4598
    },
    {
      "epoch": 0.6465626318009279,
      "grad_norm": 1.6134288311004639,
      "learning_rate": 7.906072760443927e-06,
      "loss": 1.0042,
      "step": 4599
    },
    {
      "epoch": 0.6467032194573317,
      "grad_norm": 1.3699004650115967,
      "learning_rate": 7.815726908257537e-06,
      "loss": 1.0077,
      "step": 4600
    },
    {
      "epoch": 0.6468438071137355,
      "grad_norm": 1.4933918714523315,
      "learning_rate": 7.725879238672518e-06,
      "loss": 1.0974,
      "step": 4601
    },
    {
      "epoch": 0.6469843947701391,
      "grad_norm": 1.5371718406677246,
      "learning_rate": 7.636530237243999e-06,
      "loss": 0.9565,
      "step": 4602
    },
    {
      "epoch": 0.6471249824265429,
      "grad_norm": 1.4520262479782104,
      "learning_rate": 7.547680386832257e-06,
      "loss": 1.2959,
      "step": 4603
    },
    {
      "epoch": 0.6472655700829467,
      "grad_norm": 1.5093024969100952,
      "learning_rate": 7.459330167599798e-06,
      "loss": 1.0456,
      "step": 4604
    },
    {
      "epoch": 0.6474061577393505,
      "grad_norm": 1.5406694412231445,
      "learning_rate": 7.371480057009406e-06,
      "loss": 0.9843,
      "step": 4605
    },
    {
      "epoch": 0.6475467453957543,
      "grad_norm": 1.4007389545440674,
      "learning_rate": 7.284130529820987e-06,
      "loss": 1.0817,
      "step": 4606
    },
    {
      "epoch": 0.647687333052158,
      "grad_norm": 1.5947946310043335,
      "learning_rate": 7.197282058089072e-06,
      "loss": 1.2235,
      "step": 4607
    },
    {
      "epoch": 0.6478279207085618,
      "grad_norm": 1.4811285734176636,
      "learning_rate": 7.110935111160544e-06,
      "loss": 1.2335,
      "step": 4608
    },
    {
      "epoch": 0.6479685083649656,
      "grad_norm": 1.744096279144287,
      "learning_rate": 7.025090155671932e-06,
      "loss": 1.1784,
      "step": 4609
    },
    {
      "epoch": 0.6481090960213693,
      "grad_norm": 1.4991068840026855,
      "learning_rate": 6.939747655546847e-06,
      "loss": 1.0812,
      "step": 4610
    },
    {
      "epoch": 0.6482496836777731,
      "grad_norm": 1.5459321737289429,
      "learning_rate": 6.854908071993638e-06,
      "loss": 1.1034,
      "step": 4611
    },
    {
      "epoch": 0.6483902713341768,
      "grad_norm": 1.5247722864151,
      "learning_rate": 6.770571863502506e-06,
      "loss": 1.015,
      "step": 4612
    },
    {
      "epoch": 0.6485308589905806,
      "grad_norm": 1.4616103172302246,
      "learning_rate": 6.686739485843707e-06,
      "loss": 1.0399,
      "step": 4613
    },
    {
      "epoch": 0.6486714466469844,
      "grad_norm": 2.0909390449523926,
      "learning_rate": 6.603411392064496e-06,
      "loss": 1.1416,
      "step": 4614
    },
    {
      "epoch": 0.6488120343033882,
      "grad_norm": 1.4743515253067017,
      "learning_rate": 6.520588032486685e-06,
      "loss": 1.2245,
      "step": 4615
    },
    {
      "epoch": 0.648952621959792,
      "grad_norm": 1.6310267448425293,
      "learning_rate": 6.438269854704848e-06,
      "loss": 0.9659,
      "step": 4616
    },
    {
      "epoch": 0.6490932096161957,
      "grad_norm": 1.4004312753677368,
      "learning_rate": 6.3564573035829965e-06,
      "loss": 0.9982,
      "step": 4617
    },
    {
      "epoch": 0.6492337972725994,
      "grad_norm": 1.4322093725204468,
      "learning_rate": 6.275150821252906e-06,
      "loss": 1.1035,
      "step": 4618
    },
    {
      "epoch": 0.6493743849290032,
      "grad_norm": 1.4936329126358032,
      "learning_rate": 6.194350847111385e-06,
      "loss": 1.2566,
      "step": 4619
    },
    {
      "epoch": 0.649514972585407,
      "grad_norm": 1.7867822647094727,
      "learning_rate": 6.114057817817942e-06,
      "loss": 1.1337,
      "step": 4620
    },
    {
      "epoch": 0.6496555602418108,
      "grad_norm": 1.4738588333129883,
      "learning_rate": 6.034272167292488e-06,
      "loss": 1.1463,
      "step": 4621
    },
    {
      "epoch": 0.6497961478982145,
      "grad_norm": 1.6060329675674438,
      "learning_rate": 5.954994326712992e-06,
      "loss": 1.0886,
      "step": 4622
    },
    {
      "epoch": 0.6499367355546183,
      "grad_norm": 1.4173438549041748,
      "learning_rate": 5.876224724512891e-06,
      "loss": 1.0507,
      "step": 4623
    },
    {
      "epoch": 0.6500773232110221,
      "grad_norm": 1.3353832960128784,
      "learning_rate": 5.797963786379357e-06,
      "loss": 1.0758,
      "step": 4624
    },
    {
      "epoch": 0.6502179108674259,
      "grad_norm": 1.28609037399292,
      "learning_rate": 5.72021193525043e-06,
      "loss": 1.1155,
      "step": 4625
    },
    {
      "epoch": 0.6503584985238297,
      "grad_norm": 1.2950166463851929,
      "learning_rate": 5.642969591312863e-06,
      "loss": 1.228,
      "step": 4626
    },
    {
      "epoch": 0.6504990861802333,
      "grad_norm": 1.2037038803100586,
      "learning_rate": 5.566237172000077e-06,
      "loss": 1.3189,
      "step": 4627
    },
    {
      "epoch": 0.6506396738366371,
      "grad_norm": 1.3204855918884277,
      "learning_rate": 5.490015091989709e-06,
      "loss": 1.0453,
      "step": 4628
    },
    {
      "epoch": 0.6507802614930409,
      "grad_norm": 1.8723152875900269,
      "learning_rate": 5.414303763201412e-06,
      "loss": 1.0591,
      "step": 4629
    },
    {
      "epoch": 0.6509208491494447,
      "grad_norm": 1.461222767829895,
      "learning_rate": 5.339103594794692e-06,
      "loss": 1.0136,
      "step": 4630
    },
    {
      "epoch": 0.6510614368058485,
      "grad_norm": 1.4480640888214111,
      "learning_rate": 5.264414993166433e-06,
      "loss": 1.1268,
      "step": 4631
    },
    {
      "epoch": 0.6512020244622522,
      "grad_norm": 1.3429391384124756,
      "learning_rate": 5.190238361949207e-06,
      "loss": 1.123,
      "step": 4632
    },
    {
      "epoch": 0.651342612118656,
      "grad_norm": 1.43754243850708,
      "learning_rate": 5.116574102008664e-06,
      "loss": 1.3193,
      "step": 4633
    },
    {
      "epoch": 0.6514831997750598,
      "grad_norm": 1.3405563831329346,
      "learning_rate": 5.0434226114413175e-06,
      "loss": 1.313,
      "step": 4634
    },
    {
      "epoch": 0.6516237874314635,
      "grad_norm": 1.277026891708374,
      "learning_rate": 4.970784285572938e-06,
      "loss": 1.2259,
      "step": 4635
    },
    {
      "epoch": 0.6517643750878673,
      "grad_norm": 1.3763902187347412,
      "learning_rate": 4.898659516955695e-06,
      "loss": 1.1217,
      "step": 4636
    },
    {
      "epoch": 0.651904962744271,
      "grad_norm": 1.3666484355926514,
      "learning_rate": 4.827048695366576e-06,
      "loss": 1.0983,
      "step": 4637
    },
    {
      "epoch": 0.6520455504006748,
      "grad_norm": 1.3865193128585815,
      "learning_rate": 4.75595220780507e-06,
      "loss": 0.9801,
      "step": 4638
    },
    {
      "epoch": 0.6521861380570786,
      "grad_norm": 1.4418766498565674,
      "learning_rate": 4.685370438491088e-06,
      "loss": 0.8953,
      "step": 4639
    },
    {
      "epoch": 0.6523267257134824,
      "grad_norm": 1.837701439857483,
      "learning_rate": 4.615303768862888e-06,
      "loss": 1.0736,
      "step": 4640
    },
    {
      "epoch": 0.6524673133698862,
      "grad_norm": 1.4642831087112427,
      "learning_rate": 4.5457525775750905e-06,
      "loss": 1.1013,
      "step": 4641
    },
    {
      "epoch": 0.6526079010262898,
      "grad_norm": 1.7053608894348145,
      "learning_rate": 4.476717240496364e-06,
      "loss": 1.0683,
      "step": 4642
    },
    {
      "epoch": 0.6527484886826936,
      "grad_norm": 1.3616359233856201,
      "learning_rate": 4.40819813070793e-06,
      "loss": 1.0951,
      "step": 4643
    },
    {
      "epoch": 0.6528890763390974,
      "grad_norm": 1.636445164680481,
      "learning_rate": 4.3401956185008865e-06,
      "loss": 1.1668,
      "step": 4644
    },
    {
      "epoch": 0.6530296639955012,
      "grad_norm": 1.447555422782898,
      "learning_rate": 4.272710071374708e-06,
      "loss": 1.1322,
      "step": 4645
    },
    {
      "epoch": 0.653170251651905,
      "grad_norm": 1.4610812664031982,
      "learning_rate": 4.205741854035061e-06,
      "loss": 1.189,
      "step": 4646
    },
    {
      "epoch": 0.6533108393083087,
      "grad_norm": 1.6126530170440674,
      "learning_rate": 4.139291328391826e-06,
      "loss": 1.1342,
      "step": 4647
    },
    {
      "epoch": 0.6534514269647125,
      "grad_norm": 1.4832618236541748,
      "learning_rate": 4.0733588535571964e-06,
      "loss": 1.2393,
      "step": 4648
    },
    {
      "epoch": 0.6535920146211163,
      "grad_norm": 1.3505134582519531,
      "learning_rate": 4.007944785843754e-06,
      "loss": 1.1831,
      "step": 4649
    },
    {
      "epoch": 0.6537326022775201,
      "grad_norm": 1.5674325227737427,
      "learning_rate": 3.943049478762306e-06,
      "loss": 1.1255,
      "step": 4650
    },
    {
      "epoch": 0.6538731899339238,
      "grad_norm": 1.4148610830307007,
      "learning_rate": 3.878673283020506e-06,
      "loss": 1.1137,
      "step": 4651
    },
    {
      "epoch": 0.6540137775903275,
      "grad_norm": 1.5694575309753418,
      "learning_rate": 3.814816546520305e-06,
      "loss": 1.0304,
      "step": 4652
    },
    {
      "epoch": 0.6541543652467313,
      "grad_norm": 2.029987335205078,
      "learning_rate": 3.7514796143565233e-06,
      "loss": 1.0763,
      "step": 4653
    },
    {
      "epoch": 0.6542949529031351,
      "grad_norm": 1.4760479927062988,
      "learning_rate": 3.688662828814993e-06,
      "loss": 1.1008,
      "step": 4654
    },
    {
      "epoch": 0.6544355405595389,
      "grad_norm": 1.6851032972335815,
      "learning_rate": 3.626366529370273e-06,
      "loss": 1.0602,
      "step": 4655
    },
    {
      "epoch": 0.6545761282159427,
      "grad_norm": 1.3899202346801758,
      "learning_rate": 3.5645910526843186e-06,
      "loss": 1.1824,
      "step": 4656
    },
    {
      "epoch": 0.6547167158723464,
      "grad_norm": 1.371256709098816,
      "learning_rate": 3.5033367326043896e-06,
      "loss": 1.1281,
      "step": 4657
    },
    {
      "epoch": 0.6548573035287502,
      "grad_norm": 1.3207546472549438,
      "learning_rate": 3.442603900161334e-06,
      "loss": 1.2554,
      "step": 4658
    },
    {
      "epoch": 0.654997891185154,
      "grad_norm": 1.6351217031478882,
      "learning_rate": 3.382392883567753e-06,
      "loss": 1.0777,
      "step": 4659
    },
    {
      "epoch": 0.6551384788415577,
      "grad_norm": 1.4840818643569946,
      "learning_rate": 3.322704008216282e-06,
      "loss": 1.1817,
      "step": 4660
    },
    {
      "epoch": 0.6552790664979615,
      "grad_norm": 1.4984583854675293,
      "learning_rate": 3.263537596677657e-06,
      "loss": 1.0334,
      "step": 4661
    },
    {
      "epoch": 0.6554196541543652,
      "grad_norm": 1.592917561531067,
      "learning_rate": 3.204893968699385e-06,
      "loss": 1.1682,
      "step": 4662
    },
    {
      "epoch": 0.655560241810769,
      "grad_norm": 1.3989580869674683,
      "learning_rate": 3.1467734412034656e-06,
      "loss": 0.9429,
      "step": 4663
    },
    {
      "epoch": 0.6557008294671728,
      "grad_norm": 1.6415146589279175,
      "learning_rate": 3.0891763282851038e-06,
      "loss": 1.1336,
      "step": 4664
    },
    {
      "epoch": 0.6558414171235766,
      "grad_norm": 1.6393038034439087,
      "learning_rate": 3.032102941210857e-06,
      "loss": 1.0681,
      "step": 4665
    },
    {
      "epoch": 0.6559820047799804,
      "grad_norm": 1.443495750427246,
      "learning_rate": 2.9755535884169237e-06,
      "loss": 0.9845,
      "step": 4666
    },
    {
      "epoch": 0.656122592436384,
      "grad_norm": 1.740159273147583,
      "learning_rate": 2.919528575507546e-06,
      "loss": 1.1334,
      "step": 4667
    },
    {
      "epoch": 0.6562631800927878,
      "grad_norm": 1.5363390445709229,
      "learning_rate": 2.8640282052533553e-06,
      "loss": 1.0782,
      "step": 4668
    },
    {
      "epoch": 0.6564037677491916,
      "grad_norm": 1.5020028352737427,
      "learning_rate": 2.809052777589538e-06,
      "loss": 0.9812,
      "step": 4669
    },
    {
      "epoch": 0.6565443554055954,
      "grad_norm": 1.9931001663208008,
      "learning_rate": 2.7546025896146633e-06,
      "loss": 1.1206,
      "step": 4670
    },
    {
      "epoch": 0.6566849430619992,
      "grad_norm": 1.311684012413025,
      "learning_rate": 2.700677935588536e-06,
      "loss": 1.1235,
      "step": 4671
    },
    {
      "epoch": 0.6568255307184029,
      "grad_norm": 1.5941035747528076,
      "learning_rate": 2.6472791069309776e-06,
      "loss": 1.1965,
      "step": 4672
    },
    {
      "epoch": 0.6569661183748067,
      "grad_norm": 1.464877963066101,
      "learning_rate": 2.594406392220261e-06,
      "loss": 1.0991,
      "step": 4673
    },
    {
      "epoch": 0.6571067060312105,
      "grad_norm": 1.4147855043411255,
      "learning_rate": 2.542060077191177e-06,
      "loss": 1.0422,
      "step": 4674
    },
    {
      "epoch": 0.6572472936876143,
      "grad_norm": 1.6108964681625366,
      "learning_rate": 2.490240444733949e-06,
      "loss": 0.9241,
      "step": 4675
    },
    {
      "epoch": 0.657387881344018,
      "grad_norm": 1.400757074356079,
      "learning_rate": 2.4389477748924194e-06,
      "loss": 0.9684,
      "step": 4676
    },
    {
      "epoch": 0.6575284690004217,
      "grad_norm": 1.3087785243988037,
      "learning_rate": 2.388182344862633e-06,
      "loss": 0.9708,
      "step": 4677
    },
    {
      "epoch": 0.6576690566568255,
      "grad_norm": 1.4986070394515991,
      "learning_rate": 2.3379444289913567e-06,
      "loss": 1.0709,
      "step": 4678
    },
    {
      "epoch": 0.6578096443132293,
      "grad_norm": 1.687068223953247,
      "learning_rate": 2.288234298774461e-06,
      "loss": 1.0933,
      "step": 4679
    },
    {
      "epoch": 0.6579502319696331,
      "grad_norm": 1.565130591392517,
      "learning_rate": 2.2390522228555977e-06,
      "loss": 1.138,
      "step": 4680
    },
    {
      "epoch": 0.6580908196260369,
      "grad_norm": 1.3261088132858276,
      "learning_rate": 2.190398467024868e-06,
      "loss": 0.992,
      "step": 4681
    },
    {
      "epoch": 0.6582314072824406,
      "grad_norm": 1.3626514673233032,
      "learning_rate": 2.1422732942169676e-06,
      "loss": 1.0854,
      "step": 4682
    },
    {
      "epoch": 0.6583719949388444,
      "grad_norm": 1.4164628982543945,
      "learning_rate": 2.094676964510167e-06,
      "loss": 0.8984,
      "step": 4683
    },
    {
      "epoch": 0.6585125825952481,
      "grad_norm": 1.611080527305603,
      "learning_rate": 2.0476097351247446e-06,
      "loss": 1.0359,
      "step": 4684
    },
    {
      "epoch": 0.6586531702516519,
      "grad_norm": 1.4140105247497559,
      "learning_rate": 2.0010718604215884e-06,
      "loss": 1.0064,
      "step": 4685
    },
    {
      "epoch": 0.6587937579080557,
      "grad_norm": 1.3111251592636108,
      "learning_rate": 1.9550635919009075e-06,
      "loss": 1.1599,
      "step": 4686
    },
    {
      "epoch": 0.6589343455644594,
      "grad_norm": 1.3239078521728516,
      "learning_rate": 1.909585178200679e-06,
      "loss": 1.0376,
      "step": 4687
    },
    {
      "epoch": 0.6590749332208632,
      "grad_norm": 1.3969831466674805,
      "learning_rate": 1.864636865095537e-06,
      "loss": 1.0467,
      "step": 4688
    },
    {
      "epoch": 0.659215520877267,
      "grad_norm": 1.521179437637329,
      "learning_rate": 1.820218895495418e-06,
      "loss": 1.1343,
      "step": 4689
    },
    {
      "epoch": 0.6593561085336708,
      "grad_norm": 1.7416496276855469,
      "learning_rate": 1.7763315094439847e-06,
      "loss": 1.2626,
      "step": 4690
    },
    {
      "epoch": 0.6594966961900746,
      "grad_norm": 1.2517189979553223,
      "learning_rate": 1.7329749441176268e-06,
      "loss": 1.0251,
      "step": 4691
    },
    {
      "epoch": 0.6596372838464782,
      "grad_norm": 1.5434406995773315,
      "learning_rate": 1.6901494338241397e-06,
      "loss": 1.136,
      "step": 4692
    },
    {
      "epoch": 0.659777871502882,
      "grad_norm": 1.3562062978744507,
      "learning_rate": 1.6478552100012257e-06,
      "loss": 1.218,
      "step": 4693
    },
    {
      "epoch": 0.6599184591592858,
      "grad_norm": 1.8169102668762207,
      "learning_rate": 1.606092501215517e-06,
      "loss": 1.0194,
      "step": 4694
    },
    {
      "epoch": 0.6600590468156896,
      "grad_norm": 1.404701828956604,
      "learning_rate": 1.5648615331612104e-06,
      "loss": 1.0888,
      "step": 4695
    },
    {
      "epoch": 0.6601996344720934,
      "grad_norm": 1.4749436378479004,
      "learning_rate": 1.5241625286588567e-06,
      "loss": 1.1874,
      "step": 4696
    },
    {
      "epoch": 0.6603402221284971,
      "grad_norm": 1.4246598482131958,
      "learning_rate": 1.4839957076542066e-06,
      "loss": 1.127,
      "step": 4697
    },
    {
      "epoch": 0.6604808097849009,
      "grad_norm": 1.2662742137908936,
      "learning_rate": 1.4443612872168777e-06,
      "loss": 1.1556,
      "step": 4698
    },
    {
      "epoch": 0.6606213974413047,
      "grad_norm": 1.3867974281311035,
      "learning_rate": 1.405259481539367e-06,
      "loss": 1.1098,
      "step": 4699
    },
    {
      "epoch": 0.6607619850977084,
      "grad_norm": 1.5200914144515991,
      "learning_rate": 1.3666905019358856e-06,
      "loss": 1.143,
      "step": 4700
    },
    {
      "epoch": 0.6609025727541122,
      "grad_norm": 1.4861524105072021,
      "learning_rate": 1.3286545568409802e-06,
      "loss": 1.1899,
      "step": 4701
    },
    {
      "epoch": 0.6610431604105159,
      "grad_norm": 1.3704124689102173,
      "learning_rate": 1.2911518518086695e-06,
      "loss": 1.2642,
      "step": 4702
    },
    {
      "epoch": 0.6611837480669197,
      "grad_norm": 1.318520188331604,
      "learning_rate": 1.2541825895112213e-06,
      "loss": 1.2359,
      "step": 4703
    },
    {
      "epoch": 0.6613243357233235,
      "grad_norm": 1.5450632572174072,
      "learning_rate": 1.217746969738065e-06,
      "loss": 1.0623,
      "step": 4704
    },
    {
      "epoch": 0.6614649233797273,
      "grad_norm": 1.5264941453933716,
      "learning_rate": 1.1818451893947368e-06,
      "loss": 1.1317,
      "step": 4705
    },
    {
      "epoch": 0.6616055110361311,
      "grad_norm": 1.8133238554000854,
      "learning_rate": 1.1464774425017366e-06,
      "loss": 1.2198,
      "step": 4706
    },
    {
      "epoch": 0.6617460986925348,
      "grad_norm": 1.4382520914077759,
      "learning_rate": 1.1116439201935614e-06,
      "loss": 1.1941,
      "step": 4707
    },
    {
      "epoch": 0.6618866863489385,
      "grad_norm": 1.512198567390442,
      "learning_rate": 1.077344810717751e-06,
      "loss": 1.2951,
      "step": 4708
    },
    {
      "epoch": 0.6620272740053423,
      "grad_norm": 1.3102061748504639,
      "learning_rate": 1.0435802994336108e-06,
      "loss": 1.0848,
      "step": 4709
    },
    {
      "epoch": 0.6621678616617461,
      "grad_norm": 1.4544934034347534,
      "learning_rate": 1.0103505688114245e-06,
      "loss": 1.1525,
      "step": 4710
    },
    {
      "epoch": 0.6623084493181499,
      "grad_norm": 1.369935393333435,
      "learning_rate": 9.776557984315315e-07,
      "loss": 1.1115,
      "step": 4711
    },
    {
      "epoch": 0.6624490369745536,
      "grad_norm": 1.4719254970550537,
      "learning_rate": 9.454961649830396e-07,
      "loss": 1.158,
      "step": 4712
    },
    {
      "epoch": 0.6625896246309574,
      "grad_norm": 1.5282942056655884,
      "learning_rate": 9.138718422632253e-07,
      "loss": 1.1534,
      "step": 4713
    },
    {
      "epoch": 0.6627302122873612,
      "grad_norm": 1.7642723321914673,
      "learning_rate": 8.827830011762905e-07,
      "loss": 0.9751,
      "step": 4714
    },
    {
      "epoch": 0.662870799943765,
      "grad_norm": 1.4575263261795044,
      "learning_rate": 8.522298097327408e-07,
      "loss": 1.0893,
      "step": 4715
    },
    {
      "epoch": 0.6630113876001688,
      "grad_norm": 1.3493950366973877,
      "learning_rate": 8.222124330482639e-07,
      "loss": 1.0619,
      "step": 4716
    },
    {
      "epoch": 0.6631519752565724,
      "grad_norm": 1.2762203216552734,
      "learning_rate": 7.927310333428195e-07,
      "loss": 1.2318,
      "step": 4717
    },
    {
      "epoch": 0.6632925629129762,
      "grad_norm": 1.3016866445541382,
      "learning_rate": 7.637857699399176e-07,
      "loss": 1.159,
      "step": 4718
    },
    {
      "epoch": 0.66343315056938,
      "grad_norm": 1.3848686218261719,
      "learning_rate": 7.353767992657079e-07,
      "loss": 1.1093,
      "step": 4719
    },
    {
      "epoch": 0.6635737382257838,
      "grad_norm": 1.5514755249023438,
      "learning_rate": 7.075042748480143e-07,
      "loss": 1.1296,
      "step": 4720
    },
    {
      "epoch": 0.6637143258821876,
      "grad_norm": 1.6539583206176758,
      "learning_rate": 6.801683473156683e-07,
      "loss": 1.0772,
      "step": 4721
    },
    {
      "epoch": 0.6638549135385913,
      "grad_norm": 1.7444967031478882,
      "learning_rate": 6.533691643975326e-07,
      "loss": 1.2007,
      "step": 4722
    },
    {
      "epoch": 0.6639955011949951,
      "grad_norm": 1.4749362468719482,
      "learning_rate": 6.27106870921923e-07,
      "loss": 1.0723,
      "step": 4723
    },
    {
      "epoch": 0.6641360888513989,
      "grad_norm": 1.5300449132919312,
      "learning_rate": 6.01381608815621e-07,
      "loss": 1.0595,
      "step": 4724
    },
    {
      "epoch": 0.6642766765078026,
      "grad_norm": 1.2021002769470215,
      "learning_rate": 5.761935171031407e-07,
      "loss": 1.2158,
      "step": 4725
    },
    {
      "epoch": 0.6644172641642064,
      "grad_norm": 1.4134711027145386,
      "learning_rate": 5.515427319060739e-07,
      "loss": 1.0113,
      "step": 4726
    },
    {
      "epoch": 0.6645578518206101,
      "grad_norm": 1.3078596591949463,
      "learning_rate": 5.274293864423463e-07,
      "loss": 1.0031,
      "step": 4727
    },
    {
      "epoch": 0.6646984394770139,
      "grad_norm": 1.4784148931503296,
      "learning_rate": 5.03853611025329e-07,
      "loss": 1.1576,
      "step": 4728
    },
    {
      "epoch": 0.6648390271334177,
      "grad_norm": 1.6909754276275635,
      "learning_rate": 4.80815533063339e-07,
      "loss": 0.9528,
      "step": 4729
    },
    {
      "epoch": 0.6649796147898215,
      "grad_norm": 1.5353033542633057,
      "learning_rate": 4.583152770588406e-07,
      "loss": 1.0678,
      "step": 4730
    },
    {
      "epoch": 0.6651202024462253,
      "grad_norm": 1.5983922481536865,
      "learning_rate": 4.3635296460781127e-07,
      "loss": 1.0682,
      "step": 4731
    },
    {
      "epoch": 0.665260790102629,
      "grad_norm": 1.45863676071167,
      "learning_rate": 4.149287143990765e-07,
      "loss": 1.1654,
      "step": 4732
    },
    {
      "epoch": 0.6654013777590327,
      "grad_norm": 1.607346534729004,
      "learning_rate": 3.940426422136101e-07,
      "loss": 1.1677,
      "step": 4733
    },
    {
      "epoch": 0.6655419654154365,
      "grad_norm": 1.5031582117080688,
      "learning_rate": 3.736948609240343e-07,
      "loss": 1.0674,
      "step": 4734
    },
    {
      "epoch": 0.6656825530718403,
      "grad_norm": 1.5838440656661987,
      "learning_rate": 3.5388548049392065e-07,
      "loss": 1.0731,
      "step": 4735
    },
    {
      "epoch": 0.6658231407282441,
      "grad_norm": 1.3321393728256226,
      "learning_rate": 3.3461460797715726e-07,
      "loss": 1.1096,
      "step": 4736
    },
    {
      "epoch": 0.6659637283846478,
      "grad_norm": 1.3840361833572388,
      "learning_rate": 3.158823475174821e-07,
      "loss": 0.8806,
      "step": 4737
    },
    {
      "epoch": 0.6661043160410516,
      "grad_norm": 1.4491748809814453,
      "learning_rate": 2.9768880034788395e-07,
      "loss": 1.1457,
      "step": 4738
    },
    {
      "epoch": 0.6662449036974554,
      "grad_norm": 1.6087952852249146,
      "learning_rate": 2.80034064789958e-07,
      "loss": 1.06,
      "step": 4739
    },
    {
      "epoch": 0.6663854913538592,
      "grad_norm": 1.4784824848175049,
      "learning_rate": 2.629182362535065e-07,
      "loss": 1.2269,
      "step": 4740
    },
    {
      "epoch": 0.666526079010263,
      "grad_norm": 1.3393865823745728,
      "learning_rate": 2.4634140723595044e-07,
      "loss": 1.0946,
      "step": 4741
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 1.4266458749771118,
      "learning_rate": 2.303036673218628e-07,
      "loss": 1.2321,
      "step": 4742
    },
    {
      "epoch": 0.6668072543230704,
      "grad_norm": 1.5118038654327393,
      "learning_rate": 2.1480510318245828e-07,
      "loss": 0.9111,
      "step": 4743
    },
    {
      "epoch": 0.6669478419794742,
      "grad_norm": 1.5277137756347656,
      "learning_rate": 1.9984579857512675e-07,
      "loss": 0.9602,
      "step": 4744
    },
    {
      "epoch": 0.667088429635878,
      "grad_norm": 1.620111107826233,
      "learning_rate": 1.8542583434300042e-07,
      "loss": 1.0505,
      "step": 4745
    },
    {
      "epoch": 0.6672290172922818,
      "grad_norm": 1.9711358547210693,
      "learning_rate": 1.7154528841452078e-07,
      "loss": 0.9652,
      "step": 4746
    },
    {
      "epoch": 0.6673696049486855,
      "grad_norm": 1.443314552307129,
      "learning_rate": 1.582042358029723e-07,
      "loss": 1.115,
      "step": 4747
    },
    {
      "epoch": 0.6675101926050893,
      "grad_norm": 1.417747974395752,
      "learning_rate": 1.4540274860612712e-07,
      "loss": 1.0308,
      "step": 4748
    },
    {
      "epoch": 0.667650780261493,
      "grad_norm": 1.6341999769210815,
      "learning_rate": 1.3314089600583445e-07,
      "loss": 1.1707,
      "step": 4749
    },
    {
      "epoch": 0.6677913679178968,
      "grad_norm": 1.4526551961898804,
      "learning_rate": 1.2141874426763177e-07,
      "loss": 0.9883,
      "step": 4750
    },
    {
      "epoch": 0.6679319555743006,
      "grad_norm": 1.5006890296936035,
      "learning_rate": 1.102363567404452e-07,
      "loss": 1.0936,
      "step": 4751
    },
    {
      "epoch": 0.6680725432307043,
      "grad_norm": 1.4973688125610352,
      "learning_rate": 9.959379385613421e-08,
      "loss": 1.0619,
      "step": 4752
    },
    {
      "epoch": 0.6682131308871081,
      "grad_norm": 1.4991475343704224,
      "learning_rate": 8.949111312931413e-08,
      "loss": 1.1532,
      "step": 4753
    },
    {
      "epoch": 0.6683537185435119,
      "grad_norm": 1.224552035331726,
      "learning_rate": 7.992836915692303e-08,
      "loss": 1.0841,
      "step": 4754
    },
    {
      "epoch": 0.6684943061999157,
      "grad_norm": 1.6060876846313477,
      "learning_rate": 7.090561361796644e-08,
      "loss": 1.3704,
      "step": 4755
    },
    {
      "epoch": 0.6686348938563195,
      "grad_norm": 1.951820969581604,
      "learning_rate": 6.242289527326195e-08,
      "loss": 1.038,
      "step": 4756
    },
    {
      "epoch": 0.6687754815127231,
      "grad_norm": 1.2801499366760254,
      "learning_rate": 5.4480259965139504e-08,
      "loss": 1.2058,
      "step": 4757
    },
    {
      "epoch": 0.6689160691691269,
      "grad_norm": 1.5191859006881714,
      "learning_rate": 4.707775061723041e-08,
      "loss": 1.1005,
      "step": 4758
    },
    {
      "epoch": 0.6690566568255307,
      "grad_norm": 1.6534703969955444,
      "learning_rate": 4.021540723421202e-08,
      "loss": 1.1093,
      "step": 4759
    },
    {
      "epoch": 0.6691972444819345,
      "grad_norm": 1.3649096488952637,
      "learning_rate": 3.389326690156347e-08,
      "loss": 1.1869,
      "step": 4760
    },
    {
      "epoch": 0.6693378321383383,
      "grad_norm": 1.4506586790084839,
      "learning_rate": 2.8111363785432442e-08,
      "loss": 1.1674,
      "step": 4761
    },
    {
      "epoch": 0.669478419794742,
      "grad_norm": 1.6341016292572021,
      "learning_rate": 2.2869729132413142e-08,
      "loss": 1.1764,
      "step": 4762
    },
    {
      "epoch": 0.6696190074511458,
      "grad_norm": 1.539791226387024,
      "learning_rate": 1.8168391269346442e-08,
      "loss": 1.1462,
      "step": 4763
    },
    {
      "epoch": 0.6697595951075496,
      "grad_norm": 1.467381477355957,
      "learning_rate": 1.4007375603219963e-08,
      "loss": 1.1724,
      "step": 4764
    },
    {
      "epoch": 0.6699001827639534,
      "grad_norm": 1.3140398263931274,
      "learning_rate": 1.038670462103486e-08,
      "loss": 1.0921,
      "step": 4765
    },
    {
      "epoch": 0.6700407704203571,
      "grad_norm": 1.3499788045883179,
      "learning_rate": 7.306397889617067e-09,
      "loss": 1.1207,
      "step": 4766
    },
    {
      "epoch": 0.6701813580767608,
      "grad_norm": 1.4730892181396484,
      "learning_rate": 4.766472055572901e-09,
      "loss": 1.018,
      "step": 4767
    },
    {
      "epoch": 0.6703219457331646,
      "grad_norm": 1.5563009977340698,
      "learning_rate": 2.7669408451780344e-09,
      "loss": 1.1369,
      "step": 4768
    },
    {
      "epoch": 0.6704625333895684,
      "grad_norm": 1.4129835367202759,
      "learning_rate": 1.3078150643219822e-09,
      "loss": 1.2097,
      "step": 4769
    },
    {
      "epoch": 0.6706031210459722,
      "grad_norm": 1.3660987615585327,
      "learning_rate": 3.8910259839708417e-10,
      "loss": 1.0155,
      "step": 4770
    },
    {
      "epoch": 0.670743708702376,
      "grad_norm": 1.4899345636367798,
      "learning_rate": 1.0808412320706396e-11,
      "loss": 0.9879,
      "step": 4771
    },
    {
      "epoch": 0.6708842963587797,
      "grad_norm": 1.5905715227127075,
      "learning_rate": 1.7293455047973083e-10,
      "loss": 1.0982,
      "step": 4772
    },
    {
      "epoch": 0.6710248840151835,
      "grad_norm": 1.6500169038772583,
      "learning_rate": 8.754801367083509e-10,
      "loss": 1.0685,
      "step": 4773
    },
    {
      "epoch": 0.6711654716715872,
      "grad_norm": 1.5946232080459595,
      "learning_rate": 2.1184413742880717e-09,
      "loss": 0.9942,
      "step": 4774
    },
    {
      "epoch": 0.671306059327991,
      "grad_norm": 1.3676609992980957,
      "learning_rate": 3.901811546036527e-09,
      "loss": 1.0881,
      "step": 4775
    },
    {
      "epoch": 0.6714466469843948,
      "grad_norm": 1.4879956245422363,
      "learning_rate": 6.225581014229764e-09,
      "loss": 1.106,
      "step": 4776
    },
    {
      "epoch": 0.6715872346407985,
      "grad_norm": 1.283400058746338,
      "learning_rate": 9.089737220746574e-09,
      "loss": 1.1082,
      "step": 4777
    },
    {
      "epoch": 0.6717278222972023,
      "grad_norm": 1.402266025543213,
      "learning_rate": 1.2494264687124002e-08,
      "loss": 1.179,
      "step": 4778
    },
    {
      "epoch": 0.6718684099536061,
      "grad_norm": 1.5371780395507812,
      "learning_rate": 1.643914501457955e-08,
      "loss": 1.4179,
      "step": 4779
    },
    {
      "epoch": 0.6720089976100099,
      "grad_norm": 1.7128407955169678,
      "learning_rate": 2.0924356884188812e-08,
      "loss": 1.1054,
      "step": 4780
    },
    {
      "epoch": 0.6721495852664137,
      "grad_norm": 1.6817388534545898,
      "learning_rate": 2.594987605694099e-08,
      "loss": 1.1167,
      "step": 4781
    },
    {
      "epoch": 0.6722901729228173,
      "grad_norm": 1.3260242938995361,
      "learning_rate": 3.151567537391653e-08,
      "loss": 1.1149,
      "step": 4782
    },
    {
      "epoch": 0.6724307605792211,
      "grad_norm": 1.6274785995483398,
      "learning_rate": 3.7621724756398135e-08,
      "loss": 1.0968,
      "step": 4783
    },
    {
      "epoch": 0.6725713482356249,
      "grad_norm": 1.423259973526001,
      "learning_rate": 4.426799120605951e-08,
      "loss": 1.0457,
      "step": 4784
    },
    {
      "epoch": 0.6727119358920287,
      "grad_norm": 1.5450254678726196,
      "learning_rate": 5.1454438805098595e-08,
      "loss": 1.1616,
      "step": 4785
    },
    {
      "epoch": 0.6728525235484325,
      "grad_norm": 1.4109565019607544,
      "learning_rate": 5.9181028716503994e-08,
      "loss": 1.2159,
      "step": 4786
    },
    {
      "epoch": 0.6729931112048362,
      "grad_norm": 2.057067394256592,
      "learning_rate": 6.744771918422155e-08,
      "loss": 1.1119,
      "step": 4787
    },
    {
      "epoch": 0.67313369886124,
      "grad_norm": 1.486525297164917,
      "learning_rate": 7.625446553335413e-08,
      "loss": 1.0102,
      "step": 4788
    },
    {
      "epoch": 0.6732742865176438,
      "grad_norm": 1.398446798324585,
      "learning_rate": 8.560122017042815e-08,
      "loss": 1.2336,
      "step": 4789
    },
    {
      "epoch": 0.6734148741740476,
      "grad_norm": 1.437994122505188,
      "learning_rate": 9.548793258372657e-08,
      "loss": 1.097,
      "step": 4790
    },
    {
      "epoch": 0.6735554618304513,
      "grad_norm": 1.5355684757232666,
      "learning_rate": 1.0591454934339995e-07,
      "loss": 1.0989,
      "step": 4791
    },
    {
      "epoch": 0.673696049486855,
      "grad_norm": 1.6747028827667236,
      "learning_rate": 1.1688101410191054e-07,
      "loss": 1.1462,
      "step": 4792
    },
    {
      "epoch": 0.6738366371432588,
      "grad_norm": 1.411176323890686,
      "learning_rate": 1.2838726759422105e-07,
      "loss": 1.0849,
      "step": 4793
    },
    {
      "epoch": 0.6739772247996626,
      "grad_norm": 1.586918830871582,
      "learning_rate": 1.4043324763822751e-07,
      "loss": 1.2526,
      "step": 4794
    },
    {
      "epoch": 0.6741178124560664,
      "grad_norm": 1.6840449571609497,
      "learning_rate": 1.5301888913497043e-07,
      "loss": 1.0416,
      "step": 4795
    },
    {
      "epoch": 0.6742584001124702,
      "grad_norm": 1.3676695823669434,
      "learning_rate": 1.6614412406907864e-07,
      "loss": 1.1633,
      "step": 4796
    },
    {
      "epoch": 0.6743989877688739,
      "grad_norm": 1.2136064767837524,
      "learning_rate": 1.798088815091137e-07,
      "loss": 1.1335,
      "step": 4797
    },
    {
      "epoch": 0.6745395754252776,
      "grad_norm": 1.3521567583084106,
      "learning_rate": 1.9401308760796934e-07,
      "loss": 0.9272,
      "step": 4798
    },
    {
      "epoch": 0.6746801630816814,
      "grad_norm": 1.639989972114563,
      "learning_rate": 2.0875666560316033e-07,
      "loss": 1.0096,
      "step": 4799
    },
    {
      "epoch": 0.6748207507380852,
      "grad_norm": 2.395617723464966,
      "learning_rate": 2.2403953581744407e-07,
      "loss": 1.1075,
      "step": 4800
    },
    {
      "epoch": 0.674961338394489,
      "grad_norm": 1.3341600894927979,
      "learning_rate": 2.3986161565904273e-07,
      "loss": 1.175,
      "step": 4801
    },
    {
      "epoch": 0.6751019260508927,
      "grad_norm": 1.3201384544372559,
      "learning_rate": 2.562228196222205e-07,
      "loss": 1.2108,
      "step": 4802
    },
    {
      "epoch": 0.6752425137072965,
      "grad_norm": 2.0240724086761475,
      "learning_rate": 2.7312305928769435e-07,
      "loss": 1.0369,
      "step": 4803
    },
    {
      "epoch": 0.6753831013637003,
      "grad_norm": 1.5471067428588867,
      "learning_rate": 2.9056224332312254e-07,
      "loss": 1.0714,
      "step": 4804
    },
    {
      "epoch": 0.6755236890201041,
      "grad_norm": 1.4260435104370117,
      "learning_rate": 3.0854027748359327e-07,
      "loss": 1.1911,
      "step": 4805
    },
    {
      "epoch": 0.6756642766765079,
      "grad_norm": 1.3810187578201294,
      "learning_rate": 3.2705706461219064e-07,
      "loss": 0.9947,
      "step": 4806
    },
    {
      "epoch": 0.6758048643329115,
      "grad_norm": 1.7384076118469238,
      "learning_rate": 3.461125046403502e-07,
      "loss": 1.0661,
      "step": 4807
    },
    {
      "epoch": 0.6759454519893153,
      "grad_norm": 1.3636423349380493,
      "learning_rate": 3.657064945886468e-07,
      "loss": 1.1536,
      "step": 4808
    },
    {
      "epoch": 0.6760860396457191,
      "grad_norm": 1.394768238067627,
      "learning_rate": 3.8583892856715044e-07,
      "loss": 0.9558,
      "step": 4809
    },
    {
      "epoch": 0.6762266273021229,
      "grad_norm": 1.4544743299484253,
      "learning_rate": 4.0650969777605854e-07,
      "loss": 1.0731,
      "step": 4810
    },
    {
      "epoch": 0.6763672149585267,
      "grad_norm": 1.448643445968628,
      "learning_rate": 4.2771869050636236e-07,
      "loss": 1.1305,
      "step": 4811
    },
    {
      "epoch": 0.6765078026149304,
      "grad_norm": 1.528549075126648,
      "learning_rate": 4.494657921403134e-07,
      "loss": 1.2954,
      "step": 4812
    },
    {
      "epoch": 0.6766483902713342,
      "grad_norm": 1.4048672914505005,
      "learning_rate": 4.717508851521446e-07,
      "loss": 1.0428,
      "step": 4813
    },
    {
      "epoch": 0.676788977927738,
      "grad_norm": 1.736676573753357,
      "learning_rate": 4.945738491086926e-07,
      "loss": 1.0756,
      "step": 4814
    },
    {
      "epoch": 0.6769295655841417,
      "grad_norm": 1.454715609550476,
      "learning_rate": 5.179345606699748e-07,
      "loss": 1.2006,
      "step": 4815
    },
    {
      "epoch": 0.6770701532405454,
      "grad_norm": 1.4325737953186035,
      "learning_rate": 5.418328935899553e-07,
      "loss": 1.0174,
      "step": 4816
    },
    {
      "epoch": 0.6772107408969492,
      "grad_norm": 1.606271505355835,
      "learning_rate": 5.662687187172111e-07,
      "loss": 1.0904,
      "step": 4817
    },
    {
      "epoch": 0.677351328553353,
      "grad_norm": 1.5662051439285278,
      "learning_rate": 5.912419039954986e-07,
      "loss": 1.1234,
      "step": 4818
    },
    {
      "epoch": 0.6774919162097568,
      "grad_norm": 1.3490843772888184,
      "learning_rate": 6.167523144646747e-07,
      "loss": 1.1351,
      "step": 4819
    },
    {
      "epoch": 0.6776325038661606,
      "grad_norm": 1.4742964506149292,
      "learning_rate": 6.427998122612855e-07,
      "loss": 0.9752,
      "step": 4820
    },
    {
      "epoch": 0.6777730915225643,
      "grad_norm": 1.4627928733825684,
      "learning_rate": 6.693842566193209e-07,
      "loss": 0.9268,
      "step": 4821
    },
    {
      "epoch": 0.677913679178968,
      "grad_norm": 1.3610886335372925,
      "learning_rate": 6.965055038710367e-07,
      "loss": 1.2903,
      "step": 4822
    },
    {
      "epoch": 0.6780542668353718,
      "grad_norm": 1.4165807962417603,
      "learning_rate": 7.241634074476866e-07,
      "loss": 1.0223,
      "step": 4823
    },
    {
      "epoch": 0.6781948544917756,
      "grad_norm": 1.4492093324661255,
      "learning_rate": 7.523578178802892e-07,
      "loss": 1.0594,
      "step": 4824
    },
    {
      "epoch": 0.6783354421481794,
      "grad_norm": 1.3083932399749756,
      "learning_rate": 7.810885828005821e-07,
      "loss": 1.1191,
      "step": 4825
    },
    {
      "epoch": 0.6784760298045831,
      "grad_norm": 1.3295800685882568,
      "learning_rate": 8.103555469415657e-07,
      "loss": 1.1115,
      "step": 4826
    },
    {
      "epoch": 0.6786166174609869,
      "grad_norm": 1.2235029935836792,
      "learning_rate": 8.40158552138659e-07,
      "loss": 1.1101,
      "step": 4827
    },
    {
      "epoch": 0.6787572051173907,
      "grad_norm": 1.4327434301376343,
      "learning_rate": 8.704974373303532e-07,
      "loss": 1.038,
      "step": 4828
    },
    {
      "epoch": 0.6788977927737945,
      "grad_norm": 1.3832898139953613,
      "learning_rate": 9.013720385590563e-07,
      "loss": 1.0429,
      "step": 4829
    },
    {
      "epoch": 0.6790383804301983,
      "grad_norm": 1.3806769847869873,
      "learning_rate": 9.327821889722033e-07,
      "loss": 1.13,
      "step": 4830
    },
    {
      "epoch": 0.6791789680866019,
      "grad_norm": 1.3715819120407104,
      "learning_rate": 9.64727718822922e-07,
      "loss": 1.072,
      "step": 4831
    },
    {
      "epoch": 0.6793195557430057,
      "grad_norm": 1.2648341655731201,
      "learning_rate": 9.972084554710437e-07,
      "loss": 1.0836,
      "step": 4832
    },
    {
      "epoch": 0.6794601433994095,
      "grad_norm": 1.4952607154846191,
      "learning_rate": 1.0302242233840576e-06,
      "loss": 0.9527,
      "step": 4833
    },
    {
      "epoch": 0.6796007310558133,
      "grad_norm": 1.6180636882781982,
      "learning_rate": 1.0637748441379991e-06,
      "loss": 1.1066,
      "step": 4834
    },
    {
      "epoch": 0.6797413187122171,
      "grad_norm": 1.424631953239441,
      "learning_rate": 1.097860136418516e-06,
      "loss": 1.0384,
      "step": 4835
    },
    {
      "epoch": 0.6798819063686208,
      "grad_norm": 1.2948923110961914,
      "learning_rate": 1.1324799160217448e-06,
      "loss": 1.0634,
      "step": 4836
    },
    {
      "epoch": 0.6800224940250246,
      "grad_norm": 1.404331088066101,
      "learning_rate": 1.1676339958552774e-06,
      "loss": 1.106,
      "step": 4837
    },
    {
      "epoch": 0.6801630816814284,
      "grad_norm": 1.3394215106964111,
      "learning_rate": 1.2033221859393594e-06,
      "loss": 1.2129,
      "step": 4838
    },
    {
      "epoch": 0.6803036693378322,
      "grad_norm": 1.4706966876983643,
      "learning_rate": 1.239544293407724e-06,
      "loss": 1.1644,
      "step": 4839
    },
    {
      "epoch": 0.6804442569942359,
      "grad_norm": 1.8317556381225586,
      "learning_rate": 1.2763001225087112e-06,
      "loss": 1.0744,
      "step": 4840
    },
    {
      "epoch": 0.6805848446506396,
      "grad_norm": 1.466599464416504,
      "learning_rate": 1.3135894746063248e-06,
      "loss": 1.0021,
      "step": 4841
    },
    {
      "epoch": 0.6807254323070434,
      "grad_norm": 1.6144347190856934,
      "learning_rate": 1.351412148181297e-06,
      "loss": 1.1979,
      "step": 4842
    },
    {
      "epoch": 0.6808660199634472,
      "grad_norm": 1.4847089052200317,
      "learning_rate": 1.3897679388322316e-06,
      "loss": 0.9999,
      "step": 4843
    },
    {
      "epoch": 0.681006607619851,
      "grad_norm": 1.3977535963058472,
      "learning_rate": 1.4286566392766155e-06,
      "loss": 0.9053,
      "step": 4844
    },
    {
      "epoch": 0.6811471952762548,
      "grad_norm": 1.4604250192642212,
      "learning_rate": 1.4680780393519166e-06,
      "loss": 1.2084,
      "step": 4845
    },
    {
      "epoch": 0.6812877829326585,
      "grad_norm": 1.298667311668396,
      "learning_rate": 1.5080319260169173e-06,
      "loss": 0.9536,
      "step": 4846
    },
    {
      "epoch": 0.6814283705890622,
      "grad_norm": 1.48374605178833,
      "learning_rate": 1.5485180833526903e-06,
      "loss": 1.1024,
      "step": 4847
    },
    {
      "epoch": 0.681568958245466,
      "grad_norm": 1.3797065019607544,
      "learning_rate": 1.589536292563687e-06,
      "loss": 1.0937,
      "step": 4848
    },
    {
      "epoch": 0.6817095459018698,
      "grad_norm": 1.4217805862426758,
      "learning_rate": 1.6310863319792368e-06,
      "loss": 1.0705,
      "step": 4849
    },
    {
      "epoch": 0.6818501335582736,
      "grad_norm": 1.6601895093917847,
      "learning_rate": 1.6731679770544351e-06,
      "loss": 0.9814,
      "step": 4850
    },
    {
      "epoch": 0.6819907212146773,
      "grad_norm": 1.4799463748931885,
      "learning_rate": 1.7157810003714858e-06,
      "loss": 0.9547,
      "step": 4851
    },
    {
      "epoch": 0.6821313088710811,
      "grad_norm": 1.3784663677215576,
      "learning_rate": 1.758925171640935e-06,
      "loss": 1.0076,
      "step": 4852
    },
    {
      "epoch": 0.6822718965274849,
      "grad_norm": 1.480427861213684,
      "learning_rate": 1.8026002577028356e-06,
      "loss": 0.9393,
      "step": 4853
    },
    {
      "epoch": 0.6824124841838887,
      "grad_norm": 1.6656707525253296,
      "learning_rate": 1.8468060225282136e-06,
      "loss": 1.1451,
      "step": 4854
    },
    {
      "epoch": 0.6825530718402925,
      "grad_norm": 1.343040943145752,
      "learning_rate": 1.8915422272200778e-06,
      "loss": 1.109,
      "step": 4855
    },
    {
      "epoch": 0.6826936594966961,
      "grad_norm": 1.7481480836868286,
      "learning_rate": 1.936808630014775e-06,
      "loss": 1.1319,
      "step": 4856
    },
    {
      "epoch": 0.6828342471530999,
      "grad_norm": 1.3604086637496948,
      "learning_rate": 1.9826049862835425e-06,
      "loss": 0.993,
      "step": 4857
    },
    {
      "epoch": 0.6829748348095037,
      "grad_norm": 1.3665908575057983,
      "learning_rate": 2.028931048533489e-06,
      "loss": 1.1978,
      "step": 4858
    },
    {
      "epoch": 0.6831154224659075,
      "grad_norm": 1.3871029615402222,
      "learning_rate": 2.0757865664091435e-06,
      "loss": 1.0338,
      "step": 4859
    },
    {
      "epoch": 0.6832560101223113,
      "grad_norm": 1.3779269456863403,
      "learning_rate": 2.123171286693737e-06,
      "loss": 1.2347,
      "step": 4860
    },
    {
      "epoch": 0.683396597778715,
      "grad_norm": 1.5447933673858643,
      "learning_rate": 2.1710849533105314e-06,
      "loss": 1.1246,
      "step": 4861
    },
    {
      "epoch": 0.6835371854351188,
      "grad_norm": 1.4880609512329102,
      "learning_rate": 2.219527307324398e-06,
      "loss": 1.2627,
      "step": 4862
    },
    {
      "epoch": 0.6836777730915226,
      "grad_norm": 1.5202250480651855,
      "learning_rate": 2.2684980869429717e-06,
      "loss": 1.0265,
      "step": 4863
    },
    {
      "epoch": 0.6838183607479263,
      "grad_norm": 1.786994457244873,
      "learning_rate": 2.3179970275180708e-06,
      "loss": 1.1445,
      "step": 4864
    },
    {
      "epoch": 0.6839589484043301,
      "grad_norm": 1.5646347999572754,
      "learning_rate": 2.3680238615474413e-06,
      "loss": 1.0806,
      "step": 4865
    },
    {
      "epoch": 0.6840995360607338,
      "grad_norm": 1.4246294498443604,
      "learning_rate": 2.4185783186758683e-06,
      "loss": 1.1352,
      "step": 4866
    },
    {
      "epoch": 0.6842401237171376,
      "grad_norm": 1.3302407264709473,
      "learning_rate": 2.46966012569666e-06,
      "loss": 1.0226,
      "step": 4867
    },
    {
      "epoch": 0.6843807113735414,
      "grad_norm": 1.4979499578475952,
      "learning_rate": 2.5212690065534284e-06,
      "loss": 1.0313,
      "step": 4868
    },
    {
      "epoch": 0.6845212990299452,
      "grad_norm": 1.3192657232284546,
      "learning_rate": 2.573404682341185e-06,
      "loss": 1.1196,
      "step": 4869
    },
    {
      "epoch": 0.684661886686349,
      "grad_norm": 1.2882829904556274,
      "learning_rate": 2.6260668713082306e-06,
      "loss": 1.1772,
      "step": 4870
    },
    {
      "epoch": 0.6848024743427527,
      "grad_norm": 1.2195637226104736,
      "learning_rate": 2.679255288857241e-06,
      "loss": 1.0661,
      "step": 4871
    },
    {
      "epoch": 0.6849430619991564,
      "grad_norm": 1.5533759593963623,
      "learning_rate": 2.7329696475471677e-06,
      "loss": 1.1361,
      "step": 4872
    },
    {
      "epoch": 0.6850836496555602,
      "grad_norm": 1.3703093528747559,
      "learning_rate": 2.787209657094736e-06,
      "loss": 0.9354,
      "step": 4873
    },
    {
      "epoch": 0.685224237311964,
      "grad_norm": 1.5518101453781128,
      "learning_rate": 2.841975024375909e-06,
      "loss": 1.0978,
      "step": 4874
    },
    {
      "epoch": 0.6853648249683678,
      "grad_norm": 1.6186693906784058,
      "learning_rate": 2.8972654534273223e-06,
      "loss": 1.1612,
      "step": 4875
    },
    {
      "epoch": 0.6855054126247715,
      "grad_norm": 1.4209234714508057,
      "learning_rate": 2.9530806454483804e-06,
      "loss": 0.9515,
      "step": 4876
    },
    {
      "epoch": 0.6856460002811753,
      "grad_norm": 1.3870513439178467,
      "learning_rate": 3.0094202988023012e-06,
      "loss": 0.9679,
      "step": 4877
    },
    {
      "epoch": 0.6857865879375791,
      "grad_norm": 1.9350088834762573,
      "learning_rate": 3.0662841090182583e-06,
      "loss": 1.0131,
      "step": 4878
    },
    {
      "epoch": 0.6859271755939829,
      "grad_norm": 1.4833595752716064,
      "learning_rate": 3.123671768792491e-06,
      "loss": 1.0554,
      "step": 4879
    },
    {
      "epoch": 0.6860677632503867,
      "grad_norm": 1.5023751258850098,
      "learning_rate": 3.181582967990404e-06,
      "loss": 1.1384,
      "step": 4880
    },
    {
      "epoch": 0.6862083509067903,
      "grad_norm": 1.7142322063446045,
      "learning_rate": 3.2400173936481426e-06,
      "loss": 1.0682,
      "step": 4881
    },
    {
      "epoch": 0.6863489385631941,
      "grad_norm": 1.638667106628418,
      "learning_rate": 3.298974729974169e-06,
      "loss": 1.2382,
      "step": 4882
    },
    {
      "epoch": 0.6864895262195979,
      "grad_norm": 1.3759998083114624,
      "learning_rate": 3.358454658350896e-06,
      "loss": 1.0849,
      "step": 4883
    },
    {
      "epoch": 0.6866301138760017,
      "grad_norm": 1.610538363456726,
      "learning_rate": 3.4184568573367825e-06,
      "loss": 1.0991,
      "step": 4884
    },
    {
      "epoch": 0.6867707015324055,
      "grad_norm": 1.5742664337158203,
      "learning_rate": 3.4789810026676805e-06,
      "loss": 1.1161,
      "step": 4885
    },
    {
      "epoch": 0.6869112891888092,
      "grad_norm": 1.3952248096466064,
      "learning_rate": 3.540026767258775e-06,
      "loss": 0.9984,
      "step": 4886
    },
    {
      "epoch": 0.687051876845213,
      "grad_norm": 1.6505029201507568,
      "learning_rate": 3.601593821206306e-06,
      "loss": 1.1329,
      "step": 4887
    },
    {
      "epoch": 0.6871924645016168,
      "grad_norm": 1.4024291038513184,
      "learning_rate": 3.663681831789323e-06,
      "loss": 1.1517,
      "step": 4888
    },
    {
      "epoch": 0.6873330521580205,
      "grad_norm": 1.4667930603027344,
      "learning_rate": 3.7262904634717042e-06,
      "loss": 1.0541,
      "step": 4889
    },
    {
      "epoch": 0.6874736398144243,
      "grad_norm": 1.6159905195236206,
      "learning_rate": 3.7894193779034915e-06,
      "loss": 0.9491,
      "step": 4890
    },
    {
      "epoch": 0.687614227470828,
      "grad_norm": 1.610817313194275,
      "learning_rate": 3.8530682339231624e-06,
      "loss": 1.1599,
      "step": 4891
    },
    {
      "epoch": 0.6877548151272318,
      "grad_norm": 1.3115578889846802,
      "learning_rate": 3.917236687559433e-06,
      "loss": 1.1472,
      "step": 4892
    },
    {
      "epoch": 0.6878954027836356,
      "grad_norm": 1.3280036449432373,
      "learning_rate": 3.981924392032898e-06,
      "loss": 1.0343,
      "step": 4893
    },
    {
      "epoch": 0.6880359904400394,
      "grad_norm": 1.4637577533721924,
      "learning_rate": 4.047130997757864e-06,
      "loss": 0.9651,
      "step": 4894
    },
    {
      "epoch": 0.6881765780964432,
      "grad_norm": 1.3350638151168823,
      "learning_rate": 4.112856152344702e-06,
      "loss": 0.9595,
      "step": 4895
    },
    {
      "epoch": 0.6883171657528468,
      "grad_norm": 1.706178069114685,
      "learning_rate": 4.179099500601158e-06,
      "loss": 0.964,
      "step": 4896
    },
    {
      "epoch": 0.6884577534092506,
      "grad_norm": 1.468935251235962,
      "learning_rate": 4.245860684534853e-06,
      "loss": 1.0509,
      "step": 4897
    },
    {
      "epoch": 0.6885983410656544,
      "grad_norm": 1.3223748207092285,
      "learning_rate": 4.313139343354589e-06,
      "loss": 0.8702,
      "step": 4898
    },
    {
      "epoch": 0.6887389287220582,
      "grad_norm": 1.3585405349731445,
      "learning_rate": 4.380935113472751e-06,
      "loss": 1.2057,
      "step": 4899
    },
    {
      "epoch": 0.688879516378462,
      "grad_norm": 1.9479950666427612,
      "learning_rate": 4.4492476285073135e-06,
      "loss": 0.9199,
      "step": 4900
    },
    {
      "epoch": 0.6890201040348657,
      "grad_norm": 1.6740117073059082,
      "learning_rate": 4.51807651928351e-06,
      "loss": 1.1804,
      "step": 4901
    },
    {
      "epoch": 0.6891606916912695,
      "grad_norm": 1.4004918336868286,
      "learning_rate": 4.587421413835824e-06,
      "loss": 1.1684,
      "step": 4902
    },
    {
      "epoch": 0.6893012793476733,
      "grad_norm": 1.275559902191162,
      "learning_rate": 4.657281937410462e-06,
      "loss": 1.2093,
      "step": 4903
    },
    {
      "epoch": 0.6894418670040771,
      "grad_norm": 1.2343933582305908,
      "learning_rate": 4.7276577124668934e-06,
      "loss": 1.1817,
      "step": 4904
    },
    {
      "epoch": 0.6895824546604808,
      "grad_norm": 1.6704858541488647,
      "learning_rate": 4.798548358680044e-06,
      "loss": 1.1479,
      "step": 4905
    },
    {
      "epoch": 0.6897230423168845,
      "grad_norm": 1.5787972211837769,
      "learning_rate": 4.869953492942447e-06,
      "loss": 1.0217,
      "step": 4906
    },
    {
      "epoch": 0.6898636299732883,
      "grad_norm": 1.471778392791748,
      "learning_rate": 4.941872729366126e-06,
      "loss": 0.998,
      "step": 4907
    },
    {
      "epoch": 0.6900042176296921,
      "grad_norm": 1.7716648578643799,
      "learning_rate": 5.014305679285047e-06,
      "loss": 0.9712,
      "step": 4908
    },
    {
      "epoch": 0.6901448052860959,
      "grad_norm": 1.3268769979476929,
      "learning_rate": 5.087251951256611e-06,
      "loss": 1.0978,
      "step": 4909
    },
    {
      "epoch": 0.6902853929424997,
      "grad_norm": 1.3656537532806396,
      "learning_rate": 5.1607111510643015e-06,
      "loss": 1.1487,
      "step": 4910
    },
    {
      "epoch": 0.6904259805989034,
      "grad_norm": 1.5626206398010254,
      "learning_rate": 5.2346828817197325e-06,
      "loss": 1.0883,
      "step": 4911
    },
    {
      "epoch": 0.6905665682553072,
      "grad_norm": 1.411900520324707,
      "learning_rate": 5.309166743464544e-06,
      "loss": 1.1233,
      "step": 4912
    },
    {
      "epoch": 0.690707155911711,
      "grad_norm": 1.370728611946106,
      "learning_rate": 5.384162333772769e-06,
      "loss": 1.0077,
      "step": 4913
    },
    {
      "epoch": 0.6908477435681147,
      "grad_norm": 1.7694209814071655,
      "learning_rate": 5.45966924735295e-06,
      "loss": 1.062,
      "step": 4914
    },
    {
      "epoch": 0.6909883312245185,
      "grad_norm": 1.4260045289993286,
      "learning_rate": 5.535687076150264e-06,
      "loss": 1.1748,
      "step": 4915
    },
    {
      "epoch": 0.6911289188809222,
      "grad_norm": 1.2463020086288452,
      "learning_rate": 5.6122154093490645e-06,
      "loss": 1.2363,
      "step": 4916
    },
    {
      "epoch": 0.691269506537326,
      "grad_norm": 1.3865596055984497,
      "learning_rate": 5.6892538333744525e-06,
      "loss": 1.1048,
      "step": 4917
    },
    {
      "epoch": 0.6914100941937298,
      "grad_norm": 1.3396198749542236,
      "learning_rate": 5.7668019318950935e-06,
      "loss": 1.0793,
      "step": 4918
    },
    {
      "epoch": 0.6915506818501336,
      "grad_norm": 1.3271517753601074,
      "learning_rate": 5.844859285825355e-06,
      "loss": 0.8818,
      "step": 4919
    },
    {
      "epoch": 0.6916912695065374,
      "grad_norm": 1.6613982915878296,
      "learning_rate": 5.923425473327349e-06,
      "loss": 1.1887,
      "step": 4920
    },
    {
      "epoch": 0.691831857162941,
      "grad_norm": 1.4619628190994263,
      "learning_rate": 6.002500069813366e-06,
      "loss": 1.0152,
      "step": 4921
    },
    {
      "epoch": 0.6919724448193448,
      "grad_norm": 1.462469458580017,
      "learning_rate": 6.082082647948173e-06,
      "loss": 1.0543,
      "step": 4922
    },
    {
      "epoch": 0.6921130324757486,
      "grad_norm": 1.2704381942749023,
      "learning_rate": 6.16217277765131e-06,
      "loss": 1.1018,
      "step": 4923
    },
    {
      "epoch": 0.6922536201321524,
      "grad_norm": 1.4149150848388672,
      "learning_rate": 6.242770026099387e-06,
      "loss": 1.018,
      "step": 4924
    },
    {
      "epoch": 0.6923942077885562,
      "grad_norm": 1.5590381622314453,
      "learning_rate": 6.323873957728432e-06,
      "loss": 1.1326,
      "step": 4925
    },
    {
      "epoch": 0.6925347954449599,
      "grad_norm": 1.3625110387802124,
      "learning_rate": 6.405484134236217e-06,
      "loss": 1.1023,
      "step": 4926
    },
    {
      "epoch": 0.6926753831013637,
      "grad_norm": 1.3481576442718506,
      "learning_rate": 6.487600114584902e-06,
      "loss": 1.2319,
      "step": 4927
    },
    {
      "epoch": 0.6928159707577675,
      "grad_norm": 1.5925836563110352,
      "learning_rate": 6.570221455002801e-06,
      "loss": 1.0656,
      "step": 4928
    },
    {
      "epoch": 0.6929565584141713,
      "grad_norm": 1.3884526491165161,
      "learning_rate": 6.653347708987345e-06,
      "loss": 1.1777,
      "step": 4929
    },
    {
      "epoch": 0.693097146070575,
      "grad_norm": 1.2569427490234375,
      "learning_rate": 6.736978427307439e-06,
      "loss": 1.1852,
      "step": 4930
    },
    {
      "epoch": 0.6932377337269787,
      "grad_norm": 1.6543160676956177,
      "learning_rate": 6.821113158005587e-06,
      "loss": 1.2967,
      "step": 4931
    },
    {
      "epoch": 0.6933783213833825,
      "grad_norm": 1.375723123550415,
      "learning_rate": 6.9057514464005435e-06,
      "loss": 1.1711,
      "step": 4932
    },
    {
      "epoch": 0.6935189090397863,
      "grad_norm": 1.5626740455627441,
      "learning_rate": 6.990892835089746e-06,
      "loss": 1.0695,
      "step": 4933
    },
    {
      "epoch": 0.6936594966961901,
      "grad_norm": 1.4267864227294922,
      "learning_rate": 7.0765368639517105e-06,
      "loss": 1.0664,
      "step": 4934
    },
    {
      "epoch": 0.6938000843525939,
      "grad_norm": 1.713644027709961,
      "learning_rate": 7.1626830701488566e-06,
      "loss": 1.0623,
      "step": 4935
    },
    {
      "epoch": 0.6939406720089976,
      "grad_norm": 1.309842824935913,
      "learning_rate": 7.249330988129321e-06,
      "loss": 1.0045,
      "step": 4936
    },
    {
      "epoch": 0.6940812596654014,
      "grad_norm": 1.3423738479614258,
      "learning_rate": 7.336480149630087e-06,
      "loss": 1.1984,
      "step": 4937
    },
    {
      "epoch": 0.6942218473218051,
      "grad_norm": 1.5318446159362793,
      "learning_rate": 7.424130083679459e-06,
      "loss": 0.9874,
      "step": 4938
    },
    {
      "epoch": 0.6943624349782089,
      "grad_norm": 1.309071660041809,
      "learning_rate": 7.5122803165992735e-06,
      "loss": 1.1949,
      "step": 4939
    },
    {
      "epoch": 0.6945030226346127,
      "grad_norm": 1.7288638353347778,
      "learning_rate": 7.600930372007698e-06,
      "loss": 1.128,
      "step": 4940
    },
    {
      "epoch": 0.6946436102910164,
      "grad_norm": 1.4791160821914673,
      "learning_rate": 7.690079770821756e-06,
      "loss": 1.0597,
      "step": 4941
    },
    {
      "epoch": 0.6947841979474202,
      "grad_norm": 1.5862897634506226,
      "learning_rate": 7.779728031259859e-06,
      "loss": 1.0841,
      "step": 4942
    },
    {
      "epoch": 0.694924785603824,
      "grad_norm": 1.4965940713882446,
      "learning_rate": 7.869874668844745e-06,
      "loss": 1.054,
      "step": 4943
    },
    {
      "epoch": 0.6950653732602278,
      "grad_norm": 1.4169597625732422,
      "learning_rate": 7.960519196405414e-06,
      "loss": 0.9758,
      "step": 4944
    },
    {
      "epoch": 0.6952059609166316,
      "grad_norm": 1.3030412197113037,
      "learning_rate": 8.051661124080335e-06,
      "loss": 1.1896,
      "step": 4945
    },
    {
      "epoch": 0.6953465485730352,
      "grad_norm": 1.3293075561523438,
      "learning_rate": 8.143299959320195e-06,
      "loss": 1.1278,
      "step": 4946
    },
    {
      "epoch": 0.695487136229439,
      "grad_norm": 1.5375946760177612,
      "learning_rate": 8.23543520688974e-06,
      "loss": 1.1875,
      "step": 4947
    },
    {
      "epoch": 0.6956277238858428,
      "grad_norm": 1.407959222793579,
      "learning_rate": 8.328066368871545e-06,
      "loss": 1.094,
      "step": 4948
    },
    {
      "epoch": 0.6957683115422466,
      "grad_norm": 1.3725889921188354,
      "learning_rate": 8.421192944667855e-06,
      "loss": 1.1208,
      "step": 4949
    },
    {
      "epoch": 0.6959088991986504,
      "grad_norm": 1.5156421661376953,
      "learning_rate": 8.51481443100367e-06,
      "loss": 1.121,
      "step": 4950
    },
    {
      "epoch": 0.6960494868550541,
      "grad_norm": 1.2263962030410767,
      "learning_rate": 8.608930321929398e-06,
      "loss": 1.2018,
      "step": 4951
    },
    {
      "epoch": 0.6961900745114579,
      "grad_norm": 1.7076478004455566,
      "learning_rate": 8.703540108823571e-06,
      "loss": 0.887,
      "step": 4952
    },
    {
      "epoch": 0.6963306621678617,
      "grad_norm": 1.2285683155059814,
      "learning_rate": 8.798643280395558e-06,
      "loss": 1.0297,
      "step": 4953
    },
    {
      "epoch": 0.6964712498242654,
      "grad_norm": 1.5435336828231812,
      "learning_rate": 8.894239322688635e-06,
      "loss": 0.9392,
      "step": 4954
    },
    {
      "epoch": 0.6966118374806692,
      "grad_norm": 1.5103533267974854,
      "learning_rate": 8.990327719082037e-06,
      "loss": 1.037,
      "step": 4955
    },
    {
      "epoch": 0.6967524251370729,
      "grad_norm": 1.5330473184585571,
      "learning_rate": 9.086907950294688e-06,
      "loss": 1.2102,
      "step": 4956
    },
    {
      "epoch": 0.6968930127934767,
      "grad_norm": 1.408018946647644,
      "learning_rate": 9.183979494387263e-06,
      "loss": 0.9182,
      "step": 4957
    },
    {
      "epoch": 0.6970336004498805,
      "grad_norm": 1.4160250425338745,
      "learning_rate": 9.281541826765361e-06,
      "loss": 1.2034,
      "step": 4958
    },
    {
      "epoch": 0.6971741881062843,
      "grad_norm": 1.3084138631820679,
      "learning_rate": 9.379594420182236e-06,
      "loss": 0.893,
      "step": 4959
    },
    {
      "epoch": 0.6973147757626881,
      "grad_norm": 1.3996903896331787,
      "learning_rate": 9.478136744741694e-06,
      "loss": 1.1761,
      "step": 4960
    },
    {
      "epoch": 0.6974553634190918,
      "grad_norm": 1.3063985109329224,
      "learning_rate": 9.577168267900849e-06,
      "loss": 1.1896,
      "step": 4961
    },
    {
      "epoch": 0.6975959510754955,
      "grad_norm": 1.208673119544983,
      "learning_rate": 9.676688454473404e-06,
      "loss": 0.9968,
      "step": 4962
    },
    {
      "epoch": 0.6977365387318993,
      "grad_norm": 1.190706491470337,
      "learning_rate": 9.776696766631665e-06,
      "loss": 1.2813,
      "step": 4963
    },
    {
      "epoch": 0.6978771263883031,
      "grad_norm": 1.3440732955932617,
      "learning_rate": 9.877192663910573e-06,
      "loss": 1.1604,
      "step": 4964
    },
    {
      "epoch": 0.6980177140447069,
      "grad_norm": 1.575817584991455,
      "learning_rate": 9.978175603209816e-06,
      "loss": 0.9985,
      "step": 4965
    },
    {
      "epoch": 0.6981583017011106,
      "grad_norm": 1.6623871326446533,
      "learning_rate": 1.0079645038796759e-05,
      "loss": 1.0497,
      "step": 4966
    },
    {
      "epoch": 0.6982988893575144,
      "grad_norm": 1.7487361431121826,
      "learning_rate": 1.0181600422310134e-05,
      "loss": 1.1212,
      "step": 4967
    },
    {
      "epoch": 0.6984394770139182,
      "grad_norm": 1.443806767463684,
      "learning_rate": 1.0284041202762185e-05,
      "loss": 1.0074,
      "step": 4968
    },
    {
      "epoch": 0.698580064670322,
      "grad_norm": 1.591076374053955,
      "learning_rate": 1.0386966826542056e-05,
      "loss": 1.0727,
      "step": 4969
    },
    {
      "epoch": 0.6987206523267258,
      "grad_norm": 1.5440260171890259,
      "learning_rate": 1.0490376737418706e-05,
      "loss": 1.1079,
      "step": 4970
    },
    {
      "epoch": 0.6988612399831294,
      "grad_norm": 1.3839133977890015,
      "learning_rate": 1.059427037654388e-05,
      "loss": 1.0365,
      "step": 4971
    },
    {
      "epoch": 0.6990018276395332,
      "grad_norm": 1.5883028507232666,
      "learning_rate": 1.0698647182455113e-05,
      "loss": 0.9077,
      "step": 4972
    },
    {
      "epoch": 0.699142415295937,
      "grad_norm": 1.5616477727890015,
      "learning_rate": 1.0803506591079105e-05,
      "loss": 1.1939,
      "step": 4973
    },
    {
      "epoch": 0.6992830029523408,
      "grad_norm": 1.4395846128463745,
      "learning_rate": 1.090884803573392e-05,
      "loss": 1.1671,
      "step": 4974
    },
    {
      "epoch": 0.6994235906087446,
      "grad_norm": 1.4219413995742798,
      "learning_rate": 1.1014670947133154e-05,
      "loss": 1.1285,
      "step": 4975
    },
    {
      "epoch": 0.6995641782651483,
      "grad_norm": 1.2796146869659424,
      "learning_rate": 1.1120974753388147e-05,
      "loss": 1.0766,
      "step": 4976
    },
    {
      "epoch": 0.6997047659215521,
      "grad_norm": 1.3047541379928589,
      "learning_rate": 1.1227758880011475e-05,
      "loss": 1.0575,
      "step": 4977
    },
    {
      "epoch": 0.6998453535779559,
      "grad_norm": 1.451591968536377,
      "learning_rate": 1.1335022749919933e-05,
      "loss": 1.0217,
      "step": 4978
    },
    {
      "epoch": 0.6999859412343596,
      "grad_norm": 1.3717801570892334,
      "learning_rate": 1.1442765783437714e-05,
      "loss": 1.176,
      "step": 4979
    },
    {
      "epoch": 0.7001265288907634,
      "grad_norm": 1.3661319017410278,
      "learning_rate": 1.1550987398299418e-05,
      "loss": 1.0162,
      "step": 4980
    },
    {
      "epoch": 0.7002671165471671,
      "grad_norm": 1.8349156379699707,
      "learning_rate": 1.165968700965363e-05,
      "loss": 1.1374,
      "step": 4981
    },
    {
      "epoch": 0.7004077042035709,
      "grad_norm": 1.5677218437194824,
      "learning_rate": 1.1768864030065174e-05,
      "loss": 1.1755,
      "step": 4982
    },
    {
      "epoch": 0.7005482918599747,
      "grad_norm": 1.4944318532943726,
      "learning_rate": 1.1878517869519402e-05,
      "loss": 1.1223,
      "step": 4983
    },
    {
      "epoch": 0.7006888795163785,
      "grad_norm": 1.523879885673523,
      "learning_rate": 1.1988647935424646e-05,
      "loss": 0.9827,
      "step": 4984
    },
    {
      "epoch": 0.7008294671727823,
      "grad_norm": 1.4091252088546753,
      "learning_rate": 1.2099253632615304e-05,
      "loss": 1.163,
      "step": 4985
    },
    {
      "epoch": 0.700970054829186,
      "grad_norm": 1.3410515785217285,
      "learning_rate": 1.2210334363355913e-05,
      "loss": 1.152,
      "step": 4986
    },
    {
      "epoch": 0.7011106424855897,
      "grad_norm": 1.3381253480911255,
      "learning_rate": 1.2321889527343478e-05,
      "loss": 1.0834,
      "step": 4987
    },
    {
      "epoch": 0.7012512301419935,
      "grad_norm": 1.248390793800354,
      "learning_rate": 1.243391852171114e-05,
      "loss": 1.1429,
      "step": 4988
    },
    {
      "epoch": 0.7013918177983973,
      "grad_norm": 1.5188485383987427,
      "learning_rate": 1.2546420741031396e-05,
      "loss": 0.9806,
      "step": 4989
    },
    {
      "epoch": 0.7015324054548011,
      "grad_norm": 1.5692591667175293,
      "learning_rate": 1.2659395577319221e-05,
      "loss": 1.2571,
      "step": 4990
    },
    {
      "epoch": 0.7016729931112048,
      "grad_norm": 1.438883900642395,
      "learning_rate": 1.2772842420035769e-05,
      "loss": 0.9191,
      "step": 4991
    },
    {
      "epoch": 0.7018135807676086,
      "grad_norm": 1.63066828250885,
      "learning_rate": 1.2886760656091145e-05,
      "loss": 0.9441,
      "step": 4992
    },
    {
      "epoch": 0.7019541684240124,
      "grad_norm": 1.4363404512405396,
      "learning_rate": 1.3001149669847757e-05,
      "loss": 1.0677,
      "step": 4993
    },
    {
      "epoch": 0.7020947560804162,
      "grad_norm": 1.4282046556472778,
      "learning_rate": 1.3116008843124295e-05,
      "loss": 0.9988,
      "step": 4994
    },
    {
      "epoch": 0.70223534373682,
      "grad_norm": 1.3177841901779175,
      "learning_rate": 1.3231337555198287e-05,
      "loss": 1.1169,
      "step": 4995
    },
    {
      "epoch": 0.7023759313932236,
      "grad_norm": 1.5245767831802368,
      "learning_rate": 1.3347135182809888e-05,
      "loss": 1.1572,
      "step": 4996
    },
    {
      "epoch": 0.7025165190496274,
      "grad_norm": 1.398707628250122,
      "learning_rate": 1.3463401100165097e-05,
      "loss": 1.2045,
      "step": 4997
    },
    {
      "epoch": 0.7026571067060312,
      "grad_norm": 1.5760347843170166,
      "learning_rate": 1.3580134678939105e-05,
      "loss": 1.0518,
      "step": 4998
    },
    {
      "epoch": 0.702797694362435,
      "grad_norm": 2.0028562545776367,
      "learning_rate": 1.3697335288280055e-05,
      "loss": 1.1424,
      "step": 4999
    },
    {
      "epoch": 0.7029382820188388,
      "grad_norm": 1.320900797843933,
      "learning_rate": 1.3815002294811963e-05,
      "loss": 1.0824,
      "step": 5000
    },
    {
      "epoch": 0.7029382820188388,
      "eval_loss": 1.141364574432373,
      "eval_runtime": 771.192,
      "eval_samples_per_second": 16.398,
      "eval_steps_per_second": 8.199,
      "step": 5000
    },
    {
      "epoch": 0.7030788696752425,
      "grad_norm": 2.921252965927124,
      "learning_rate": 1.3933135062638059e-05,
      "loss": 1.0814,
      "step": 5001
    },
    {
      "epoch": 0.7032194573316463,
      "grad_norm": 1.404411792755127,
      "learning_rate": 1.4051732953345064e-05,
      "loss": 0.9691,
      "step": 5002
    },
    {
      "epoch": 0.70336004498805,
      "grad_norm": 1.3602023124694824,
      "learning_rate": 1.4170795326005815e-05,
      "loss": 1.0298,
      "step": 5003
    },
    {
      "epoch": 0.7035006326444538,
      "grad_norm": 1.749941110610962,
      "learning_rate": 1.4290321537182693e-05,
      "loss": 1.2724,
      "step": 5004
    },
    {
      "epoch": 0.7036412203008576,
      "grad_norm": 1.4249433279037476,
      "learning_rate": 1.4410310940931971e-05,
      "loss": 1.1439,
      "step": 5005
    },
    {
      "epoch": 0.7037818079572613,
      "grad_norm": 1.5607659816741943,
      "learning_rate": 1.453076288880637e-05,
      "loss": 0.9836,
      "step": 5006
    },
    {
      "epoch": 0.7039223956136651,
      "grad_norm": 1.560676097869873,
      "learning_rate": 1.4651676729858999e-05,
      "loss": 1.1375,
      "step": 5007
    },
    {
      "epoch": 0.7040629832700689,
      "grad_norm": 1.3218481540679932,
      "learning_rate": 1.4773051810646821e-05,
      "loss": 1.1077,
      "step": 5008
    },
    {
      "epoch": 0.7042035709264727,
      "grad_norm": 1.455933690071106,
      "learning_rate": 1.4894887475234066e-05,
      "loss": 0.9806,
      "step": 5009
    },
    {
      "epoch": 0.7043441585828765,
      "grad_norm": 1.4135358333587646,
      "learning_rate": 1.501718306519615e-05,
      "loss": 1.2568,
      "step": 5010
    },
    {
      "epoch": 0.7044847462392801,
      "grad_norm": 1.3584680557250977,
      "learning_rate": 1.5139937919622793e-05,
      "loss": 1.1591,
      "step": 5011
    },
    {
      "epoch": 0.7046253338956839,
      "grad_norm": 1.4605896472930908,
      "learning_rate": 1.5263151375121442e-05,
      "loss": 1.1445,
      "step": 5012
    },
    {
      "epoch": 0.7047659215520877,
      "grad_norm": 1.4781129360198975,
      "learning_rate": 1.5386822765821773e-05,
      "loss": 1.1332,
      "step": 5013
    },
    {
      "epoch": 0.7049065092084915,
      "grad_norm": 1.4243106842041016,
      "learning_rate": 1.5510951423378272e-05,
      "loss": 1.0218,
      "step": 5014
    },
    {
      "epoch": 0.7050470968648953,
      "grad_norm": 1.5827454328536987,
      "learning_rate": 1.563553667697437e-05,
      "loss": 0.9792,
      "step": 5015
    },
    {
      "epoch": 0.705187684521299,
      "grad_norm": 1.4305530786514282,
      "learning_rate": 1.5760577853325975e-05,
      "loss": 1.0405,
      "step": 5016
    },
    {
      "epoch": 0.7053282721777028,
      "grad_norm": 1.9864399433135986,
      "learning_rate": 1.588607427668498e-05,
      "loss": 0.9106,
      "step": 5017
    },
    {
      "epoch": 0.7054688598341066,
      "grad_norm": 1.4926599264144897,
      "learning_rate": 1.6012025268843335e-05,
      "loss": 1.044,
      "step": 5018
    },
    {
      "epoch": 0.7056094474905104,
      "grad_norm": 1.4765375852584839,
      "learning_rate": 1.6138430149136196e-05,
      "loss": 1.1018,
      "step": 5019
    },
    {
      "epoch": 0.7057500351469141,
      "grad_norm": 1.2326396703720093,
      "learning_rate": 1.6265288234445542e-05,
      "loss": 1.2444,
      "step": 5020
    },
    {
      "epoch": 0.7058906228033178,
      "grad_norm": 1.570879578590393,
      "learning_rate": 1.639259883920471e-05,
      "loss": 1.0997,
      "step": 5021
    },
    {
      "epoch": 0.7060312104597216,
      "grad_norm": 1.5067118406295776,
      "learning_rate": 1.6520361275401242e-05,
      "loss": 1.1035,
      "step": 5022
    },
    {
      "epoch": 0.7061717981161254,
      "grad_norm": 1.4903509616851807,
      "learning_rate": 1.66485748525806e-05,
      "loss": 1.1649,
      "step": 5023
    },
    {
      "epoch": 0.7063123857725292,
      "grad_norm": 1.4378719329833984,
      "learning_rate": 1.6777238877850764e-05,
      "loss": 1.0717,
      "step": 5024
    },
    {
      "epoch": 0.706452973428933,
      "grad_norm": 1.4192732572555542,
      "learning_rate": 1.690635265588494e-05,
      "loss": 1.0854,
      "step": 5025
    },
    {
      "epoch": 0.7065935610853367,
      "grad_norm": 1.4854215383529663,
      "learning_rate": 1.7035915488926247e-05,
      "loss": 0.9708,
      "step": 5026
    },
    {
      "epoch": 0.7067341487417405,
      "grad_norm": 1.8635921478271484,
      "learning_rate": 1.7165926676790413e-05,
      "loss": 1.034,
      "step": 5027
    },
    {
      "epoch": 0.7068747363981442,
      "grad_norm": 1.4041444063186646,
      "learning_rate": 1.7296385516870484e-05,
      "loss": 1.1052,
      "step": 5028
    },
    {
      "epoch": 0.707015324054548,
      "grad_norm": 1.3712893724441528,
      "learning_rate": 1.7427291304140502e-05,
      "loss": 1.0678,
      "step": 5029
    },
    {
      "epoch": 0.7071559117109518,
      "grad_norm": 1.4774610996246338,
      "learning_rate": 1.755864333115892e-05,
      "loss": 1.1693,
      "step": 5030
    },
    {
      "epoch": 0.7072964993673555,
      "grad_norm": 1.4219928979873657,
      "learning_rate": 1.7690440888072292e-05,
      "loss": 1.0206,
      "step": 5031
    },
    {
      "epoch": 0.7074370870237593,
      "grad_norm": 1.7145816087722778,
      "learning_rate": 1.7822683262620077e-05,
      "loss": 1.2017,
      "step": 5032
    },
    {
      "epoch": 0.7075776746801631,
      "grad_norm": 1.6930980682373047,
      "learning_rate": 1.7955369740137307e-05,
      "loss": 1.0946,
      "step": 5033
    },
    {
      "epoch": 0.7077182623365669,
      "grad_norm": 1.4460967779159546,
      "learning_rate": 1.8088499603559515e-05,
      "loss": 0.9596,
      "step": 5034
    },
    {
      "epoch": 0.7078588499929707,
      "grad_norm": 1.4091733694076538,
      "learning_rate": 1.8222072133425504e-05,
      "loss": 1.2007,
      "step": 5035
    },
    {
      "epoch": 0.7079994376493743,
      "grad_norm": 1.4032169580459595,
      "learning_rate": 1.8356086607882082e-05,
      "loss": 0.9837,
      "step": 5036
    },
    {
      "epoch": 0.7081400253057781,
      "grad_norm": 1.2292309999465942,
      "learning_rate": 1.849054230268792e-05,
      "loss": 1.1476,
      "step": 5037
    },
    {
      "epoch": 0.7082806129621819,
      "grad_norm": 1.7575289011001587,
      "learning_rate": 1.862543849121703e-05,
      "loss": 1.1933,
      "step": 5038
    },
    {
      "epoch": 0.7084212006185857,
      "grad_norm": 1.3454101085662842,
      "learning_rate": 1.8760774444462603e-05,
      "loss": 1.3912,
      "step": 5039
    },
    {
      "epoch": 0.7085617882749895,
      "grad_norm": 1.3507156372070312,
      "learning_rate": 1.88965494310419e-05,
      "loss": 1.0795,
      "step": 5040
    },
    {
      "epoch": 0.7087023759313932,
      "grad_norm": 1.691261887550354,
      "learning_rate": 1.903276271719908e-05,
      "loss": 1.0935,
      "step": 5041
    },
    {
      "epoch": 0.708842963587797,
      "grad_norm": 1.258744239807129,
      "learning_rate": 1.9169413566809802e-05,
      "loss": 1.1848,
      "step": 5042
    },
    {
      "epoch": 0.7089835512442008,
      "grad_norm": 1.3457298278808594,
      "learning_rate": 1.930650124138501e-05,
      "loss": 1.0482,
      "step": 5043
    },
    {
      "epoch": 0.7091241389006046,
      "grad_norm": 1.5675537586212158,
      "learning_rate": 1.944402500007487e-05,
      "loss": 1.0465,
      "step": 5044
    },
    {
      "epoch": 0.7092647265570083,
      "grad_norm": 1.7646533250808716,
      "learning_rate": 1.958198409967329e-05,
      "loss": 0.996,
      "step": 5045
    },
    {
      "epoch": 0.709405314213412,
      "grad_norm": 1.2808046340942383,
      "learning_rate": 1.9720377794620824e-05,
      "loss": 0.9918,
      "step": 5046
    },
    {
      "epoch": 0.7095459018698158,
      "grad_norm": 1.482823371887207,
      "learning_rate": 1.9859205337009766e-05,
      "loss": 0.9829,
      "step": 5047
    },
    {
      "epoch": 0.7096864895262196,
      "grad_norm": 1.3885877132415771,
      "learning_rate": 1.9998465976587988e-05,
      "loss": 1.2135,
      "step": 5048
    },
    {
      "epoch": 0.7098270771826234,
      "grad_norm": 1.5537773370742798,
      "learning_rate": 2.013815896076259e-05,
      "loss": 1.023,
      "step": 5049
    },
    {
      "epoch": 0.7099676648390272,
      "grad_norm": 1.4322688579559326,
      "learning_rate": 2.0278283534603915e-05,
      "loss": 0.9707,
      "step": 5050
    },
    {
      "epoch": 0.7101082524954309,
      "grad_norm": 1.4919099807739258,
      "learning_rate": 2.0418838940850515e-05,
      "loss": 1.1296,
      "step": 5051
    },
    {
      "epoch": 0.7102488401518346,
      "grad_norm": 1.4127334356307983,
      "learning_rate": 2.055982441991211e-05,
      "loss": 1.0238,
      "step": 5052
    },
    {
      "epoch": 0.7103894278082384,
      "grad_norm": 1.2954659461975098,
      "learning_rate": 2.0701239209874755e-05,
      "loss": 1.1308,
      "step": 5053
    },
    {
      "epoch": 0.7105300154646422,
      "grad_norm": 1.5916810035705566,
      "learning_rate": 2.084308254650379e-05,
      "loss": 1.0773,
      "step": 5054
    },
    {
      "epoch": 0.710670603121046,
      "grad_norm": 1.2937370538711548,
      "learning_rate": 2.0985353663248907e-05,
      "loss": 1.0939,
      "step": 5055
    },
    {
      "epoch": 0.7108111907774497,
      "grad_norm": 1.322975993156433,
      "learning_rate": 2.1128051791248192e-05,
      "loss": 1.0298,
      "step": 5056
    },
    {
      "epoch": 0.7109517784338535,
      "grad_norm": 1.3694440126419067,
      "learning_rate": 2.1271176159331907e-05,
      "loss": 1.0945,
      "step": 5057
    },
    {
      "epoch": 0.7110923660902573,
      "grad_norm": 1.310219407081604,
      "learning_rate": 2.141472599402644e-05,
      "loss": 1.0216,
      "step": 5058
    },
    {
      "epoch": 0.7112329537466611,
      "grad_norm": 1.4444305896759033,
      "learning_rate": 2.1558700519559572e-05,
      "loss": 1.0777,
      "step": 5059
    },
    {
      "epoch": 0.7113735414030649,
      "grad_norm": 1.2830818891525269,
      "learning_rate": 2.17030989578635e-05,
      "loss": 1.0599,
      "step": 5060
    },
    {
      "epoch": 0.7115141290594685,
      "grad_norm": 1.2233829498291016,
      "learning_rate": 2.1847920528579634e-05,
      "loss": 1.0878,
      "step": 5061
    },
    {
      "epoch": 0.7116547167158723,
      "grad_norm": 1.6383781433105469,
      "learning_rate": 2.19931644490627e-05,
      "loss": 1.0903,
      "step": 5062
    },
    {
      "epoch": 0.7117953043722761,
      "grad_norm": 1.5202101469039917,
      "learning_rate": 2.2138829934384807e-05,
      "loss": 1.1201,
      "step": 5063
    },
    {
      "epoch": 0.7119358920286799,
      "grad_norm": 1.4271974563598633,
      "learning_rate": 2.2284916197340345e-05,
      "loss": 1.0491,
      "step": 5064
    },
    {
      "epoch": 0.7120764796850837,
      "grad_norm": 1.3867093324661255,
      "learning_rate": 2.2431422448449014e-05,
      "loss": 1.0811,
      "step": 5065
    },
    {
      "epoch": 0.7122170673414874,
      "grad_norm": 1.3822288513183594,
      "learning_rate": 2.2578347895961194e-05,
      "loss": 0.9684,
      "step": 5066
    },
    {
      "epoch": 0.7123576549978912,
      "grad_norm": 1.500529408454895,
      "learning_rate": 2.272569174586203e-05,
      "loss": 1.2278,
      "step": 5067
    },
    {
      "epoch": 0.712498242654295,
      "grad_norm": 1.4182180166244507,
      "learning_rate": 2.2873453201875218e-05,
      "loss": 1.0373,
      "step": 5068
    },
    {
      "epoch": 0.7126388303106987,
      "grad_norm": 1.4086917638778687,
      "learning_rate": 2.302163146546772e-05,
      "loss": 1.2571,
      "step": 5069
    },
    {
      "epoch": 0.7127794179671025,
      "grad_norm": 1.4072638750076294,
      "learning_rate": 2.317022573585401e-05,
      "loss": 1.0523,
      "step": 5070
    },
    {
      "epoch": 0.7129200056235062,
      "grad_norm": 1.6621103286743164,
      "learning_rate": 2.3319235210000245e-05,
      "loss": 1.0472,
      "step": 5071
    },
    {
      "epoch": 0.71306059327991,
      "grad_norm": 1.6470837593078613,
      "learning_rate": 2.3468659082629208e-05,
      "loss": 1.0403,
      "step": 5072
    },
    {
      "epoch": 0.7132011809363138,
      "grad_norm": 1.7782124280929565,
      "learning_rate": 2.3618496546223467e-05,
      "loss": 1.1622,
      "step": 5073
    },
    {
      "epoch": 0.7133417685927176,
      "grad_norm": 1.3854808807373047,
      "learning_rate": 2.376874679103085e-05,
      "loss": 0.9912,
      "step": 5074
    },
    {
      "epoch": 0.7134823562491214,
      "grad_norm": 1.9716534614562988,
      "learning_rate": 2.3919409005068626e-05,
      "loss": 0.9869,
      "step": 5075
    },
    {
      "epoch": 0.713622943905525,
      "grad_norm": 1.594011902809143,
      "learning_rate": 2.407048237412738e-05,
      "loss": 1.0166,
      "step": 5076
    },
    {
      "epoch": 0.7137635315619288,
      "grad_norm": 1.4897948503494263,
      "learning_rate": 2.4221966081775817e-05,
      "loss": 1.1136,
      "step": 5077
    },
    {
      "epoch": 0.7139041192183326,
      "grad_norm": 1.3209940195083618,
      "learning_rate": 2.437385930936511e-05,
      "loss": 1.3008,
      "step": 5078
    },
    {
      "epoch": 0.7140447068747364,
      "grad_norm": 1.4511011838912964,
      "learning_rate": 2.4526161236033295e-05,
      "loss": 0.9831,
      "step": 5079
    },
    {
      "epoch": 0.7141852945311402,
      "grad_norm": 1.413233757019043,
      "learning_rate": 2.4678871038709727e-05,
      "loss": 1.084,
      "step": 5080
    },
    {
      "epoch": 0.7143258821875439,
      "grad_norm": 1.5195097923278809,
      "learning_rate": 2.48319878921195e-05,
      "loss": 0.9602,
      "step": 5081
    },
    {
      "epoch": 0.7144664698439477,
      "grad_norm": 1.5137646198272705,
      "learning_rate": 2.498551096878782e-05,
      "loss": 1.072,
      "step": 5082
    },
    {
      "epoch": 0.7146070575003515,
      "grad_norm": 1.4378291368484497,
      "learning_rate": 2.5139439439045022e-05,
      "loss": 1.0683,
      "step": 5083
    },
    {
      "epoch": 0.7147476451567553,
      "grad_norm": 1.3300946950912476,
      "learning_rate": 2.529377247102993e-05,
      "loss": 1.2189,
      "step": 5084
    },
    {
      "epoch": 0.714888232813159,
      "grad_norm": 1.6513850688934326,
      "learning_rate": 2.5448509230695407e-05,
      "loss": 1.02,
      "step": 5085
    },
    {
      "epoch": 0.7150288204695627,
      "grad_norm": 1.4555330276489258,
      "learning_rate": 2.560364888181267e-05,
      "loss": 1.0585,
      "step": 5086
    },
    {
      "epoch": 0.7151694081259665,
      "grad_norm": 1.6769942045211792,
      "learning_rate": 2.575919058597531e-05,
      "loss": 1.1365,
      "step": 5087
    },
    {
      "epoch": 0.7153099957823703,
      "grad_norm": 1.74495530128479,
      "learning_rate": 2.591513350260426e-05,
      "loss": 1.054,
      "step": 5088
    },
    {
      "epoch": 0.7154505834387741,
      "grad_norm": 1.4007655382156372,
      "learning_rate": 2.607147678895222e-05,
      "loss": 1.0112,
      "step": 5089
    },
    {
      "epoch": 0.7155911710951779,
      "grad_norm": 1.4773788452148438,
      "learning_rate": 2.6228219600108084e-05,
      "loss": 1.1799,
      "step": 5090
    },
    {
      "epoch": 0.7157317587515816,
      "grad_norm": 1.2975196838378906,
      "learning_rate": 2.6385361089002092e-05,
      "loss": 1.0219,
      "step": 5091
    },
    {
      "epoch": 0.7158723464079854,
      "grad_norm": 1.5171356201171875,
      "learning_rate": 2.6542900406409267e-05,
      "loss": 1.0044,
      "step": 5092
    },
    {
      "epoch": 0.7160129340643892,
      "grad_norm": 1.717629075050354,
      "learning_rate": 2.6700836700955023e-05,
      "loss": 1.0921,
      "step": 5093
    },
    {
      "epoch": 0.7161535217207929,
      "grad_norm": 1.4404336214065552,
      "learning_rate": 2.6859169119119663e-05,
      "loss": 1.0503,
      "step": 5094
    },
    {
      "epoch": 0.7162941093771967,
      "grad_norm": 1.4715195894241333,
      "learning_rate": 2.7017896805242425e-05,
      "loss": 0.8386,
      "step": 5095
    },
    {
      "epoch": 0.7164346970336004,
      "grad_norm": 1.3613841533660889,
      "learning_rate": 2.717701890152653e-05,
      "loss": 1.23,
      "step": 5096
    },
    {
      "epoch": 0.7165752846900042,
      "grad_norm": 1.4388577938079834,
      "learning_rate": 2.7336534548043735e-05,
      "loss": 0.9571,
      "step": 5097
    },
    {
      "epoch": 0.716715872346408,
      "grad_norm": 1.5045839548110962,
      "learning_rate": 2.7496442882738972e-05,
      "loss": 1.1862,
      "step": 5098
    },
    {
      "epoch": 0.7168564600028118,
      "grad_norm": 1.5927168130874634,
      "learning_rate": 2.765674304143501e-05,
      "loss": 1.1197,
      "step": 5099
    },
    {
      "epoch": 0.7169970476592156,
      "grad_norm": 1.5803582668304443,
      "learning_rate": 2.7817434157837118e-05,
      "loss": 1.0155,
      "step": 5100
    },
    {
      "epoch": 0.7171376353156192,
      "grad_norm": 1.4054594039916992,
      "learning_rate": 2.7978515363537628e-05,
      "loss": 0.9355,
      "step": 5101
    },
    {
      "epoch": 0.717278222972023,
      "grad_norm": 1.7124093770980835,
      "learning_rate": 2.813998578802124e-05,
      "loss": 1.1495,
      "step": 5102
    },
    {
      "epoch": 0.7174188106284268,
      "grad_norm": 1.5772989988327026,
      "learning_rate": 2.8301844558668377e-05,
      "loss": 0.9562,
      "step": 5103
    },
    {
      "epoch": 0.7175593982848306,
      "grad_norm": 1.535008430480957,
      "learning_rate": 2.846409080076161e-05,
      "loss": 0.9287,
      "step": 5104
    },
    {
      "epoch": 0.7176999859412344,
      "grad_norm": 1.448206901550293,
      "learning_rate": 2.8626723637489073e-05,
      "loss": 1.168,
      "step": 5105
    },
    {
      "epoch": 0.7178405735976381,
      "grad_norm": 1.3722233772277832,
      "learning_rate": 2.8789742189949742e-05,
      "loss": 0.9557,
      "step": 5106
    },
    {
      "epoch": 0.7179811612540419,
      "grad_norm": 1.2610207796096802,
      "learning_rate": 2.895314557715816e-05,
      "loss": 1.1261,
      "step": 5107
    },
    {
      "epoch": 0.7181217489104457,
      "grad_norm": 1.7515277862548828,
      "learning_rate": 2.91169329160491e-05,
      "loss": 1.0638,
      "step": 5108
    },
    {
      "epoch": 0.7182623365668495,
      "grad_norm": 1.4402310848236084,
      "learning_rate": 2.9281103321482273e-05,
      "loss": 0.9887,
      "step": 5109
    },
    {
      "epoch": 0.7184029242232532,
      "grad_norm": 1.3183423280715942,
      "learning_rate": 2.9445655906247728e-05,
      "loss": 1.0038,
      "step": 5110
    },
    {
      "epoch": 0.7185435118796569,
      "grad_norm": 1.498550295829773,
      "learning_rate": 2.9610589781069254e-05,
      "loss": 1.0943,
      "step": 5111
    },
    {
      "epoch": 0.7186840995360607,
      "grad_norm": 1.6317098140716553,
      "learning_rate": 2.9775904054610936e-05,
      "loss": 1.1004,
      "step": 5112
    },
    {
      "epoch": 0.7188246871924645,
      "grad_norm": 1.2800447940826416,
      "learning_rate": 2.994159783348065e-05,
      "loss": 1.0246,
      "step": 5113
    },
    {
      "epoch": 0.7189652748488683,
      "grad_norm": 1.4673877954483032,
      "learning_rate": 3.0107670222235405e-05,
      "loss": 1.1634,
      "step": 5114
    },
    {
      "epoch": 0.7191058625052721,
      "grad_norm": 1.601762056350708,
      "learning_rate": 3.0274120323386202e-05,
      "loss": 1.111,
      "step": 5115
    },
    {
      "epoch": 0.7192464501616758,
      "grad_norm": 1.1524897813796997,
      "learning_rate": 3.044094723740273e-05,
      "loss": 1.2334,
      "step": 5116
    },
    {
      "epoch": 0.7193870378180796,
      "grad_norm": 1.3889868259429932,
      "learning_rate": 3.0608150062718235e-05,
      "loss": 1.1294,
      "step": 5117
    },
    {
      "epoch": 0.7195276254744833,
      "grad_norm": 1.346666932106018,
      "learning_rate": 3.0775727895734954e-05,
      "loss": 0.9094,
      "step": 5118
    },
    {
      "epoch": 0.7196682131308871,
      "grad_norm": 1.5472166538238525,
      "learning_rate": 3.094367983082772e-05,
      "loss": 1.0214,
      "step": 5119
    },
    {
      "epoch": 0.7198088007872909,
      "grad_norm": 1.690266489982605,
      "learning_rate": 3.111200496035005e-05,
      "loss": 1.0553,
      "step": 5120
    },
    {
      "epoch": 0.7199493884436946,
      "grad_norm": 1.325559139251709,
      "learning_rate": 3.128070237463904e-05,
      "loss": 1.2053,
      "step": 5121
    },
    {
      "epoch": 0.7200899761000984,
      "grad_norm": 1.5367271900177002,
      "learning_rate": 3.144977116201892e-05,
      "loss": 1.2625,
      "step": 5122
    },
    {
      "epoch": 0.7202305637565022,
      "grad_norm": 1.5380946397781372,
      "learning_rate": 3.161921040880794e-05,
      "loss": 0.9639,
      "step": 5123
    },
    {
      "epoch": 0.720371151412906,
      "grad_norm": 1.455798625946045,
      "learning_rate": 3.1789019199321715e-05,
      "loss": 1.2015,
      "step": 5124
    },
    {
      "epoch": 0.7205117390693098,
      "grad_norm": 1.4700946807861328,
      "learning_rate": 3.195919661587893e-05,
      "loss": 1.0272,
      "step": 5125
    },
    {
      "epoch": 0.7206523267257134,
      "grad_norm": 1.510307788848877,
      "learning_rate": 3.212974173880615e-05,
      "loss": 1.0911,
      "step": 5126
    },
    {
      "epoch": 0.7207929143821172,
      "grad_norm": 1.444344401359558,
      "learning_rate": 3.230065364644276e-05,
      "loss": 0.8661,
      "step": 5127
    },
    {
      "epoch": 0.720933502038521,
      "grad_norm": 1.534865379333496,
      "learning_rate": 3.247193141514585e-05,
      "loss": 1.1059,
      "step": 5128
    },
    {
      "epoch": 0.7210740896949248,
      "grad_norm": 1.663016676902771,
      "learning_rate": 3.2643574119295825e-05,
      "loss": 1.0328,
      "step": 5129
    },
    {
      "epoch": 0.7212146773513286,
      "grad_norm": 1.5684951543807983,
      "learning_rate": 3.281558083130003e-05,
      "loss": 1.0684,
      "step": 5130
    },
    {
      "epoch": 0.7213552650077323,
      "grad_norm": 1.5656399726867676,
      "learning_rate": 3.2987950621599506e-05,
      "loss": 1.1641,
      "step": 5131
    },
    {
      "epoch": 0.7214958526641361,
      "grad_norm": 1.46066415309906,
      "learning_rate": 3.316068255867265e-05,
      "loss": 1.168,
      "step": 5132
    },
    {
      "epoch": 0.7216364403205399,
      "grad_norm": 1.4967045783996582,
      "learning_rate": 3.333377570904086e-05,
      "loss": 1.1019,
      "step": 5133
    },
    {
      "epoch": 0.7217770279769437,
      "grad_norm": 1.5539056062698364,
      "learning_rate": 3.3507229137273476e-05,
      "loss": 1.1405,
      "step": 5134
    },
    {
      "epoch": 0.7219176156333474,
      "grad_norm": 1.2928998470306396,
      "learning_rate": 3.368104190599283e-05,
      "loss": 1.0688,
      "step": 5135
    },
    {
      "epoch": 0.7220582032897511,
      "grad_norm": 1.4684479236602783,
      "learning_rate": 3.3855213075879125e-05,
      "loss": 1.1063,
      "step": 5136
    },
    {
      "epoch": 0.7221987909461549,
      "grad_norm": 1.4775816202163696,
      "learning_rate": 3.402974170567629e-05,
      "loss": 0.9497,
      "step": 5137
    },
    {
      "epoch": 0.7223393786025587,
      "grad_norm": 1.2297914028167725,
      "learning_rate": 3.4204626852195476e-05,
      "loss": 1.3326,
      "step": 5138
    },
    {
      "epoch": 0.7224799662589625,
      "grad_norm": 1.7451540231704712,
      "learning_rate": 3.437986757032213e-05,
      "loss": 1.0723,
      "step": 5139
    },
    {
      "epoch": 0.7226205539153663,
      "grad_norm": 1.451252818107605,
      "learning_rate": 3.455546291301969e-05,
      "loss": 0.916,
      "step": 5140
    },
    {
      "epoch": 0.72276114157177,
      "grad_norm": 1.5200122594833374,
      "learning_rate": 3.473141193133471e-05,
      "loss": 0.9239,
      "step": 5141
    },
    {
      "epoch": 0.7229017292281738,
      "grad_norm": 1.4452885389328003,
      "learning_rate": 3.4907713674403255e-05,
      "loss": 1.06,
      "step": 5142
    },
    {
      "epoch": 0.7230423168845775,
      "grad_norm": 1.349530577659607,
      "learning_rate": 3.5084367189454635e-05,
      "loss": 1.1144,
      "step": 5143
    },
    {
      "epoch": 0.7231829045409813,
      "grad_norm": 1.6087793111801147,
      "learning_rate": 3.526137152181723e-05,
      "loss": 1.0064,
      "step": 5144
    },
    {
      "epoch": 0.7233234921973851,
      "grad_norm": 1.643750548362732,
      "learning_rate": 3.543872571492357e-05,
      "loss": 1.2159,
      "step": 5145
    },
    {
      "epoch": 0.7234640798537888,
      "grad_norm": 1.3270338773727417,
      "learning_rate": 3.5616428810315306e-05,
      "loss": 1.2163,
      "step": 5146
    },
    {
      "epoch": 0.7236046675101926,
      "grad_norm": 1.4748362302780151,
      "learning_rate": 3.579447984764896e-05,
      "loss": 1.0708,
      "step": 5147
    },
    {
      "epoch": 0.7237452551665964,
      "grad_norm": 1.4146596193313599,
      "learning_rate": 3.597287786470043e-05,
      "loss": 1.1576,
      "step": 5148
    },
    {
      "epoch": 0.7238858428230002,
      "grad_norm": 1.4727798700332642,
      "learning_rate": 3.615162189737006e-05,
      "loss": 1.1099,
      "step": 5149
    },
    {
      "epoch": 0.724026430479404,
      "grad_norm": 1.5401214361190796,
      "learning_rate": 3.6330710979689076e-05,
      "loss": 1.1022,
      "step": 5150
    },
    {
      "epoch": 0.7241670181358076,
      "grad_norm": 1.4448827505111694,
      "learning_rate": 3.6510144143823445e-05,
      "loss": 0.9661,
      "step": 5151
    },
    {
      "epoch": 0.7243076057922114,
      "grad_norm": 1.6585123538970947,
      "learning_rate": 3.668992042007976e-05,
      "loss": 1.0055,
      "step": 5152
    },
    {
      "epoch": 0.7244481934486152,
      "grad_norm": 1.6157805919647217,
      "learning_rate": 3.6870038836910404e-05,
      "loss": 1.0071,
      "step": 5153
    },
    {
      "epoch": 0.724588781105019,
      "grad_norm": 1.384234070777893,
      "learning_rate": 3.705049842091858e-05,
      "loss": 1.2116,
      "step": 5154
    },
    {
      "epoch": 0.7247293687614228,
      "grad_norm": 1.5986098051071167,
      "learning_rate": 3.72312981968642e-05,
      "loss": 1.1031,
      "step": 5155
    },
    {
      "epoch": 0.7248699564178265,
      "grad_norm": 1.358021855354309,
      "learning_rate": 3.741243718766841e-05,
      "loss": 1.0434,
      "step": 5156
    },
    {
      "epoch": 0.7250105440742303,
      "grad_norm": 1.9547767639160156,
      "learning_rate": 3.7593914414418795e-05,
      "loss": 1.0219,
      "step": 5157
    },
    {
      "epoch": 0.7251511317306341,
      "grad_norm": 1.8195929527282715,
      "learning_rate": 3.7775728896375816e-05,
      "loss": 0.9131,
      "step": 5158
    },
    {
      "epoch": 0.7252917193870378,
      "grad_norm": 1.6590352058410645,
      "learning_rate": 3.795787965097698e-05,
      "loss": 1.1712,
      "step": 5159
    },
    {
      "epoch": 0.7254323070434416,
      "grad_norm": 1.4661225080490112,
      "learning_rate": 3.8140365693842054e-05,
      "loss": 1.1105,
      "step": 5160
    },
    {
      "epoch": 0.7255728946998453,
      "grad_norm": 1.2051458358764648,
      "learning_rate": 3.8323186038779677e-05,
      "loss": 1.0182,
      "step": 5161
    },
    {
      "epoch": 0.7257134823562491,
      "grad_norm": 1.5070620775222778,
      "learning_rate": 3.850633969779131e-05,
      "loss": 1.0924,
      "step": 5162
    },
    {
      "epoch": 0.7258540700126529,
      "grad_norm": 1.2599800825119019,
      "learning_rate": 3.868982568107725e-05,
      "loss": 1.0049,
      "step": 5163
    },
    {
      "epoch": 0.7259946576690567,
      "grad_norm": 1.8337546586990356,
      "learning_rate": 3.887364299704185e-05,
      "loss": 1.1996,
      "step": 5164
    },
    {
      "epoch": 0.7261352453254605,
      "grad_norm": 1.5559680461883545,
      "learning_rate": 3.90577906522987e-05,
      "loss": 1.0626,
      "step": 5165
    },
    {
      "epoch": 0.7262758329818642,
      "grad_norm": 1.6130889654159546,
      "learning_rate": 3.924226765167665e-05,
      "loss": 1.0393,
      "step": 5166
    },
    {
      "epoch": 0.726416420638268,
      "grad_norm": 1.3451756238937378,
      "learning_rate": 3.942707299822437e-05,
      "loss": 1.2778,
      "step": 5167
    },
    {
      "epoch": 0.7265570082946717,
      "grad_norm": 1.284605622291565,
      "learning_rate": 3.96122056932157e-05,
      "loss": 1.0482,
      "step": 5168
    },
    {
      "epoch": 0.7266975959510755,
      "grad_norm": 1.5336910486221313,
      "learning_rate": 3.979766473615623e-05,
      "loss": 1.04,
      "step": 5169
    },
    {
      "epoch": 0.7268381836074793,
      "grad_norm": 1.6023290157318115,
      "learning_rate": 3.998344912478733e-05,
      "loss": 1.2351,
      "step": 5170
    },
    {
      "epoch": 0.726978771263883,
      "grad_norm": 1.3640739917755127,
      "learning_rate": 4.016955785509234e-05,
      "loss": 1.0787,
      "step": 5171
    },
    {
      "epoch": 0.7271193589202868,
      "grad_norm": 1.4293287992477417,
      "learning_rate": 4.035598992130172e-05,
      "loss": 1.1902,
      "step": 5172
    },
    {
      "epoch": 0.7272599465766906,
      "grad_norm": 1.5437184572219849,
      "learning_rate": 4.054274431589844e-05,
      "loss": 1.1501,
      "step": 5173
    },
    {
      "epoch": 0.7274005342330944,
      "grad_norm": 1.4636178016662598,
      "learning_rate": 4.072982002962398e-05,
      "loss": 1.1431,
      "step": 5174
    },
    {
      "epoch": 0.7275411218894982,
      "grad_norm": 1.6322813034057617,
      "learning_rate": 4.0917216051483054e-05,
      "loss": 1.0938,
      "step": 5175
    },
    {
      "epoch": 0.7276817095459018,
      "grad_norm": 1.5582051277160645,
      "learning_rate": 4.110493136874892e-05,
      "loss": 0.9558,
      "step": 5176
    },
    {
      "epoch": 0.7278222972023056,
      "grad_norm": 1.4727298021316528,
      "learning_rate": 4.129296496697019e-05,
      "loss": 1.1515,
      "step": 5177
    },
    {
      "epoch": 0.7279628848587094,
      "grad_norm": 1.4713739156723022,
      "learning_rate": 4.1481315829974964e-05,
      "loss": 1.2103,
      "step": 5178
    },
    {
      "epoch": 0.7281034725151132,
      "grad_norm": 1.272148847579956,
      "learning_rate": 4.166998293987636e-05,
      "loss": 1.2469,
      "step": 5179
    },
    {
      "epoch": 0.728244060171517,
      "grad_norm": 1.4540225267410278,
      "learning_rate": 4.185896527707929e-05,
      "loss": 1.0139,
      "step": 5180
    },
    {
      "epoch": 0.7283846478279207,
      "grad_norm": 1.55671226978302,
      "learning_rate": 4.2048261820284394e-05,
      "loss": 1.1012,
      "step": 5181
    },
    {
      "epoch": 0.7285252354843245,
      "grad_norm": 1.511956810951233,
      "learning_rate": 4.2237871546495004e-05,
      "loss": 1.0108,
      "step": 5182
    },
    {
      "epoch": 0.7286658231407283,
      "grad_norm": 1.5407170057296753,
      "learning_rate": 4.242779343102108e-05,
      "loss": 1.1732,
      "step": 5183
    },
    {
      "epoch": 0.728806410797132,
      "grad_norm": 1.5406509637832642,
      "learning_rate": 4.261802644748605e-05,
      "loss": 1.0743,
      "step": 5184
    },
    {
      "epoch": 0.7289469984535358,
      "grad_norm": 1.6264762878417969,
      "learning_rate": 4.280856956783219e-05,
      "loss": 1.2639,
      "step": 5185
    },
    {
      "epoch": 0.7290875861099395,
      "grad_norm": 1.5373024940490723,
      "learning_rate": 4.2999421762325666e-05,
      "loss": 0.9919,
      "step": 5186
    },
    {
      "epoch": 0.7292281737663433,
      "grad_norm": 1.5172511339187622,
      "learning_rate": 4.3190581999561864e-05,
      "loss": 0.9366,
      "step": 5187
    },
    {
      "epoch": 0.7293687614227471,
      "grad_norm": 1.5667402744293213,
      "learning_rate": 4.3382049246472344e-05,
      "loss": 1.152,
      "step": 5188
    },
    {
      "epoch": 0.7295093490791509,
      "grad_norm": 1.6167898178100586,
      "learning_rate": 4.357382246832877e-05,
      "loss": 1.0644,
      "step": 5189
    },
    {
      "epoch": 0.7296499367355547,
      "grad_norm": 2.0274605751037598,
      "learning_rate": 4.3765900628750037e-05,
      "loss": 1.0915,
      "step": 5190
    },
    {
      "epoch": 0.7297905243919584,
      "grad_norm": 1.5673877000808716,
      "learning_rate": 4.3958282689706145e-05,
      "loss": 1.1608,
      "step": 5191
    },
    {
      "epoch": 0.7299311120483621,
      "grad_norm": 1.4108526706695557,
      "learning_rate": 4.4150967611525215e-05,
      "loss": 1.146,
      "step": 5192
    },
    {
      "epoch": 0.7300716997047659,
      "grad_norm": 1.3137731552124023,
      "learning_rate": 4.4343954352898986e-05,
      "loss": 1.0727,
      "step": 5193
    },
    {
      "epoch": 0.7302122873611697,
      "grad_norm": 1.4181054830551147,
      "learning_rate": 4.453724187088775e-05,
      "loss": 1.0271,
      "step": 5194
    },
    {
      "epoch": 0.7303528750175735,
      "grad_norm": 1.2049615383148193,
      "learning_rate": 4.473082912092592e-05,
      "loss": 1.0495,
      "step": 5195
    },
    {
      "epoch": 0.7304934626739772,
      "grad_norm": 1.536152720451355,
      "learning_rate": 4.492471505682898e-05,
      "loss": 1.2221,
      "step": 5196
    },
    {
      "epoch": 0.730634050330381,
      "grad_norm": 1.558337688446045,
      "learning_rate": 4.511889863079782e-05,
      "loss": 1.1,
      "step": 5197
    },
    {
      "epoch": 0.7307746379867848,
      "grad_norm": 1.4691828489303589,
      "learning_rate": 4.5313378793424387e-05,
      "loss": 1.185,
      "step": 5198
    },
    {
      "epoch": 0.7309152256431886,
      "grad_norm": 1.5632073879241943,
      "learning_rate": 4.5508154493698676e-05,
      "loss": 1.0999,
      "step": 5199
    },
    {
      "epoch": 0.7310558132995923,
      "grad_norm": 1.2636840343475342,
      "learning_rate": 4.57032246790128e-05,
      "loss": 1.0136,
      "step": 5200
    },
    {
      "epoch": 0.731196400955996,
      "grad_norm": 1.4621537923812866,
      "learning_rate": 4.5898588295168175e-05,
      "loss": 1.0899,
      "step": 5201
    },
    {
      "epoch": 0.7313369886123998,
      "grad_norm": 1.8799059391021729,
      "learning_rate": 4.609424428637956e-05,
      "loss": 1.1148,
      "step": 5202
    },
    {
      "epoch": 0.7314775762688036,
      "grad_norm": 1.3494035005569458,
      "learning_rate": 4.6290191595282085e-05,
      "loss": 1.0717,
      "step": 5203
    },
    {
      "epoch": 0.7316181639252074,
      "grad_norm": 1.827477216720581,
      "learning_rate": 4.648642916293685e-05,
      "loss": 1.1522,
      "step": 5204
    },
    {
      "epoch": 0.7317587515816112,
      "grad_norm": 1.4591060876846313,
      "learning_rate": 4.6682955928836046e-05,
      "loss": 0.9169,
      "step": 5205
    },
    {
      "epoch": 0.7318993392380149,
      "grad_norm": 1.7474573850631714,
      "learning_rate": 4.687977083090852e-05,
      "loss": 1.0636,
      "step": 5206
    },
    {
      "epoch": 0.7320399268944187,
      "grad_norm": 1.3763030767440796,
      "learning_rate": 4.707687280552688e-05,
      "loss": 0.9897,
      "step": 5207
    },
    {
      "epoch": 0.7321805145508224,
      "grad_norm": 1.477509617805481,
      "learning_rate": 4.7274260787511596e-05,
      "loss": 1.0353,
      "step": 5208
    },
    {
      "epoch": 0.7323211022072262,
      "grad_norm": 1.5268422365188599,
      "learning_rate": 4.747193371013819e-05,
      "loss": 1.1415,
      "step": 5209
    },
    {
      "epoch": 0.73246168986363,
      "grad_norm": 1.4810867309570312,
      "learning_rate": 4.766989050514149e-05,
      "loss": 0.9924,
      "step": 5210
    },
    {
      "epoch": 0.7326022775200337,
      "grad_norm": 1.2574759721755981,
      "learning_rate": 4.78681301027226e-05,
      "loss": 1.1319,
      "step": 5211
    },
    {
      "epoch": 0.7327428651764375,
      "grad_norm": 1.3029568195343018,
      "learning_rate": 4.806665143155463e-05,
      "loss": 0.9147,
      "step": 5212
    },
    {
      "epoch": 0.7328834528328413,
      "grad_norm": 1.1912997961044312,
      "learning_rate": 4.8265453418787874e-05,
      "loss": 0.9996,
      "step": 5213
    },
    {
      "epoch": 0.7330240404892451,
      "grad_norm": 1.5119787454605103,
      "learning_rate": 4.84645349900554e-05,
      "loss": 1.0442,
      "step": 5214
    },
    {
      "epoch": 0.7331646281456489,
      "grad_norm": 1.5476953983306885,
      "learning_rate": 4.866389506948028e-05,
      "loss": 1.1573,
      "step": 5215
    },
    {
      "epoch": 0.7333052158020525,
      "grad_norm": 1.7250189781188965,
      "learning_rate": 4.8863532579679864e-05,
      "loss": 1.172,
      "step": 5216
    },
    {
      "epoch": 0.7334458034584563,
      "grad_norm": 1.4740110635757446,
      "learning_rate": 4.9063446441772345e-05,
      "loss": 1.1451,
      "step": 5217
    },
    {
      "epoch": 0.7335863911148601,
      "grad_norm": 1.4685451984405518,
      "learning_rate": 4.9263635575382464e-05,
      "loss": 0.9052,
      "step": 5218
    },
    {
      "epoch": 0.7337269787712639,
      "grad_norm": 1.3665598630905151,
      "learning_rate": 4.946409889864719e-05,
      "loss": 1.1949,
      "step": 5219
    },
    {
      "epoch": 0.7338675664276677,
      "grad_norm": 1.2414376735687256,
      "learning_rate": 4.966483532822233e-05,
      "loss": 1.3086,
      "step": 5220
    },
    {
      "epoch": 0.7340081540840714,
      "grad_norm": 1.3362243175506592,
      "learning_rate": 4.9865843779286836e-05,
      "loss": 1.1393,
      "step": 5221
    },
    {
      "epoch": 0.7341487417404752,
      "grad_norm": 1.4964226484298706,
      "learning_rate": 5.0067123165550065e-05,
      "loss": 1.2226,
      "step": 5222
    },
    {
      "epoch": 0.734289329396879,
      "grad_norm": 1.6839885711669922,
      "learning_rate": 5.0268672399257496e-05,
      "loss": 1.0207,
      "step": 5223
    },
    {
      "epoch": 0.7344299170532828,
      "grad_norm": 1.3886445760726929,
      "learning_rate": 5.0470490391195855e-05,
      "loss": 1.0347,
      "step": 5224
    },
    {
      "epoch": 0.7345705047096865,
      "grad_norm": 1.3482694625854492,
      "learning_rate": 5.067257605069955e-05,
      "loss": 0.9163,
      "step": 5225
    },
    {
      "epoch": 0.7347110923660902,
      "grad_norm": 1.3079732656478882,
      "learning_rate": 5.087492828565651e-05,
      "loss": 0.8394,
      "step": 5226
    },
    {
      "epoch": 0.734851680022494,
      "grad_norm": 1.5347014665603638,
      "learning_rate": 5.1077546002513844e-05,
      "loss": 1.2028,
      "step": 5227
    },
    {
      "epoch": 0.7349922676788978,
      "grad_norm": 1.355405330657959,
      "learning_rate": 5.1280428106284564e-05,
      "loss": 1.1252,
      "step": 5228
    },
    {
      "epoch": 0.7351328553353016,
      "grad_norm": 1.5107446908950806,
      "learning_rate": 5.148357350055199e-05,
      "loss": 1.2608,
      "step": 5229
    },
    {
      "epoch": 0.7352734429917054,
      "grad_norm": 1.4972312450408936,
      "learning_rate": 5.1686981087476985e-05,
      "loss": 1.1534,
      "step": 5230
    },
    {
      "epoch": 0.7354140306481091,
      "grad_norm": 1.6418769359588623,
      "learning_rate": 5.18906497678038e-05,
      "loss": 1.1178,
      "step": 5231
    },
    {
      "epoch": 0.7355546183045129,
      "grad_norm": 1.2796071767807007,
      "learning_rate": 5.209457844086524e-05,
      "loss": 1.1217,
      "step": 5232
    },
    {
      "epoch": 0.7356952059609166,
      "grad_norm": 1.331477403640747,
      "learning_rate": 5.229876600458916e-05,
      "loss": 1.0963,
      "step": 5233
    },
    {
      "epoch": 0.7358357936173204,
      "grad_norm": 1.2709838151931763,
      "learning_rate": 5.2503211355504376e-05,
      "loss": 1.148,
      "step": 5234
    },
    {
      "epoch": 0.7359763812737241,
      "grad_norm": 1.4944126605987549,
      "learning_rate": 5.270791338874652e-05,
      "loss": 1.1024,
      "step": 5235
    },
    {
      "epoch": 0.7361169689301279,
      "grad_norm": 1.4201240539550781,
      "learning_rate": 5.291287099806409e-05,
      "loss": 1.1055,
      "step": 5236
    },
    {
      "epoch": 0.7362575565865317,
      "grad_norm": 1.3540663719177246,
      "learning_rate": 5.311808307582438e-05,
      "loss": 1.1678,
      "step": 5237
    },
    {
      "epoch": 0.7363981442429355,
      "grad_norm": 1.458297610282898,
      "learning_rate": 5.332354851301935e-05,
      "loss": 1.0456,
      "step": 5238
    },
    {
      "epoch": 0.7365387318993393,
      "grad_norm": 1.384600043296814,
      "learning_rate": 5.3529266199272364e-05,
      "loss": 0.8495,
      "step": 5239
    },
    {
      "epoch": 0.736679319555743,
      "grad_norm": 1.3977196216583252,
      "learning_rate": 5.373523502284263e-05,
      "loss": 1.0036,
      "step": 5240
    },
    {
      "epoch": 0.7368199072121467,
      "grad_norm": 1.8023971319198608,
      "learning_rate": 5.3941453870632664e-05,
      "loss": 0.9685,
      "step": 5241
    },
    {
      "epoch": 0.7369604948685505,
      "grad_norm": 1.710236668586731,
      "learning_rate": 5.4147921628194085e-05,
      "loss": 1.0166,
      "step": 5242
    },
    {
      "epoch": 0.7371010825249543,
      "grad_norm": 1.4299685955047607,
      "learning_rate": 5.4354637179732894e-05,
      "loss": 1.1145,
      "step": 5243
    },
    {
      "epoch": 0.7372416701813581,
      "grad_norm": 1.84723699092865,
      "learning_rate": 5.4561599408116114e-05,
      "loss": 0.8735,
      "step": 5244
    },
    {
      "epoch": 0.7373822578377618,
      "grad_norm": 1.4263161420822144,
      "learning_rate": 5.476880719487767e-05,
      "loss": 0.9656,
      "step": 5245
    },
    {
      "epoch": 0.7375228454941656,
      "grad_norm": 1.2079124450683594,
      "learning_rate": 5.4976259420224306e-05,
      "loss": 1.2032,
      "step": 5246
    },
    {
      "epoch": 0.7376634331505694,
      "grad_norm": 1.5209518671035767,
      "learning_rate": 5.518395496304237e-05,
      "loss": 0.9543,
      "step": 5247
    },
    {
      "epoch": 0.7378040208069732,
      "grad_norm": 1.504423975944519,
      "learning_rate": 5.539189270090235e-05,
      "loss": 1.0302,
      "step": 5248
    },
    {
      "epoch": 0.737944608463377,
      "grad_norm": 1.409773588180542,
      "learning_rate": 5.560007151006633e-05,
      "loss": 1.0991,
      "step": 5249
    },
    {
      "epoch": 0.7380851961197806,
      "grad_norm": 1.3150978088378906,
      "learning_rate": 5.5808490265493904e-05,
      "loss": 1.2769,
      "step": 5250
    },
    {
      "epoch": 0.7382257837761844,
      "grad_norm": 1.6275994777679443,
      "learning_rate": 5.6017147840847484e-05,
      "loss": 1.0682,
      "step": 5251
    },
    {
      "epoch": 0.7383663714325882,
      "grad_norm": 1.52139413356781,
      "learning_rate": 5.6226043108498996e-05,
      "loss": 1.0706,
      "step": 5252
    },
    {
      "epoch": 0.738506959088992,
      "grad_norm": 1.513227105140686,
      "learning_rate": 5.643517493953583e-05,
      "loss": 1.1548,
      "step": 5253
    },
    {
      "epoch": 0.7386475467453958,
      "grad_norm": 1.5025076866149902,
      "learning_rate": 5.664454220376696e-05,
      "loss": 1.0072,
      "step": 5254
    },
    {
      "epoch": 0.7387881344017995,
      "grad_norm": 1.4733140468597412,
      "learning_rate": 5.6854143769729006e-05,
      "loss": 1.1774,
      "step": 5255
    },
    {
      "epoch": 0.7389287220582033,
      "grad_norm": 1.3341091871261597,
      "learning_rate": 5.7063978504692385e-05,
      "loss": 1.1234,
      "step": 5256
    },
    {
      "epoch": 0.739069309714607,
      "grad_norm": 1.5044634342193604,
      "learning_rate": 5.727404527466726e-05,
      "loss": 1.1379,
      "step": 5257
    },
    {
      "epoch": 0.7392098973710108,
      "grad_norm": 1.9237945079803467,
      "learning_rate": 5.748434294441046e-05,
      "loss": 1.0836,
      "step": 5258
    },
    {
      "epoch": 0.7393504850274146,
      "grad_norm": 1.4697003364562988,
      "learning_rate": 5.7694870377429885e-05,
      "loss": 0.9938,
      "step": 5259
    },
    {
      "epoch": 0.7394910726838183,
      "grad_norm": 1.345574140548706,
      "learning_rate": 5.7905626435992864e-05,
      "loss": 1.1154,
      "step": 5260
    },
    {
      "epoch": 0.7396316603402221,
      "grad_norm": 1.536887288093567,
      "learning_rate": 5.811660998113051e-05,
      "loss": 1.0866,
      "step": 5261
    },
    {
      "epoch": 0.7397722479966259,
      "grad_norm": 1.560431718826294,
      "learning_rate": 5.832781987264477e-05,
      "loss": 0.981,
      "step": 5262
    },
    {
      "epoch": 0.7399128356530297,
      "grad_norm": 1.4894108772277832,
      "learning_rate": 5.853925496911432e-05,
      "loss": 1.2522,
      "step": 5263
    },
    {
      "epoch": 0.7400534233094335,
      "grad_norm": 1.8384276628494263,
      "learning_rate": 5.875091412790082e-05,
      "loss": 0.9499,
      "step": 5264
    },
    {
      "epoch": 0.7401940109658371,
      "grad_norm": 1.4205719232559204,
      "learning_rate": 5.8962796205154856e-05,
      "loss": 1.0791,
      "step": 5265
    },
    {
      "epoch": 0.7403345986222409,
      "grad_norm": 1.2460823059082031,
      "learning_rate": 5.9174900055823e-05,
      "loss": 1.1109,
      "step": 5266
    },
    {
      "epoch": 0.7404751862786447,
      "grad_norm": 1.2720468044281006,
      "learning_rate": 5.938722453365214e-05,
      "loss": 1.1744,
      "step": 5267
    },
    {
      "epoch": 0.7406157739350485,
      "grad_norm": 1.388856053352356,
      "learning_rate": 5.959976849119803e-05,
      "loss": 1.069,
      "step": 5268
    },
    {
      "epoch": 0.7407563615914523,
      "grad_norm": 1.4284722805023193,
      "learning_rate": 5.981253077982961e-05,
      "loss": 0.9496,
      "step": 5269
    },
    {
      "epoch": 0.740896949247856,
      "grad_norm": 1.309838056564331,
      "learning_rate": 6.002551024973613e-05,
      "loss": 1.0792,
      "step": 5270
    },
    {
      "epoch": 0.7410375369042598,
      "grad_norm": 1.3103395700454712,
      "learning_rate": 6.023870574993312e-05,
      "loss": 1.0517,
      "step": 5271
    },
    {
      "epoch": 0.7411781245606636,
      "grad_norm": 1.6839008331298828,
      "learning_rate": 6.0452116128268646e-05,
      "loss": 1.1073,
      "step": 5272
    },
    {
      "epoch": 0.7413187122170674,
      "grad_norm": 1.708111047744751,
      "learning_rate": 6.0665740231429344e-05,
      "loss": 1.2783,
      "step": 5273
    },
    {
      "epoch": 0.7414592998734711,
      "grad_norm": 1.569547176361084,
      "learning_rate": 6.0879576904947524e-05,
      "loss": 1.0418,
      "step": 5274
    },
    {
      "epoch": 0.7415998875298748,
      "grad_norm": 1.365343689918518,
      "learning_rate": 6.10936249932057e-05,
      "loss": 1.1208,
      "step": 5275
    },
    {
      "epoch": 0.7417404751862786,
      "grad_norm": 1.5469835996627808,
      "learning_rate": 6.130788333944441e-05,
      "loss": 1.0736,
      "step": 5276
    },
    {
      "epoch": 0.7418810628426824,
      "grad_norm": 1.509407877922058,
      "learning_rate": 6.152235078576836e-05,
      "loss": 0.994,
      "step": 5277
    },
    {
      "epoch": 0.7420216504990862,
      "grad_norm": 1.6288894414901733,
      "learning_rate": 6.17370261731511e-05,
      "loss": 0.9378,
      "step": 5278
    },
    {
      "epoch": 0.74216223815549,
      "grad_norm": 1.3841811418533325,
      "learning_rate": 6.195190834144357e-05,
      "loss": 1.1678,
      "step": 5279
    },
    {
      "epoch": 0.7423028258118937,
      "grad_norm": 1.5142526626586914,
      "learning_rate": 6.216699612937856e-05,
      "loss": 1.0074,
      "step": 5280
    },
    {
      "epoch": 0.7424434134682975,
      "grad_norm": 1.692191481590271,
      "learning_rate": 6.238228837457782e-05,
      "loss": 1.09,
      "step": 5281
    },
    {
      "epoch": 0.7425840011247012,
      "grad_norm": 1.4265773296356201,
      "learning_rate": 6.259778391355821e-05,
      "loss": 1.1086,
      "step": 5282
    },
    {
      "epoch": 0.742724588781105,
      "grad_norm": 1.3299533128738403,
      "learning_rate": 6.281348158173788e-05,
      "loss": 1.176,
      "step": 5283
    },
    {
      "epoch": 0.7428651764375088,
      "grad_norm": 1.3546382188796997,
      "learning_rate": 6.30293802134425e-05,
      "loss": 1.0979,
      "step": 5284
    },
    {
      "epoch": 0.7430057640939125,
      "grad_norm": 1.338637113571167,
      "learning_rate": 6.324547864191237e-05,
      "loss": 1.1407,
      "step": 5285
    },
    {
      "epoch": 0.7431463517503163,
      "grad_norm": 1.392252802848816,
      "learning_rate": 6.346177569930688e-05,
      "loss": 1.0595,
      "step": 5286
    },
    {
      "epoch": 0.7432869394067201,
      "grad_norm": 1.4773297309875488,
      "learning_rate": 6.367827021671314e-05,
      "loss": 1.0706,
      "step": 5287
    },
    {
      "epoch": 0.7434275270631239,
      "grad_norm": 1.4831146001815796,
      "learning_rate": 6.389496102415044e-05,
      "loss": 0.9707,
      "step": 5288
    },
    {
      "epoch": 0.7435681147195277,
      "grad_norm": 1.9262382984161377,
      "learning_rate": 6.411184695057755e-05,
      "loss": 0.9962,
      "step": 5289
    },
    {
      "epoch": 0.7437087023759313,
      "grad_norm": 1.5699487924575806,
      "learning_rate": 6.43289268238987e-05,
      "loss": 1.3045,
      "step": 5290
    },
    {
      "epoch": 0.7438492900323351,
      "grad_norm": 1.419165849685669,
      "learning_rate": 6.454619947096998e-05,
      "loss": 1.2838,
      "step": 5291
    },
    {
      "epoch": 0.7439898776887389,
      "grad_norm": 1.3697706460952759,
      "learning_rate": 6.476366371760558e-05,
      "loss": 1.2424,
      "step": 5292
    },
    {
      "epoch": 0.7441304653451427,
      "grad_norm": 1.4661139249801636,
      "learning_rate": 6.498131838858489e-05,
      "loss": 1.069,
      "step": 5293
    },
    {
      "epoch": 0.7442710530015465,
      "grad_norm": 1.272864580154419,
      "learning_rate": 6.519916230765702e-05,
      "loss": 1.222,
      "step": 5294
    },
    {
      "epoch": 0.7444116406579502,
      "grad_norm": 1.586114764213562,
      "learning_rate": 6.541719429754954e-05,
      "loss": 1.1174,
      "step": 5295
    },
    {
      "epoch": 0.744552228314354,
      "grad_norm": 1.3049507141113281,
      "learning_rate": 6.563541317997311e-05,
      "loss": 1.2108,
      "step": 5296
    },
    {
      "epoch": 0.7446928159707578,
      "grad_norm": 1.2726781368255615,
      "learning_rate": 6.585381777562796e-05,
      "loss": 1.1541,
      "step": 5297
    },
    {
      "epoch": 0.7448334036271615,
      "grad_norm": 1.6503196954727173,
      "learning_rate": 6.607240690421159e-05,
      "loss": 0.8664,
      "step": 5298
    },
    {
      "epoch": 0.7449739912835653,
      "grad_norm": 1.3925321102142334,
      "learning_rate": 6.629117938442365e-05,
      "loss": 0.9434,
      "step": 5299
    },
    {
      "epoch": 0.745114578939969,
      "grad_norm": 1.32786226272583,
      "learning_rate": 6.651013403397307e-05,
      "loss": 1.1509,
      "step": 5300
    },
    {
      "epoch": 0.7452551665963728,
      "grad_norm": 1.7618815898895264,
      "learning_rate": 6.672926966958429e-05,
      "loss": 1.1058,
      "step": 5301
    },
    {
      "epoch": 0.7453957542527766,
      "grad_norm": 1.4670830965042114,
      "learning_rate": 6.694858510700351e-05,
      "loss": 1.0922,
      "step": 5302
    },
    {
      "epoch": 0.7455363419091804,
      "grad_norm": 1.4868276119232178,
      "learning_rate": 6.71680791610057e-05,
      "loss": 1.2107,
      "step": 5303
    },
    {
      "epoch": 0.7456769295655842,
      "grad_norm": 1.5144661664962769,
      "learning_rate": 6.73877506454003e-05,
      "loss": 1.0172,
      "step": 5304
    },
    {
      "epoch": 0.7458175172219879,
      "grad_norm": 1.5091643333435059,
      "learning_rate": 6.760759837303732e-05,
      "loss": 0.9419,
      "step": 5305
    },
    {
      "epoch": 0.7459581048783916,
      "grad_norm": 1.604432463645935,
      "learning_rate": 6.782762115581534e-05,
      "loss": 0.8777,
      "step": 5306
    },
    {
      "epoch": 0.7460986925347954,
      "grad_norm": 2.0736241340637207,
      "learning_rate": 6.804781780468619e-05,
      "loss": 1.0613,
      "step": 5307
    },
    {
      "epoch": 0.7462392801911992,
      "grad_norm": 1.6338483095169067,
      "learning_rate": 6.826818712966232e-05,
      "loss": 1.1345,
      "step": 5308
    },
    {
      "epoch": 0.746379867847603,
      "grad_norm": 1.8005859851837158,
      "learning_rate": 6.848872793982299e-05,
      "loss": 1.3495,
      "step": 5309
    },
    {
      "epoch": 0.7465204555040067,
      "grad_norm": 1.5873109102249146,
      "learning_rate": 6.870943904332055e-05,
      "loss": 1.0473,
      "step": 5310
    },
    {
      "epoch": 0.7466610431604105,
      "grad_norm": 1.210644006729126,
      "learning_rate": 6.893031924738751e-05,
      "loss": 1.1673,
      "step": 5311
    },
    {
      "epoch": 0.7468016308168143,
      "grad_norm": 1.6030234098434448,
      "learning_rate": 6.915136735834225e-05,
      "loss": 1.0541,
      "step": 5312
    },
    {
      "epoch": 0.7469422184732181,
      "grad_norm": 1.4622976779937744,
      "learning_rate": 6.937258218159521e-05,
      "loss": 1.0677,
      "step": 5313
    },
    {
      "epoch": 0.7470828061296219,
      "grad_norm": 1.5559067726135254,
      "learning_rate": 6.959396252165692e-05,
      "loss": 1.1503,
      "step": 5314
    },
    {
      "epoch": 0.7472233937860255,
      "grad_norm": 1.5478278398513794,
      "learning_rate": 6.981550718214286e-05,
      "loss": 1.0233,
      "step": 5315
    },
    {
      "epoch": 0.7473639814424293,
      "grad_norm": 1.650622844696045,
      "learning_rate": 7.003721496578007e-05,
      "loss": 1.0245,
      "step": 5316
    },
    {
      "epoch": 0.7475045690988331,
      "grad_norm": 1.3995699882507324,
      "learning_rate": 7.025908467441497e-05,
      "loss": 1.0992,
      "step": 5317
    },
    {
      "epoch": 0.7476451567552369,
      "grad_norm": 1.6078470945358276,
      "learning_rate": 7.04811151090183e-05,
      "loss": 0.894,
      "step": 5318
    },
    {
      "epoch": 0.7477857444116407,
      "grad_norm": 1.4208348989486694,
      "learning_rate": 7.070330506969228e-05,
      "loss": 1.0284,
      "step": 5319
    },
    {
      "epoch": 0.7479263320680444,
      "grad_norm": 1.3263992071151733,
      "learning_rate": 7.092565335567709e-05,
      "loss": 1.1211,
      "step": 5320
    },
    {
      "epoch": 0.7480669197244482,
      "grad_norm": 1.5520212650299072,
      "learning_rate": 7.114815876535715e-05,
      "loss": 0.9622,
      "step": 5321
    },
    {
      "epoch": 0.748207507380852,
      "grad_norm": 1.786577582359314,
      "learning_rate": 7.137082009626816e-05,
      "loss": 1.2021,
      "step": 5322
    },
    {
      "epoch": 0.7483480950372557,
      "grad_norm": 1.4136769771575928,
      "learning_rate": 7.159363614510292e-05,
      "loss": 1.0787,
      "step": 5323
    },
    {
      "epoch": 0.7484886826936595,
      "grad_norm": 1.4533040523529053,
      "learning_rate": 7.181660570771758e-05,
      "loss": 1.1176,
      "step": 5324
    },
    {
      "epoch": 0.7486292703500632,
      "grad_norm": 1.6941392421722412,
      "learning_rate": 7.203972757913968e-05,
      "loss": 0.9721,
      "step": 5325
    },
    {
      "epoch": 0.748769858006467,
      "grad_norm": 1.5779014825820923,
      "learning_rate": 7.2263000553573e-05,
      "loss": 1.035,
      "step": 5326
    },
    {
      "epoch": 0.7489104456628708,
      "grad_norm": 1.4030929803848267,
      "learning_rate": 7.248642342440486e-05,
      "loss": 0.9646,
      "step": 5327
    },
    {
      "epoch": 0.7490510333192746,
      "grad_norm": 1.5416651964187622,
      "learning_rate": 7.270999498421252e-05,
      "loss": 1.0297,
      "step": 5328
    },
    {
      "epoch": 0.7491916209756784,
      "grad_norm": 1.4988633394241333,
      "learning_rate": 7.293371402476954e-05,
      "loss": 1.0317,
      "step": 5329
    },
    {
      "epoch": 0.749332208632082,
      "grad_norm": 1.3487820625305176,
      "learning_rate": 7.315757933705296e-05,
      "loss": 0.9959,
      "step": 5330
    },
    {
      "epoch": 0.7494727962884858,
      "grad_norm": 1.6135162115097046,
      "learning_rate": 7.338158971124901e-05,
      "loss": 1.2789,
      "step": 5331
    },
    {
      "epoch": 0.7496133839448896,
      "grad_norm": 1.5050746202468872,
      "learning_rate": 7.360574393675946e-05,
      "loss": 1.0207,
      "step": 5332
    },
    {
      "epoch": 0.7497539716012934,
      "grad_norm": 1.3999042510986328,
      "learning_rate": 7.383004080220968e-05,
      "loss": 1.2135,
      "step": 5333
    },
    {
      "epoch": 0.7498945592576972,
      "grad_norm": 1.4998618364334106,
      "learning_rate": 7.40544790954537e-05,
      "loss": 0.9559,
      "step": 5334
    },
    {
      "epoch": 0.7500351469141009,
      "grad_norm": 1.7631990909576416,
      "learning_rate": 7.427905760358074e-05,
      "loss": 1.0201,
      "step": 5335
    },
    {
      "epoch": 0.7501757345705047,
      "grad_norm": 1.6888673305511475,
      "learning_rate": 7.450377511292328e-05,
      "loss": 1.1137,
      "step": 5336
    },
    {
      "epoch": 0.7503163222269085,
      "grad_norm": 1.4081411361694336,
      "learning_rate": 7.472863040906172e-05,
      "loss": 1.0339,
      "step": 5337
    },
    {
      "epoch": 0.7504569098833123,
      "grad_norm": 1.4289932250976562,
      "learning_rate": 7.495362227683276e-05,
      "loss": 1.2098,
      "step": 5338
    },
    {
      "epoch": 0.750597497539716,
      "grad_norm": 1.2752811908721924,
      "learning_rate": 7.517874950033394e-05,
      "loss": 1.2272,
      "step": 5339
    },
    {
      "epoch": 0.7507380851961197,
      "grad_norm": 1.626650094985962,
      "learning_rate": 7.540401086293189e-05,
      "loss": 1.1869,
      "step": 5340
    },
    {
      "epoch": 0.7508786728525235,
      "grad_norm": 1.7253526449203491,
      "learning_rate": 7.562940514726862e-05,
      "loss": 1.1179,
      "step": 5341
    },
    {
      "epoch": 0.7510192605089273,
      "grad_norm": 1.4361252784729004,
      "learning_rate": 7.585493113526746e-05,
      "loss": 1.1858,
      "step": 5342
    },
    {
      "epoch": 0.7511598481653311,
      "grad_norm": 1.8434196710586548,
      "learning_rate": 7.608058760813953e-05,
      "loss": 1.2294,
      "step": 5343
    },
    {
      "epoch": 0.7513004358217349,
      "grad_norm": 1.3862097263336182,
      "learning_rate": 7.630637334639172e-05,
      "loss": 1.0727,
      "step": 5344
    },
    {
      "epoch": 0.7514410234781386,
      "grad_norm": 1.4383560419082642,
      "learning_rate": 7.653228712983158e-05,
      "loss": 1.1135,
      "step": 5345
    },
    {
      "epoch": 0.7515816111345424,
      "grad_norm": 1.2651022672653198,
      "learning_rate": 7.675832773757548e-05,
      "loss": 1.1502,
      "step": 5346
    },
    {
      "epoch": 0.7517221987909462,
      "grad_norm": 1.4223278760910034,
      "learning_rate": 7.698449394805336e-05,
      "loss": 0.9655,
      "step": 5347
    },
    {
      "epoch": 0.7518627864473499,
      "grad_norm": 1.3136907815933228,
      "learning_rate": 7.721078453901694e-05,
      "loss": 1.195,
      "step": 5348
    },
    {
      "epoch": 0.7520033741037537,
      "grad_norm": 1.37447190284729,
      "learning_rate": 7.74371982875461e-05,
      "loss": 1.2629,
      "step": 5349
    },
    {
      "epoch": 0.7521439617601574,
      "grad_norm": 1.3895930051803589,
      "learning_rate": 7.766373397005477e-05,
      "loss": 1.0433,
      "step": 5350
    },
    {
      "epoch": 0.7522845494165612,
      "grad_norm": 1.425257682800293,
      "learning_rate": 7.789039036229743e-05,
      "loss": 1.0938,
      "step": 5351
    },
    {
      "epoch": 0.752425137072965,
      "grad_norm": 1.4701745510101318,
      "learning_rate": 7.81171662393773e-05,
      "loss": 1.0487,
      "step": 5352
    },
    {
      "epoch": 0.7525657247293688,
      "grad_norm": 1.2588233947753906,
      "learning_rate": 7.834406037575125e-05,
      "loss": 1.1234,
      "step": 5353
    },
    {
      "epoch": 0.7527063123857726,
      "grad_norm": 1.3957531452178955,
      "learning_rate": 7.857107154523666e-05,
      "loss": 1.0568,
      "step": 5354
    },
    {
      "epoch": 0.7528469000421762,
      "grad_norm": 1.5003902912139893,
      "learning_rate": 7.87981985210194e-05,
      "loss": 0.9328,
      "step": 5355
    },
    {
      "epoch": 0.75298748769858,
      "grad_norm": 1.2984360456466675,
      "learning_rate": 7.902544007565862e-05,
      "loss": 1.2342,
      "step": 5356
    },
    {
      "epoch": 0.7531280753549838,
      "grad_norm": 2.816767930984497,
      "learning_rate": 7.92527949810952e-05,
      "loss": 1.1077,
      "step": 5357
    },
    {
      "epoch": 0.7532686630113876,
      "grad_norm": 1.3727279901504517,
      "learning_rate": 7.948026200865635e-05,
      "loss": 1.1311,
      "step": 5358
    },
    {
      "epoch": 0.7534092506677914,
      "grad_norm": 1.4927536249160767,
      "learning_rate": 7.970783992906391e-05,
      "loss": 1.3186,
      "step": 5359
    },
    {
      "epoch": 0.7535498383241951,
      "grad_norm": 1.2997090816497803,
      "learning_rate": 7.993552751244074e-05,
      "loss": 1.3134,
      "step": 5360
    },
    {
      "epoch": 0.7536904259805989,
      "grad_norm": 1.5226658582687378,
      "learning_rate": 8.016332352831672e-05,
      "loss": 0.9519,
      "step": 5361
    },
    {
      "epoch": 0.7538310136370027,
      "grad_norm": 1.3433340787887573,
      "learning_rate": 8.039122674563526e-05,
      "loss": 1.1808,
      "step": 5362
    },
    {
      "epoch": 0.7539716012934065,
      "grad_norm": 1.4863228797912598,
      "learning_rate": 8.061923593276143e-05,
      "loss": 1.0358,
      "step": 5363
    },
    {
      "epoch": 0.7541121889498102,
      "grad_norm": 1.4247759580612183,
      "learning_rate": 8.084734985748674e-05,
      "loss": 0.9879,
      "step": 5364
    },
    {
      "epoch": 0.7542527766062139,
      "grad_norm": 1.6492676734924316,
      "learning_rate": 8.10755672870376e-05,
      "loss": 0.9264,
      "step": 5365
    },
    {
      "epoch": 0.7543933642626177,
      "grad_norm": 1.3319462537765503,
      "learning_rate": 8.130388698807995e-05,
      "loss": 1.0222,
      "step": 5366
    },
    {
      "epoch": 0.7545339519190215,
      "grad_norm": 1.3230377435684204,
      "learning_rate": 8.153230772672762e-05,
      "loss": 1.1963,
      "step": 5367
    },
    {
      "epoch": 0.7546745395754253,
      "grad_norm": 1.388152837753296,
      "learning_rate": 8.17608282685487e-05,
      "loss": 1.1979,
      "step": 5368
    },
    {
      "epoch": 0.7548151272318291,
      "grad_norm": 1.4801323413848877,
      "learning_rate": 8.198944737857161e-05,
      "loss": 1.0715,
      "step": 5369
    },
    {
      "epoch": 0.7549557148882328,
      "grad_norm": 1.2793148756027222,
      "learning_rate": 8.221816382129158e-05,
      "loss": 1.0964,
      "step": 5370
    },
    {
      "epoch": 0.7550963025446366,
      "grad_norm": 1.6060523986816406,
      "learning_rate": 8.244697636067885e-05,
      "loss": 1.3248,
      "step": 5371
    },
    {
      "epoch": 0.7552368902010403,
      "grad_norm": 1.2606897354125977,
      "learning_rate": 8.267588376018361e-05,
      "loss": 1.0864,
      "step": 5372
    },
    {
      "epoch": 0.7553774778574441,
      "grad_norm": 1.4110023975372314,
      "learning_rate": 8.290488478274366e-05,
      "loss": 1.1036,
      "step": 5373
    },
    {
      "epoch": 0.7555180655138479,
      "grad_norm": 1.4287656545639038,
      "learning_rate": 8.313397819079076e-05,
      "loss": 1.101,
      "step": 5374
    },
    {
      "epoch": 0.7556586531702516,
      "grad_norm": 1.4051427841186523,
      "learning_rate": 8.33631627462573e-05,
      "loss": 1.0241,
      "step": 5375
    },
    {
      "epoch": 0.7557992408266554,
      "grad_norm": 1.691651463508606,
      "learning_rate": 8.359243721058373e-05,
      "loss": 1.0697,
      "step": 5376
    },
    {
      "epoch": 0.7559398284830592,
      "grad_norm": 1.4664592742919922,
      "learning_rate": 8.382180034472358e-05,
      "loss": 1.183,
      "step": 5377
    },
    {
      "epoch": 0.756080416139463,
      "grad_norm": 1.6471012830734253,
      "learning_rate": 8.405125090915176e-05,
      "loss": 1.1393,
      "step": 5378
    },
    {
      "epoch": 0.7562210037958668,
      "grad_norm": 1.4669466018676758,
      "learning_rate": 8.428078766387098e-05,
      "loss": 0.9799,
      "step": 5379
    },
    {
      "epoch": 0.7563615914522704,
      "grad_norm": 1.552144169807434,
      "learning_rate": 8.451040936841763e-05,
      "loss": 0.9142,
      "step": 5380
    },
    {
      "epoch": 0.7565021791086742,
      "grad_norm": 1.3698182106018066,
      "learning_rate": 8.474011478186926e-05,
      "loss": 0.956,
      "step": 5381
    },
    {
      "epoch": 0.756642766765078,
      "grad_norm": 1.4861034154891968,
      "learning_rate": 8.496990266285097e-05,
      "loss": 1.1571,
      "step": 5382
    },
    {
      "epoch": 0.7567833544214818,
      "grad_norm": 1.2533947229385376,
      "learning_rate": 8.519977176954204e-05,
      "loss": 1.0939,
      "step": 5383
    },
    {
      "epoch": 0.7569239420778856,
      "grad_norm": 1.4379841089248657,
      "learning_rate": 8.542972085968351e-05,
      "loss": 1.0383,
      "step": 5384
    },
    {
      "epoch": 0.7570645297342893,
      "grad_norm": 1.3614472150802612,
      "learning_rate": 8.565974869058307e-05,
      "loss": 1.1258,
      "step": 5385
    },
    {
      "epoch": 0.7572051173906931,
      "grad_norm": 1.4784165620803833,
      "learning_rate": 8.588985401912346e-05,
      "loss": 1.1339,
      "step": 5386
    },
    {
      "epoch": 0.7573457050470969,
      "grad_norm": 1.5052164793014526,
      "learning_rate": 8.612003560176894e-05,
      "loss": 1.0927,
      "step": 5387
    },
    {
      "epoch": 0.7574862927035007,
      "grad_norm": 1.5336036682128906,
      "learning_rate": 8.635029219457111e-05,
      "loss": 1.1738,
      "step": 5388
    },
    {
      "epoch": 0.7576268803599044,
      "grad_norm": 1.4074761867523193,
      "learning_rate": 8.658062255317642e-05,
      "loss": 1.1922,
      "step": 5389
    },
    {
      "epoch": 0.7577674680163081,
      "grad_norm": 1.7060742378234863,
      "learning_rate": 8.68110254328327e-05,
      "loss": 1.0477,
      "step": 5390
    },
    {
      "epoch": 0.7579080556727119,
      "grad_norm": 1.4848105907440186,
      "learning_rate": 8.704149958839581e-05,
      "loss": 1.0868,
      "step": 5391
    },
    {
      "epoch": 0.7580486433291157,
      "grad_norm": 1.3744168281555176,
      "learning_rate": 8.727204377433641e-05,
      "loss": 1.0198,
      "step": 5392
    },
    {
      "epoch": 0.7581892309855195,
      "grad_norm": 1.6432400941848755,
      "learning_rate": 8.750265674474675e-05,
      "loss": 1.2183,
      "step": 5393
    },
    {
      "epoch": 0.7583298186419233,
      "grad_norm": 1.2895923852920532,
      "learning_rate": 8.773333725334717e-05,
      "loss": 1.0391,
      "step": 5394
    },
    {
      "epoch": 0.758470406298327,
      "grad_norm": 1.765105962753296,
      "learning_rate": 8.796408405349367e-05,
      "loss": 1.1423,
      "step": 5395
    },
    {
      "epoch": 0.7586109939547308,
      "grad_norm": 1.5386505126953125,
      "learning_rate": 8.819489589818302e-05,
      "loss": 1.0627,
      "step": 5396
    },
    {
      "epoch": 0.7587515816111345,
      "grad_norm": 1.520706295967102,
      "learning_rate": 8.842577154006095e-05,
      "loss": 1.0724,
      "step": 5397
    },
    {
      "epoch": 0.7588921692675383,
      "grad_norm": 1.347184658050537,
      "learning_rate": 8.865670973142878e-05,
      "loss": 1.1944,
      "step": 5398
    },
    {
      "epoch": 0.7590327569239421,
      "grad_norm": 1.3330023288726807,
      "learning_rate": 8.888770922424928e-05,
      "loss": 1.0366,
      "step": 5399
    },
    {
      "epoch": 0.7591733445803458,
      "grad_norm": 1.4517403841018677,
      "learning_rate": 8.911876877015413e-05,
      "loss": 1.1249,
      "step": 5400
    },
    {
      "epoch": 0.7593139322367496,
      "grad_norm": 1.5430153608322144,
      "learning_rate": 8.934988712045041e-05,
      "loss": 1.0316,
      "step": 5401
    },
    {
      "epoch": 0.7594545198931534,
      "grad_norm": 1.3863049745559692,
      "learning_rate": 8.95810630261273e-05,
      "loss": 1.111,
      "step": 5402
    },
    {
      "epoch": 0.7595951075495572,
      "grad_norm": 1.4324114322662354,
      "learning_rate": 8.98122952378636e-05,
      "loss": 0.9958,
      "step": 5403
    },
    {
      "epoch": 0.759735695205961,
      "grad_norm": 1.8085713386535645,
      "learning_rate": 9.004358250603278e-05,
      "loss": 1.0262,
      "step": 5404
    },
    {
      "epoch": 0.7598762828623646,
      "grad_norm": 1.7172257900238037,
      "learning_rate": 9.02749235807113e-05,
      "loss": 1.1631,
      "step": 5405
    },
    {
      "epoch": 0.7600168705187684,
      "grad_norm": 1.7382696866989136,
      "learning_rate": 9.05063172116852e-05,
      "loss": 1.0444,
      "step": 5406
    },
    {
      "epoch": 0.7601574581751722,
      "grad_norm": 1.5063060522079468,
      "learning_rate": 9.073776214845595e-05,
      "loss": 1.0788,
      "step": 5407
    },
    {
      "epoch": 0.760298045831576,
      "grad_norm": 1.4869520664215088,
      "learning_rate": 9.096925714024793e-05,
      "loss": 0.9771,
      "step": 5408
    },
    {
      "epoch": 0.7604386334879798,
      "grad_norm": 1.5705692768096924,
      "learning_rate": 9.120080093601504e-05,
      "loss": 0.9444,
      "step": 5409
    },
    {
      "epoch": 0.7605792211443835,
      "grad_norm": 1.7625998258590698,
      "learning_rate": 9.143239228444739e-05,
      "loss": 1.0834,
      "step": 5410
    },
    {
      "epoch": 0.7607198088007873,
      "grad_norm": 1.4511125087738037,
      "learning_rate": 9.166402993397814e-05,
      "loss": 1.1545,
      "step": 5411
    },
    {
      "epoch": 0.7608603964571911,
      "grad_norm": 1.3516312837600708,
      "learning_rate": 9.189571263279022e-05,
      "loss": 1.1695,
      "step": 5412
    },
    {
      "epoch": 0.7610009841135948,
      "grad_norm": 1.3404661417007446,
      "learning_rate": 9.212743912882294e-05,
      "loss": 0.9308,
      "step": 5413
    },
    {
      "epoch": 0.7611415717699986,
      "grad_norm": 1.6325422525405884,
      "learning_rate": 9.235920816977958e-05,
      "loss": 0.9425,
      "step": 5414
    },
    {
      "epoch": 0.7612821594264023,
      "grad_norm": 1.9405518770217896,
      "learning_rate": 9.25910185031323e-05,
      "loss": 1.0087,
      "step": 5415
    },
    {
      "epoch": 0.7614227470828061,
      "grad_norm": 1.3001155853271484,
      "learning_rate": 9.282286887613138e-05,
      "loss": 0.9242,
      "step": 5416
    },
    {
      "epoch": 0.7615633347392099,
      "grad_norm": 1.376131534576416,
      "learning_rate": 9.305475803580992e-05,
      "loss": 1.1825,
      "step": 5417
    },
    {
      "epoch": 0.7617039223956137,
      "grad_norm": 1.3605438470840454,
      "learning_rate": 9.328668472899167e-05,
      "loss": 1.1496,
      "step": 5418
    },
    {
      "epoch": 0.7618445100520175,
      "grad_norm": 1.3806957006454468,
      "learning_rate": 9.351864770229748e-05,
      "loss": 1.0801,
      "step": 5419
    },
    {
      "epoch": 0.7619850977084212,
      "grad_norm": 1.3381540775299072,
      "learning_rate": 9.375064570215219e-05,
      "loss": 1.0056,
      "step": 5420
    },
    {
      "epoch": 0.762125685364825,
      "grad_norm": 1.2808128595352173,
      "learning_rate": 9.398267747479113e-05,
      "loss": 1.1029,
      "step": 5421
    },
    {
      "epoch": 0.7622662730212287,
      "grad_norm": 1.443454623222351,
      "learning_rate": 9.421474176626785e-05,
      "loss": 1.1954,
      "step": 5422
    },
    {
      "epoch": 0.7624068606776325,
      "grad_norm": 1.3334343433380127,
      "learning_rate": 9.44468373224589e-05,
      "loss": 0.9464,
      "step": 5423
    },
    {
      "epoch": 0.7625474483340363,
      "grad_norm": 1.7124524116516113,
      "learning_rate": 9.46789628890731e-05,
      "loss": 1.1066,
      "step": 5424
    },
    {
      "epoch": 0.76268803599044,
      "grad_norm": 1.3430200815200806,
      "learning_rate": 9.491111721165642e-05,
      "loss": 1.203,
      "step": 5425
    },
    {
      "epoch": 0.7628286236468438,
      "grad_norm": 1.8050833940505981,
      "learning_rate": 9.514329903559958e-05,
      "loss": 0.9956,
      "step": 5426
    },
    {
      "epoch": 0.7629692113032476,
      "grad_norm": 1.441994547843933,
      "learning_rate": 9.537550710614467e-05,
      "loss": 1.1703,
      "step": 5427
    },
    {
      "epoch": 0.7631097989596514,
      "grad_norm": 1.4922010898590088,
      "learning_rate": 9.560774016839196e-05,
      "loss": 1.0965,
      "step": 5428
    },
    {
      "epoch": 0.7632503866160552,
      "grad_norm": 1.528695821762085,
      "learning_rate": 9.583999696730646e-05,
      "loss": 1.0414,
      "step": 5429
    },
    {
      "epoch": 0.7633909742724588,
      "grad_norm": 1.420166254043579,
      "learning_rate": 9.607227624772562e-05,
      "loss": 1.0444,
      "step": 5430
    },
    {
      "epoch": 0.7635315619288626,
      "grad_norm": 1.4705331325531006,
      "learning_rate": 9.630457675436432e-05,
      "loss": 1.0866,
      "step": 5431
    },
    {
      "epoch": 0.7636721495852664,
      "grad_norm": 1.4203083515167236,
      "learning_rate": 9.653689723182331e-05,
      "loss": 1.1992,
      "step": 5432
    },
    {
      "epoch": 0.7638127372416702,
      "grad_norm": 1.2495427131652832,
      "learning_rate": 9.676923642459588e-05,
      "loss": 0.9442,
      "step": 5433
    },
    {
      "epoch": 0.763953324898074,
      "grad_norm": 1.5685738325119019,
      "learning_rate": 9.700159307707297e-05,
      "loss": 1.1436,
      "step": 5434
    },
    {
      "epoch": 0.7640939125544777,
      "grad_norm": 1.4802474975585938,
      "learning_rate": 9.723396593355239e-05,
      "loss": 0.9595,
      "step": 5435
    },
    {
      "epoch": 0.7642345002108815,
      "grad_norm": 1.3096531629562378,
      "learning_rate": 9.746635373824373e-05,
      "loss": 1.2968,
      "step": 5436
    },
    {
      "epoch": 0.7643750878672853,
      "grad_norm": 1.2808921337127686,
      "learning_rate": 9.769875523527585e-05,
      "loss": 1.1368,
      "step": 5437
    },
    {
      "epoch": 0.764515675523689,
      "grad_norm": 1.4281468391418457,
      "learning_rate": 9.793116916870377e-05,
      "loss": 1.1838,
      "step": 5438
    },
    {
      "epoch": 0.7646562631800928,
      "grad_norm": 1.3643267154693604,
      "learning_rate": 9.816359428251515e-05,
      "loss": 1.1702,
      "step": 5439
    },
    {
      "epoch": 0.7647968508364965,
      "grad_norm": 1.211414098739624,
      "learning_rate": 9.839602932063718e-05,
      "loss": 1.1102,
      "step": 5440
    },
    {
      "epoch": 0.7649374384929003,
      "grad_norm": 1.510221004486084,
      "learning_rate": 9.862847302694405e-05,
      "loss": 1.0786,
      "step": 5441
    },
    {
      "epoch": 0.7650780261493041,
      "grad_norm": 1.563236117362976,
      "learning_rate": 9.886092414526187e-05,
      "loss": 1.0723,
      "step": 5442
    },
    {
      "epoch": 0.7652186138057079,
      "grad_norm": 1.2483630180358887,
      "learning_rate": 9.909338141937796e-05,
      "loss": 1.1784,
      "step": 5443
    },
    {
      "epoch": 0.7653592014621117,
      "grad_norm": 1.6528750658035278,
      "learning_rate": 9.932584359304567e-05,
      "loss": 1.0173,
      "step": 5444
    },
    {
      "epoch": 0.7654997891185154,
      "grad_norm": 1.2788829803466797,
      "learning_rate": 9.9558309409992e-05,
      "loss": 1.3163,
      "step": 5445
    },
    {
      "epoch": 0.7656403767749191,
      "grad_norm": 1.342461109161377,
      "learning_rate": 9.979077761392431e-05,
      "loss": 0.958,
      "step": 5446
    },
    {
      "epoch": 0.7657809644313229,
      "grad_norm": 1.3864604234695435,
      "learning_rate": 0.00010002324694853702,
      "loss": 1.2684,
      "step": 5447
    },
    {
      "epoch": 0.7659215520877267,
      "grad_norm": 1.4951329231262207,
      "learning_rate": 0.00010025571615751832,
      "loss": 1.1209,
      "step": 5448
    },
    {
      "epoch": 0.7660621397441305,
      "grad_norm": 1.557915210723877,
      "learning_rate": 0.00010048818398455775,
      "loss": 1.1252,
      "step": 5449
    },
    {
      "epoch": 0.7662027274005342,
      "grad_norm": 1.3347430229187012,
      "learning_rate": 0.00010072064917335105,
      "loss": 1.1372,
      "step": 5450
    },
    {
      "epoch": 0.766343315056938,
      "grad_norm": 1.3139671087265015,
      "learning_rate": 0.00010095311046760951,
      "loss": 1.0534,
      "step": 5451
    },
    {
      "epoch": 0.7664839027133418,
      "grad_norm": 1.4455411434173584,
      "learning_rate": 0.00010118556661106491,
      "loss": 1.1592,
      "step": 5452
    },
    {
      "epoch": 0.7666244903697456,
      "grad_norm": 1.511698603630066,
      "learning_rate": 0.00010141801634747636,
      "loss": 1.1127,
      "step": 5453
    },
    {
      "epoch": 0.7667650780261493,
      "grad_norm": 1.485671877861023,
      "learning_rate": 0.00010165045842063858,
      "loss": 1.0226,
      "step": 5454
    },
    {
      "epoch": 0.766905665682553,
      "grad_norm": 1.340990662574768,
      "learning_rate": 0.00010188289157438712,
      "loss": 1.1478,
      "step": 5455
    },
    {
      "epoch": 0.7670462533389568,
      "grad_norm": 1.311906099319458,
      "learning_rate": 0.00010211531455260579,
      "loss": 1.2151,
      "step": 5456
    },
    {
      "epoch": 0.7671868409953606,
      "grad_norm": 1.4611523151397705,
      "learning_rate": 0.00010234772609923349,
      "loss": 1.0312,
      "step": 5457
    },
    {
      "epoch": 0.7673274286517644,
      "grad_norm": 1.4277955293655396,
      "learning_rate": 0.0001025801249582706,
      "loss": 1.0284,
      "step": 5458
    },
    {
      "epoch": 0.7674680163081682,
      "grad_norm": 1.5118184089660645,
      "learning_rate": 0.00010281250987378675,
      "loss": 1.1734,
      "step": 5459
    },
    {
      "epoch": 0.7676086039645719,
      "grad_norm": 1.6353014707565308,
      "learning_rate": 0.00010304487958992643,
      "loss": 1.0172,
      "step": 5460
    },
    {
      "epoch": 0.7677491916209757,
      "grad_norm": 1.3926796913146973,
      "learning_rate": 0.00010327723285091586,
      "loss": 1.0346,
      "step": 5461
    },
    {
      "epoch": 0.7678897792773794,
      "grad_norm": 1.3487756252288818,
      "learning_rate": 0.00010350956840107114,
      "loss": 0.9866,
      "step": 5462
    },
    {
      "epoch": 0.7680303669337832,
      "grad_norm": 1.634243130683899,
      "learning_rate": 0.00010374188498480341,
      "loss": 1.141,
      "step": 5463
    },
    {
      "epoch": 0.768170954590187,
      "grad_norm": 1.2520289421081543,
      "learning_rate": 0.00010397418134662655,
      "loss": 1.2076,
      "step": 5464
    },
    {
      "epoch": 0.7683115422465907,
      "grad_norm": 1.4183924198150635,
      "learning_rate": 0.00010420645623116365,
      "loss": 1.1634,
      "step": 5465
    },
    {
      "epoch": 0.7684521299029945,
      "grad_norm": 1.5784406661987305,
      "learning_rate": 0.00010443870838315371,
      "loss": 1.1497,
      "step": 5466
    },
    {
      "epoch": 0.7685927175593983,
      "grad_norm": 1.6441493034362793,
      "learning_rate": 0.00010467093654745912,
      "loss": 1.154,
      "step": 5467
    },
    {
      "epoch": 0.7687333052158021,
      "grad_norm": 1.6765402555465698,
      "learning_rate": 0.00010490313946907153,
      "loss": 0.9609,
      "step": 5468
    },
    {
      "epoch": 0.7688738928722059,
      "grad_norm": 1.3366221189498901,
      "learning_rate": 0.00010513531589311854,
      "loss": 1.0256,
      "step": 5469
    },
    {
      "epoch": 0.7690144805286095,
      "grad_norm": 1.5933122634887695,
      "learning_rate": 0.00010536746456487189,
      "loss": 1.1484,
      "step": 5470
    },
    {
      "epoch": 0.7691550681850133,
      "grad_norm": 2.1714155673980713,
      "learning_rate": 0.00010559958422975289,
      "loss": 0.9501,
      "step": 5471
    },
    {
      "epoch": 0.7692956558414171,
      "grad_norm": 1.487620234489441,
      "learning_rate": 0.00010583167363333903,
      "loss": 1.0847,
      "step": 5472
    },
    {
      "epoch": 0.7694362434978209,
      "grad_norm": 1.4001129865646362,
      "learning_rate": 0.00010606373152137237,
      "loss": 1.1535,
      "step": 5473
    },
    {
      "epoch": 0.7695768311542247,
      "grad_norm": 1.4177616834640503,
      "learning_rate": 0.00010629575663976463,
      "loss": 1.1038,
      "step": 5474
    },
    {
      "epoch": 0.7697174188106284,
      "grad_norm": 1.5543391704559326,
      "learning_rate": 0.00010652774773460476,
      "loss": 1.0954,
      "step": 5475
    },
    {
      "epoch": 0.7698580064670322,
      "grad_norm": 1.3415855169296265,
      "learning_rate": 0.00010675970355216561,
      "loss": 1.0247,
      "step": 5476
    },
    {
      "epoch": 0.769998594123436,
      "grad_norm": 1.6205615997314453,
      "learning_rate": 0.00010699162283891047,
      "loss": 1.1377,
      "step": 5477
    },
    {
      "epoch": 0.7701391817798398,
      "grad_norm": 1.3740359544754028,
      "learning_rate": 0.00010722350434150061,
      "loss": 1.1851,
      "step": 5478
    },
    {
      "epoch": 0.7702797694362435,
      "grad_norm": 1.5614345073699951,
      "learning_rate": 0.00010745534680680114,
      "loss": 1.1796,
      "step": 5479
    },
    {
      "epoch": 0.7704203570926472,
      "grad_norm": 2.0259976387023926,
      "learning_rate": 0.00010768714898188756,
      "loss": 1.0921,
      "step": 5480
    },
    {
      "epoch": 0.770560944749051,
      "grad_norm": 1.4283287525177002,
      "learning_rate": 0.00010791890961405422,
      "loss": 1.0881,
      "step": 5481
    },
    {
      "epoch": 0.7707015324054548,
      "grad_norm": 1.8582700490951538,
      "learning_rate": 0.00010815062745081923,
      "loss": 1.1271,
      "step": 5482
    },
    {
      "epoch": 0.7708421200618586,
      "grad_norm": 1.3672877550125122,
      "learning_rate": 0.0001083823012399322,
      "loss": 1.2893,
      "step": 5483
    },
    {
      "epoch": 0.7709827077182624,
      "grad_norm": 1.3410208225250244,
      "learning_rate": 0.00010861392972938073,
      "loss": 1.206,
      "step": 5484
    },
    {
      "epoch": 0.7711232953746661,
      "grad_norm": 1.5196852684020996,
      "learning_rate": 0.00010884551166739707,
      "loss": 1.0374,
      "step": 5485
    },
    {
      "epoch": 0.7712638830310699,
      "grad_norm": 1.8199872970581055,
      "learning_rate": 0.0001090770458024656,
      "loss": 1.1083,
      "step": 5486
    },
    {
      "epoch": 0.7714044706874736,
      "grad_norm": 1.5110101699829102,
      "learning_rate": 0.00010930853088332866,
      "loss": 0.936,
      "step": 5487
    },
    {
      "epoch": 0.7715450583438774,
      "grad_norm": 1.4139792919158936,
      "learning_rate": 0.00010953996565899314,
      "loss": 1.0146,
      "step": 5488
    },
    {
      "epoch": 0.7716856460002812,
      "grad_norm": 1.3802932500839233,
      "learning_rate": 0.00010977134887873885,
      "loss": 0.9961,
      "step": 5489
    },
    {
      "epoch": 0.7718262336566849,
      "grad_norm": 1.2879211902618408,
      "learning_rate": 0.00011000267929212378,
      "loss": 1.0622,
      "step": 5490
    },
    {
      "epoch": 0.7719668213130887,
      "grad_norm": 1.4274189472198486,
      "learning_rate": 0.00011023395564899062,
      "loss": 1.1386,
      "step": 5491
    },
    {
      "epoch": 0.7721074089694925,
      "grad_norm": 1.9872201681137085,
      "learning_rate": 0.00011046517669947538,
      "loss": 1.0529,
      "step": 5492
    },
    {
      "epoch": 0.7722479966258963,
      "grad_norm": 1.5062114000320435,
      "learning_rate": 0.000110696341194012,
      "loss": 1.1821,
      "step": 5493
    },
    {
      "epoch": 0.7723885842823001,
      "grad_norm": 1.1962296962738037,
      "learning_rate": 0.00011092744788334095,
      "loss": 1.1499,
      "step": 5494
    },
    {
      "epoch": 0.7725291719387037,
      "grad_norm": 1.3849213123321533,
      "learning_rate": 0.00011115849551851407,
      "loss": 1.2284,
      "step": 5495
    },
    {
      "epoch": 0.7726697595951075,
      "grad_norm": 1.3935222625732422,
      "learning_rate": 0.00011138948285090283,
      "loss": 1.1122,
      "step": 5496
    },
    {
      "epoch": 0.7728103472515113,
      "grad_norm": 1.4547971487045288,
      "learning_rate": 0.00011162040863220496,
      "loss": 1.0737,
      "step": 5497
    },
    {
      "epoch": 0.7729509349079151,
      "grad_norm": 1.1961450576782227,
      "learning_rate": 0.00011185127161445048,
      "loss": 0.9877,
      "step": 5498
    },
    {
      "epoch": 0.7730915225643189,
      "grad_norm": 1.2929692268371582,
      "learning_rate": 0.00011208207055000823,
      "loss": 1.0615,
      "step": 5499
    },
    {
      "epoch": 0.7732321102207226,
      "grad_norm": 1.6532726287841797,
      "learning_rate": 0.00011231280419159426,
      "loss": 1.0553,
      "step": 5500
    },
    {
      "epoch": 0.7732321102207226,
      "eval_loss": 1.1459760665893555,
      "eval_runtime": 771.5148,
      "eval_samples_per_second": 16.391,
      "eval_steps_per_second": 8.196,
      "step": 5500
    },
    {
      "epoch": 0.7733726978771264,
      "grad_norm": 1.2698248624801636,
      "learning_rate": 0.0001125434712922766,
      "loss": 1.1914,
      "step": 5501
    },
    {
      "epoch": 0.7735132855335302,
      "grad_norm": 1.7252577543258667,
      "learning_rate": 0.00011277407060548373,
      "loss": 1.1052,
      "step": 5502
    },
    {
      "epoch": 0.773653873189934,
      "grad_norm": 1.3226394653320312,
      "learning_rate": 0.00011300460088500944,
      "loss": 1.114,
      "step": 5503
    },
    {
      "epoch": 0.7737944608463377,
      "grad_norm": 1.6742491722106934,
      "learning_rate": 0.00011323506088502111,
      "loss": 1.001,
      "step": 5504
    },
    {
      "epoch": 0.7739350485027414,
      "grad_norm": 1.4988573789596558,
      "learning_rate": 0.00011346544936006629,
      "loss": 0.9822,
      "step": 5505
    },
    {
      "epoch": 0.7740756361591452,
      "grad_norm": 1.6405117511749268,
      "learning_rate": 0.00011369576506507869,
      "loss": 1.0969,
      "step": 5506
    },
    {
      "epoch": 0.774216223815549,
      "grad_norm": 1.5313422679901123,
      "learning_rate": 0.00011392600675538474,
      "loss": 1.2033,
      "step": 5507
    },
    {
      "epoch": 0.7743568114719528,
      "grad_norm": 1.4461616277694702,
      "learning_rate": 0.00011415617318671198,
      "loss": 1.0984,
      "step": 5508
    },
    {
      "epoch": 0.7744973991283566,
      "grad_norm": 1.629564642906189,
      "learning_rate": 0.00011438626311519411,
      "loss": 1.0671,
      "step": 5509
    },
    {
      "epoch": 0.7746379867847603,
      "grad_norm": 1.3945621252059937,
      "learning_rate": 0.00011461627529737771,
      "loss": 1.1941,
      "step": 5510
    },
    {
      "epoch": 0.774778574441164,
      "grad_norm": 1.4565906524658203,
      "learning_rate": 0.0001148462084902306,
      "loss": 1.1012,
      "step": 5511
    },
    {
      "epoch": 0.7749191620975678,
      "grad_norm": 1.569475769996643,
      "learning_rate": 0.00011507606145114657,
      "loss": 0.9551,
      "step": 5512
    },
    {
      "epoch": 0.7750597497539716,
      "grad_norm": 1.548414707183838,
      "learning_rate": 0.00011530583293795392,
      "loss": 1.051,
      "step": 5513
    },
    {
      "epoch": 0.7752003374103754,
      "grad_norm": 1.4310904741287231,
      "learning_rate": 0.00011553552170892018,
      "loss": 1.2146,
      "step": 5514
    },
    {
      "epoch": 0.7753409250667791,
      "grad_norm": 2.152693271636963,
      "learning_rate": 0.00011576512652276037,
      "loss": 0.9179,
      "step": 5515
    },
    {
      "epoch": 0.7754815127231829,
      "grad_norm": 1.5413626432418823,
      "learning_rate": 0.00011599464613864368,
      "loss": 1.1686,
      "step": 5516
    },
    {
      "epoch": 0.7756221003795867,
      "grad_norm": 1.6530675888061523,
      "learning_rate": 0.00011622407931619927,
      "loss": 1.1397,
      "step": 5517
    },
    {
      "epoch": 0.7757626880359905,
      "grad_norm": 1.4358413219451904,
      "learning_rate": 0.00011645342481552301,
      "loss": 1.0134,
      "step": 5518
    },
    {
      "epoch": 0.7759032756923943,
      "grad_norm": 1.4288928508758545,
      "learning_rate": 0.00011668268139718559,
      "loss": 1.2383,
      "step": 5519
    },
    {
      "epoch": 0.7760438633487979,
      "grad_norm": 1.7640436887741089,
      "learning_rate": 0.00011691184782223735,
      "loss": 0.9707,
      "step": 5520
    },
    {
      "epoch": 0.7761844510052017,
      "grad_norm": 1.6170525550842285,
      "learning_rate": 0.00011714092285221674,
      "loss": 1.1632,
      "step": 5521
    },
    {
      "epoch": 0.7763250386616055,
      "grad_norm": 1.582384705543518,
      "learning_rate": 0.00011736990524915508,
      "loss": 1.1946,
      "step": 5522
    },
    {
      "epoch": 0.7764656263180093,
      "grad_norm": 1.4584747552871704,
      "learning_rate": 0.00011759879377558485,
      "loss": 1.1679,
      "step": 5523
    },
    {
      "epoch": 0.7766062139744131,
      "grad_norm": 1.5388820171356201,
      "learning_rate": 0.00011782758719454612,
      "loss": 1.2476,
      "step": 5524
    },
    {
      "epoch": 0.7767468016308168,
      "grad_norm": 1.3941351175308228,
      "learning_rate": 0.00011805628426959266,
      "loss": 1.3018,
      "step": 5525
    },
    {
      "epoch": 0.7768873892872206,
      "grad_norm": 1.4740849733352661,
      "learning_rate": 0.00011828488376479828,
      "loss": 0.9744,
      "step": 5526
    },
    {
      "epoch": 0.7770279769436244,
      "grad_norm": 1.610359787940979,
      "learning_rate": 0.00011851338444476527,
      "loss": 1.1878,
      "step": 5527
    },
    {
      "epoch": 0.7771685646000281,
      "grad_norm": 1.3510148525238037,
      "learning_rate": 0.00011874178507462921,
      "loss": 1.0118,
      "step": 5528
    },
    {
      "epoch": 0.7773091522564319,
      "grad_norm": 1.4790215492248535,
      "learning_rate": 0.00011897008442006654,
      "loss": 1.1149,
      "step": 5529
    },
    {
      "epoch": 0.7774497399128356,
      "grad_norm": 1.4522075653076172,
      "learning_rate": 0.00011919828124730106,
      "loss": 1.0955,
      "step": 5530
    },
    {
      "epoch": 0.7775903275692394,
      "grad_norm": 1.5414754152297974,
      "learning_rate": 0.00011942637432311044,
      "loss": 1.1956,
      "step": 5531
    },
    {
      "epoch": 0.7777309152256432,
      "grad_norm": 1.4726097583770752,
      "learning_rate": 0.0001196543624148337,
      "loss": 1.2357,
      "step": 5532
    },
    {
      "epoch": 0.777871502882047,
      "grad_norm": 1.6013164520263672,
      "learning_rate": 0.00011988224429037622,
      "loss": 1.0539,
      "step": 5533
    },
    {
      "epoch": 0.7780120905384508,
      "grad_norm": 1.358528971672058,
      "learning_rate": 0.0001201100187182179,
      "loss": 0.9743,
      "step": 5534
    },
    {
      "epoch": 0.7781526781948545,
      "grad_norm": 1.4335037469863892,
      "learning_rate": 0.00012033768446741964,
      "loss": 1.1016,
      "step": 5535
    },
    {
      "epoch": 0.7782932658512582,
      "grad_norm": 1.4006026983261108,
      "learning_rate": 0.00012056524030762916,
      "loss": 1.0348,
      "step": 5536
    },
    {
      "epoch": 0.778433853507662,
      "grad_norm": 1.6717196702957153,
      "learning_rate": 0.00012079268500908833,
      "loss": 1.0769,
      "step": 5537
    },
    {
      "epoch": 0.7785744411640658,
      "grad_norm": 1.7739191055297852,
      "learning_rate": 0.00012102001734263962,
      "loss": 1.1113,
      "step": 5538
    },
    {
      "epoch": 0.7787150288204696,
      "grad_norm": 1.748705267906189,
      "learning_rate": 0.00012124723607973256,
      "loss": 1.0496,
      "step": 5539
    },
    {
      "epoch": 0.7788556164768733,
      "grad_norm": 1.5208414793014526,
      "learning_rate": 0.00012147433999243134,
      "loss": 1.1185,
      "step": 5540
    },
    {
      "epoch": 0.7789962041332771,
      "grad_norm": 1.5571465492248535,
      "learning_rate": 0.00012170132785341959,
      "loss": 0.9324,
      "step": 5541
    },
    {
      "epoch": 0.7791367917896809,
      "grad_norm": 1.5662784576416016,
      "learning_rate": 0.00012192819843600865,
      "loss": 1.1571,
      "step": 5542
    },
    {
      "epoch": 0.7792773794460847,
      "grad_norm": 1.7923420667648315,
      "learning_rate": 0.00012215495051414395,
      "loss": 1.1033,
      "step": 5543
    },
    {
      "epoch": 0.7794179671024885,
      "grad_norm": 1.4136308431625366,
      "learning_rate": 0.00012238158286241113,
      "loss": 1.0235,
      "step": 5544
    },
    {
      "epoch": 0.7795585547588921,
      "grad_norm": 1.5282459259033203,
      "learning_rate": 0.0001226080942560422,
      "loss": 1.0131,
      "step": 5545
    },
    {
      "epoch": 0.7796991424152959,
      "grad_norm": 1.3774656057357788,
      "learning_rate": 0.00012283448347092394,
      "loss": 1.0862,
      "step": 5546
    },
    {
      "epoch": 0.7798397300716997,
      "grad_norm": 1.3188681602478027,
      "learning_rate": 0.00012306074928360275,
      "loss": 1.1468,
      "step": 5547
    },
    {
      "epoch": 0.7799803177281035,
      "grad_norm": 1.6134953498840332,
      "learning_rate": 0.00012328689047129204,
      "loss": 1.0852,
      "step": 5548
    },
    {
      "epoch": 0.7801209053845073,
      "grad_norm": 1.786989688873291,
      "learning_rate": 0.00012351290581187873,
      "loss": 1.0076,
      "step": 5549
    },
    {
      "epoch": 0.780261493040911,
      "grad_norm": 1.4669002294540405,
      "learning_rate": 0.00012373879408392972,
      "loss": 1.1699,
      "step": 5550
    },
    {
      "epoch": 0.7804020806973148,
      "grad_norm": 1.602731704711914,
      "learning_rate": 0.0001239645540666992,
      "loss": 1.2429,
      "step": 5551
    },
    {
      "epoch": 0.7805426683537185,
      "grad_norm": 1.4792585372924805,
      "learning_rate": 0.00012419018454013373,
      "loss": 1.0552,
      "step": 5552
    },
    {
      "epoch": 0.7806832560101223,
      "grad_norm": 1.660774827003479,
      "learning_rate": 0.00012441568428488023,
      "loss": 1.0405,
      "step": 5553
    },
    {
      "epoch": 0.7808238436665261,
      "grad_norm": 1.5376135110855103,
      "learning_rate": 0.0001246410520822925,
      "loss": 1.2104,
      "step": 5554
    },
    {
      "epoch": 0.7809644313229298,
      "grad_norm": 1.5293513536453247,
      "learning_rate": 0.00012486628671443679,
      "loss": 1.0458,
      "step": 5555
    },
    {
      "epoch": 0.7811050189793336,
      "grad_norm": 1.7144984006881714,
      "learning_rate": 0.00012509138696409927,
      "loss": 0.9319,
      "step": 5556
    },
    {
      "epoch": 0.7812456066357374,
      "grad_norm": 1.2997870445251465,
      "learning_rate": 0.0001253163516147923,
      "loss": 1.1437,
      "step": 5557
    },
    {
      "epoch": 0.7813861942921412,
      "grad_norm": 1.3640278577804565,
      "learning_rate": 0.00012554117945076093,
      "loss": 1.0736,
      "step": 5558
    },
    {
      "epoch": 0.781526781948545,
      "grad_norm": 1.3622573614120483,
      "learning_rate": 0.00012576586925699013,
      "loss": 1.0605,
      "step": 5559
    },
    {
      "epoch": 0.7816673696049486,
      "grad_norm": 1.6923670768737793,
      "learning_rate": 0.00012599041981920997,
      "loss": 1.149,
      "step": 5560
    },
    {
      "epoch": 0.7818079572613524,
      "grad_norm": 1.5631389617919922,
      "learning_rate": 0.0001262148299239034,
      "loss": 1.1489,
      "step": 5561
    },
    {
      "epoch": 0.7819485449177562,
      "grad_norm": 1.3515472412109375,
      "learning_rate": 0.00012643909835831281,
      "loss": 1.1279,
      "step": 5562
    },
    {
      "epoch": 0.78208913257416,
      "grad_norm": 1.6097798347473145,
      "learning_rate": 0.00012666322391044572,
      "loss": 0.9772,
      "step": 5563
    },
    {
      "epoch": 0.7822297202305638,
      "grad_norm": 1.2947033643722534,
      "learning_rate": 0.0001268872053690819,
      "loss": 1.1287,
      "step": 5564
    },
    {
      "epoch": 0.7823703078869675,
      "grad_norm": 1.4198120832443237,
      "learning_rate": 0.00012711104152378,
      "loss": 1.0299,
      "step": 5565
    },
    {
      "epoch": 0.7825108955433713,
      "grad_norm": 1.415968656539917,
      "learning_rate": 0.00012733473116488374,
      "loss": 1.1621,
      "step": 5566
    },
    {
      "epoch": 0.7826514831997751,
      "grad_norm": 1.5678163766860962,
      "learning_rate": 0.00012755827308352872,
      "loss": 0.9933,
      "step": 5567
    },
    {
      "epoch": 0.7827920708561789,
      "grad_norm": 1.6019206047058105,
      "learning_rate": 0.00012778166607164886,
      "loss": 1.0861,
      "step": 5568
    },
    {
      "epoch": 0.7829326585125826,
      "grad_norm": 1.6614123582839966,
      "learning_rate": 0.00012800490892198274,
      "loss": 1.1858,
      "step": 5569
    },
    {
      "epoch": 0.7830732461689863,
      "grad_norm": 1.6095753908157349,
      "learning_rate": 0.000128228000428081,
      "loss": 1.0735,
      "step": 5570
    },
    {
      "epoch": 0.7832138338253901,
      "grad_norm": 1.421250820159912,
      "learning_rate": 0.00012845093938431098,
      "loss": 1.0489,
      "step": 5571
    },
    {
      "epoch": 0.7833544214817939,
      "grad_norm": 1.5175213813781738,
      "learning_rate": 0.00012867372458586567,
      "loss": 1.1077,
      "step": 5572
    },
    {
      "epoch": 0.7834950091381977,
      "grad_norm": 1.3379660844802856,
      "learning_rate": 0.00012889635482876824,
      "loss": 0.9964,
      "step": 5573
    },
    {
      "epoch": 0.7836355967946015,
      "grad_norm": 1.5404894351959229,
      "learning_rate": 0.00012911882890987946,
      "loss": 1.0018,
      "step": 5574
    },
    {
      "epoch": 0.7837761844510052,
      "grad_norm": 1.65001380443573,
      "learning_rate": 0.00012934114562690406,
      "loss": 1.2196,
      "step": 5575
    },
    {
      "epoch": 0.783916772107409,
      "grad_norm": 1.322904109954834,
      "learning_rate": 0.0001295633037783972,
      "loss": 1.0365,
      "step": 5576
    },
    {
      "epoch": 0.7840573597638127,
      "grad_norm": 1.5621763467788696,
      "learning_rate": 0.00012978530216377078,
      "loss": 1.0583,
      "step": 5577
    },
    {
      "epoch": 0.7841979474202165,
      "grad_norm": 1.376332402229309,
      "learning_rate": 0.00013000713958330077,
      "loss": 1.0072,
      "step": 5578
    },
    {
      "epoch": 0.7843385350766203,
      "grad_norm": 1.296595573425293,
      "learning_rate": 0.00013022881483813186,
      "loss": 1.0079,
      "step": 5579
    },
    {
      "epoch": 0.784479122733024,
      "grad_norm": 1.4044209718704224,
      "learning_rate": 0.0001304503267302863,
      "loss": 1.0362,
      "step": 5580
    },
    {
      "epoch": 0.7846197103894278,
      "grad_norm": 1.5517892837524414,
      "learning_rate": 0.0001306716740626685,
      "loss": 1.0055,
      "step": 5581
    },
    {
      "epoch": 0.7847602980458316,
      "grad_norm": 1.451204538345337,
      "learning_rate": 0.00013089285563907242,
      "loss": 1.0962,
      "step": 5582
    },
    {
      "epoch": 0.7849008857022354,
      "grad_norm": 1.4309781789779663,
      "learning_rate": 0.00013111387026418765,
      "loss": 1.1922,
      "step": 5583
    },
    {
      "epoch": 0.7850414733586392,
      "grad_norm": 1.6148037910461426,
      "learning_rate": 0.00013133471674360613,
      "loss": 1.1475,
      "step": 5584
    },
    {
      "epoch": 0.7851820610150428,
      "grad_norm": 1.5660749673843384,
      "learning_rate": 0.00013155539388382852,
      "loss": 1.0952,
      "step": 5585
    },
    {
      "epoch": 0.7853226486714466,
      "grad_norm": 1.5160824060440063,
      "learning_rate": 0.0001317759004922705,
      "loss": 1.0711,
      "step": 5586
    },
    {
      "epoch": 0.7854632363278504,
      "grad_norm": 1.8241794109344482,
      "learning_rate": 0.0001319962353772695,
      "loss": 1.0456,
      "step": 5587
    },
    {
      "epoch": 0.7856038239842542,
      "grad_norm": 1.465441346168518,
      "learning_rate": 0.00013221639734809062,
      "loss": 0.9879,
      "step": 5588
    },
    {
      "epoch": 0.785744411640658,
      "grad_norm": 1.572426676750183,
      "learning_rate": 0.00013243638521493432,
      "loss": 1.0694,
      "step": 5589
    },
    {
      "epoch": 0.7858849992970617,
      "grad_norm": 1.533159613609314,
      "learning_rate": 0.00013265619778894054,
      "loss": 1.2315,
      "step": 5590
    },
    {
      "epoch": 0.7860255869534655,
      "grad_norm": 1.3051337003707886,
      "learning_rate": 0.00013287583388219788,
      "loss": 1.2876,
      "step": 5591
    },
    {
      "epoch": 0.7861661746098693,
      "grad_norm": 1.730096697807312,
      "learning_rate": 0.00013309529230774798,
      "loss": 1.039,
      "step": 5592
    },
    {
      "epoch": 0.786306762266273,
      "grad_norm": 1.4026286602020264,
      "learning_rate": 0.00013331457187959274,
      "loss": 1.0175,
      "step": 5593
    },
    {
      "epoch": 0.7864473499226768,
      "grad_norm": 1.7236859798431396,
      "learning_rate": 0.0001335336714127007,
      "loss": 0.9509,
      "step": 5594
    },
    {
      "epoch": 0.7865879375790805,
      "grad_norm": 1.5127487182617188,
      "learning_rate": 0.0001337525897230133,
      "loss": 1.1074,
      "step": 5595
    },
    {
      "epoch": 0.7867285252354843,
      "grad_norm": 1.946820616722107,
      "learning_rate": 0.00013397132562745117,
      "loss": 1.1296,
      "step": 5596
    },
    {
      "epoch": 0.7868691128918881,
      "grad_norm": 1.4466369152069092,
      "learning_rate": 0.00013418987794392145,
      "loss": 1.0444,
      "step": 5597
    },
    {
      "epoch": 0.7870097005482919,
      "grad_norm": 1.4259413480758667,
      "learning_rate": 0.00013440824549132215,
      "loss": 1.087,
      "step": 5598
    },
    {
      "epoch": 0.7871502882046957,
      "grad_norm": 1.3837573528289795,
      "learning_rate": 0.0001346264270895511,
      "loss": 1.0958,
      "step": 5599
    },
    {
      "epoch": 0.7872908758610994,
      "grad_norm": 1.7041758298873901,
      "learning_rate": 0.0001348444215595103,
      "loss": 1.0768,
      "step": 5600
    },
    {
      "epoch": 0.7874314635175031,
      "grad_norm": 1.7288165092468262,
      "learning_rate": 0.00013506222772311316,
      "loss": 1.2027,
      "step": 5601
    },
    {
      "epoch": 0.7875720511739069,
      "grad_norm": 1.2921909093856812,
      "learning_rate": 0.0001352798444032908,
      "loss": 1.0523,
      "step": 5602
    },
    {
      "epoch": 0.7877126388303107,
      "grad_norm": 1.8259844779968262,
      "learning_rate": 0.00013549727042399832,
      "loss": 1.0083,
      "step": 5603
    },
    {
      "epoch": 0.7878532264867145,
      "grad_norm": 1.414042353630066,
      "learning_rate": 0.000135714504610221,
      "loss": 1.0231,
      "step": 5604
    },
    {
      "epoch": 0.7879938141431182,
      "grad_norm": 1.3684697151184082,
      "learning_rate": 0.00013593154578798157,
      "loss": 1.0622,
      "step": 5605
    },
    {
      "epoch": 0.788134401799522,
      "grad_norm": 1.401868224143982,
      "learning_rate": 0.00013614839278434457,
      "loss": 0.9551,
      "step": 5606
    },
    {
      "epoch": 0.7882749894559258,
      "grad_norm": 1.4144128561019897,
      "learning_rate": 0.00013636504442742512,
      "loss": 1.1625,
      "step": 5607
    },
    {
      "epoch": 0.7884155771123296,
      "grad_norm": 1.4678175449371338,
      "learning_rate": 0.0001365814995463936,
      "loss": 1.1232,
      "step": 5608
    },
    {
      "epoch": 0.7885561647687334,
      "grad_norm": 1.3464306592941284,
      "learning_rate": 0.00013679775697148194,
      "loss": 0.9356,
      "step": 5609
    },
    {
      "epoch": 0.788696752425137,
      "grad_norm": 1.4150291681289673,
      "learning_rate": 0.0001370138155339914,
      "loss": 1.2191,
      "step": 5610
    },
    {
      "epoch": 0.7888373400815408,
      "grad_norm": 1.5973212718963623,
      "learning_rate": 0.00013722967406629732,
      "loss": 0.9093,
      "step": 5611
    },
    {
      "epoch": 0.7889779277379446,
      "grad_norm": 1.2328665256500244,
      "learning_rate": 0.00013744533140185622,
      "loss": 1.1888,
      "step": 5612
    },
    {
      "epoch": 0.7891185153943484,
      "grad_norm": 1.6444063186645508,
      "learning_rate": 0.00013766078637521183,
      "loss": 0.8679,
      "step": 5613
    },
    {
      "epoch": 0.7892591030507522,
      "grad_norm": 1.3145257234573364,
      "learning_rate": 0.00013787603782200146,
      "loss": 1.1918,
      "step": 5614
    },
    {
      "epoch": 0.7893996907071559,
      "grad_norm": 1.5414947271347046,
      "learning_rate": 0.00013809108457896275,
      "loss": 1.0688,
      "step": 5615
    },
    {
      "epoch": 0.7895402783635597,
      "grad_norm": 1.3908318281173706,
      "learning_rate": 0.00013830592548393916,
      "loss": 1.0648,
      "step": 5616
    },
    {
      "epoch": 0.7896808660199635,
      "grad_norm": 1.480819582939148,
      "learning_rate": 0.00013852055937588618,
      "loss": 1.2854,
      "step": 5617
    },
    {
      "epoch": 0.7898214536763672,
      "grad_norm": 1.3167513608932495,
      "learning_rate": 0.00013873498509487896,
      "loss": 1.1598,
      "step": 5618
    },
    {
      "epoch": 0.789962041332771,
      "grad_norm": 1.723859429359436,
      "learning_rate": 0.00013894920148211707,
      "loss": 1.2355,
      "step": 5619
    },
    {
      "epoch": 0.7901026289891747,
      "grad_norm": 1.5559337139129639,
      "learning_rate": 0.00013916320737993154,
      "loss": 1.2829,
      "step": 5620
    },
    {
      "epoch": 0.7902432166455785,
      "grad_norm": 1.6864264011383057,
      "learning_rate": 0.0001393770016317908,
      "loss": 1.1167,
      "step": 5621
    },
    {
      "epoch": 0.7903838043019823,
      "grad_norm": 1.5447458028793335,
      "learning_rate": 0.00013959058308230717,
      "loss": 1.0327,
      "step": 5622
    },
    {
      "epoch": 0.7905243919583861,
      "grad_norm": 1.7024704217910767,
      "learning_rate": 0.00013980395057724282,
      "loss": 1.1748,
      "step": 5623
    },
    {
      "epoch": 0.7906649796147899,
      "grad_norm": 1.7572596073150635,
      "learning_rate": 0.00014001710296351677,
      "loss": 1.1019,
      "step": 5624
    },
    {
      "epoch": 0.7908055672711936,
      "grad_norm": 1.7711588144302368,
      "learning_rate": 0.0001402300390892094,
      "loss": 1.0094,
      "step": 5625
    },
    {
      "epoch": 0.7909461549275973,
      "grad_norm": 1.3348605632781982,
      "learning_rate": 0.00014044275780357107,
      "loss": 1.0278,
      "step": 5626
    },
    {
      "epoch": 0.7910867425840011,
      "grad_norm": 1.4561105966567993,
      "learning_rate": 0.0001406552579570264,
      "loss": 0.9291,
      "step": 5627
    },
    {
      "epoch": 0.7912273302404049,
      "grad_norm": 1.5254851579666138,
      "learning_rate": 0.0001408675384011808,
      "loss": 0.9715,
      "step": 5628
    },
    {
      "epoch": 0.7913679178968087,
      "grad_norm": 1.6428312063217163,
      "learning_rate": 0.00014107959798882792,
      "loss": 1.1775,
      "step": 5629
    },
    {
      "epoch": 0.7915085055532124,
      "grad_norm": 1.477127194404602,
      "learning_rate": 0.00014129143557395447,
      "loss": 1.0734,
      "step": 5630
    },
    {
      "epoch": 0.7916490932096162,
      "grad_norm": 1.3617712259292603,
      "learning_rate": 0.00014150305001174685,
      "loss": 1.0805,
      "step": 5631
    },
    {
      "epoch": 0.79178968086602,
      "grad_norm": 1.3348193168640137,
      "learning_rate": 0.00014171444015859754,
      "loss": 0.9355,
      "step": 5632
    },
    {
      "epoch": 0.7919302685224238,
      "grad_norm": 1.544110655784607,
      "learning_rate": 0.00014192560487211097,
      "loss": 1.247,
      "step": 5633
    },
    {
      "epoch": 0.7920708561788276,
      "grad_norm": 1.5258915424346924,
      "learning_rate": 0.0001421365430111103,
      "loss": 1.0826,
      "step": 5634
    },
    {
      "epoch": 0.7922114438352312,
      "grad_norm": 1.6430495977401733,
      "learning_rate": 0.0001423472534356428,
      "loss": 1.0478,
      "step": 5635
    },
    {
      "epoch": 0.792352031491635,
      "grad_norm": 1.5684338808059692,
      "learning_rate": 0.0001425577350069859,
      "loss": 0.9938,
      "step": 5636
    },
    {
      "epoch": 0.7924926191480388,
      "grad_norm": 1.5994559526443481,
      "learning_rate": 0.00014276798658765483,
      "loss": 1.0314,
      "step": 5637
    },
    {
      "epoch": 0.7926332068044426,
      "grad_norm": 1.4762688875198364,
      "learning_rate": 0.000142978007041407,
      "loss": 0.9539,
      "step": 5638
    },
    {
      "epoch": 0.7927737944608464,
      "grad_norm": 1.5123330354690552,
      "learning_rate": 0.00014318779523324904,
      "loss": 1.0758,
      "step": 5639
    },
    {
      "epoch": 0.7929143821172501,
      "grad_norm": 1.7013332843780518,
      "learning_rate": 0.00014339735002944285,
      "loss": 1.0451,
      "step": 5640
    },
    {
      "epoch": 0.7930549697736539,
      "grad_norm": 1.6365585327148438,
      "learning_rate": 0.00014360667029751138,
      "loss": 1.03,
      "step": 5641
    },
    {
      "epoch": 0.7931955574300577,
      "grad_norm": 1.7630963325500488,
      "learning_rate": 0.0001438157549062456,
      "loss": 1.0132,
      "step": 5642
    },
    {
      "epoch": 0.7933361450864614,
      "grad_norm": 1.7885102033615112,
      "learning_rate": 0.0001440246027257097,
      "loss": 0.9106,
      "step": 5643
    },
    {
      "epoch": 0.7934767327428652,
      "grad_norm": 1.5088754892349243,
      "learning_rate": 0.00014423321262724693,
      "loss": 1.07,
      "step": 5644
    },
    {
      "epoch": 0.7936173203992689,
      "grad_norm": 1.746026873588562,
      "learning_rate": 0.00014444158348348742,
      "loss": 1.1331,
      "step": 5645
    },
    {
      "epoch": 0.7937579080556727,
      "grad_norm": 1.6333361864089966,
      "learning_rate": 0.00014464971416835252,
      "loss": 1.1466,
      "step": 5646
    },
    {
      "epoch": 0.7938984957120765,
      "grad_norm": 1.7398219108581543,
      "learning_rate": 0.0001448576035570612,
      "loss": 1.0189,
      "step": 5647
    },
    {
      "epoch": 0.7940390833684803,
      "grad_norm": 1.4198212623596191,
      "learning_rate": 0.0001450652505261372,
      "loss": 1.1773,
      "step": 5648
    },
    {
      "epoch": 0.794179671024884,
      "grad_norm": 1.8787436485290527,
      "learning_rate": 0.00014527265395341375,
      "loss": 1.2396,
      "step": 5649
    },
    {
      "epoch": 0.7943202586812877,
      "grad_norm": 1.4699925184249878,
      "learning_rate": 0.00014547981271804085,
      "loss": 1.0412,
      "step": 5650
    },
    {
      "epoch": 0.7944608463376915,
      "grad_norm": 1.6392946243286133,
      "learning_rate": 0.00014568672570048992,
      "loss": 1.0506,
      "step": 5651
    },
    {
      "epoch": 0.7946014339940953,
      "grad_norm": 1.3838471174240112,
      "learning_rate": 0.00014589339178256095,
      "loss": 1.2437,
      "step": 5652
    },
    {
      "epoch": 0.7947420216504991,
      "grad_norm": 1.4227930307388306,
      "learning_rate": 0.00014609980984738868,
      "loss": 1.1783,
      "step": 5653
    },
    {
      "epoch": 0.7948826093069028,
      "grad_norm": 1.7484363317489624,
      "learning_rate": 0.0001463059787794478,
      "loss": 0.9739,
      "step": 5654
    },
    {
      "epoch": 0.7950231969633066,
      "grad_norm": 1.5847816467285156,
      "learning_rate": 0.00014651189746455888,
      "loss": 1.1094,
      "step": 5655
    },
    {
      "epoch": 0.7951637846197104,
      "grad_norm": 1.541422724723816,
      "learning_rate": 0.00014671756478989588,
      "loss": 1.1106,
      "step": 5656
    },
    {
      "epoch": 0.7953043722761142,
      "grad_norm": 1.5829159021377563,
      "learning_rate": 0.00014692297964399032,
      "loss": 0.9962,
      "step": 5657
    },
    {
      "epoch": 0.795444959932518,
      "grad_norm": 1.32750403881073,
      "learning_rate": 0.00014712814091673892,
      "loss": 1.0505,
      "step": 5658
    },
    {
      "epoch": 0.7955855475889216,
      "grad_norm": 1.6743700504302979,
      "learning_rate": 0.0001473330474994079,
      "loss": 1.0747,
      "step": 5659
    },
    {
      "epoch": 0.7957261352453254,
      "grad_norm": 1.4660942554473877,
      "learning_rate": 0.00014753769828464034,
      "loss": 1.3035,
      "step": 5660
    },
    {
      "epoch": 0.7958667229017292,
      "grad_norm": 1.260180115699768,
      "learning_rate": 0.00014774209216646193,
      "loss": 1.0845,
      "step": 5661
    },
    {
      "epoch": 0.796007310558133,
      "grad_norm": 1.5140784978866577,
      "learning_rate": 0.00014794622804028653,
      "loss": 1.1241,
      "step": 5662
    },
    {
      "epoch": 0.7961478982145368,
      "grad_norm": 1.3824011087417603,
      "learning_rate": 0.0001481501048029218,
      "loss": 1.1292,
      "step": 5663
    },
    {
      "epoch": 0.7962884858709405,
      "grad_norm": 1.6494665145874023,
      "learning_rate": 0.00014835372135257657,
      "loss": 1.0817,
      "step": 5664
    },
    {
      "epoch": 0.7964290735273443,
      "grad_norm": 1.3390727043151855,
      "learning_rate": 0.00014855707658886557,
      "loss": 1.0441,
      "step": 5665
    },
    {
      "epoch": 0.7965696611837481,
      "grad_norm": 2.166602611541748,
      "learning_rate": 0.00014876016941281513,
      "loss": 1.404,
      "step": 5666
    },
    {
      "epoch": 0.7967102488401518,
      "grad_norm": 1.5688440799713135,
      "learning_rate": 0.00014896299872687075,
      "loss": 1.2379,
      "step": 5667
    },
    {
      "epoch": 0.7968508364965556,
      "grad_norm": 2.0063350200653076,
      "learning_rate": 0.00014916556343490115,
      "loss": 1.1094,
      "step": 5668
    },
    {
      "epoch": 0.7969914241529593,
      "grad_norm": 1.9332396984100342,
      "learning_rate": 0.0001493678624422058,
      "loss": 1.1097,
      "step": 5669
    },
    {
      "epoch": 0.7971320118093631,
      "grad_norm": 2.1014201641082764,
      "learning_rate": 0.00014956989465551921,
      "loss": 1.1234,
      "step": 5670
    },
    {
      "epoch": 0.7972725994657669,
      "grad_norm": 1.3399276733398438,
      "learning_rate": 0.00014977165898301807,
      "loss": 1.1089,
      "step": 5671
    },
    {
      "epoch": 0.7974131871221707,
      "grad_norm": 1.5425918102264404,
      "learning_rate": 0.00014997315433432716,
      "loss": 1.0159,
      "step": 5672
    },
    {
      "epoch": 0.7975537747785745,
      "grad_norm": 1.4200782775878906,
      "learning_rate": 0.00015017437962052457,
      "loss": 0.9066,
      "step": 5673
    },
    {
      "epoch": 0.7976943624349782,
      "grad_norm": 1.7302223443984985,
      "learning_rate": 0.0001503753337541473,
      "loss": 1.0471,
      "step": 5674
    },
    {
      "epoch": 0.797834950091382,
      "grad_norm": 1.2505970001220703,
      "learning_rate": 0.00015057601564919874,
      "loss": 1.2027,
      "step": 5675
    },
    {
      "epoch": 0.7979755377477857,
      "grad_norm": 1.2446095943450928,
      "learning_rate": 0.00015077642422115276,
      "loss": 1.1713,
      "step": 5676
    },
    {
      "epoch": 0.7981161254041895,
      "grad_norm": 1.4951272010803223,
      "learning_rate": 0.00015097655838696103,
      "loss": 1.0648,
      "step": 5677
    },
    {
      "epoch": 0.7982567130605933,
      "grad_norm": 1.7118096351623535,
      "learning_rate": 0.00015117641706505726,
      "loss": 1.1926,
      "step": 5678
    },
    {
      "epoch": 0.798397300716997,
      "grad_norm": 1.3575928211212158,
      "learning_rate": 0.0001513759991753644,
      "loss": 1.2677,
      "step": 5679
    },
    {
      "epoch": 0.7985378883734008,
      "grad_norm": 1.6323256492614746,
      "learning_rate": 0.00015157530363930035,
      "loss": 0.989,
      "step": 5680
    },
    {
      "epoch": 0.7986784760298046,
      "grad_norm": 1.3392562866210938,
      "learning_rate": 0.0001517743293797832,
      "loss": 1.2168,
      "step": 5681
    },
    {
      "epoch": 0.7988190636862084,
      "grad_norm": 1.3770902156829834,
      "learning_rate": 0.00015197307532123676,
      "loss": 1.2245,
      "step": 5682
    },
    {
      "epoch": 0.7989596513426122,
      "grad_norm": 1.6267471313476562,
      "learning_rate": 0.00015217154038959797,
      "loss": 1.1198,
      "step": 5683
    },
    {
      "epoch": 0.7991002389990158,
      "grad_norm": 1.3441733121871948,
      "learning_rate": 0.000152369723512321,
      "loss": 1.1828,
      "step": 5684
    },
    {
      "epoch": 0.7992408266554196,
      "grad_norm": 1.4427199363708496,
      "learning_rate": 0.00015256762361838384,
      "loss": 1.2098,
      "step": 5685
    },
    {
      "epoch": 0.7993814143118234,
      "grad_norm": 2.031794786453247,
      "learning_rate": 0.00015276523963829395,
      "loss": 1.0708,
      "step": 5686
    },
    {
      "epoch": 0.7995220019682272,
      "grad_norm": 1.4735536575317383,
      "learning_rate": 0.00015296257050409398,
      "loss": 1.0756,
      "step": 5687
    },
    {
      "epoch": 0.799662589624631,
      "grad_norm": 1.6772555112838745,
      "learning_rate": 0.00015315961514936812,
      "loss": 1.268,
      "step": 5688
    },
    {
      "epoch": 0.7998031772810347,
      "grad_norm": 1.6445684432983398,
      "learning_rate": 0.00015335637250924654,
      "loss": 1.178,
      "step": 5689
    },
    {
      "epoch": 0.7999437649374385,
      "grad_norm": 1.5837604999542236,
      "learning_rate": 0.0001535528415204123,
      "loss": 1.0395,
      "step": 5690
    },
    {
      "epoch": 0.8000843525938423,
      "grad_norm": 1.5402393341064453,
      "learning_rate": 0.00015374902112110715,
      "loss": 1.1315,
      "step": 5691
    },
    {
      "epoch": 0.800224940250246,
      "grad_norm": 1.525162935256958,
      "learning_rate": 0.0001539449102511364,
      "loss": 0.9369,
      "step": 5692
    },
    {
      "epoch": 0.8003655279066498,
      "grad_norm": 1.4625253677368164,
      "learning_rate": 0.00015414050785187526,
      "loss": 1.2137,
      "step": 5693
    },
    {
      "epoch": 0.8005061155630535,
      "grad_norm": 1.7157238721847534,
      "learning_rate": 0.00015433581286627442,
      "loss": 1.0715,
      "step": 5694
    },
    {
      "epoch": 0.8006467032194573,
      "grad_norm": 1.378341794013977,
      "learning_rate": 0.00015453082423886564,
      "loss": 1.087,
      "step": 5695
    },
    {
      "epoch": 0.8007872908758611,
      "grad_norm": 1.3470582962036133,
      "learning_rate": 0.00015472554091576813,
      "loss": 1.1046,
      "step": 5696
    },
    {
      "epoch": 0.8009278785322649,
      "grad_norm": 1.4451959133148193,
      "learning_rate": 0.00015491996184469287,
      "loss": 1.4123,
      "step": 5697
    },
    {
      "epoch": 0.8010684661886687,
      "grad_norm": 1.6420955657958984,
      "learning_rate": 0.0001551140859749495,
      "loss": 1.1789,
      "step": 5698
    },
    {
      "epoch": 0.8012090538450723,
      "grad_norm": 1.397789716720581,
      "learning_rate": 0.000155307912257452,
      "loss": 1.0206,
      "step": 5699
    },
    {
      "epoch": 0.8013496415014761,
      "grad_norm": 1.521833896636963,
      "learning_rate": 0.00015550143964472354,
      "loss": 1.1297,
      "step": 5700
    },
    {
      "epoch": 0.8014902291578799,
      "grad_norm": 1.5013799667358398,
      "learning_rate": 0.0001556946670909023,
      "loss": 1.0724,
      "step": 5701
    },
    {
      "epoch": 0.8016308168142837,
      "grad_norm": 1.7038522958755493,
      "learning_rate": 0.00015588759355174823,
      "loss": 1.0359,
      "step": 5702
    },
    {
      "epoch": 0.8017714044706875,
      "grad_norm": 1.7611379623413086,
      "learning_rate": 0.00015608021798464722,
      "loss": 1.0374,
      "step": 5703
    },
    {
      "epoch": 0.8019119921270912,
      "grad_norm": 1.465014934539795,
      "learning_rate": 0.0001562725393486176,
      "loss": 1.0924,
      "step": 5704
    },
    {
      "epoch": 0.802052579783495,
      "grad_norm": 2.1919872760772705,
      "learning_rate": 0.00015646455660431552,
      "loss": 1.1464,
      "step": 5705
    },
    {
      "epoch": 0.8021931674398988,
      "grad_norm": 1.4380505084991455,
      "learning_rate": 0.00015665626871404044,
      "loss": 1.1747,
      "step": 5706
    },
    {
      "epoch": 0.8023337550963026,
      "grad_norm": 1.7741841077804565,
      "learning_rate": 0.00015684767464174145,
      "loss": 1.0196,
      "step": 5707
    },
    {
      "epoch": 0.8024743427527063,
      "grad_norm": 1.659224271774292,
      "learning_rate": 0.00015703877335302154,
      "loss": 1.1425,
      "step": 5708
    },
    {
      "epoch": 0.80261493040911,
      "grad_norm": 1.545297622680664,
      "learning_rate": 0.00015722956381514425,
      "loss": 1.1056,
      "step": 5709
    },
    {
      "epoch": 0.8027555180655138,
      "grad_norm": 1.6022683382034302,
      "learning_rate": 0.00015742004499703932,
      "loss": 1.0439,
      "step": 5710
    },
    {
      "epoch": 0.8028961057219176,
      "grad_norm": 1.3503202199935913,
      "learning_rate": 0.00015761021586930754,
      "loss": 1.0151,
      "step": 5711
    },
    {
      "epoch": 0.8030366933783214,
      "grad_norm": 1.5832364559173584,
      "learning_rate": 0.0001578000754042267,
      "loss": 1.1397,
      "step": 5712
    },
    {
      "epoch": 0.8031772810347252,
      "grad_norm": 1.6288056373596191,
      "learning_rate": 0.00015798962257575726,
      "loss": 1.0815,
      "step": 5713
    },
    {
      "epoch": 0.8033178686911289,
      "grad_norm": 1.432247519493103,
      "learning_rate": 0.00015817885635954738,
      "loss": 1.1311,
      "step": 5714
    },
    {
      "epoch": 0.8034584563475327,
      "grad_norm": 1.627925157546997,
      "learning_rate": 0.00015836777573293969,
      "loss": 1.2746,
      "step": 5715
    },
    {
      "epoch": 0.8035990440039364,
      "grad_norm": 1.5419883728027344,
      "learning_rate": 0.00015855637967497485,
      "loss": 0.9149,
      "step": 5716
    },
    {
      "epoch": 0.8037396316603402,
      "grad_norm": 1.4279956817626953,
      "learning_rate": 0.0001587446671663988,
      "loss": 1.1017,
      "step": 5717
    },
    {
      "epoch": 0.803880219316744,
      "grad_norm": 1.568623661994934,
      "learning_rate": 0.0001589326371896678,
      "loss": 1.1843,
      "step": 5718
    },
    {
      "epoch": 0.8040208069731477,
      "grad_norm": 1.4893977642059326,
      "learning_rate": 0.00015912028872895343,
      "loss": 1.243,
      "step": 5719
    },
    {
      "epoch": 0.8041613946295515,
      "grad_norm": 1.5769963264465332,
      "learning_rate": 0.00015930762077014847,
      "loss": 1.0853,
      "step": 5720
    },
    {
      "epoch": 0.8043019822859553,
      "grad_norm": 1.6960009336471558,
      "learning_rate": 0.0001594946323008724,
      "loss": 1.0485,
      "step": 5721
    },
    {
      "epoch": 0.8044425699423591,
      "grad_norm": 1.4665942192077637,
      "learning_rate": 0.00015968132231047682,
      "loss": 1.2148,
      "step": 5722
    },
    {
      "epoch": 0.8045831575987629,
      "grad_norm": 1.5795809030532837,
      "learning_rate": 0.00015986768979005085,
      "loss": 1.0435,
      "step": 5723
    },
    {
      "epoch": 0.8047237452551665,
      "grad_norm": 1.5913575887680054,
      "learning_rate": 0.0001600537337324266,
      "loss": 1.2155,
      "step": 5724
    },
    {
      "epoch": 0.8048643329115703,
      "grad_norm": 1.5109450817108154,
      "learning_rate": 0.00016023945313218462,
      "loss": 1.1125,
      "step": 5725
    },
    {
      "epoch": 0.8050049205679741,
      "grad_norm": 1.451585292816162,
      "learning_rate": 0.00016042484698565977,
      "loss": 0.9053,
      "step": 5726
    },
    {
      "epoch": 0.8051455082243779,
      "grad_norm": 1.5580374002456665,
      "learning_rate": 0.0001606099142909454,
      "loss": 1.114,
      "step": 5727
    },
    {
      "epoch": 0.8052860958807817,
      "grad_norm": 1.4874242544174194,
      "learning_rate": 0.00016079465404790039,
      "loss": 1.061,
      "step": 5728
    },
    {
      "epoch": 0.8054266835371854,
      "grad_norm": 1.4909095764160156,
      "learning_rate": 0.00016097906525815338,
      "loss": 1.0107,
      "step": 5729
    },
    {
      "epoch": 0.8055672711935892,
      "grad_norm": 1.5545469522476196,
      "learning_rate": 0.00016116314692510856,
      "loss": 1.0742,
      "step": 5730
    },
    {
      "epoch": 0.805707858849993,
      "grad_norm": 1.5215692520141602,
      "learning_rate": 0.00016134689805395112,
      "loss": 1.1531,
      "step": 5731
    },
    {
      "epoch": 0.8058484465063968,
      "grad_norm": 1.5538671016693115,
      "learning_rate": 0.00016153031765165244,
      "loss": 1.0998,
      "step": 5732
    },
    {
      "epoch": 0.8059890341628005,
      "grad_norm": 1.512155294418335,
      "learning_rate": 0.00016171340472697553,
      "loss": 1.0497,
      "step": 5733
    },
    {
      "epoch": 0.8061296218192042,
      "grad_norm": 1.3926509618759155,
      "learning_rate": 0.00016189615829048098,
      "loss": 1.2417,
      "step": 5734
    },
    {
      "epoch": 0.806270209475608,
      "grad_norm": 2.133988380432129,
      "learning_rate": 0.00016207857735453063,
      "loss": 1.1426,
      "step": 5735
    },
    {
      "epoch": 0.8064107971320118,
      "grad_norm": 1.5682400465011597,
      "learning_rate": 0.00016226066093329516,
      "loss": 1.1925,
      "step": 5736
    },
    {
      "epoch": 0.8065513847884156,
      "grad_norm": 1.5079593658447266,
      "learning_rate": 0.00016244240804275765,
      "loss": 1.2005,
      "step": 5737
    },
    {
      "epoch": 0.8066919724448194,
      "grad_norm": 1.4995418787002563,
      "learning_rate": 0.00016262381770071964,
      "loss": 1.1118,
      "step": 5738
    },
    {
      "epoch": 0.8068325601012231,
      "grad_norm": 1.5487691164016724,
      "learning_rate": 0.00016280488892680644,
      "loss": 0.9962,
      "step": 5739
    },
    {
      "epoch": 0.8069731477576269,
      "grad_norm": 1.5999518632888794,
      "learning_rate": 0.0001629856207424721,
      "loss": 0.9394,
      "step": 5740
    },
    {
      "epoch": 0.8071137354140306,
      "grad_norm": 1.5059882402420044,
      "learning_rate": 0.0001631660121710052,
      "loss": 1.075,
      "step": 5741
    },
    {
      "epoch": 0.8072543230704344,
      "grad_norm": 1.2887698411941528,
      "learning_rate": 0.00016334606223753363,
      "loss": 1.0792,
      "step": 5742
    },
    {
      "epoch": 0.8073949107268382,
      "grad_norm": 1.6343480348587036,
      "learning_rate": 0.0001635257699690301,
      "loss": 1.1148,
      "step": 5743
    },
    {
      "epoch": 0.8075354983832419,
      "grad_norm": 1.7865283489227295,
      "learning_rate": 0.0001637051343943173,
      "loss": 1.0572,
      "step": 5744
    },
    {
      "epoch": 0.8076760860396457,
      "grad_norm": 1.4392718076705933,
      "learning_rate": 0.00016388415454407374,
      "loss": 1.2039,
      "step": 5745
    },
    {
      "epoch": 0.8078166736960495,
      "grad_norm": 1.3723490238189697,
      "learning_rate": 0.00016406282945083747,
      "loss": 1.2145,
      "step": 5746
    },
    {
      "epoch": 0.8079572613524533,
      "grad_norm": 1.7210584878921509,
      "learning_rate": 0.00016424115814901326,
      "loss": 1.0788,
      "step": 5747
    },
    {
      "epoch": 0.8080978490088571,
      "grad_norm": 1.3565744161605835,
      "learning_rate": 0.0001644191396748764,
      "loss": 1.2574,
      "step": 5748
    },
    {
      "epoch": 0.8082384366652607,
      "grad_norm": 1.523324966430664,
      "learning_rate": 0.00016459677306657837,
      "loss": 1.0979,
      "step": 5749
    },
    {
      "epoch": 0.8083790243216645,
      "grad_norm": 1.4342013597488403,
      "learning_rate": 0.0001647740573641522,
      "loss": 1.1804,
      "step": 5750
    },
    {
      "epoch": 0.8085196119780683,
      "grad_norm": 1.5282158851623535,
      "learning_rate": 0.00016495099160951737,
      "loss": 1.1458,
      "step": 5751
    },
    {
      "epoch": 0.8086601996344721,
      "grad_norm": 1.5015190839767456,
      "learning_rate": 0.00016512757484648502,
      "loss": 1.1253,
      "step": 5752
    },
    {
      "epoch": 0.8088007872908759,
      "grad_norm": 1.8936903476715088,
      "learning_rate": 0.00016530380612076373,
      "loss": 1.0385,
      "step": 5753
    },
    {
      "epoch": 0.8089413749472796,
      "grad_norm": 1.670353889465332,
      "learning_rate": 0.00016547968447996327,
      "loss": 1.0397,
      "step": 5754
    },
    {
      "epoch": 0.8090819626036834,
      "grad_norm": 1.7460968494415283,
      "learning_rate": 0.0001656552089736015,
      "loss": 1.0467,
      "step": 5755
    },
    {
      "epoch": 0.8092225502600872,
      "grad_norm": 1.4712988138198853,
      "learning_rate": 0.00016583037865310822,
      "loss": 1.1452,
      "step": 5756
    },
    {
      "epoch": 0.809363137916491,
      "grad_norm": 2.1142280101776123,
      "learning_rate": 0.0001660051925718307,
      "loss": 1.2491,
      "step": 5757
    },
    {
      "epoch": 0.8095037255728947,
      "grad_norm": 1.9158499240875244,
      "learning_rate": 0.00016617964978503892,
      "loss": 1.2036,
      "step": 5758
    },
    {
      "epoch": 0.8096443132292984,
      "grad_norm": 1.5129985809326172,
      "learning_rate": 0.00016635374934993057,
      "loss": 1.082,
      "step": 5759
    },
    {
      "epoch": 0.8097849008857022,
      "grad_norm": 1.4786715507507324,
      "learning_rate": 0.00016652749032563591,
      "loss": 1.0775,
      "step": 5760
    },
    {
      "epoch": 0.809925488542106,
      "grad_norm": 1.5206594467163086,
      "learning_rate": 0.00016670087177322377,
      "loss": 1.0725,
      "step": 5761
    },
    {
      "epoch": 0.8100660761985098,
      "grad_norm": 1.438468337059021,
      "learning_rate": 0.00016687389275570492,
      "loss": 1.1579,
      "step": 5762
    },
    {
      "epoch": 0.8102066638549136,
      "grad_norm": 1.5131474733352661,
      "learning_rate": 0.00016704655233803903,
      "loss": 1.1815,
      "step": 5763
    },
    {
      "epoch": 0.8103472515113173,
      "grad_norm": 1.5328601598739624,
      "learning_rate": 0.00016721884958713863,
      "loss": 1.1084,
      "step": 5764
    },
    {
      "epoch": 0.810487839167721,
      "grad_norm": 1.290543794631958,
      "learning_rate": 0.00016739078357187382,
      "loss": 1.1824,
      "step": 5765
    },
    {
      "epoch": 0.8106284268241248,
      "grad_norm": 1.3997935056686401,
      "learning_rate": 0.00016756235336307866,
      "loss": 1.0045,
      "step": 5766
    },
    {
      "epoch": 0.8107690144805286,
      "grad_norm": 1.455236554145813,
      "learning_rate": 0.00016773355803355498,
      "loss": 1.3117,
      "step": 5767
    },
    {
      "epoch": 0.8109096021369324,
      "grad_norm": 1.3410011529922485,
      "learning_rate": 0.00016790439665807775,
      "loss": 1.1312,
      "step": 5768
    },
    {
      "epoch": 0.8110501897933361,
      "grad_norm": 1.5433731079101562,
      "learning_rate": 0.0001680748683134003,
      "loss": 1.134,
      "step": 5769
    },
    {
      "epoch": 0.8111907774497399,
      "grad_norm": 1.546483039855957,
      "learning_rate": 0.00016824497207825888,
      "loss": 0.9889,
      "step": 5770
    },
    {
      "epoch": 0.8113313651061437,
      "grad_norm": 1.4178482294082642,
      "learning_rate": 0.0001684147070333784,
      "loss": 1.1241,
      "step": 5771
    },
    {
      "epoch": 0.8114719527625475,
      "grad_norm": 1.326355218887329,
      "learning_rate": 0.00016858407226147648,
      "loss": 0.9307,
      "step": 5772
    },
    {
      "epoch": 0.8116125404189513,
      "grad_norm": 1.414809226989746,
      "learning_rate": 0.00016875306684726858,
      "loss": 1.0763,
      "step": 5773
    },
    {
      "epoch": 0.8117531280753549,
      "grad_norm": 1.6696404218673706,
      "learning_rate": 0.00016892168987747382,
      "loss": 1.18,
      "step": 5774
    },
    {
      "epoch": 0.8118937157317587,
      "grad_norm": 1.3377764225006104,
      "learning_rate": 0.00016908994044081895,
      "loss": 1.1563,
      "step": 5775
    },
    {
      "epoch": 0.8120343033881625,
      "grad_norm": 1.3739420175552368,
      "learning_rate": 0.0001692578176280436,
      "loss": 0.9673,
      "step": 5776
    },
    {
      "epoch": 0.8121748910445663,
      "grad_norm": 1.4684382677078247,
      "learning_rate": 0.00016942532053190517,
      "loss": 1.16,
      "step": 5777
    },
    {
      "epoch": 0.8123154787009701,
      "grad_norm": 1.4818003177642822,
      "learning_rate": 0.00016959244824718393,
      "loss": 1.0684,
      "step": 5778
    },
    {
      "epoch": 0.8124560663573738,
      "grad_norm": 1.4691050052642822,
      "learning_rate": 0.00016975919987068742,
      "loss": 1.0208,
      "step": 5779
    },
    {
      "epoch": 0.8125966540137776,
      "grad_norm": 1.3566340208053589,
      "learning_rate": 0.00016992557450125627,
      "loss": 1.1719,
      "step": 5780
    },
    {
      "epoch": 0.8127372416701814,
      "grad_norm": 1.4202677011489868,
      "learning_rate": 0.00017009157123976765,
      "loss": 1.1659,
      "step": 5781
    },
    {
      "epoch": 0.8128778293265851,
      "grad_norm": 1.711361050605774,
      "learning_rate": 0.00017025718918914161,
      "loss": 1.1488,
      "step": 5782
    },
    {
      "epoch": 0.8130184169829889,
      "grad_norm": 1.3953537940979004,
      "learning_rate": 0.00017042242745434508,
      "loss": 1.0681,
      "step": 5783
    },
    {
      "epoch": 0.8131590046393926,
      "grad_norm": 1.4220561981201172,
      "learning_rate": 0.00017058728514239636,
      "loss": 1.1057,
      "step": 5784
    },
    {
      "epoch": 0.8132995922957964,
      "grad_norm": 1.9043923616409302,
      "learning_rate": 0.00017075176136237133,
      "loss": 1.0527,
      "step": 5785
    },
    {
      "epoch": 0.8134401799522002,
      "grad_norm": 1.6898139715194702,
      "learning_rate": 0.0001709158552254068,
      "loss": 0.9847,
      "step": 5786
    },
    {
      "epoch": 0.813580767608604,
      "grad_norm": 1.3999412059783936,
      "learning_rate": 0.0001710795658447061,
      "loss": 1.1591,
      "step": 5787
    },
    {
      "epoch": 0.8137213552650078,
      "grad_norm": 1.3460698127746582,
      "learning_rate": 0.00017124289233554372,
      "loss": 1.22,
      "step": 5788
    },
    {
      "epoch": 0.8138619429214115,
      "grad_norm": 1.371561050415039,
      "learning_rate": 0.00017140583381526986,
      "loss": 1.0945,
      "step": 5789
    },
    {
      "epoch": 0.8140025305778152,
      "grad_norm": 1.5573794841766357,
      "learning_rate": 0.00017156838940331575,
      "loss": 1.0005,
      "step": 5790
    },
    {
      "epoch": 0.814143118234219,
      "grad_norm": 1.608441948890686,
      "learning_rate": 0.00017173055822119793,
      "loss": 0.9757,
      "step": 5791
    },
    {
      "epoch": 0.8142837058906228,
      "grad_norm": 1.4796730279922485,
      "learning_rate": 0.00017189233939252262,
      "loss": 1.0384,
      "step": 5792
    },
    {
      "epoch": 0.8144242935470266,
      "grad_norm": 1.5651180744171143,
      "learning_rate": 0.00017205373204299182,
      "loss": 0.9544,
      "step": 5793
    },
    {
      "epoch": 0.8145648812034303,
      "grad_norm": 1.9007986783981323,
      "learning_rate": 0.00017221473530040662,
      "loss": 1.0585,
      "step": 5794
    },
    {
      "epoch": 0.8147054688598341,
      "grad_norm": 1.4783306121826172,
      "learning_rate": 0.00017237534829467256,
      "loss": 1.0407,
      "step": 5795
    },
    {
      "epoch": 0.8148460565162379,
      "grad_norm": 1.6285356283187866,
      "learning_rate": 0.00017253557015780433,
      "loss": 1.1033,
      "step": 5796
    },
    {
      "epoch": 0.8149866441726417,
      "grad_norm": 1.7649298906326294,
      "learning_rate": 0.00017269540002393016,
      "loss": 1.2908,
      "step": 5797
    },
    {
      "epoch": 0.8151272318290455,
      "grad_norm": 1.793298363685608,
      "learning_rate": 0.0001728548370292972,
      "loss": 1.0274,
      "step": 5798
    },
    {
      "epoch": 0.8152678194854491,
      "grad_norm": 1.7948448657989502,
      "learning_rate": 0.00017301388031227524,
      "loss": 1.0774,
      "step": 5799
    },
    {
      "epoch": 0.8154084071418529,
      "grad_norm": 2.266706705093384,
      "learning_rate": 0.0001731725290133617,
      "loss": 1.0117,
      "step": 5800
    },
    {
      "epoch": 0.8155489947982567,
      "grad_norm": 1.4295145273208618,
      "learning_rate": 0.00017333078227518703,
      "loss": 1.073,
      "step": 5801
    },
    {
      "epoch": 0.8156895824546605,
      "grad_norm": 1.2857609987258911,
      "learning_rate": 0.0001734886392425183,
      "loss": 1.029,
      "step": 5802
    },
    {
      "epoch": 0.8158301701110643,
      "grad_norm": 1.5508610010147095,
      "learning_rate": 0.0001736460990622639,
      "loss": 1.0891,
      "step": 5803
    },
    {
      "epoch": 0.815970757767468,
      "grad_norm": 1.4223897457122803,
      "learning_rate": 0.00017380316088347933,
      "loss": 1.1008,
      "step": 5804
    },
    {
      "epoch": 0.8161113454238718,
      "grad_norm": 1.5619823932647705,
      "learning_rate": 0.00017395982385737016,
      "loss": 1.0046,
      "step": 5805
    },
    {
      "epoch": 0.8162519330802755,
      "grad_norm": 1.4877030849456787,
      "learning_rate": 0.00017411608713729815,
      "loss": 1.1834,
      "step": 5806
    },
    {
      "epoch": 0.8163925207366793,
      "grad_norm": 1.4040204286575317,
      "learning_rate": 0.0001742719498787843,
      "loss": 1.1257,
      "step": 5807
    },
    {
      "epoch": 0.8165331083930831,
      "grad_norm": 1.7330501079559326,
      "learning_rate": 0.0001744274112395146,
      "loss": 1.1737,
      "step": 5808
    },
    {
      "epoch": 0.8166736960494868,
      "grad_norm": 1.8112233877182007,
      "learning_rate": 0.00017458247037934442,
      "loss": 1.0542,
      "step": 5809
    },
    {
      "epoch": 0.8168142837058906,
      "grad_norm": 1.6762362718582153,
      "learning_rate": 0.00017473712646030257,
      "loss": 1.0094,
      "step": 5810
    },
    {
      "epoch": 0.8169548713622944,
      "grad_norm": 1.546201467514038,
      "learning_rate": 0.00017489137864659568,
      "loss": 1.0544,
      "step": 5811
    },
    {
      "epoch": 0.8170954590186982,
      "grad_norm": 1.520206093788147,
      "learning_rate": 0.00017504522610461388,
      "loss": 1.0969,
      "step": 5812
    },
    {
      "epoch": 0.817236046675102,
      "grad_norm": 1.6897534132003784,
      "learning_rate": 0.00017519866800293386,
      "loss": 1.1973,
      "step": 5813
    },
    {
      "epoch": 0.8173766343315056,
      "grad_norm": 1.4974918365478516,
      "learning_rate": 0.00017535170351232473,
      "loss": 1.1151,
      "step": 5814
    },
    {
      "epoch": 0.8175172219879094,
      "grad_norm": 1.4645960330963135,
      "learning_rate": 0.00017550433180575103,
      "loss": 1.239,
      "step": 5815
    },
    {
      "epoch": 0.8176578096443132,
      "grad_norm": 1.5788077116012573,
      "learning_rate": 0.00017565655205837837,
      "loss": 1.0783,
      "step": 5816
    },
    {
      "epoch": 0.817798397300717,
      "grad_norm": 1.5918527841567993,
      "learning_rate": 0.0001758083634475777,
      "loss": 1.1872,
      "step": 5817
    },
    {
      "epoch": 0.8179389849571208,
      "grad_norm": 1.7419222593307495,
      "learning_rate": 0.00017595976515292932,
      "loss": 1.0214,
      "step": 5818
    },
    {
      "epoch": 0.8180795726135245,
      "grad_norm": 1.3909293413162231,
      "learning_rate": 0.0001761107563562272,
      "loss": 1.1353,
      "step": 5819
    },
    {
      "epoch": 0.8182201602699283,
      "grad_norm": 1.3777424097061157,
      "learning_rate": 0.00017626133624148448,
      "loss": 1.0974,
      "step": 5820
    },
    {
      "epoch": 0.8183607479263321,
      "grad_norm": 1.5204271078109741,
      "learning_rate": 0.00017641150399493678,
      "loss": 0.9377,
      "step": 5821
    },
    {
      "epoch": 0.8185013355827359,
      "grad_norm": 1.4435449838638306,
      "learning_rate": 0.00017656125880504656,
      "loss": 1.1271,
      "step": 5822
    },
    {
      "epoch": 0.8186419232391396,
      "grad_norm": 1.4259570837020874,
      "learning_rate": 0.00017671059986250866,
      "loss": 1.0431,
      "step": 5823
    },
    {
      "epoch": 0.8187825108955433,
      "grad_norm": 1.399613380432129,
      "learning_rate": 0.00017685952636025337,
      "loss": 1.1579,
      "step": 5824
    },
    {
      "epoch": 0.8189230985519471,
      "grad_norm": 1.6581363677978516,
      "learning_rate": 0.00017700803749345182,
      "loss": 1.1227,
      "step": 5825
    },
    {
      "epoch": 0.8190636862083509,
      "grad_norm": 1.6004389524459839,
      "learning_rate": 0.00017715613245951927,
      "loss": 1.0415,
      "step": 5826
    },
    {
      "epoch": 0.8192042738647547,
      "grad_norm": 1.548508644104004,
      "learning_rate": 0.0001773038104581204,
      "loss": 1.0832,
      "step": 5827
    },
    {
      "epoch": 0.8193448615211585,
      "grad_norm": 1.577166199684143,
      "learning_rate": 0.0001774510706911733,
      "loss": 1.0706,
      "step": 5828
    },
    {
      "epoch": 0.8194854491775622,
      "grad_norm": 1.4846118688583374,
      "learning_rate": 0.00017759791236285376,
      "loss": 1.0193,
      "step": 5829
    },
    {
      "epoch": 0.819626036833966,
      "grad_norm": 1.518710970878601,
      "learning_rate": 0.00017774433467959905,
      "loss": 1.0304,
      "step": 5830
    },
    {
      "epoch": 0.8197666244903697,
      "grad_norm": 1.6334466934204102,
      "learning_rate": 0.00017789033685011355,
      "loss": 1.0469,
      "step": 5831
    },
    {
      "epoch": 0.8199072121467735,
      "grad_norm": 1.352867841720581,
      "learning_rate": 0.00017803591808537145,
      "loss": 1.3038,
      "step": 5832
    },
    {
      "epoch": 0.8200477998031773,
      "grad_norm": 1.4934836626052856,
      "learning_rate": 0.00017818107759862248,
      "loss": 1.1838,
      "step": 5833
    },
    {
      "epoch": 0.820188387459581,
      "grad_norm": 1.9229391813278198,
      "learning_rate": 0.00017832581460539468,
      "loss": 0.9266,
      "step": 5834
    },
    {
      "epoch": 0.8203289751159848,
      "grad_norm": 1.6088701486587524,
      "learning_rate": 0.0001784701283234998,
      "loss": 1.0511,
      "step": 5835
    },
    {
      "epoch": 0.8204695627723886,
      "grad_norm": 1.6429166793823242,
      "learning_rate": 0.00017861401797303724,
      "loss": 0.9433,
      "step": 5836
    },
    {
      "epoch": 0.8206101504287924,
      "grad_norm": 1.5431374311447144,
      "learning_rate": 0.00017875748277639808,
      "loss": 1.1237,
      "step": 5837
    },
    {
      "epoch": 0.8207507380851962,
      "grad_norm": 1.9579063653945923,
      "learning_rate": 0.00017890052195826894,
      "loss": 1.2331,
      "step": 5838
    },
    {
      "epoch": 0.8208913257415998,
      "grad_norm": 1.5910776853561401,
      "learning_rate": 0.00017904313474563721,
      "loss": 1.1749,
      "step": 5839
    },
    {
      "epoch": 0.8210319133980036,
      "grad_norm": 1.8251785039901733,
      "learning_rate": 0.00017918532036779422,
      "loss": 1.2289,
      "step": 5840
    },
    {
      "epoch": 0.8211725010544074,
      "grad_norm": 1.4791548252105713,
      "learning_rate": 0.00017932707805633994,
      "loss": 1.1391,
      "step": 5841
    },
    {
      "epoch": 0.8213130887108112,
      "grad_norm": 1.7422658205032349,
      "learning_rate": 0.00017946840704518687,
      "loss": 1.1383,
      "step": 5842
    },
    {
      "epoch": 0.821453676367215,
      "grad_norm": 1.9155553579330444,
      "learning_rate": 0.00017960930657056427,
      "loss": 1.0473,
      "step": 5843
    },
    {
      "epoch": 0.8215942640236187,
      "grad_norm": 1.4329042434692383,
      "learning_rate": 0.00017974977587102272,
      "loss": 1.0367,
      "step": 5844
    },
    {
      "epoch": 0.8217348516800225,
      "grad_norm": 1.9214993715286255,
      "learning_rate": 0.00017988981418743714,
      "loss": 1.0691,
      "step": 5845
    },
    {
      "epoch": 0.8218754393364263,
      "grad_norm": 1.3206554651260376,
      "learning_rate": 0.0001800294207630119,
      "loss": 1.1629,
      "step": 5846
    },
    {
      "epoch": 0.82201602699283,
      "grad_norm": 1.4965832233428955,
      "learning_rate": 0.00018016859484328487,
      "loss": 1.0505,
      "step": 5847
    },
    {
      "epoch": 0.8221566146492338,
      "grad_norm": 1.573905110359192,
      "learning_rate": 0.00018030733567613085,
      "loss": 1.0702,
      "step": 5848
    },
    {
      "epoch": 0.8222972023056375,
      "grad_norm": 1.425039291381836,
      "learning_rate": 0.00018044564251176607,
      "loss": 1.1299,
      "step": 5849
    },
    {
      "epoch": 0.8224377899620413,
      "grad_norm": 1.5792194604873657,
      "learning_rate": 0.00018058351460275218,
      "loss": 1.0587,
      "step": 5850
    },
    {
      "epoch": 0.8225783776184451,
      "grad_norm": 1.4262478351593018,
      "learning_rate": 0.00018072095120400027,
      "loss": 1.1629,
      "step": 5851
    },
    {
      "epoch": 0.8227189652748489,
      "grad_norm": 1.4793559312820435,
      "learning_rate": 0.00018085795157277517,
      "loss": 1.1445,
      "step": 5852
    },
    {
      "epoch": 0.8228595529312527,
      "grad_norm": 1.4962836503982544,
      "learning_rate": 0.0001809945149686987,
      "loss": 1.2628,
      "step": 5853
    },
    {
      "epoch": 0.8230001405876564,
      "grad_norm": 1.6014152765274048,
      "learning_rate": 0.00018113064065375443,
      "loss": 1.0906,
      "step": 5854
    },
    {
      "epoch": 0.8231407282440601,
      "grad_norm": 2.052616834640503,
      "learning_rate": 0.00018126632789229157,
      "loss": 1.0755,
      "step": 5855
    },
    {
      "epoch": 0.8232813159004639,
      "grad_norm": 1.4809298515319824,
      "learning_rate": 0.00018140157595102866,
      "loss": 1.1324,
      "step": 5856
    },
    {
      "epoch": 0.8234219035568677,
      "grad_norm": 1.9319586753845215,
      "learning_rate": 0.00018153638409905732,
      "loss": 1.2302,
      "step": 5857
    },
    {
      "epoch": 0.8235624912132715,
      "grad_norm": 1.6330680847167969,
      "learning_rate": 0.00018167075160784715,
      "loss": 0.9988,
      "step": 5858
    },
    {
      "epoch": 0.8237030788696752,
      "grad_norm": 1.4707117080688477,
      "learning_rate": 0.0001818046777512487,
      "loss": 0.8977,
      "step": 5859
    },
    {
      "epoch": 0.823843666526079,
      "grad_norm": 1.407244324684143,
      "learning_rate": 0.00018193816180549764,
      "loss": 1.1299,
      "step": 5860
    },
    {
      "epoch": 0.8239842541824828,
      "grad_norm": 1.7882940769195557,
      "learning_rate": 0.00018207120304921905,
      "loss": 1.1249,
      "step": 5861
    },
    {
      "epoch": 0.8241248418388866,
      "grad_norm": 1.556605339050293,
      "learning_rate": 0.0001822038007634308,
      "loss": 1.0504,
      "step": 5862
    },
    {
      "epoch": 0.8242654294952904,
      "grad_norm": 1.4826678037643433,
      "learning_rate": 0.00018233595423154816,
      "loss": 1.0265,
      "step": 5863
    },
    {
      "epoch": 0.824406017151694,
      "grad_norm": 1.488984227180481,
      "learning_rate": 0.00018246766273938647,
      "loss": 1.1522,
      "step": 5864
    },
    {
      "epoch": 0.8245466048080978,
      "grad_norm": 1.414135217666626,
      "learning_rate": 0.00018259892557516617,
      "loss": 1.0611,
      "step": 5865
    },
    {
      "epoch": 0.8246871924645016,
      "grad_norm": 1.7302489280700684,
      "learning_rate": 0.0001827297420295163,
      "loss": 1.0318,
      "step": 5866
    },
    {
      "epoch": 0.8248277801209054,
      "grad_norm": 1.473582148551941,
      "learning_rate": 0.000182860111395478,
      "loss": 1.2567,
      "step": 5867
    },
    {
      "epoch": 0.8249683677773092,
      "grad_norm": 1.6083279848098755,
      "learning_rate": 0.00018299003296850863,
      "loss": 1.1232,
      "step": 5868
    },
    {
      "epoch": 0.8251089554337129,
      "grad_norm": 1.3910558223724365,
      "learning_rate": 0.00018311950604648556,
      "loss": 1.0135,
      "step": 5869
    },
    {
      "epoch": 0.8252495430901167,
      "grad_norm": 1.622670292854309,
      "learning_rate": 0.00018324852992970976,
      "loss": 1.0938,
      "step": 5870
    },
    {
      "epoch": 0.8253901307465205,
      "grad_norm": 1.5981030464172363,
      "learning_rate": 0.00018337710392091018,
      "loss": 1.0627,
      "step": 5871
    },
    {
      "epoch": 0.8255307184029242,
      "grad_norm": 1.694669485092163,
      "learning_rate": 0.00018350522732524645,
      "loss": 1.1464,
      "step": 5872
    },
    {
      "epoch": 0.825671306059328,
      "grad_norm": 1.4717962741851807,
      "learning_rate": 0.00018363289945031358,
      "loss": 1.1185,
      "step": 5873
    },
    {
      "epoch": 0.8258118937157317,
      "grad_norm": 1.7109782695770264,
      "learning_rate": 0.00018376011960614561,
      "loss": 1.2647,
      "step": 5874
    },
    {
      "epoch": 0.8259524813721355,
      "grad_norm": 1.5914998054504395,
      "learning_rate": 0.00018388688710521876,
      "loss": 1.0978,
      "step": 5875
    },
    {
      "epoch": 0.8260930690285393,
      "grad_norm": 1.7676033973693848,
      "learning_rate": 0.00018401320126245564,
      "loss": 1.0879,
      "step": 5876
    },
    {
      "epoch": 0.8262336566849431,
      "grad_norm": 1.522753119468689,
      "learning_rate": 0.00018413906139522874,
      "loss": 1.0275,
      "step": 5877
    },
    {
      "epoch": 0.8263742443413469,
      "grad_norm": 1.5342464447021484,
      "learning_rate": 0.00018426446682336434,
      "loss": 1.1688,
      "step": 5878
    },
    {
      "epoch": 0.8265148319977506,
      "grad_norm": 1.4747552871704102,
      "learning_rate": 0.0001843894168691459,
      "loss": 1.139,
      "step": 5879
    },
    {
      "epoch": 0.8266554196541543,
      "grad_norm": 1.3897693157196045,
      "learning_rate": 0.00018451391085731787,
      "loss": 1.078,
      "step": 5880
    },
    {
      "epoch": 0.8267960073105581,
      "grad_norm": 1.469297170639038,
      "learning_rate": 0.00018463794811508937,
      "loss": 1.1189,
      "step": 5881
    },
    {
      "epoch": 0.8269365949669619,
      "grad_norm": 1.9336961507797241,
      "learning_rate": 0.00018476152797213796,
      "loss": 1.1462,
      "step": 5882
    },
    {
      "epoch": 0.8270771826233657,
      "grad_norm": 1.4648991823196411,
      "learning_rate": 0.00018488464976061257,
      "loss": 1.041,
      "step": 5883
    },
    {
      "epoch": 0.8272177702797694,
      "grad_norm": 1.5434668064117432,
      "learning_rate": 0.0001850073128151382,
      "loss": 1.0703,
      "step": 5884
    },
    {
      "epoch": 0.8273583579361732,
      "grad_norm": 2.090346097946167,
      "learning_rate": 0.00018512951647281872,
      "loss": 0.9833,
      "step": 5885
    },
    {
      "epoch": 0.827498945592577,
      "grad_norm": 1.825767993927002,
      "learning_rate": 0.00018525126007324052,
      "loss": 1.0855,
      "step": 5886
    },
    {
      "epoch": 0.8276395332489808,
      "grad_norm": 1.5066967010498047,
      "learning_rate": 0.00018537254295847644,
      "loss": 1.0051,
      "step": 5887
    },
    {
      "epoch": 0.8277801209053846,
      "grad_norm": 1.7884889841079712,
      "learning_rate": 0.00018549336447308906,
      "loss": 1.0878,
      "step": 5888
    },
    {
      "epoch": 0.8279207085617882,
      "grad_norm": 1.7422735691070557,
      "learning_rate": 0.0001856137239641341,
      "loss": 1.0817,
      "step": 5889
    },
    {
      "epoch": 0.828061296218192,
      "grad_norm": 1.4196370840072632,
      "learning_rate": 0.00018573362078116477,
      "loss": 0.9833,
      "step": 5890
    },
    {
      "epoch": 0.8282018838745958,
      "grad_norm": 1.492509126663208,
      "learning_rate": 0.00018585305427623383,
      "loss": 1.1259,
      "step": 5891
    },
    {
      "epoch": 0.8283424715309996,
      "grad_norm": 1.4626104831695557,
      "learning_rate": 0.00018597202380389837,
      "loss": 1.1801,
      "step": 5892
    },
    {
      "epoch": 0.8284830591874034,
      "grad_norm": 1.3039798736572266,
      "learning_rate": 0.000186090528721223,
      "loss": 1.0526,
      "step": 5893
    },
    {
      "epoch": 0.8286236468438071,
      "grad_norm": 1.6295067071914673,
      "learning_rate": 0.0001862085683877829,
      "loss": 1.0156,
      "step": 5894
    },
    {
      "epoch": 0.8287642345002109,
      "grad_norm": 1.4658045768737793,
      "learning_rate": 0.00018632614216566767,
      "loss": 1.0141,
      "step": 5895
    },
    {
      "epoch": 0.8289048221566147,
      "grad_norm": 1.3608834743499756,
      "learning_rate": 0.00018644324941948458,
      "loss": 1.1922,
      "step": 5896
    },
    {
      "epoch": 0.8290454098130184,
      "grad_norm": 1.6327542066574097,
      "learning_rate": 0.00018655988951636227,
      "loss": 1.1705,
      "step": 5897
    },
    {
      "epoch": 0.8291859974694222,
      "grad_norm": 1.682275414466858,
      "learning_rate": 0.00018667606182595382,
      "loss": 1.0147,
      "step": 5898
    },
    {
      "epoch": 0.8293265851258259,
      "grad_norm": 1.3682678937911987,
      "learning_rate": 0.00018679176572044035,
      "loss": 1.0439,
      "step": 5899
    },
    {
      "epoch": 0.8294671727822297,
      "grad_norm": 1.4350112676620483,
      "learning_rate": 0.00018690700057453442,
      "loss": 1.1576,
      "step": 5900
    },
    {
      "epoch": 0.8296077604386335,
      "grad_norm": 1.8254261016845703,
      "learning_rate": 0.00018702176576548357,
      "loss": 1.1657,
      "step": 5901
    },
    {
      "epoch": 0.8297483480950373,
      "grad_norm": 1.3804636001586914,
      "learning_rate": 0.00018713606067307308,
      "loss": 1.1713,
      "step": 5902
    },
    {
      "epoch": 0.8298889357514411,
      "grad_norm": 1.5870754718780518,
      "learning_rate": 0.00018724988467963013,
      "loss": 1.1485,
      "step": 5903
    },
    {
      "epoch": 0.8300295234078447,
      "grad_norm": 1.4452159404754639,
      "learning_rate": 0.00018736323717002655,
      "loss": 1.2034,
      "step": 5904
    },
    {
      "epoch": 0.8301701110642485,
      "grad_norm": 1.2495286464691162,
      "learning_rate": 0.0001874761175316824,
      "loss": 1.1278,
      "step": 5905
    },
    {
      "epoch": 0.8303106987206523,
      "grad_norm": 1.479017972946167,
      "learning_rate": 0.00018758852515456912,
      "loss": 1.0619,
      "step": 5906
    },
    {
      "epoch": 0.8304512863770561,
      "grad_norm": 1.2515045404434204,
      "learning_rate": 0.00018770045943121308,
      "loss": 1.3192,
      "step": 5907
    },
    {
      "epoch": 0.8305918740334599,
      "grad_norm": 1.4598861932754517,
      "learning_rate": 0.0001878119197566985,
      "loss": 1.0373,
      "step": 5908
    },
    {
      "epoch": 0.8307324616898636,
      "grad_norm": 1.5429744720458984,
      "learning_rate": 0.00018792290552867137,
      "loss": 1.1546,
      "step": 5909
    },
    {
      "epoch": 0.8308730493462674,
      "grad_norm": 1.488864779472351,
      "learning_rate": 0.00018803341614734157,
      "loss": 1.198,
      "step": 5910
    },
    {
      "epoch": 0.8310136370026712,
      "grad_norm": 1.7490814924240112,
      "learning_rate": 0.00018814345101548745,
      "loss": 1.1159,
      "step": 5911
    },
    {
      "epoch": 0.831154224659075,
      "grad_norm": 1.3731272220611572,
      "learning_rate": 0.0001882530095384581,
      "loss": 1.1029,
      "step": 5912
    },
    {
      "epoch": 0.8312948123154787,
      "grad_norm": 1.7752777338027954,
      "learning_rate": 0.00018836209112417689,
      "loss": 1.0501,
      "step": 5913
    },
    {
      "epoch": 0.8314353999718824,
      "grad_norm": 1.815822720527649,
      "learning_rate": 0.0001884706951831447,
      "loss": 1.0411,
      "step": 5914
    },
    {
      "epoch": 0.8315759876282862,
      "grad_norm": 1.8113523721694946,
      "learning_rate": 0.0001885788211284431,
      "loss": 1.0958,
      "step": 5915
    },
    {
      "epoch": 0.83171657528469,
      "grad_norm": 1.3298790454864502,
      "learning_rate": 0.00018868646837573745,
      "loss": 1.2689,
      "step": 5916
    },
    {
      "epoch": 0.8318571629410938,
      "grad_norm": 1.665807843208313,
      "learning_rate": 0.00018879363634328002,
      "loss": 1.1082,
      "step": 5917
    },
    {
      "epoch": 0.8319977505974976,
      "grad_norm": 1.4741203784942627,
      "learning_rate": 0.00018890032445191324,
      "loss": 1.1303,
      "step": 5918
    },
    {
      "epoch": 0.8321383382539013,
      "grad_norm": 1.8953452110290527,
      "learning_rate": 0.00018900653212507295,
      "loss": 1.1444,
      "step": 5919
    },
    {
      "epoch": 0.8322789259103051,
      "grad_norm": 1.4035890102386475,
      "learning_rate": 0.00018911225878879127,
      "loss": 1.1062,
      "step": 5920
    },
    {
      "epoch": 0.8324195135667088,
      "grad_norm": 1.5970633029937744,
      "learning_rate": 0.0001892175038716995,
      "loss": 1.1629,
      "step": 5921
    },
    {
      "epoch": 0.8325601012231126,
      "grad_norm": 1.4602802991867065,
      "learning_rate": 0.00018932226680503202,
      "loss": 1.0447,
      "step": 5922
    },
    {
      "epoch": 0.8327006888795164,
      "grad_norm": 1.3861241340637207,
      "learning_rate": 0.00018942654702262845,
      "loss": 1.11,
      "step": 5923
    },
    {
      "epoch": 0.8328412765359201,
      "grad_norm": 1.307676076889038,
      "learning_rate": 0.00018953034396093732,
      "loss": 1.1178,
      "step": 5924
    },
    {
      "epoch": 0.8329818641923239,
      "grad_norm": 1.42476487159729,
      "learning_rate": 0.00018963365705901877,
      "loss": 1.1591,
      "step": 5925
    },
    {
      "epoch": 0.8331224518487277,
      "grad_norm": 1.5130529403686523,
      "learning_rate": 0.00018973648575854767,
      "loss": 1.0831,
      "step": 5926
    },
    {
      "epoch": 0.8332630395051315,
      "grad_norm": 1.4839224815368652,
      "learning_rate": 0.00018983882950381696,
      "loss": 1.0765,
      "step": 5927
    },
    {
      "epoch": 0.8334036271615353,
      "grad_norm": 1.8331331014633179,
      "learning_rate": 0.00018994068774174017,
      "loss": 1.0919,
      "step": 5928
    },
    {
      "epoch": 0.8335442148179389,
      "grad_norm": 1.7166328430175781,
      "learning_rate": 0.00019004205992185436,
      "loss": 1.0143,
      "step": 5929
    },
    {
      "epoch": 0.8336848024743427,
      "grad_norm": 1.4868155717849731,
      "learning_rate": 0.00019014294549632386,
      "loss": 1.2003,
      "step": 5930
    },
    {
      "epoch": 0.8338253901307465,
      "grad_norm": 1.4680668115615845,
      "learning_rate": 0.00019024334391994243,
      "loss": 0.9848,
      "step": 5931
    },
    {
      "epoch": 0.8339659777871503,
      "grad_norm": 1.7208677530288696,
      "learning_rate": 0.00019034325465013644,
      "loss": 1.1249,
      "step": 5932
    },
    {
      "epoch": 0.8341065654435541,
      "grad_norm": 1.4576504230499268,
      "learning_rate": 0.000190442677146968,
      "loss": 1.2359,
      "step": 5933
    },
    {
      "epoch": 0.8342471530999578,
      "grad_norm": 1.7475160360336304,
      "learning_rate": 0.00019054161087313765,
      "loss": 1.0916,
      "step": 5934
    },
    {
      "epoch": 0.8343877407563616,
      "grad_norm": 1.4630885124206543,
      "learning_rate": 0.00019064005529398728,
      "loss": 1.2059,
      "step": 5935
    },
    {
      "epoch": 0.8345283284127654,
      "grad_norm": 1.412188172340393,
      "learning_rate": 0.00019073800987750344,
      "loss": 1.1068,
      "step": 5936
    },
    {
      "epoch": 0.8346689160691692,
      "grad_norm": 1.4575237035751343,
      "learning_rate": 0.00019083547409431926,
      "loss": 1.1048,
      "step": 5937
    },
    {
      "epoch": 0.8348095037255729,
      "grad_norm": 1.4687055349349976,
      "learning_rate": 0.00019093244741771848,
      "loss": 1.2573,
      "step": 5938
    },
    {
      "epoch": 0.8349500913819766,
      "grad_norm": 1.555798888206482,
      "learning_rate": 0.0001910289293236375,
      "loss": 1.096,
      "step": 5939
    },
    {
      "epoch": 0.8350906790383804,
      "grad_norm": 1.6410865783691406,
      "learning_rate": 0.0001911249192906682,
      "loss": 1.3766,
      "step": 5940
    },
    {
      "epoch": 0.8352312666947842,
      "grad_norm": 1.7532354593276978,
      "learning_rate": 0.00019122041680006145,
      "loss": 0.9955,
      "step": 5941
    },
    {
      "epoch": 0.835371854351188,
      "grad_norm": 1.5114244222640991,
      "learning_rate": 0.0001913154213357292,
      "loss": 1.2178,
      "step": 5942
    },
    {
      "epoch": 0.8355124420075918,
      "grad_norm": 1.2675491571426392,
      "learning_rate": 0.00019140993238424758,
      "loss": 1.2431,
      "step": 5943
    },
    {
      "epoch": 0.8356530296639955,
      "grad_norm": 1.5359234809875488,
      "learning_rate": 0.00019150394943485957,
      "loss": 1.0918,
      "step": 5944
    },
    {
      "epoch": 0.8357936173203993,
      "grad_norm": 1.6032987833023071,
      "learning_rate": 0.00019159747197947783,
      "loss": 1.0715,
      "step": 5945
    },
    {
      "epoch": 0.835934204976803,
      "grad_norm": 1.3906646966934204,
      "learning_rate": 0.00019169049951268762,
      "loss": 1.1211,
      "step": 5946
    },
    {
      "epoch": 0.8360747926332068,
      "grad_norm": 1.6246758699417114,
      "learning_rate": 0.0001917830315317491,
      "loss": 1.0676,
      "step": 5947
    },
    {
      "epoch": 0.8362153802896106,
      "grad_norm": 1.6799509525299072,
      "learning_rate": 0.00019187506753660032,
      "loss": 0.9654,
      "step": 5948
    },
    {
      "epoch": 0.8363559679460143,
      "grad_norm": 1.578428030014038,
      "learning_rate": 0.00019196660702986004,
      "loss": 1.089,
      "step": 5949
    },
    {
      "epoch": 0.8364965556024181,
      "grad_norm": 1.628301739692688,
      "learning_rate": 0.00019205764951683017,
      "loss": 1.126,
      "step": 5950
    },
    {
      "epoch": 0.8366371432588219,
      "grad_norm": 1.625022292137146,
      "learning_rate": 0.0001921481945054985,
      "loss": 1.1229,
      "step": 5951
    },
    {
      "epoch": 0.8367777309152257,
      "grad_norm": 1.4488615989685059,
      "learning_rate": 0.00019223824150654153,
      "loss": 1.1204,
      "step": 5952
    },
    {
      "epoch": 0.8369183185716295,
      "grad_norm": 1.4631729125976562,
      "learning_rate": 0.00019232779003332677,
      "loss": 1.1359,
      "step": 5953
    },
    {
      "epoch": 0.8370589062280331,
      "grad_norm": 1.4217629432678223,
      "learning_rate": 0.00019241683960191593,
      "loss": 1.0841,
      "step": 5954
    },
    {
      "epoch": 0.8371994938844369,
      "grad_norm": 1.4967546463012695,
      "learning_rate": 0.0001925053897310669,
      "loss": 1.0006,
      "step": 5955
    },
    {
      "epoch": 0.8373400815408407,
      "grad_norm": 1.4561681747436523,
      "learning_rate": 0.00019259343994223663,
      "loss": 1.1762,
      "step": 5956
    },
    {
      "epoch": 0.8374806691972445,
      "grad_norm": 1.746760368347168,
      "learning_rate": 0.000192680989759584,
      "loss": 0.97,
      "step": 5957
    },
    {
      "epoch": 0.8376212568536483,
      "grad_norm": 2.1877620220184326,
      "learning_rate": 0.00019276803870997192,
      "loss": 1.0829,
      "step": 5958
    },
    {
      "epoch": 0.837761844510052,
      "grad_norm": 1.5877281427383423,
      "learning_rate": 0.00019285458632296995,
      "loss": 1.0482,
      "step": 5959
    },
    {
      "epoch": 0.8379024321664558,
      "grad_norm": 1.8219314813613892,
      "learning_rate": 0.00019294063213085732,
      "loss": 1.1726,
      "step": 5960
    },
    {
      "epoch": 0.8380430198228596,
      "grad_norm": 1.5504529476165771,
      "learning_rate": 0.00019302617566862486,
      "loss": 1.0722,
      "step": 5961
    },
    {
      "epoch": 0.8381836074792633,
      "grad_norm": 1.4696111679077148,
      "learning_rate": 0.00019311121647397803,
      "loss": 1.0718,
      "step": 5962
    },
    {
      "epoch": 0.8383241951356671,
      "grad_norm": 2.116523265838623,
      "learning_rate": 0.0001931957540873388,
      "loss": 1.2589,
      "step": 5963
    },
    {
      "epoch": 0.8384647827920708,
      "grad_norm": 1.643581509590149,
      "learning_rate": 0.00019327978805184866,
      "loss": 1.0774,
      "step": 5964
    },
    {
      "epoch": 0.8386053704484746,
      "grad_norm": 1.359900712966919,
      "learning_rate": 0.00019336331791337103,
      "loss": 1.2336,
      "step": 5965
    },
    {
      "epoch": 0.8387459581048784,
      "grad_norm": 1.8663924932479858,
      "learning_rate": 0.0001934463432204936,
      "loss": 1.1978,
      "step": 5966
    },
    {
      "epoch": 0.8388865457612822,
      "grad_norm": 1.9023118019104004,
      "learning_rate": 0.00019352886352453039,
      "loss": 1.0289,
      "step": 5967
    },
    {
      "epoch": 0.839027133417686,
      "grad_norm": 1.4507088661193848,
      "learning_rate": 0.00019361087837952513,
      "loss": 1.2631,
      "step": 5968
    },
    {
      "epoch": 0.8391677210740897,
      "grad_norm": 1.7893881797790527,
      "learning_rate": 0.00019369238734225276,
      "loss": 0.9952,
      "step": 5969
    },
    {
      "epoch": 0.8393083087304934,
      "grad_norm": 1.6266541481018066,
      "learning_rate": 0.00019377338997222221,
      "loss": 1.0321,
      "step": 5970
    },
    {
      "epoch": 0.8394488963868972,
      "grad_norm": 1.7825284004211426,
      "learning_rate": 0.00019385388583167875,
      "loss": 1.1235,
      "step": 5971
    },
    {
      "epoch": 0.839589484043301,
      "grad_norm": 1.3919070959091187,
      "learning_rate": 0.0001939338744856063,
      "loss": 1.1165,
      "step": 5972
    },
    {
      "epoch": 0.8397300716997048,
      "grad_norm": 1.5269519090652466,
      "learning_rate": 0.00019401335550172996,
      "loss": 1.1512,
      "step": 5973
    },
    {
      "epoch": 0.8398706593561085,
      "grad_norm": 1.780094861984253,
      "learning_rate": 0.00019409232845051818,
      "loss": 1.2934,
      "step": 5974
    },
    {
      "epoch": 0.8400112470125123,
      "grad_norm": 1.9017629623413086,
      "learning_rate": 0.0001941707929051848,
      "loss": 1.2326,
      "step": 5975
    },
    {
      "epoch": 0.8401518346689161,
      "grad_norm": 1.9500749111175537,
      "learning_rate": 0.00019424874844169217,
      "loss": 1.0793,
      "step": 5976
    },
    {
      "epoch": 0.8402924223253199,
      "grad_norm": 1.6347146034240723,
      "learning_rate": 0.00019432619463875258,
      "loss": 1.0544,
      "step": 5977
    },
    {
      "epoch": 0.8404330099817237,
      "grad_norm": 1.9184274673461914,
      "learning_rate": 0.00019440313107783094,
      "loss": 1.1067,
      "step": 5978
    },
    {
      "epoch": 0.8405735976381273,
      "grad_norm": 1.6473196744918823,
      "learning_rate": 0.00019447955734314712,
      "loss": 1.1278,
      "step": 5979
    },
    {
      "epoch": 0.8407141852945311,
      "grad_norm": 1.5821847915649414,
      "learning_rate": 0.00019455547302167794,
      "loss": 1.0496,
      "step": 5980
    },
    {
      "epoch": 0.8408547729509349,
      "grad_norm": 1.5718687772750854,
      "learning_rate": 0.00019463087770315974,
      "loss": 1.1368,
      "step": 5981
    },
    {
      "epoch": 0.8409953606073387,
      "grad_norm": 1.4894031286239624,
      "learning_rate": 0.00019470577098009017,
      "loss": 1.0662,
      "step": 5982
    },
    {
      "epoch": 0.8411359482637425,
      "grad_norm": 1.4276140928268433,
      "learning_rate": 0.00019478015244773058,
      "loss": 1.2244,
      "step": 5983
    },
    {
      "epoch": 0.8412765359201462,
      "grad_norm": 1.6701074838638306,
      "learning_rate": 0.00019485402170410856,
      "loss": 1.1319,
      "step": 5984
    },
    {
      "epoch": 0.84141712357655,
      "grad_norm": 1.5479501485824585,
      "learning_rate": 0.00019492737835001954,
      "loss": 1.0419,
      "step": 5985
    },
    {
      "epoch": 0.8415577112329538,
      "grad_norm": 1.5314816236495972,
      "learning_rate": 0.00019500022198902904,
      "loss": 1.0545,
      "step": 5986
    },
    {
      "epoch": 0.8416982988893575,
      "grad_norm": 1.6646918058395386,
      "learning_rate": 0.0001950725522274755,
      "loss": 1.1497,
      "step": 5987
    },
    {
      "epoch": 0.8418388865457613,
      "grad_norm": 1.5840744972229004,
      "learning_rate": 0.0001951443686744713,
      "loss": 1.0571,
      "step": 5988
    },
    {
      "epoch": 0.841979474202165,
      "grad_norm": 1.5935938358306885,
      "learning_rate": 0.000195215670941906,
      "loss": 1.1626,
      "step": 5989
    },
    {
      "epoch": 0.8421200618585688,
      "grad_norm": 1.530623197555542,
      "learning_rate": 0.00019528645864444743,
      "loss": 1.2604,
      "step": 5990
    },
    {
      "epoch": 0.8422606495149726,
      "grad_norm": 1.503002405166626,
      "learning_rate": 0.00019535673139954435,
      "loss": 1.1846,
      "step": 5991
    },
    {
      "epoch": 0.8424012371713764,
      "grad_norm": 1.63442063331604,
      "learning_rate": 0.00019542648882742863,
      "loss": 1.0003,
      "step": 5992
    },
    {
      "epoch": 0.8425418248277802,
      "grad_norm": 1.8872716426849365,
      "learning_rate": 0.00019549573055111695,
      "loss": 1.106,
      "step": 5993
    },
    {
      "epoch": 0.8426824124841839,
      "grad_norm": 1.5691394805908203,
      "learning_rate": 0.0001955644561964127,
      "loss": 0.9473,
      "step": 5994
    },
    {
      "epoch": 0.8428230001405876,
      "grad_norm": 1.606658697128296,
      "learning_rate": 0.00019563266539190862,
      "loss": 1.0766,
      "step": 5995
    },
    {
      "epoch": 0.8429635877969914,
      "grad_norm": 1.498555064201355,
      "learning_rate": 0.0001957003577689883,
      "loss": 1.1991,
      "step": 5996
    },
    {
      "epoch": 0.8431041754533952,
      "grad_norm": 1.5831938982009888,
      "learning_rate": 0.00019576753296182835,
      "loss": 1.15,
      "step": 5997
    },
    {
      "epoch": 0.843244763109799,
      "grad_norm": 1.7592809200286865,
      "learning_rate": 0.00019583419060740034,
      "loss": 1.0714,
      "step": 5998
    },
    {
      "epoch": 0.8433853507662027,
      "grad_norm": 2.051140785217285,
      "learning_rate": 0.00019590033034547267,
      "loss": 0.8927,
      "step": 5999
    },
    {
      "epoch": 0.8435259384226065,
      "grad_norm": 1.625166893005371,
      "learning_rate": 0.00019596595181861292,
      "loss": 1.2475,
      "step": 6000
    },
    {
      "epoch": 0.8435259384226065,
      "eval_loss": 1.1662399768829346,
      "eval_runtime": 771.5332,
      "eval_samples_per_second": 16.391,
      "eval_steps_per_second": 8.195,
      "step": 6000
    },
    {
      "epoch": 0.8436665260790103,
      "grad_norm": 1.4842838048934937,
      "learning_rate": 0.0001960310546721891,
      "loss": 1.257,
      "step": 6001
    },
    {
      "epoch": 0.8438071137354141,
      "grad_norm": 1.6678732633590698,
      "learning_rate": 0.00019609563855437208,
      "loss": 1.2974,
      "step": 6002
    },
    {
      "epoch": 0.8439477013918179,
      "grad_norm": 1.6141902208328247,
      "learning_rate": 0.0001961597031161376,
      "loss": 1.2132,
      "step": 6003
    },
    {
      "epoch": 0.8440882890482215,
      "grad_norm": 1.4225987195968628,
      "learning_rate": 0.00019622324801126756,
      "loss": 1.1282,
      "step": 6004
    },
    {
      "epoch": 0.8442288767046253,
      "grad_norm": 1.6598849296569824,
      "learning_rate": 0.00019628627289635243,
      "loss": 1.1275,
      "step": 6005
    },
    {
      "epoch": 0.8443694643610291,
      "grad_norm": 1.4414657354354858,
      "learning_rate": 0.00019634877743079286,
      "loss": 1.2339,
      "step": 6006
    },
    {
      "epoch": 0.8445100520174329,
      "grad_norm": 1.4277397394180298,
      "learning_rate": 0.00019641076127680163,
      "loss": 1.018,
      "step": 6007
    },
    {
      "epoch": 0.8446506396738367,
      "grad_norm": 1.5345675945281982,
      "learning_rate": 0.00019647222409940545,
      "loss": 1.1138,
      "step": 6008
    },
    {
      "epoch": 0.8447912273302404,
      "grad_norm": 1.487858533859253,
      "learning_rate": 0.00019653316556644663,
      "loss": 1.099,
      "step": 6009
    },
    {
      "epoch": 0.8449318149866442,
      "grad_norm": 2.1305909156799316,
      "learning_rate": 0.00019659358534858504,
      "loss": 1.1004,
      "step": 6010
    },
    {
      "epoch": 0.845072402643048,
      "grad_norm": 1.5892137289047241,
      "learning_rate": 0.00019665348311929994,
      "loss": 1.0144,
      "step": 6011
    },
    {
      "epoch": 0.8452129902994517,
      "grad_norm": 1.9201478958129883,
      "learning_rate": 0.00019671285855489158,
      "loss": 1.2703,
      "step": 6012
    },
    {
      "epoch": 0.8453535779558555,
      "grad_norm": 1.4777084589004517,
      "learning_rate": 0.00019677171133448287,
      "loss": 1.1731,
      "step": 6013
    },
    {
      "epoch": 0.8454941656122592,
      "grad_norm": 1.856252908706665,
      "learning_rate": 0.0001968300411400215,
      "loss": 1.0934,
      "step": 6014
    },
    {
      "epoch": 0.845634753268663,
      "grad_norm": 1.4859765768051147,
      "learning_rate": 0.00019688784765628132,
      "loss": 1.1316,
      "step": 6015
    },
    {
      "epoch": 0.8457753409250668,
      "grad_norm": 1.5976370573043823,
      "learning_rate": 0.0001969451305708641,
      "loss": 1.1116,
      "step": 6016
    },
    {
      "epoch": 0.8459159285814706,
      "grad_norm": 1.9214733839035034,
      "learning_rate": 0.00019700188957420128,
      "loss": 1.1111,
      "step": 6017
    },
    {
      "epoch": 0.8460565162378744,
      "grad_norm": 1.5786629915237427,
      "learning_rate": 0.00019705812435955565,
      "loss": 1.0094,
      "step": 6018
    },
    {
      "epoch": 0.846197103894278,
      "grad_norm": 1.405492901802063,
      "learning_rate": 0.00019711383462302302,
      "loss": 1.2884,
      "step": 6019
    },
    {
      "epoch": 0.8463376915506818,
      "grad_norm": 1.6673741340637207,
      "learning_rate": 0.00019716902006353368,
      "loss": 1.1475,
      "step": 6020
    },
    {
      "epoch": 0.8464782792070856,
      "grad_norm": 1.6119686365127563,
      "learning_rate": 0.0001972236803828543,
      "loss": 1.1302,
      "step": 6021
    },
    {
      "epoch": 0.8466188668634894,
      "grad_norm": 1.6133556365966797,
      "learning_rate": 0.00019727781528558938,
      "loss": 1.0906,
      "step": 6022
    },
    {
      "epoch": 0.8467594545198932,
      "grad_norm": 1.5618221759796143,
      "learning_rate": 0.0001973314244791829,
      "loss": 1.1336,
      "step": 6023
    },
    {
      "epoch": 0.8469000421762969,
      "grad_norm": 1.5144115686416626,
      "learning_rate": 0.0001973845076739198,
      "loss": 1.3238,
      "step": 6024
    },
    {
      "epoch": 0.8470406298327007,
      "grad_norm": 1.664770483970642,
      "learning_rate": 0.00019743706458292772,
      "loss": 1.2125,
      "step": 6025
    },
    {
      "epoch": 0.8471812174891045,
      "grad_norm": 1.7499644756317139,
      "learning_rate": 0.00019748909492217832,
      "loss": 1.0376,
      "step": 6026
    },
    {
      "epoch": 0.8473218051455083,
      "grad_norm": 1.5143380165100098,
      "learning_rate": 0.00019754059841048922,
      "loss": 0.9377,
      "step": 6027
    },
    {
      "epoch": 0.847462392801912,
      "grad_norm": 1.4634941816329956,
      "learning_rate": 0.0001975915747695249,
      "loss": 1.1752,
      "step": 6028
    },
    {
      "epoch": 0.8476029804583157,
      "grad_norm": 1.4417890310287476,
      "learning_rate": 0.00019764202372379877,
      "loss": 1.1128,
      "step": 6029
    },
    {
      "epoch": 0.8477435681147195,
      "grad_norm": 1.5765267610549927,
      "learning_rate": 0.00019769194500067442,
      "loss": 1.1419,
      "step": 6030
    },
    {
      "epoch": 0.8478841557711233,
      "grad_norm": 1.7081711292266846,
      "learning_rate": 0.00019774133833036712,
      "loss": 0.9049,
      "step": 6031
    },
    {
      "epoch": 0.8480247434275271,
      "grad_norm": 2.105071783065796,
      "learning_rate": 0.00019779020344594522,
      "loss": 1.0262,
      "step": 6032
    },
    {
      "epoch": 0.8481653310839309,
      "grad_norm": 1.5579105615615845,
      "learning_rate": 0.0001978385400833316,
      "loss": 1.1631,
      "step": 6033
    },
    {
      "epoch": 0.8483059187403346,
      "grad_norm": 1.4506206512451172,
      "learning_rate": 0.00019788634798130537,
      "loss": 1.0906,
      "step": 6034
    },
    {
      "epoch": 0.8484465063967384,
      "grad_norm": 1.814211130142212,
      "learning_rate": 0.00019793362688150283,
      "loss": 1.1152,
      "step": 6035
    },
    {
      "epoch": 0.8485870940531421,
      "grad_norm": 1.6775009632110596,
      "learning_rate": 0.00019798037652841915,
      "loss": 1.0888,
      "step": 6036
    },
    {
      "epoch": 0.8487276817095459,
      "grad_norm": 1.5924047231674194,
      "learning_rate": 0.00019802659666940967,
      "loss": 1.1347,
      "step": 6037
    },
    {
      "epoch": 0.8488682693659497,
      "grad_norm": 1.4933830499649048,
      "learning_rate": 0.00019807228705469147,
      "loss": 1.4288,
      "step": 6038
    },
    {
      "epoch": 0.8490088570223534,
      "grad_norm": 1.468808650970459,
      "learning_rate": 0.00019811744743734422,
      "loss": 1.3121,
      "step": 6039
    },
    {
      "epoch": 0.8491494446787572,
      "grad_norm": 1.7285569906234741,
      "learning_rate": 0.00019816207757331213,
      "loss": 1.2607,
      "step": 6040
    },
    {
      "epoch": 0.849290032335161,
      "grad_norm": 1.5500061511993408,
      "learning_rate": 0.00019820617722140482,
      "loss": 1.29,
      "step": 6041
    },
    {
      "epoch": 0.8494306199915648,
      "grad_norm": 1.5596911907196045,
      "learning_rate": 0.00019824974614329882,
      "loss": 1.2475,
      "step": 6042
    },
    {
      "epoch": 0.8495712076479686,
      "grad_norm": 1.52439284324646,
      "learning_rate": 0.00019829278410353872,
      "loss": 1.0256,
      "step": 6043
    },
    {
      "epoch": 0.8497117953043722,
      "grad_norm": 1.3950785398483276,
      "learning_rate": 0.0001983352908695387,
      "loss": 1.0468,
      "step": 6044
    },
    {
      "epoch": 0.849852382960776,
      "grad_norm": 1.4521901607513428,
      "learning_rate": 0.00019837726621158345,
      "loss": 1.1392,
      "step": 6045
    },
    {
      "epoch": 0.8499929706171798,
      "grad_norm": 1.8365533351898193,
      "learning_rate": 0.00019841870990282975,
      "loss": 1.1933,
      "step": 6046
    },
    {
      "epoch": 0.8501335582735836,
      "grad_norm": 1.5154433250427246,
      "learning_rate": 0.0001984596217193074,
      "loss": 1.18,
      "step": 6047
    },
    {
      "epoch": 0.8502741459299874,
      "grad_norm": 2.003979444503784,
      "learning_rate": 0.00019850000143992056,
      "loss": 1.2098,
      "step": 6048
    },
    {
      "epoch": 0.8504147335863911,
      "grad_norm": 1.3231438398361206,
      "learning_rate": 0.00019853984884644907,
      "loss": 1.1443,
      "step": 6049
    },
    {
      "epoch": 0.8505553212427949,
      "grad_norm": 1.6560726165771484,
      "learning_rate": 0.00019857916372354942,
      "loss": 1.2798,
      "step": 6050
    },
    {
      "epoch": 0.8506959088991987,
      "grad_norm": 1.386256217956543,
      "learning_rate": 0.00019861794585875593,
      "loss": 1.2171,
      "step": 6051
    },
    {
      "epoch": 0.8508364965556025,
      "grad_norm": 1.5850603580474854,
      "learning_rate": 0.0001986561950424821,
      "loss": 1.0832,
      "step": 6052
    },
    {
      "epoch": 0.8509770842120062,
      "grad_norm": 1.8223907947540283,
      "learning_rate": 0.00019869391106802154,
      "loss": 1.1332,
      "step": 6053
    },
    {
      "epoch": 0.8511176718684099,
      "grad_norm": 1.850203275680542,
      "learning_rate": 0.0001987310937315491,
      "loss": 1.0981,
      "step": 6054
    },
    {
      "epoch": 0.8512582595248137,
      "grad_norm": 1.5282913446426392,
      "learning_rate": 0.00019876774283212215,
      "loss": 1.2067,
      "step": 6055
    },
    {
      "epoch": 0.8513988471812175,
      "grad_norm": 1.5437836647033691,
      "learning_rate": 0.00019880385817168145,
      "loss": 1.2575,
      "step": 6056
    },
    {
      "epoch": 0.8515394348376213,
      "grad_norm": 1.6665188074111938,
      "learning_rate": 0.0001988394395550524,
      "loss": 1.0578,
      "step": 6057
    },
    {
      "epoch": 0.8516800224940251,
      "grad_norm": 1.5002418756484985,
      "learning_rate": 0.00019887448678994587,
      "loss": 1.0129,
      "step": 6058
    },
    {
      "epoch": 0.8518206101504288,
      "grad_norm": 1.425171136856079,
      "learning_rate": 0.00019890899968695952,
      "loss": 0.9764,
      "step": 6059
    },
    {
      "epoch": 0.8519611978068325,
      "grad_norm": 1.493937373161316,
      "learning_rate": 0.00019894297805957858,
      "loss": 1.2348,
      "step": 6060
    },
    {
      "epoch": 0.8521017854632363,
      "grad_norm": 1.3702573776245117,
      "learning_rate": 0.000198976421724177,
      "loss": 1.065,
      "step": 6061
    },
    {
      "epoch": 0.8522423731196401,
      "grad_norm": 1.671837329864502,
      "learning_rate": 0.00019900933050001846,
      "loss": 1.0886,
      "step": 6062
    },
    {
      "epoch": 0.8523829607760439,
      "grad_norm": 1.6466151475906372,
      "learning_rate": 0.00019904170420925716,
      "loss": 1.1992,
      "step": 6063
    },
    {
      "epoch": 0.8525235484324476,
      "grad_norm": 1.5970338582992554,
      "learning_rate": 0.00019907354267693897,
      "loss": 1.2508,
      "step": 6064
    },
    {
      "epoch": 0.8526641360888514,
      "grad_norm": 1.5661513805389404,
      "learning_rate": 0.00019910484573100242,
      "loss": 1.0371,
      "step": 6065
    },
    {
      "epoch": 0.8528047237452552,
      "grad_norm": 1.5002433061599731,
      "learning_rate": 0.0001991356132022793,
      "loss": 1.1311,
      "step": 6066
    },
    {
      "epoch": 0.852945311401659,
      "grad_norm": 1.5863333940505981,
      "learning_rate": 0.000199165844924496,
      "loss": 1.1388,
      "step": 6067
    },
    {
      "epoch": 0.8530858990580626,
      "grad_norm": 1.8095427751541138,
      "learning_rate": 0.00019919554073427408,
      "loss": 1.2299,
      "step": 6068
    },
    {
      "epoch": 0.8532264867144664,
      "grad_norm": 1.475136160850525,
      "learning_rate": 0.0001992247004711314,
      "loss": 1.0152,
      "step": 6069
    },
    {
      "epoch": 0.8533670743708702,
      "grad_norm": 1.5142878293991089,
      "learning_rate": 0.00019925332397748277,
      "loss": 1.0129,
      "step": 6070
    },
    {
      "epoch": 0.853507662027274,
      "grad_norm": 1.3030288219451904,
      "learning_rate": 0.00019928141109864088,
      "loss": 1.2281,
      "step": 6071
    },
    {
      "epoch": 0.8536482496836778,
      "grad_norm": 1.9224004745483398,
      "learning_rate": 0.00019930896168281727,
      "loss": 1.2788,
      "step": 6072
    },
    {
      "epoch": 0.8537888373400815,
      "grad_norm": 1.5901966094970703,
      "learning_rate": 0.00019933597558112294,
      "loss": 1.1711,
      "step": 6073
    },
    {
      "epoch": 0.8539294249964853,
      "grad_norm": 1.7138621807098389,
      "learning_rate": 0.00019936245264756924,
      "loss": 0.997,
      "step": 6074
    },
    {
      "epoch": 0.8540700126528891,
      "grad_norm": 1.5857902765274048,
      "learning_rate": 0.00019938839273906877,
      "loss": 1.0957,
      "step": 6075
    },
    {
      "epoch": 0.8542106003092929,
      "grad_norm": 1.6597809791564941,
      "learning_rate": 0.00019941379571543596,
      "loss": 1.1066,
      "step": 6076
    },
    {
      "epoch": 0.8543511879656966,
      "grad_norm": 1.6365077495574951,
      "learning_rate": 0.00019943866143938793,
      "loss": 1.2367,
      "step": 6077
    },
    {
      "epoch": 0.8544917756221003,
      "grad_norm": 1.5892115831375122,
      "learning_rate": 0.0001994629897765453,
      "loss": 1.2084,
      "step": 6078
    },
    {
      "epoch": 0.8546323632785041,
      "grad_norm": 1.6594325304031372,
      "learning_rate": 0.00019948678059543271,
      "loss": 1.0576,
      "step": 6079
    },
    {
      "epoch": 0.8547729509349079,
      "grad_norm": 1.4591797590255737,
      "learning_rate": 0.00019951003376747974,
      "loss": 1.1482,
      "step": 6080
    },
    {
      "epoch": 0.8549135385913117,
      "grad_norm": 1.597563624382019,
      "learning_rate": 0.00019953274916702154,
      "loss": 0.9287,
      "step": 6081
    },
    {
      "epoch": 0.8550541262477155,
      "grad_norm": 1.4245452880859375,
      "learning_rate": 0.0001995549266712994,
      "loss": 1.0531,
      "step": 6082
    },
    {
      "epoch": 0.8551947139041192,
      "grad_norm": 1.6714739799499512,
      "learning_rate": 0.00019957656616046164,
      "loss": 1.1701,
      "step": 6083
    },
    {
      "epoch": 0.855335301560523,
      "grad_norm": 1.4690399169921875,
      "learning_rate": 0.00019959766751756402,
      "loss": 0.991,
      "step": 6084
    },
    {
      "epoch": 0.8554758892169267,
      "grad_norm": 1.9270836114883423,
      "learning_rate": 0.00019961823062857048,
      "loss": 1.3126,
      "step": 6085
    },
    {
      "epoch": 0.8556164768733305,
      "grad_norm": 1.595717191696167,
      "learning_rate": 0.0001996382553823538,
      "loss": 1.0591,
      "step": 6086
    },
    {
      "epoch": 0.8557570645297343,
      "grad_norm": 1.4572665691375732,
      "learning_rate": 0.00019965774167069613,
      "loss": 1.1713,
      "step": 6087
    },
    {
      "epoch": 0.855897652186138,
      "grad_norm": 1.725117564201355,
      "learning_rate": 0.00019967668938828962,
      "loss": 1.2278,
      "step": 6088
    },
    {
      "epoch": 0.8560382398425418,
      "grad_norm": 1.3636226654052734,
      "learning_rate": 0.0001996950984327369,
      "loss": 1.044,
      "step": 6089
    },
    {
      "epoch": 0.8561788274989456,
      "grad_norm": 1.5100599527359009,
      "learning_rate": 0.00019971296870455177,
      "loss": 1.2371,
      "step": 6090
    },
    {
      "epoch": 0.8563194151553494,
      "grad_norm": 1.54267418384552,
      "learning_rate": 0.0001997303001071596,
      "loss": 0.8834,
      "step": 6091
    },
    {
      "epoch": 0.8564600028117532,
      "grad_norm": 1.5044151544570923,
      "learning_rate": 0.00019974709254689797,
      "loss": 1.1336,
      "step": 6092
    },
    {
      "epoch": 0.8566005904681568,
      "grad_norm": 1.790626049041748,
      "learning_rate": 0.00019976334593301714,
      "loss": 1.164,
      "step": 6093
    },
    {
      "epoch": 0.8567411781245606,
      "grad_norm": 1.888482928276062,
      "learning_rate": 0.00019977906017768052,
      "loss": 1.0559,
      "step": 6094
    },
    {
      "epoch": 0.8568817657809644,
      "grad_norm": 1.4013934135437012,
      "learning_rate": 0.00019979423519596504,
      "loss": 1.1993,
      "step": 6095
    },
    {
      "epoch": 0.8570223534373682,
      "grad_norm": 1.5337351560592651,
      "learning_rate": 0.00019980887090586188,
      "loss": 1.1365,
      "step": 6096
    },
    {
      "epoch": 0.857162941093772,
      "grad_norm": 1.791982889175415,
      "learning_rate": 0.00019982296722827666,
      "loss": 1.0008,
      "step": 6097
    },
    {
      "epoch": 0.8573035287501757,
      "grad_norm": 1.5414128303527832,
      "learning_rate": 0.00019983652408703002,
      "loss": 1.2672,
      "step": 6098
    },
    {
      "epoch": 0.8574441164065795,
      "grad_norm": 1.4475752115249634,
      "learning_rate": 0.00019984954140885787,
      "loss": 1.162,
      "step": 6099
    },
    {
      "epoch": 0.8575847040629833,
      "grad_norm": 1.5225797891616821,
      "learning_rate": 0.00019986201912341195,
      "loss": 1.094,
      "step": 6100
    },
    {
      "epoch": 0.857725291719387,
      "grad_norm": 1.4602535963058472,
      "learning_rate": 0.0001998739571632602,
      "loss": 1.2062,
      "step": 6101
    },
    {
      "epoch": 0.8578658793757908,
      "grad_norm": 1.4407458305358887,
      "learning_rate": 0.000199885355463887,
      "loss": 1.1281,
      "step": 6102
    },
    {
      "epoch": 0.8580064670321945,
      "grad_norm": 1.4825701713562012,
      "learning_rate": 0.0001998962139636936,
      "loss": 1.052,
      "step": 6103
    },
    {
      "epoch": 0.8581470546885983,
      "grad_norm": 1.655100703239441,
      "learning_rate": 0.00019990653260399844,
      "loss": 1.1039,
      "step": 6104
    },
    {
      "epoch": 0.8582876423450021,
      "grad_norm": 1.3157027959823608,
      "learning_rate": 0.00019991631132903746,
      "loss": 1.0967,
      "step": 6105
    },
    {
      "epoch": 0.8584282300014059,
      "grad_norm": 1.5023869276046753,
      "learning_rate": 0.00019992555008596454,
      "loss": 0.9876,
      "step": 6106
    },
    {
      "epoch": 0.8585688176578097,
      "grad_norm": 1.6842833757400513,
      "learning_rate": 0.00019993424882485146,
      "loss": 1.2737,
      "step": 6107
    },
    {
      "epoch": 0.8587094053142134,
      "grad_norm": 1.5314263105392456,
      "learning_rate": 0.0001999424074986885,
      "loss": 1.165,
      "step": 6108
    },
    {
      "epoch": 0.8588499929706171,
      "grad_norm": 1.4606941938400269,
      "learning_rate": 0.0001999500260633845,
      "loss": 1.1645,
      "step": 6109
    },
    {
      "epoch": 0.8589905806270209,
      "grad_norm": 2.2398877143859863,
      "learning_rate": 0.00019995710447776724,
      "loss": 0.9753,
      "step": 6110
    },
    {
      "epoch": 0.8591311682834247,
      "grad_norm": 1.6762396097183228,
      "learning_rate": 0.00019996364270358346,
      "loss": 1.2541,
      "step": 6111
    },
    {
      "epoch": 0.8592717559398285,
      "grad_norm": 1.6597589254379272,
      "learning_rate": 0.00019996964070549927,
      "loss": 1.1324,
      "step": 6112
    },
    {
      "epoch": 0.8594123435962322,
      "grad_norm": 1.492126703262329,
      "learning_rate": 0.00019997509845110027,
      "loss": 1.2672,
      "step": 6113
    },
    {
      "epoch": 0.859552931252636,
      "grad_norm": 1.693613886833191,
      "learning_rate": 0.00019998001591089168,
      "loss": 1.1572,
      "step": 6114
    },
    {
      "epoch": 0.8596935189090398,
      "grad_norm": 1.6239246129989624,
      "learning_rate": 0.00019998439305829856,
      "loss": 1.107,
      "step": 6115
    },
    {
      "epoch": 0.8598341065654436,
      "grad_norm": 1.3944828510284424,
      "learning_rate": 0.00019998822986966585,
      "loss": 1.1223,
      "step": 6116
    },
    {
      "epoch": 0.8599746942218474,
      "grad_norm": 1.3561604022979736,
      "learning_rate": 0.0001999915263242587,
      "loss": 1.0306,
      "step": 6117
    },
    {
      "epoch": 0.860115281878251,
      "grad_norm": 1.5423094034194946,
      "learning_rate": 0.00019999428240426238,
      "loss": 1.032,
      "step": 6118
    },
    {
      "epoch": 0.8602558695346548,
      "grad_norm": 1.4678648710250854,
      "learning_rate": 0.00019999649809478252,
      "loss": 1.0563,
      "step": 6119
    },
    {
      "epoch": 0.8603964571910586,
      "grad_norm": 1.5015610456466675,
      "learning_rate": 0.00019999817338384497,
      "loss": 1.079,
      "step": 6120
    },
    {
      "epoch": 0.8605370448474624,
      "grad_norm": 1.4384807348251343,
      "learning_rate": 0.0001999993082623962,
      "loss": 1.0564,
      "step": 6121
    },
    {
      "epoch": 0.8606776325038662,
      "grad_norm": 1.2739685773849487,
      "learning_rate": 0.0001999999027243031,
      "loss": 1.2697,
      "step": 6122
    },
    {
      "epoch": 0.8608182201602699,
      "grad_norm": 1.4408586025238037,
      "learning_rate": 0.00019999995676635304,
      "loss": 1.1017,
      "step": 6123
    },
    {
      "epoch": 0.8609588078166737,
      "grad_norm": 1.4765459299087524,
      "learning_rate": 0.000199999470388254,
      "loss": 1.1303,
      "step": 6124
    },
    {
      "epoch": 0.8610993954730775,
      "grad_norm": 1.5434603691101074,
      "learning_rate": 0.00019999844359263445,
      "loss": 1.2655,
      "step": 6125
    },
    {
      "epoch": 0.8612399831294812,
      "grad_norm": 1.4110441207885742,
      "learning_rate": 0.00019999687638504337,
      "loss": 1.0602,
      "step": 6126
    },
    {
      "epoch": 0.861380570785885,
      "grad_norm": 1.7094694375991821,
      "learning_rate": 0.00019999476877395034,
      "loss": 1.056,
      "step": 6127
    },
    {
      "epoch": 0.8615211584422887,
      "grad_norm": 1.4843864440917969,
      "learning_rate": 0.00019999212077074528,
      "loss": 1.1994,
      "step": 6128
    },
    {
      "epoch": 0.8616617460986925,
      "grad_norm": 1.5939384698867798,
      "learning_rate": 0.0001999889323897385,
      "loss": 1.1358,
      "step": 6129
    },
    {
      "epoch": 0.8618023337550963,
      "grad_norm": 1.7701234817504883,
      "learning_rate": 0.00019998520364816074,
      "loss": 1.0937,
      "step": 6130
    },
    {
      "epoch": 0.8619429214115001,
      "grad_norm": 1.7024245262145996,
      "learning_rate": 0.0001999809345661628,
      "loss": 1.1553,
      "step": 6131
    },
    {
      "epoch": 0.8620835090679039,
      "grad_norm": 1.4059864282608032,
      "learning_rate": 0.00019997612516681578,
      "loss": 1.3175,
      "step": 6132
    },
    {
      "epoch": 0.8622240967243076,
      "grad_norm": 1.701541543006897,
      "learning_rate": 0.00019997077547611057,
      "loss": 1.0466,
      "step": 6133
    },
    {
      "epoch": 0.8623646843807113,
      "grad_norm": 1.5355850458145142,
      "learning_rate": 0.00019996488552295798,
      "loss": 1.282,
      "step": 6134
    },
    {
      "epoch": 0.8625052720371151,
      "grad_norm": 1.768146276473999,
      "learning_rate": 0.00019995845533918853,
      "loss": 1.2927,
      "step": 6135
    },
    {
      "epoch": 0.8626458596935189,
      "grad_norm": 1.694832444190979,
      "learning_rate": 0.00019995148495955228,
      "loss": 1.1202,
      "step": 6136
    },
    {
      "epoch": 0.8627864473499227,
      "grad_norm": 1.2775850296020508,
      "learning_rate": 0.00019994397442171856,
      "loss": 1.3053,
      "step": 6137
    },
    {
      "epoch": 0.8629270350063264,
      "grad_norm": 1.7648521661758423,
      "learning_rate": 0.0001999359237662758,
      "loss": 1.2739,
      "step": 6138
    },
    {
      "epoch": 0.8630676226627302,
      "grad_norm": 1.7829097509384155,
      "learning_rate": 0.0001999273330367315,
      "loss": 1.2235,
      "step": 6139
    },
    {
      "epoch": 0.863208210319134,
      "grad_norm": 1.4383522272109985,
      "learning_rate": 0.0001999182022795116,
      "loss": 1.0993,
      "step": 6140
    },
    {
      "epoch": 0.8633487979755378,
      "grad_norm": 1.6176992654800415,
      "learning_rate": 0.0001999085315439606,
      "loss": 1.0698,
      "step": 6141
    },
    {
      "epoch": 0.8634893856319416,
      "grad_norm": 1.6800168752670288,
      "learning_rate": 0.00019989832088234115,
      "loss": 1.0964,
      "step": 6142
    },
    {
      "epoch": 0.8636299732883452,
      "grad_norm": 1.5263217687606812,
      "learning_rate": 0.00019988757034983373,
      "loss": 1.1528,
      "step": 6143
    },
    {
      "epoch": 0.863770560944749,
      "grad_norm": 1.653136134147644,
      "learning_rate": 0.00019987628000453647,
      "loss": 1.061,
      "step": 6144
    },
    {
      "epoch": 0.8639111486011528,
      "grad_norm": 1.7288063764572144,
      "learning_rate": 0.0001998644499074646,
      "loss": 1.0087,
      "step": 6145
    },
    {
      "epoch": 0.8640517362575566,
      "grad_norm": 1.634347915649414,
      "learning_rate": 0.00019985208012255042,
      "loss": 1.1191,
      "step": 6146
    },
    {
      "epoch": 0.8641923239139604,
      "grad_norm": 1.490736484527588,
      "learning_rate": 0.00019983917071664275,
      "loss": 1.2242,
      "step": 6147
    },
    {
      "epoch": 0.8643329115703641,
      "grad_norm": 1.6289029121398926,
      "learning_rate": 0.0001998257217595067,
      "loss": 1.2388,
      "step": 6148
    },
    {
      "epoch": 0.8644734992267679,
      "grad_norm": 1.7319447994232178,
      "learning_rate": 0.00019981173332382315,
      "loss": 1.0814,
      "step": 6149
    },
    {
      "epoch": 0.8646140868831717,
      "grad_norm": 1.7341259717941284,
      "learning_rate": 0.00019979720548518842,
      "loss": 1.0684,
      "step": 6150
    },
    {
      "epoch": 0.8647546745395754,
      "grad_norm": 1.5209671258926392,
      "learning_rate": 0.00019978213832211398,
      "loss": 1.1127,
      "step": 6151
    },
    {
      "epoch": 0.8648952621959792,
      "grad_norm": 1.5926756858825684,
      "learning_rate": 0.00019976653191602578,
      "loss": 1.2912,
      "step": 6152
    },
    {
      "epoch": 0.8650358498523829,
      "grad_norm": 1.4908852577209473,
      "learning_rate": 0.00019975038635126405,
      "loss": 1.0749,
      "step": 6153
    },
    {
      "epoch": 0.8651764375087867,
      "grad_norm": 1.704007625579834,
      "learning_rate": 0.00019973370171508275,
      "loss": 1.1793,
      "step": 6154
    },
    {
      "epoch": 0.8653170251651905,
      "grad_norm": 1.6004278659820557,
      "learning_rate": 0.00019971647809764905,
      "loss": 1.0859,
      "step": 6155
    },
    {
      "epoch": 0.8654576128215943,
      "grad_norm": 1.6006426811218262,
      "learning_rate": 0.00019969871559204276,
      "loss": 1.1265,
      "step": 6156
    },
    {
      "epoch": 0.8655982004779981,
      "grad_norm": 1.5208874940872192,
      "learning_rate": 0.00019968041429425622,
      "loss": 1.0418,
      "step": 6157
    },
    {
      "epoch": 0.8657387881344017,
      "grad_norm": 1.5680872201919556,
      "learning_rate": 0.0001996615743031934,
      "loss": 1.089,
      "step": 6158
    },
    {
      "epoch": 0.8658793757908055,
      "grad_norm": 1.6558834314346313,
      "learning_rate": 0.00019964219572066934,
      "loss": 1.2096,
      "step": 6159
    },
    {
      "epoch": 0.8660199634472093,
      "grad_norm": 1.5796432495117188,
      "learning_rate": 0.00019962227865140988,
      "loss": 1.1234,
      "step": 6160
    },
    {
      "epoch": 0.8661605511036131,
      "grad_norm": 1.5294408798217773,
      "learning_rate": 0.00019960182320305093,
      "loss": 1.1575,
      "step": 6161
    },
    {
      "epoch": 0.8663011387600169,
      "grad_norm": 1.5493537187576294,
      "learning_rate": 0.00019958082948613794,
      "loss": 0.976,
      "step": 6162
    },
    {
      "epoch": 0.8664417264164206,
      "grad_norm": 1.5243096351623535,
      "learning_rate": 0.00019955929761412522,
      "loss": 1.1565,
      "step": 6163
    },
    {
      "epoch": 0.8665823140728244,
      "grad_norm": 1.3936110734939575,
      "learning_rate": 0.00019953722770337534,
      "loss": 1.1434,
      "step": 6164
    },
    {
      "epoch": 0.8667229017292282,
      "grad_norm": 1.9467121362686157,
      "learning_rate": 0.00019951461987315864,
      "loss": 1.1091,
      "step": 6165
    },
    {
      "epoch": 0.866863489385632,
      "grad_norm": 1.4705027341842651,
      "learning_rate": 0.00019949147424565248,
      "loss": 1.2285,
      "step": 6166
    },
    {
      "epoch": 0.8670040770420357,
      "grad_norm": 1.3761117458343506,
      "learning_rate": 0.00019946779094594046,
      "loss": 1.1029,
      "step": 6167
    },
    {
      "epoch": 0.8671446646984394,
      "grad_norm": 1.8962184190750122,
      "learning_rate": 0.00019944357010201203,
      "loss": 1.1313,
      "step": 6168
    },
    {
      "epoch": 0.8672852523548432,
      "grad_norm": 1.8026325702667236,
      "learning_rate": 0.00019941881184476154,
      "loss": 1.216,
      "step": 6169
    },
    {
      "epoch": 0.867425840011247,
      "grad_norm": 1.4864004850387573,
      "learning_rate": 0.00019939351630798768,
      "loss": 1.1557,
      "step": 6170
    },
    {
      "epoch": 0.8675664276676508,
      "grad_norm": 1.52595853805542,
      "learning_rate": 0.00019936768362839265,
      "loss": 1.0061,
      "step": 6171
    },
    {
      "epoch": 0.8677070153240546,
      "grad_norm": 1.7342872619628906,
      "learning_rate": 0.00019934131394558154,
      "loss": 1.1207,
      "step": 6172
    },
    {
      "epoch": 0.8678476029804583,
      "grad_norm": 1.5026956796646118,
      "learning_rate": 0.00019931440740206146,
      "loss": 1.3073,
      "step": 6173
    },
    {
      "epoch": 0.8679881906368621,
      "grad_norm": 1.7497482299804688,
      "learning_rate": 0.00019928696414324093,
      "loss": 1.2266,
      "step": 6174
    },
    {
      "epoch": 0.8681287782932658,
      "grad_norm": 1.3876404762268066,
      "learning_rate": 0.00019925898431742884,
      "loss": 1.0226,
      "step": 6175
    },
    {
      "epoch": 0.8682693659496696,
      "grad_norm": 1.687936782836914,
      "learning_rate": 0.0001992304680758339,
      "loss": 1.2169,
      "step": 6176
    },
    {
      "epoch": 0.8684099536060734,
      "grad_norm": 1.5885920524597168,
      "learning_rate": 0.0001992014155725637,
      "loss": 1.0014,
      "step": 6177
    },
    {
      "epoch": 0.8685505412624771,
      "grad_norm": 1.4930663108825684,
      "learning_rate": 0.00019917182696462386,
      "loss": 1.1776,
      "step": 6178
    },
    {
      "epoch": 0.8686911289188809,
      "grad_norm": 1.3335028886795044,
      "learning_rate": 0.00019914170241191727,
      "loss": 1.2458,
      "step": 6179
    },
    {
      "epoch": 0.8688317165752847,
      "grad_norm": 1.4297091960906982,
      "learning_rate": 0.00019911104207724316,
      "loss": 1.1675,
      "step": 6180
    },
    {
      "epoch": 0.8689723042316885,
      "grad_norm": 1.6465222835540771,
      "learning_rate": 0.0001990798461262962,
      "loss": 1.0303,
      "step": 6181
    },
    {
      "epoch": 0.8691128918880923,
      "grad_norm": 1.4053986072540283,
      "learning_rate": 0.00019904811472766574,
      "loss": 1.0782,
      "step": 6182
    },
    {
      "epoch": 0.8692534795444959,
      "grad_norm": 1.486846923828125,
      "learning_rate": 0.00019901584805283459,
      "loss": 1.0639,
      "step": 6183
    },
    {
      "epoch": 0.8693940672008997,
      "grad_norm": 1.597578525543213,
      "learning_rate": 0.00019898304627617857,
      "loss": 1.1583,
      "step": 6184
    },
    {
      "epoch": 0.8695346548573035,
      "grad_norm": 1.456912636756897,
      "learning_rate": 0.00019894970957496513,
      "loss": 1.0025,
      "step": 6185
    },
    {
      "epoch": 0.8696752425137073,
      "grad_norm": 1.375084400177002,
      "learning_rate": 0.00019891583812935254,
      "loss": 1.2217,
      "step": 6186
    },
    {
      "epoch": 0.8698158301701111,
      "grad_norm": 1.7421189546585083,
      "learning_rate": 0.00019888143212238907,
      "loss": 1.2681,
      "step": 6187
    },
    {
      "epoch": 0.8699564178265148,
      "grad_norm": 1.7346243858337402,
      "learning_rate": 0.00019884649174001175,
      "loss": 1.1399,
      "step": 6188
    },
    {
      "epoch": 0.8700970054829186,
      "grad_norm": 1.4927830696105957,
      "learning_rate": 0.00019881101717104556,
      "loss": 0.9755,
      "step": 6189
    },
    {
      "epoch": 0.8702375931393224,
      "grad_norm": 1.7113436460494995,
      "learning_rate": 0.00019877500860720227,
      "loss": 1.2004,
      "step": 6190
    },
    {
      "epoch": 0.8703781807957262,
      "grad_norm": 1.8939406871795654,
      "learning_rate": 0.0001987384662430795,
      "loss": 1.0836,
      "step": 6191
    },
    {
      "epoch": 0.8705187684521299,
      "grad_norm": 1.5908331871032715,
      "learning_rate": 0.00019870139027615966,
      "loss": 1.0833,
      "step": 6192
    },
    {
      "epoch": 0.8706593561085336,
      "grad_norm": 1.4946908950805664,
      "learning_rate": 0.00019866378090680885,
      "loss": 1.245,
      "step": 6193
    },
    {
      "epoch": 0.8707999437649374,
      "grad_norm": 1.758683443069458,
      "learning_rate": 0.00019862563833827567,
      "loss": 1.1093,
      "step": 6194
    },
    {
      "epoch": 0.8709405314213412,
      "grad_norm": 1.7881842851638794,
      "learning_rate": 0.00019858696277669046,
      "loss": 1.1133,
      "step": 6195
    },
    {
      "epoch": 0.871081119077745,
      "grad_norm": 1.4499465227127075,
      "learning_rate": 0.00019854775443106377,
      "loss": 1.1755,
      "step": 6196
    },
    {
      "epoch": 0.8712217067341488,
      "grad_norm": 1.532599687576294,
      "learning_rate": 0.00019850801351328548,
      "loss": 1.0932,
      "step": 6197
    },
    {
      "epoch": 0.8713622943905525,
      "grad_norm": 1.4890328645706177,
      "learning_rate": 0.00019846774023812364,
      "loss": 1.1525,
      "step": 6198
    },
    {
      "epoch": 0.8715028820469563,
      "grad_norm": 1.4294342994689941,
      "learning_rate": 0.00019842693482322323,
      "loss": 1.2282,
      "step": 6199
    },
    {
      "epoch": 0.87164346970336,
      "grad_norm": 1.4407870769500732,
      "learning_rate": 0.00019838559748910505,
      "loss": 1.178,
      "step": 6200
    },
    {
      "epoch": 0.8717840573597638,
      "grad_norm": 1.6586512327194214,
      "learning_rate": 0.00019834372845916446,
      "loss": 1.1086,
      "step": 6201
    },
    {
      "epoch": 0.8719246450161676,
      "grad_norm": 1.6664986610412598,
      "learning_rate": 0.00019830132795967015,
      "loss": 1.1761,
      "step": 6202
    },
    {
      "epoch": 0.8720652326725713,
      "grad_norm": 1.306622862815857,
      "learning_rate": 0.00019825839621976317,
      "loss": 1.1971,
      "step": 6203
    },
    {
      "epoch": 0.8722058203289751,
      "grad_norm": 1.666748285293579,
      "learning_rate": 0.00019821493347145543,
      "loss": 1.1046,
      "step": 6204
    },
    {
      "epoch": 0.8723464079853789,
      "grad_norm": 1.7657784223556519,
      "learning_rate": 0.00019817093994962837,
      "loss": 1.167,
      "step": 6205
    },
    {
      "epoch": 0.8724869956417827,
      "grad_norm": 1.7931715250015259,
      "learning_rate": 0.00019812641589203202,
      "loss": 0.9289,
      "step": 6206
    },
    {
      "epoch": 0.8726275832981865,
      "grad_norm": 1.6488878726959229,
      "learning_rate": 0.0001980813615392834,
      "loss": 1.1752,
      "step": 6207
    },
    {
      "epoch": 0.8727681709545901,
      "grad_norm": 1.5380927324295044,
      "learning_rate": 0.00019803577713486546,
      "loss": 1.1744,
      "step": 6208
    },
    {
      "epoch": 0.8729087586109939,
      "grad_norm": 1.6321132183074951,
      "learning_rate": 0.00019798966292512558,
      "loss": 1.042,
      "step": 6209
    },
    {
      "epoch": 0.8730493462673977,
      "grad_norm": 1.4531956911087036,
      "learning_rate": 0.0001979430191592744,
      "loss": 0.9495,
      "step": 6210
    },
    {
      "epoch": 0.8731899339238015,
      "grad_norm": 1.510131597518921,
      "learning_rate": 0.0001978958460893843,
      "loss": 1.2615,
      "step": 6211
    },
    {
      "epoch": 0.8733305215802053,
      "grad_norm": 1.550824522972107,
      "learning_rate": 0.00019784814397038822,
      "loss": 1.2603,
      "step": 6212
    },
    {
      "epoch": 0.873471109236609,
      "grad_norm": 1.6592379808425903,
      "learning_rate": 0.000197799913060078,
      "loss": 1.0632,
      "step": 6213
    },
    {
      "epoch": 0.8736116968930128,
      "grad_norm": 1.3620550632476807,
      "learning_rate": 0.0001977511536191035,
      "loss": 1.1808,
      "step": 6214
    },
    {
      "epoch": 0.8737522845494166,
      "grad_norm": 1.4227341413497925,
      "learning_rate": 0.00019770186591097055,
      "loss": 1.1627,
      "step": 6215
    },
    {
      "epoch": 0.8738928722058203,
      "grad_norm": 1.4327707290649414,
      "learning_rate": 0.00019765205020204,
      "loss": 1.0825,
      "step": 6216
    },
    {
      "epoch": 0.8740334598622241,
      "grad_norm": 1.363798975944519,
      "learning_rate": 0.00019760170676152603,
      "loss": 1.3218,
      "step": 6217
    },
    {
      "epoch": 0.8741740475186278,
      "grad_norm": 1.5900293588638306,
      "learning_rate": 0.00019755083586149496,
      "loss": 0.9668,
      "step": 6218
    },
    {
      "epoch": 0.8743146351750316,
      "grad_norm": 1.5782054662704468,
      "learning_rate": 0.00019749943777686347,
      "loss": 1.1599,
      "step": 6219
    },
    {
      "epoch": 0.8744552228314354,
      "grad_norm": 1.372436285018921,
      "learning_rate": 0.00019744751278539727,
      "loss": 1.2888,
      "step": 6220
    },
    {
      "epoch": 0.8745958104878392,
      "grad_norm": 1.6607648134231567,
      "learning_rate": 0.00019739506116770957,
      "loss": 1.0799,
      "step": 6221
    },
    {
      "epoch": 0.874736398144243,
      "grad_norm": 1.4729442596435547,
      "learning_rate": 0.00019734208320725972,
      "loss": 0.9825,
      "step": 6222
    },
    {
      "epoch": 0.8748769858006467,
      "grad_norm": 1.8251850605010986,
      "learning_rate": 0.00019728857919035125,
      "loss": 1.2521,
      "step": 6223
    },
    {
      "epoch": 0.8750175734570504,
      "grad_norm": 1.7067652940750122,
      "learning_rate": 0.00019723454940613088,
      "loss": 1.1862,
      "step": 6224
    },
    {
      "epoch": 0.8751581611134542,
      "grad_norm": 1.5229543447494507,
      "learning_rate": 0.00019717999414658655,
      "loss": 1.1365,
      "step": 6225
    },
    {
      "epoch": 0.875298748769858,
      "grad_norm": 1.4963674545288086,
      "learning_rate": 0.000197124913706546,
      "loss": 1.2076,
      "step": 6226
    },
    {
      "epoch": 0.8754393364262618,
      "grad_norm": 1.6534603834152222,
      "learning_rate": 0.00019706930838367517,
      "loss": 1.2654,
      "step": 6227
    },
    {
      "epoch": 0.8755799240826655,
      "grad_norm": 1.5388236045837402,
      "learning_rate": 0.00019701317847847652,
      "loss": 1.0325,
      "step": 6228
    },
    {
      "epoch": 0.8757205117390693,
      "grad_norm": 1.5922486782073975,
      "learning_rate": 0.00019695652429428754,
      "loss": 1.1181,
      "step": 6229
    },
    {
      "epoch": 0.8758610993954731,
      "grad_norm": 1.5083140134811401,
      "learning_rate": 0.00019689934613727902,
      "loss": 1.1231,
      "step": 6230
    },
    {
      "epoch": 0.8760016870518769,
      "grad_norm": 1.523826241493225,
      "learning_rate": 0.00019684164431645328,
      "loss": 1.213,
      "step": 6231
    },
    {
      "epoch": 0.8761422747082807,
      "grad_norm": 1.4014438390731812,
      "learning_rate": 0.00019678341914364274,
      "loss": 1.2392,
      "step": 6232
    },
    {
      "epoch": 0.8762828623646843,
      "grad_norm": 1.482775330543518,
      "learning_rate": 0.00019672467093350822,
      "loss": 1.178,
      "step": 6233
    },
    {
      "epoch": 0.8764234500210881,
      "grad_norm": 1.5870132446289062,
      "learning_rate": 0.0001966654000035369,
      "loss": 1.1547,
      "step": 6234
    },
    {
      "epoch": 0.8765640376774919,
      "grad_norm": 1.6379015445709229,
      "learning_rate": 0.00019660560667404098,
      "loss": 0.989,
      "step": 6235
    },
    {
      "epoch": 0.8767046253338957,
      "grad_norm": 1.8849948644638062,
      "learning_rate": 0.00019654529126815582,
      "loss": 1.0101,
      "step": 6236
    },
    {
      "epoch": 0.8768452129902995,
      "grad_norm": 1.940879464149475,
      "learning_rate": 0.00019648445411183816,
      "loss": 1.0609,
      "step": 6237
    },
    {
      "epoch": 0.8769858006467032,
      "grad_norm": 1.443382978439331,
      "learning_rate": 0.0001964230955338645,
      "loss": 1.2278,
      "step": 6238
    },
    {
      "epoch": 0.877126388303107,
      "grad_norm": 1.4613780975341797,
      "learning_rate": 0.0001963612158658289,
      "loss": 1.184,
      "step": 6239
    },
    {
      "epoch": 0.8772669759595108,
      "grad_norm": 1.462450385093689,
      "learning_rate": 0.00019629881544214176,
      "loss": 1.1319,
      "step": 6240
    },
    {
      "epoch": 0.8774075636159145,
      "grad_norm": 1.4693776369094849,
      "learning_rate": 0.00019623589460002783,
      "loss": 1.0803,
      "step": 6241
    },
    {
      "epoch": 0.8775481512723183,
      "grad_norm": 1.3492608070373535,
      "learning_rate": 0.00019617245367952403,
      "loss": 1.0576,
      "step": 6242
    },
    {
      "epoch": 0.877688738928722,
      "grad_norm": 1.559926152229309,
      "learning_rate": 0.00019610849302347804,
      "loss": 1.1316,
      "step": 6243
    },
    {
      "epoch": 0.8778293265851258,
      "grad_norm": 1.5201525688171387,
      "learning_rate": 0.00019604401297754635,
      "loss": 1.0137,
      "step": 6244
    },
    {
      "epoch": 0.8779699142415296,
      "grad_norm": 1.4809287786483765,
      "learning_rate": 0.0001959790138901922,
      "loss": 1.149,
      "step": 6245
    },
    {
      "epoch": 0.8781105018979334,
      "grad_norm": 1.5154500007629395,
      "learning_rate": 0.00019591349611268393,
      "loss": 1.2308,
      "step": 6246
    },
    {
      "epoch": 0.8782510895543372,
      "grad_norm": 1.4307491779327393,
      "learning_rate": 0.00019584745999909299,
      "loss": 0.9676,
      "step": 6247
    },
    {
      "epoch": 0.8783916772107409,
      "grad_norm": 1.8054863214492798,
      "learning_rate": 0.00019578090590629188,
      "loss": 1.006,
      "step": 6248
    },
    {
      "epoch": 0.8785322648671446,
      "grad_norm": 1.3877809047698975,
      "learning_rate": 0.0001957138341939527,
      "loss": 1.3351,
      "step": 6249
    },
    {
      "epoch": 0.8786728525235484,
      "grad_norm": 1.55948007106781,
      "learning_rate": 0.00019564624522454448,
      "loss": 0.9729,
      "step": 6250
    },
    {
      "epoch": 0.8788134401799522,
      "grad_norm": 1.6335482597351074,
      "learning_rate": 0.00019557813936333176,
      "loss": 1.1122,
      "step": 6251
    },
    {
      "epoch": 0.878954027836356,
      "grad_norm": 1.6987074613571167,
      "learning_rate": 0.0001955095169783727,
      "loss": 1.1003,
      "step": 6252
    },
    {
      "epoch": 0.8790946154927597,
      "grad_norm": 1.607047438621521,
      "learning_rate": 0.00019544037844051646,
      "loss": 1.1269,
      "step": 6253
    },
    {
      "epoch": 0.8792352031491635,
      "grad_norm": 1.823833703994751,
      "learning_rate": 0.00019537072412340186,
      "loss": 0.9292,
      "step": 6254
    },
    {
      "epoch": 0.8793757908055673,
      "grad_norm": 2.0652248859405518,
      "learning_rate": 0.00019530055440345506,
      "loss": 0.9585,
      "step": 6255
    },
    {
      "epoch": 0.8795163784619711,
      "grad_norm": 1.5573054552078247,
      "learning_rate": 0.00019522986965988745,
      "loss": 1.0223,
      "step": 6256
    },
    {
      "epoch": 0.8796569661183749,
      "grad_norm": 1.3964755535125732,
      "learning_rate": 0.00019515867027469392,
      "loss": 1.1722,
      "step": 6257
    },
    {
      "epoch": 0.8797975537747785,
      "grad_norm": 1.6252235174179077,
      "learning_rate": 0.00019508695663265031,
      "loss": 1.1172,
      "step": 6258
    },
    {
      "epoch": 0.8799381414311823,
      "grad_norm": 1.627620816230774,
      "learning_rate": 0.00019501472912131175,
      "loss": 1.1131,
      "step": 6259
    },
    {
      "epoch": 0.8800787290875861,
      "grad_norm": 2.025327205657959,
      "learning_rate": 0.00019494198813101065,
      "loss": 1.0675,
      "step": 6260
    },
    {
      "epoch": 0.8802193167439899,
      "grad_norm": 1.503050446510315,
      "learning_rate": 0.00019486873405485393,
      "loss": 1.0409,
      "step": 6261
    },
    {
      "epoch": 0.8803599044003937,
      "grad_norm": 1.4362365007400513,
      "learning_rate": 0.00019479496728872167,
      "loss": 1.0607,
      "step": 6262
    },
    {
      "epoch": 0.8805004920567974,
      "grad_norm": 1.410366177558899,
      "learning_rate": 0.0001947206882312644,
      "loss": 1.0454,
      "step": 6263
    },
    {
      "epoch": 0.8806410797132012,
      "grad_norm": 1.5220593214035034,
      "learning_rate": 0.0001946458972839014,
      "loss": 1.2778,
      "step": 6264
    },
    {
      "epoch": 0.880781667369605,
      "grad_norm": 1.7586288452148438,
      "learning_rate": 0.00019457059485081818,
      "loss": 1.2868,
      "step": 6265
    },
    {
      "epoch": 0.8809222550260087,
      "grad_norm": 1.7254953384399414,
      "learning_rate": 0.00019449478133896437,
      "loss": 1.0897,
      "step": 6266
    },
    {
      "epoch": 0.8810628426824125,
      "grad_norm": 1.543230652809143,
      "learning_rate": 0.0001944184571580516,
      "loss": 1.1527,
      "step": 6267
    },
    {
      "epoch": 0.8812034303388162,
      "grad_norm": 1.5498888492584229,
      "learning_rate": 0.00019434162272055148,
      "loss": 1.0726,
      "step": 6268
    },
    {
      "epoch": 0.88134401799522,
      "grad_norm": 1.760201334953308,
      "learning_rate": 0.0001942642784416928,
      "loss": 1.1273,
      "step": 6269
    },
    {
      "epoch": 0.8814846056516238,
      "grad_norm": 1.5934005975723267,
      "learning_rate": 0.00019418642473945986,
      "loss": 1.1804,
      "step": 6270
    },
    {
      "epoch": 0.8816251933080276,
      "grad_norm": 1.4877043962478638,
      "learning_rate": 0.00019410806203459002,
      "loss": 0.961,
      "step": 6271
    },
    {
      "epoch": 0.8817657809644314,
      "grad_norm": 1.5874605178833008,
      "learning_rate": 0.00019402919075057118,
      "loss": 1.0357,
      "step": 6272
    },
    {
      "epoch": 0.881906368620835,
      "grad_norm": 1.8418123722076416,
      "learning_rate": 0.00019394981131363998,
      "loss": 1.0466,
      "step": 6273
    },
    {
      "epoch": 0.8820469562772388,
      "grad_norm": 2.0968949794769287,
      "learning_rate": 0.00019386992415277888,
      "loss": 1.1216,
      "step": 6274
    },
    {
      "epoch": 0.8821875439336426,
      "grad_norm": 1.4984673261642456,
      "learning_rate": 0.0001937895296997145,
      "loss": 1.1645,
      "step": 6275
    },
    {
      "epoch": 0.8823281315900464,
      "grad_norm": 1.4272727966308594,
      "learning_rate": 0.00019370862838891494,
      "loss": 1.0875,
      "step": 6276
    },
    {
      "epoch": 0.8824687192464502,
      "grad_norm": 1.5758718252182007,
      "learning_rate": 0.00019362722065758718,
      "loss": 1.1545,
      "step": 6277
    },
    {
      "epoch": 0.8826093069028539,
      "grad_norm": 1.4487682580947876,
      "learning_rate": 0.00019354530694567528,
      "loss": 1.1255,
      "step": 6278
    },
    {
      "epoch": 0.8827498945592577,
      "grad_norm": 1.723207712173462,
      "learning_rate": 0.00019346288769585772,
      "loss": 1.0867,
      "step": 6279
    },
    {
      "epoch": 0.8828904822156615,
      "grad_norm": 1.7369710206985474,
      "learning_rate": 0.0001933799633535448,
      "loss": 1.267,
      "step": 6280
    },
    {
      "epoch": 0.8830310698720653,
      "grad_norm": 1.5821813344955444,
      "learning_rate": 0.00019329653436687662,
      "loss": 0.9647,
      "step": 6281
    },
    {
      "epoch": 0.883171657528469,
      "grad_norm": 1.5088082551956177,
      "learning_rate": 0.00019321260118672042,
      "loss": 1.1097,
      "step": 6282
    },
    {
      "epoch": 0.8833122451848727,
      "grad_norm": 1.6615917682647705,
      "learning_rate": 0.00019312816426666823,
      "loss": 1.1272,
      "step": 6283
    },
    {
      "epoch": 0.8834528328412765,
      "grad_norm": 1.6318162679672241,
      "learning_rate": 0.0001930432240630344,
      "loss": 1.1957,
      "step": 6284
    },
    {
      "epoch": 0.8835934204976803,
      "grad_norm": 1.699544906616211,
      "learning_rate": 0.00019295778103485303,
      "loss": 1.092,
      "step": 6285
    },
    {
      "epoch": 0.8837340081540841,
      "grad_norm": 1.452155351638794,
      "learning_rate": 0.00019287183564387563,
      "loss": 1.1093,
      "step": 6286
    },
    {
      "epoch": 0.8838745958104879,
      "grad_norm": 1.962667465209961,
      "learning_rate": 0.0001927853883545688,
      "loss": 0.9863,
      "step": 6287
    },
    {
      "epoch": 0.8840151834668916,
      "grad_norm": 1.726178765296936,
      "learning_rate": 0.00019269843963411117,
      "loss": 1.0038,
      "step": 6288
    },
    {
      "epoch": 0.8841557711232954,
      "grad_norm": 1.7817556858062744,
      "learning_rate": 0.00019261098995239123,
      "loss": 0.9891,
      "step": 6289
    },
    {
      "epoch": 0.8842963587796991,
      "grad_norm": 1.5124626159667969,
      "learning_rate": 0.00019252303978200522,
      "loss": 1.1666,
      "step": 6290
    },
    {
      "epoch": 0.8844369464361029,
      "grad_norm": 1.627833366394043,
      "learning_rate": 0.00019243458959825347,
      "loss": 1.1291,
      "step": 6291
    },
    {
      "epoch": 0.8845775340925067,
      "grad_norm": 1.400123953819275,
      "learning_rate": 0.000192345639879139,
      "loss": 0.9903,
      "step": 6292
    },
    {
      "epoch": 0.8847181217489104,
      "grad_norm": 1.5058045387268066,
      "learning_rate": 0.00019225619110536403,
      "loss": 1.1122,
      "step": 6293
    },
    {
      "epoch": 0.8848587094053142,
      "grad_norm": 1.5628327131271362,
      "learning_rate": 0.00019216624376032816,
      "loss": 1.0261,
      "step": 6294
    },
    {
      "epoch": 0.884999297061718,
      "grad_norm": 1.5974706411361694,
      "learning_rate": 0.00019207579833012512,
      "loss": 1.0237,
      "step": 6295
    },
    {
      "epoch": 0.8851398847181218,
      "grad_norm": 1.7940621376037598,
      "learning_rate": 0.0001919848553035404,
      "loss": 1.1585,
      "step": 6296
    },
    {
      "epoch": 0.8852804723745256,
      "grad_norm": 1.5367661714553833,
      "learning_rate": 0.00019189341517204857,
      "loss": 1.069,
      "step": 6297
    },
    {
      "epoch": 0.8854210600309292,
      "grad_norm": 1.9608746767044067,
      "learning_rate": 0.00019180147842981103,
      "loss": 1.0906,
      "step": 6298
    },
    {
      "epoch": 0.885561647687333,
      "grad_norm": 1.4846490621566772,
      "learning_rate": 0.0001917090455736724,
      "loss": 1.2059,
      "step": 6299
    },
    {
      "epoch": 0.8857022353437368,
      "grad_norm": 1.9853787422180176,
      "learning_rate": 0.00019161611710315882,
      "loss": 1.0617,
      "step": 6300
    },
    {
      "epoch": 0.8858428230001406,
      "grad_norm": 1.5920952558517456,
      "learning_rate": 0.0001915226935204745,
      "loss": 1.0634,
      "step": 6301
    },
    {
      "epoch": 0.8859834106565444,
      "grad_norm": 1.5354681015014648,
      "learning_rate": 0.00019142877533049975,
      "loss": 1.1546,
      "step": 6302
    },
    {
      "epoch": 0.8861239983129481,
      "grad_norm": 1.4436358213424683,
      "learning_rate": 0.00019133436304078754,
      "loss": 1.071,
      "step": 6303
    },
    {
      "epoch": 0.8862645859693519,
      "grad_norm": 1.700414776802063,
      "learning_rate": 0.00019123945716156105,
      "loss": 1.0366,
      "step": 6304
    },
    {
      "epoch": 0.8864051736257557,
      "grad_norm": 1.5852826833724976,
      "learning_rate": 0.000191144058205711,
      "loss": 1.0831,
      "step": 6305
    },
    {
      "epoch": 0.8865457612821595,
      "grad_norm": 1.7942229509353638,
      "learning_rate": 0.00019104816668879298,
      "loss": 1.2648,
      "step": 6306
    },
    {
      "epoch": 0.8866863489385632,
      "grad_norm": 1.7961047887802124,
      "learning_rate": 0.00019095178312902405,
      "loss": 1.1179,
      "step": 6307
    },
    {
      "epoch": 0.8868269365949669,
      "grad_norm": 1.5688387155532837,
      "learning_rate": 0.00019085490804728075,
      "loss": 1.082,
      "step": 6308
    },
    {
      "epoch": 0.8869675242513707,
      "grad_norm": 1.7847216129302979,
      "learning_rate": 0.00019075754196709572,
      "loss": 1.176,
      "step": 6309
    },
    {
      "epoch": 0.8871081119077745,
      "grad_norm": 1.7829781770706177,
      "learning_rate": 0.00019065968541465513,
      "loss": 1.1243,
      "step": 6310
    },
    {
      "epoch": 0.8872486995641783,
      "grad_norm": 1.3460321426391602,
      "learning_rate": 0.00019056133891879579,
      "loss": 1.0228,
      "step": 6311
    },
    {
      "epoch": 0.8873892872205821,
      "grad_norm": 1.7017979621887207,
      "learning_rate": 0.00019046250301100199,
      "loss": 1.097,
      "step": 6312
    },
    {
      "epoch": 0.8875298748769858,
      "grad_norm": 1.5155030488967896,
      "learning_rate": 0.00019036317822540336,
      "loss": 1.0929,
      "step": 6313
    },
    {
      "epoch": 0.8876704625333895,
      "grad_norm": 1.5356099605560303,
      "learning_rate": 0.00019026336509877126,
      "loss": 1.0342,
      "step": 6314
    },
    {
      "epoch": 0.8878110501897933,
      "grad_norm": 2.386984348297119,
      "learning_rate": 0.00019016306417051602,
      "loss": 1.0894,
      "step": 6315
    },
    {
      "epoch": 0.8879516378461971,
      "grad_norm": 2.086090326309204,
      "learning_rate": 0.00019006227598268433,
      "loss": 0.9626,
      "step": 6316
    },
    {
      "epoch": 0.8880922255026009,
      "grad_norm": 1.4490245580673218,
      "learning_rate": 0.00018996100107995632,
      "loss": 1.1968,
      "step": 6317
    },
    {
      "epoch": 0.8882328131590046,
      "grad_norm": 1.8368602991104126,
      "learning_rate": 0.00018985924000964195,
      "loss": 1.0438,
      "step": 6318
    },
    {
      "epoch": 0.8883734008154084,
      "grad_norm": 1.5259191989898682,
      "learning_rate": 0.000189756993321679,
      "loss": 1.2751,
      "step": 6319
    },
    {
      "epoch": 0.8885139884718122,
      "grad_norm": 2.003312349319458,
      "learning_rate": 0.0001896542615686291,
      "loss": 1.4439,
      "step": 6320
    },
    {
      "epoch": 0.888654576128216,
      "grad_norm": 1.4312918186187744,
      "learning_rate": 0.00018955104530567587,
      "loss": 1.2727,
      "step": 6321
    },
    {
      "epoch": 0.8887951637846198,
      "grad_norm": 1.3547639846801758,
      "learning_rate": 0.00018944734509062104,
      "loss": 0.9829,
      "step": 6322
    },
    {
      "epoch": 0.8889357514410234,
      "grad_norm": 1.5510088205337524,
      "learning_rate": 0.0001893431614838815,
      "loss": 1.189,
      "step": 6323
    },
    {
      "epoch": 0.8890763390974272,
      "grad_norm": 1.4649487733840942,
      "learning_rate": 0.0001892384950484867,
      "loss": 1.093,
      "step": 6324
    },
    {
      "epoch": 0.889216926753831,
      "grad_norm": 1.478615403175354,
      "learning_rate": 0.00018913334635007573,
      "loss": 1.181,
      "step": 6325
    },
    {
      "epoch": 0.8893575144102348,
      "grad_norm": 1.3818132877349854,
      "learning_rate": 0.00018902771595689325,
      "loss": 1.0702,
      "step": 6326
    },
    {
      "epoch": 0.8894981020666386,
      "grad_norm": 1.664141297340393,
      "learning_rate": 0.00018892160443978752,
      "loss": 1.2168,
      "step": 6327
    },
    {
      "epoch": 0.8896386897230423,
      "grad_norm": 1.3369783163070679,
      "learning_rate": 0.00018881501237220683,
      "loss": 1.1759,
      "step": 6328
    },
    {
      "epoch": 0.8897792773794461,
      "grad_norm": 1.81209135055542,
      "learning_rate": 0.00018870794033019645,
      "loss": 1.0203,
      "step": 6329
    },
    {
      "epoch": 0.8899198650358499,
      "grad_norm": 1.4312351942062378,
      "learning_rate": 0.00018860038889239558,
      "loss": 1.2242,
      "step": 6330
    },
    {
      "epoch": 0.8900604526922536,
      "grad_norm": 1.5400738716125488,
      "learning_rate": 0.00018849235864003386,
      "loss": 1.1914,
      "step": 6331
    },
    {
      "epoch": 0.8902010403486574,
      "grad_norm": 1.4642376899719238,
      "learning_rate": 0.0001883838501569291,
      "loss": 1.3285,
      "step": 6332
    },
    {
      "epoch": 0.8903416280050611,
      "grad_norm": 1.579999566078186,
      "learning_rate": 0.00018827486402948318,
      "loss": 1.1617,
      "step": 6333
    },
    {
      "epoch": 0.8904822156614649,
      "grad_norm": 1.7391313314437866,
      "learning_rate": 0.0001881654008466792,
      "loss": 1.1801,
      "step": 6334
    },
    {
      "epoch": 0.8906228033178687,
      "grad_norm": 1.484275460243225,
      "learning_rate": 0.00018805546120007863,
      "loss": 1.0459,
      "step": 6335
    },
    {
      "epoch": 0.8907633909742725,
      "grad_norm": 1.5246872901916504,
      "learning_rate": 0.0001879450456838177,
      "loss": 1.1465,
      "step": 6336
    },
    {
      "epoch": 0.8909039786306763,
      "grad_norm": 1.6901015043258667,
      "learning_rate": 0.00018783415489460442,
      "loss": 1.012,
      "step": 6337
    },
    {
      "epoch": 0.89104456628708,
      "grad_norm": 1.5391663312911987,
      "learning_rate": 0.0001877227894317152,
      "loss": 1.0717,
      "step": 6338
    },
    {
      "epoch": 0.8911851539434837,
      "grad_norm": 1.3827171325683594,
      "learning_rate": 0.0001876109498969915,
      "loss": 1.1567,
      "step": 6339
    },
    {
      "epoch": 0.8913257415998875,
      "grad_norm": 1.7718429565429688,
      "learning_rate": 0.00018749863689483722,
      "loss": 1.0981,
      "step": 6340
    },
    {
      "epoch": 0.8914663292562913,
      "grad_norm": 1.6265588998794556,
      "learning_rate": 0.00018738585103221473,
      "loss": 0.9565,
      "step": 6341
    },
    {
      "epoch": 0.8916069169126951,
      "grad_norm": 1.3795002698898315,
      "learning_rate": 0.00018727259291864164,
      "loss": 1.1312,
      "step": 6342
    },
    {
      "epoch": 0.8917475045690988,
      "grad_norm": 1.700629711151123,
      "learning_rate": 0.00018715886316618793,
      "loss": 1.0978,
      "step": 6343
    },
    {
      "epoch": 0.8918880922255026,
      "grad_norm": 1.5042741298675537,
      "learning_rate": 0.00018704466238947243,
      "loss": 1.1104,
      "step": 6344
    },
    {
      "epoch": 0.8920286798819064,
      "grad_norm": 1.664193034172058,
      "learning_rate": 0.00018692999120565934,
      "loss": 1.0792,
      "step": 6345
    },
    {
      "epoch": 0.8921692675383102,
      "grad_norm": 1.6032724380493164,
      "learning_rate": 0.00018681485023445518,
      "loss": 1.0361,
      "step": 6346
    },
    {
      "epoch": 0.892309855194714,
      "grad_norm": 1.562300443649292,
      "learning_rate": 0.00018669924009810518,
      "loss": 0.9804,
      "step": 6347
    },
    {
      "epoch": 0.8924504428511176,
      "grad_norm": 1.4831124544143677,
      "learning_rate": 0.00018658316142139007,
      "loss": 1.0278,
      "step": 6348
    },
    {
      "epoch": 0.8925910305075214,
      "grad_norm": 1.561264157295227,
      "learning_rate": 0.00018646661483162286,
      "loss": 1.2374,
      "step": 6349
    },
    {
      "epoch": 0.8927316181639252,
      "grad_norm": 1.4529964923858643,
      "learning_rate": 0.00018634960095864468,
      "loss": 1.185,
      "step": 6350
    },
    {
      "epoch": 0.892872205820329,
      "grad_norm": 1.5258409976959229,
      "learning_rate": 0.00018623212043482277,
      "loss": 1.093,
      "step": 6351
    },
    {
      "epoch": 0.8930127934767328,
      "grad_norm": 1.4549062252044678,
      "learning_rate": 0.0001861141738950456,
      "loss": 1.1798,
      "step": 6352
    },
    {
      "epoch": 0.8931533811331365,
      "grad_norm": 1.5218631029129028,
      "learning_rate": 0.00018599576197672035,
      "loss": 1.1266,
      "step": 6353
    },
    {
      "epoch": 0.8932939687895403,
      "grad_norm": 2.2403485774993896,
      "learning_rate": 0.00018587688531976918,
      "loss": 1.0561,
      "step": 6354
    },
    {
      "epoch": 0.893434556445944,
      "grad_norm": 1.8621560335159302,
      "learning_rate": 0.00018575754456662575,
      "loss": 1.1599,
      "step": 6355
    },
    {
      "epoch": 0.8935751441023478,
      "grad_norm": 1.5919421911239624,
      "learning_rate": 0.0001856377403622318,
      "loss": 1.1737,
      "step": 6356
    },
    {
      "epoch": 0.8937157317587516,
      "grad_norm": 1.4922878742218018,
      "learning_rate": 0.00018551747335403385,
      "loss": 1.0942,
      "step": 6357
    },
    {
      "epoch": 0.8938563194151553,
      "grad_norm": 1.8164589405059814,
      "learning_rate": 0.00018539674419197895,
      "loss": 1.3627,
      "step": 6358
    },
    {
      "epoch": 0.8939969070715591,
      "grad_norm": 1.8989347219467163,
      "learning_rate": 0.00018527555352851246,
      "loss": 1.0352,
      "step": 6359
    },
    {
      "epoch": 0.8941374947279629,
      "grad_norm": 1.9497973918914795,
      "learning_rate": 0.00018515390201857335,
      "loss": 0.9997,
      "step": 6360
    },
    {
      "epoch": 0.8942780823843667,
      "grad_norm": 1.4469410181045532,
      "learning_rate": 0.00018503179031959102,
      "loss": 1.2748,
      "step": 6361
    },
    {
      "epoch": 0.8944186700407705,
      "grad_norm": 1.5533922910690308,
      "learning_rate": 0.0001849092190914821,
      "loss": 0.9936,
      "step": 6362
    },
    {
      "epoch": 0.8945592576971741,
      "grad_norm": 1.5229958295822144,
      "learning_rate": 0.00018478618899664645,
      "loss": 1.0281,
      "step": 6363
    },
    {
      "epoch": 0.8946998453535779,
      "grad_norm": 1.5649421215057373,
      "learning_rate": 0.00018466270069996383,
      "loss": 1.1463,
      "step": 6364
    },
    {
      "epoch": 0.8948404330099817,
      "grad_norm": 1.5594871044158936,
      "learning_rate": 0.0001845387548687901,
      "loss": 1.2037,
      "step": 6365
    },
    {
      "epoch": 0.8949810206663855,
      "grad_norm": 1.7333632707595825,
      "learning_rate": 0.00018441435217295387,
      "loss": 1.0283,
      "step": 6366
    },
    {
      "epoch": 0.8951216083227893,
      "grad_norm": 1.5330479145050049,
      "learning_rate": 0.00018428949328475261,
      "loss": 1.1675,
      "step": 6367
    },
    {
      "epoch": 0.895262195979193,
      "grad_norm": 1.6849862337112427,
      "learning_rate": 0.0001841641788789493,
      "loss": 1.0626,
      "step": 6368
    },
    {
      "epoch": 0.8954027836355968,
      "grad_norm": 1.461786150932312,
      "learning_rate": 0.00018403840963276827,
      "loss": 1.0598,
      "step": 6369
    },
    {
      "epoch": 0.8955433712920006,
      "grad_norm": 1.4823565483093262,
      "learning_rate": 0.0001839121862258925,
      "loss": 1.2585,
      "step": 6370
    },
    {
      "epoch": 0.8956839589484044,
      "grad_norm": 1.6111935377120972,
      "learning_rate": 0.00018378550934045876,
      "loss": 0.994,
      "step": 6371
    },
    {
      "epoch": 0.8958245466048081,
      "grad_norm": 1.6366283893585205,
      "learning_rate": 0.00018365837966105486,
      "loss": 1.0852,
      "step": 6372
    },
    {
      "epoch": 0.8959651342612118,
      "grad_norm": 1.9053990840911865,
      "learning_rate": 0.0001835307978747155,
      "loss": 1.1538,
      "step": 6373
    },
    {
      "epoch": 0.8961057219176156,
      "grad_norm": 1.6906307935714722,
      "learning_rate": 0.00018340276467091862,
      "loss": 1.1129,
      "step": 6374
    },
    {
      "epoch": 0.8962463095740194,
      "grad_norm": 1.6204514503479004,
      "learning_rate": 0.00018327428074158174,
      "loss": 1.0773,
      "step": 6375
    },
    {
      "epoch": 0.8963868972304232,
      "grad_norm": 1.5157361030578613,
      "learning_rate": 0.0001831453467810584,
      "loss": 1.0869,
      "step": 6376
    },
    {
      "epoch": 0.896527484886827,
      "grad_norm": 1.6222038269042969,
      "learning_rate": 0.00018301596348613362,
      "loss": 1.003,
      "step": 6377
    },
    {
      "epoch": 0.8966680725432307,
      "grad_norm": 1.7853180170059204,
      "learning_rate": 0.0001828861315560215,
      "loss": 0.97,
      "step": 6378
    },
    {
      "epoch": 0.8968086601996345,
      "grad_norm": 1.358960509300232,
      "learning_rate": 0.00018275585169235997,
      "loss": 1.195,
      "step": 6379
    },
    {
      "epoch": 0.8969492478560382,
      "grad_norm": 1.5706301927566528,
      "learning_rate": 0.00018262512459920807,
      "loss": 1.2116,
      "step": 6380
    },
    {
      "epoch": 0.897089835512442,
      "grad_norm": 1.6354516744613647,
      "learning_rate": 0.00018249395098304168,
      "loss": 1.0305,
      "step": 6381
    },
    {
      "epoch": 0.8972304231688458,
      "grad_norm": 1.452207326889038,
      "learning_rate": 0.0001823623315527497,
      "loss": 1.2923,
      "step": 6382
    },
    {
      "epoch": 0.8973710108252495,
      "grad_norm": 1.8590906858444214,
      "learning_rate": 0.0001822302670196304,
      "loss": 1.1947,
      "step": 6383
    },
    {
      "epoch": 0.8975115984816533,
      "grad_norm": 1.9794228076934814,
      "learning_rate": 0.00018209775809738745,
      "loss": 1.1399,
      "step": 6384
    },
    {
      "epoch": 0.8976521861380571,
      "grad_norm": 1.7283499240875244,
      "learning_rate": 0.00018196480550212603,
      "loss": 1.0648,
      "step": 6385
    },
    {
      "epoch": 0.8977927737944609,
      "grad_norm": 1.5011584758758545,
      "learning_rate": 0.00018183140995234917,
      "loss": 1.1469,
      "step": 6386
    },
    {
      "epoch": 0.8979333614508647,
      "grad_norm": 1.577842354774475,
      "learning_rate": 0.00018169757216895347,
      "loss": 1.0114,
      "step": 6387
    },
    {
      "epoch": 0.8980739491072683,
      "grad_norm": 1.6186087131500244,
      "learning_rate": 0.00018156329287522552,
      "loss": 1.0746,
      "step": 6388
    },
    {
      "epoch": 0.8982145367636721,
      "grad_norm": 1.5673028230667114,
      "learning_rate": 0.0001814285727968383,
      "loss": 1.1323,
      "step": 6389
    },
    {
      "epoch": 0.8983551244200759,
      "grad_norm": 1.663134217262268,
      "learning_rate": 0.00018129341266184633,
      "loss": 1.1743,
      "step": 6390
    },
    {
      "epoch": 0.8984957120764797,
      "grad_norm": 1.5014607906341553,
      "learning_rate": 0.0001811578132006826,
      "loss": 1.1924,
      "step": 6391
    },
    {
      "epoch": 0.8986362997328835,
      "grad_norm": 1.4714635610580444,
      "learning_rate": 0.0001810217751461542,
      "loss": 1.1135,
      "step": 6392
    },
    {
      "epoch": 0.8987768873892872,
      "grad_norm": 1.5610110759735107,
      "learning_rate": 0.00018088529923343852,
      "loss": 1.2171,
      "step": 6393
    },
    {
      "epoch": 0.898917475045691,
      "grad_norm": 1.5925227403640747,
      "learning_rate": 0.00018074838620007927,
      "loss": 1.0544,
      "step": 6394
    },
    {
      "epoch": 0.8990580627020948,
      "grad_norm": 1.6463463306427002,
      "learning_rate": 0.00018061103678598242,
      "loss": 1.0578,
      "step": 6395
    },
    {
      "epoch": 0.8991986503584986,
      "grad_norm": 1.5895497798919678,
      "learning_rate": 0.00018047325173341193,
      "loss": 1.207,
      "step": 6396
    },
    {
      "epoch": 0.8993392380149023,
      "grad_norm": 1.597874402999878,
      "learning_rate": 0.00018033503178698664,
      "loss": 1.2434,
      "step": 6397
    },
    {
      "epoch": 0.899479825671306,
      "grad_norm": 1.6492507457733154,
      "learning_rate": 0.00018019637769367514,
      "loss": 0.925,
      "step": 6398
    },
    {
      "epoch": 0.8996204133277098,
      "grad_norm": 1.350553274154663,
      "learning_rate": 0.00018005729020279242,
      "loss": 1.0922,
      "step": 6399
    },
    {
      "epoch": 0.8997610009841136,
      "grad_norm": 1.4840247631072998,
      "learning_rate": 0.00017991777006599565,
      "loss": 1.1694,
      "step": 6400
    },
    {
      "epoch": 0.8999015886405174,
      "grad_norm": 1.6107951402664185,
      "learning_rate": 0.00017977781803728014,
      "loss": 1.0152,
      "step": 6401
    },
    {
      "epoch": 0.9000421762969212,
      "grad_norm": 1.5258146524429321,
      "learning_rate": 0.00017963743487297507,
      "loss": 1.2058,
      "step": 6402
    },
    {
      "epoch": 0.9001827639533249,
      "grad_norm": 1.5930966138839722,
      "learning_rate": 0.00017949662133173977,
      "loss": 1.2802,
      "step": 6403
    },
    {
      "epoch": 0.9003233516097287,
      "grad_norm": 1.5087382793426514,
      "learning_rate": 0.0001793553781745593,
      "loss": 1.1449,
      "step": 6404
    },
    {
      "epoch": 0.9004639392661324,
      "grad_norm": 1.8385536670684814,
      "learning_rate": 0.0001792137061647405,
      "loss": 1.0766,
      "step": 6405
    },
    {
      "epoch": 0.9006045269225362,
      "grad_norm": 2.1027214527130127,
      "learning_rate": 0.00017907160606790766,
      "loss": 1.1998,
      "step": 6406
    },
    {
      "epoch": 0.90074511457894,
      "grad_norm": 1.3805711269378662,
      "learning_rate": 0.00017892907865199869,
      "loss": 1.0941,
      "step": 6407
    },
    {
      "epoch": 0.9008857022353437,
      "grad_norm": 1.5898939371109009,
      "learning_rate": 0.000178786124687261,
      "loss": 1.1087,
      "step": 6408
    },
    {
      "epoch": 0.9010262898917475,
      "grad_norm": 1.648245930671692,
      "learning_rate": 0.00017864274494624677,
      "loss": 1.0373,
      "step": 6409
    },
    {
      "epoch": 0.9011668775481513,
      "grad_norm": 1.480545163154602,
      "learning_rate": 0.0001784989402038093,
      "loss": 0.9882,
      "step": 6410
    },
    {
      "epoch": 0.9013074652045551,
      "grad_norm": 1.467873454093933,
      "learning_rate": 0.00017835471123709887,
      "loss": 1.0726,
      "step": 6411
    },
    {
      "epoch": 0.9014480528609589,
      "grad_norm": 1.449565052986145,
      "learning_rate": 0.00017821005882555816,
      "loss": 1.0618,
      "step": 6412
    },
    {
      "epoch": 0.9015886405173625,
      "grad_norm": 1.4734703302383423,
      "learning_rate": 0.00017806498375091837,
      "loss": 1.0533,
      "step": 6413
    },
    {
      "epoch": 0.9017292281737663,
      "grad_norm": 1.5945206880569458,
      "learning_rate": 0.00017791948679719458,
      "loss": 1.0787,
      "step": 6414
    },
    {
      "epoch": 0.9018698158301701,
      "grad_norm": 1.4688513278961182,
      "learning_rate": 0.000177773568750682,
      "loss": 1.1385,
      "step": 6415
    },
    {
      "epoch": 0.9020104034865739,
      "grad_norm": 1.3910737037658691,
      "learning_rate": 0.00017762723039995177,
      "loss": 1.0934,
      "step": 6416
    },
    {
      "epoch": 0.9021509911429777,
      "grad_norm": 1.3325132131576538,
      "learning_rate": 0.0001774804725358459,
      "loss": 1.1799,
      "step": 6417
    },
    {
      "epoch": 0.9022915787993814,
      "grad_norm": 2.0555496215820312,
      "learning_rate": 0.00017733329595147384,
      "loss": 1.0889,
      "step": 6418
    },
    {
      "epoch": 0.9024321664557852,
      "grad_norm": 1.5097869634628296,
      "learning_rate": 0.00017718570144220795,
      "loss": 1.1177,
      "step": 6419
    },
    {
      "epoch": 0.902572754112189,
      "grad_norm": 1.9092854261398315,
      "learning_rate": 0.00017703768980567892,
      "loss": 0.9628,
      "step": 6420
    },
    {
      "epoch": 0.9027133417685927,
      "grad_norm": 1.5738064050674438,
      "learning_rate": 0.000176889261841772,
      "loss": 1.1285,
      "step": 6421
    },
    {
      "epoch": 0.9028539294249965,
      "grad_norm": 1.5052008628845215,
      "learning_rate": 0.00017674041835262193,
      "loss": 1.1053,
      "step": 6422
    },
    {
      "epoch": 0.9029945170814002,
      "grad_norm": 1.4882272481918335,
      "learning_rate": 0.00017659116014260925,
      "loss": 1.2357,
      "step": 6423
    },
    {
      "epoch": 0.903135104737804,
      "grad_norm": 1.8263726234436035,
      "learning_rate": 0.00017644148801835606,
      "loss": 1.055,
      "step": 6424
    },
    {
      "epoch": 0.9032756923942078,
      "grad_norm": 1.932105541229248,
      "learning_rate": 0.00017629140278872075,
      "loss": 1.105,
      "step": 6425
    },
    {
      "epoch": 0.9034162800506116,
      "grad_norm": 1.8241925239562988,
      "learning_rate": 0.00017614090526479442,
      "loss": 0.9397,
      "step": 6426
    },
    {
      "epoch": 0.9035568677070154,
      "grad_norm": 1.509896993637085,
      "learning_rate": 0.00017598999625989675,
      "loss": 1.1698,
      "step": 6427
    },
    {
      "epoch": 0.903697455363419,
      "grad_norm": 1.6506980657577515,
      "learning_rate": 0.00017583867658957041,
      "loss": 0.9559,
      "step": 6428
    },
    {
      "epoch": 0.9038380430198228,
      "grad_norm": 1.7758029699325562,
      "learning_rate": 0.00017568694707157797,
      "loss": 1.0795,
      "step": 6429
    },
    {
      "epoch": 0.9039786306762266,
      "grad_norm": 1.634661078453064,
      "learning_rate": 0.00017553480852589636,
      "loss": 0.9192,
      "step": 6430
    },
    {
      "epoch": 0.9041192183326304,
      "grad_norm": 1.4851421117782593,
      "learning_rate": 0.00017538226177471356,
      "loss": 1.1764,
      "step": 6431
    },
    {
      "epoch": 0.9042598059890342,
      "grad_norm": 2.0490009784698486,
      "learning_rate": 0.00017522930764242336,
      "loss": 0.9901,
      "step": 6432
    },
    {
      "epoch": 0.9044003936454379,
      "grad_norm": 1.7594674825668335,
      "learning_rate": 0.0001750759469556208,
      "loss": 1.0774,
      "step": 6433
    },
    {
      "epoch": 0.9045409813018417,
      "grad_norm": 2.054579019546509,
      "learning_rate": 0.00017492218054309836,
      "loss": 1.2097,
      "step": 6434
    },
    {
      "epoch": 0.9046815689582455,
      "grad_norm": 1.58494234085083,
      "learning_rate": 0.00017476800923584137,
      "loss": 1.1214,
      "step": 6435
    },
    {
      "epoch": 0.9048221566146493,
      "grad_norm": 1.4708749055862427,
      "learning_rate": 0.00017461343386702266,
      "loss": 1.1703,
      "step": 6436
    },
    {
      "epoch": 0.904962744271053,
      "grad_norm": 1.6256266832351685,
      "learning_rate": 0.00017445845527199917,
      "loss": 0.9327,
      "step": 6437
    },
    {
      "epoch": 0.9051033319274567,
      "grad_norm": 1.487226128578186,
      "learning_rate": 0.00017430307428830674,
      "loss": 1.1677,
      "step": 6438
    },
    {
      "epoch": 0.9052439195838605,
      "grad_norm": 2.2381091117858887,
      "learning_rate": 0.00017414729175565597,
      "loss": 1.0257,
      "step": 6439
    },
    {
      "epoch": 0.9053845072402643,
      "grad_norm": 1.6512130498886108,
      "learning_rate": 0.00017399110851592747,
      "loss": 1.0079,
      "step": 6440
    },
    {
      "epoch": 0.9055250948966681,
      "grad_norm": 1.796645164489746,
      "learning_rate": 0.0001738345254131671,
      "loss": 1.0531,
      "step": 6441
    },
    {
      "epoch": 0.9056656825530719,
      "grad_norm": 1.7165007591247559,
      "learning_rate": 0.00017367754329358185,
      "loss": 1.1111,
      "step": 6442
    },
    {
      "epoch": 0.9058062702094756,
      "grad_norm": 1.8889862298965454,
      "learning_rate": 0.00017352016300553545,
      "loss": 1.1374,
      "step": 6443
    },
    {
      "epoch": 0.9059468578658794,
      "grad_norm": 1.535206913948059,
      "learning_rate": 0.0001733623853995427,
      "loss": 1.1941,
      "step": 6444
    },
    {
      "epoch": 0.9060874455222832,
      "grad_norm": 1.6663532257080078,
      "learning_rate": 0.0001732042113282659,
      "loss": 1.0994,
      "step": 6445
    },
    {
      "epoch": 0.9062280331786869,
      "grad_norm": 1.553877830505371,
      "learning_rate": 0.00017304564164651045,
      "loss": 1.2882,
      "step": 6446
    },
    {
      "epoch": 0.9063686208350907,
      "grad_norm": 1.3083009719848633,
      "learning_rate": 0.00017288667721121877,
      "loss": 1.2892,
      "step": 6447
    },
    {
      "epoch": 0.9065092084914944,
      "grad_norm": 1.467901587486267,
      "learning_rate": 0.00017272731888146728,
      "loss": 1.1591,
      "step": 6448
    },
    {
      "epoch": 0.9066497961478982,
      "grad_norm": 1.4598779678344727,
      "learning_rate": 0.00017256756751846063,
      "loss": 1.1777,
      "step": 6449
    },
    {
      "epoch": 0.906790383804302,
      "grad_norm": 1.4959535598754883,
      "learning_rate": 0.000172407423985528,
      "loss": 1.0108,
      "step": 6450
    },
    {
      "epoch": 0.9069309714607058,
      "grad_norm": 1.7344943284988403,
      "learning_rate": 0.00017224688914811774,
      "loss": 1.188,
      "step": 6451
    },
    {
      "epoch": 0.9070715591171096,
      "grad_norm": 1.4673240184783936,
      "learning_rate": 0.00017208596387379262,
      "loss": 1.166,
      "step": 6452
    },
    {
      "epoch": 0.9072121467735133,
      "grad_norm": 1.6872568130493164,
      "learning_rate": 0.00017192464903222554,
      "loss": 1.1948,
      "step": 6453
    },
    {
      "epoch": 0.907352734429917,
      "grad_norm": 1.8632227182388306,
      "learning_rate": 0.00017176294549519518,
      "loss": 1.1661,
      "step": 6454
    },
    {
      "epoch": 0.9074933220863208,
      "grad_norm": 1.4482210874557495,
      "learning_rate": 0.0001716008541365801,
      "loss": 1.1283,
      "step": 6455
    },
    {
      "epoch": 0.9076339097427246,
      "grad_norm": 1.806378960609436,
      "learning_rate": 0.00017143837583235522,
      "loss": 1.0424,
      "step": 6456
    },
    {
      "epoch": 0.9077744973991284,
      "grad_norm": 1.6169887781143188,
      "learning_rate": 0.00017127551146058607,
      "loss": 1.1595,
      "step": 6457
    },
    {
      "epoch": 0.9079150850555321,
      "grad_norm": 1.686741590499878,
      "learning_rate": 0.00017111226190142528,
      "loss": 1.0266,
      "step": 6458
    },
    {
      "epoch": 0.9080556727119359,
      "grad_norm": 1.803505301475525,
      "learning_rate": 0.00017094862803710673,
      "loss": 1.019,
      "step": 6459
    },
    {
      "epoch": 0.9081962603683397,
      "grad_norm": 1.7755167484283447,
      "learning_rate": 0.000170784610751941,
      "loss": 1.0453,
      "step": 6460
    },
    {
      "epoch": 0.9083368480247435,
      "grad_norm": 2.183396577835083,
      "learning_rate": 0.00017062021093231083,
      "loss": 1.1797,
      "step": 6461
    },
    {
      "epoch": 0.9084774356811472,
      "grad_norm": 1.800002932548523,
      "learning_rate": 0.00017045542946666676,
      "loss": 1.1146,
      "step": 6462
    },
    {
      "epoch": 0.9086180233375509,
      "grad_norm": 1.435413122177124,
      "learning_rate": 0.00017029026724552104,
      "loss": 1.1596,
      "step": 6463
    },
    {
      "epoch": 0.9087586109939547,
      "grad_norm": 1.7970134019851685,
      "learning_rate": 0.00017012472516144414,
      "loss": 1.0938,
      "step": 6464
    },
    {
      "epoch": 0.9088991986503585,
      "grad_norm": 1.7299251556396484,
      "learning_rate": 0.00016995880410905918,
      "loss": 1.1357,
      "step": 6465
    },
    {
      "epoch": 0.9090397863067623,
      "grad_norm": 1.965127944946289,
      "learning_rate": 0.00016979250498503736,
      "loss": 1.1147,
      "step": 6466
    },
    {
      "epoch": 0.9091803739631661,
      "grad_norm": 1.747715950012207,
      "learning_rate": 0.00016962582868809318,
      "loss": 1.155,
      "step": 6467
    },
    {
      "epoch": 0.9093209616195698,
      "grad_norm": 1.6939440965652466,
      "learning_rate": 0.00016945877611897894,
      "loss": 1.0453,
      "step": 6468
    },
    {
      "epoch": 0.9094615492759736,
      "grad_norm": 1.6203558444976807,
      "learning_rate": 0.00016929134818048115,
      "loss": 0.9897,
      "step": 6469
    },
    {
      "epoch": 0.9096021369323773,
      "grad_norm": 1.499569296836853,
      "learning_rate": 0.00016912354577741453,
      "loss": 1.0271,
      "step": 6470
    },
    {
      "epoch": 0.9097427245887811,
      "grad_norm": 1.5112069845199585,
      "learning_rate": 0.00016895536981661717,
      "loss": 0.9109,
      "step": 6471
    },
    {
      "epoch": 0.9098833122451849,
      "grad_norm": 1.4448838233947754,
      "learning_rate": 0.00016878682120694626,
      "loss": 1.0678,
      "step": 6472
    },
    {
      "epoch": 0.9100238999015886,
      "grad_norm": 1.3693716526031494,
      "learning_rate": 0.00016861790085927325,
      "loss": 1.152,
      "step": 6473
    },
    {
      "epoch": 0.9101644875579924,
      "grad_norm": 1.5944666862487793,
      "learning_rate": 0.00016844860968647773,
      "loss": 1.0419,
      "step": 6474
    },
    {
      "epoch": 0.9103050752143962,
      "grad_norm": 1.4088836908340454,
      "learning_rate": 0.00016827894860344395,
      "loss": 1.0718,
      "step": 6475
    },
    {
      "epoch": 0.9104456628708,
      "grad_norm": 1.6105141639709473,
      "learning_rate": 0.0001681089185270546,
      "loss": 1.1452,
      "step": 6476
    },
    {
      "epoch": 0.9105862505272038,
      "grad_norm": 1.6385246515274048,
      "learning_rate": 0.0001679385203761873,
      "loss": 1.015,
      "step": 6477
    },
    {
      "epoch": 0.9107268381836074,
      "grad_norm": 1.4777799844741821,
      "learning_rate": 0.00016776775507170835,
      "loss": 0.9971,
      "step": 6478
    },
    {
      "epoch": 0.9108674258400112,
      "grad_norm": 1.528877854347229,
      "learning_rate": 0.00016759662353646795,
      "loss": 1.131,
      "step": 6479
    },
    {
      "epoch": 0.911008013496415,
      "grad_norm": 1.638806700706482,
      "learning_rate": 0.00016742512669529593,
      "loss": 1.1725,
      "step": 6480
    },
    {
      "epoch": 0.9111486011528188,
      "grad_norm": 1.8165045976638794,
      "learning_rate": 0.00016725326547499644,
      "loss": 1.1684,
      "step": 6481
    },
    {
      "epoch": 0.9112891888092226,
      "grad_norm": 1.8175681829452515,
      "learning_rate": 0.0001670810408043422,
      "loss": 1.0095,
      "step": 6482
    },
    {
      "epoch": 0.9114297764656263,
      "grad_norm": 1.5764129161834717,
      "learning_rate": 0.0001669084536140706,
      "loss": 1.0272,
      "step": 6483
    },
    {
      "epoch": 0.9115703641220301,
      "grad_norm": 1.6867483854293823,
      "learning_rate": 0.00016673550483687792,
      "loss": 1.0815,
      "step": 6484
    },
    {
      "epoch": 0.9117109517784339,
      "grad_norm": 1.461264967918396,
      "learning_rate": 0.00016656219540741468,
      "loss": 1.3025,
      "step": 6485
    },
    {
      "epoch": 0.9118515394348377,
      "grad_norm": 1.8908863067626953,
      "learning_rate": 0.00016638852626228042,
      "loss": 0.9165,
      "step": 6486
    },
    {
      "epoch": 0.9119921270912413,
      "grad_norm": 1.4980528354644775,
      "learning_rate": 0.00016621449834001826,
      "loss": 1.0986,
      "step": 6487
    },
    {
      "epoch": 0.9121327147476451,
      "grad_norm": 1.5398097038269043,
      "learning_rate": 0.00016604011258111094,
      "loss": 1.1878,
      "step": 6488
    },
    {
      "epoch": 0.9122733024040489,
      "grad_norm": 1.7722392082214355,
      "learning_rate": 0.00016586536992797466,
      "loss": 1.0674,
      "step": 6489
    },
    {
      "epoch": 0.9124138900604527,
      "grad_norm": 1.5136586427688599,
      "learning_rate": 0.00016569027132495406,
      "loss": 0.9756,
      "step": 6490
    },
    {
      "epoch": 0.9125544777168565,
      "grad_norm": 1.6385295391082764,
      "learning_rate": 0.00016551481771831777,
      "loss": 1.1825,
      "step": 6491
    },
    {
      "epoch": 0.9126950653732602,
      "grad_norm": 1.4922282695770264,
      "learning_rate": 0.0001653390100562529,
      "loss": 1.0636,
      "step": 6492
    },
    {
      "epoch": 0.912835653029664,
      "grad_norm": 1.3083932399749756,
      "learning_rate": 0.0001651628492888599,
      "loss": 1.2061,
      "step": 6493
    },
    {
      "epoch": 0.9129762406860678,
      "grad_norm": 1.4748148918151855,
      "learning_rate": 0.0001649863363681475,
      "loss": 0.9789,
      "step": 6494
    },
    {
      "epoch": 0.9131168283424715,
      "grad_norm": 1.5133260488510132,
      "learning_rate": 0.00016480947224802722,
      "loss": 1.1881,
      "step": 6495
    },
    {
      "epoch": 0.9132574159988753,
      "grad_norm": 1.5259737968444824,
      "learning_rate": 0.00016463225788430918,
      "loss": 1.0954,
      "step": 6496
    },
    {
      "epoch": 0.913398003655279,
      "grad_norm": 1.631985068321228,
      "learning_rate": 0.00016445469423469588,
      "loss": 1.1003,
      "step": 6497
    },
    {
      "epoch": 0.9135385913116828,
      "grad_norm": 1.471838116645813,
      "learning_rate": 0.00016427678225877724,
      "loss": 1.0641,
      "step": 6498
    },
    {
      "epoch": 0.9136791789680866,
      "grad_norm": 1.5136257410049438,
      "learning_rate": 0.00016409852291802594,
      "loss": 1.2235,
      "step": 6499
    },
    {
      "epoch": 0.9138197666244904,
      "grad_norm": 1.5724480152130127,
      "learning_rate": 0.00016391991717579186,
      "loss": 1.1526,
      "step": 6500
    },
    {
      "epoch": 0.9138197666244904,
      "eval_loss": 1.167434811592102,
      "eval_runtime": 771.8258,
      "eval_samples_per_second": 16.385,
      "eval_steps_per_second": 8.192,
      "step": 6500
    },
    {
      "epoch": 0.9139603542808942,
      "grad_norm": 1.565011978149414,
      "learning_rate": 0.00016374096599729678,
      "loss": 1.0654,
      "step": 6501
    },
    {
      "epoch": 0.9141009419372979,
      "grad_norm": 1.6404300928115845,
      "learning_rate": 0.00016356167034962935,
      "loss": 1.1936,
      "step": 6502
    },
    {
      "epoch": 0.9142415295937016,
      "grad_norm": 1.4811471700668335,
      "learning_rate": 0.00016338203120173982,
      "loss": 1.0881,
      "step": 6503
    },
    {
      "epoch": 0.9143821172501054,
      "grad_norm": 1.9758319854736328,
      "learning_rate": 0.00016320204952443472,
      "loss": 1.199,
      "step": 6504
    },
    {
      "epoch": 0.9145227049065092,
      "grad_norm": 1.5140650272369385,
      "learning_rate": 0.0001630217262903719,
      "loss": 1.1846,
      "step": 6505
    },
    {
      "epoch": 0.914663292562913,
      "grad_norm": 1.5420124530792236,
      "learning_rate": 0.00016284106247405443,
      "loss": 1.1598,
      "step": 6506
    },
    {
      "epoch": 0.9148038802193167,
      "grad_norm": 1.5033739805221558,
      "learning_rate": 0.000162660059051827,
      "loss": 1.2326,
      "step": 6507
    },
    {
      "epoch": 0.9149444678757205,
      "grad_norm": 1.7205308675765991,
      "learning_rate": 0.0001624787170018685,
      "loss": 1.2471,
      "step": 6508
    },
    {
      "epoch": 0.9150850555321243,
      "grad_norm": 1.3691825866699219,
      "learning_rate": 0.00016229703730418851,
      "loss": 1.0816,
      "step": 6509
    },
    {
      "epoch": 0.9152256431885281,
      "grad_norm": 1.5032117366790771,
      "learning_rate": 0.00016211502094062117,
      "loss": 1.0879,
      "step": 6510
    },
    {
      "epoch": 0.9153662308449318,
      "grad_norm": 1.8461569547653198,
      "learning_rate": 0.00016193266889482,
      "loss": 1.0341,
      "step": 6511
    },
    {
      "epoch": 0.9155068185013355,
      "grad_norm": 1.7466171979904175,
      "learning_rate": 0.00016174998215225258,
      "loss": 1.0044,
      "step": 6512
    },
    {
      "epoch": 0.9156474061577393,
      "grad_norm": 1.727466106414795,
      "learning_rate": 0.00016156696170019545,
      "loss": 0.9111,
      "step": 6513
    },
    {
      "epoch": 0.9157879938141431,
      "grad_norm": 1.4870117902755737,
      "learning_rate": 0.0001613836085277281,
      "loss": 1.1949,
      "step": 6514
    },
    {
      "epoch": 0.9159285814705469,
      "grad_norm": 2.302819013595581,
      "learning_rate": 0.00016119992362572874,
      "loss": 1.0363,
      "step": 6515
    },
    {
      "epoch": 0.9160691691269507,
      "grad_norm": 1.4231330156326294,
      "learning_rate": 0.00016101590798686815,
      "loss": 1.2416,
      "step": 6516
    },
    {
      "epoch": 0.9162097567833544,
      "grad_norm": 1.4687601327896118,
      "learning_rate": 0.00016083156260560398,
      "loss": 1.1473,
      "step": 6517
    },
    {
      "epoch": 0.9163503444397582,
      "grad_norm": 1.524886965751648,
      "learning_rate": 0.00016064688847817639,
      "loss": 1.2734,
      "step": 6518
    },
    {
      "epoch": 0.916490932096162,
      "grad_norm": 1.5301485061645508,
      "learning_rate": 0.00016046188660260205,
      "loss": 1.046,
      "step": 6519
    },
    {
      "epoch": 0.9166315197525657,
      "grad_norm": 1.6526577472686768,
      "learning_rate": 0.00016027655797866877,
      "loss": 1.1024,
      "step": 6520
    },
    {
      "epoch": 0.9167721074089695,
      "grad_norm": 1.5091158151626587,
      "learning_rate": 0.00016009090360793023,
      "loss": 0.9972,
      "step": 6521
    },
    {
      "epoch": 0.9169126950653732,
      "grad_norm": 1.741461992263794,
      "learning_rate": 0.00015990492449370047,
      "loss": 1.2161,
      "step": 6522
    },
    {
      "epoch": 0.917053282721777,
      "grad_norm": 1.5016896724700928,
      "learning_rate": 0.00015971862164104855,
      "loss": 1.1306,
      "step": 6523
    },
    {
      "epoch": 0.9171938703781808,
      "grad_norm": 1.2519829273223877,
      "learning_rate": 0.0001595319960567932,
      "loss": 1.1693,
      "step": 6524
    },
    {
      "epoch": 0.9173344580345846,
      "grad_norm": 2.0255234241485596,
      "learning_rate": 0.00015934504874949665,
      "loss": 1.0875,
      "step": 6525
    },
    {
      "epoch": 0.9174750456909884,
      "grad_norm": 1.5795334577560425,
      "learning_rate": 0.0001591577807294609,
      "loss": 0.9242,
      "step": 6526
    },
    {
      "epoch": 0.917615633347392,
      "grad_norm": 1.7880427837371826,
      "learning_rate": 0.00015897019300872,
      "loss": 0.9739,
      "step": 6527
    },
    {
      "epoch": 0.9177562210037958,
      "grad_norm": 1.5193064212799072,
      "learning_rate": 0.0001587822866010364,
      "loss": 1.2804,
      "step": 6528
    },
    {
      "epoch": 0.9178968086601996,
      "grad_norm": 1.4684398174285889,
      "learning_rate": 0.00015859406252189464,
      "loss": 1.1127,
      "step": 6529
    },
    {
      "epoch": 0.9180373963166034,
      "grad_norm": 1.530775785446167,
      "learning_rate": 0.00015840552178849598,
      "loss": 1.1877,
      "step": 6530
    },
    {
      "epoch": 0.9181779839730072,
      "grad_norm": 1.643715739250183,
      "learning_rate": 0.000158216665419753,
      "loss": 1.1144,
      "step": 6531
    },
    {
      "epoch": 0.9183185716294109,
      "grad_norm": 1.5237538814544678,
      "learning_rate": 0.00015802749443628413,
      "loss": 1.1303,
      "step": 6532
    },
    {
      "epoch": 0.9184591592858147,
      "grad_norm": 1.4913338422775269,
      "learning_rate": 0.0001578380098604075,
      "loss": 1.2558,
      "step": 6533
    },
    {
      "epoch": 0.9185997469422185,
      "grad_norm": 1.3497978448867798,
      "learning_rate": 0.000157648212716137,
      "loss": 1.1094,
      "step": 6534
    },
    {
      "epoch": 0.9187403345986223,
      "grad_norm": 1.5759609937667847,
      "learning_rate": 0.00015745810402917457,
      "loss": 1.0758,
      "step": 6535
    },
    {
      "epoch": 0.918880922255026,
      "grad_norm": 1.4091315269470215,
      "learning_rate": 0.00015726768482690647,
      "loss": 1.0612,
      "step": 6536
    },
    {
      "epoch": 0.9190215099114297,
      "grad_norm": 1.616421103477478,
      "learning_rate": 0.00015707695613839694,
      "loss": 1.187,
      "step": 6537
    },
    {
      "epoch": 0.9191620975678335,
      "grad_norm": 1.3584909439086914,
      "learning_rate": 0.00015688591899438263,
      "loss": 1.1635,
      "step": 6538
    },
    {
      "epoch": 0.9193026852242373,
      "grad_norm": 1.541855812072754,
      "learning_rate": 0.00015669457442726726,
      "loss": 1.1971,
      "step": 6539
    },
    {
      "epoch": 0.9194432728806411,
      "grad_norm": 1.5813138484954834,
      "learning_rate": 0.00015650292347111578,
      "loss": 1.1337,
      "step": 6540
    },
    {
      "epoch": 0.9195838605370449,
      "grad_norm": 1.5295896530151367,
      "learning_rate": 0.0001563109671616491,
      "loss": 1.0732,
      "step": 6541
    },
    {
      "epoch": 0.9197244481934486,
      "grad_norm": 1.4889997243881226,
      "learning_rate": 0.00015611870653623838,
      "loss": 1.1719,
      "step": 6542
    },
    {
      "epoch": 0.9198650358498524,
      "grad_norm": 1.660107970237732,
      "learning_rate": 0.00015592614263389888,
      "loss": 1.1107,
      "step": 6543
    },
    {
      "epoch": 0.9200056235062561,
      "grad_norm": 1.5469095706939697,
      "learning_rate": 0.00015573327649528524,
      "loss": 1.1196,
      "step": 6544
    },
    {
      "epoch": 0.9201462111626599,
      "grad_norm": 1.55607271194458,
      "learning_rate": 0.0001555401091626858,
      "loss": 1.0613,
      "step": 6545
    },
    {
      "epoch": 0.9202867988190637,
      "grad_norm": 1.8101212978363037,
      "learning_rate": 0.00015534664168001574,
      "loss": 0.9151,
      "step": 6546
    },
    {
      "epoch": 0.9204273864754674,
      "grad_norm": 1.3947913646697998,
      "learning_rate": 0.00015515287509281295,
      "loss": 1.1339,
      "step": 6547
    },
    {
      "epoch": 0.9205679741318712,
      "grad_norm": 1.6096855401992798,
      "learning_rate": 0.00015495881044823154,
      "loss": 1.1346,
      "step": 6548
    },
    {
      "epoch": 0.920708561788275,
      "grad_norm": 1.7149146795272827,
      "learning_rate": 0.00015476444879503642,
      "loss": 1.1264,
      "step": 6549
    },
    {
      "epoch": 0.9208491494446788,
      "grad_norm": 1.4933573007583618,
      "learning_rate": 0.00015456979118359758,
      "loss": 1.2065,
      "step": 6550
    },
    {
      "epoch": 0.9209897371010826,
      "grad_norm": 1.7156918048858643,
      "learning_rate": 0.00015437483866588452,
      "loss": 1.0646,
      "step": 6551
    },
    {
      "epoch": 0.9211303247574862,
      "grad_norm": 1.5422724485397339,
      "learning_rate": 0.00015417959229546005,
      "loss": 1.1819,
      "step": 6552
    },
    {
      "epoch": 0.92127091241389,
      "grad_norm": 1.5522266626358032,
      "learning_rate": 0.00015398405312747583,
      "loss": 1.1185,
      "step": 6553
    },
    {
      "epoch": 0.9214115000702938,
      "grad_norm": 1.4369760751724243,
      "learning_rate": 0.00015378822221866502,
      "loss": 1.0613,
      "step": 6554
    },
    {
      "epoch": 0.9215520877266976,
      "grad_norm": 1.5522005558013916,
      "learning_rate": 0.0001535921006273379,
      "loss": 1.1211,
      "step": 6555
    },
    {
      "epoch": 0.9216926753831014,
      "grad_norm": 1.336404800415039,
      "learning_rate": 0.00015339568941337545,
      "loss": 1.0075,
      "step": 6556
    },
    {
      "epoch": 0.9218332630395051,
      "grad_norm": 1.903290867805481,
      "learning_rate": 0.000153198989638224,
      "loss": 1.197,
      "step": 6557
    },
    {
      "epoch": 0.9219738506959089,
      "grad_norm": 1.6431081295013428,
      "learning_rate": 0.00015300200236488916,
      "loss": 1.2515,
      "step": 6558
    },
    {
      "epoch": 0.9221144383523127,
      "grad_norm": 1.5804325342178345,
      "learning_rate": 0.00015280472865793034,
      "loss": 1.1575,
      "step": 6559
    },
    {
      "epoch": 0.9222550260087164,
      "grad_norm": 1.5077961683273315,
      "learning_rate": 0.00015260716958345483,
      "loss": 1.24,
      "step": 6560
    },
    {
      "epoch": 0.9223956136651202,
      "grad_norm": 1.7798374891281128,
      "learning_rate": 0.00015240932620911237,
      "loss": 1.1609,
      "step": 6561
    },
    {
      "epoch": 0.9225362013215239,
      "grad_norm": 1.3063669204711914,
      "learning_rate": 0.00015221119960408846,
      "loss": 1.213,
      "step": 6562
    },
    {
      "epoch": 0.9226767889779277,
      "grad_norm": 1.4664530754089355,
      "learning_rate": 0.00015201279083909975,
      "loss": 1.2487,
      "step": 6563
    },
    {
      "epoch": 0.9228173766343315,
      "grad_norm": 1.3663679361343384,
      "learning_rate": 0.0001518141009863881,
      "loss": 1.1797,
      "step": 6564
    },
    {
      "epoch": 0.9229579642907353,
      "grad_norm": 1.561635136604309,
      "learning_rate": 0.00015161513111971344,
      "loss": 1.0801,
      "step": 6565
    },
    {
      "epoch": 0.9230985519471391,
      "grad_norm": 1.457321047782898,
      "learning_rate": 0.0001514158823143497,
      "loss": 1.1099,
      "step": 6566
    },
    {
      "epoch": 0.9232391396035428,
      "grad_norm": 1.4330271482467651,
      "learning_rate": 0.000151216355647078,
      "loss": 1.0556,
      "step": 6567
    },
    {
      "epoch": 0.9233797272599465,
      "grad_norm": 1.6265289783477783,
      "learning_rate": 0.00015101655219618116,
      "loss": 0.9606,
      "step": 6568
    },
    {
      "epoch": 0.9235203149163503,
      "grad_norm": 1.5503175258636475,
      "learning_rate": 0.00015081647304143788,
      "loss": 1.0722,
      "step": 6569
    },
    {
      "epoch": 0.9236609025727541,
      "grad_norm": 1.3504834175109863,
      "learning_rate": 0.00015061611926411641,
      "loss": 1.1927,
      "step": 6570
    },
    {
      "epoch": 0.9238014902291579,
      "grad_norm": 1.5498241186141968,
      "learning_rate": 0.00015041549194696934,
      "loss": 1.2345,
      "step": 6571
    },
    {
      "epoch": 0.9239420778855616,
      "grad_norm": 1.5600383281707764,
      "learning_rate": 0.00015021459217422813,
      "loss": 1.308,
      "step": 6572
    },
    {
      "epoch": 0.9240826655419654,
      "grad_norm": 1.4596388339996338,
      "learning_rate": 0.00015001342103159555,
      "loss": 1.0649,
      "step": 6573
    },
    {
      "epoch": 0.9242232531983692,
      "grad_norm": 1.344085931777954,
      "learning_rate": 0.00014981197960624168,
      "loss": 1.2349,
      "step": 6574
    },
    {
      "epoch": 0.924363840854773,
      "grad_norm": 1.69893479347229,
      "learning_rate": 0.00014961026898679706,
      "loss": 1.1538,
      "step": 6575
    },
    {
      "epoch": 0.9245044285111768,
      "grad_norm": 1.6587278842926025,
      "learning_rate": 0.0001494082902633469,
      "loss": 1.157,
      "step": 6576
    },
    {
      "epoch": 0.9246450161675804,
      "grad_norm": 1.5773556232452393,
      "learning_rate": 0.00014920604452742565,
      "loss": 1.0948,
      "step": 6577
    },
    {
      "epoch": 0.9247856038239842,
      "grad_norm": 1.5464980602264404,
      "learning_rate": 0.00014900353287201006,
      "loss": 1.0033,
      "step": 6578
    },
    {
      "epoch": 0.924926191480388,
      "grad_norm": 1.3299825191497803,
      "learning_rate": 0.0001488007563915145,
      "loss": 1.0703,
      "step": 6579
    },
    {
      "epoch": 0.9250667791367918,
      "grad_norm": 1.4213758707046509,
      "learning_rate": 0.00014859771618178487,
      "loss": 1.1262,
      "step": 6580
    },
    {
      "epoch": 0.9252073667931956,
      "grad_norm": 1.4658987522125244,
      "learning_rate": 0.00014839441334009128,
      "loss": 1.0986,
      "step": 6581
    },
    {
      "epoch": 0.9253479544495993,
      "grad_norm": 1.4134169816970825,
      "learning_rate": 0.00014819084896512382,
      "loss": 1.1078,
      "step": 6582
    },
    {
      "epoch": 0.9254885421060031,
      "grad_norm": 1.6320602893829346,
      "learning_rate": 0.00014798702415698626,
      "loss": 1.1775,
      "step": 6583
    },
    {
      "epoch": 0.9256291297624069,
      "grad_norm": 1.6916264295578003,
      "learning_rate": 0.000147782940017189,
      "loss": 1.2929,
      "step": 6584
    },
    {
      "epoch": 0.9257697174188106,
      "grad_norm": 1.5852468013763428,
      "learning_rate": 0.00014757859764864465,
      "loss": 1.105,
      "step": 6585
    },
    {
      "epoch": 0.9259103050752144,
      "grad_norm": 1.8864388465881348,
      "learning_rate": 0.00014737399815566046,
      "loss": 1.1493,
      "step": 6586
    },
    {
      "epoch": 0.9260508927316181,
      "grad_norm": 1.5588845014572144,
      "learning_rate": 0.00014716914264393434,
      "loss": 1.2515,
      "step": 6587
    },
    {
      "epoch": 0.9261914803880219,
      "grad_norm": 1.6230597496032715,
      "learning_rate": 0.00014696403222054725,
      "loss": 1.129,
      "step": 6588
    },
    {
      "epoch": 0.9263320680444257,
      "grad_norm": 1.2616040706634521,
      "learning_rate": 0.00014675866799395736,
      "loss": 1.0251,
      "step": 6589
    },
    {
      "epoch": 0.9264726557008295,
      "grad_norm": 1.3786712884902954,
      "learning_rate": 0.00014655305107399478,
      "loss": 1.3065,
      "step": 6590
    },
    {
      "epoch": 0.9266132433572333,
      "grad_norm": 1.3810410499572754,
      "learning_rate": 0.0001463471825718558,
      "loss": 1.1031,
      "step": 6591
    },
    {
      "epoch": 0.926753831013637,
      "grad_norm": 1.3754889965057373,
      "learning_rate": 0.00014614106360009527,
      "loss": 1.1832,
      "step": 6592
    },
    {
      "epoch": 0.9268944186700407,
      "grad_norm": 1.7965134382247925,
      "learning_rate": 0.00014593469527262226,
      "loss": 1.0986,
      "step": 6593
    },
    {
      "epoch": 0.9270350063264445,
      "grad_norm": 1.4691957235336304,
      "learning_rate": 0.00014572807870469331,
      "loss": 1.396,
      "step": 6594
    },
    {
      "epoch": 0.9271755939828483,
      "grad_norm": 1.400891900062561,
      "learning_rate": 0.00014552121501290645,
      "loss": 1.1136,
      "step": 6595
    },
    {
      "epoch": 0.9273161816392521,
      "grad_norm": 1.7431964874267578,
      "learning_rate": 0.0001453141053151954,
      "loss": 0.9416,
      "step": 6596
    },
    {
      "epoch": 0.9274567692956558,
      "grad_norm": 1.6872656345367432,
      "learning_rate": 0.00014510675073082285,
      "loss": 1.0275,
      "step": 6597
    },
    {
      "epoch": 0.9275973569520596,
      "grad_norm": 1.9617735147476196,
      "learning_rate": 0.00014489915238037523,
      "loss": 1.2307,
      "step": 6598
    },
    {
      "epoch": 0.9277379446084634,
      "grad_norm": 1.7294666767120361,
      "learning_rate": 0.00014469131138575687,
      "loss": 1.26,
      "step": 6599
    },
    {
      "epoch": 0.9278785322648672,
      "grad_norm": 1.4973591566085815,
      "learning_rate": 0.00014448322887018232,
      "loss": 1.0628,
      "step": 6600
    },
    {
      "epoch": 0.928019119921271,
      "grad_norm": 1.625969409942627,
      "learning_rate": 0.00014427490595817193,
      "loss": 1.1354,
      "step": 6601
    },
    {
      "epoch": 0.9281597075776746,
      "grad_norm": 1.6516152620315552,
      "learning_rate": 0.00014406634377554572,
      "loss": 1.1306,
      "step": 6602
    },
    {
      "epoch": 0.9283002952340784,
      "grad_norm": 1.5144785642623901,
      "learning_rate": 0.00014385754344941565,
      "loss": 1.0695,
      "step": 6603
    },
    {
      "epoch": 0.9284408828904822,
      "grad_norm": 1.4733822345733643,
      "learning_rate": 0.00014364850610818164,
      "loss": 1.2314,
      "step": 6604
    },
    {
      "epoch": 0.928581470546886,
      "grad_norm": 1.5547142028808594,
      "learning_rate": 0.00014343923288152356,
      "loss": 1.0417,
      "step": 6605
    },
    {
      "epoch": 0.9287220582032898,
      "grad_norm": 1.5468695163726807,
      "learning_rate": 0.0001432297249003971,
      "loss": 1.069,
      "step": 6606
    },
    {
      "epoch": 0.9288626458596935,
      "grad_norm": 1.4928470849990845,
      "learning_rate": 0.00014301998329702609,
      "loss": 1.0303,
      "step": 6607
    },
    {
      "epoch": 0.9290032335160973,
      "grad_norm": 1.7649849653244019,
      "learning_rate": 0.00014281000920489655,
      "loss": 0.9794,
      "step": 6608
    },
    {
      "epoch": 0.929143821172501,
      "grad_norm": 1.5291699171066284,
      "learning_rate": 0.00014259980375875132,
      "loss": 1.2354,
      "step": 6609
    },
    {
      "epoch": 0.9292844088289048,
      "grad_norm": 1.7141178846359253,
      "learning_rate": 0.00014238936809458393,
      "loss": 1.2122,
      "step": 6610
    },
    {
      "epoch": 0.9294249964853086,
      "grad_norm": 1.5573865175247192,
      "learning_rate": 0.00014217870334963113,
      "loss": 1.1228,
      "step": 6611
    },
    {
      "epoch": 0.9295655841417123,
      "grad_norm": 1.747048258781433,
      "learning_rate": 0.00014196781066236843,
      "loss": 0.9781,
      "step": 6612
    },
    {
      "epoch": 0.9297061717981161,
      "grad_norm": 1.5511046648025513,
      "learning_rate": 0.00014175669117250247,
      "loss": 0.991,
      "step": 6613
    },
    {
      "epoch": 0.9298467594545199,
      "grad_norm": 1.3327819108963013,
      "learning_rate": 0.00014154534602096646,
      "loss": 1.1789,
      "step": 6614
    },
    {
      "epoch": 0.9299873471109237,
      "grad_norm": 1.430248737335205,
      "learning_rate": 0.00014133377634991266,
      "loss": 1.205,
      "step": 6615
    },
    {
      "epoch": 0.9301279347673275,
      "grad_norm": 1.5649933815002441,
      "learning_rate": 0.0001411219833027064,
      "loss": 1.1923,
      "step": 6616
    },
    {
      "epoch": 0.9302685224237311,
      "grad_norm": 1.447715401649475,
      "learning_rate": 0.00014090996802392045,
      "loss": 1.1548,
      "step": 6617
    },
    {
      "epoch": 0.9304091100801349,
      "grad_norm": 1.311497449874878,
      "learning_rate": 0.00014069773165932908,
      "loss": 1.1417,
      "step": 6618
    },
    {
      "epoch": 0.9305496977365387,
      "grad_norm": 1.5603049993515015,
      "learning_rate": 0.00014048527535590041,
      "loss": 1.1132,
      "step": 6619
    },
    {
      "epoch": 0.9306902853929425,
      "grad_norm": 1.2894965410232544,
      "learning_rate": 0.00014027260026179176,
      "loss": 1.1549,
      "step": 6620
    },
    {
      "epoch": 0.9308308730493463,
      "grad_norm": 1.4634639024734497,
      "learning_rate": 0.00014005970752634266,
      "loss": 1.0684,
      "step": 6621
    },
    {
      "epoch": 0.93097146070575,
      "grad_norm": 1.6356134414672852,
      "learning_rate": 0.0001398465983000689,
      "loss": 1.2746,
      "step": 6622
    },
    {
      "epoch": 0.9311120483621538,
      "grad_norm": 1.5023365020751953,
      "learning_rate": 0.00013963327373465624,
      "loss": 1.0939,
      "step": 6623
    },
    {
      "epoch": 0.9312526360185576,
      "grad_norm": 1.3733617067337036,
      "learning_rate": 0.0001394197349829537,
      "loss": 1.1134,
      "step": 6624
    },
    {
      "epoch": 0.9313932236749614,
      "grad_norm": 1.787007212638855,
      "learning_rate": 0.0001392059831989687,
      "loss": 1.3154,
      "step": 6625
    },
    {
      "epoch": 0.9315338113313651,
      "grad_norm": 1.5656317472457886,
      "learning_rate": 0.00013899201953785945,
      "loss": 1.1828,
      "step": 6626
    },
    {
      "epoch": 0.9316743989877688,
      "grad_norm": 1.618103265762329,
      "learning_rate": 0.00013877784515592886,
      "loss": 1.0501,
      "step": 6627
    },
    {
      "epoch": 0.9318149866441726,
      "grad_norm": 1.372018814086914,
      "learning_rate": 0.00013856346121061893,
      "loss": 1.0461,
      "step": 6628
    },
    {
      "epoch": 0.9319555743005764,
      "grad_norm": 1.6968333721160889,
      "learning_rate": 0.00013834886886050467,
      "loss": 1.0995,
      "step": 6629
    },
    {
      "epoch": 0.9320961619569802,
      "grad_norm": 1.5547258853912354,
      "learning_rate": 0.00013813406926528642,
      "loss": 1.208,
      "step": 6630
    },
    {
      "epoch": 0.932236749613384,
      "grad_norm": 1.3797279596328735,
      "learning_rate": 0.0001379190635857853,
      "loss": 1.2998,
      "step": 6631
    },
    {
      "epoch": 0.9323773372697877,
      "grad_norm": 1.4869781732559204,
      "learning_rate": 0.00013770385298393534,
      "loss": 0.9379,
      "step": 6632
    },
    {
      "epoch": 0.9325179249261915,
      "grad_norm": 1.2871925830841064,
      "learning_rate": 0.00013748843862277895,
      "loss": 1.2386,
      "step": 6633
    },
    {
      "epoch": 0.9326585125825952,
      "grad_norm": 1.5122178792953491,
      "learning_rate": 0.00013727282166645931,
      "loss": 1.1996,
      "step": 6634
    },
    {
      "epoch": 0.932799100238999,
      "grad_norm": 1.6147407293319702,
      "learning_rate": 0.00013705700328021402,
      "loss": 0.9634,
      "step": 6635
    },
    {
      "epoch": 0.9329396878954028,
      "grad_norm": 1.6249438524246216,
      "learning_rate": 0.00013684098463036964,
      "loss": 1.0607,
      "step": 6636
    },
    {
      "epoch": 0.9330802755518065,
      "grad_norm": 1.4856665134429932,
      "learning_rate": 0.00013662476688433548,
      "loss": 1.0575,
      "step": 6637
    },
    {
      "epoch": 0.9332208632082103,
      "grad_norm": 1.5484623908996582,
      "learning_rate": 0.00013640835121059584,
      "loss": 1.2473,
      "step": 6638
    },
    {
      "epoch": 0.9333614508646141,
      "grad_norm": 1.6568489074707031,
      "learning_rate": 0.00013619173877870523,
      "loss": 1.1483,
      "step": 6639
    },
    {
      "epoch": 0.9335020385210179,
      "grad_norm": 1.4713611602783203,
      "learning_rate": 0.00013597493075928147,
      "loss": 1.1147,
      "step": 6640
    },
    {
      "epoch": 0.9336426261774217,
      "grad_norm": 1.565947413444519,
      "learning_rate": 0.00013575792832399924,
      "loss": 1.0538,
      "step": 6641
    },
    {
      "epoch": 0.9337832138338253,
      "grad_norm": 1.6453301906585693,
      "learning_rate": 0.00013554073264558406,
      "loss": 1.0557,
      "step": 6642
    },
    {
      "epoch": 0.9339238014902291,
      "grad_norm": 1.94373619556427,
      "learning_rate": 0.00013532334489780524,
      "loss": 1.0798,
      "step": 6643
    },
    {
      "epoch": 0.9340643891466329,
      "grad_norm": 1.5408059358596802,
      "learning_rate": 0.00013510576625547094,
      "loss": 1.0819,
      "step": 6644
    },
    {
      "epoch": 0.9342049768030367,
      "grad_norm": 2.3572540283203125,
      "learning_rate": 0.00013488799789442043,
      "loss": 1.1222,
      "step": 6645
    },
    {
      "epoch": 0.9343455644594405,
      "grad_norm": 1.3312740325927734,
      "learning_rate": 0.00013467004099151808,
      "loss": 1.2408,
      "step": 6646
    },
    {
      "epoch": 0.9344861521158442,
      "grad_norm": 1.5414916276931763,
      "learning_rate": 0.0001344518967246475,
      "loss": 1.0494,
      "step": 6647
    },
    {
      "epoch": 0.934626739772248,
      "grad_norm": 1.6014783382415771,
      "learning_rate": 0.00013423356627270485,
      "loss": 1.0584,
      "step": 6648
    },
    {
      "epoch": 0.9347673274286518,
      "grad_norm": 1.4233450889587402,
      "learning_rate": 0.00013401505081559234,
      "loss": 1.0724,
      "step": 6649
    },
    {
      "epoch": 0.9349079150850556,
      "grad_norm": 1.4477014541625977,
      "learning_rate": 0.00013379635153421224,
      "loss": 1.1561,
      "step": 6650
    },
    {
      "epoch": 0.9350485027414593,
      "grad_norm": 1.6945290565490723,
      "learning_rate": 0.00013357746961045953,
      "loss": 1.1847,
      "step": 6651
    },
    {
      "epoch": 0.935189090397863,
      "grad_norm": 1.5527747869491577,
      "learning_rate": 0.0001333584062272172,
      "loss": 1.1605,
      "step": 6652
    },
    {
      "epoch": 0.9353296780542668,
      "grad_norm": 1.4748785495758057,
      "learning_rate": 0.00013313916256834849,
      "loss": 0.9056,
      "step": 6653
    },
    {
      "epoch": 0.9354702657106706,
      "grad_norm": 1.4621069431304932,
      "learning_rate": 0.00013291973981869045,
      "loss": 1.0786,
      "step": 6654
    },
    {
      "epoch": 0.9356108533670744,
      "grad_norm": 1.5380610227584839,
      "learning_rate": 0.00013270013916404853,
      "loss": 1.1562,
      "step": 6655
    },
    {
      "epoch": 0.9357514410234782,
      "grad_norm": 1.720798373222351,
      "learning_rate": 0.00013248036179118938,
      "loss": 1.1845,
      "step": 6656
    },
    {
      "epoch": 0.9358920286798819,
      "grad_norm": 2.17158842086792,
      "learning_rate": 0.0001322604088878348,
      "loss": 1.1571,
      "step": 6657
    },
    {
      "epoch": 0.9360326163362856,
      "grad_norm": 1.8797101974487305,
      "learning_rate": 0.00013204028164265504,
      "loss": 1.1654,
      "step": 6658
    },
    {
      "epoch": 0.9361732039926894,
      "grad_norm": 1.5245221853256226,
      "learning_rate": 0.00013181998124526265,
      "loss": 1.2828,
      "step": 6659
    },
    {
      "epoch": 0.9363137916490932,
      "grad_norm": 1.5401314496994019,
      "learning_rate": 0.00013159950888620584,
      "loss": 1.1151,
      "step": 6660
    },
    {
      "epoch": 0.936454379305497,
      "grad_norm": 1.884827971458435,
      "learning_rate": 0.00013137886575696238,
      "loss": 1.0169,
      "step": 6661
    },
    {
      "epoch": 0.9365949669619007,
      "grad_norm": 1.424190878868103,
      "learning_rate": 0.00013115805304993218,
      "loss": 1.2529,
      "step": 6662
    },
    {
      "epoch": 0.9367355546183045,
      "grad_norm": 1.7138316631317139,
      "learning_rate": 0.0001309370719584328,
      "loss": 1.0168,
      "step": 6663
    },
    {
      "epoch": 0.9368761422747083,
      "grad_norm": 1.3895223140716553,
      "learning_rate": 0.0001307159236766906,
      "loss": 1.2038,
      "step": 6664
    },
    {
      "epoch": 0.9370167299311121,
      "grad_norm": 1.601276159286499,
      "learning_rate": 0.00013049460939983614,
      "loss": 1.3602,
      "step": 6665
    },
    {
      "epoch": 0.9371573175875159,
      "grad_norm": 1.5713335275650024,
      "learning_rate": 0.0001302731303238969,
      "loss": 1.0731,
      "step": 6666
    },
    {
      "epoch": 0.9372979052439195,
      "grad_norm": 1.5697047710418701,
      "learning_rate": 0.00013005148764579086,
      "loss": 1.19,
      "step": 6667
    },
    {
      "epoch": 0.9374384929003233,
      "grad_norm": 1.6842886209487915,
      "learning_rate": 0.00012982968256332032,
      "loss": 0.8614,
      "step": 6668
    },
    {
      "epoch": 0.9375790805567271,
      "grad_norm": 1.305751919746399,
      "learning_rate": 0.00012960771627516533,
      "loss": 1.1623,
      "step": 6669
    },
    {
      "epoch": 0.9377196682131309,
      "grad_norm": 1.5143744945526123,
      "learning_rate": 0.0001293855899808764,
      "loss": 1.0051,
      "step": 6670
    },
    {
      "epoch": 0.9378602558695347,
      "grad_norm": 1.5461031198501587,
      "learning_rate": 0.00012916330488086983,
      "loss": 1.2341,
      "step": 6671
    },
    {
      "epoch": 0.9380008435259384,
      "grad_norm": 1.3624789714813232,
      "learning_rate": 0.00012894086217641958,
      "loss": 1.0155,
      "step": 6672
    },
    {
      "epoch": 0.9381414311823422,
      "grad_norm": 1.578914761543274,
      "learning_rate": 0.00012871826306965107,
      "loss": 1.1574,
      "step": 6673
    },
    {
      "epoch": 0.938282018838746,
      "grad_norm": 1.7654705047607422,
      "learning_rate": 0.00012849550876353538,
      "loss": 0.9921,
      "step": 6674
    },
    {
      "epoch": 0.9384226064951497,
      "grad_norm": 1.5753889083862305,
      "learning_rate": 0.0001282726004618822,
      "loss": 0.9205,
      "step": 6675
    },
    {
      "epoch": 0.9385631941515535,
      "grad_norm": 1.5041799545288086,
      "learning_rate": 0.0001280495393693334,
      "loss": 1.1751,
      "step": 6676
    },
    {
      "epoch": 0.9387037818079572,
      "grad_norm": 1.7249274253845215,
      "learning_rate": 0.00012782632669135666,
      "loss": 1.0899,
      "step": 6677
    },
    {
      "epoch": 0.938844369464361,
      "grad_norm": 1.395675539970398,
      "learning_rate": 0.00012760296363423872,
      "loss": 1.1737,
      "step": 6678
    },
    {
      "epoch": 0.9389849571207648,
      "grad_norm": 1.6182572841644287,
      "learning_rate": 0.0001273794514050791,
      "loss": 1.1992,
      "step": 6679
    },
    {
      "epoch": 0.9391255447771686,
      "grad_norm": 1.5974676609039307,
      "learning_rate": 0.00012715579121178356,
      "loss": 1.1035,
      "step": 6680
    },
    {
      "epoch": 0.9392661324335724,
      "grad_norm": 1.3990097045898438,
      "learning_rate": 0.00012693198426305693,
      "loss": 1.1031,
      "step": 6681
    },
    {
      "epoch": 0.939406720089976,
      "grad_norm": 1.401865005493164,
      "learning_rate": 0.00012670803176839822,
      "loss": 1.1418,
      "step": 6682
    },
    {
      "epoch": 0.9395473077463798,
      "grad_norm": 1.7556618452072144,
      "learning_rate": 0.00012648393493809189,
      "loss": 1.2828,
      "step": 6683
    },
    {
      "epoch": 0.9396878954027836,
      "grad_norm": 1.5242141485214233,
      "learning_rate": 0.000126259694983203,
      "loss": 1.1224,
      "step": 6684
    },
    {
      "epoch": 0.9398284830591874,
      "grad_norm": 1.6732149124145508,
      "learning_rate": 0.00012603531311557002,
      "loss": 1.1571,
      "step": 6685
    },
    {
      "epoch": 0.9399690707155912,
      "grad_norm": 1.3888534307479858,
      "learning_rate": 0.00012581079054779827,
      "loss": 1.1518,
      "step": 6686
    },
    {
      "epoch": 0.9401096583719949,
      "grad_norm": 1.739017367362976,
      "learning_rate": 0.00012558612849325349,
      "loss": 0.9912,
      "step": 6687
    },
    {
      "epoch": 0.9402502460283987,
      "grad_norm": 1.523190975189209,
      "learning_rate": 0.00012536132816605536,
      "loss": 1.0989,
      "step": 6688
    },
    {
      "epoch": 0.9403908336848025,
      "grad_norm": 1.4798834323883057,
      "learning_rate": 0.00012513639078107018,
      "loss": 1.1494,
      "step": 6689
    },
    {
      "epoch": 0.9405314213412063,
      "grad_norm": 1.617917537689209,
      "learning_rate": 0.00012491131755390611,
      "loss": 1.0681,
      "step": 6690
    },
    {
      "epoch": 0.94067200899761,
      "grad_norm": 1.6774353981018066,
      "learning_rate": 0.0001246861097009042,
      "loss": 1.157,
      "step": 6691
    },
    {
      "epoch": 0.9408125966540137,
      "grad_norm": 1.741906762123108,
      "learning_rate": 0.0001244607684391338,
      "loss": 0.9898,
      "step": 6692
    },
    {
      "epoch": 0.9409531843104175,
      "grad_norm": 1.9367271661758423,
      "learning_rate": 0.000124235294986385,
      "loss": 1.1331,
      "step": 6693
    },
    {
      "epoch": 0.9410937719668213,
      "grad_norm": 1.3999614715576172,
      "learning_rate": 0.0001240096905611623,
      "loss": 1.1235,
      "step": 6694
    },
    {
      "epoch": 0.9412343596232251,
      "grad_norm": 1.841403603553772,
      "learning_rate": 0.00012378395638267803,
      "loss": 1.0328,
      "step": 6695
    },
    {
      "epoch": 0.9413749472796289,
      "grad_norm": 1.751081109046936,
      "learning_rate": 0.00012355809367084564,
      "loss": 1.159,
      "step": 6696
    },
    {
      "epoch": 0.9415155349360326,
      "grad_norm": 1.5646120309829712,
      "learning_rate": 0.0001233321036462733,
      "loss": 1.1647,
      "step": 6697
    },
    {
      "epoch": 0.9416561225924364,
      "grad_norm": 1.5338282585144043,
      "learning_rate": 0.0001231059875302573,
      "loss": 1.134,
      "step": 6698
    },
    {
      "epoch": 0.9417967102488402,
      "grad_norm": 1.315643548965454,
      "learning_rate": 0.00012287974654477492,
      "loss": 1.0973,
      "step": 6699
    },
    {
      "epoch": 0.9419372979052439,
      "grad_norm": 1.6651527881622314,
      "learning_rate": 0.00012265338191247855,
      "loss": 1.1113,
      "step": 6700
    },
    {
      "epoch": 0.9420778855616477,
      "grad_norm": 1.8094213008880615,
      "learning_rate": 0.00012242689485668931,
      "loss": 1.062,
      "step": 6701
    },
    {
      "epoch": 0.9422184732180514,
      "grad_norm": 1.981522798538208,
      "learning_rate": 0.0001222002866013889,
      "loss": 1.1684,
      "step": 6702
    },
    {
      "epoch": 0.9423590608744552,
      "grad_norm": 1.2999273538589478,
      "learning_rate": 0.00012197355837121471,
      "loss": 1.2341,
      "step": 6703
    },
    {
      "epoch": 0.942499648530859,
      "grad_norm": 1.3006291389465332,
      "learning_rate": 0.00012174671139145227,
      "loss": 1.1893,
      "step": 6704
    },
    {
      "epoch": 0.9426402361872628,
      "grad_norm": 1.3505628108978271,
      "learning_rate": 0.00012151974688802892,
      "loss": 1.2378,
      "step": 6705
    },
    {
      "epoch": 0.9427808238436666,
      "grad_norm": 1.2462129592895508,
      "learning_rate": 0.0001212926660875071,
      "loss": 1.0818,
      "step": 6706
    },
    {
      "epoch": 0.9429214115000703,
      "grad_norm": 1.514076590538025,
      "learning_rate": 0.0001210654702170779,
      "loss": 1.1438,
      "step": 6707
    },
    {
      "epoch": 0.943061999156474,
      "grad_norm": 1.5616083145141602,
      "learning_rate": 0.00012083816050455362,
      "loss": 1.0652,
      "step": 6708
    },
    {
      "epoch": 0.9432025868128778,
      "grad_norm": 1.4772124290466309,
      "learning_rate": 0.00012061073817836293,
      "loss": 1.0475,
      "step": 6709
    },
    {
      "epoch": 0.9433431744692816,
      "grad_norm": 1.386083722114563,
      "learning_rate": 0.00012038320446754194,
      "loss": 1.1696,
      "step": 6710
    },
    {
      "epoch": 0.9434837621256854,
      "grad_norm": 1.3619487285614014,
      "learning_rate": 0.00012015556060172939,
      "loss": 0.9349,
      "step": 6711
    },
    {
      "epoch": 0.9436243497820891,
      "grad_norm": 1.3930156230926514,
      "learning_rate": 0.00011992780781115912,
      "loss": 0.9652,
      "step": 6712
    },
    {
      "epoch": 0.9437649374384929,
      "grad_norm": 1.3860338926315308,
      "learning_rate": 0.0001196999473266536,
      "loss": 1.0328,
      "step": 6713
    },
    {
      "epoch": 0.9439055250948967,
      "grad_norm": 1.63722825050354,
      "learning_rate": 0.00011947198037961735,
      "loss": 1.0565,
      "step": 6714
    },
    {
      "epoch": 0.9440461127513005,
      "grad_norm": 1.602644920349121,
      "learning_rate": 0.00011924390820203025,
      "loss": 1.1891,
      "step": 6715
    },
    {
      "epoch": 0.9441867004077042,
      "grad_norm": 1.686791181564331,
      "learning_rate": 0.00011901573202644079,
      "loss": 1.1316,
      "step": 6716
    },
    {
      "epoch": 0.9443272880641079,
      "grad_norm": 1.4192028045654297,
      "learning_rate": 0.00011878745308595973,
      "loss": 1.194,
      "step": 6717
    },
    {
      "epoch": 0.9444678757205117,
      "grad_norm": 1.519303560256958,
      "learning_rate": 0.00011855907261425265,
      "loss": 1.1193,
      "step": 6718
    },
    {
      "epoch": 0.9446084633769155,
      "grad_norm": 1.6665433645248413,
      "learning_rate": 0.00011833059184553417,
      "loss": 1.234,
      "step": 6719
    },
    {
      "epoch": 0.9447490510333193,
      "grad_norm": 1.4482470750808716,
      "learning_rate": 0.00011810201201456141,
      "loss": 1.0141,
      "step": 6720
    },
    {
      "epoch": 0.9448896386897231,
      "grad_norm": 1.4632521867752075,
      "learning_rate": 0.00011787333435662591,
      "loss": 1.2013,
      "step": 6721
    },
    {
      "epoch": 0.9450302263461268,
      "grad_norm": 1.4546656608581543,
      "learning_rate": 0.00011764456010754849,
      "loss": 1.1087,
      "step": 6722
    },
    {
      "epoch": 0.9451708140025306,
      "grad_norm": 1.361142873764038,
      "learning_rate": 0.0001174156905036718,
      "loss": 0.99,
      "step": 6723
    },
    {
      "epoch": 0.9453114016589343,
      "grad_norm": 1.5417817831039429,
      "learning_rate": 0.00011718672678185387,
      "loss": 1.0269,
      "step": 6724
    },
    {
      "epoch": 0.9454519893153381,
      "grad_norm": 1.8072704076766968,
      "learning_rate": 0.00011695767017946149,
      "loss": 1.0243,
      "step": 6725
    },
    {
      "epoch": 0.9455925769717419,
      "grad_norm": 1.928001046180725,
      "learning_rate": 0.00011672852193436283,
      "loss": 1.119,
      "step": 6726
    },
    {
      "epoch": 0.9457331646281456,
      "grad_norm": 1.8044129610061646,
      "learning_rate": 0.00011649928328492176,
      "loss": 1.1347,
      "step": 6727
    },
    {
      "epoch": 0.9458737522845494,
      "grad_norm": 1.4587112665176392,
      "learning_rate": 0.00011626995546999119,
      "loss": 1.1367,
      "step": 6728
    },
    {
      "epoch": 0.9460143399409532,
      "grad_norm": 1.3911974430084229,
      "learning_rate": 0.00011604053972890483,
      "loss": 1.0427,
      "step": 6729
    },
    {
      "epoch": 0.946154927597357,
      "grad_norm": 1.4361572265625,
      "learning_rate": 0.00011581103730147229,
      "loss": 1.1263,
      "step": 6730
    },
    {
      "epoch": 0.9462955152537608,
      "grad_norm": 1.9030529260635376,
      "learning_rate": 0.00011558144942797149,
      "loss": 1.1815,
      "step": 6731
    },
    {
      "epoch": 0.9464361029101644,
      "grad_norm": 1.3918246030807495,
      "learning_rate": 0.00011535177734914205,
      "loss": 1.1804,
      "step": 6732
    },
    {
      "epoch": 0.9465766905665682,
      "grad_norm": 1.457699179649353,
      "learning_rate": 0.00011512202230617888,
      "loss": 1.143,
      "step": 6733
    },
    {
      "epoch": 0.946717278222972,
      "grad_norm": 1.7398134469985962,
      "learning_rate": 0.00011489218554072456,
      "loss": 1.0719,
      "step": 6734
    },
    {
      "epoch": 0.9468578658793758,
      "grad_norm": 1.648774266242981,
      "learning_rate": 0.00011466226829486433,
      "loss": 1.0147,
      "step": 6735
    },
    {
      "epoch": 0.9469984535357796,
      "grad_norm": 1.5459599494934082,
      "learning_rate": 0.00011443227181111786,
      "loss": 0.982,
      "step": 6736
    },
    {
      "epoch": 0.9471390411921833,
      "grad_norm": 1.779218316078186,
      "learning_rate": 0.00011420219733243266,
      "loss": 1.1484,
      "step": 6737
    },
    {
      "epoch": 0.9472796288485871,
      "grad_norm": 1.5598056316375732,
      "learning_rate": 0.00011397204610217813,
      "loss": 1.0961,
      "step": 6738
    },
    {
      "epoch": 0.9474202165049909,
      "grad_norm": 1.3873672485351562,
      "learning_rate": 0.00011374181936413892,
      "loss": 1.1671,
      "step": 6739
    },
    {
      "epoch": 0.9475608041613947,
      "grad_norm": 1.6494487524032593,
      "learning_rate": 0.00011351151836250677,
      "loss": 1.2616,
      "step": 6740
    },
    {
      "epoch": 0.9477013918177984,
      "grad_norm": 1.636997938156128,
      "learning_rate": 0.00011328114434187538,
      "loss": 1.0333,
      "step": 6741
    },
    {
      "epoch": 0.9478419794742021,
      "grad_norm": 1.4334580898284912,
      "learning_rate": 0.00011305069854723291,
      "loss": 1.1627,
      "step": 6742
    },
    {
      "epoch": 0.9479825671306059,
      "grad_norm": 1.933579444885254,
      "learning_rate": 0.00011282018222395543,
      "loss": 1.1236,
      "step": 6743
    },
    {
      "epoch": 0.9481231547870097,
      "grad_norm": 1.56564462184906,
      "learning_rate": 0.00011258959661780029,
      "loss": 1.0975,
      "step": 6744
    },
    {
      "epoch": 0.9482637424434135,
      "grad_norm": 1.415169596672058,
      "learning_rate": 0.00011235894297489875,
      "loss": 1.1032,
      "step": 6745
    },
    {
      "epoch": 0.9484043300998173,
      "grad_norm": 1.733755111694336,
      "learning_rate": 0.0001121282225417501,
      "loss": 1.2372,
      "step": 6746
    },
    {
      "epoch": 0.948544917756221,
      "grad_norm": 1.534226894378662,
      "learning_rate": 0.00011189743656521519,
      "loss": 1.0915,
      "step": 6747
    },
    {
      "epoch": 0.9486855054126248,
      "grad_norm": 1.3613048791885376,
      "learning_rate": 0.00011166658629250784,
      "loss": 1.1283,
      "step": 6748
    },
    {
      "epoch": 0.9488260930690285,
      "grad_norm": 1.3693784475326538,
      "learning_rate": 0.00011143567297119024,
      "loss": 1.1792,
      "step": 6749
    },
    {
      "epoch": 0.9489666807254323,
      "grad_norm": 1.5162739753723145,
      "learning_rate": 0.00011120469784916498,
      "loss": 1.1664,
      "step": 6750
    },
    {
      "epoch": 0.9491072683818361,
      "grad_norm": 1.4609289169311523,
      "learning_rate": 0.00011097366217466876,
      "loss": 1.0354,
      "step": 6751
    },
    {
      "epoch": 0.9492478560382398,
      "grad_norm": 1.3869426250457764,
      "learning_rate": 0.00011074256719626559,
      "loss": 1.1956,
      "step": 6752
    },
    {
      "epoch": 0.9493884436946436,
      "grad_norm": 1.6166478395462036,
      "learning_rate": 0.0001105114141628395,
      "loss": 1.102,
      "step": 6753
    },
    {
      "epoch": 0.9495290313510474,
      "grad_norm": 1.5187437534332275,
      "learning_rate": 0.00011028020432358861,
      "loss": 1.1389,
      "step": 6754
    },
    {
      "epoch": 0.9496696190074512,
      "grad_norm": 1.6310714483261108,
      "learning_rate": 0.0001100489389280185,
      "loss": 1.1234,
      "step": 6755
    },
    {
      "epoch": 0.949810206663855,
      "grad_norm": 2.0720696449279785,
      "learning_rate": 0.00010981761922593403,
      "loss": 1.1317,
      "step": 6756
    },
    {
      "epoch": 0.9499507943202586,
      "grad_norm": 1.4006198644638062,
      "learning_rate": 0.00010958624646743399,
      "loss": 1.0419,
      "step": 6757
    },
    {
      "epoch": 0.9500913819766624,
      "grad_norm": 1.4844857454299927,
      "learning_rate": 0.00010935482190290444,
      "loss": 1.2115,
      "step": 6758
    },
    {
      "epoch": 0.9502319696330662,
      "grad_norm": 1.5137664079666138,
      "learning_rate": 0.00010912334678301043,
      "loss": 1.2168,
      "step": 6759
    },
    {
      "epoch": 0.95037255728947,
      "grad_norm": 1.4789561033248901,
      "learning_rate": 0.00010889182235869098,
      "loss": 1.1862,
      "step": 6760
    },
    {
      "epoch": 0.9505131449458738,
      "grad_norm": 1.3465934991836548,
      "learning_rate": 0.00010866024988115077,
      "loss": 0.9934,
      "step": 6761
    },
    {
      "epoch": 0.9506537326022775,
      "grad_norm": 1.4727259874343872,
      "learning_rate": 0.0001084286306018552,
      "loss": 0.9574,
      "step": 6762
    },
    {
      "epoch": 0.9507943202586813,
      "grad_norm": 1.6159754991531372,
      "learning_rate": 0.00010819696577252203,
      "loss": 1.0951,
      "step": 6763
    },
    {
      "epoch": 0.9509349079150851,
      "grad_norm": 1.6645151376724243,
      "learning_rate": 0.0001079652566451149,
      "loss": 1.2014,
      "step": 6764
    },
    {
      "epoch": 0.9510754955714888,
      "grad_norm": 1.8559683561325073,
      "learning_rate": 0.00010773350447183717,
      "loss": 1.1769,
      "step": 6765
    },
    {
      "epoch": 0.9512160832278926,
      "grad_norm": 1.3991363048553467,
      "learning_rate": 0.00010750171050512531,
      "loss": 1.1729,
      "step": 6766
    },
    {
      "epoch": 0.9513566708842963,
      "grad_norm": 1.3634390830993652,
      "learning_rate": 0.00010726987599764067,
      "loss": 0.9861,
      "step": 6767
    },
    {
      "epoch": 0.9514972585407001,
      "grad_norm": 1.6229690313339233,
      "learning_rate": 0.00010703800220226455,
      "loss": 1.0437,
      "step": 6768
    },
    {
      "epoch": 0.9516378461971039,
      "grad_norm": 1.5484777688980103,
      "learning_rate": 0.00010680609037208959,
      "loss": 0.799,
      "step": 6769
    },
    {
      "epoch": 0.9517784338535077,
      "grad_norm": 1.2131210565567017,
      "learning_rate": 0.00010657414176041523,
      "loss": 1.2553,
      "step": 6770
    },
    {
      "epoch": 0.9519190215099115,
      "grad_norm": 1.4492294788360596,
      "learning_rate": 0.00010634215762073896,
      "loss": 1.0768,
      "step": 6771
    },
    {
      "epoch": 0.9520596091663152,
      "grad_norm": 1.8306665420532227,
      "learning_rate": 0.00010611013920675001,
      "loss": 1.1515,
      "step": 6772
    },
    {
      "epoch": 0.952200196822719,
      "grad_norm": 1.5225064754486084,
      "learning_rate": 0.00010587808777232324,
      "loss": 1.0946,
      "step": 6773
    },
    {
      "epoch": 0.9523407844791227,
      "grad_norm": 1.6275482177734375,
      "learning_rate": 0.0001056460045715124,
      "loss": 1.0644,
      "step": 6774
    },
    {
      "epoch": 0.9524813721355265,
      "grad_norm": 1.4119380712509155,
      "learning_rate": 0.00010541389085854183,
      "loss": 1.2653,
      "step": 6775
    },
    {
      "epoch": 0.9526219597919303,
      "grad_norm": 1.3208789825439453,
      "learning_rate": 0.0001051817478878016,
      "loss": 1.0682,
      "step": 6776
    },
    {
      "epoch": 0.952762547448334,
      "grad_norm": 1.4160929918289185,
      "learning_rate": 0.0001049495769138396,
      "loss": 1.1407,
      "step": 6777
    },
    {
      "epoch": 0.9529031351047378,
      "grad_norm": 1.9084817171096802,
      "learning_rate": 0.0001047173791913551,
      "loss": 1.0365,
      "step": 6778
    },
    {
      "epoch": 0.9530437227611416,
      "grad_norm": 1.748084545135498,
      "learning_rate": 0.00010448515597519211,
      "loss": 1.0539,
      "step": 6779
    },
    {
      "epoch": 0.9531843104175454,
      "grad_norm": 1.683795690536499,
      "learning_rate": 0.00010425290852033167,
      "loss": 1.19,
      "step": 6780
    },
    {
      "epoch": 0.9533248980739492,
      "grad_norm": 1.513442873954773,
      "learning_rate": 0.00010402063808188691,
      "loss": 1.1002,
      "step": 6781
    },
    {
      "epoch": 0.9534654857303528,
      "grad_norm": 1.293067216873169,
      "learning_rate": 0.00010378834591509461,
      "loss": 1.0218,
      "step": 6782
    },
    {
      "epoch": 0.9536060733867566,
      "grad_norm": 1.3904697895050049,
      "learning_rate": 0.00010355603327530868,
      "loss": 1.0566,
      "step": 6783
    },
    {
      "epoch": 0.9537466610431604,
      "grad_norm": 1.8799470663070679,
      "learning_rate": 0.0001033237014179939,
      "loss": 1.1368,
      "step": 6784
    },
    {
      "epoch": 0.9538872486995642,
      "grad_norm": 1.3664745092391968,
      "learning_rate": 0.00010309135159871956,
      "loss": 1.1105,
      "step": 6785
    },
    {
      "epoch": 0.954027836355968,
      "grad_norm": 1.4892727136611938,
      "learning_rate": 0.00010285898507315092,
      "loss": 1.0465,
      "step": 6786
    },
    {
      "epoch": 0.9541684240123717,
      "grad_norm": 1.4610118865966797,
      "learning_rate": 0.00010262660309704437,
      "loss": 1.1689,
      "step": 6787
    },
    {
      "epoch": 0.9543090116687755,
      "grad_norm": 1.3492438793182373,
      "learning_rate": 0.00010239420692623898,
      "loss": 1.3101,
      "step": 6788
    },
    {
      "epoch": 0.9544495993251793,
      "grad_norm": 1.3992981910705566,
      "learning_rate": 0.00010216179781665152,
      "loss": 1.1512,
      "step": 6789
    },
    {
      "epoch": 0.954590186981583,
      "grad_norm": 1.6134124994277954,
      "learning_rate": 0.00010192937702426822,
      "loss": 1.1618,
      "step": 6790
    },
    {
      "epoch": 0.9547307746379868,
      "grad_norm": 1.699350118637085,
      "learning_rate": 0.00010169694580513802,
      "loss": 1.1291,
      "step": 6791
    },
    {
      "epoch": 0.9548713622943905,
      "grad_norm": 1.4614404439926147,
      "learning_rate": 0.00010146450541536665,
      "loss": 1.2838,
      "step": 6792
    },
    {
      "epoch": 0.9550119499507943,
      "grad_norm": 1.4564318656921387,
      "learning_rate": 0.00010123205711110986,
      "loss": 0.9897,
      "step": 6793
    },
    {
      "epoch": 0.9551525376071981,
      "grad_norm": 1.5704106092453003,
      "learning_rate": 0.00010099960214856511,
      "loss": 1.1676,
      "step": 6794
    },
    {
      "epoch": 0.9552931252636019,
      "grad_norm": 1.5085735321044922,
      "learning_rate": 0.00010076714178396664,
      "loss": 1.1776,
      "step": 6795
    },
    {
      "epoch": 0.9554337129200057,
      "grad_norm": 1.6291038990020752,
      "learning_rate": 0.00010053467727357759,
      "loss": 1.0049,
      "step": 6796
    },
    {
      "epoch": 0.9555743005764094,
      "grad_norm": 1.2386654615402222,
      "learning_rate": 0.00010030220987368357,
      "loss": 1.1165,
      "step": 6797
    },
    {
      "epoch": 0.9557148882328131,
      "grad_norm": 1.6076996326446533,
      "learning_rate": 0.000100069740840586,
      "loss": 1.1212,
      "step": 6798
    },
    {
      "epoch": 0.9558554758892169,
      "grad_norm": 1.4445931911468506,
      "learning_rate": 9.983727143059432e-05,
      "loss": 1.2868,
      "step": 6799
    },
    {
      "epoch": 0.9559960635456207,
      "grad_norm": 1.400575041770935,
      "learning_rate": 9.96048029000212e-05,
      "loss": 1.0049,
      "step": 6800
    },
    {
      "epoch": 0.9561366512020245,
      "grad_norm": 1.451474905014038,
      "learning_rate": 9.937233650517393e-05,
      "loss": 1.1804,
      "step": 6801
    },
    {
      "epoch": 0.9562772388584282,
      "grad_norm": 1.3411791324615479,
      "learning_rate": 9.913987350234794e-05,
      "loss": 0.939,
      "step": 6802
    },
    {
      "epoch": 0.956417826514832,
      "grad_norm": 1.490064024925232,
      "learning_rate": 9.890741514782086e-05,
      "loss": 1.0484,
      "step": 6803
    },
    {
      "epoch": 0.9565584141712358,
      "grad_norm": 1.6005958318710327,
      "learning_rate": 9.867496269784502e-05,
      "loss": 1.2615,
      "step": 6804
    },
    {
      "epoch": 0.9566990018276396,
      "grad_norm": 1.4992308616638184,
      "learning_rate": 9.844251740864084e-05,
      "loss": 1.1202,
      "step": 6805
    },
    {
      "epoch": 0.9568395894840434,
      "grad_norm": 1.7090251445770264,
      "learning_rate": 9.821008053639022e-05,
      "loss": 1.1586,
      "step": 6806
    },
    {
      "epoch": 0.956980177140447,
      "grad_norm": 1.6370277404785156,
      "learning_rate": 9.797765333722884e-05,
      "loss": 1.1541,
      "step": 6807
    },
    {
      "epoch": 0.9571207647968508,
      "grad_norm": 1.8294792175292969,
      "learning_rate": 9.774523706724124e-05,
      "loss": 1.2044,
      "step": 6808
    },
    {
      "epoch": 0.9572613524532546,
      "grad_norm": 1.8806610107421875,
      "learning_rate": 9.751283298245226e-05,
      "loss": 1.35,
      "step": 6809
    },
    {
      "epoch": 0.9574019401096584,
      "grad_norm": 1.533388376235962,
      "learning_rate": 9.728044233882063e-05,
      "loss": 1.0307,
      "step": 6810
    },
    {
      "epoch": 0.9575425277660622,
      "grad_norm": 1.2972668409347534,
      "learning_rate": 9.704806639223292e-05,
      "loss": 1.1311,
      "step": 6811
    },
    {
      "epoch": 0.9576831154224659,
      "grad_norm": 1.7908935546875,
      "learning_rate": 9.681570639849611e-05,
      "loss": 1.1479,
      "step": 6812
    },
    {
      "epoch": 0.9578237030788697,
      "grad_norm": 1.4883158206939697,
      "learning_rate": 9.658336361333095e-05,
      "loss": 1.2812,
      "step": 6813
    },
    {
      "epoch": 0.9579642907352734,
      "grad_norm": 1.5619202852249146,
      "learning_rate": 9.635103929236525e-05,
      "loss": 1.1112,
      "step": 6814
    },
    {
      "epoch": 0.9581048783916772,
      "grad_norm": 1.6798022985458374,
      "learning_rate": 9.611873469112695e-05,
      "loss": 1.0436,
      "step": 6815
    },
    {
      "epoch": 0.958245466048081,
      "grad_norm": 1.840875267982483,
      "learning_rate": 9.588645106503749e-05,
      "loss": 1.1965,
      "step": 6816
    },
    {
      "epoch": 0.9583860537044847,
      "grad_norm": 1.2950009107589722,
      "learning_rate": 9.565418966940509e-05,
      "loss": 1.1705,
      "step": 6817
    },
    {
      "epoch": 0.9585266413608885,
      "grad_norm": 1.6205487251281738,
      "learning_rate": 9.542195175941716e-05,
      "loss": 0.9348,
      "step": 6818
    },
    {
      "epoch": 0.9586672290172923,
      "grad_norm": 1.4711651802062988,
      "learning_rate": 9.518973859013522e-05,
      "loss": 1.0652,
      "step": 6819
    },
    {
      "epoch": 0.9588078166736961,
      "grad_norm": 1.694783329963684,
      "learning_rate": 9.495755141648659e-05,
      "loss": 1.1628,
      "step": 6820
    },
    {
      "epoch": 0.9589484043300999,
      "grad_norm": 1.9514949321746826,
      "learning_rate": 9.472539149325769e-05,
      "loss": 1.1664,
      "step": 6821
    },
    {
      "epoch": 0.9590889919865035,
      "grad_norm": 1.5163946151733398,
      "learning_rate": 9.44932600750883e-05,
      "loss": 1.2432,
      "step": 6822
    },
    {
      "epoch": 0.9592295796429073,
      "grad_norm": 1.4111179113388062,
      "learning_rate": 9.426115841646386e-05,
      "loss": 1.1603,
      "step": 6823
    },
    {
      "epoch": 0.9593701672993111,
      "grad_norm": 1.5625234842300415,
      "learning_rate": 9.402908777170905e-05,
      "loss": 1.1082,
      "step": 6824
    },
    {
      "epoch": 0.9595107549557149,
      "grad_norm": 1.6442183256149292,
      "learning_rate": 9.379704939498116e-05,
      "loss": 1.0317,
      "step": 6825
    },
    {
      "epoch": 0.9596513426121187,
      "grad_norm": 1.4944127798080444,
      "learning_rate": 9.35650445402623e-05,
      "loss": 1.0213,
      "step": 6826
    },
    {
      "epoch": 0.9597919302685224,
      "grad_norm": 1.3639795780181885,
      "learning_rate": 9.333307446135452e-05,
      "loss": 0.9715,
      "step": 6827
    },
    {
      "epoch": 0.9599325179249262,
      "grad_norm": 1.783484697341919,
      "learning_rate": 9.310114041187144e-05,
      "loss": 1.0495,
      "step": 6828
    },
    {
      "epoch": 0.96007310558133,
      "grad_norm": 1.6898469924926758,
      "learning_rate": 9.286924364523156e-05,
      "loss": 1.0872,
      "step": 6829
    },
    {
      "epoch": 0.9602136932377338,
      "grad_norm": 1.5557194948196411,
      "learning_rate": 9.263738541465245e-05,
      "loss": 1.218,
      "step": 6830
    },
    {
      "epoch": 0.9603542808941375,
      "grad_norm": 1.5620765686035156,
      "learning_rate": 9.240556697314326e-05,
      "loss": 0.8775,
      "step": 6831
    },
    {
      "epoch": 0.9604948685505412,
      "grad_norm": 1.8593417406082153,
      "learning_rate": 9.217378957349811e-05,
      "loss": 1.1138,
      "step": 6832
    },
    {
      "epoch": 0.960635456206945,
      "grad_norm": 1.6951953172683716,
      "learning_rate": 9.194205446828928e-05,
      "loss": 1.0636,
      "step": 6833
    },
    {
      "epoch": 0.9607760438633488,
      "grad_norm": 1.5085084438323975,
      "learning_rate": 9.171036290986053e-05,
      "loss": 1.0973,
      "step": 6834
    },
    {
      "epoch": 0.9609166315197526,
      "grad_norm": 1.6766443252563477,
      "learning_rate": 9.147871615032025e-05,
      "loss": 1.1821,
      "step": 6835
    },
    {
      "epoch": 0.9610572191761564,
      "grad_norm": 1.3871210813522339,
      "learning_rate": 9.124711544153494e-05,
      "loss": 0.9911,
      "step": 6836
    },
    {
      "epoch": 0.9611978068325601,
      "grad_norm": 1.5403169393539429,
      "learning_rate": 9.10155620351215e-05,
      "loss": 1.1222,
      "step": 6837
    },
    {
      "epoch": 0.9613383944889639,
      "grad_norm": 1.278633952140808,
      "learning_rate": 9.078405718244243e-05,
      "loss": 1.1022,
      "step": 6838
    },
    {
      "epoch": 0.9614789821453676,
      "grad_norm": 1.508164882659912,
      "learning_rate": 9.055260213459664e-05,
      "loss": 1.047,
      "step": 6839
    },
    {
      "epoch": 0.9616195698017714,
      "grad_norm": 1.6084500551223755,
      "learning_rate": 9.032119814241454e-05,
      "loss": 1.2206,
      "step": 6840
    },
    {
      "epoch": 0.9617601574581752,
      "grad_norm": 1.361277461051941,
      "learning_rate": 9.008984645645046e-05,
      "loss": 1.1275,
      "step": 6841
    },
    {
      "epoch": 0.9619007451145789,
      "grad_norm": 1.4482784271240234,
      "learning_rate": 8.985854832697609e-05,
      "loss": 1.028,
      "step": 6842
    },
    {
      "epoch": 0.9620413327709827,
      "grad_norm": 1.5694421529769897,
      "learning_rate": 8.962730500397366e-05,
      "loss": 1.2192,
      "step": 6843
    },
    {
      "epoch": 0.9621819204273865,
      "grad_norm": 1.5783997774124146,
      "learning_rate": 8.93961177371294e-05,
      "loss": 1.0761,
      "step": 6844
    },
    {
      "epoch": 0.9623225080837903,
      "grad_norm": 1.3813986778259277,
      "learning_rate": 8.916498777582587e-05,
      "loss": 1.0865,
      "step": 6845
    },
    {
      "epoch": 0.9624630957401941,
      "grad_norm": 1.3125511407852173,
      "learning_rate": 8.893391636913721e-05,
      "loss": 1.1816,
      "step": 6846
    },
    {
      "epoch": 0.9626036833965977,
      "grad_norm": 1.650430679321289,
      "learning_rate": 8.87029047658199e-05,
      "loss": 1.032,
      "step": 6847
    },
    {
      "epoch": 0.9627442710530015,
      "grad_norm": 1.5131962299346924,
      "learning_rate": 8.847195421430786e-05,
      "loss": 1.2179,
      "step": 6848
    },
    {
      "epoch": 0.9628848587094053,
      "grad_norm": 1.5433132648468018,
      "learning_rate": 8.824106596270497e-05,
      "loss": 1.2124,
      "step": 6849
    },
    {
      "epoch": 0.9630254463658091,
      "grad_norm": 1.490655541419983,
      "learning_rate": 8.801024125877841e-05,
      "loss": 1.0916,
      "step": 6850
    },
    {
      "epoch": 0.9631660340222129,
      "grad_norm": 1.5451295375823975,
      "learning_rate": 8.77794813499519e-05,
      "loss": 1.0943,
      "step": 6851
    },
    {
      "epoch": 0.9633066216786166,
      "grad_norm": 1.7356069087982178,
      "learning_rate": 8.754878748329904e-05,
      "loss": 1.0917,
      "step": 6852
    },
    {
      "epoch": 0.9634472093350204,
      "grad_norm": 1.3452706336975098,
      "learning_rate": 8.731816090553653e-05,
      "loss": 1.1508,
      "step": 6853
    },
    {
      "epoch": 0.9635877969914242,
      "grad_norm": 1.28617525100708,
      "learning_rate": 8.708760286301754e-05,
      "loss": 0.991,
      "step": 6854
    },
    {
      "epoch": 0.963728384647828,
      "grad_norm": 1.8137849569320679,
      "learning_rate": 8.685711460172439e-05,
      "loss": 0.9669,
      "step": 6855
    },
    {
      "epoch": 0.9638689723042317,
      "grad_norm": 1.9882409572601318,
      "learning_rate": 8.66266973672626e-05,
      "loss": 1.0583,
      "step": 6856
    },
    {
      "epoch": 0.9640095599606354,
      "grad_norm": 1.5536166429519653,
      "learning_rate": 8.639635240485446e-05,
      "loss": 1.1049,
      "step": 6857
    },
    {
      "epoch": 0.9641501476170392,
      "grad_norm": 1.3959935903549194,
      "learning_rate": 8.616608095933051e-05,
      "loss": 1.1548,
      "step": 6858
    },
    {
      "epoch": 0.964290735273443,
      "grad_norm": 1.6206402778625488,
      "learning_rate": 8.593588427512477e-05,
      "loss": 1.1107,
      "step": 6859
    },
    {
      "epoch": 0.9644313229298468,
      "grad_norm": 1.468593955039978,
      "learning_rate": 8.570576359626706e-05,
      "loss": 1.0581,
      "step": 6860
    },
    {
      "epoch": 0.9645719105862506,
      "grad_norm": 1.4406644105911255,
      "learning_rate": 8.547572016637642e-05,
      "loss": 1.0414,
      "step": 6861
    },
    {
      "epoch": 0.9647124982426543,
      "grad_norm": 1.5465401411056519,
      "learning_rate": 8.524575522865442e-05,
      "loss": 1.0405,
      "step": 6862
    },
    {
      "epoch": 0.964853085899058,
      "grad_norm": 1.4239354133605957,
      "learning_rate": 8.501587002587865e-05,
      "loss": 1.2309,
      "step": 6863
    },
    {
      "epoch": 0.9649936735554618,
      "grad_norm": 1.5161837339401245,
      "learning_rate": 8.478606580039506e-05,
      "loss": 1.1687,
      "step": 6864
    },
    {
      "epoch": 0.9651342612118656,
      "grad_norm": 1.3569387197494507,
      "learning_rate": 8.455634379411326e-05,
      "loss": 0.9929,
      "step": 6865
    },
    {
      "epoch": 0.9652748488682694,
      "grad_norm": 1.4249228239059448,
      "learning_rate": 8.432670524849723e-05,
      "loss": 1.1258,
      "step": 6866
    },
    {
      "epoch": 0.9654154365246731,
      "grad_norm": 1.3417073488235474,
      "learning_rate": 8.409715140456067e-05,
      "loss": 1.0769,
      "step": 6867
    },
    {
      "epoch": 0.9655560241810769,
      "grad_norm": 1.542946696281433,
      "learning_rate": 8.386768350285933e-05,
      "loss": 1.0551,
      "step": 6868
    },
    {
      "epoch": 0.9656966118374807,
      "grad_norm": 1.3178001642227173,
      "learning_rate": 8.363830278348455e-05,
      "loss": 1.106,
      "step": 6869
    },
    {
      "epoch": 0.9658371994938845,
      "grad_norm": 1.2758049964904785,
      "learning_rate": 8.340901048605647e-05,
      "loss": 1.0802,
      "step": 6870
    },
    {
      "epoch": 0.9659777871502883,
      "grad_norm": 1.503786325454712,
      "learning_rate": 8.317980784971739e-05,
      "loss": 1.0979,
      "step": 6871
    },
    {
      "epoch": 0.9661183748066919,
      "grad_norm": 1.583203673362732,
      "learning_rate": 8.295069611312512e-05,
      "loss": 1.0826,
      "step": 6872
    },
    {
      "epoch": 0.9662589624630957,
      "grad_norm": 1.5261930227279663,
      "learning_rate": 8.27216765144463e-05,
      "loss": 1.2401,
      "step": 6873
    },
    {
      "epoch": 0.9663995501194995,
      "grad_norm": 1.4802663326263428,
      "learning_rate": 8.249275029134921e-05,
      "loss": 1.1166,
      "step": 6874
    },
    {
      "epoch": 0.9665401377759033,
      "grad_norm": 1.9045013189315796,
      "learning_rate": 8.22639186809978e-05,
      "loss": 1.1158,
      "step": 6875
    },
    {
      "epoch": 0.9666807254323071,
      "grad_norm": 1.5785642862319946,
      "learning_rate": 8.203518292004528e-05,
      "loss": 0.9579,
      "step": 6876
    },
    {
      "epoch": 0.9668213130887108,
      "grad_norm": 1.6761881113052368,
      "learning_rate": 8.180654424462579e-05,
      "loss": 1.1258,
      "step": 6877
    },
    {
      "epoch": 0.9669619007451146,
      "grad_norm": 1.5290614366531372,
      "learning_rate": 8.15780038903495e-05,
      "loss": 1.107,
      "step": 6878
    },
    {
      "epoch": 0.9671024884015184,
      "grad_norm": 1.549878716468811,
      "learning_rate": 8.134956309229509e-05,
      "loss": 1.0793,
      "step": 6879
    },
    {
      "epoch": 0.9672430760579221,
      "grad_norm": 1.4726579189300537,
      "learning_rate": 8.112122308500316e-05,
      "loss": 1.0752,
      "step": 6880
    },
    {
      "epoch": 0.9673836637143259,
      "grad_norm": 1.5350258350372314,
      "learning_rate": 8.089298510246986e-05,
      "loss": 1.0799,
      "step": 6881
    },
    {
      "epoch": 0.9675242513707296,
      "grad_norm": 1.4702682495117188,
      "learning_rate": 8.06648503781394e-05,
      "loss": 1.0514,
      "step": 6882
    },
    {
      "epoch": 0.9676648390271334,
      "grad_norm": 1.3353904485702515,
      "learning_rate": 8.043682014489835e-05,
      "loss": 1.1403,
      "step": 6883
    },
    {
      "epoch": 0.9678054266835372,
      "grad_norm": 1.8351938724517822,
      "learning_rate": 8.020889563506905e-05,
      "loss": 1.0527,
      "step": 6884
    },
    {
      "epoch": 0.967946014339941,
      "grad_norm": 1.4033453464508057,
      "learning_rate": 7.998107808040154e-05,
      "loss": 1.012,
      "step": 6885
    },
    {
      "epoch": 0.9680866019963448,
      "grad_norm": 1.4938366413116455,
      "learning_rate": 7.975336871206847e-05,
      "loss": 1.1847,
      "step": 6886
    },
    {
      "epoch": 0.9682271896527485,
      "grad_norm": 1.6872587203979492,
      "learning_rate": 7.952576876065767e-05,
      "loss": 1.1654,
      "step": 6887
    },
    {
      "epoch": 0.9683677773091522,
      "grad_norm": 1.4420396089553833,
      "learning_rate": 7.929827945616566e-05,
      "loss": 1.1214,
      "step": 6888
    },
    {
      "epoch": 0.968508364965556,
      "grad_norm": 1.7147608995437622,
      "learning_rate": 7.907090202799121e-05,
      "loss": 1.1853,
      "step": 6889
    },
    {
      "epoch": 0.9686489526219598,
      "grad_norm": 1.430894374847412,
      "learning_rate": 7.884363770492777e-05,
      "loss": 1.1445,
      "step": 6890
    },
    {
      "epoch": 0.9687895402783636,
      "grad_norm": 1.463761329650879,
      "learning_rate": 7.861648771515855e-05,
      "loss": 1.1388,
      "step": 6891
    },
    {
      "epoch": 0.9689301279347673,
      "grad_norm": 1.5189073085784912,
      "learning_rate": 7.838945328624843e-05,
      "loss": 1.109,
      "step": 6892
    },
    {
      "epoch": 0.9690707155911711,
      "grad_norm": 1.690994381904602,
      "learning_rate": 7.816253564513736e-05,
      "loss": 1.1457,
      "step": 6893
    },
    {
      "epoch": 0.9692113032475749,
      "grad_norm": 1.5181734561920166,
      "learning_rate": 7.793573601813456e-05,
      "loss": 1.1233,
      "step": 6894
    },
    {
      "epoch": 0.9693518909039787,
      "grad_norm": 1.3520088195800781,
      "learning_rate": 7.770905563091194e-05,
      "loss": 1.0747,
      "step": 6895
    },
    {
      "epoch": 0.9694924785603825,
      "grad_norm": 1.5394023656845093,
      "learning_rate": 7.748249570849601e-05,
      "loss": 1.2377,
      "step": 6896
    },
    {
      "epoch": 0.9696330662167861,
      "grad_norm": 1.2523149251937866,
      "learning_rate": 7.72560574752629e-05,
      "loss": 1.0808,
      "step": 6897
    },
    {
      "epoch": 0.9697736538731899,
      "grad_norm": 1.2592214345932007,
      "learning_rate": 7.702974215493104e-05,
      "loss": 0.9826,
      "step": 6898
    },
    {
      "epoch": 0.9699142415295937,
      "grad_norm": 1.569685935974121,
      "learning_rate": 7.680355097055452e-05,
      "loss": 1.0999,
      "step": 6899
    },
    {
      "epoch": 0.9700548291859975,
      "grad_norm": 1.5708723068237305,
      "learning_rate": 7.657748514451677e-05,
      "loss": 1.0191,
      "step": 6900
    },
    {
      "epoch": 0.9701954168424013,
      "grad_norm": 1.7383564710617065,
      "learning_rate": 7.635154589852325e-05,
      "loss": 1.0996,
      "step": 6901
    },
    {
      "epoch": 0.970336004498805,
      "grad_norm": 1.475844144821167,
      "learning_rate": 7.61257344535957e-05,
      "loss": 0.9886,
      "step": 6902
    },
    {
      "epoch": 0.9704765921552088,
      "grad_norm": 1.3312478065490723,
      "learning_rate": 7.590005203006571e-05,
      "loss": 1.3043,
      "step": 6903
    },
    {
      "epoch": 0.9706171798116126,
      "grad_norm": 1.543102741241455,
      "learning_rate": 7.567449984756658e-05,
      "loss": 1.0945,
      "step": 6904
    },
    {
      "epoch": 0.9707577674680163,
      "grad_norm": 1.597222924232483,
      "learning_rate": 7.544907912502847e-05,
      "loss": 1.0692,
      "step": 6905
    },
    {
      "epoch": 0.97089835512442,
      "grad_norm": 1.6704896688461304,
      "learning_rate": 7.522379108067088e-05,
      "loss": 1.0702,
      "step": 6906
    },
    {
      "epoch": 0.9710389427808238,
      "grad_norm": 1.3987748622894287,
      "learning_rate": 7.499863693199634e-05,
      "loss": 1.2361,
      "step": 6907
    },
    {
      "epoch": 0.9711795304372276,
      "grad_norm": 1.2784814834594727,
      "learning_rate": 7.477361789578392e-05,
      "loss": 1.1657,
      "step": 6908
    },
    {
      "epoch": 0.9713201180936314,
      "grad_norm": 1.3739652633666992,
      "learning_rate": 7.454873518808204e-05,
      "loss": 1.1949,
      "step": 6909
    },
    {
      "epoch": 0.9714607057500352,
      "grad_norm": 1.4236023426055908,
      "learning_rate": 7.432399002420266e-05,
      "loss": 0.9165,
      "step": 6910
    },
    {
      "epoch": 0.9716012934064389,
      "grad_norm": 1.6174434423446655,
      "learning_rate": 7.409938361871498e-05,
      "loss": 1.052,
      "step": 6911
    },
    {
      "epoch": 0.9717418810628426,
      "grad_norm": 1.2940337657928467,
      "learning_rate": 7.387491718543729e-05,
      "loss": 1.0827,
      "step": 6912
    },
    {
      "epoch": 0.9718824687192464,
      "grad_norm": 1.5632166862487793,
      "learning_rate": 7.365059193743193e-05,
      "loss": 1.1002,
      "step": 6913
    },
    {
      "epoch": 0.9720230563756502,
      "grad_norm": 1.4583629369735718,
      "learning_rate": 7.34264090869988e-05,
      "loss": 1.1322,
      "step": 6914
    },
    {
      "epoch": 0.972163644032054,
      "grad_norm": 1.5475852489471436,
      "learning_rate": 7.320236984566718e-05,
      "loss": 1.0947,
      "step": 6915
    },
    {
      "epoch": 0.9723042316884577,
      "grad_norm": 1.50823175907135,
      "learning_rate": 7.297847542419116e-05,
      "loss": 1.1484,
      "step": 6916
    },
    {
      "epoch": 0.9724448193448615,
      "grad_norm": 1.4926129579544067,
      "learning_rate": 7.27547270325413e-05,
      "loss": 1.1683,
      "step": 6917
    },
    {
      "epoch": 0.9725854070012653,
      "grad_norm": 1.6797758340835571,
      "learning_rate": 7.253112587990002e-05,
      "loss": 1.1233,
      "step": 6918
    },
    {
      "epoch": 0.9727259946576691,
      "grad_norm": 1.8521652221679688,
      "learning_rate": 7.230767317465355e-05,
      "loss": 1.1393,
      "step": 6919
    },
    {
      "epoch": 0.9728665823140729,
      "grad_norm": 1.5792642831802368,
      "learning_rate": 7.208437012438551e-05,
      "loss": 0.9893,
      "step": 6920
    },
    {
      "epoch": 0.9730071699704765,
      "grad_norm": 1.4197769165039062,
      "learning_rate": 7.186121793587109e-05,
      "loss": 0.9747,
      "step": 6921
    },
    {
      "epoch": 0.9731477576268803,
      "grad_norm": 1.4667415618896484,
      "learning_rate": 7.163821781507069e-05,
      "loss": 1.1654,
      "step": 6922
    },
    {
      "epoch": 0.9732883452832841,
      "grad_norm": 1.4208130836486816,
      "learning_rate": 7.141537096712194e-05,
      "loss": 1.14,
      "step": 6923
    },
    {
      "epoch": 0.9734289329396879,
      "grad_norm": 1.6065688133239746,
      "learning_rate": 7.119267859633495e-05,
      "loss": 1.1382,
      "step": 6924
    },
    {
      "epoch": 0.9735695205960917,
      "grad_norm": 1.6730635166168213,
      "learning_rate": 7.097014190618418e-05,
      "loss": 1.2099,
      "step": 6925
    },
    {
      "epoch": 0.9737101082524954,
      "grad_norm": 1.5132180452346802,
      "learning_rate": 7.074776209930378e-05,
      "loss": 1.0678,
      "step": 6926
    },
    {
      "epoch": 0.9738506959088992,
      "grad_norm": 1.6182069778442383,
      "learning_rate": 7.052554037747953e-05,
      "loss": 1.0788,
      "step": 6927
    },
    {
      "epoch": 0.973991283565303,
      "grad_norm": 1.7683730125427246,
      "learning_rate": 7.03034779416426e-05,
      "loss": 1.1522,
      "step": 6928
    },
    {
      "epoch": 0.9741318712217067,
      "grad_norm": 1.845323920249939,
      "learning_rate": 7.008157599186363e-05,
      "loss": 1.1663,
      "step": 6929
    },
    {
      "epoch": 0.9742724588781105,
      "grad_norm": 1.4028069972991943,
      "learning_rate": 6.98598357273465e-05,
      "loss": 1.0545,
      "step": 6930
    },
    {
      "epoch": 0.9744130465345142,
      "grad_norm": 1.4146915674209595,
      "learning_rate": 6.963825834642028e-05,
      "loss": 0.9758,
      "step": 6931
    },
    {
      "epoch": 0.974553634190918,
      "grad_norm": 1.4085866212844849,
      "learning_rate": 6.94168450465345e-05,
      "loss": 1.0022,
      "step": 6932
    },
    {
      "epoch": 0.9746942218473218,
      "grad_norm": 1.5087003707885742,
      "learning_rate": 6.919559702425174e-05,
      "loss": 1.0615,
      "step": 6933
    },
    {
      "epoch": 0.9748348095037256,
      "grad_norm": 1.5092992782592773,
      "learning_rate": 6.89745154752414e-05,
      "loss": 1.1164,
      "step": 6934
    },
    {
      "epoch": 0.9749753971601294,
      "grad_norm": 2.005453586578369,
      "learning_rate": 6.875360159427342e-05,
      "loss": 1.178,
      "step": 6935
    },
    {
      "epoch": 0.975115984816533,
      "grad_norm": 1.511159062385559,
      "learning_rate": 6.853285657521092e-05,
      "loss": 1.091,
      "step": 6936
    },
    {
      "epoch": 0.9752565724729368,
      "grad_norm": 1.3348125219345093,
      "learning_rate": 6.83122816110055e-05,
      "loss": 1.1909,
      "step": 6937
    },
    {
      "epoch": 0.9753971601293406,
      "grad_norm": 1.4853235483169556,
      "learning_rate": 6.809187789368923e-05,
      "loss": 1.0402,
      "step": 6938
    },
    {
      "epoch": 0.9755377477857444,
      "grad_norm": 1.439894437789917,
      "learning_rate": 6.787164661436835e-05,
      "loss": 1.0159,
      "step": 6939
    },
    {
      "epoch": 0.9756783354421482,
      "grad_norm": 1.5562024116516113,
      "learning_rate": 6.76515889632176e-05,
      "loss": 1.217,
      "step": 6940
    },
    {
      "epoch": 0.9758189230985519,
      "grad_norm": 1.2688101530075073,
      "learning_rate": 6.74317061294739e-05,
      "loss": 1.0886,
      "step": 6941
    },
    {
      "epoch": 0.9759595107549557,
      "grad_norm": 1.583418369293213,
      "learning_rate": 6.721199930142838e-05,
      "loss": 0.8656,
      "step": 6942
    },
    {
      "epoch": 0.9761000984113595,
      "grad_norm": 1.711207628250122,
      "learning_rate": 6.699246966642188e-05,
      "loss": 1.1965,
      "step": 6943
    },
    {
      "epoch": 0.9762406860677633,
      "grad_norm": 1.5033369064331055,
      "learning_rate": 6.677311841083675e-05,
      "loss": 1.1361,
      "step": 6944
    },
    {
      "epoch": 0.976381273724167,
      "grad_norm": 1.5265699625015259,
      "learning_rate": 6.655394672009243e-05,
      "loss": 1.1068,
      "step": 6945
    },
    {
      "epoch": 0.9765218613805707,
      "grad_norm": 1.5450338125228882,
      "learning_rate": 6.633495577863739e-05,
      "loss": 1.2234,
      "step": 6946
    },
    {
      "epoch": 0.9766624490369745,
      "grad_norm": 1.4601378440856934,
      "learning_rate": 6.611614676994298e-05,
      "loss": 1.1948,
      "step": 6947
    },
    {
      "epoch": 0.9768030366933783,
      "grad_norm": 1.3029245138168335,
      "learning_rate": 6.589752087649766e-05,
      "loss": 1.1401,
      "step": 6948
    },
    {
      "epoch": 0.9769436243497821,
      "grad_norm": 1.35009765625,
      "learning_rate": 6.567907927980086e-05,
      "loss": 1.242,
      "step": 6949
    },
    {
      "epoch": 0.9770842120061859,
      "grad_norm": 1.4422743320465088,
      "learning_rate": 6.546082316035498e-05,
      "loss": 1.0536,
      "step": 6950
    },
    {
      "epoch": 0.9772247996625896,
      "grad_norm": 1.6354024410247803,
      "learning_rate": 6.524275369766073e-05,
      "loss": 0.9131,
      "step": 6951
    },
    {
      "epoch": 0.9773653873189934,
      "grad_norm": 1.5509005784988403,
      "learning_rate": 6.502487207020993e-05,
      "loss": 0.9587,
      "step": 6952
    },
    {
      "epoch": 0.9775059749753972,
      "grad_norm": 1.7896506786346436,
      "learning_rate": 6.480717945547934e-05,
      "loss": 1.0052,
      "step": 6953
    },
    {
      "epoch": 0.9776465626318009,
      "grad_norm": 1.2442312240600586,
      "learning_rate": 6.458967702992434e-05,
      "loss": 1.0595,
      "step": 6954
    },
    {
      "epoch": 0.9777871502882047,
      "grad_norm": 1.4085705280303955,
      "learning_rate": 6.437236596897191e-05,
      "loss": 1.133,
      "step": 6955
    },
    {
      "epoch": 0.9779277379446084,
      "grad_norm": 1.390135645866394,
      "learning_rate": 6.41552474470158e-05,
      "loss": 0.8678,
      "step": 6956
    },
    {
      "epoch": 0.9780683256010122,
      "grad_norm": 1.7850888967514038,
      "learning_rate": 6.393832263740877e-05,
      "loss": 0.9971,
      "step": 6957
    },
    {
      "epoch": 0.978208913257416,
      "grad_norm": 1.6166343688964844,
      "learning_rate": 6.372159271245634e-05,
      "loss": 1.0052,
      "step": 6958
    },
    {
      "epoch": 0.9783495009138198,
      "grad_norm": 1.3940684795379639,
      "learning_rate": 6.350505884341141e-05,
      "loss": 1.0795,
      "step": 6959
    },
    {
      "epoch": 0.9784900885702236,
      "grad_norm": 1.2823158502578735,
      "learning_rate": 6.328872220046711e-05,
      "loss": 1.1859,
      "step": 6960
    },
    {
      "epoch": 0.9786306762266272,
      "grad_norm": 1.372239351272583,
      "learning_rate": 6.307258395275077e-05,
      "loss": 1.0756,
      "step": 6961
    },
    {
      "epoch": 0.978771263883031,
      "grad_norm": 1.4542557001113892,
      "learning_rate": 6.285664526831771e-05,
      "loss": 1.1669,
      "step": 6962
    },
    {
      "epoch": 0.9789118515394348,
      "grad_norm": 1.4421824216842651,
      "learning_rate": 6.264090731414407e-05,
      "loss": 1.1183,
      "step": 6963
    },
    {
      "epoch": 0.9790524391958386,
      "grad_norm": 1.5484731197357178,
      "learning_rate": 6.242537125612224e-05,
      "loss": 1.0566,
      "step": 6964
    },
    {
      "epoch": 0.9791930268522424,
      "grad_norm": 1.6086316108703613,
      "learning_rate": 6.2210038259053e-05,
      "loss": 1.0932,
      "step": 6965
    },
    {
      "epoch": 0.9793336145086461,
      "grad_norm": 1.320766806602478,
      "learning_rate": 6.199490948663946e-05,
      "loss": 1.1128,
      "step": 6966
    },
    {
      "epoch": 0.9794742021650499,
      "grad_norm": 1.7345912456512451,
      "learning_rate": 6.177998610148148e-05,
      "loss": 1.0801,
      "step": 6967
    },
    {
      "epoch": 0.9796147898214537,
      "grad_norm": 1.3780122995376587,
      "learning_rate": 6.156526926506884e-05,
      "loss": 1.0595,
      "step": 6968
    },
    {
      "epoch": 0.9797553774778575,
      "grad_norm": 1.5253669023513794,
      "learning_rate": 6.135076013777513e-05,
      "loss": 0.9701,
      "step": 6969
    },
    {
      "epoch": 0.9798959651342612,
      "grad_norm": 1.3239765167236328,
      "learning_rate": 6.113645987885135e-05,
      "loss": 1.0924,
      "step": 6970
    },
    {
      "epoch": 0.9800365527906649,
      "grad_norm": 1.4777753353118896,
      "learning_rate": 6.092236964641982e-05,
      "loss": 1.3117,
      "step": 6971
    },
    {
      "epoch": 0.9801771404470687,
      "grad_norm": 1.4878495931625366,
      "learning_rate": 6.070849059746777e-05,
      "loss": 1.1314,
      "step": 6972
    },
    {
      "epoch": 0.9803177281034725,
      "grad_norm": 1.6350319385528564,
      "learning_rate": 6.049482388784133e-05,
      "loss": 0.9398,
      "step": 6973
    },
    {
      "epoch": 0.9804583157598763,
      "grad_norm": 1.5433138608932495,
      "learning_rate": 6.028137067223847e-05,
      "loss": 1.217,
      "step": 6974
    },
    {
      "epoch": 0.9805989034162801,
      "grad_norm": 1.480332612991333,
      "learning_rate": 6.0068132104204374e-05,
      "loss": 1.1434,
      "step": 6975
    },
    {
      "epoch": 0.9807394910726838,
      "grad_norm": 1.3822287321090698,
      "learning_rate": 5.985510933612373e-05,
      "loss": 1.0643,
      "step": 6976
    },
    {
      "epoch": 0.9808800787290876,
      "grad_norm": 1.5032798051834106,
      "learning_rate": 5.964230351921464e-05,
      "loss": 1.316,
      "step": 6977
    },
    {
      "epoch": 0.9810206663854913,
      "grad_norm": 1.3768588304519653,
      "learning_rate": 5.94297158035233e-05,
      "loss": 1.2898,
      "step": 6978
    },
    {
      "epoch": 0.9811612540418951,
      "grad_norm": 1.3427734375,
      "learning_rate": 5.9217347337917054e-05,
      "loss": 1.0574,
      "step": 6979
    },
    {
      "epoch": 0.9813018416982989,
      "grad_norm": 1.361555814743042,
      "learning_rate": 5.900519927007837e-05,
      "loss": 1.1317,
      "step": 6980
    },
    {
      "epoch": 0.9814424293547026,
      "grad_norm": 1.3869307041168213,
      "learning_rate": 5.879327274649882e-05,
      "loss": 1.1709,
      "step": 6981
    },
    {
      "epoch": 0.9815830170111064,
      "grad_norm": 1.5682485103607178,
      "learning_rate": 5.858156891247204e-05,
      "loss": 1.1076,
      "step": 6982
    },
    {
      "epoch": 0.9817236046675102,
      "grad_norm": 1.3626019954681396,
      "learning_rate": 5.83700889120892e-05,
      "loss": 1.0532,
      "step": 6983
    },
    {
      "epoch": 0.981864192323914,
      "grad_norm": 1.5095198154449463,
      "learning_rate": 5.815883388823129e-05,
      "loss": 1.0344,
      "step": 6984
    },
    {
      "epoch": 0.9820047799803178,
      "grad_norm": 1.5640226602554321,
      "learning_rate": 5.7947804982563245e-05,
      "loss": 0.9981,
      "step": 6985
    },
    {
      "epoch": 0.9821453676367214,
      "grad_norm": 1.6817700862884521,
      "learning_rate": 5.773700333552841e-05,
      "loss": 1.33,
      "step": 6986
    },
    {
      "epoch": 0.9822859552931252,
      "grad_norm": 1.4244813919067383,
      "learning_rate": 5.7526430086341864e-05,
      "loss": 1.1397,
      "step": 6987
    },
    {
      "epoch": 0.982426542949529,
      "grad_norm": 1.503833293914795,
      "learning_rate": 5.731608637298438e-05,
      "loss": 1.1764,
      "step": 6988
    },
    {
      "epoch": 0.9825671306059328,
      "grad_norm": 1.4407190084457397,
      "learning_rate": 5.710597333219623e-05,
      "loss": 1.1221,
      "step": 6989
    },
    {
      "epoch": 0.9827077182623366,
      "grad_norm": 1.1421538591384888,
      "learning_rate": 5.6896092099471135e-05,
      "loss": 1.1632,
      "step": 6990
    },
    {
      "epoch": 0.9828483059187403,
      "grad_norm": 1.7203139066696167,
      "learning_rate": 5.6686443809050036e-05,
      "loss": 0.9909,
      "step": 6991
    },
    {
      "epoch": 0.9829888935751441,
      "grad_norm": 1.6034400463104248,
      "learning_rate": 5.647702959391521e-05,
      "loss": 1.0488,
      "step": 6992
    },
    {
      "epoch": 0.9831294812315479,
      "grad_norm": 1.4659302234649658,
      "learning_rate": 5.626785058578328e-05,
      "loss": 1.0268,
      "step": 6993
    },
    {
      "epoch": 0.9832700688879517,
      "grad_norm": 1.6702873706817627,
      "learning_rate": 5.6058907915100846e-05,
      "loss": 1.0084,
      "step": 6994
    },
    {
      "epoch": 0.9834106565443554,
      "grad_norm": 1.5733299255371094,
      "learning_rate": 5.5850202711036246e-05,
      "loss": 1.0379,
      "step": 6995
    },
    {
      "epoch": 0.9835512442007591,
      "grad_norm": 1.4653249979019165,
      "learning_rate": 5.5641736101475094e-05,
      "loss": 0.9918,
      "step": 6996
    },
    {
      "epoch": 0.9836918318571629,
      "grad_norm": 2.0999574661254883,
      "learning_rate": 5.5433509213013426e-05,
      "loss": 1.0394,
      "step": 6997
    },
    {
      "epoch": 0.9838324195135667,
      "grad_norm": 1.4086732864379883,
      "learning_rate": 5.52255231709518e-05,
      "loss": 1.1773,
      "step": 6998
    },
    {
      "epoch": 0.9839730071699705,
      "grad_norm": 1.6002137660980225,
      "learning_rate": 5.501777909928918e-05,
      "loss": 0.9906,
      "step": 6999
    },
    {
      "epoch": 0.9841135948263743,
      "grad_norm": 1.454197883605957,
      "learning_rate": 5.481027812071703e-05,
      "loss": 1.0777,
      "step": 7000
    },
    {
      "epoch": 0.9841135948263743,
      "eval_loss": 1.1470837593078613,
      "eval_runtime": 771.2764,
      "eval_samples_per_second": 16.396,
      "eval_steps_per_second": 8.198,
      "step": 7000
    },
    {
      "epoch": 0.984254182482778,
      "grad_norm": 1.8197898864746094,
      "learning_rate": 5.460302135661246e-05,
      "loss": 1.1249,
      "step": 7001
    },
    {
      "epoch": 0.9843947701391818,
      "grad_norm": 1.3743577003479004,
      "learning_rate": 5.4396009927033885e-05,
      "loss": 0.9913,
      "step": 7002
    },
    {
      "epoch": 0.9845353577955855,
      "grad_norm": 1.4603700637817383,
      "learning_rate": 5.418924495071282e-05,
      "loss": 1.1552,
      "step": 7003
    },
    {
      "epoch": 0.9846759454519893,
      "grad_norm": 1.381390929222107,
      "learning_rate": 5.3982727545049474e-05,
      "loss": 1.1372,
      "step": 7004
    },
    {
      "epoch": 0.9848165331083931,
      "grad_norm": 1.5838944911956787,
      "learning_rate": 5.3776458826106e-05,
      "loss": 0.9288,
      "step": 7005
    },
    {
      "epoch": 0.9849571207647968,
      "grad_norm": 1.3668098449707031,
      "learning_rate": 5.3570439908600626e-05,
      "loss": 1.209,
      "step": 7006
    },
    {
      "epoch": 0.9850977084212006,
      "grad_norm": 1.4661860466003418,
      "learning_rate": 5.3364671905901556e-05,
      "loss": 1.1082,
      "step": 7007
    },
    {
      "epoch": 0.9852382960776044,
      "grad_norm": 1.3951280117034912,
      "learning_rate": 5.3159155930021e-05,
      "loss": 1.0508,
      "step": 7008
    },
    {
      "epoch": 0.9853788837340082,
      "grad_norm": 2.0603625774383545,
      "learning_rate": 5.295389309160921e-05,
      "loss": 1.2133,
      "step": 7009
    },
    {
      "epoch": 0.985519471390412,
      "grad_norm": 1.4380947351455688,
      "learning_rate": 5.274888449994855e-05,
      "loss": 0.9638,
      "step": 7010
    },
    {
      "epoch": 0.9856600590468156,
      "grad_norm": 1.443787932395935,
      "learning_rate": 5.2544131262946906e-05,
      "loss": 1.0608,
      "step": 7011
    },
    {
      "epoch": 0.9858006467032194,
      "grad_norm": 1.6421949863433838,
      "learning_rate": 5.2339634487132526e-05,
      "loss": 0.9033,
      "step": 7012
    },
    {
      "epoch": 0.9859412343596232,
      "grad_norm": 1.7539867162704468,
      "learning_rate": 5.2135395277648146e-05,
      "loss": 1.2432,
      "step": 7013
    },
    {
      "epoch": 0.986081822016027,
      "grad_norm": 1.5662457942962646,
      "learning_rate": 5.193141473824359e-05,
      "loss": 1.0036,
      "step": 7014
    },
    {
      "epoch": 0.9862224096724308,
      "grad_norm": 1.3705506324768066,
      "learning_rate": 5.172769397127142e-05,
      "loss": 1.0282,
      "step": 7015
    },
    {
      "epoch": 0.9863629973288345,
      "grad_norm": 1.5273228883743286,
      "learning_rate": 5.152423407768021e-05,
      "loss": 0.9982,
      "step": 7016
    },
    {
      "epoch": 0.9865035849852383,
      "grad_norm": 1.3906829357147217,
      "learning_rate": 5.1321036157008694e-05,
      "loss": 1.1737,
      "step": 7017
    },
    {
      "epoch": 0.9866441726416421,
      "grad_norm": 1.7082486152648926,
      "learning_rate": 5.111810130737984e-05,
      "loss": 1.3203,
      "step": 7018
    },
    {
      "epoch": 0.9867847602980458,
      "grad_norm": 1.5947237014770508,
      "learning_rate": 5.0915430625495084e-05,
      "loss": 0.9058,
      "step": 7019
    },
    {
      "epoch": 0.9869253479544496,
      "grad_norm": 1.5866906642913818,
      "learning_rate": 5.0713025206627666e-05,
      "loss": 0.9016,
      "step": 7020
    },
    {
      "epoch": 0.9870659356108533,
      "grad_norm": 1.265514850616455,
      "learning_rate": 5.051088614461832e-05,
      "loss": 1.182,
      "step": 7021
    },
    {
      "epoch": 0.9872065232672571,
      "grad_norm": 1.576364517211914,
      "learning_rate": 5.030901453186727e-05,
      "loss": 0.9591,
      "step": 7022
    },
    {
      "epoch": 0.9873471109236609,
      "grad_norm": 2.1358134746551514,
      "learning_rate": 5.0107411459330034e-05,
      "loss": 1.1584,
      "step": 7023
    },
    {
      "epoch": 0.9874876985800647,
      "grad_norm": 1.5574533939361572,
      "learning_rate": 4.9906078016510705e-05,
      "loss": 1.0329,
      "step": 7024
    },
    {
      "epoch": 0.9876282862364685,
      "grad_norm": 1.5096094608306885,
      "learning_rate": 4.9705015291456224e-05,
      "loss": 1.0142,
      "step": 7025
    },
    {
      "epoch": 0.9877688738928722,
      "grad_norm": 1.996420979499817,
      "learning_rate": 4.950422437075056e-05,
      "loss": 1.0764,
      "step": 7026
    },
    {
      "epoch": 0.987909461549276,
      "grad_norm": 1.3893033266067505,
      "learning_rate": 4.930370633950878e-05,
      "loss": 1.227,
      "step": 7027
    },
    {
      "epoch": 0.9880500492056797,
      "grad_norm": 1.4948948621749878,
      "learning_rate": 4.910346228137117e-05,
      "loss": 1.0722,
      "step": 7028
    },
    {
      "epoch": 0.9881906368620835,
      "grad_norm": 1.7939300537109375,
      "learning_rate": 4.89034932784976e-05,
      "loss": 1.1695,
      "step": 7029
    },
    {
      "epoch": 0.9883312245184873,
      "grad_norm": 1.5397518873214722,
      "learning_rate": 4.870380041156103e-05,
      "loss": 1.2103,
      "step": 7030
    },
    {
      "epoch": 0.988471812174891,
      "grad_norm": 1.4981374740600586,
      "learning_rate": 4.850438475974239e-05,
      "loss": 1.0819,
      "step": 7031
    },
    {
      "epoch": 0.9886123998312948,
      "grad_norm": 1.5639245510101318,
      "learning_rate": 4.8305247400724994e-05,
      "loss": 0.9436,
      "step": 7032
    },
    {
      "epoch": 0.9887529874876986,
      "grad_norm": 1.394305944442749,
      "learning_rate": 4.810638941068724e-05,
      "loss": 1.0881,
      "step": 7033
    },
    {
      "epoch": 0.9888935751441024,
      "grad_norm": 1.553920030593872,
      "learning_rate": 4.790781186429839e-05,
      "loss": 1.0205,
      "step": 7034
    },
    {
      "epoch": 0.9890341628005062,
      "grad_norm": 1.5767360925674438,
      "learning_rate": 4.7709515834711995e-05,
      "loss": 1.1096,
      "step": 7035
    },
    {
      "epoch": 0.9891747504569098,
      "grad_norm": 1.4725526571273804,
      "learning_rate": 4.751150239356021e-05,
      "loss": 1.2149,
      "step": 7036
    },
    {
      "epoch": 0.9893153381133136,
      "grad_norm": 1.4838228225708008,
      "learning_rate": 4.731377261094818e-05,
      "loss": 1.1533,
      "step": 7037
    },
    {
      "epoch": 0.9894559257697174,
      "grad_norm": 1.5752273797988892,
      "learning_rate": 4.711632755544766e-05,
      "loss": 1.0205,
      "step": 7038
    },
    {
      "epoch": 0.9895965134261212,
      "grad_norm": 1.613810658454895,
      "learning_rate": 4.691916829409196e-05,
      "loss": 1.2338,
      "step": 7039
    },
    {
      "epoch": 0.989737101082525,
      "grad_norm": 1.424438714981079,
      "learning_rate": 4.6722295892370384e-05,
      "loss": 1.0513,
      "step": 7040
    },
    {
      "epoch": 0.9898776887389287,
      "grad_norm": 1.416923999786377,
      "learning_rate": 4.652571141422105e-05,
      "loss": 1.1061,
      "step": 7041
    },
    {
      "epoch": 0.9900182763953325,
      "grad_norm": 1.305484652519226,
      "learning_rate": 4.632941592202671e-05,
      "loss": 1.0197,
      "step": 7042
    },
    {
      "epoch": 0.9901588640517363,
      "grad_norm": 1.754050374031067,
      "learning_rate": 4.613341047660821e-05,
      "loss": 1.2079,
      "step": 7043
    },
    {
      "epoch": 0.99029945170814,
      "grad_norm": 1.4438799619674683,
      "learning_rate": 4.593769613721891e-05,
      "loss": 1.161,
      "step": 7044
    },
    {
      "epoch": 0.9904400393645438,
      "grad_norm": 1.4338399171829224,
      "learning_rate": 4.5742273961539174e-05,
      "loss": 1.1091,
      "step": 7045
    },
    {
      "epoch": 0.9905806270209475,
      "grad_norm": 1.5512346029281616,
      "learning_rate": 4.5547145005669814e-05,
      "loss": 1.2938,
      "step": 7046
    },
    {
      "epoch": 0.9907212146773513,
      "grad_norm": 1.3071600198745728,
      "learning_rate": 4.535231032412791e-05,
      "loss": 1.0814,
      "step": 7047
    },
    {
      "epoch": 0.9908618023337551,
      "grad_norm": 1.5164111852645874,
      "learning_rate": 4.515777096983981e-05,
      "loss": 1.02,
      "step": 7048
    },
    {
      "epoch": 0.9910023899901589,
      "grad_norm": 1.4089860916137695,
      "learning_rate": 4.4963527994135524e-05,
      "loss": 0.976,
      "step": 7049
    },
    {
      "epoch": 0.9911429776465627,
      "grad_norm": 1.2942383289337158,
      "learning_rate": 4.4769582446743706e-05,
      "loss": 1.0386,
      "step": 7050
    },
    {
      "epoch": 0.9912835653029664,
      "grad_norm": 1.5589377880096436,
      "learning_rate": 4.457593537578606e-05,
      "loss": 1.1914,
      "step": 7051
    },
    {
      "epoch": 0.9914241529593701,
      "grad_norm": 1.6001297235488892,
      "learning_rate": 4.438258782777037e-05,
      "loss": 1.1662,
      "step": 7052
    },
    {
      "epoch": 0.9915647406157739,
      "grad_norm": 1.3034141063690186,
      "learning_rate": 4.4189540847586375e-05,
      "loss": 1.1688,
      "step": 7053
    },
    {
      "epoch": 0.9917053282721777,
      "grad_norm": 1.5921030044555664,
      "learning_rate": 4.3996795478499264e-05,
      "loss": 1.0191,
      "step": 7054
    },
    {
      "epoch": 0.9918459159285815,
      "grad_norm": 1.756485939025879,
      "learning_rate": 4.380435276214433e-05,
      "loss": 1.0218,
      "step": 7055
    },
    {
      "epoch": 0.9919865035849852,
      "grad_norm": 1.5053188800811768,
      "learning_rate": 4.361221373852137e-05,
      "loss": 1.1033,
      "step": 7056
    },
    {
      "epoch": 0.992127091241389,
      "grad_norm": 1.3770291805267334,
      "learning_rate": 4.342037944598855e-05,
      "loss": 1.0579,
      "step": 7057
    },
    {
      "epoch": 0.9922676788977928,
      "grad_norm": 1.399750828742981,
      "learning_rate": 4.322885092125746e-05,
      "loss": 1.1515,
      "step": 7058
    },
    {
      "epoch": 0.9924082665541966,
      "grad_norm": 1.4204837083816528,
      "learning_rate": 4.303762919938775e-05,
      "loss": 1.0683,
      "step": 7059
    },
    {
      "epoch": 0.9925488542106004,
      "grad_norm": 1.725449800491333,
      "learning_rate": 4.284671531378014e-05,
      "loss": 0.9091,
      "step": 7060
    },
    {
      "epoch": 0.992689441867004,
      "grad_norm": 1.518404245376587,
      "learning_rate": 4.2656110296172314e-05,
      "loss": 1.1988,
      "step": 7061
    },
    {
      "epoch": 0.9928300295234078,
      "grad_norm": 1.3507180213928223,
      "learning_rate": 4.246581517663265e-05,
      "loss": 1.1464,
      "step": 7062
    },
    {
      "epoch": 0.9929706171798116,
      "grad_norm": 1.5258634090423584,
      "learning_rate": 4.227583098355475e-05,
      "loss": 1.0516,
      "step": 7063
    },
    {
      "epoch": 0.9931112048362154,
      "grad_norm": 1.2326143980026245,
      "learning_rate": 4.2086158743652084e-05,
      "loss": 1.1623,
      "step": 7064
    },
    {
      "epoch": 0.9932517924926192,
      "grad_norm": 1.6842622756958008,
      "learning_rate": 4.1896799481951785e-05,
      "loss": 1.0989,
      "step": 7065
    },
    {
      "epoch": 0.9933923801490229,
      "grad_norm": 1.5034457445144653,
      "learning_rate": 4.170775422178991e-05,
      "loss": 1.0513,
      "step": 7066
    },
    {
      "epoch": 0.9935329678054267,
      "grad_norm": 1.8134254217147827,
      "learning_rate": 4.1519023984805997e-05,
      "loss": 0.9055,
      "step": 7067
    },
    {
      "epoch": 0.9936735554618304,
      "grad_norm": 1.3622709512710571,
      "learning_rate": 4.1330609790936305e-05,
      "loss": 0.9586,
      "step": 7068
    },
    {
      "epoch": 0.9938141431182342,
      "grad_norm": 1.7575784921646118,
      "learning_rate": 4.114251265840952e-05,
      "loss": 0.9406,
      "step": 7069
    },
    {
      "epoch": 0.993954730774638,
      "grad_norm": 1.5277637243270874,
      "learning_rate": 4.095473360374133e-05,
      "loss": 0.9859,
      "step": 7070
    },
    {
      "epoch": 0.9940953184310417,
      "grad_norm": 1.3744220733642578,
      "learning_rate": 4.0767273641727575e-05,
      "loss": 1.1131,
      "step": 7071
    },
    {
      "epoch": 0.9942359060874455,
      "grad_norm": 1.3571871519088745,
      "learning_rate": 4.058013378544041e-05,
      "loss": 0.8911,
      "step": 7072
    },
    {
      "epoch": 0.9943764937438493,
      "grad_norm": 1.3261908292770386,
      "learning_rate": 4.039331504622131e-05,
      "loss": 0.9257,
      "step": 7073
    },
    {
      "epoch": 0.9945170814002531,
      "grad_norm": 1.3632086515426636,
      "learning_rate": 4.0206818433677276e-05,
      "loss": 0.889,
      "step": 7074
    },
    {
      "epoch": 0.9946576690566569,
      "grad_norm": 1.4098873138427734,
      "learning_rate": 4.002064495567398e-05,
      "loss": 1.0629,
      "step": 7075
    },
    {
      "epoch": 0.9947982567130605,
      "grad_norm": 1.4357661008834839,
      "learning_rate": 3.9834795618330575e-05,
      "loss": 1.1157,
      "step": 7076
    },
    {
      "epoch": 0.9949388443694643,
      "grad_norm": 1.54239821434021,
      "learning_rate": 3.964927142601478e-05,
      "loss": 1.0632,
      "step": 7077
    },
    {
      "epoch": 0.9950794320258681,
      "grad_norm": 1.6670222282409668,
      "learning_rate": 3.946407338133754e-05,
      "loss": 0.987,
      "step": 7078
    },
    {
      "epoch": 0.9952200196822719,
      "grad_norm": 1.4960718154907227,
      "learning_rate": 3.9279202485146435e-05,
      "loss": 1.1749,
      "step": 7079
    },
    {
      "epoch": 0.9953606073386757,
      "grad_norm": 1.7500368356704712,
      "learning_rate": 3.9094659736521736e-05,
      "loss": 1.0985,
      "step": 7080
    },
    {
      "epoch": 0.9955011949950794,
      "grad_norm": 1.6385846138000488,
      "learning_rate": 3.891044613276966e-05,
      "loss": 0.9837,
      "step": 7081
    },
    {
      "epoch": 0.9956417826514832,
      "grad_norm": 1.868635892868042,
      "learning_rate": 3.872656266941848e-05,
      "loss": 1.0697,
      "step": 7082
    },
    {
      "epoch": 0.995782370307887,
      "grad_norm": 1.5955666303634644,
      "learning_rate": 3.854301034021193e-05,
      "loss": 0.8776,
      "step": 7083
    },
    {
      "epoch": 0.9959229579642908,
      "grad_norm": 1.5362552404403687,
      "learning_rate": 3.83597901371039e-05,
      "loss": 1.0975,
      "step": 7084
    },
    {
      "epoch": 0.9960635456206945,
      "grad_norm": 1.5052447319030762,
      "learning_rate": 3.8176903050253745e-05,
      "loss": 1.2186,
      "step": 7085
    },
    {
      "epoch": 0.9962041332770982,
      "grad_norm": 1.4731197357177734,
      "learning_rate": 3.7994350068020954e-05,
      "loss": 1.2456,
      "step": 7086
    },
    {
      "epoch": 0.996344720933502,
      "grad_norm": 1.5566811561584473,
      "learning_rate": 3.781213217695866e-05,
      "loss": 1.1057,
      "step": 7087
    },
    {
      "epoch": 0.9964853085899058,
      "grad_norm": 1.3439768552780151,
      "learning_rate": 3.763025036180946e-05,
      "loss": 1.1664,
      "step": 7088
    },
    {
      "epoch": 0.9966258962463096,
      "grad_norm": 1.2813482284545898,
      "learning_rate": 3.744870560550019e-05,
      "loss": 1.1708,
      "step": 7089
    },
    {
      "epoch": 0.9967664839027134,
      "grad_norm": 1.4667906761169434,
      "learning_rate": 3.7267498889135285e-05,
      "loss": 1.0428,
      "step": 7090
    },
    {
      "epoch": 0.9969070715591171,
      "grad_norm": 1.3082643747329712,
      "learning_rate": 3.708663119199307e-05,
      "loss": 1.0578,
      "step": 7091
    },
    {
      "epoch": 0.9970476592155209,
      "grad_norm": 1.7264735698699951,
      "learning_rate": 3.690610349151902e-05,
      "loss": 1.034,
      "step": 7092
    },
    {
      "epoch": 0.9971882468719246,
      "grad_norm": 1.7701631784439087,
      "learning_rate": 3.672591676332204e-05,
      "loss": 1.1178,
      "step": 7093
    },
    {
      "epoch": 0.9973288345283284,
      "grad_norm": 1.5156649351119995,
      "learning_rate": 3.654607198116794e-05,
      "loss": 0.985,
      "step": 7094
    },
    {
      "epoch": 0.9974694221847322,
      "grad_norm": 1.6105211973190308,
      "learning_rate": 3.63665701169743e-05,
      "loss": 1.2828,
      "step": 7095
    },
    {
      "epoch": 0.9976100098411359,
      "grad_norm": 1.55417799949646,
      "learning_rate": 3.6187412140805775e-05,
      "loss": 1.1582,
      "step": 7096
    },
    {
      "epoch": 0.9977505974975397,
      "grad_norm": 1.3564273118972778,
      "learning_rate": 3.600859902086902e-05,
      "loss": 1.1395,
      "step": 7097
    },
    {
      "epoch": 0.9978911851539435,
      "grad_norm": 1.639605164527893,
      "learning_rate": 3.583013172350615e-05,
      "loss": 1.103,
      "step": 7098
    },
    {
      "epoch": 0.9980317728103473,
      "grad_norm": 1.4231047630310059,
      "learning_rate": 3.5652011213191085e-05,
      "loss": 0.9508,
      "step": 7099
    },
    {
      "epoch": 0.9981723604667511,
      "grad_norm": 1.5504980087280273,
      "learning_rate": 3.547423845252298e-05,
      "loss": 1.0799,
      "step": 7100
    },
    {
      "epoch": 0.9983129481231547,
      "grad_norm": 1.3950064182281494,
      "learning_rate": 3.529681440222247e-05,
      "loss": 1.1981,
      "step": 7101
    },
    {
      "epoch": 0.9984535357795585,
      "grad_norm": 1.3455109596252441,
      "learning_rate": 3.511974002112528e-05,
      "loss": 1.0674,
      "step": 7102
    },
    {
      "epoch": 0.9985941234359623,
      "grad_norm": 1.7137280702590942,
      "learning_rate": 3.494301626617719e-05,
      "loss": 1.2208,
      "step": 7103
    },
    {
      "epoch": 0.9987347110923661,
      "grad_norm": 1.3555837869644165,
      "learning_rate": 3.476664409242939e-05,
      "loss": 1.1406,
      "step": 7104
    },
    {
      "epoch": 0.9988752987487699,
      "grad_norm": 1.2667356729507446,
      "learning_rate": 3.459062445303346e-05,
      "loss": 0.9894,
      "step": 7105
    },
    {
      "epoch": 0.9990158864051736,
      "grad_norm": 1.5275588035583496,
      "learning_rate": 3.441495829923501e-05,
      "loss": 1.2042,
      "step": 7106
    },
    {
      "epoch": 0.9991564740615774,
      "grad_norm": 1.5426528453826904,
      "learning_rate": 3.423964658036989e-05,
      "loss": 1.0572,
      "step": 7107
    },
    {
      "epoch": 0.9992970617179812,
      "grad_norm": 1.4342981576919556,
      "learning_rate": 3.406469024385833e-05,
      "loss": 1.1874,
      "step": 7108
    },
    {
      "epoch": 0.999437649374385,
      "grad_norm": 1.4254449605941772,
      "learning_rate": 3.3890090235200045e-05,
      "loss": 1.1984,
      "step": 7109
    },
    {
      "epoch": 0.9995782370307887,
      "grad_norm": 1.451080083847046,
      "learning_rate": 3.371584749796922e-05,
      "loss": 1.0495,
      "step": 7110
    },
    {
      "epoch": 0.9997188246871924,
      "grad_norm": 1.4365315437316895,
      "learning_rate": 3.35419629738087e-05,
      "loss": 0.9821,
      "step": 7111
    },
    {
      "epoch": 0.9998594123435962,
      "grad_norm": 2.3914244174957275,
      "learning_rate": 3.3368437602426304e-05,
      "loss": 1.0643,
      "step": 7112
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.5359708070755005,
      "learning_rate": 3.319527232158852e-05,
      "loss": 0.9348,
      "step": 7113
    },
    {
      "epoch": 1.0001405876564038,
      "grad_norm": 1.3420566320419312,
      "learning_rate": 3.302246806711553e-05,
      "loss": 1.1932,
      "step": 7114
    },
    {
      "epoch": 1.0002811753128076,
      "grad_norm": 1.597287654876709,
      "learning_rate": 3.28500257728769e-05,
      "loss": 1.0109,
      "step": 7115
    },
    {
      "epoch": 1.0004217629692114,
      "grad_norm": 1.269960641860962,
      "learning_rate": 3.267794637078594e-05,
      "loss": 1.2056,
      "step": 7116
    },
    {
      "epoch": 1.0005623506256152,
      "grad_norm": 1.3499561548233032,
      "learning_rate": 3.250623079079479e-05,
      "loss": 1.0944,
      "step": 7117
    },
    {
      "epoch": 1.0007029382820187,
      "grad_norm": 1.3291095495224,
      "learning_rate": 3.2334879960889595e-05,
      "loss": 0.9819,
      "step": 7118
    },
    {
      "epoch": 1.0008435259384225,
      "grad_norm": 1.573964238166809,
      "learning_rate": 3.216389480708477e-05,
      "loss": 1.0504,
      "step": 7119
    },
    {
      "epoch": 1.0009841135948263,
      "grad_norm": 1.5405292510986328,
      "learning_rate": 3.199327625341933e-05,
      "loss": 1.1106,
      "step": 7120
    },
    {
      "epoch": 1.00112470125123,
      "grad_norm": 1.3118336200714111,
      "learning_rate": 3.18230252219507e-05,
      "loss": 0.9191,
      "step": 7121
    },
    {
      "epoch": 1.0012652889076339,
      "grad_norm": 1.4346309900283813,
      "learning_rate": 3.165314263274991e-05,
      "loss": 1.1476,
      "step": 7122
    },
    {
      "epoch": 1.0014058765640377,
      "grad_norm": 1.4038722515106201,
      "learning_rate": 3.148362940389723e-05,
      "loss": 1.0508,
      "step": 7123
    },
    {
      "epoch": 1.0015464642204415,
      "grad_norm": 1.377076268196106,
      "learning_rate": 3.13144864514767e-05,
      "loss": 0.9729,
      "step": 7124
    },
    {
      "epoch": 1.0016870518768453,
      "grad_norm": 1.3491467237472534,
      "learning_rate": 3.1145714689571305e-05,
      "loss": 1.0587,
      "step": 7125
    },
    {
      "epoch": 1.001827639533249,
      "grad_norm": 1.3749178647994995,
      "learning_rate": 3.097731503025805e-05,
      "loss": 1.1039,
      "step": 7126
    },
    {
      "epoch": 1.0019682271896528,
      "grad_norm": 1.5089621543884277,
      "learning_rate": 3.080928838360301e-05,
      "loss": 1.1397,
      "step": 7127
    },
    {
      "epoch": 1.0021088148460564,
      "grad_norm": 1.399984359741211,
      "learning_rate": 3.064163565765644e-05,
      "loss": 0.9735,
      "step": 7128
    },
    {
      "epoch": 1.0022494025024602,
      "grad_norm": 1.8229072093963623,
      "learning_rate": 3.0474357758447968e-05,
      "loss": 0.992,
      "step": 7129
    },
    {
      "epoch": 1.002389990158864,
      "grad_norm": 1.5823179483413696,
      "learning_rate": 3.030745558998108e-05,
      "loss": 1.0032,
      "step": 7130
    },
    {
      "epoch": 1.0025305778152678,
      "grad_norm": 1.5762693881988525,
      "learning_rate": 3.014093005422949e-05,
      "loss": 0.9516,
      "step": 7131
    },
    {
      "epoch": 1.0026711654716716,
      "grad_norm": 1.3645492792129517,
      "learning_rate": 2.9974782051131134e-05,
      "loss": 1.015,
      "step": 7132
    },
    {
      "epoch": 1.0028117531280754,
      "grad_norm": 1.531137466430664,
      "learning_rate": 2.9809012478583432e-05,
      "loss": 1.0399,
      "step": 7133
    },
    {
      "epoch": 1.0029523407844791,
      "grad_norm": 1.6920922994613647,
      "learning_rate": 2.9643622232439074e-05,
      "loss": 1.0722,
      "step": 7134
    },
    {
      "epoch": 1.003092928440883,
      "grad_norm": 1.3649088144302368,
      "learning_rate": 2.9478612206500665e-05,
      "loss": 1.3356,
      "step": 7135
    },
    {
      "epoch": 1.0032335160972867,
      "grad_norm": 1.5856913328170776,
      "learning_rate": 2.9313983292516012e-05,
      "loss": 0.985,
      "step": 7136
    },
    {
      "epoch": 1.0033741037536905,
      "grad_norm": 1.4418002367019653,
      "learning_rate": 2.9149736380173444e-05,
      "loss": 1.1342,
      "step": 7137
    },
    {
      "epoch": 1.003514691410094,
      "grad_norm": 1.4687809944152832,
      "learning_rate": 2.8985872357096378e-05,
      "loss": 1.1065,
      "step": 7138
    },
    {
      "epoch": 1.0036552790664979,
      "grad_norm": 1.603753924369812,
      "learning_rate": 2.8822392108839768e-05,
      "loss": 0.9771,
      "step": 7139
    },
    {
      "epoch": 1.0037958667229017,
      "grad_norm": 1.4927492141723633,
      "learning_rate": 2.8659296518884216e-05,
      "loss": 1.0867,
      "step": 7140
    },
    {
      "epoch": 1.0039364543793055,
      "grad_norm": 1.3703263998031616,
      "learning_rate": 2.849658646863126e-05,
      "loss": 1.1518,
      "step": 7141
    },
    {
      "epoch": 1.0040770420357092,
      "grad_norm": 1.371498703956604,
      "learning_rate": 2.8334262837399316e-05,
      "loss": 0.9477,
      "step": 7142
    },
    {
      "epoch": 1.004217629692113,
      "grad_norm": 1.556583285331726,
      "learning_rate": 2.8172326502418368e-05,
      "loss": 1.2042,
      "step": 7143
    },
    {
      "epoch": 1.0043582173485168,
      "grad_norm": 1.5986324548721313,
      "learning_rate": 2.8010778338825372e-05,
      "loss": 0.9466,
      "step": 7144
    },
    {
      "epoch": 1.0044988050049206,
      "grad_norm": 1.5237778425216675,
      "learning_rate": 2.784961921965953e-05,
      "loss": 0.918,
      "step": 7145
    },
    {
      "epoch": 1.0046393926613244,
      "grad_norm": 1.6682713031768799,
      "learning_rate": 2.7688850015857594e-05,
      "loss": 1.1109,
      "step": 7146
    },
    {
      "epoch": 1.0047799803177282,
      "grad_norm": 1.494329810142517,
      "learning_rate": 2.7528471596249083e-05,
      "loss": 1.0312,
      "step": 7147
    },
    {
      "epoch": 1.0049205679741318,
      "grad_norm": 1.5172903537750244,
      "learning_rate": 2.73684848275518e-05,
      "loss": 1.0511,
      "step": 7148
    },
    {
      "epoch": 1.0050611556305356,
      "grad_norm": 1.4277836084365845,
      "learning_rate": 2.7208890574366484e-05,
      "loss": 1.0488,
      "step": 7149
    },
    {
      "epoch": 1.0052017432869393,
      "grad_norm": 1.664438009262085,
      "learning_rate": 2.7049689699173496e-05,
      "loss": 0.9892,
      "step": 7150
    },
    {
      "epoch": 1.0053423309433431,
      "grad_norm": 1.4506053924560547,
      "learning_rate": 2.6890883062326457e-05,
      "loss": 1.136,
      "step": 7151
    },
    {
      "epoch": 1.005482918599747,
      "grad_norm": 1.729062795639038,
      "learning_rate": 2.673247152204891e-05,
      "loss": 1.0092,
      "step": 7152
    },
    {
      "epoch": 1.0056235062561507,
      "grad_norm": 1.6269538402557373,
      "learning_rate": 2.657445593442912e-05,
      "loss": 1.0438,
      "step": 7153
    },
    {
      "epoch": 1.0057640939125545,
      "grad_norm": 1.4367226362228394,
      "learning_rate": 2.6416837153415518e-05,
      "loss": 1.1125,
      "step": 7154
    },
    {
      "epoch": 1.0059046815689583,
      "grad_norm": 1.5254462957382202,
      "learning_rate": 2.625961603081213e-05,
      "loss": 1.0288,
      "step": 7155
    },
    {
      "epoch": 1.006045269225362,
      "grad_norm": 1.5985896587371826,
      "learning_rate": 2.6102793416274075e-05,
      "loss": 1.0204,
      "step": 7156
    },
    {
      "epoch": 1.0061858568817659,
      "grad_norm": 1.5975406169891357,
      "learning_rate": 2.5946370157302357e-05,
      "loss": 1.0672,
      "step": 7157
    },
    {
      "epoch": 1.0063264445381694,
      "grad_norm": 1.7659188508987427,
      "learning_rate": 2.579034709924063e-05,
      "loss": 0.9647,
      "step": 7158
    },
    {
      "epoch": 1.0064670321945732,
      "grad_norm": 1.5538651943206787,
      "learning_rate": 2.5634725085268952e-05,
      "loss": 0.9579,
      "step": 7159
    },
    {
      "epoch": 1.006607619850977,
      "grad_norm": 1.5662903785705566,
      "learning_rate": 2.5479504956400513e-05,
      "loss": 0.9807,
      "step": 7160
    },
    {
      "epoch": 1.0067482075073808,
      "grad_norm": 1.5109504461288452,
      "learning_rate": 2.5324687551476534e-05,
      "loss": 1.1528,
      "step": 7161
    },
    {
      "epoch": 1.0068887951637846,
      "grad_norm": 1.3583396673202515,
      "learning_rate": 2.517027370716183e-05,
      "loss": 1.0596,
      "step": 7162
    },
    {
      "epoch": 1.0070293828201884,
      "grad_norm": 1.3149935007095337,
      "learning_rate": 2.501626425794028e-05,
      "loss": 1.3209,
      "step": 7163
    },
    {
      "epoch": 1.0071699704765922,
      "grad_norm": 1.4422473907470703,
      "learning_rate": 2.486266003611034e-05,
      "loss": 1.1718,
      "step": 7164
    },
    {
      "epoch": 1.007310558132996,
      "grad_norm": 1.6137259006500244,
      "learning_rate": 2.4709461871780547e-05,
      "loss": 1.1048,
      "step": 7165
    },
    {
      "epoch": 1.0074511457893998,
      "grad_norm": 1.5410677194595337,
      "learning_rate": 2.4556670592865007e-05,
      "loss": 0.8588,
      "step": 7166
    },
    {
      "epoch": 1.0075917334458036,
      "grad_norm": 1.6106072664260864,
      "learning_rate": 2.440428702507903e-05,
      "loss": 1.1315,
      "step": 7167
    },
    {
      "epoch": 1.0077323211022071,
      "grad_norm": 1.6490023136138916,
      "learning_rate": 2.425231199193414e-05,
      "loss": 1.0868,
      "step": 7168
    },
    {
      "epoch": 1.007872908758611,
      "grad_norm": 1.3732061386108398,
      "learning_rate": 2.4100746314734858e-05,
      "loss": 1.069,
      "step": 7169
    },
    {
      "epoch": 1.0080134964150147,
      "grad_norm": 1.523517370223999,
      "learning_rate": 2.3949590812572643e-05,
      "loss": 1.1345,
      "step": 7170
    },
    {
      "epoch": 1.0081540840714185,
      "grad_norm": 1.5410020351409912,
      "learning_rate": 2.379884630232275e-05,
      "loss": 0.9583,
      "step": 7171
    },
    {
      "epoch": 1.0082946717278223,
      "grad_norm": 1.306692361831665,
      "learning_rate": 2.3648513598639266e-05,
      "loss": 1.0899,
      "step": 7172
    },
    {
      "epoch": 1.008435259384226,
      "grad_norm": 1.4949569702148438,
      "learning_rate": 2.349859351395073e-05,
      "loss": 0.9494,
      "step": 7173
    },
    {
      "epoch": 1.0085758470406299,
      "grad_norm": 1.3254300355911255,
      "learning_rate": 2.334908685845585e-05,
      "loss": 1.0903,
      "step": 7174
    },
    {
      "epoch": 1.0087164346970336,
      "grad_norm": 1.5935840606689453,
      "learning_rate": 2.319999444011919e-05,
      "loss": 1.0589,
      "step": 7175
    },
    {
      "epoch": 1.0088570223534374,
      "grad_norm": 1.4763513803482056,
      "learning_rate": 2.30513170646662e-05,
      "loss": 0.9974,
      "step": 7176
    },
    {
      "epoch": 1.0089976100098412,
      "grad_norm": 1.5197933912277222,
      "learning_rate": 2.2903055535580208e-05,
      "loss": 0.8442,
      "step": 7177
    },
    {
      "epoch": 1.0091381976662448,
      "grad_norm": 1.4842288494110107,
      "learning_rate": 2.275521065409637e-05,
      "loss": 1.0965,
      "step": 7178
    },
    {
      "epoch": 1.0092787853226486,
      "grad_norm": 1.6083124876022339,
      "learning_rate": 2.2607783219198687e-05,
      "loss": 1.2891,
      "step": 7179
    },
    {
      "epoch": 1.0094193729790524,
      "grad_norm": 1.4966883659362793,
      "learning_rate": 2.2460774027615072e-05,
      "loss": 1.0489,
      "step": 7180
    },
    {
      "epoch": 1.0095599606354562,
      "grad_norm": 1.3921725749969482,
      "learning_rate": 2.2314183873813145e-05,
      "loss": 1.1536,
      "step": 7181
    },
    {
      "epoch": 1.00970054829186,
      "grad_norm": 1.5813889503479004,
      "learning_rate": 2.216801354999598e-05,
      "loss": 1.0689,
      "step": 7182
    },
    {
      "epoch": 1.0098411359482637,
      "grad_norm": 1.8177207708358765,
      "learning_rate": 2.2022263846097812e-05,
      "loss": 1.1084,
      "step": 7183
    },
    {
      "epoch": 1.0099817236046675,
      "grad_norm": 1.257386565208435,
      "learning_rate": 2.187693554977974e-05,
      "loss": 1.1432,
      "step": 7184
    },
    {
      "epoch": 1.0101223112610713,
      "grad_norm": 1.3818007707595825,
      "learning_rate": 2.1732029446425605e-05,
      "loss": 0.9816,
      "step": 7185
    },
    {
      "epoch": 1.0102628989174751,
      "grad_norm": 1.5492340326309204,
      "learning_rate": 2.158754631913732e-05,
      "loss": 0.9701,
      "step": 7186
    },
    {
      "epoch": 1.010403486573879,
      "grad_norm": 1.4685629606246948,
      "learning_rate": 2.144348694873113e-05,
      "loss": 1.0169,
      "step": 7187
    },
    {
      "epoch": 1.0105440742302825,
      "grad_norm": 1.3837218284606934,
      "learning_rate": 2.129985211373359e-05,
      "loss": 0.9737,
      "step": 7188
    },
    {
      "epoch": 1.0106846618866863,
      "grad_norm": 1.500636100769043,
      "learning_rate": 2.1156642590376274e-05,
      "loss": 0.979,
      "step": 7189
    },
    {
      "epoch": 1.01082524954309,
      "grad_norm": 1.3121509552001953,
      "learning_rate": 2.1013859152592775e-05,
      "loss": 1.0525,
      "step": 7190
    },
    {
      "epoch": 1.0109658371994938,
      "grad_norm": 1.4288678169250488,
      "learning_rate": 2.0871502572013855e-05,
      "loss": 1.0247,
      "step": 7191
    },
    {
      "epoch": 1.0111064248558976,
      "grad_norm": 1.4085490703582764,
      "learning_rate": 2.0729573617963527e-05,
      "loss": 0.9604,
      "step": 7192
    },
    {
      "epoch": 1.0112470125123014,
      "grad_norm": 1.4107152223587036,
      "learning_rate": 2.058807305745488e-05,
      "loss": 0.962,
      "step": 7193
    },
    {
      "epoch": 1.0113876001687052,
      "grad_norm": 1.7004605531692505,
      "learning_rate": 2.044700165518556e-05,
      "loss": 1.0036,
      "step": 7194
    },
    {
      "epoch": 1.011528187825109,
      "grad_norm": 1.505251169204712,
      "learning_rate": 2.03063601735342e-05,
      "loss": 1.1326,
      "step": 7195
    },
    {
      "epoch": 1.0116687754815128,
      "grad_norm": 1.4610600471496582,
      "learning_rate": 2.016614937255633e-05,
      "loss": 1.0987,
      "step": 7196
    },
    {
      "epoch": 1.0118093631379166,
      "grad_norm": 2.0142178535461426,
      "learning_rate": 2.00263700099794e-05,
      "loss": 1.1393,
      "step": 7197
    },
    {
      "epoch": 1.0119499507943202,
      "grad_norm": 1.7644439935684204,
      "learning_rate": 1.9887022841199677e-05,
      "loss": 0.7873,
      "step": 7198
    },
    {
      "epoch": 1.012090538450724,
      "grad_norm": 1.6255332231521606,
      "learning_rate": 1.974810861927765e-05,
      "loss": 1.0,
      "step": 7199
    },
    {
      "epoch": 1.0122311261071277,
      "grad_norm": 1.768632411956787,
      "learning_rate": 1.9609628094934107e-05,
      "loss": 1.1506,
      "step": 7200
    },
    {
      "epoch": 1.0123717137635315,
      "grad_norm": 1.403295874595642,
      "learning_rate": 1.947158201654612e-05,
      "loss": 1.076,
      "step": 7201
    },
    {
      "epoch": 1.0125123014199353,
      "grad_norm": 1.8181434869766235,
      "learning_rate": 1.9333971130142515e-05,
      "loss": 1.0677,
      "step": 7202
    },
    {
      "epoch": 1.012652889076339,
      "grad_norm": 1.4645088911056519,
      "learning_rate": 1.91967961794009e-05,
      "loss": 1.116,
      "step": 7203
    },
    {
      "epoch": 1.012793476732743,
      "grad_norm": 1.407679557800293,
      "learning_rate": 1.906005790564266e-05,
      "loss": 1.1066,
      "step": 7204
    },
    {
      "epoch": 1.0129340643891467,
      "grad_norm": 1.5630123615264893,
      "learning_rate": 1.8923757047829093e-05,
      "loss": 1.2118,
      "step": 7205
    },
    {
      "epoch": 1.0130746520455505,
      "grad_norm": 1.7162995338439941,
      "learning_rate": 1.878789434255781e-05,
      "loss": 0.9742,
      "step": 7206
    },
    {
      "epoch": 1.0132152397019543,
      "grad_norm": 1.680738091468811,
      "learning_rate": 1.8652470524058884e-05,
      "loss": 1.0014,
      "step": 7207
    },
    {
      "epoch": 1.0133558273583578,
      "grad_norm": 1.5014522075653076,
      "learning_rate": 1.8517486324189916e-05,
      "loss": 1.116,
      "step": 7208
    },
    {
      "epoch": 1.0134964150147616,
      "grad_norm": 1.4506645202636719,
      "learning_rate": 1.838294247243314e-05,
      "loss": 0.9782,
      "step": 7209
    },
    {
      "epoch": 1.0136370026711654,
      "grad_norm": 1.324790358543396,
      "learning_rate": 1.824883969589094e-05,
      "loss": 1.0642,
      "step": 7210
    },
    {
      "epoch": 1.0137775903275692,
      "grad_norm": 2.066535472869873,
      "learning_rate": 1.8115178719282078e-05,
      "loss": 1.0252,
      "step": 7211
    },
    {
      "epoch": 1.013918177983973,
      "grad_norm": 1.3600592613220215,
      "learning_rate": 1.7981960264937814e-05,
      "loss": 0.9262,
      "step": 7212
    },
    {
      "epoch": 1.0140587656403768,
      "grad_norm": 1.3873893022537231,
      "learning_rate": 1.784918505279761e-05,
      "loss": 1.1005,
      "step": 7213
    },
    {
      "epoch": 1.0141993532967806,
      "grad_norm": 1.3841638565063477,
      "learning_rate": 1.7716853800405787e-05,
      "loss": 1.0462,
      "step": 7214
    },
    {
      "epoch": 1.0143399409531844,
      "grad_norm": 1.5362440347671509,
      "learning_rate": 1.75849672229077e-05,
      "loss": 0.9708,
      "step": 7215
    },
    {
      "epoch": 1.0144805286095882,
      "grad_norm": 1.615399718284607,
      "learning_rate": 1.7453526033044998e-05,
      "loss": 1.1249,
      "step": 7216
    },
    {
      "epoch": 1.014621116265992,
      "grad_norm": 1.6218479871749878,
      "learning_rate": 1.732253094115277e-05,
      "loss": 1.0535,
      "step": 7217
    },
    {
      "epoch": 1.0147617039223955,
      "grad_norm": 1.4778813123703003,
      "learning_rate": 1.7191982655155182e-05,
      "loss": 0.9882,
      "step": 7218
    },
    {
      "epoch": 1.0149022915787993,
      "grad_norm": 1.459153175354004,
      "learning_rate": 1.7061881880561792e-05,
      "loss": 0.8351,
      "step": 7219
    },
    {
      "epoch": 1.015042879235203,
      "grad_norm": 1.3794708251953125,
      "learning_rate": 1.693222932046379e-05,
      "loss": 1.1483,
      "step": 7220
    },
    {
      "epoch": 1.0151834668916069,
      "grad_norm": 1.4329463243484497,
      "learning_rate": 1.6803025675529748e-05,
      "loss": 0.9041,
      "step": 7221
    },
    {
      "epoch": 1.0153240545480107,
      "grad_norm": 1.3548088073730469,
      "learning_rate": 1.6674271644002793e-05,
      "loss": 1.1566,
      "step": 7222
    },
    {
      "epoch": 1.0154646422044145,
      "grad_norm": 1.7825151681900024,
      "learning_rate": 1.6545967921695947e-05,
      "loss": 1.0495,
      "step": 7223
    },
    {
      "epoch": 1.0156052298608182,
      "grad_norm": 1.4358363151550293,
      "learning_rate": 1.6418115201988504e-05,
      "loss": 0.9508,
      "step": 7224
    },
    {
      "epoch": 1.015745817517222,
      "grad_norm": 1.3745015859603882,
      "learning_rate": 1.6290714175822598e-05,
      "loss": 1.0892,
      "step": 7225
    },
    {
      "epoch": 1.0158864051736258,
      "grad_norm": 1.45799720287323,
      "learning_rate": 1.6163765531699647e-05,
      "loss": 0.9421,
      "step": 7226
    },
    {
      "epoch": 1.0160269928300296,
      "grad_norm": 1.5777772665023804,
      "learning_rate": 1.6037269955675692e-05,
      "loss": 1.0516,
      "step": 7227
    },
    {
      "epoch": 1.0161675804864332,
      "grad_norm": 1.4504019021987915,
      "learning_rate": 1.5911228131358823e-05,
      "loss": 0.9868,
      "step": 7228
    },
    {
      "epoch": 1.016308168142837,
      "grad_norm": 2.021071195602417,
      "learning_rate": 1.578564073990445e-05,
      "loss": 1.0651,
      "step": 7229
    },
    {
      "epoch": 1.0164487557992408,
      "grad_norm": 1.2744444608688354,
      "learning_rate": 1.5660508460012723e-05,
      "loss": 1.1187,
      "step": 7230
    },
    {
      "epoch": 1.0165893434556446,
      "grad_norm": 1.7761133909225464,
      "learning_rate": 1.5535831967923996e-05,
      "loss": 0.9226,
      "step": 7231
    },
    {
      "epoch": 1.0167299311120483,
      "grad_norm": 1.6068034172058105,
      "learning_rate": 1.541161193741524e-05,
      "loss": 1.1031,
      "step": 7232
    },
    {
      "epoch": 1.0168705187684521,
      "grad_norm": 1.699084758758545,
      "learning_rate": 1.528784903979682e-05,
      "loss": 1.2531,
      "step": 7233
    },
    {
      "epoch": 1.017011106424856,
      "grad_norm": 1.421723484992981,
      "learning_rate": 1.516454394390896e-05,
      "loss": 1.226,
      "step": 7234
    },
    {
      "epoch": 1.0171516940812597,
      "grad_norm": 1.48567533493042,
      "learning_rate": 1.5041697316117231e-05,
      "loss": 1.1472,
      "step": 7235
    },
    {
      "epoch": 1.0172922817376635,
      "grad_norm": 1.5358043909072876,
      "learning_rate": 1.4919309820310046e-05,
      "loss": 1.1655,
      "step": 7236
    },
    {
      "epoch": 1.0174328693940673,
      "grad_norm": 1.6314585208892822,
      "learning_rate": 1.4797382117894087e-05,
      "loss": 1.0491,
      "step": 7237
    },
    {
      "epoch": 1.0175734570504709,
      "grad_norm": 1.4880248308181763,
      "learning_rate": 1.4675914867791807e-05,
      "loss": 0.9249,
      "step": 7238
    },
    {
      "epoch": 1.0177140447068747,
      "grad_norm": 1.290797472000122,
      "learning_rate": 1.4554908726436977e-05,
      "loss": 1.1422,
      "step": 7239
    },
    {
      "epoch": 1.0178546323632784,
      "grad_norm": 1.3299438953399658,
      "learning_rate": 1.4434364347771257e-05,
      "loss": 0.9415,
      "step": 7240
    },
    {
      "epoch": 1.0179952200196822,
      "grad_norm": 1.6577279567718506,
      "learning_rate": 1.4314282383241052e-05,
      "loss": 1.0884,
      "step": 7241
    },
    {
      "epoch": 1.018135807676086,
      "grad_norm": 1.4665013551712036,
      "learning_rate": 1.4194663481794035e-05,
      "loss": 1.1098,
      "step": 7242
    },
    {
      "epoch": 1.0182763953324898,
      "grad_norm": 1.8941113948822021,
      "learning_rate": 1.407550828987486e-05,
      "loss": 1.1212,
      "step": 7243
    },
    {
      "epoch": 1.0184169829888936,
      "grad_norm": 1.3331152200698853,
      "learning_rate": 1.3956817451422489e-05,
      "loss": 1.0709,
      "step": 7244
    },
    {
      "epoch": 1.0185575706452974,
      "grad_norm": 1.3274990320205688,
      "learning_rate": 1.3838591607866658e-05,
      "loss": 1.1874,
      "step": 7245
    },
    {
      "epoch": 1.0186981583017012,
      "grad_norm": 1.4853616952896118,
      "learning_rate": 1.3720831398123646e-05,
      "loss": 0.9209,
      "step": 7246
    },
    {
      "epoch": 1.018838745958105,
      "grad_norm": 1.6424025297164917,
      "learning_rate": 1.360353745859383e-05,
      "loss": 1.0608,
      "step": 7247
    },
    {
      "epoch": 1.0189793336145085,
      "grad_norm": 1.2963334321975708,
      "learning_rate": 1.3486710423157289e-05,
      "loss": 1.1024,
      "step": 7248
    },
    {
      "epoch": 1.0191199212709123,
      "grad_norm": 1.2700539827346802,
      "learning_rate": 1.3370350923171404e-05,
      "loss": 0.869,
      "step": 7249
    },
    {
      "epoch": 1.0192605089273161,
      "grad_norm": 1.6258535385131836,
      "learning_rate": 1.3254459587466627e-05,
      "loss": 1.0479,
      "step": 7250
    },
    {
      "epoch": 1.01940109658372,
      "grad_norm": 1.4653228521347046,
      "learning_rate": 1.3139037042343172e-05,
      "loss": 0.9455,
      "step": 7251
    },
    {
      "epoch": 1.0195416842401237,
      "grad_norm": 1.4495126008987427,
      "learning_rate": 1.302408391156802e-05,
      "loss": 1.1203,
      "step": 7252
    },
    {
      "epoch": 1.0196822718965275,
      "grad_norm": 1.4283028841018677,
      "learning_rate": 1.290960081637157e-05,
      "loss": 0.9379,
      "step": 7253
    },
    {
      "epoch": 1.0198228595529313,
      "grad_norm": 1.3744627237319946,
      "learning_rate": 1.2795588375443546e-05,
      "loss": 0.9157,
      "step": 7254
    },
    {
      "epoch": 1.019963447209335,
      "grad_norm": 1.5965403318405151,
      "learning_rate": 1.268204720493058e-05,
      "loss": 0.9915,
      "step": 7255
    },
    {
      "epoch": 1.0201040348657389,
      "grad_norm": 1.683557152748108,
      "learning_rate": 1.2568977918432056e-05,
      "loss": 0.8233,
      "step": 7256
    },
    {
      "epoch": 1.0202446225221427,
      "grad_norm": 1.3486860990524292,
      "learning_rate": 1.2456381126997696e-05,
      "loss": 1.131,
      "step": 7257
    },
    {
      "epoch": 1.0203852101785462,
      "grad_norm": 1.6343109607696533,
      "learning_rate": 1.2344257439123508e-05,
      "loss": 1.0602,
      "step": 7258
    },
    {
      "epoch": 1.02052579783495,
      "grad_norm": 1.5759952068328857,
      "learning_rate": 1.223260746074859e-05,
      "loss": 1.2356,
      "step": 7259
    },
    {
      "epoch": 1.0206663854913538,
      "grad_norm": 1.4166204929351807,
      "learning_rate": 1.212143179525218e-05,
      "loss": 1.1323,
      "step": 7260
    },
    {
      "epoch": 1.0208069731477576,
      "grad_norm": 1.5639125108718872,
      "learning_rate": 1.2010731043450496e-05,
      "loss": 1.1877,
      "step": 7261
    },
    {
      "epoch": 1.0209475608041614,
      "grad_norm": 1.462023138999939,
      "learning_rate": 1.19005058035927e-05,
      "loss": 0.9564,
      "step": 7262
    },
    {
      "epoch": 1.0210881484605652,
      "grad_norm": 1.435020923614502,
      "learning_rate": 1.1790756671358538e-05,
      "loss": 1.0198,
      "step": 7263
    },
    {
      "epoch": 1.021228736116969,
      "grad_norm": 1.6229645013809204,
      "learning_rate": 1.16814842398547e-05,
      "loss": 0.955,
      "step": 7264
    },
    {
      "epoch": 1.0213693237733728,
      "grad_norm": 1.5336557626724243,
      "learning_rate": 1.1572689099611678e-05,
      "loss": 1.0227,
      "step": 7265
    },
    {
      "epoch": 1.0215099114297765,
      "grad_norm": 1.3408896923065186,
      "learning_rate": 1.1464371838580657e-05,
      "loss": 0.9782,
      "step": 7266
    },
    {
      "epoch": 1.0216504990861803,
      "grad_norm": 1.5095473527908325,
      "learning_rate": 1.1356533042129958e-05,
      "loss": 1.0988,
      "step": 7267
    },
    {
      "epoch": 1.021791086742584,
      "grad_norm": 1.7253886461257935,
      "learning_rate": 1.124917329304266e-05,
      "loss": 1.1919,
      "step": 7268
    },
    {
      "epoch": 1.0219316743989877,
      "grad_norm": 1.5368906259536743,
      "learning_rate": 1.114229317151273e-05,
      "loss": 1.0883,
      "step": 7269
    },
    {
      "epoch": 1.0220722620553915,
      "grad_norm": 1.4344271421432495,
      "learning_rate": 1.103589325514196e-05,
      "loss": 1.0079,
      "step": 7270
    },
    {
      "epoch": 1.0222128497117953,
      "grad_norm": 1.5866864919662476,
      "learning_rate": 1.092997411893728e-05,
      "loss": 1.0185,
      "step": 7271
    },
    {
      "epoch": 1.022353437368199,
      "grad_norm": 1.3767361640930176,
      "learning_rate": 1.0824536335307279e-05,
      "loss": 1.0952,
      "step": 7272
    },
    {
      "epoch": 1.0224940250246028,
      "grad_norm": 1.8260283470153809,
      "learning_rate": 1.0719580474059243e-05,
      "loss": 0.9678,
      "step": 7273
    },
    {
      "epoch": 1.0226346126810066,
      "grad_norm": 1.3202228546142578,
      "learning_rate": 1.0615107102396104e-05,
      "loss": 1.1173,
      "step": 7274
    },
    {
      "epoch": 1.0227752003374104,
      "grad_norm": 1.445241093635559,
      "learning_rate": 1.0511116784913022e-05,
      "loss": 1.0019,
      "step": 7275
    },
    {
      "epoch": 1.0229157879938142,
      "grad_norm": 1.433897852897644,
      "learning_rate": 1.040761008359512e-05,
      "loss": 0.9616,
      "step": 7276
    },
    {
      "epoch": 1.023056375650218,
      "grad_norm": 1.577541708946228,
      "learning_rate": 1.0304587557813694e-05,
      "loss": 1.03,
      "step": 7277
    },
    {
      "epoch": 1.0231969633066216,
      "grad_norm": 1.4882720708847046,
      "learning_rate": 1.0202049764323318e-05,
      "loss": 1.0899,
      "step": 7278
    },
    {
      "epoch": 1.0233375509630254,
      "grad_norm": 1.6175670623779297,
      "learning_rate": 1.0099997257259209e-05,
      "loss": 0.9631,
      "step": 7279
    },
    {
      "epoch": 1.0234781386194292,
      "grad_norm": 1.5523431301116943,
      "learning_rate": 9.998430588133911e-06,
      "loss": 0.9702,
      "step": 7280
    },
    {
      "epoch": 1.023618726275833,
      "grad_norm": 1.8178857564926147,
      "learning_rate": 9.897350305834418e-06,
      "loss": 1.0406,
      "step": 7281
    },
    {
      "epoch": 1.0237593139322367,
      "grad_norm": 1.3142282962799072,
      "learning_rate": 9.796756956619168e-06,
      "loss": 1.0544,
      "step": 7282
    },
    {
      "epoch": 1.0238999015886405,
      "grad_norm": 1.466064214706421,
      "learning_rate": 9.696651084115116e-06,
      "loss": 0.8843,
      "step": 7283
    },
    {
      "epoch": 1.0240404892450443,
      "grad_norm": 1.878197193145752,
      "learning_rate": 9.597033229314823e-06,
      "loss": 0.9256,
      "step": 7284
    },
    {
      "epoch": 1.024181076901448,
      "grad_norm": 1.3471401929855347,
      "learning_rate": 9.497903930573548e-06,
      "loss": 0.8889,
      "step": 7285
    },
    {
      "epoch": 1.024321664557852,
      "grad_norm": 1.489101529121399,
      "learning_rate": 9.399263723605988e-06,
      "loss": 1.1121,
      "step": 7286
    },
    {
      "epoch": 1.0244622522142557,
      "grad_norm": 1.5779095888137817,
      "learning_rate": 9.301113141484174e-06,
      "loss": 0.9978,
      "step": 7287
    },
    {
      "epoch": 1.0246028398706593,
      "grad_norm": 1.7173233032226562,
      "learning_rate": 9.203452714633832e-06,
      "loss": 1.3257,
      "step": 7288
    },
    {
      "epoch": 1.024743427527063,
      "grad_norm": 1.5250251293182373,
      "learning_rate": 9.106282970831692e-06,
      "loss": 1.1331,
      "step": 7289
    },
    {
      "epoch": 1.0248840151834668,
      "grad_norm": 1.3517266511917114,
      "learning_rate": 9.009604435202901e-06,
      "loss": 1.0637,
      "step": 7290
    },
    {
      "epoch": 1.0250246028398706,
      "grad_norm": 1.5456668138504028,
      "learning_rate": 8.913417630218002e-06,
      "loss": 0.8971,
      "step": 7291
    },
    {
      "epoch": 1.0251651904962744,
      "grad_norm": 1.4483518600463867,
      "learning_rate": 8.817723075690064e-06,
      "loss": 0.8366,
      "step": 7292
    },
    {
      "epoch": 1.0253057781526782,
      "grad_norm": 1.5994899272918701,
      "learning_rate": 8.722521288772067e-06,
      "loss": 1.0183,
      "step": 7293
    },
    {
      "epoch": 1.025446365809082,
      "grad_norm": 1.6329482793807983,
      "learning_rate": 8.627812783953626e-06,
      "loss": 0.9254,
      "step": 7294
    },
    {
      "epoch": 1.0255869534654858,
      "grad_norm": 1.572769284248352,
      "learning_rate": 8.533598073059047e-06,
      "loss": 1.0849,
      "step": 7295
    },
    {
      "epoch": 1.0257275411218896,
      "grad_norm": 1.5653319358825684,
      "learning_rate": 8.439877665243823e-06,
      "loss": 1.131,
      "step": 7296
    },
    {
      "epoch": 1.0258681287782934,
      "grad_norm": 1.28908109664917,
      "learning_rate": 8.346652066991978e-06,
      "loss": 1.1773,
      "step": 7297
    },
    {
      "epoch": 1.026008716434697,
      "grad_norm": 1.375524878501892,
      "learning_rate": 8.25392178211375e-06,
      "loss": 1.0515,
      "step": 7298
    },
    {
      "epoch": 1.0261493040911007,
      "grad_norm": 1.7017937898635864,
      "learning_rate": 8.161687311742471e-06,
      "loss": 1.2185,
      "step": 7299
    },
    {
      "epoch": 1.0262898917475045,
      "grad_norm": 1.4430265426635742,
      "learning_rate": 8.069949154332045e-06,
      "loss": 0.9711,
      "step": 7300
    },
    {
      "epoch": 1.0264304794039083,
      "grad_norm": 1.3410766124725342,
      "learning_rate": 7.97870780565415e-06,
      "loss": 1.1916,
      "step": 7301
    },
    {
      "epoch": 1.026571067060312,
      "grad_norm": 1.9308757781982422,
      "learning_rate": 7.887963758795647e-06,
      "loss": 1.127,
      "step": 7302
    },
    {
      "epoch": 1.0267116547167159,
      "grad_norm": 1.5695571899414062,
      "learning_rate": 7.797717504155855e-06,
      "loss": 1.0913,
      "step": 7303
    },
    {
      "epoch": 1.0268522423731197,
      "grad_norm": 1.3744035959243774,
      "learning_rate": 7.707969529443993e-06,
      "loss": 1.0745,
      "step": 7304
    },
    {
      "epoch": 1.0269928300295235,
      "grad_norm": 1.5892186164855957,
      "learning_rate": 7.61872031967622e-06,
      "loss": 1.0108,
      "step": 7305
    },
    {
      "epoch": 1.0271334176859273,
      "grad_norm": 1.5681127309799194,
      "learning_rate": 7.52997035717371e-06,
      "loss": 1.2911,
      "step": 7306
    },
    {
      "epoch": 1.027274005342331,
      "grad_norm": 1.390880823135376,
      "learning_rate": 7.441720121559159e-06,
      "loss": 1.0517,
      "step": 7307
    },
    {
      "epoch": 1.0274145929987346,
      "grad_norm": 1.816503882408142,
      "learning_rate": 7.3539700897548915e-06,
      "loss": 1.1135,
      "step": 7308
    },
    {
      "epoch": 1.0275551806551384,
      "grad_norm": 1.542775273323059,
      "learning_rate": 7.266720735979959e-06,
      "loss": 1.1368,
      "step": 7309
    },
    {
      "epoch": 1.0276957683115422,
      "grad_norm": 1.5376708507537842,
      "learning_rate": 7.1799725317476585e-06,
      "loss": 1.0395,
      "step": 7310
    },
    {
      "epoch": 1.027836355967946,
      "grad_norm": 1.8192800283432007,
      "learning_rate": 7.093725945862972e-06,
      "loss": 0.9325,
      "step": 7311
    },
    {
      "epoch": 1.0279769436243498,
      "grad_norm": 1.6886401176452637,
      "learning_rate": 7.0079814444200975e-06,
      "loss": 1.0644,
      "step": 7312
    },
    {
      "epoch": 1.0281175312807536,
      "grad_norm": 1.538917064666748,
      "learning_rate": 6.92273949079959e-06,
      "loss": 1.0107,
      "step": 7313
    },
    {
      "epoch": 1.0282581189371574,
      "grad_norm": 1.4504956007003784,
      "learning_rate": 6.838000545666601e-06,
      "loss": 1.0025,
      "step": 7314
    },
    {
      "epoch": 1.0283987065935611,
      "grad_norm": 1.2843478918075562,
      "learning_rate": 6.7537650669674765e-06,
      "loss": 1.1566,
      "step": 7315
    },
    {
      "epoch": 1.028539294249965,
      "grad_norm": 1.5761348009109497,
      "learning_rate": 6.670033509927975e-06,
      "loss": 1.2232,
      "step": 7316
    },
    {
      "epoch": 1.0286798819063687,
      "grad_norm": 1.1856194734573364,
      "learning_rate": 6.586806327050498e-06,
      "loss": 1.0516,
      "step": 7317
    },
    {
      "epoch": 1.0288204695627723,
      "grad_norm": 1.3811259269714355,
      "learning_rate": 6.504083968111707e-06,
      "loss": 1.1098,
      "step": 7318
    },
    {
      "epoch": 1.028961057219176,
      "grad_norm": 1.7371106147766113,
      "learning_rate": 6.421866880160077e-06,
      "loss": 0.8946,
      "step": 7319
    },
    {
      "epoch": 1.0291016448755799,
      "grad_norm": 1.7434462308883667,
      "learning_rate": 6.340155507513534e-06,
      "loss": 1.0253,
      "step": 7320
    },
    {
      "epoch": 1.0292422325319837,
      "grad_norm": 1.4227527379989624,
      "learning_rate": 6.258950291756971e-06,
      "loss": 0.9328,
      "step": 7321
    },
    {
      "epoch": 1.0293828201883874,
      "grad_norm": 1.5113885402679443,
      "learning_rate": 6.178251671739932e-06,
      "loss": 1.1261,
      "step": 7322
    },
    {
      "epoch": 1.0295234078447912,
      "grad_norm": 1.529666781425476,
      "learning_rate": 6.098060083574264e-06,
      "loss": 1.0405,
      "step": 7323
    },
    {
      "epoch": 1.029663995501195,
      "grad_norm": 1.387815237045288,
      "learning_rate": 6.018375960631484e-06,
      "loss": 0.9626,
      "step": 7324
    },
    {
      "epoch": 1.0298045831575988,
      "grad_norm": 1.4615569114685059,
      "learning_rate": 5.939199733541056e-06,
      "loss": 1.0175,
      "step": 7325
    },
    {
      "epoch": 1.0299451708140026,
      "grad_norm": 1.5500293970108032,
      "learning_rate": 5.860531830187299e-06,
      "loss": 0.9669,
      "step": 7326
    },
    {
      "epoch": 1.0300857584704064,
      "grad_norm": 1.4499295949935913,
      "learning_rate": 5.782372675707626e-06,
      "loss": 1.1545,
      "step": 7327
    },
    {
      "epoch": 1.03022634612681,
      "grad_norm": 1.7740131616592407,
      "learning_rate": 5.704722692490061e-06,
      "loss": 1.0551,
      "step": 7328
    },
    {
      "epoch": 1.0303669337832138,
      "grad_norm": 1.4130096435546875,
      "learning_rate": 5.62758230017093e-06,
      "loss": 1.1935,
      "step": 7329
    },
    {
      "epoch": 1.0305075214396175,
      "grad_norm": 1.644066333770752,
      "learning_rate": 5.5509519156326405e-06,
      "loss": 0.9971,
      "step": 7330
    },
    {
      "epoch": 1.0306481090960213,
      "grad_norm": 1.6475977897644043,
      "learning_rate": 5.4748319530014584e-06,
      "loss": 1.0471,
      "step": 7331
    },
    {
      "epoch": 1.0307886967524251,
      "grad_norm": 1.8083319664001465,
      "learning_rate": 5.399222823645023e-06,
      "loss": 1.0407,
      "step": 7332
    },
    {
      "epoch": 1.030929284408829,
      "grad_norm": 1.4511609077453613,
      "learning_rate": 5.3241249361706935e-06,
      "loss": 0.9441,
      "step": 7333
    },
    {
      "epoch": 1.0310698720652327,
      "grad_norm": 1.4462833404541016,
      "learning_rate": 5.249538696422596e-06,
      "loss": 0.9439,
      "step": 7334
    },
    {
      "epoch": 1.0312104597216365,
      "grad_norm": 1.5828665494918823,
      "learning_rate": 5.175464507480021e-06,
      "loss": 1.0505,
      "step": 7335
    },
    {
      "epoch": 1.0313510473780403,
      "grad_norm": 1.274104356765747,
      "learning_rate": 5.101902769654998e-06,
      "loss": 1.1646,
      "step": 7336
    },
    {
      "epoch": 1.031491635034444,
      "grad_norm": 1.4121143817901611,
      "learning_rate": 5.028853880490136e-06,
      "loss": 0.9409,
      "step": 7337
    },
    {
      "epoch": 1.0316322226908476,
      "grad_norm": 1.415476679801941,
      "learning_rate": 4.95631823475654e-06,
      "loss": 0.9444,
      "step": 7338
    },
    {
      "epoch": 1.0317728103472514,
      "grad_norm": 1.5451878309249878,
      "learning_rate": 4.884296224451612e-06,
      "loss": 1.1847,
      "step": 7339
    },
    {
      "epoch": 1.0319133980036552,
      "grad_norm": 1.5770334005355835,
      "learning_rate": 4.812788238796972e-06,
      "loss": 1.0382,
      "step": 7340
    },
    {
      "epoch": 1.032053985660059,
      "grad_norm": 1.3503503799438477,
      "learning_rate": 4.741794664236421e-06,
      "loss": 0.8386,
      "step": 7341
    },
    {
      "epoch": 1.0321945733164628,
      "grad_norm": 1.8396228551864624,
      "learning_rate": 4.67131588443358e-06,
      "loss": 1.008,
      "step": 7342
    },
    {
      "epoch": 1.0323351609728666,
      "grad_norm": 1.6986032724380493,
      "learning_rate": 4.601352280270144e-06,
      "loss": 0.9977,
      "step": 7343
    },
    {
      "epoch": 1.0324757486292704,
      "grad_norm": 1.4830639362335205,
      "learning_rate": 4.531904229843864e-06,
      "loss": 1.0261,
      "step": 7344
    },
    {
      "epoch": 1.0326163362856742,
      "grad_norm": 1.640820026397705,
      "learning_rate": 4.4629721084659904e-06,
      "loss": 0.9801,
      "step": 7345
    },
    {
      "epoch": 1.032756923942078,
      "grad_norm": 1.3836439847946167,
      "learning_rate": 4.3945562886598015e-06,
      "loss": 0.9295,
      "step": 7346
    },
    {
      "epoch": 1.0328975115984818,
      "grad_norm": 1.382209300994873,
      "learning_rate": 4.326657140158341e-06,
      "loss": 1.1024,
      "step": 7347
    },
    {
      "epoch": 1.0330380992548853,
      "grad_norm": 1.6988322734832764,
      "learning_rate": 4.259275029902454e-06,
      "loss": 1.0463,
      "step": 7348
    },
    {
      "epoch": 1.0331786869112891,
      "grad_norm": 1.341835856437683,
      "learning_rate": 4.192410322038853e-06,
      "loss": 1.0303,
      "step": 7349
    },
    {
      "epoch": 1.033319274567693,
      "grad_norm": 1.4652339220046997,
      "learning_rate": 4.126063377917922e-06,
      "loss": 1.1759,
      "step": 7350
    },
    {
      "epoch": 1.0334598622240967,
      "grad_norm": 1.3527277708053589,
      "learning_rate": 4.06023455609209e-06,
      "loss": 1.2372,
      "step": 7351
    },
    {
      "epoch": 1.0336004498805005,
      "grad_norm": 1.5057971477508545,
      "learning_rate": 3.994924212313877e-06,
      "loss": 1.0617,
      "step": 7352
    },
    {
      "epoch": 1.0337410375369043,
      "grad_norm": 1.3917059898376465,
      "learning_rate": 3.930132699533528e-06,
      "loss": 1.0283,
      "step": 7353
    },
    {
      "epoch": 1.033881625193308,
      "grad_norm": 1.2661685943603516,
      "learning_rate": 3.8658603678976555e-06,
      "loss": 0.9592,
      "step": 7354
    },
    {
      "epoch": 1.0340222128497119,
      "grad_norm": 1.4580186605453491,
      "learning_rate": 3.802107564747026e-06,
      "loss": 1.0403,
      "step": 7355
    },
    {
      "epoch": 1.0341628005061156,
      "grad_norm": 1.604075312614441,
      "learning_rate": 3.7388746346147863e-06,
      "loss": 1.1236,
      "step": 7356
    },
    {
      "epoch": 1.0343033881625194,
      "grad_norm": 1.5621041059494019,
      "learning_rate": 3.67616191922463e-06,
      "loss": 0.9988,
      "step": 7357
    },
    {
      "epoch": 1.034443975818923,
      "grad_norm": 1.4339983463287354,
      "learning_rate": 3.613969757488711e-06,
      "loss": 1.0961,
      "step": 7358
    },
    {
      "epoch": 1.0345845634753268,
      "grad_norm": 1.4659901857376099,
      "learning_rate": 3.552298485506278e-06,
      "loss": 1.0885,
      "step": 7359
    },
    {
      "epoch": 1.0347251511317306,
      "grad_norm": 1.699198603630066,
      "learning_rate": 3.4911484365614756e-06,
      "loss": 0.9589,
      "step": 7360
    },
    {
      "epoch": 1.0348657387881344,
      "grad_norm": 1.516530990600586,
      "learning_rate": 3.4305199411215126e-06,
      "loss": 0.9974,
      "step": 7361
    },
    {
      "epoch": 1.0350063264445382,
      "grad_norm": 1.6149194240570068,
      "learning_rate": 3.3704133268351645e-06,
      "loss": 1.0901,
      "step": 7362
    },
    {
      "epoch": 1.035146914100942,
      "grad_norm": 1.422570824623108,
      "learning_rate": 3.3108289185309395e-06,
      "loss": 1.0441,
      "step": 7363
    },
    {
      "epoch": 1.0352875017573457,
      "grad_norm": 1.6459753513336182,
      "learning_rate": 3.2517670382150032e-06,
      "loss": 0.9776,
      "step": 7364
    },
    {
      "epoch": 1.0354280894137495,
      "grad_norm": 1.5483568906784058,
      "learning_rate": 3.193228005069837e-06,
      "loss": 0.9789,
      "step": 7365
    },
    {
      "epoch": 1.0355686770701533,
      "grad_norm": 1.5683586597442627,
      "learning_rate": 3.1352121354523035e-06,
      "loss": 1.0192,
      "step": 7366
    },
    {
      "epoch": 1.035709264726557,
      "grad_norm": 1.6272835731506348,
      "learning_rate": 3.0777197428919825e-06,
      "loss": 0.9916,
      "step": 7367
    },
    {
      "epoch": 1.0358498523829607,
      "grad_norm": 1.5113105773925781,
      "learning_rate": 3.0207511380895504e-06,
      "loss": 1.0993,
      "step": 7368
    },
    {
      "epoch": 1.0359904400393645,
      "grad_norm": 1.5294599533081055,
      "learning_rate": 2.964306628914848e-06,
      "loss": 0.9523,
      "step": 7369
    },
    {
      "epoch": 1.0361310276957683,
      "grad_norm": 1.360330581665039,
      "learning_rate": 2.908386520405515e-06,
      "loss": 1.0931,
      "step": 7370
    },
    {
      "epoch": 1.036271615352172,
      "grad_norm": 1.2664473056793213,
      "learning_rate": 2.8529911147653466e-06,
      "loss": 1.0941,
      "step": 7371
    },
    {
      "epoch": 1.0364122030085758,
      "grad_norm": 1.3432201147079468,
      "learning_rate": 2.7981207113622732e-06,
      "loss": 1.1324,
      "step": 7372
    },
    {
      "epoch": 1.0365527906649796,
      "grad_norm": 1.5031368732452393,
      "learning_rate": 2.7437756067271834e-06,
      "loss": 0.9085,
      "step": 7373
    },
    {
      "epoch": 1.0366933783213834,
      "grad_norm": 1.7502188682556152,
      "learning_rate": 2.689956094552104e-06,
      "loss": 1.079,
      "step": 7374
    },
    {
      "epoch": 1.0368339659777872,
      "grad_norm": 1.5532528162002563,
      "learning_rate": 2.636662465688644e-06,
      "loss": 1.1044,
      "step": 7375
    },
    {
      "epoch": 1.036974553634191,
      "grad_norm": 1.4202972650527954,
      "learning_rate": 2.5838950081464753e-06,
      "loss": 1.1916,
      "step": 7376
    },
    {
      "epoch": 1.0371151412905948,
      "grad_norm": 1.2843525409698486,
      "learning_rate": 2.5316540070915774e-06,
      "loss": 1.0023,
      "step": 7377
    },
    {
      "epoch": 1.0372557289469984,
      "grad_norm": 1.330298900604248,
      "learning_rate": 2.4799397448450944e-06,
      "loss": 1.0228,
      "step": 7378
    },
    {
      "epoch": 1.0373963166034021,
      "grad_norm": 1.5532774925231934,
      "learning_rate": 2.428752500881426e-06,
      "loss": 1.0088,
      "step": 7379
    },
    {
      "epoch": 1.037536904259806,
      "grad_norm": 1.4063657522201538,
      "learning_rate": 2.378092551826794e-06,
      "loss": 1.0501,
      "step": 7380
    },
    {
      "epoch": 1.0376774919162097,
      "grad_norm": 1.4077318906784058,
      "learning_rate": 2.3279601714578993e-06,
      "loss": 1.0724,
      "step": 7381
    },
    {
      "epoch": 1.0378180795726135,
      "grad_norm": 1.3763009309768677,
      "learning_rate": 2.278355630700457e-06,
      "loss": 1.0208,
      "step": 7382
    },
    {
      "epoch": 1.0379586672290173,
      "grad_norm": 1.6546697616577148,
      "learning_rate": 2.2292791976273984e-06,
      "loss": 1.0325,
      "step": 7383
    },
    {
      "epoch": 1.038099254885421,
      "grad_norm": 1.4320520162582397,
      "learning_rate": 2.1807311374578365e-06,
      "loss": 1.0778,
      "step": 7384
    },
    {
      "epoch": 1.0382398425418249,
      "grad_norm": 1.5389633178710938,
      "learning_rate": 2.1327117125552907e-06,
      "loss": 1.0948,
      "step": 7385
    },
    {
      "epoch": 1.0383804301982287,
      "grad_norm": 1.8929367065429688,
      "learning_rate": 2.085221182426633e-06,
      "loss": 0.9378,
      "step": 7386
    },
    {
      "epoch": 1.0385210178546325,
      "grad_norm": 1.5684199333190918,
      "learning_rate": 2.038259803720344e-06,
      "loss": 0.9484,
      "step": 7387
    },
    {
      "epoch": 1.038661605511036,
      "grad_norm": 1.3743641376495361,
      "learning_rate": 1.9918278302252346e-06,
      "loss": 1.1932,
      "step": 7388
    },
    {
      "epoch": 1.0388021931674398,
      "grad_norm": 1.368688941001892,
      "learning_rate": 1.945925512869151e-06,
      "loss": 0.9626,
      "step": 7389
    },
    {
      "epoch": 1.0389427808238436,
      "grad_norm": 1.7270339727401733,
      "learning_rate": 1.9005530997176612e-06,
      "loss": 1.012,
      "step": 7390
    },
    {
      "epoch": 1.0390833684802474,
      "grad_norm": 1.357089638710022,
      "learning_rate": 1.8557108359724572e-06,
      "loss": 1.0802,
      "step": 7391
    },
    {
      "epoch": 1.0392239561366512,
      "grad_norm": 1.68410325050354,
      "learning_rate": 1.8113989639703233e-06,
      "loss": 0.996,
      "step": 7392
    },
    {
      "epoch": 1.039364543793055,
      "grad_norm": 1.4111616611480713,
      "learning_rate": 1.7676177231815361e-06,
      "loss": 0.9108,
      "step": 7393
    },
    {
      "epoch": 1.0395051314494588,
      "grad_norm": 1.4396833181381226,
      "learning_rate": 1.7243673502089775e-06,
      "loss": 1.0703,
      "step": 7394
    },
    {
      "epoch": 1.0396457191058626,
      "grad_norm": 1.816718578338623,
      "learning_rate": 1.6816480787864464e-06,
      "loss": 0.8724,
      "step": 7395
    },
    {
      "epoch": 1.0397863067622664,
      "grad_norm": 1.956730842590332,
      "learning_rate": 1.6394601397775488e-06,
      "loss": 1.045,
      "step": 7396
    },
    {
      "epoch": 1.0399268944186701,
      "grad_norm": 1.4896612167358398,
      "learning_rate": 1.597803761174488e-06,
      "loss": 0.9701,
      "step": 7397
    },
    {
      "epoch": 1.0400674820750737,
      "grad_norm": 1.377267837524414,
      "learning_rate": 1.5566791680969417e-06,
      "loss": 1.0179,
      "step": 7398
    },
    {
      "epoch": 1.0402080697314775,
      "grad_norm": 1.7097358703613281,
      "learning_rate": 1.516086582790477e-06,
      "loss": 1.096,
      "step": 7399
    },
    {
      "epoch": 1.0403486573878813,
      "grad_norm": 1.7153594493865967,
      "learning_rate": 1.476026224625715e-06,
      "loss": 0.9676,
      "step": 7400
    },
    {
      "epoch": 1.040489245044285,
      "grad_norm": 1.674796462059021,
      "learning_rate": 1.4364983100970786e-06,
      "loss": 0.9914,
      "step": 7401
    },
    {
      "epoch": 1.0406298327006889,
      "grad_norm": 1.5090444087982178,
      "learning_rate": 1.3975030528213918e-06,
      "loss": 1.01,
      "step": 7402
    },
    {
      "epoch": 1.0407704203570927,
      "grad_norm": 1.565069317817688,
      "learning_rate": 1.3590406635370257e-06,
      "loss": 1.0363,
      "step": 7403
    },
    {
      "epoch": 1.0409110080134965,
      "grad_norm": 1.784616231918335,
      "learning_rate": 1.3211113501024885e-06,
      "loss": 1.0202,
      "step": 7404
    },
    {
      "epoch": 1.0410515956699002,
      "grad_norm": 1.7871835231781006,
      "learning_rate": 1.2837153174956041e-06,
      "loss": 1.1047,
      "step": 7405
    },
    {
      "epoch": 1.041192183326304,
      "grad_norm": 1.5277364253997803,
      "learning_rate": 1.2468527678121567e-06,
      "loss": 1.2073,
      "step": 7406
    },
    {
      "epoch": 1.0413327709827078,
      "grad_norm": 1.5287423133850098,
      "learning_rate": 1.210523900264826e-06,
      "loss": 1.091,
      "step": 7407
    },
    {
      "epoch": 1.0414733586391114,
      "grad_norm": 1.4245407581329346,
      "learning_rate": 1.1747289111822102e-06,
      "loss": 1.0364,
      "step": 7408
    },
    {
      "epoch": 1.0416139462955152,
      "grad_norm": 1.4376643896102905,
      "learning_rate": 1.1394679940078145e-06,
      "loss": 0.9161,
      "step": 7409
    },
    {
      "epoch": 1.041754533951919,
      "grad_norm": 1.700852394104004,
      "learning_rate": 1.1047413392987315e-06,
      "loss": 0.9123,
      "step": 7410
    },
    {
      "epoch": 1.0418951216083228,
      "grad_norm": 1.9847846031188965,
      "learning_rate": 1.0705491347249519e-06,
      "loss": 1.0002,
      "step": 7411
    },
    {
      "epoch": 1.0420357092647266,
      "grad_norm": 1.4610950946807861,
      "learning_rate": 1.0368915650680212e-06,
      "loss": 0.9523,
      "step": 7412
    },
    {
      "epoch": 1.0421762969211303,
      "grad_norm": 1.5872931480407715,
      "learning_rate": 1.0037688122203958e-06,
      "loss": 0.999,
      "step": 7413
    },
    {
      "epoch": 1.0423168845775341,
      "grad_norm": 1.535187840461731,
      "learning_rate": 9.711810551841783e-07,
      "loss": 0.9944,
      "step": 7414
    },
    {
      "epoch": 1.042457472233938,
      "grad_norm": 1.6387301683425903,
      "learning_rate": 9.391284700701941e-07,
      "loss": 1.1221,
      "step": 7415
    },
    {
      "epoch": 1.0425980598903417,
      "grad_norm": 1.8672726154327393,
      "learning_rate": 9.076112300971607e-07,
      "loss": 1.0116,
      "step": 7416
    },
    {
      "epoch": 1.0427386475467455,
      "grad_norm": 1.4490668773651123,
      "learning_rate": 8.766295055907315e-07,
      "loss": 1.0497,
      "step": 7417
    },
    {
      "epoch": 1.042879235203149,
      "grad_norm": 1.4482072591781616,
      "learning_rate": 8.461834639823863e-07,
      "loss": 1.1171,
      "step": 7418
    },
    {
      "epoch": 1.0430198228595529,
      "grad_norm": 1.4973803758621216,
      "learning_rate": 8.16273269808776e-07,
      "loss": 1.0689,
      "step": 7419
    },
    {
      "epoch": 1.0431604105159566,
      "grad_norm": 1.6939635276794434,
      "learning_rate": 7.868990847106683e-07,
      "loss": 1.0822,
      "step": 7420
    },
    {
      "epoch": 1.0433009981723604,
      "grad_norm": 1.6266913414001465,
      "learning_rate": 7.580610674321587e-07,
      "loss": 1.0916,
      "step": 7421
    },
    {
      "epoch": 1.0434415858287642,
      "grad_norm": 1.3450113534927368,
      "learning_rate": 7.297593738197938e-07,
      "loss": 1.0565,
      "step": 7422
    },
    {
      "epoch": 1.043582173485168,
      "grad_norm": 1.787305474281311,
      "learning_rate": 7.019941568216171e-07,
      "loss": 0.9877,
      "step": 7423
    },
    {
      "epoch": 1.0437227611415718,
      "grad_norm": 1.5477948188781738,
      "learning_rate": 6.747655664866015e-07,
      "loss": 1.0593,
      "step": 7424
    },
    {
      "epoch": 1.0438633487979756,
      "grad_norm": 1.3710073232650757,
      "learning_rate": 6.480737499635847e-07,
      "loss": 1.0522,
      "step": 7425
    },
    {
      "epoch": 1.0440039364543794,
      "grad_norm": 1.4484994411468506,
      "learning_rate": 6.219188515005469e-07,
      "loss": 1.0035,
      "step": 7426
    },
    {
      "epoch": 1.0441445241107832,
      "grad_norm": 1.4611567258834839,
      "learning_rate": 5.963010124439116e-07,
      "loss": 0.9474,
      "step": 7427
    },
    {
      "epoch": 1.0442851117671867,
      "grad_norm": 1.5895005464553833,
      "learning_rate": 5.712203712377018e-07,
      "loss": 0.8511,
      "step": 7428
    },
    {
      "epoch": 1.0444256994235905,
      "grad_norm": 1.3179590702056885,
      "learning_rate": 5.46677063422818e-07,
      "loss": 1.0514,
      "step": 7429
    },
    {
      "epoch": 1.0445662870799943,
      "grad_norm": 1.781865119934082,
      "learning_rate": 5.226712216363172e-07,
      "loss": 0.982,
      "step": 7430
    },
    {
      "epoch": 1.0447068747363981,
      "grad_norm": 1.7055500745773315,
      "learning_rate": 4.992029756105909e-07,
      "loss": 1.0635,
      "step": 7431
    },
    {
      "epoch": 1.044847462392802,
      "grad_norm": 1.402642011642456,
      "learning_rate": 4.7627245217291004e-07,
      "loss": 1.1124,
      "step": 7432
    },
    {
      "epoch": 1.0449880500492057,
      "grad_norm": 1.835249900817871,
      "learning_rate": 4.53879775244459e-07,
      "loss": 1.1382,
      "step": 7433
    },
    {
      "epoch": 1.0451286377056095,
      "grad_norm": 1.7065354585647583,
      "learning_rate": 4.3202506583982504e-07,
      "loss": 0.9466,
      "step": 7434
    },
    {
      "epoch": 1.0452692253620133,
      "grad_norm": 1.2742799520492554,
      "learning_rate": 4.107084420663099e-07,
      "loss": 1.0804,
      "step": 7435
    },
    {
      "epoch": 1.045409813018417,
      "grad_norm": 1.5067863464355469,
      "learning_rate": 3.8993001912330795e-07,
      "loss": 1.0044,
      "step": 7436
    },
    {
      "epoch": 1.0455504006748209,
      "grad_norm": 1.4375189542770386,
      "learning_rate": 3.6968990930164035e-07,
      "loss": 0.9066,
      "step": 7437
    },
    {
      "epoch": 1.0456909883312244,
      "grad_norm": 1.5230399370193481,
      "learning_rate": 3.4998822198298865e-07,
      "loss": 1.0881,
      "step": 7438
    },
    {
      "epoch": 1.0458315759876282,
      "grad_norm": 1.6760952472686768,
      "learning_rate": 3.30825063639284e-07,
      "loss": 1.0949,
      "step": 7439
    },
    {
      "epoch": 1.045972163644032,
      "grad_norm": 1.4915120601654053,
      "learning_rate": 3.122005378321524e-07,
      "loss": 0.9557,
      "step": 7440
    },
    {
      "epoch": 1.0461127513004358,
      "grad_norm": 1.401726484298706,
      "learning_rate": 2.9411474521232605e-07,
      "loss": 1.036,
      "step": 7441
    },
    {
      "epoch": 1.0462533389568396,
      "grad_norm": 1.5590780973434448,
      "learning_rate": 2.765677835190772e-07,
      "loss": 1.0347,
      "step": 7442
    },
    {
      "epoch": 1.0463939266132434,
      "grad_norm": 1.6937497854232788,
      "learning_rate": 2.5955974757981836e-07,
      "loss": 0.9473,
      "step": 7443
    },
    {
      "epoch": 1.0465345142696472,
      "grad_norm": 1.486203908920288,
      "learning_rate": 2.4309072930942536e-07,
      "loss": 1.0475,
      "step": 7444
    },
    {
      "epoch": 1.046675101926051,
      "grad_norm": 1.490031361579895,
      "learning_rate": 2.2716081770981502e-07,
      "loss": 1.0657,
      "step": 7445
    },
    {
      "epoch": 1.0468156895824547,
      "grad_norm": 1.538040041923523,
      "learning_rate": 2.117700988694793e-07,
      "loss": 0.9285,
      "step": 7446
    },
    {
      "epoch": 1.0469562772388585,
      "grad_norm": 1.4191606044769287,
      "learning_rate": 1.9691865596299653e-07,
      "loss": 1.2666,
      "step": 7447
    },
    {
      "epoch": 1.047096864895262,
      "grad_norm": 1.401591181755066,
      "learning_rate": 1.826065692506096e-07,
      "loss": 1.155,
      "step": 7448
    },
    {
      "epoch": 1.047237452551666,
      "grad_norm": 1.326306700706482,
      "learning_rate": 1.6883391607774857e-07,
      "loss": 1.1122,
      "step": 7449
    },
    {
      "epoch": 1.0473780402080697,
      "grad_norm": 1.423309564590454,
      "learning_rate": 1.556007708746199e-07,
      "loss": 1.1154,
      "step": 7450
    },
    {
      "epoch": 1.0475186278644735,
      "grad_norm": 1.9978773593902588,
      "learning_rate": 1.429072051558511e-07,
      "loss": 1.0549,
      "step": 7451
    },
    {
      "epoch": 1.0476592155208773,
      "grad_norm": 1.4259705543518066,
      "learning_rate": 1.3075328752006898e-07,
      "loss": 1.0509,
      "step": 7452
    },
    {
      "epoch": 1.047799803177281,
      "grad_norm": 1.561458706855774,
      "learning_rate": 1.1913908364949988e-07,
      "loss": 0.9862,
      "step": 7453
    },
    {
      "epoch": 1.0479403908336848,
      "grad_norm": 1.5968376398086548,
      "learning_rate": 1.0806465630966989e-07,
      "loss": 1.0862,
      "step": 7454
    },
    {
      "epoch": 1.0480809784900886,
      "grad_norm": 1.5060172080993652,
      "learning_rate": 9.753006534906073e-08,
      "loss": 1.0206,
      "step": 7455
    },
    {
      "epoch": 1.0482215661464924,
      "grad_norm": 1.7504246234893799,
      "learning_rate": 8.753536769874337e-08,
      "loss": 1.044,
      "step": 7456
    },
    {
      "epoch": 1.0483621538028962,
      "grad_norm": 1.5773710012435913,
      "learning_rate": 7.808061737208938e-08,
      "loss": 1.0635,
      "step": 7457
    },
    {
      "epoch": 1.0485027414592998,
      "grad_norm": 1.5233796834945679,
      "learning_rate": 6.916586546450444e-08,
      "loss": 1.2576,
      "step": 7458
    },
    {
      "epoch": 1.0486433291157036,
      "grad_norm": 1.6016193628311157,
      "learning_rate": 6.079116015311747e-08,
      "loss": 0.8984,
      "step": 7459
    },
    {
      "epoch": 1.0487839167721074,
      "grad_norm": 1.4310550689697266,
      "learning_rate": 5.295654669655869e-08,
      "loss": 0.8607,
      "step": 7460
    },
    {
      "epoch": 1.0489245044285112,
      "grad_norm": 1.3714909553527832,
      "learning_rate": 4.566206743465973e-08,
      "loss": 1.0885,
      "step": 7461
    },
    {
      "epoch": 1.049065092084915,
      "grad_norm": 1.483629822731018,
      "learning_rate": 3.8907761788298247e-08,
      "loss": 1.1072,
      "step": 7462
    },
    {
      "epoch": 1.0492056797413187,
      "grad_norm": 1.5710159540176392,
      "learning_rate": 3.26936662590871e-08,
      "loss": 0.9771,
      "step": 7463
    },
    {
      "epoch": 1.0493462673977225,
      "grad_norm": 1.3411877155303955,
      "learning_rate": 2.7019814429274372e-08,
      "loss": 0.9996,
      "step": 7464
    },
    {
      "epoch": 1.0494868550541263,
      "grad_norm": 1.4881250858306885,
      "learning_rate": 2.1886236961521365e-08,
      "loss": 1.1029,
      "step": 7465
    },
    {
      "epoch": 1.04962744271053,
      "grad_norm": 1.8134263753890991,
      "learning_rate": 1.7292961598724956e-08,
      "loss": 1.0964,
      "step": 7466
    },
    {
      "epoch": 1.0497680303669337,
      "grad_norm": 1.6593881845474243,
      "learning_rate": 1.3240013163884346e-08,
      "loss": 0.8947,
      "step": 7467
    },
    {
      "epoch": 1.0499086180233375,
      "grad_norm": 1.267920732498169,
      "learning_rate": 9.727413559945664e-09,
      "loss": 1.0521,
      "step": 7468
    },
    {
      "epoch": 1.0500492056797412,
      "grad_norm": 1.6211906671524048,
      "learning_rate": 6.755181769724228e-09,
      "loss": 1.0345,
      "step": 7469
    },
    {
      "epoch": 1.050189793336145,
      "grad_norm": 1.3643829822540283,
      "learning_rate": 4.323333855760226e-09,
      "loss": 0.9688,
      "step": 7470
    },
    {
      "epoch": 1.0503303809925488,
      "grad_norm": 1.466988444328308,
      "learning_rate": 2.4318829602631986e-09,
      "loss": 0.9335,
      "step": 7471
    },
    {
      "epoch": 1.0504709686489526,
      "grad_norm": 1.3193769454956055,
      "learning_rate": 1.0808393050121267e-09,
      "loss": 1.0899,
      "step": 7472
    },
    {
      "epoch": 1.0506115563053564,
      "grad_norm": 1.464869737625122,
      "learning_rate": 2.702101913221178e-10,
      "loss": 1.0823,
      "step": 7473
    },
    {
      "epoch": 1.0507521439617602,
      "grad_norm": 1.4806318283081055,
      "learning_rate": 0.0,
      "loss": 0.9492,
      "step": 7474
    },
    {
      "epoch": 1.050892731618164,
      "grad_norm": 1.7873996496200562,
      "learning_rate": 2.702101913221178e-10,
      "loss": 1.2153,
      "step": 7475
    },
    {
      "epoch": 1.0510333192745678,
      "grad_norm": 1.7119433879852295,
      "learning_rate": 1.0808393050121267e-09,
      "loss": 0.9788,
      "step": 7476
    },
    {
      "epoch": 1.0511739069309716,
      "grad_norm": 1.6421407461166382,
      "learning_rate": 2.4318829602631986e-09,
      "loss": 0.8924,
      "step": 7477
    },
    {
      "epoch": 1.0513144945873751,
      "grad_norm": 1.535558819770813,
      "learning_rate": 4.323333855760226e-09,
      "loss": 1.0256,
      "step": 7478
    },
    {
      "epoch": 1.051455082243779,
      "grad_norm": 1.5903263092041016,
      "learning_rate": 6.755181769724228e-09,
      "loss": 0.972,
      "step": 7479
    },
    {
      "epoch": 1.0515956699001827,
      "grad_norm": 1.2777702808380127,
      "learning_rate": 9.727413559945664e-09,
      "loss": 1.1756,
      "step": 7480
    },
    {
      "epoch": 1.0517362575565865,
      "grad_norm": 1.5013158321380615,
      "learning_rate": 1.3240013163884346e-08,
      "loss": 0.9112,
      "step": 7481
    },
    {
      "epoch": 1.0518768452129903,
      "grad_norm": 1.5822244882583618,
      "learning_rate": 1.7292961598736057e-08,
      "loss": 1.0671,
      "step": 7482
    },
    {
      "epoch": 1.052017432869394,
      "grad_norm": 1.6155567169189453,
      "learning_rate": 2.1886236961521365e-08,
      "loss": 1.0956,
      "step": 7483
    },
    {
      "epoch": 1.0521580205257979,
      "grad_norm": 1.374367117881775,
      "learning_rate": 2.7019814429274372e-08,
      "loss": 1.162,
      "step": 7484
    },
    {
      "epoch": 1.0522986081822017,
      "grad_norm": 1.5334498882293701,
      "learning_rate": 3.269366625907599e-08,
      "loss": 1.0906,
      "step": 7485
    },
    {
      "epoch": 1.0524391958386055,
      "grad_norm": 1.3204318284988403,
      "learning_rate": 3.890776178828714e-08,
      "loss": 1.0025,
      "step": 7486
    },
    {
      "epoch": 1.052579783495009,
      "grad_norm": 1.5051853656768799,
      "learning_rate": 4.566206743465973e-08,
      "loss": 1.2161,
      "step": 7487
    },
    {
      "epoch": 1.0527203711514128,
      "grad_norm": 1.6620244979858398,
      "learning_rate": 5.295654669655869e-08,
      "loss": 0.9906,
      "step": 7488
    },
    {
      "epoch": 1.0528609588078166,
      "grad_norm": 1.8043627738952637,
      "learning_rate": 6.079116015311747e-08,
      "loss": 0.9619,
      "step": 7489
    },
    {
      "epoch": 1.0530015464642204,
      "grad_norm": 1.8138917684555054,
      "learning_rate": 6.916586546449333e-08,
      "loss": 1.0194,
      "step": 7490
    },
    {
      "epoch": 1.0531421341206242,
      "grad_norm": 1.6782792806625366,
      "learning_rate": 7.808061737207828e-08,
      "loss": 1.1349,
      "step": 7491
    },
    {
      "epoch": 1.053282721777028,
      "grad_norm": 1.4648350477218628,
      "learning_rate": 8.753536769874337e-08,
      "loss": 1.0809,
      "step": 7492
    },
    {
      "epoch": 1.0534233094334318,
      "grad_norm": 1.4836149215698242,
      "learning_rate": 9.753006534904962e-08,
      "loss": 1.1695,
      "step": 7493
    },
    {
      "epoch": 1.0535638970898356,
      "grad_norm": 1.7466017007827759,
      "learning_rate": 1.0806465630965879e-07,
      "loss": 0.9852,
      "step": 7494
    },
    {
      "epoch": 1.0537044847462393,
      "grad_norm": 1.332489013671875,
      "learning_rate": 1.1913908364948879e-07,
      "loss": 1.2166,
      "step": 7495
    },
    {
      "epoch": 1.0538450724026431,
      "grad_norm": 1.6642223596572876,
      "learning_rate": 1.307532875200579e-07,
      "loss": 0.9873,
      "step": 7496
    },
    {
      "epoch": 1.053985660059047,
      "grad_norm": 1.9976531267166138,
      "learning_rate": 1.429072051558511e-07,
      "loss": 1.1523,
      "step": 7497
    },
    {
      "epoch": 1.0541262477154505,
      "grad_norm": 1.533713459968567,
      "learning_rate": 1.556007708746199e-07,
      "loss": 0.9338,
      "step": 7498
    },
    {
      "epoch": 1.0542668353718543,
      "grad_norm": 1.2866524457931519,
      "learning_rate": 1.6883391607773746e-07,
      "loss": 1.0839,
      "step": 7499
    },
    {
      "epoch": 1.054407423028258,
      "grad_norm": 1.5042338371276855,
      "learning_rate": 1.826065692506096e-07,
      "loss": 0.9863,
      "step": 7500
    },
    {
      "epoch": 1.054407423028258,
      "eval_loss": 1.1404186487197876,
      "eval_runtime": 771.6905,
      "eval_samples_per_second": 16.387,
      "eval_steps_per_second": 8.194,
      "step": 7500
    },
    {
      "epoch": 1.0545480106846619,
      "grad_norm": 1.2974368333816528,
      "learning_rate": 1.9691865596299653e-07,
      "loss": 1.0749,
      "step": 7501
    },
    {
      "epoch": 1.0546885983410657,
      "grad_norm": 1.6719281673431396,
      "learning_rate": 2.1177009886946818e-07,
      "loss": 1.0733,
      "step": 7502
    },
    {
      "epoch": 1.0548291859974694,
      "grad_norm": 1.4457497596740723,
      "learning_rate": 2.2716081770981502e-07,
      "loss": 1.0203,
      "step": 7503
    },
    {
      "epoch": 1.0549697736538732,
      "grad_norm": 1.5708450078964233,
      "learning_rate": 2.430907293094031e-07,
      "loss": 0.9572,
      "step": 7504
    },
    {
      "epoch": 1.055110361310277,
      "grad_norm": 1.9069745540618896,
      "learning_rate": 2.595597475798073e-07,
      "loss": 1.1248,
      "step": 7505
    },
    {
      "epoch": 1.0552509489666808,
      "grad_norm": 1.483353614807129,
      "learning_rate": 2.765677835190772e-07,
      "loss": 1.0511,
      "step": 7506
    },
    {
      "epoch": 1.0553915366230844,
      "grad_norm": 1.3298616409301758,
      "learning_rate": 2.94114745212315e-07,
      "loss": 1.1297,
      "step": 7507
    },
    {
      "epoch": 1.0555321242794882,
      "grad_norm": 1.4353033304214478,
      "learning_rate": 3.122005378321524e-07,
      "loss": 0.9861,
      "step": 7508
    },
    {
      "epoch": 1.055672711935892,
      "grad_norm": 1.5081331729888916,
      "learning_rate": 3.308250636392618e-07,
      "loss": 1.0269,
      "step": 7509
    },
    {
      "epoch": 1.0558132995922958,
      "grad_norm": 1.5431466102600098,
      "learning_rate": 3.4998822198295535e-07,
      "loss": 1.0362,
      "step": 7510
    },
    {
      "epoch": 1.0559538872486995,
      "grad_norm": 1.5416865348815918,
      "learning_rate": 3.696899093016293e-07,
      "loss": 0.9684,
      "step": 7511
    },
    {
      "epoch": 1.0560944749051033,
      "grad_norm": 1.576179027557373,
      "learning_rate": 3.8993001912329683e-07,
      "loss": 0.9967,
      "step": 7512
    },
    {
      "epoch": 1.0562350625615071,
      "grad_norm": 1.7365208864212036,
      "learning_rate": 4.1070844206629877e-07,
      "loss": 0.9319,
      "step": 7513
    },
    {
      "epoch": 1.056375650217911,
      "grad_norm": 1.5179431438446045,
      "learning_rate": 4.3202506583981393e-07,
      "loss": 0.9037,
      "step": 7514
    },
    {
      "epoch": 1.0565162378743147,
      "grad_norm": 1.5765818357467651,
      "learning_rate": 4.5387977524443684e-07,
      "loss": 0.9666,
      "step": 7515
    },
    {
      "epoch": 1.0566568255307185,
      "grad_norm": 1.3905575275421143,
      "learning_rate": 4.7627245217288786e-07,
      "loss": 1.1203,
      "step": 7516
    },
    {
      "epoch": 1.0567974131871223,
      "grad_norm": 1.4948029518127441,
      "learning_rate": 4.992029756105798e-07,
      "loss": 1.0026,
      "step": 7517
    },
    {
      "epoch": 1.0569380008435258,
      "grad_norm": 1.3580808639526367,
      "learning_rate": 5.226712216362951e-07,
      "loss": 0.9722,
      "step": 7518
    },
    {
      "epoch": 1.0570785884999296,
      "grad_norm": 1.254178524017334,
      "learning_rate": 5.46677063422818e-07,
      "loss": 1.1195,
      "step": 7519
    },
    {
      "epoch": 1.0572191761563334,
      "grad_norm": 1.424198865890503,
      "learning_rate": 5.712203712376685e-07,
      "loss": 1.096,
      "step": 7520
    },
    {
      "epoch": 1.0573597638127372,
      "grad_norm": 1.4470114707946777,
      "learning_rate": 5.963010124438784e-07,
      "loss": 1.0065,
      "step": 7521
    },
    {
      "epoch": 1.057500351469141,
      "grad_norm": 1.2995045185089111,
      "learning_rate": 6.219188515005358e-07,
      "loss": 1.0214,
      "step": 7522
    },
    {
      "epoch": 1.0576409391255448,
      "grad_norm": 1.494919776916504,
      "learning_rate": 6.480737499635625e-07,
      "loss": 1.1944,
      "step": 7523
    },
    {
      "epoch": 1.0577815267819486,
      "grad_norm": 1.2921452522277832,
      "learning_rate": 6.747655664866015e-07,
      "loss": 1.0667,
      "step": 7524
    },
    {
      "epoch": 1.0579221144383524,
      "grad_norm": 1.4504281282424927,
      "learning_rate": 7.019941568216282e-07,
      "loss": 1.2005,
      "step": 7525
    },
    {
      "epoch": 1.0580627020947562,
      "grad_norm": 1.4284292459487915,
      "learning_rate": 7.297593738197606e-07,
      "loss": 0.9898,
      "step": 7526
    },
    {
      "epoch": 1.0582032897511597,
      "grad_norm": 1.3166258335113525,
      "learning_rate": 7.580610674321475e-07,
      "loss": 0.9969,
      "step": 7527
    },
    {
      "epoch": 1.0583438774075635,
      "grad_norm": 1.429358720779419,
      "learning_rate": 7.868990847106573e-07,
      "loss": 1.0495,
      "step": 7528
    },
    {
      "epoch": 1.0584844650639673,
      "grad_norm": 1.4984984397888184,
      "learning_rate": 8.16273269808765e-07,
      "loss": 1.0634,
      "step": 7529
    },
    {
      "epoch": 1.058625052720371,
      "grad_norm": 1.5670768022537231,
      "learning_rate": 8.461834639823974e-07,
      "loss": 1.0432,
      "step": 7530
    },
    {
      "epoch": 1.058765640376775,
      "grad_norm": 1.8118327856063843,
      "learning_rate": 8.766295055906981e-07,
      "loss": 1.024,
      "step": 7531
    },
    {
      "epoch": 1.0589062280331787,
      "grad_norm": 1.550458550453186,
      "learning_rate": 9.076112300971496e-07,
      "loss": 1.0865,
      "step": 7532
    },
    {
      "epoch": 1.0590468156895825,
      "grad_norm": 1.3819161653518677,
      "learning_rate": 9.39128470070183e-07,
      "loss": 1.0977,
      "step": 7533
    },
    {
      "epoch": 1.0591874033459863,
      "grad_norm": 1.540151596069336,
      "learning_rate": 9.71181055184156e-07,
      "loss": 1.0664,
      "step": 7534
    },
    {
      "epoch": 1.05932799100239,
      "grad_norm": 1.5149235725402832,
      "learning_rate": 1.0037688122203958e-06,
      "loss": 1.1502,
      "step": 7535
    },
    {
      "epoch": 1.0594685786587938,
      "grad_norm": 1.4674099683761597,
      "learning_rate": 1.0368915650679767e-06,
      "loss": 1.0071,
      "step": 7536
    },
    {
      "epoch": 1.0596091663151976,
      "grad_norm": 1.592854619026184,
      "learning_rate": 1.0705491347249074e-06,
      "loss": 1.0613,
      "step": 7537
    },
    {
      "epoch": 1.0597497539716012,
      "grad_norm": 1.4145671129226685,
      "learning_rate": 1.1047413392987095e-06,
      "loss": 0.9335,
      "step": 7538
    },
    {
      "epoch": 1.059890341628005,
      "grad_norm": 1.6950422525405884,
      "learning_rate": 1.1394679940077923e-06,
      "loss": 1.2074,
      "step": 7539
    },
    {
      "epoch": 1.0600309292844088,
      "grad_norm": 1.486403226852417,
      "learning_rate": 1.1747289111822212e-06,
      "loss": 1.1004,
      "step": 7540
    },
    {
      "epoch": 1.0601715169408126,
      "grad_norm": 1.4569374322891235,
      "learning_rate": 1.2105239002648371e-06,
      "loss": 1.0257,
      "step": 7541
    },
    {
      "epoch": 1.0603121045972164,
      "grad_norm": 1.5236237049102783,
      "learning_rate": 1.2468527678121123e-06,
      "loss": 0.9773,
      "step": 7542
    },
    {
      "epoch": 1.0604526922536202,
      "grad_norm": 1.706689476966858,
      "learning_rate": 1.2837153174955819e-06,
      "loss": 1.0951,
      "step": 7543
    },
    {
      "epoch": 1.060593279910024,
      "grad_norm": 1.4382357597351074,
      "learning_rate": 1.3211113501024662e-06,
      "loss": 1.1478,
      "step": 7544
    },
    {
      "epoch": 1.0607338675664277,
      "grad_norm": 1.5257270336151123,
      "learning_rate": 1.3590406635370035e-06,
      "loss": 1.206,
      "step": 7545
    },
    {
      "epoch": 1.0608744552228315,
      "grad_norm": 1.4588719606399536,
      "learning_rate": 1.3975030528214028e-06,
      "loss": 0.901,
      "step": 7546
    },
    {
      "epoch": 1.061015042879235,
      "grad_norm": 1.6819897890090942,
      "learning_rate": 1.4364983100970231e-06,
      "loss": 0.9701,
      "step": 7547
    },
    {
      "epoch": 1.0611556305356389,
      "grad_norm": 1.3491835594177246,
      "learning_rate": 1.4760262246256928e-06,
      "loss": 0.9524,
      "step": 7548
    },
    {
      "epoch": 1.0612962181920427,
      "grad_norm": 1.4729050397872925,
      "learning_rate": 1.5160865827904548e-06,
      "loss": 1.0575,
      "step": 7549
    },
    {
      "epoch": 1.0614368058484465,
      "grad_norm": 1.4277278184890747,
      "learning_rate": 1.5566791680969195e-06,
      "loss": 0.9526,
      "step": 7550
    },
    {
      "epoch": 1.0615773935048503,
      "grad_norm": 1.425094723701477,
      "learning_rate": 1.597803761174499e-06,
      "loss": 1.0199,
      "step": 7551
    },
    {
      "epoch": 1.061717981161254,
      "grad_norm": 1.413521647453308,
      "learning_rate": 1.6394601397775268e-06,
      "loss": 1.0862,
      "step": 7552
    },
    {
      "epoch": 1.0618585688176578,
      "grad_norm": 1.4185785055160522,
      "learning_rate": 1.681648078786391e-06,
      "loss": 1.1517,
      "step": 7553
    },
    {
      "epoch": 1.0619991564740616,
      "grad_norm": 1.8018213510513306,
      "learning_rate": 1.7243673502089553e-06,
      "loss": 1.0697,
      "step": 7554
    },
    {
      "epoch": 1.0621397441304654,
      "grad_norm": 1.6009984016418457,
      "learning_rate": 1.767617723181514e-06,
      "loss": 1.041,
      "step": 7555
    },
    {
      "epoch": 1.0622803317868692,
      "grad_norm": 1.4294462203979492,
      "learning_rate": 1.8113989639702899e-06,
      "loss": 1.0573,
      "step": 7556
    },
    {
      "epoch": 1.062420919443273,
      "grad_norm": 1.774080753326416,
      "learning_rate": 1.8557108359724685e-06,
      "loss": 1.136,
      "step": 7557
    },
    {
      "epoch": 1.0625615070996766,
      "grad_norm": 1.3736109733581543,
      "learning_rate": 1.9005530997176058e-06,
      "loss": 0.9643,
      "step": 7558
    },
    {
      "epoch": 1.0627020947560804,
      "grad_norm": 1.544493317604065,
      "learning_rate": 1.9459255128691177e-06,
      "loss": 0.9906,
      "step": 7559
    },
    {
      "epoch": 1.0628426824124841,
      "grad_norm": 1.3348678350448608,
      "learning_rate": 1.9918278302252126e-06,
      "loss": 1.2223,
      "step": 7560
    },
    {
      "epoch": 1.062983270068888,
      "grad_norm": 1.7784899473190308,
      "learning_rate": 2.0382598037203215e-06,
      "loss": 1.0488,
      "step": 7561
    },
    {
      "epoch": 1.0631238577252917,
      "grad_norm": 1.566510558128357,
      "learning_rate": 2.0852211824266445e-06,
      "loss": 1.1453,
      "step": 7562
    },
    {
      "epoch": 1.0632644453816955,
      "grad_norm": 1.61956787109375,
      "learning_rate": 2.1327117125552352e-06,
      "loss": 1.058,
      "step": 7563
    },
    {
      "epoch": 1.0634050330380993,
      "grad_norm": 1.7936275005340576,
      "learning_rate": 2.180731137457781e-06,
      "loss": 1.0814,
      "step": 7564
    },
    {
      "epoch": 1.063545620694503,
      "grad_norm": 1.4486342668533325,
      "learning_rate": 2.2292791976273653e-06,
      "loss": 1.1498,
      "step": 7565
    },
    {
      "epoch": 1.0636862083509069,
      "grad_norm": 1.7238483428955078,
      "learning_rate": 2.278355630700424e-06,
      "loss": 0.9045,
      "step": 7566
    },
    {
      "epoch": 1.0638267960073104,
      "grad_norm": 1.6770517826080322,
      "learning_rate": 2.3279601714579103e-06,
      "loss": 1.1191,
      "step": 7567
    },
    {
      "epoch": 1.0639673836637142,
      "grad_norm": 1.6059991121292114,
      "learning_rate": 2.378092551826805e-06,
      "loss": 1.0752,
      "step": 7568
    },
    {
      "epoch": 1.064107971320118,
      "grad_norm": 1.4534837007522583,
      "learning_rate": 2.4287525008813594e-06,
      "loss": 1.0446,
      "step": 7569
    },
    {
      "epoch": 1.0642485589765218,
      "grad_norm": 1.582134485244751,
      "learning_rate": 2.4799397448450614e-06,
      "loss": 1.0361,
      "step": 7570
    },
    {
      "epoch": 1.0643891466329256,
      "grad_norm": 1.5538090467453003,
      "learning_rate": 2.531654007091555e-06,
      "loss": 1.0726,
      "step": 7571
    },
    {
      "epoch": 1.0645297342893294,
      "grad_norm": 1.3152004480361938,
      "learning_rate": 2.583895008146453e-06,
      "loss": 1.165,
      "step": 7572
    },
    {
      "epoch": 1.0646703219457332,
      "grad_norm": 2.4225685596466064,
      "learning_rate": 2.6366624656886554e-06,
      "loss": 1.0993,
      "step": 7573
    },
    {
      "epoch": 1.064810909602137,
      "grad_norm": 1.7924308776855469,
      "learning_rate": 2.6899560945520375e-06,
      "loss": 0.9629,
      "step": 7574
    },
    {
      "epoch": 1.0649514972585408,
      "grad_norm": 1.3300851583480835,
      "learning_rate": 2.7437756067271504e-06,
      "loss": 1.2103,
      "step": 7575
    },
    {
      "epoch": 1.0650920849149446,
      "grad_norm": 1.765349268913269,
      "learning_rate": 2.7981207113622398e-06,
      "loss": 1.0278,
      "step": 7576
    },
    {
      "epoch": 1.0652326725713483,
      "grad_norm": 1.3900808095932007,
      "learning_rate": 2.852991114765313e-06,
      "loss": 1.188,
      "step": 7577
    },
    {
      "epoch": 1.065373260227752,
      "grad_norm": 1.497376561164856,
      "learning_rate": 2.908386520405526e-06,
      "loss": 1.0335,
      "step": 7578
    },
    {
      "epoch": 1.0655138478841557,
      "grad_norm": 1.7596478462219238,
      "learning_rate": 2.9643066289147702e-06,
      "loss": 0.9678,
      "step": 7579
    },
    {
      "epoch": 1.0656544355405595,
      "grad_norm": 1.4742902517318726,
      "learning_rate": 3.0207511380894725e-06,
      "loss": 1.0469,
      "step": 7580
    },
    {
      "epoch": 1.0657950231969633,
      "grad_norm": 1.280154824256897,
      "learning_rate": 3.07771974289196e-06,
      "loss": 1.0835,
      "step": 7581
    },
    {
      "epoch": 1.065935610853367,
      "grad_norm": 1.5031871795654297,
      "learning_rate": 3.13521213545227e-06,
      "loss": 1.2168,
      "step": 7582
    },
    {
      "epoch": 1.0660761985097709,
      "grad_norm": 1.3833279609680176,
      "learning_rate": 3.193228005069848e-06,
      "loss": 1.1682,
      "step": 7583
    },
    {
      "epoch": 1.0662167861661747,
      "grad_norm": 1.4870816469192505,
      "learning_rate": 3.2517670382150147e-06,
      "loss": 0.999,
      "step": 7584
    },
    {
      "epoch": 1.0663573738225784,
      "grad_norm": 1.7917933464050293,
      "learning_rate": 3.3108289185308616e-06,
      "loss": 1.1514,
      "step": 7585
    },
    {
      "epoch": 1.0664979614789822,
      "grad_norm": 1.4415546655654907,
      "learning_rate": 3.370413326835131e-06,
      "loss": 0.9552,
      "step": 7586
    },
    {
      "epoch": 1.0666385491353858,
      "grad_norm": 1.6579047441482544,
      "learning_rate": 3.4305199411214796e-06,
      "loss": 1.0642,
      "step": 7587
    },
    {
      "epoch": 1.0667791367917896,
      "grad_norm": 1.4309881925582886,
      "learning_rate": 3.491148436561442e-06,
      "loss": 1.3123,
      "step": 7588
    },
    {
      "epoch": 1.0669197244481934,
      "grad_norm": 1.5368642807006836,
      "learning_rate": 3.5522984855063e-06,
      "loss": 1.0017,
      "step": 7589
    },
    {
      "epoch": 1.0670603121045972,
      "grad_norm": 1.4481183290481567,
      "learning_rate": 3.613969757488678e-06,
      "loss": 0.9283,
      "step": 7590
    },
    {
      "epoch": 1.067200899761001,
      "grad_norm": 1.7736029624938965,
      "learning_rate": 3.6761619192245523e-06,
      "loss": 1.0302,
      "step": 7591
    },
    {
      "epoch": 1.0673414874174048,
      "grad_norm": 1.8456751108169556,
      "learning_rate": 3.738874634614753e-06,
      "loss": 1.1016,
      "step": 7592
    },
    {
      "epoch": 1.0674820750738085,
      "grad_norm": 1.4660898447036743,
      "learning_rate": 3.8021075647469927e-06,
      "loss": 1.0502,
      "step": 7593
    },
    {
      "epoch": 1.0676226627302123,
      "grad_norm": 1.299400806427002,
      "learning_rate": 3.8658603678976665e-06,
      "loss": 1.3496,
      "step": 7594
    },
    {
      "epoch": 1.0677632503866161,
      "grad_norm": 1.6917648315429688,
      "learning_rate": 3.930132699533551e-06,
      "loss": 1.1829,
      "step": 7595
    },
    {
      "epoch": 1.06790383804302,
      "grad_norm": 1.5883735418319702,
      "learning_rate": 3.994924212313788e-06,
      "loss": 0.9924,
      "step": 7596
    },
    {
      "epoch": 1.0680444256994237,
      "grad_norm": 1.5358706712722778,
      "learning_rate": 4.060234556092057e-06,
      "loss": 0.992,
      "step": 7597
    },
    {
      "epoch": 1.0681850133558273,
      "grad_norm": 1.4552007913589478,
      "learning_rate": 4.126063377917888e-06,
      "loss": 1.228,
      "step": 7598
    },
    {
      "epoch": 1.068325601012231,
      "grad_norm": 1.4546085596084595,
      "learning_rate": 4.192410322038809e-06,
      "loss": 1.0248,
      "step": 7599
    },
    {
      "epoch": 1.0684661886686349,
      "grad_norm": 2.0269877910614014,
      "learning_rate": 4.259275029902532e-06,
      "loss": 1.0181,
      "step": 7600
    },
    {
      "epoch": 1.0686067763250386,
      "grad_norm": 1.4507973194122314,
      "learning_rate": 4.3266571401583075e-06,
      "loss": 0.9566,
      "step": 7601
    },
    {
      "epoch": 1.0687473639814424,
      "grad_norm": 1.5106302499771118,
      "learning_rate": 4.394556288659768e-06,
      "loss": 0.9271,
      "step": 7602
    },
    {
      "epoch": 1.0688879516378462,
      "grad_norm": 1.9653428792953491,
      "learning_rate": 4.4629721084659455e-06,
      "loss": 0.9206,
      "step": 7603
    },
    {
      "epoch": 1.06902853929425,
      "grad_norm": 1.5793572664260864,
      "learning_rate": 4.531904229843831e-06,
      "loss": 1.2006,
      "step": 7604
    },
    {
      "epoch": 1.0691691269506538,
      "grad_norm": 1.83607816696167,
      "learning_rate": 4.601352280270166e-06,
      "loss": 0.9449,
      "step": 7605
    },
    {
      "epoch": 1.0693097146070576,
      "grad_norm": 1.3722200393676758,
      "learning_rate": 4.671315884433436e-06,
      "loss": 0.9071,
      "step": 7606
    },
    {
      "epoch": 1.0694503022634612,
      "grad_norm": 1.4874130487442017,
      "learning_rate": 4.741794664236277e-06,
      "loss": 0.8247,
      "step": 7607
    },
    {
      "epoch": 1.069590889919865,
      "grad_norm": 1.3872942924499512,
      "learning_rate": 4.812788238796995e-06,
      "loss": 0.9046,
      "step": 7608
    },
    {
      "epoch": 1.0697314775762687,
      "grad_norm": 1.3905807733535767,
      "learning_rate": 4.884296224451623e-06,
      "loss": 0.9551,
      "step": 7609
    },
    {
      "epoch": 1.0698720652326725,
      "grad_norm": 1.5343624353408813,
      "learning_rate": 4.956318234756551e-06,
      "loss": 1.004,
      "step": 7610
    },
    {
      "epoch": 1.0700126528890763,
      "grad_norm": 1.3348400592803955,
      "learning_rate": 5.028853880490158e-06,
      "loss": 1.0871,
      "step": 7611
    },
    {
      "epoch": 1.0701532405454801,
      "grad_norm": 1.5138661861419678,
      "learning_rate": 5.101902769654898e-06,
      "loss": 0.8787,
      "step": 7612
    },
    {
      "epoch": 1.070293828201884,
      "grad_norm": 1.6948344707489014,
      "learning_rate": 5.175464507479921e-06,
      "loss": 1.0106,
      "step": 7613
    },
    {
      "epoch": 1.0704344158582877,
      "grad_norm": 1.3809024095535278,
      "learning_rate": 5.249538696422496e-06,
      "loss": 1.002,
      "step": 7614
    },
    {
      "epoch": 1.0705750035146915,
      "grad_norm": 1.4856010675430298,
      "learning_rate": 5.3241249361707045e-06,
      "loss": 1.0372,
      "step": 7615
    },
    {
      "epoch": 1.0707155911710953,
      "grad_norm": 1.692852258682251,
      "learning_rate": 5.3992228236451005e-06,
      "loss": 1.0873,
      "step": 7616
    },
    {
      "epoch": 1.070856178827499,
      "grad_norm": 1.7191356420516968,
      "learning_rate": 5.4748319530013695e-06,
      "loss": 1.0444,
      "step": 7617
    },
    {
      "epoch": 1.0709967664839026,
      "grad_norm": 1.7170023918151855,
      "learning_rate": 5.5509519156325964e-06,
      "loss": 0.9127,
      "step": 7618
    },
    {
      "epoch": 1.0711373541403064,
      "grad_norm": 1.3879516124725342,
      "learning_rate": 5.627582300170886e-06,
      "loss": 0.9728,
      "step": 7619
    },
    {
      "epoch": 1.0712779417967102,
      "grad_norm": 1.484866976737976,
      "learning_rate": 5.7047226924900165e-06,
      "loss": 0.9382,
      "step": 7620
    },
    {
      "epoch": 1.071418529453114,
      "grad_norm": 1.4223469495773315,
      "learning_rate": 5.782372675707582e-06,
      "loss": 1.1235,
      "step": 7621
    },
    {
      "epoch": 1.0715591171095178,
      "grad_norm": 1.5206254720687866,
      "learning_rate": 5.860531830187133e-06,
      "loss": 1.0274,
      "step": 7622
    },
    {
      "epoch": 1.0716997047659216,
      "grad_norm": 1.458348274230957,
      "learning_rate": 5.939199733541023e-06,
      "loss": 0.9888,
      "step": 7623
    },
    {
      "epoch": 1.0718402924223254,
      "grad_norm": 1.6202799081802368,
      "learning_rate": 6.018375960631495e-06,
      "loss": 1.1183,
      "step": 7624
    },
    {
      "epoch": 1.0719808800787292,
      "grad_norm": 1.7359994649887085,
      "learning_rate": 6.09806008357422e-06,
      "loss": 0.9802,
      "step": 7625
    },
    {
      "epoch": 1.072121467735133,
      "grad_norm": 1.4082937240600586,
      "learning_rate": 6.178251671739943e-06,
      "loss": 1.1181,
      "step": 7626
    },
    {
      "epoch": 1.0722620553915365,
      "grad_norm": 1.439765453338623,
      "learning_rate": 6.258950291756982e-06,
      "loss": 0.8491,
      "step": 7627
    },
    {
      "epoch": 1.0724026430479403,
      "grad_norm": 1.5195555686950684,
      "learning_rate": 6.3401555075134235e-06,
      "loss": 1.2427,
      "step": 7628
    },
    {
      "epoch": 1.072543230704344,
      "grad_norm": 1.510003924369812,
      "learning_rate": 6.421866880159977e-06,
      "loss": 1.0141,
      "step": 7629
    },
    {
      "epoch": 1.0726838183607479,
      "grad_norm": 1.5310614109039307,
      "learning_rate": 6.504083968111729e-06,
      "loss": 1.1886,
      "step": 7630
    },
    {
      "epoch": 1.0728244060171517,
      "grad_norm": 1.775181770324707,
      "learning_rate": 6.586806327050521e-06,
      "loss": 1.0025,
      "step": 7631
    },
    {
      "epoch": 1.0729649936735555,
      "grad_norm": 1.4907737970352173,
      "learning_rate": 6.6700335099279974e-06,
      "loss": 0.8439,
      "step": 7632
    },
    {
      "epoch": 1.0731055813299593,
      "grad_norm": 1.3737210035324097,
      "learning_rate": 6.753765066967377e-06,
      "loss": 0.9412,
      "step": 7633
    },
    {
      "epoch": 1.073246168986363,
      "grad_norm": 1.5563627481460571,
      "learning_rate": 6.83800054566649e-06,
      "loss": 1.0019,
      "step": 7634
    },
    {
      "epoch": 1.0733867566427668,
      "grad_norm": 1.5916659832000732,
      "learning_rate": 6.9227394907995454e-06,
      "loss": 0.8518,
      "step": 7635
    },
    {
      "epoch": 1.0735273442991706,
      "grad_norm": 1.568564772605896,
      "learning_rate": 7.0079814444199866e-06,
      "loss": 1.0063,
      "step": 7636
    },
    {
      "epoch": 1.0736679319555744,
      "grad_norm": 1.556060552597046,
      "learning_rate": 7.093725945862928e-06,
      "loss": 0.9542,
      "step": 7637
    },
    {
      "epoch": 1.073808519611978,
      "grad_norm": 1.515141248703003,
      "learning_rate": 7.179972531747747e-06,
      "loss": 0.924,
      "step": 7638
    },
    {
      "epoch": 1.0739491072683818,
      "grad_norm": 1.4444221258163452,
      "learning_rate": 7.266720735979915e-06,
      "loss": 1.0952,
      "step": 7639
    },
    {
      "epoch": 1.0740896949247856,
      "grad_norm": 1.8240028619766235,
      "learning_rate": 7.3539700897548356e-06,
      "loss": 1.023,
      "step": 7640
    },
    {
      "epoch": 1.0742302825811894,
      "grad_norm": 1.3686672449111938,
      "learning_rate": 7.4417201215591145e-06,
      "loss": 1.0353,
      "step": 7641
    },
    {
      "epoch": 1.0743708702375931,
      "grad_norm": 1.5114552974700928,
      "learning_rate": 7.529970357173665e-06,
      "loss": 0.8997,
      "step": 7642
    },
    {
      "epoch": 1.074511457893997,
      "grad_norm": 1.2720465660095215,
      "learning_rate": 7.6187203196762424e-06,
      "loss": 1.0842,
      "step": 7643
    },
    {
      "epoch": 1.0746520455504007,
      "grad_norm": 1.46778404712677,
      "learning_rate": 7.707969529443815e-06,
      "loss": 1.0065,
      "step": 7644
    },
    {
      "epoch": 1.0747926332068045,
      "grad_norm": 1.82071852684021,
      "learning_rate": 7.797717504155743e-06,
      "loss": 1.0178,
      "step": 7645
    },
    {
      "epoch": 1.0749332208632083,
      "grad_norm": 1.5554766654968262,
      "learning_rate": 7.887963758795669e-06,
      "loss": 1.0555,
      "step": 7646
    },
    {
      "epoch": 1.0750738085196119,
      "grad_norm": 1.6054444313049316,
      "learning_rate": 7.978707805654173e-06,
      "loss": 1.1203,
      "step": 7647
    },
    {
      "epoch": 1.0752143961760157,
      "grad_norm": 1.670786738395691,
      "learning_rate": 8.069949154332069e-06,
      "loss": 0.972,
      "step": 7648
    },
    {
      "epoch": 1.0753549838324195,
      "grad_norm": 1.4161821603775024,
      "learning_rate": 8.161687311742361e-06,
      "loss": 1.2104,
      "step": 7649
    },
    {
      "epoch": 1.0754955714888232,
      "grad_norm": 1.5039936304092407,
      "learning_rate": 8.253921782113626e-06,
      "loss": 1.134,
      "step": 7650
    },
    {
      "epoch": 1.075636159145227,
      "grad_norm": 1.4248892068862915,
      "learning_rate": 8.346652066991868e-06,
      "loss": 1.1418,
      "step": 7651
    },
    {
      "epoch": 1.0757767468016308,
      "grad_norm": 1.5301259756088257,
      "learning_rate": 8.4398776652437e-06,
      "loss": 1.0613,
      "step": 7652
    },
    {
      "epoch": 1.0759173344580346,
      "grad_norm": 1.5535019636154175,
      "learning_rate": 8.533598073059147e-06,
      "loss": 1.1629,
      "step": 7653
    },
    {
      "epoch": 1.0760579221144384,
      "grad_norm": 1.6467928886413574,
      "learning_rate": 8.627812783953726e-06,
      "loss": 1.0529,
      "step": 7654
    },
    {
      "epoch": 1.0761985097708422,
      "grad_norm": 1.3641315698623657,
      "learning_rate": 8.722521288771945e-06,
      "loss": 1.0387,
      "step": 7655
    },
    {
      "epoch": 1.076339097427246,
      "grad_norm": 1.46486234664917,
      "learning_rate": 8.81772307569002e-06,
      "loss": 1.1059,
      "step": 7656
    },
    {
      "epoch": 1.0764796850836498,
      "grad_norm": 1.8572417497634888,
      "learning_rate": 8.913417630217946e-06,
      "loss": 1.0231,
      "step": 7657
    },
    {
      "epoch": 1.0766202727400533,
      "grad_norm": 1.571033000946045,
      "learning_rate": 9.009604435202857e-06,
      "loss": 1.1153,
      "step": 7658
    },
    {
      "epoch": 1.0767608603964571,
      "grad_norm": 1.2974190711975098,
      "learning_rate": 9.106282970831636e-06,
      "loss": 1.0674,
      "step": 7659
    },
    {
      "epoch": 1.076901448052861,
      "grad_norm": 1.5318596363067627,
      "learning_rate": 9.203452714633632e-06,
      "loss": 1.2278,
      "step": 7660
    },
    {
      "epoch": 1.0770420357092647,
      "grad_norm": 1.4541805982589722,
      "learning_rate": 9.301113141484196e-06,
      "loss": 1.1907,
      "step": 7661
    },
    {
      "epoch": 1.0771826233656685,
      "grad_norm": 1.4727864265441895,
      "learning_rate": 9.39926372360601e-06,
      "loss": 1.0804,
      "step": 7662
    },
    {
      "epoch": 1.0773232110220723,
      "grad_norm": 1.6566400527954102,
      "learning_rate": 9.497903930573494e-06,
      "loss": 0.9891,
      "step": 7663
    },
    {
      "epoch": 1.077463798678476,
      "grad_norm": 1.7612003087997437,
      "learning_rate": 9.597033229314845e-06,
      "loss": 1.1237,
      "step": 7664
    },
    {
      "epoch": 1.0776043863348799,
      "grad_norm": 1.4051235914230347,
      "learning_rate": 9.696651084114994e-06,
      "loss": 1.2362,
      "step": 7665
    },
    {
      "epoch": 1.0777449739912837,
      "grad_norm": 1.5281331539154053,
      "learning_rate": 9.796756956619035e-06,
      "loss": 1.1692,
      "step": 7666
    },
    {
      "epoch": 1.0778855616476872,
      "grad_norm": 1.5674617290496826,
      "learning_rate": 9.897350305834286e-06,
      "loss": 1.0567,
      "step": 7667
    },
    {
      "epoch": 1.078026149304091,
      "grad_norm": 1.777355432510376,
      "learning_rate": 9.998430588133777e-06,
      "loss": 1.2109,
      "step": 7668
    },
    {
      "epoch": 1.0781667369604948,
      "grad_norm": 1.6983052492141724,
      "learning_rate": 1.009999725725923e-05,
      "loss": 1.0156,
      "step": 7669
    },
    {
      "epoch": 1.0783073246168986,
      "grad_norm": 1.6042579412460327,
      "learning_rate": 1.020204976432334e-05,
      "loss": 1.0764,
      "step": 7670
    },
    {
      "epoch": 1.0784479122733024,
      "grad_norm": 1.5201841592788696,
      "learning_rate": 1.0304587557813562e-05,
      "loss": 1.0873,
      "step": 7671
    },
    {
      "epoch": 1.0785884999297062,
      "grad_norm": 1.6250780820846558,
      "learning_rate": 1.0407610083595065e-05,
      "loss": 1.0437,
      "step": 7672
    },
    {
      "epoch": 1.07872908758611,
      "grad_norm": 1.594515323638916,
      "learning_rate": 1.0511116784912966e-05,
      "loss": 1.1352,
      "step": 7673
    },
    {
      "epoch": 1.0788696752425138,
      "grad_norm": 1.4831093549728394,
      "learning_rate": 1.0615107102395972e-05,
      "loss": 1.1166,
      "step": 7674
    },
    {
      "epoch": 1.0790102628989175,
      "grad_norm": 1.5498623847961426,
      "learning_rate": 1.0719580474059187e-05,
      "loss": 1.0897,
      "step": 7675
    },
    {
      "epoch": 1.0791508505553213,
      "grad_norm": 1.7373263835906982,
      "learning_rate": 1.0824536335307223e-05,
      "loss": 0.9588,
      "step": 7676
    },
    {
      "epoch": 1.079291438211725,
      "grad_norm": 1.4593071937561035,
      "learning_rate": 1.0929974118937225e-05,
      "loss": 1.1108,
      "step": 7677
    },
    {
      "epoch": 1.0794320258681287,
      "grad_norm": 1.3457814455032349,
      "learning_rate": 1.1035893255141905e-05,
      "loss": 1.0002,
      "step": 7678
    },
    {
      "epoch": 1.0795726135245325,
      "grad_norm": 1.5281860828399658,
      "learning_rate": 1.1142293171512675e-05,
      "loss": 0.9076,
      "step": 7679
    },
    {
      "epoch": 1.0797132011809363,
      "grad_norm": 1.4744161367416382,
      "learning_rate": 1.1249173293042681e-05,
      "loss": 1.1197,
      "step": 7680
    },
    {
      "epoch": 1.07985378883734,
      "grad_norm": 1.5899327993392944,
      "learning_rate": 1.135653304212998e-05,
      "loss": 1.0208,
      "step": 7681
    },
    {
      "epoch": 1.0799943764937439,
      "grad_norm": 1.7623028755187988,
      "learning_rate": 1.1464371838580435e-05,
      "loss": 1.0879,
      "step": 7682
    },
    {
      "epoch": 1.0801349641501476,
      "grad_norm": 1.3190147876739502,
      "learning_rate": 1.1572689099611534e-05,
      "loss": 1.1516,
      "step": 7683
    },
    {
      "epoch": 1.0802755518065514,
      "grad_norm": 1.3950145244598389,
      "learning_rate": 1.1681484239854722e-05,
      "loss": 1.0804,
      "step": 7684
    },
    {
      "epoch": 1.0804161394629552,
      "grad_norm": 1.561371922492981,
      "learning_rate": 1.179075667135856e-05,
      "loss": 1.0876,
      "step": 7685
    },
    {
      "epoch": 1.080556727119359,
      "grad_norm": 1.4008588790893555,
      "learning_rate": 1.1900505803592721e-05,
      "loss": 0.9894,
      "step": 7686
    },
    {
      "epoch": 1.0806973147757626,
      "grad_norm": 1.3317615985870361,
      "learning_rate": 1.2010731043450352e-05,
      "loss": 1.1801,
      "step": 7687
    },
    {
      "epoch": 1.0808379024321664,
      "grad_norm": 1.5143578052520752,
      "learning_rate": 1.2121431795252124e-05,
      "loss": 1.1757,
      "step": 7688
    },
    {
      "epoch": 1.0809784900885702,
      "grad_norm": 1.612565040588379,
      "learning_rate": 1.2232607460748447e-05,
      "loss": 0.9203,
      "step": 7689
    },
    {
      "epoch": 1.081119077744974,
      "grad_norm": 1.3897243738174438,
      "learning_rate": 1.2344257439123363e-05,
      "loss": 1.0998,
      "step": 7690
    },
    {
      "epoch": 1.0812596654013777,
      "grad_norm": 1.410320520401001,
      "learning_rate": 1.2456381126997808e-05,
      "loss": 1.1078,
      "step": 7691
    },
    {
      "epoch": 1.0814002530577815,
      "grad_norm": 1.5895731449127197,
      "learning_rate": 1.256897791843199e-05,
      "loss": 1.036,
      "step": 7692
    },
    {
      "epoch": 1.0815408407141853,
      "grad_norm": 1.775536298751831,
      "learning_rate": 1.2682047204930436e-05,
      "loss": 1.0958,
      "step": 7693
    },
    {
      "epoch": 1.0816814283705891,
      "grad_norm": 1.518997073173523,
      "learning_rate": 1.2795588375443478e-05,
      "loss": 0.983,
      "step": 7694
    },
    {
      "epoch": 1.081822016026993,
      "grad_norm": 1.3921091556549072,
      "learning_rate": 1.2909600816371504e-05,
      "loss": 1.0256,
      "step": 7695
    },
    {
      "epoch": 1.0819626036833967,
      "grad_norm": 2.28194522857666,
      "learning_rate": 1.3024083911568042e-05,
      "loss": 0.922,
      "step": 7696
    },
    {
      "epoch": 1.0821031913398003,
      "grad_norm": 1.5586000680923462,
      "learning_rate": 1.3139037042343116e-05,
      "loss": 1.0506,
      "step": 7697
    },
    {
      "epoch": 1.082243778996204,
      "grad_norm": 1.2755956649780273,
      "learning_rate": 1.3254459587466383e-05,
      "loss": 1.0806,
      "step": 7698
    },
    {
      "epoch": 1.0823843666526078,
      "grad_norm": 1.8547762632369995,
      "learning_rate": 1.3370350923171426e-05,
      "loss": 1.0096,
      "step": 7699
    },
    {
      "epoch": 1.0825249543090116,
      "grad_norm": 1.7242828607559204,
      "learning_rate": 1.3486710423157323e-05,
      "loss": 0.895,
      "step": 7700
    },
    {
      "epoch": 1.0826655419654154,
      "grad_norm": 1.3552380800247192,
      "learning_rate": 1.3603537458593774e-05,
      "loss": 0.9061,
      "step": 7701
    },
    {
      "epoch": 1.0828061296218192,
      "grad_norm": 1.6830592155456543,
      "learning_rate": 1.3720831398123678e-05,
      "loss": 1.1384,
      "step": 7702
    },
    {
      "epoch": 1.082946717278223,
      "grad_norm": 1.469437599182129,
      "learning_rate": 1.3838591607866502e-05,
      "loss": 1.0796,
      "step": 7703
    },
    {
      "epoch": 1.0830873049346268,
      "grad_norm": 1.4056177139282227,
      "learning_rate": 1.3956817451422421e-05,
      "loss": 0.9391,
      "step": 7704
    },
    {
      "epoch": 1.0832278925910306,
      "grad_norm": 1.5958421230316162,
      "learning_rate": 1.4075508289874717e-05,
      "loss": 1.0319,
      "step": 7705
    },
    {
      "epoch": 1.0833684802474344,
      "grad_norm": 1.641786813735962,
      "learning_rate": 1.4194663481793879e-05,
      "loss": 1.0719,
      "step": 7706
    },
    {
      "epoch": 1.083509067903838,
      "grad_norm": 1.2988569736480713,
      "learning_rate": 1.4314282383241174e-05,
      "loss": 1.1158,
      "step": 7707
    },
    {
      "epoch": 1.0836496555602417,
      "grad_norm": 1.5496385097503662,
      "learning_rate": 1.443436434777129e-05,
      "loss": 1.0718,
      "step": 7708
    },
    {
      "epoch": 1.0837902432166455,
      "grad_norm": 1.5578267574310303,
      "learning_rate": 1.4554908726436822e-05,
      "loss": 1.1021,
      "step": 7709
    },
    {
      "epoch": 1.0839308308730493,
      "grad_norm": 1.6231746673583984,
      "learning_rate": 1.467591486779175e-05,
      "loss": 1.0096,
      "step": 7710
    },
    {
      "epoch": 1.084071418529453,
      "grad_norm": 1.4935288429260254,
      "learning_rate": 1.479738211789402e-05,
      "loss": 0.904,
      "step": 7711
    },
    {
      "epoch": 1.084212006185857,
      "grad_norm": 1.4438711404800415,
      "learning_rate": 1.4919309820309879e-05,
      "loss": 0.8111,
      "step": 7712
    },
    {
      "epoch": 1.0843525938422607,
      "grad_norm": 1.3264319896697998,
      "learning_rate": 1.5041697316117165e-05,
      "loss": 1.1638,
      "step": 7713
    },
    {
      "epoch": 1.0844931814986645,
      "grad_norm": 1.5287920236587524,
      "learning_rate": 1.5164543943908893e-05,
      "loss": 0.9739,
      "step": 7714
    },
    {
      "epoch": 1.0846337691550683,
      "grad_norm": 1.4461350440979004,
      "learning_rate": 1.5287849039796854e-05,
      "loss": 0.9777,
      "step": 7715
    },
    {
      "epoch": 1.084774356811472,
      "grad_norm": 1.6047371625900269,
      "learning_rate": 1.541161193741517e-05,
      "loss": 1.0809,
      "step": 7716
    },
    {
      "epoch": 1.0849149444678756,
      "grad_norm": 1.4332935810089111,
      "learning_rate": 1.553583196792393e-05,
      "loss": 1.0656,
      "step": 7717
    },
    {
      "epoch": 1.0850555321242794,
      "grad_norm": 1.4226932525634766,
      "learning_rate": 1.5660508460012757e-05,
      "loss": 1.2675,
      "step": 7718
    },
    {
      "epoch": 1.0851961197806832,
      "grad_norm": 1.69802725315094,
      "learning_rate": 1.5785640739904296e-05,
      "loss": 0.9255,
      "step": 7719
    },
    {
      "epoch": 1.085336707437087,
      "grad_norm": 1.3761496543884277,
      "learning_rate": 1.591122813135857e-05,
      "loss": 1.0177,
      "step": 7720
    },
    {
      "epoch": 1.0854772950934908,
      "grad_norm": 1.56014883518219,
      "learning_rate": 1.603726995567554e-05,
      "loss": 1.0084,
      "step": 7721
    },
    {
      "epoch": 1.0856178827498946,
      "grad_norm": 1.583262324333191,
      "learning_rate": 1.616376553169967e-05,
      "loss": 1.0642,
      "step": 7722
    },
    {
      "epoch": 1.0857584704062984,
      "grad_norm": 1.474304437637329,
      "learning_rate": 1.629071417582273e-05,
      "loss": 0.8791,
      "step": 7723
    },
    {
      "epoch": 1.0858990580627021,
      "grad_norm": 1.4812438488006592,
      "learning_rate": 1.6418115201988537e-05,
      "loss": 1.0724,
      "step": 7724
    },
    {
      "epoch": 1.086039645719106,
      "grad_norm": 1.4800291061401367,
      "learning_rate": 1.654596792169579e-05,
      "loss": 1.053,
      "step": 7725
    },
    {
      "epoch": 1.0861802333755097,
      "grad_norm": 1.7456525564193726,
      "learning_rate": 1.6674271644002726e-05,
      "loss": 1.1179,
      "step": 7726
    },
    {
      "epoch": 1.0863208210319133,
      "grad_norm": 1.429742693901062,
      "learning_rate": 1.6803025675529684e-05,
      "loss": 1.1112,
      "step": 7727
    },
    {
      "epoch": 1.086461408688317,
      "grad_norm": 1.9685254096984863,
      "learning_rate": 1.6932229320463623e-05,
      "loss": 1.1208,
      "step": 7728
    },
    {
      "epoch": 1.0866019963447209,
      "grad_norm": 1.6772583723068237,
      "learning_rate": 1.7061881880561914e-05,
      "loss": 1.2268,
      "step": 7729
    },
    {
      "epoch": 1.0867425840011247,
      "grad_norm": 1.5441615581512451,
      "learning_rate": 1.7191982655155114e-05,
      "loss": 1.017,
      "step": 7730
    },
    {
      "epoch": 1.0868831716575285,
      "grad_norm": 1.7092124223709106,
      "learning_rate": 1.7322530941152705e-05,
      "loss": 1.0195,
      "step": 7731
    },
    {
      "epoch": 1.0870237593139322,
      "grad_norm": 1.3342162370681763,
      "learning_rate": 1.745352603304493e-05,
      "loss": 1.1827,
      "step": 7732
    },
    {
      "epoch": 1.087164346970336,
      "grad_norm": 1.4916080236434937,
      "learning_rate": 1.7584967222907635e-05,
      "loss": 1.1515,
      "step": 7733
    },
    {
      "epoch": 1.0873049346267398,
      "grad_norm": 1.706263780593872,
      "learning_rate": 1.7716853800405807e-05,
      "loss": 0.9938,
      "step": 7734
    },
    {
      "epoch": 1.0874455222831436,
      "grad_norm": 1.3456329107284546,
      "learning_rate": 1.7849185052797336e-05,
      "loss": 0.9753,
      "step": 7735
    },
    {
      "epoch": 1.0875861099395474,
      "grad_norm": 1.6018050909042358,
      "learning_rate": 1.7981960264937536e-05,
      "loss": 1.1649,
      "step": 7736
    },
    {
      "epoch": 1.087726697595951,
      "grad_norm": 1.5431888103485107,
      "learning_rate": 1.8115178719282112e-05,
      "loss": 0.9338,
      "step": 7737
    },
    {
      "epoch": 1.0878672852523548,
      "grad_norm": 1.3936315774917603,
      "learning_rate": 1.8248839695890974e-05,
      "loss": 1.0096,
      "step": 7738
    },
    {
      "epoch": 1.0880078729087586,
      "grad_norm": 1.3770583868026733,
      "learning_rate": 1.8382942472433163e-05,
      "loss": 0.9445,
      "step": 7739
    },
    {
      "epoch": 1.0881484605651623,
      "grad_norm": 1.3043593168258667,
      "learning_rate": 1.8517486324189946e-05,
      "loss": 1.2281,
      "step": 7740
    },
    {
      "epoch": 1.0882890482215661,
      "grad_norm": 1.2850086688995361,
      "learning_rate": 1.8652470524058708e-05,
      "loss": 1.0615,
      "step": 7741
    },
    {
      "epoch": 1.08842963587797,
      "grad_norm": 1.5812381505966187,
      "learning_rate": 1.8787894342557743e-05,
      "loss": 1.0797,
      "step": 7742
    },
    {
      "epoch": 1.0885702235343737,
      "grad_norm": 1.6155269145965576,
      "learning_rate": 1.8923757047828917e-05,
      "loss": 1.0886,
      "step": 7743
    },
    {
      "epoch": 1.0887108111907775,
      "grad_norm": 1.395843267440796,
      "learning_rate": 1.906005790564248e-05,
      "loss": 1.2231,
      "step": 7744
    },
    {
      "epoch": 1.0888513988471813,
      "grad_norm": 1.2612642049789429,
      "learning_rate": 1.9196796179401034e-05,
      "loss": 1.0217,
      "step": 7745
    },
    {
      "epoch": 1.088991986503585,
      "grad_norm": 1.4123172760009766,
      "learning_rate": 1.9333971130142447e-05,
      "loss": 1.2602,
      "step": 7746
    },
    {
      "epoch": 1.0891325741599887,
      "grad_norm": 1.5884568691253662,
      "learning_rate": 1.947158201654594e-05,
      "loss": 0.879,
      "step": 7747
    },
    {
      "epoch": 1.0892731618163924,
      "grad_norm": 1.3134346008300781,
      "learning_rate": 1.960962809493403e-05,
      "loss": 1.2482,
      "step": 7748
    },
    {
      "epoch": 1.0894137494727962,
      "grad_norm": 1.3274883031845093,
      "learning_rate": 1.9748108619277582e-05,
      "loss": 0.9726,
      "step": 7749
    },
    {
      "epoch": 1.0895543371292,
      "grad_norm": 1.4656354188919067,
      "learning_rate": 1.98870228411996e-05,
      "loss": 1.0338,
      "step": 7750
    },
    {
      "epoch": 1.0896949247856038,
      "grad_norm": 1.459166169166565,
      "learning_rate": 2.0026370009979335e-05,
      "loss": 0.9585,
      "step": 7751
    },
    {
      "epoch": 1.0898355124420076,
      "grad_norm": 1.9358081817626953,
      "learning_rate": 2.0166149372556265e-05,
      "loss": 0.9365,
      "step": 7752
    },
    {
      "epoch": 1.0899761000984114,
      "grad_norm": 1.4240763187408447,
      "learning_rate": 2.0306360173534235e-05,
      "loss": 0.9784,
      "step": 7753
    },
    {
      "epoch": 1.0901166877548152,
      "grad_norm": 1.6133391857147217,
      "learning_rate": 2.0447001655185482e-05,
      "loss": 0.9446,
      "step": 7754
    },
    {
      "epoch": 1.090257275411219,
      "grad_norm": 1.6446492671966553,
      "learning_rate": 2.0588073057454804e-05,
      "loss": 1.1773,
      "step": 7755
    },
    {
      "epoch": 1.0903978630676228,
      "grad_norm": 1.4118220806121826,
      "learning_rate": 2.072957361796356e-05,
      "loss": 1.2866,
      "step": 7756
    },
    {
      "epoch": 1.0905384507240263,
      "grad_norm": 1.5523968935012817,
      "learning_rate": 2.087150257201368e-05,
      "loss": 1.059,
      "step": 7757
    },
    {
      "epoch": 1.0906790383804301,
      "grad_norm": 1.7813712358474731,
      "learning_rate": 2.1013859152592586e-05,
      "loss": 1.1119,
      "step": 7758
    },
    {
      "epoch": 1.090819626036834,
      "grad_norm": 1.4947700500488281,
      "learning_rate": 2.1156642590376098e-05,
      "loss": 1.2178,
      "step": 7759
    },
    {
      "epoch": 1.0909602136932377,
      "grad_norm": 1.3963366746902466,
      "learning_rate": 2.129985211373362e-05,
      "loss": 1.1506,
      "step": 7760
    },
    {
      "epoch": 1.0911008013496415,
      "grad_norm": 1.5226809978485107,
      "learning_rate": 2.1443486948731274e-05,
      "loss": 0.9706,
      "step": 7761
    },
    {
      "epoch": 1.0912413890060453,
      "grad_norm": 1.4119855165481567,
      "learning_rate": 2.158754631913713e-05,
      "loss": 0.9671,
      "step": 7762
    },
    {
      "epoch": 1.091381976662449,
      "grad_norm": 1.6501706838607788,
      "learning_rate": 2.1732029446425415e-05,
      "loss": 1.1414,
      "step": 7763
    },
    {
      "epoch": 1.0915225643188529,
      "grad_norm": 1.4566634893417358,
      "learning_rate": 2.1876935549779663e-05,
      "loss": 1.0151,
      "step": 7764
    },
    {
      "epoch": 1.0916631519752567,
      "grad_norm": 1.4509358406066895,
      "learning_rate": 2.2022263846097734e-05,
      "loss": 0.9559,
      "step": 7765
    },
    {
      "epoch": 1.0918037396316604,
      "grad_norm": 1.4286967515945435,
      "learning_rate": 2.21680135499959e-05,
      "loss": 0.974,
      "step": 7766
    },
    {
      "epoch": 1.091944327288064,
      "grad_norm": 1.647512435913086,
      "learning_rate": 2.2314183873813067e-05,
      "loss": 1.0148,
      "step": 7767
    },
    {
      "epoch": 1.0920849149444678,
      "grad_norm": 1.4301561117172241,
      "learning_rate": 2.2460774027614994e-05,
      "loss": 1.0539,
      "step": 7768
    },
    {
      "epoch": 1.0922255026008716,
      "grad_norm": 1.4007726907730103,
      "learning_rate": 2.260778321919861e-05,
      "loss": 1.0406,
      "step": 7769
    },
    {
      "epoch": 1.0923660902572754,
      "grad_norm": 1.310465931892395,
      "learning_rate": 2.2755210654096293e-05,
      "loss": 0.9813,
      "step": 7770
    },
    {
      "epoch": 1.0925066779136792,
      "grad_norm": 1.7978153228759766,
      "learning_rate": 2.290305553558013e-05,
      "loss": 1.0295,
      "step": 7771
    },
    {
      "epoch": 1.092647265570083,
      "grad_norm": 1.4234975576400757,
      "learning_rate": 2.3051317064666243e-05,
      "loss": 0.834,
      "step": 7772
    },
    {
      "epoch": 1.0927878532264867,
      "grad_norm": 1.5047249794006348,
      "learning_rate": 2.3199994440118877e-05,
      "loss": 1.1925,
      "step": 7773
    },
    {
      "epoch": 1.0929284408828905,
      "grad_norm": 1.6484166383743286,
      "learning_rate": 2.334908685845566e-05,
      "loss": 1.0142,
      "step": 7774
    },
    {
      "epoch": 1.0930690285392943,
      "grad_norm": 1.5253608226776123,
      "learning_rate": 2.3498593513950763e-05,
      "loss": 0.9746,
      "step": 7775
    },
    {
      "epoch": 1.0932096161956981,
      "grad_norm": 1.4417623281478882,
      "learning_rate": 2.36485135986393e-05,
      "loss": 1.1017,
      "step": 7776
    },
    {
      "epoch": 1.0933502038521017,
      "grad_norm": 1.4259668588638306,
      "learning_rate": 2.37988463023228e-05,
      "loss": 0.9365,
      "step": 7777
    },
    {
      "epoch": 1.0934907915085055,
      "grad_norm": 1.659018874168396,
      "learning_rate": 2.3949590812572443e-05,
      "loss": 1.0449,
      "step": 7778
    },
    {
      "epoch": 1.0936313791649093,
      "grad_norm": 1.9021551609039307,
      "learning_rate": 2.410074631473466e-05,
      "loss": 1.0627,
      "step": 7779
    },
    {
      "epoch": 1.093771966821313,
      "grad_norm": 1.5574105978012085,
      "learning_rate": 2.4252311991934062e-05,
      "loss": 1.2079,
      "step": 7780
    },
    {
      "epoch": 1.0939125544777168,
      "grad_norm": 1.5512738227844238,
      "learning_rate": 2.440428702507883e-05,
      "loss": 1.0781,
      "step": 7781
    },
    {
      "epoch": 1.0940531421341206,
      "grad_norm": 1.947492003440857,
      "learning_rate": 2.455667059286493e-05,
      "loss": 1.0202,
      "step": 7782
    },
    {
      "epoch": 1.0941937297905244,
      "grad_norm": 1.4615358114242554,
      "learning_rate": 2.4709461871780703e-05,
      "loss": 0.9689,
      "step": 7783
    },
    {
      "epoch": 1.0943343174469282,
      "grad_norm": 1.552018642425537,
      "learning_rate": 2.4862660036110264e-05,
      "loss": 0.9943,
      "step": 7784
    },
    {
      "epoch": 1.094474905103332,
      "grad_norm": 1.7110774517059326,
      "learning_rate": 2.5016264257940202e-05,
      "loss": 1.0437,
      "step": 7785
    },
    {
      "epoch": 1.0946154927597358,
      "grad_norm": 1.5972321033477783,
      "learning_rate": 2.517027370716174e-05,
      "loss": 1.0189,
      "step": 7786
    },
    {
      "epoch": 1.0947560804161394,
      "grad_norm": 1.6838866472244263,
      "learning_rate": 2.5324687551476446e-05,
      "loss": 0.9457,
      "step": 7787
    },
    {
      "epoch": 1.0948966680725432,
      "grad_norm": 1.5528881549835205,
      "learning_rate": 2.5479504956400425e-05,
      "loss": 1.001,
      "step": 7788
    },
    {
      "epoch": 1.095037255728947,
      "grad_norm": 1.5242118835449219,
      "learning_rate": 2.563472508526863e-05,
      "loss": 1.1316,
      "step": 7789
    },
    {
      "epoch": 1.0951778433853507,
      "grad_norm": 1.4344483613967896,
      "learning_rate": 2.5790347099240553e-05,
      "loss": 1.067,
      "step": 7790
    },
    {
      "epoch": 1.0953184310417545,
      "grad_norm": 1.6613954305648804,
      "learning_rate": 2.594637015730239e-05,
      "loss": 1.0513,
      "step": 7791
    },
    {
      "epoch": 1.0954590186981583,
      "grad_norm": 1.38158118724823,
      "learning_rate": 2.6102793416273997e-05,
      "loss": 1.0991,
      "step": 7792
    },
    {
      "epoch": 1.095599606354562,
      "grad_norm": 1.4413321018218994,
      "learning_rate": 2.6259616030812174e-05,
      "loss": 1.0271,
      "step": 7793
    },
    {
      "epoch": 1.095740194010966,
      "grad_norm": 2.8124148845672607,
      "learning_rate": 2.641683715341555e-05,
      "loss": 0.9621,
      "step": 7794
    },
    {
      "epoch": 1.0958807816673697,
      "grad_norm": 1.4814436435699463,
      "learning_rate": 2.657445593442891e-05,
      "loss": 1.0372,
      "step": 7795
    },
    {
      "epoch": 1.0960213693237735,
      "grad_norm": 1.5962332487106323,
      "learning_rate": 2.673247152204871e-05,
      "loss": 0.9404,
      "step": 7796
    },
    {
      "epoch": 1.096161956980177,
      "grad_norm": 1.487452745437622,
      "learning_rate": 2.6890883062326243e-05,
      "loss": 1.125,
      "step": 7797
    },
    {
      "epoch": 1.0963025446365808,
      "grad_norm": 1.6293123960494995,
      "learning_rate": 2.704968969917353e-05,
      "loss": 0.9483,
      "step": 7798
    },
    {
      "epoch": 1.0964431322929846,
      "grad_norm": 1.8828271627426147,
      "learning_rate": 2.720889057436664e-05,
      "loss": 1.1857,
      "step": 7799
    },
    {
      "epoch": 1.0965837199493884,
      "grad_norm": 1.7038354873657227,
      "learning_rate": 2.736848482755159e-05,
      "loss": 1.0669,
      "step": 7800
    },
    {
      "epoch": 1.0967243076057922,
      "grad_norm": 1.628753900527954,
      "learning_rate": 2.7528471596248995e-05,
      "loss": 1.0681,
      "step": 7801
    },
    {
      "epoch": 1.096864895262196,
      "grad_norm": 1.666140079498291,
      "learning_rate": 2.7688850015857502e-05,
      "loss": 1.0575,
      "step": 7802
    },
    {
      "epoch": 1.0970054829185998,
      "grad_norm": 1.3949599266052246,
      "learning_rate": 2.7849619219659452e-05,
      "loss": 1.0799,
      "step": 7803
    },
    {
      "epoch": 1.0971460705750036,
      "grad_norm": 1.518371820449829,
      "learning_rate": 2.8010778338825284e-05,
      "loss": 1.1259,
      "step": 7804
    },
    {
      "epoch": 1.0972866582314074,
      "grad_norm": 1.7984519004821777,
      "learning_rate": 2.817232650241828e-05,
      "loss": 0.8885,
      "step": 7805
    },
    {
      "epoch": 1.0974272458878112,
      "grad_norm": 1.731583833694458,
      "learning_rate": 2.833426283739923e-05,
      "loss": 0.9537,
      "step": 7806
    },
    {
      "epoch": 1.0975678335442147,
      "grad_norm": 1.655996322631836,
      "learning_rate": 2.849658646863117e-05,
      "loss": 1.0163,
      "step": 7807
    },
    {
      "epoch": 1.0977084212006185,
      "grad_norm": 1.6298633813858032,
      "learning_rate": 2.8659296518884128e-05,
      "loss": 1.2297,
      "step": 7808
    },
    {
      "epoch": 1.0978490088570223,
      "grad_norm": 1.4870564937591553,
      "learning_rate": 2.8822392108839812e-05,
      "loss": 1.1579,
      "step": 7809
    },
    {
      "epoch": 1.097989596513426,
      "grad_norm": 1.3555018901824951,
      "learning_rate": 2.8985872357096412e-05,
      "loss": 0.8493,
      "step": 7810
    },
    {
      "epoch": 1.0981301841698299,
      "grad_norm": 1.4079304933547974,
      "learning_rate": 2.9149736380173098e-05,
      "loss": 1.0635,
      "step": 7811
    },
    {
      "epoch": 1.0982707718262337,
      "grad_norm": 1.644815444946289,
      "learning_rate": 2.931398329251579e-05,
      "loss": 0.8592,
      "step": 7812
    },
    {
      "epoch": 1.0984113594826375,
      "grad_norm": 1.5343973636627197,
      "learning_rate": 2.94786122065007e-05,
      "loss": 1.0987,
      "step": 7813
    },
    {
      "epoch": 1.0985519471390413,
      "grad_norm": 1.457076907157898,
      "learning_rate": 2.9643622232439118e-05,
      "loss": 0.9865,
      "step": 7814
    },
    {
      "epoch": 1.098692534795445,
      "grad_norm": 1.8411927223205566,
      "learning_rate": 2.9809012478583476e-05,
      "loss": 0.9596,
      "step": 7815
    },
    {
      "epoch": 1.0988331224518488,
      "grad_norm": 1.539038062095642,
      "learning_rate": 2.9974782051130924e-05,
      "loss": 1.0187,
      "step": 7816
    },
    {
      "epoch": 1.0989737101082524,
      "grad_norm": 1.337794303894043,
      "learning_rate": 3.01409300542294e-05,
      "loss": 0.9787,
      "step": 7817
    },
    {
      "epoch": 1.0991142977646562,
      "grad_norm": 1.6686553955078125,
      "learning_rate": 3.0307455589980993e-05,
      "loss": 0.8199,
      "step": 7818
    },
    {
      "epoch": 1.09925488542106,
      "grad_norm": 1.4642328023910522,
      "learning_rate": 3.0474357758447758e-05,
      "loss": 1.0986,
      "step": 7819
    },
    {
      "epoch": 1.0993954730774638,
      "grad_norm": 1.459033489227295,
      "learning_rate": 3.0641635657656354e-05,
      "loss": 0.9713,
      "step": 7820
    },
    {
      "epoch": 1.0995360607338676,
      "grad_norm": 1.5468670129776,
      "learning_rate": 3.0809288383602916e-05,
      "loss": 1.0657,
      "step": 7821
    },
    {
      "epoch": 1.0996766483902713,
      "grad_norm": 1.4887806177139282,
      "learning_rate": 3.097731503025796e-05,
      "loss": 1.0604,
      "step": 7822
    },
    {
      "epoch": 1.0998172360466751,
      "grad_norm": 1.408673882484436,
      "learning_rate": 3.114571468957122e-05,
      "loss": 1.0361,
      "step": 7823
    },
    {
      "epoch": 1.099957823703079,
      "grad_norm": 1.99740469455719,
      "learning_rate": 3.131448645147661e-05,
      "loss": 1.0075,
      "step": 7824
    },
    {
      "epoch": 1.1000984113594827,
      "grad_norm": 1.6392226219177246,
      "learning_rate": 3.148362940389714e-05,
      "loss": 0.9843,
      "step": 7825
    },
    {
      "epoch": 1.1002389990158865,
      "grad_norm": 1.4734517335891724,
      "learning_rate": 3.165314263274982e-05,
      "loss": 1.0071,
      "step": 7826
    },
    {
      "epoch": 1.10037958667229,
      "grad_norm": 1.4516432285308838,
      "learning_rate": 3.182302522195035e-05,
      "loss": 0.9085,
      "step": 7827
    },
    {
      "epoch": 1.1005201743286939,
      "grad_norm": 1.3409602642059326,
      "learning_rate": 3.199327625341937e-05,
      "loss": 1.0401,
      "step": 7828
    },
    {
      "epoch": 1.1006607619850977,
      "grad_norm": 1.5317832231521606,
      "learning_rate": 3.2163894807084816e-05,
      "loss": 1.0554,
      "step": 7829
    },
    {
      "epoch": 1.1008013496415014,
      "grad_norm": 1.6291999816894531,
      "learning_rate": 3.233487996088951e-05,
      "loss": 1.0181,
      "step": 7830
    },
    {
      "epoch": 1.1009419372979052,
      "grad_norm": 1.4603968858718872,
      "learning_rate": 3.250623079079483e-05,
      "loss": 1.0431,
      "step": 7831
    },
    {
      "epoch": 1.101082524954309,
      "grad_norm": 1.3450168371200562,
      "learning_rate": 3.267794637078572e-05,
      "loss": 1.096,
      "step": 7832
    },
    {
      "epoch": 1.1012231126107128,
      "grad_norm": 1.4562668800354004,
      "learning_rate": 3.285002577287668e-05,
      "loss": 1.0953,
      "step": 7833
    },
    {
      "epoch": 1.1013637002671166,
      "grad_norm": 1.4175511598587036,
      "learning_rate": 3.302246806711531e-05,
      "loss": 1.1193,
      "step": 7834
    },
    {
      "epoch": 1.1015042879235204,
      "grad_norm": 1.4504954814910889,
      "learning_rate": 3.31952723215883e-05,
      "loss": 1.1495,
      "step": 7835
    },
    {
      "epoch": 1.101644875579924,
      "grad_norm": 1.7099275588989258,
      "learning_rate": 3.3368437602426487e-05,
      "loss": 0.9342,
      "step": 7836
    },
    {
      "epoch": 1.1017854632363278,
      "grad_norm": 1.556854009628296,
      "learning_rate": 3.354196297380887e-05,
      "loss": 1.1444,
      "step": 7837
    },
    {
      "epoch": 1.1019260508927315,
      "grad_norm": 1.6294986009597778,
      "learning_rate": 3.371584749796899e-05,
      "loss": 1.108,
      "step": 7838
    },
    {
      "epoch": 1.1020666385491353,
      "grad_norm": 1.4379794597625732,
      "learning_rate": 3.389009023519996e-05,
      "loss": 1.0241,
      "step": 7839
    },
    {
      "epoch": 1.1022072262055391,
      "grad_norm": 1.3269388675689697,
      "learning_rate": 3.406469024385823e-05,
      "loss": 0.9789,
      "step": 7840
    },
    {
      "epoch": 1.102347813861943,
      "grad_norm": 1.523784875869751,
      "learning_rate": 3.4239646580369787e-05,
      "loss": 0.9252,
      "step": 7841
    },
    {
      "epoch": 1.1024884015183467,
      "grad_norm": 1.5624134540557861,
      "learning_rate": 3.441495829923492e-05,
      "loss": 0.9403,
      "step": 7842
    },
    {
      "epoch": 1.1026289891747505,
      "grad_norm": 1.3606758117675781,
      "learning_rate": 3.459062445303337e-05,
      "loss": 1.0133,
      "step": 7843
    },
    {
      "epoch": 1.1027695768311543,
      "grad_norm": 1.5273842811584473,
      "learning_rate": 3.476664409242942e-05,
      "loss": 1.0364,
      "step": 7844
    },
    {
      "epoch": 1.102910164487558,
      "grad_norm": 1.6799002885818481,
      "learning_rate": 3.4943016266177086e-05,
      "loss": 0.953,
      "step": 7845
    },
    {
      "epoch": 1.1030507521439619,
      "grad_norm": 1.5040037631988525,
      "learning_rate": 3.511974002112518e-05,
      "loss": 1.0225,
      "step": 7846
    },
    {
      "epoch": 1.1031913398003654,
      "grad_norm": 1.5151273012161255,
      "learning_rate": 3.529681440222251e-05,
      "loss": 1.1004,
      "step": 7847
    },
    {
      "epoch": 1.1033319274567692,
      "grad_norm": 1.6755423545837402,
      "learning_rate": 3.5474238452522755e-05,
      "loss": 1.0557,
      "step": 7848
    },
    {
      "epoch": 1.103472515113173,
      "grad_norm": 1.453334093093872,
      "learning_rate": 3.565201121319072e-05,
      "loss": 1.1142,
      "step": 7849
    },
    {
      "epoch": 1.1036131027695768,
      "grad_norm": 1.3066785335540771,
      "learning_rate": 3.583013172350591e-05,
      "loss": 1.2327,
      "step": 7850
    },
    {
      "epoch": 1.1037536904259806,
      "grad_norm": 1.7947720289230347,
      "learning_rate": 3.600859902086906e-05,
      "loss": 1.1346,
      "step": 7851
    },
    {
      "epoch": 1.1038942780823844,
      "grad_norm": 1.539158582687378,
      "learning_rate": 3.618741214080595e-05,
      "loss": 0.943,
      "step": 7852
    },
    {
      "epoch": 1.1040348657387882,
      "grad_norm": 1.4725549221038818,
      "learning_rate": 3.636657011697434e-05,
      "loss": 1.0851,
      "step": 7853
    },
    {
      "epoch": 1.104175453395192,
      "grad_norm": 1.4900853633880615,
      "learning_rate": 3.654607198116771e-05,
      "loss": 0.9863,
      "step": 7854
    },
    {
      "epoch": 1.1043160410515958,
      "grad_norm": 1.400316596031189,
      "learning_rate": 3.6725916763321945e-05,
      "loss": 1.0725,
      "step": 7855
    },
    {
      "epoch": 1.1044566287079993,
      "grad_norm": 1.5137991905212402,
      "learning_rate": 3.6906103491518925e-05,
      "loss": 1.0123,
      "step": 7856
    },
    {
      "epoch": 1.1045972163644031,
      "grad_norm": 1.483134388923645,
      "learning_rate": 3.7086631191992835e-05,
      "loss": 1.0199,
      "step": 7857
    },
    {
      "epoch": 1.104737804020807,
      "grad_norm": 1.600327730178833,
      "learning_rate": 3.726749888913519e-05,
      "loss": 1.1533,
      "step": 7858
    },
    {
      "epoch": 1.1048783916772107,
      "grad_norm": 1.3715656995773315,
      "learning_rate": 3.744870560550009e-05,
      "loss": 1.3126,
      "step": 7859
    },
    {
      "epoch": 1.1050189793336145,
      "grad_norm": 1.4890981912612915,
      "learning_rate": 3.763025036180951e-05,
      "loss": 1.0227,
      "step": 7860
    },
    {
      "epoch": 1.1051595669900183,
      "grad_norm": 1.620696783065796,
      "learning_rate": 3.7812132176958557e-05,
      "loss": 1.0354,
      "step": 7861
    },
    {
      "epoch": 1.105300154646422,
      "grad_norm": 1.355808973312378,
      "learning_rate": 3.7994350068020866e-05,
      "loss": 1.2721,
      "step": 7862
    },
    {
      "epoch": 1.1054407423028259,
      "grad_norm": 1.6032540798187256,
      "learning_rate": 3.8176903050253786e-05,
      "loss": 0.9166,
      "step": 7863
    },
    {
      "epoch": 1.1055813299592296,
      "grad_norm": 1.5114414691925049,
      "learning_rate": 3.83597901371038e-05,
      "loss": 0.9524,
      "step": 7864
    },
    {
      "epoch": 1.1057219176156334,
      "grad_norm": 1.6334044933319092,
      "learning_rate": 3.854301034021155e-05,
      "loss": 0.9988,
      "step": 7865
    },
    {
      "epoch": 1.1058625052720372,
      "grad_norm": 1.4789531230926514,
      "learning_rate": 3.872656266941852e-05,
      "loss": 1.0319,
      "step": 7866
    },
    {
      "epoch": 1.1060030929284408,
      "grad_norm": 1.6051043272018433,
      "learning_rate": 3.8910446132769696e-05,
      "loss": 1.0598,
      "step": 7867
    },
    {
      "epoch": 1.1061436805848446,
      "grad_norm": 1.4525437355041504,
      "learning_rate": 3.909465973652164e-05,
      "loss": 1.1452,
      "step": 7868
    },
    {
      "epoch": 1.1062842682412484,
      "grad_norm": 1.5192784070968628,
      "learning_rate": 3.927920248514648e-05,
      "loss": 1.2197,
      "step": 7869
    },
    {
      "epoch": 1.1064248558976522,
      "grad_norm": 1.3208376169204712,
      "learning_rate": 3.946407338133731e-05,
      "loss": 1.0808,
      "step": 7870
    },
    {
      "epoch": 1.106565443554056,
      "grad_norm": 1.5668574571609497,
      "learning_rate": 3.964927142601468e-05,
      "loss": 1.1138,
      "step": 7871
    },
    {
      "epoch": 1.1067060312104597,
      "grad_norm": 1.2960551977157593,
      "learning_rate": 3.9834795618330345e-05,
      "loss": 1.0412,
      "step": 7872
    },
    {
      "epoch": 1.1068466188668635,
      "grad_norm": 1.2784868478775024,
      "learning_rate": 4.002064495567373e-05,
      "loss": 0.9497,
      "step": 7873
    },
    {
      "epoch": 1.1069872065232673,
      "grad_norm": 1.5779016017913818,
      "learning_rate": 4.020681843367745e-05,
      "loss": 1.1199,
      "step": 7874
    },
    {
      "epoch": 1.107127794179671,
      "grad_norm": 1.5163078308105469,
      "learning_rate": 4.039331504622121e-05,
      "loss": 0.8169,
      "step": 7875
    },
    {
      "epoch": 1.1072683818360747,
      "grad_norm": 1.3817194700241089,
      "learning_rate": 4.0580133785440165e-05,
      "loss": 0.9736,
      "step": 7876
    },
    {
      "epoch": 1.1074089694924785,
      "grad_norm": 1.6865366697311401,
      "learning_rate": 4.076727364172748e-05,
      "loss": 1.1877,
      "step": 7877
    },
    {
      "epoch": 1.1075495571488823,
      "grad_norm": 1.4314329624176025,
      "learning_rate": 4.0954733603741225e-05,
      "loss": 0.9843,
      "step": 7878
    },
    {
      "epoch": 1.107690144805286,
      "grad_norm": 1.317398190498352,
      "learning_rate": 4.114251265840956e-05,
      "loss": 1.0966,
      "step": 7879
    },
    {
      "epoch": 1.1078307324616898,
      "grad_norm": 1.6348400115966797,
      "learning_rate": 4.13306097909362e-05,
      "loss": 1.1201,
      "step": 7880
    },
    {
      "epoch": 1.1079713201180936,
      "grad_norm": 1.5259648561477661,
      "learning_rate": 4.1519023984805604e-05,
      "loss": 1.0963,
      "step": 7881
    },
    {
      "epoch": 1.1081119077744974,
      "grad_norm": 1.6296329498291016,
      "learning_rate": 4.1707754221789954e-05,
      "loss": 0.9808,
      "step": 7882
    },
    {
      "epoch": 1.1082524954309012,
      "grad_norm": 1.4138864278793335,
      "learning_rate": 4.189679948195169e-05,
      "loss": 0.9863,
      "step": 7883
    },
    {
      "epoch": 1.108393083087305,
      "grad_norm": 1.3486350774765015,
      "learning_rate": 4.208615874365198e-05,
      "loss": 1.1282,
      "step": 7884
    },
    {
      "epoch": 1.1085336707437088,
      "grad_norm": 1.473254919052124,
      "learning_rate": 4.22758309835548e-05,
      "loss": 1.0829,
      "step": 7885
    },
    {
      "epoch": 1.1086742584001126,
      "grad_norm": 1.4412426948547363,
      "learning_rate": 4.246581517663241e-05,
      "loss": 1.1438,
      "step": 7886
    },
    {
      "epoch": 1.1088148460565161,
      "grad_norm": 1.4558470249176025,
      "learning_rate": 4.265611029617207e-05,
      "loss": 1.0954,
      "step": 7887
    },
    {
      "epoch": 1.10895543371292,
      "grad_norm": 1.5786532163619995,
      "learning_rate": 4.28467153137799e-05,
      "loss": 1.0917,
      "step": 7888
    },
    {
      "epoch": 1.1090960213693237,
      "grad_norm": 1.5764168500900269,
      "learning_rate": 4.30376291993878e-05,
      "loss": 1.0268,
      "step": 7889
    },
    {
      "epoch": 1.1092366090257275,
      "grad_norm": 1.5024412870407104,
      "learning_rate": 4.322885092125766e-05,
      "loss": 0.9823,
      "step": 7890
    },
    {
      "epoch": 1.1093771966821313,
      "grad_norm": 1.41814386844635,
      "learning_rate": 4.342037944598829e-05,
      "loss": 0.9697,
      "step": 7891
    },
    {
      "epoch": 1.109517784338535,
      "grad_norm": 1.4785349369049072,
      "learning_rate": 4.3612213738521126e-05,
      "loss": 1.1834,
      "step": 7892
    },
    {
      "epoch": 1.1096583719949389,
      "grad_norm": 1.602854609489441,
      "learning_rate": 4.380435276214423e-05,
      "loss": 0.8386,
      "step": 7893
    },
    {
      "epoch": 1.1097989596513427,
      "grad_norm": 1.829559564590454,
      "learning_rate": 4.399679547849916e-05,
      "loss": 1.0878,
      "step": 7894
    },
    {
      "epoch": 1.1099395473077465,
      "grad_norm": 1.359503984451294,
      "learning_rate": 4.418954084758627e-05,
      "loss": 1.1782,
      "step": 7895
    },
    {
      "epoch": 1.11008013496415,
      "grad_norm": 1.370397925376892,
      "learning_rate": 4.4382587827770275e-05,
      "loss": 1.1248,
      "step": 7896
    },
    {
      "epoch": 1.1102207226205538,
      "grad_norm": 1.4744514226913452,
      "learning_rate": 4.457593537578596e-05,
      "loss": 0.8539,
      "step": 7897
    },
    {
      "epoch": 1.1103613102769576,
      "grad_norm": 1.8199747800827026,
      "learning_rate": 4.476958244674375e-05,
      "loss": 1.3176,
      "step": 7898
    },
    {
      "epoch": 1.1105018979333614,
      "grad_norm": 1.4221042394638062,
      "learning_rate": 4.496352799413542e-05,
      "loss": 1.0566,
      "step": 7899
    },
    {
      "epoch": 1.1106424855897652,
      "grad_norm": 1.36775541305542,
      "learning_rate": 4.515777096983971e-05,
      "loss": 1.0123,
      "step": 7900
    },
    {
      "epoch": 1.110783073246169,
      "grad_norm": 1.651718258857727,
      "learning_rate": 4.5352310324127955e-05,
      "loss": 1.0139,
      "step": 7901
    },
    {
      "epoch": 1.1109236609025728,
      "grad_norm": 1.709584355354309,
      "learning_rate": 4.554714500566956e-05,
      "loss": 1.1801,
      "step": 7902
    },
    {
      "epoch": 1.1110642485589766,
      "grad_norm": 1.6195406913757324,
      "learning_rate": 4.5742273961538775e-05,
      "loss": 0.9931,
      "step": 7903
    },
    {
      "epoch": 1.1112048362153804,
      "grad_norm": 1.7617369890213013,
      "learning_rate": 4.5937696137218966e-05,
      "loss": 0.866,
      "step": 7904
    },
    {
      "epoch": 1.1113454238717841,
      "grad_norm": 1.3472628593444824,
      "learning_rate": 4.613341047660825e-05,
      "loss": 1.2694,
      "step": 7905
    },
    {
      "epoch": 1.111486011528188,
      "grad_norm": 1.708240270614624,
      "learning_rate": 4.6329415922026756e-05,
      "loss": 1.0319,
      "step": 7906
    },
    {
      "epoch": 1.1116265991845915,
      "grad_norm": 1.5368905067443848,
      "learning_rate": 4.65257114142211e-05,
      "loss": 0.9435,
      "step": 7907
    },
    {
      "epoch": 1.1117671868409953,
      "grad_norm": 1.2923450469970703,
      "learning_rate": 4.6722295892370126e-05,
      "loss": 1.2371,
      "step": 7908
    },
    {
      "epoch": 1.111907774497399,
      "grad_norm": 1.2553902864456177,
      "learning_rate": 4.691916829409187e-05,
      "loss": 0.9351,
      "step": 7909
    },
    {
      "epoch": 1.1120483621538029,
      "grad_norm": 1.4772344827651978,
      "learning_rate": 4.711632755544741e-05,
      "loss": 1.1483,
      "step": 7910
    },
    {
      "epoch": 1.1121889498102067,
      "grad_norm": 1.5147767066955566,
      "learning_rate": 4.731377261094793e-05,
      "loss": 0.9702,
      "step": 7911
    },
    {
      "epoch": 1.1123295374666105,
      "grad_norm": 1.5177935361862183,
      "learning_rate": 4.7511502393560414e-05,
      "loss": 0.9033,
      "step": 7912
    },
    {
      "epoch": 1.1124701251230142,
      "grad_norm": 1.337714672088623,
      "learning_rate": 4.770951583471189e-05,
      "loss": 1.2071,
      "step": 7913
    },
    {
      "epoch": 1.112610712779418,
      "grad_norm": 1.4255610704421997,
      "learning_rate": 4.790781186429829e-05,
      "loss": 0.878,
      "step": 7914
    },
    {
      "epoch": 1.1127513004358218,
      "grad_norm": 1.8556663990020752,
      "learning_rate": 4.810638941068714e-05,
      "loss": 1.21,
      "step": 7915
    },
    {
      "epoch": 1.1128918880922254,
      "grad_norm": 1.4839733839035034,
      "learning_rate": 4.8305247400724885e-05,
      "loss": 1.1605,
      "step": 7916
    },
    {
      "epoch": 1.1130324757486292,
      "grad_norm": 1.578125238418579,
      "learning_rate": 4.850438475974244e-05,
      "loss": 1.0195,
      "step": 7917
    },
    {
      "epoch": 1.113173063405033,
      "grad_norm": 1.57439386844635,
      "learning_rate": 4.8703800411560617e-05,
      "loss": 1.0589,
      "step": 7918
    },
    {
      "epoch": 1.1133136510614368,
      "grad_norm": 1.5192112922668457,
      "learning_rate": 4.8903493278497194e-05,
      "loss": 0.9895,
      "step": 7919
    },
    {
      "epoch": 1.1134542387178405,
      "grad_norm": 1.5751876831054688,
      "learning_rate": 4.910346228137122e-05,
      "loss": 1.0719,
      "step": 7920
    },
    {
      "epoch": 1.1135948263742443,
      "grad_norm": 1.518311619758606,
      "learning_rate": 4.930370633950883e-05,
      "loss": 1.0809,
      "step": 7921
    },
    {
      "epoch": 1.1137354140306481,
      "grad_norm": 1.5506244897842407,
      "learning_rate": 4.9504224370750616e-05,
      "loss": 1.0798,
      "step": 7922
    },
    {
      "epoch": 1.113876001687052,
      "grad_norm": 1.3660707473754883,
      "learning_rate": 4.970501529145628e-05,
      "loss": 0.9848,
      "step": 7923
    },
    {
      "epoch": 1.1140165893434557,
      "grad_norm": 1.5942387580871582,
      "learning_rate": 4.990607801651044e-05,
      "loss": 1.062,
      "step": 7924
    },
    {
      "epoch": 1.1141571769998595,
      "grad_norm": 1.6674541234970093,
      "learning_rate": 5.0107411459329776e-05,
      "loss": 0.9636,
      "step": 7925
    },
    {
      "epoch": 1.1142977646562633,
      "grad_norm": 1.8387149572372437,
      "learning_rate": 5.0309014531867004e-05,
      "loss": 1.1829,
      "step": 7926
    },
    {
      "epoch": 1.1144383523126669,
      "grad_norm": 1.5636913776397705,
      "learning_rate": 5.0510886144618366e-05,
      "loss": 1.2018,
      "step": 7927
    },
    {
      "epoch": 1.1145789399690706,
      "grad_norm": 1.4404199123382568,
      "learning_rate": 5.071302520662786e-05,
      "loss": 1.2407,
      "step": 7928
    },
    {
      "epoch": 1.1147195276254744,
      "grad_norm": 1.4563047885894775,
      "learning_rate": 5.091543062549482e-05,
      "loss": 1.095,
      "step": 7929
    },
    {
      "epoch": 1.1148601152818782,
      "grad_norm": 1.6091216802597046,
      "learning_rate": 5.111810130737973e-05,
      "loss": 1.1718,
      "step": 7930
    },
    {
      "epoch": 1.115000702938282,
      "grad_norm": 1.328715205192566,
      "learning_rate": 5.1321036157008586e-05,
      "loss": 1.0043,
      "step": 7931
    },
    {
      "epoch": 1.1151412905946858,
      "grad_norm": 1.6586750745773315,
      "learning_rate": 5.15242340776801e-05,
      "loss": 1.0744,
      "step": 7932
    },
    {
      "epoch": 1.1152818782510896,
      "grad_norm": 1.3809967041015625,
      "learning_rate": 5.172769397127132e-05,
      "loss": 1.1155,
      "step": 7933
    },
    {
      "epoch": 1.1154224659074934,
      "grad_norm": 1.5830093622207642,
      "learning_rate": 5.193141473824317e-05,
      "loss": 1.0119,
      "step": 7934
    },
    {
      "epoch": 1.1155630535638972,
      "grad_norm": 1.6008999347686768,
      "learning_rate": 5.213539527764804e-05,
      "loss": 1.0278,
      "step": 7935
    },
    {
      "epoch": 1.1157036412203007,
      "grad_norm": 1.6205440759658813,
      "learning_rate": 5.233963448713258e-05,
      "loss": 0.9428,
      "step": 7936
    },
    {
      "epoch": 1.1158442288767045,
      "grad_norm": 1.3415147066116333,
      "learning_rate": 5.254413126294679e-05,
      "loss": 0.9587,
      "step": 7937
    },
    {
      "epoch": 1.1159848165331083,
      "grad_norm": 1.4445840120315552,
      "learning_rate": 5.274888449994844e-05,
      "loss": 1.1761,
      "step": 7938
    },
    {
      "epoch": 1.1161254041895121,
      "grad_norm": 1.5387409925460815,
      "learning_rate": 5.295389309160925e-05,
      "loss": 1.0154,
      "step": 7939
    },
    {
      "epoch": 1.116265991845916,
      "grad_norm": 1.7489383220672607,
      "learning_rate": 5.3159155930020744e-05,
      "loss": 1.0725,
      "step": 7940
    },
    {
      "epoch": 1.1164065795023197,
      "grad_norm": 1.4648561477661133,
      "learning_rate": 5.3364671905901285e-05,
      "loss": 0.9453,
      "step": 7941
    },
    {
      "epoch": 1.1165471671587235,
      "grad_norm": 1.48971688747406,
      "learning_rate": 5.357043990860068e-05,
      "loss": 0.9265,
      "step": 7942
    },
    {
      "epoch": 1.1166877548151273,
      "grad_norm": 1.5295847654342651,
      "learning_rate": 5.377645882610606e-05,
      "loss": 1.0381,
      "step": 7943
    },
    {
      "epoch": 1.116828342471531,
      "grad_norm": 1.4630861282348633,
      "learning_rate": 5.3982727545049514e-05,
      "loss": 1.2894,
      "step": 7944
    },
    {
      "epoch": 1.1169689301279349,
      "grad_norm": 1.5402311086654663,
      "learning_rate": 5.418924495071256e-05,
      "loss": 1.1568,
      "step": 7945
    },
    {
      "epoch": 1.1171095177843386,
      "grad_norm": 1.630549669265747,
      "learning_rate": 5.439600992703362e-05,
      "loss": 0.9492,
      "step": 7946
    },
    {
      "epoch": 1.1172501054407422,
      "grad_norm": 1.590765118598938,
      "learning_rate": 5.460302135661235e-05,
      "loss": 1.045,
      "step": 7947
    },
    {
      "epoch": 1.117390693097146,
      "grad_norm": 1.5419087409973145,
      "learning_rate": 5.481027812071676e-05,
      "loss": 1.1407,
      "step": 7948
    },
    {
      "epoch": 1.1175312807535498,
      "grad_norm": 1.4255270957946777,
      "learning_rate": 5.501777909928907e-05,
      "loss": 1.0504,
      "step": 7949
    },
    {
      "epoch": 1.1176718684099536,
      "grad_norm": 1.401212453842163,
      "learning_rate": 5.522552317095201e-05,
      "loss": 0.907,
      "step": 7950
    },
    {
      "epoch": 1.1178124560663574,
      "grad_norm": 1.408838152885437,
      "learning_rate": 5.543350921301331e-05,
      "loss": 1.1341,
      "step": 7951
    },
    {
      "epoch": 1.1179530437227612,
      "grad_norm": 1.8662331104278564,
      "learning_rate": 5.5641736101474986e-05,
      "loss": 0.9859,
      "step": 7952
    },
    {
      "epoch": 1.118093631379165,
      "grad_norm": 1.5770279169082642,
      "learning_rate": 5.585020271103614e-05,
      "loss": 0.8627,
      "step": 7953
    },
    {
      "epoch": 1.1182342190355687,
      "grad_norm": 1.4958443641662598,
      "learning_rate": 5.605890791510073e-05,
      "loss": 1.2544,
      "step": 7954
    },
    {
      "epoch": 1.1183748066919725,
      "grad_norm": 1.5972412824630737,
      "learning_rate": 5.626785058578332e-05,
      "loss": 0.9813,
      "step": 7955
    },
    {
      "epoch": 1.118515394348376,
      "grad_norm": 1.5777820348739624,
      "learning_rate": 5.647702959391479e-05,
      "loss": 0.992,
      "step": 7956
    },
    {
      "epoch": 1.11865598200478,
      "grad_norm": 1.3535972833633423,
      "learning_rate": 5.668644380904977e-05,
      "loss": 0.9923,
      "step": 7957
    },
    {
      "epoch": 1.1187965696611837,
      "grad_norm": 1.7189544439315796,
      "learning_rate": 5.6896092099471176e-05,
      "loss": 1.0802,
      "step": 7958
    },
    {
      "epoch": 1.1189371573175875,
      "grad_norm": 1.3111454248428345,
      "learning_rate": 5.710597333219627e-05,
      "loss": 1.0394,
      "step": 7959
    },
    {
      "epoch": 1.1190777449739913,
      "grad_norm": 1.6484637260437012,
      "learning_rate": 5.7316086372984426e-05,
      "loss": 0.9136,
      "step": 7960
    },
    {
      "epoch": 1.119218332630395,
      "grad_norm": 1.4452550411224365,
      "learning_rate": 5.75264300863416e-05,
      "loss": 1.1732,
      "step": 7961
    },
    {
      "epoch": 1.1193589202867988,
      "grad_norm": 1.6288236379623413,
      "learning_rate": 5.773700333552814e-05,
      "loss": 1.0822,
      "step": 7962
    },
    {
      "epoch": 1.1194995079432026,
      "grad_norm": 1.4663652181625366,
      "learning_rate": 5.794780498256297e-05,
      "loss": 1.0446,
      "step": 7963
    },
    {
      "epoch": 1.1196400955996064,
      "grad_norm": 1.626695156097412,
      "learning_rate": 5.8158833888231024e-05,
      "loss": 1.0909,
      "step": 7964
    },
    {
      "epoch": 1.1197806832560102,
      "grad_norm": 1.5957512855529785,
      "learning_rate": 5.837008891208941e-05,
      "loss": 0.9785,
      "step": 7965
    },
    {
      "epoch": 1.119921270912414,
      "grad_norm": 1.7162519693374634,
      "learning_rate": 5.8581568912472254e-05,
      "loss": 0.9633,
      "step": 7966
    },
    {
      "epoch": 1.1200618585688176,
      "grad_norm": 1.4954910278320312,
      "learning_rate": 5.8793272746498554e-05,
      "loss": 1.2382,
      "step": 7967
    },
    {
      "epoch": 1.1202024462252214,
      "grad_norm": 1.5099384784698486,
      "learning_rate": 5.900519927007826e-05,
      "loss": 0.9968,
      "step": 7968
    },
    {
      "epoch": 1.1203430338816251,
      "grad_norm": 1.430387258529663,
      "learning_rate": 5.921734733791694e-05,
      "loss": 1.1743,
      "step": 7969
    },
    {
      "epoch": 1.120483621538029,
      "grad_norm": 1.425026774406433,
      "learning_rate": 5.942971580352319e-05,
      "loss": 1.24,
      "step": 7970
    },
    {
      "epoch": 1.1206242091944327,
      "grad_norm": 1.507201910018921,
      "learning_rate": 5.964230351921452e-05,
      "loss": 1.031,
      "step": 7971
    },
    {
      "epoch": 1.1207647968508365,
      "grad_norm": 1.455224633216858,
      "learning_rate": 5.985510933612328e-05,
      "loss": 1.1249,
      "step": 7972
    },
    {
      "epoch": 1.1209053845072403,
      "grad_norm": 1.7587783336639404,
      "learning_rate": 6.0068132104204435e-05,
      "loss": 1.1507,
      "step": 7973
    },
    {
      "epoch": 1.121045972163644,
      "grad_norm": 1.5784313678741455,
      "learning_rate": 6.028137067223852e-05,
      "loss": 0.8126,
      "step": 7974
    },
    {
      "epoch": 1.121186559820048,
      "grad_norm": 1.5240775346755981,
      "learning_rate": 6.0494823887841225e-05,
      "loss": 0.9116,
      "step": 7975
    },
    {
      "epoch": 1.1213271474764515,
      "grad_norm": 1.269802212715149,
      "learning_rate": 6.070849059746782e-05,
      "loss": 1.1228,
      "step": 7976
    },
    {
      "epoch": 1.1214677351328552,
      "grad_norm": 1.5440120697021484,
      "learning_rate": 6.0922369646419875e-05,
      "loss": 1.1362,
      "step": 7977
    },
    {
      "epoch": 1.121608322789259,
      "grad_norm": 1.6962108612060547,
      "learning_rate": 6.113645987885108e-05,
      "loss": 0.9861,
      "step": 7978
    },
    {
      "epoch": 1.1217489104456628,
      "grad_norm": 1.3980650901794434,
      "learning_rate": 6.135076013777485e-05,
      "loss": 1.0126,
      "step": 7979
    },
    {
      "epoch": 1.1218894981020666,
      "grad_norm": 1.4710735082626343,
      "learning_rate": 6.156526926506859e-05,
      "loss": 1.069,
      "step": 7980
    },
    {
      "epoch": 1.1220300857584704,
      "grad_norm": 1.5446945428848267,
      "learning_rate": 6.177998610148152e-05,
      "loss": 0.9332,
      "step": 7981
    },
    {
      "epoch": 1.1221706734148742,
      "grad_norm": 1.5846105813980103,
      "learning_rate": 6.199490948663951e-05,
      "loss": 1.0901,
      "step": 7982
    },
    {
      "epoch": 1.122311261071278,
      "grad_norm": 1.293689489364624,
      "learning_rate": 6.221003825905273e-05,
      "loss": 1.0442,
      "step": 7983
    },
    {
      "epoch": 1.1224518487276818,
      "grad_norm": 1.4767522811889648,
      "learning_rate": 6.242537125612213e-05,
      "loss": 1.059,
      "step": 7984
    },
    {
      "epoch": 1.1225924363840856,
      "grad_norm": 1.4490946531295776,
      "learning_rate": 6.264090731414397e-05,
      "loss": 1.0175,
      "step": 7985
    },
    {
      "epoch": 1.1227330240404894,
      "grad_norm": 1.500760793685913,
      "learning_rate": 6.285664526831743e-05,
      "loss": 0.8364,
      "step": 7986
    },
    {
      "epoch": 1.122873611696893,
      "grad_norm": 1.3824079036712646,
      "learning_rate": 6.307258395275067e-05,
      "loss": 1.0863,
      "step": 7987
    },
    {
      "epoch": 1.1230141993532967,
      "grad_norm": 1.859919786453247,
      "learning_rate": 6.3288722200467e-05,
      "loss": 0.8538,
      "step": 7988
    },
    {
      "epoch": 1.1231547870097005,
      "grad_norm": 1.7978018522262573,
      "learning_rate": 6.350505884341129e-05,
      "loss": 1.1617,
      "step": 7989
    },
    {
      "epoch": 1.1232953746661043,
      "grad_norm": 1.5698349475860596,
      "learning_rate": 6.372159271245623e-05,
      "loss": 1.0636,
      "step": 7990
    },
    {
      "epoch": 1.123435962322508,
      "grad_norm": 1.6500861644744873,
      "learning_rate": 6.393832263740865e-05,
      "loss": 1.09,
      "step": 7991
    },
    {
      "epoch": 1.1235765499789119,
      "grad_norm": 1.5823208093643188,
      "learning_rate": 6.415524744701587e-05,
      "loss": 1.1573,
      "step": 7992
    },
    {
      "epoch": 1.1237171376353157,
      "grad_norm": 1.3578847646713257,
      "learning_rate": 6.437236596897196e-05,
      "loss": 1.1355,
      "step": 7993
    },
    {
      "epoch": 1.1238577252917195,
      "grad_norm": 1.699781894683838,
      "learning_rate": 6.45896770299239e-05,
      "loss": 1.1667,
      "step": 7994
    },
    {
      "epoch": 1.1239983129481232,
      "grad_norm": 1.4669208526611328,
      "learning_rate": 6.480717945547905e-05,
      "loss": 1.0412,
      "step": 7995
    },
    {
      "epoch": 1.1241389006045268,
      "grad_norm": 1.7962921857833862,
      "learning_rate": 6.502487207020998e-05,
      "loss": 0.9855,
      "step": 7996
    },
    {
      "epoch": 1.1242794882609306,
      "grad_norm": 1.4989782571792603,
      "learning_rate": 6.524275369766077e-05,
      "loss": 1.0891,
      "step": 7997
    },
    {
      "epoch": 1.1244200759173344,
      "grad_norm": 1.4818774461746216,
      "learning_rate": 6.546082316035502e-05,
      "loss": 1.0732,
      "step": 7998
    },
    {
      "epoch": 1.1245606635737382,
      "grad_norm": 1.6167851686477661,
      "learning_rate": 6.567907927980059e-05,
      "loss": 0.9603,
      "step": 7999
    },
    {
      "epoch": 1.124701251230142,
      "grad_norm": 1.5754917860031128,
      "learning_rate": 6.589752087649754e-05,
      "loss": 1.1493,
      "step": 8000
    },
    {
      "epoch": 1.124701251230142,
      "eval_loss": 1.1398881673812866,
      "eval_runtime": 771.9541,
      "eval_samples_per_second": 16.382,
      "eval_steps_per_second": 8.191,
      "step": 8000
    }
  ],
  "logging_steps": 1,
  "max_steps": 14226,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.3512600024597873e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
