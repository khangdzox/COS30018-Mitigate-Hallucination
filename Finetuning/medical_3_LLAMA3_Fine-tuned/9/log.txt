training_args = TrainingArguments(
    learning_rate = 2e-4, # Learning rate change
    lr_scheduler_type = "cosine", # Control learning rate change
    warmup_ratio = 0.03,
    weight_decay = 0.01,
    save_strategy= "steps",
    save_steps= 10,
    logging_steps = 1,
    gradient_accumulation_steps = 4, # Accumulate gradients for larger batch size
    per_device_train_batch_size= 1, # Batch size per GPU (1 batch contain 1000 data points)
    max_steps = 110,
    seed = 3407,
    fp16 = True, # Use mixed precision training for faster training
    optim = "adamw_8bit", # Use 8-bit optimization for faster training
    group_by_length = True, # Group samples of same length to reduce padding and speed up training
    output_dir = "Finetuning/Fine-tuned_checkpoint/medical_3/9",
)

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = filtered_tokenized_dataset,
    dataset_text_field = "text",
    packing = False, # Can make training 5x faster for short sequences.
    eval_packing= False,
    args = training_args,
    max_seq_length= 2048,
)