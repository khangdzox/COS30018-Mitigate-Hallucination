# Config arguments for the training process
training_args = TrainingArguments(
    learning_rate = 2e-4, # Learning rate change 
    # lr_scheduler_type = "cosine_with_restarts", # Control learning rate change
    # lr_scheduler_kwargs= {"num_cycles": 2},
    warmup_ratio= 0.05,
    weight_decay = 0.01,
    save_strategy= "steps",
    save_steps= 10,
    eval_steps= 50,
    eval_strategy= "steps",
    logging_steps= 1,
    gradient_accumulation_steps = 16, # Accumulate gradients for larger batch size
    eval_accumulation_steps= 32,
    per_device_train_batch_size= 1, # Batch size per GPU 
    per_device_eval_batch_size= 2,
    max_steps = 500,
    seed = 3407,
    fp16 = True, # Use mixed precision training for faster training
    optim = "adafactor",
    # group_by_length = True, # Group samples of same length to reduce padding and speed up training
    output_dir = "Finetuning/Fine-tuned_checkpoint/medical_3/QLoRA/6",
    max_grad_norm= 1.0  # Apply gradient clipping
)
# LoRA config (adapter)
config = LoraConfig(
    r = 8,
    lora_alpha=32,
    lora_dropout=0.05, #kind of like a regularization dropout
    bias="none",
    task_type="CAUSAL_LM",
    target_modules= ["q_proj", "k_proj", "v_proj", "o_proj","gate_proj", "up_proj", "down_proj"]
)