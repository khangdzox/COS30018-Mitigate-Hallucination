training_args = TrainingArguments(
        learning_rate = 1e-4, # Learning rate change 
        lr_scheduler_type = "cosine", # Control learning rate change
        warmup_ratio= 0.01,
        weight_decay = 0.01,
        save_strategy= "steps",
        save_steps= 10,
        logging_steps= 1,
        gradient_accumulation_steps = 8, # Accumulate gradients for larger batch size
        per_device_train_batch_size= 1, # Batch size per GPU (1 batch contain 1000 data points)
        max_steps = 500,
        seed = 3407,
        fp16 = True, # Use mixed precision training for faster training
        optim = "adafactor",
        # group_by_length = True, # Group samples of same length to reduce padding and speed up training
        output_dir = "Finetuning/Fine-tuned_checkpoint/medical_3/QLoRA/4",
        max_grad_norm= 1.0  # Apply gradient clipping
    )
config = LoraConfig(
    r = 2,
    lora_alpha=32,
    lora_dropout=0.05, #kind of like a regularization dropout
    bias="none",
    task_type="CAUSAL_LM"
)
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = filtered_tokenized_dataset,
    dataset_text_field = "text",
    # packing = False, # Can make training 5x faster for short sequences.
    args = training_args,
    max_seq_length= 1300,
    dataset_batch_size= 5000,
)