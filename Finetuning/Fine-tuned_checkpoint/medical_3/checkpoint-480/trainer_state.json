{
  "best_metric": 1.0542235374450684,
  "best_model_checkpoint": "Finetuning/Fine-tuned_checkpoint/medical_3\\checkpoint-320",
  "epoch": 0.010502018356652919,
  "eval_steps": 20,
  "global_step": 480,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 2.187920490969358e-05,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.3,
      "step": 1
    },
    {
      "epoch": 4.375840981938716e-05,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.134,
      "step": 2
    },
    {
      "epoch": 6.563761472908074e-05,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.2576,
      "step": 3
    },
    {
      "epoch": 8.751681963877433e-05,
      "grad_norm": 3.582977533340454,
      "learning_rate": 4e-05,
      "loss": 2.0945,
      "step": 4
    },
    {
      "epoch": 0.00010939602454846791,
      "grad_norm": 3.747514247894287,
      "learning_rate": 8e-05,
      "loss": 1.8236,
      "step": 5
    },
    {
      "epoch": 0.0001312752294581615,
      "grad_norm": 3.6239430904388428,
      "learning_rate": 0.00012,
      "loss": 1.8854,
      "step": 6
    },
    {
      "epoch": 0.00015315443436785507,
      "grad_norm": NaN,
      "learning_rate": 0.00012,
      "loss": 2.0524,
      "step": 7
    },
    {
      "epoch": 0.00017503363927754865,
      "grad_norm": 3.4393887519836426,
      "learning_rate": 0.00016,
      "loss": 2.0364,
      "step": 8
    },
    {
      "epoch": 0.00019691284418724223,
      "grad_norm": NaN,
      "learning_rate": 0.00016,
      "loss": 2.0879,
      "step": 9
    },
    {
      "epoch": 0.00021879204909693582,
      "grad_norm": 10.043916702270508,
      "learning_rate": 0.0002,
      "loss": 2.2239,
      "step": 10
    },
    {
      "epoch": 0.0002406712540066294,
      "grad_norm": 4.174642086029053,
      "learning_rate": 0.0001995959595959596,
      "loss": 2.11,
      "step": 11
    },
    {
      "epoch": 0.000262550458916323,
      "grad_norm": 5.000838279724121,
      "learning_rate": 0.0001991919191919192,
      "loss": 2.2663,
      "step": 12
    },
    {
      "epoch": 0.00028442966382601656,
      "grad_norm": 10.119012832641602,
      "learning_rate": 0.00019878787878787878,
      "loss": 2.1576,
      "step": 13
    },
    {
      "epoch": 0.00030630886873571014,
      "grad_norm": 6.535268783569336,
      "learning_rate": 0.00019838383838383837,
      "loss": 2.1494,
      "step": 14
    },
    {
      "epoch": 0.0003281880736454037,
      "grad_norm": 7.976975917816162,
      "learning_rate": 0.000197979797979798,
      "loss": 1.7698,
      "step": 15
    },
    {
      "epoch": 0.0003500672785550973,
      "grad_norm": NaN,
      "learning_rate": 0.000197979797979798,
      "loss": 1.6329,
      "step": 16
    },
    {
      "epoch": 0.0003719464834647909,
      "grad_norm": 7.913826942443848,
      "learning_rate": 0.0001975757575757576,
      "loss": 1.5018,
      "step": 17
    },
    {
      "epoch": 0.00039382568837448447,
      "grad_norm": NaN,
      "learning_rate": 0.0001975757575757576,
      "loss": 1.7756,
      "step": 18
    },
    {
      "epoch": 0.00041570489328417805,
      "grad_norm": 28.892213821411133,
      "learning_rate": 0.0001971717171717172,
      "loss": 1.4315,
      "step": 19
    },
    {
      "epoch": 0.00043758409819387163,
      "grad_norm": 30.55006217956543,
      "learning_rate": 0.00019676767676767677,
      "loss": 1.7918,
      "step": 20
    },
    {
      "epoch": 0.00043758409819387163,
      "eval_loss": 1.5775611400604248,
      "eval_runtime": 1207.3533,
      "eval_samples_per_second": 0.347,
      "eval_steps_per_second": 0.347,
      "step": 20
    },
    {
      "epoch": 0.0004594633031035652,
      "grad_norm": 16.522403717041016,
      "learning_rate": 0.00019636363636363636,
      "loss": 1.5203,
      "step": 21
    },
    {
      "epoch": 0.0004813425080132588,
      "grad_norm": 13.282205581665039,
      "learning_rate": 0.00019595959595959596,
      "loss": 1.8017,
      "step": 22
    },
    {
      "epoch": 0.0005032217129229524,
      "grad_norm": 9.916335105895996,
      "learning_rate": 0.00019555555555555556,
      "loss": 1.4519,
      "step": 23
    },
    {
      "epoch": 0.000525100917832646,
      "grad_norm": 6.676116466522217,
      "learning_rate": 0.00019515151515151516,
      "loss": 1.202,
      "step": 24
    },
    {
      "epoch": 0.0005469801227423396,
      "grad_norm": 6.057275295257568,
      "learning_rate": 0.00019474747474747476,
      "loss": 1.2828,
      "step": 25
    },
    {
      "epoch": 0.0005688593276520331,
      "grad_norm": 7.44789981842041,
      "learning_rate": 0.00019434343434343435,
      "loss": 1.0523,
      "step": 26
    },
    {
      "epoch": 0.0005907385325617268,
      "grad_norm": 6.50221061706543,
      "learning_rate": 0.00019393939393939395,
      "loss": 1.2899,
      "step": 27
    },
    {
      "epoch": 0.0006126177374714203,
      "grad_norm": 6.900063991546631,
      "learning_rate": 0.00019353535353535355,
      "loss": 1.2485,
      "step": 28
    },
    {
      "epoch": 0.0006344969423811139,
      "grad_norm": 6.068929672241211,
      "learning_rate": 0.00019313131313131315,
      "loss": 1.0439,
      "step": 29
    },
    {
      "epoch": 0.0006563761472908074,
      "grad_norm": 7.316030025482178,
      "learning_rate": 0.00019272727272727274,
      "loss": 1.2418,
      "step": 30
    },
    {
      "epoch": 0.0006782553522005011,
      "grad_norm": 5.879920959472656,
      "learning_rate": 0.00019232323232323232,
      "loss": 1.1358,
      "step": 31
    },
    {
      "epoch": 0.0007001345571101946,
      "grad_norm": 5.274486064910889,
      "learning_rate": 0.00019191919191919191,
      "loss": 0.9103,
      "step": 32
    },
    {
      "epoch": 0.0007220137620198882,
      "grad_norm": 5.948697566986084,
      "learning_rate": 0.0001915151515151515,
      "loss": 0.7902,
      "step": 33
    },
    {
      "epoch": 0.0007438929669295818,
      "grad_norm": 7.261547565460205,
      "learning_rate": 0.00019111111111111114,
      "loss": 1.0552,
      "step": 34
    },
    {
      "epoch": 0.0007657721718392754,
      "grad_norm": 6.084803104400635,
      "learning_rate": 0.00019070707070707073,
      "loss": 0.8835,
      "step": 35
    },
    {
      "epoch": 0.0007876513767489689,
      "grad_norm": 7.025145053863525,
      "learning_rate": 0.0001903030303030303,
      "loss": 0.9343,
      "step": 36
    },
    {
      "epoch": 0.0008095305816586626,
      "grad_norm": 7.298849582672119,
      "learning_rate": 0.0001898989898989899,
      "loss": 0.8876,
      "step": 37
    },
    {
      "epoch": 0.0008314097865683561,
      "grad_norm": 4.5801215171813965,
      "learning_rate": 0.0001894949494949495,
      "loss": 0.6371,
      "step": 38
    },
    {
      "epoch": 0.0008532889914780497,
      "grad_norm": 6.978671073913574,
      "learning_rate": 0.0001890909090909091,
      "loss": 0.9304,
      "step": 39
    },
    {
      "epoch": 0.0008751681963877433,
      "grad_norm": 5.251047134399414,
      "learning_rate": 0.0001886868686868687,
      "loss": 0.7401,
      "step": 40
    },
    {
      "epoch": 0.0008751681963877433,
      "eval_loss": 1.1186294555664062,
      "eval_runtime": 1247.1505,
      "eval_samples_per_second": 0.336,
      "eval_steps_per_second": 0.336,
      "step": 40
    },
    {
      "epoch": 0.0008970474012974369,
      "grad_norm": 4.654073715209961,
      "learning_rate": 0.0001882828282828283,
      "loss": 0.6543,
      "step": 41
    },
    {
      "epoch": 0.0009189266062071304,
      "grad_norm": 4.651910781860352,
      "learning_rate": 0.0001878787878787879,
      "loss": 0.6413,
      "step": 42
    },
    {
      "epoch": 0.0009408058111168241,
      "grad_norm": 4.910744667053223,
      "learning_rate": 0.0001874747474747475,
      "loss": 0.5807,
      "step": 43
    },
    {
      "epoch": 0.0009626850160265176,
      "grad_norm": 6.588287830352783,
      "learning_rate": 0.0001870707070707071,
      "loss": 0.7039,
      "step": 44
    },
    {
      "epoch": 0.0009845642209362112,
      "grad_norm": 5.525004863739014,
      "learning_rate": 0.0001866666666666667,
      "loss": 0.4344,
      "step": 45
    },
    {
      "epoch": 0.0010064434258459049,
      "grad_norm": 9.238670349121094,
      "learning_rate": 0.00018626262626262628,
      "loss": 0.6316,
      "step": 46
    },
    {
      "epoch": 0.0010283226307555983,
      "grad_norm": 7.444749355316162,
      "learning_rate": 0.00018585858585858586,
      "loss": 0.5477,
      "step": 47
    },
    {
      "epoch": 0.001050201835665292,
      "grad_norm": 5.9416422843933105,
      "learning_rate": 0.00018545454545454545,
      "loss": 0.609,
      "step": 48
    },
    {
      "epoch": 0.0010720810405749855,
      "grad_norm": 6.438233852386475,
      "learning_rate": 0.00018505050505050505,
      "loss": 0.4394,
      "step": 49
    },
    {
      "epoch": 0.0010939602454846792,
      "grad_norm": 8.061407089233398,
      "learning_rate": 0.00018464646464646465,
      "loss": 0.4812,
      "step": 50
    },
    {
      "epoch": 0.0011158394503943726,
      "grad_norm": 4.240417003631592,
      "learning_rate": 0.00018424242424242427,
      "loss": 1.8898,
      "step": 51
    },
    {
      "epoch": 0.0011377186553040662,
      "grad_norm": 3.834597110748291,
      "learning_rate": 0.00018383838383838384,
      "loss": 1.5857,
      "step": 52
    },
    {
      "epoch": 0.0011595978602137599,
      "grad_norm": 3.5741536617279053,
      "learning_rate": 0.00018343434343434344,
      "loss": 1.7066,
      "step": 53
    },
    {
      "epoch": 0.0011814770651234535,
      "grad_norm": 4.111266613006592,
      "learning_rate": 0.00018303030303030304,
      "loss": 1.395,
      "step": 54
    },
    {
      "epoch": 0.001203356270033147,
      "grad_norm": 3.1388490200042725,
      "learning_rate": 0.00018262626262626264,
      "loss": 1.4979,
      "step": 55
    },
    {
      "epoch": 0.0012252354749428406,
      "grad_norm": 3.169434070587158,
      "learning_rate": 0.00018222222222222224,
      "loss": 1.4344,
      "step": 56
    },
    {
      "epoch": 0.0012471146798525342,
      "grad_norm": 3.5518546104431152,
      "learning_rate": 0.00018181818181818183,
      "loss": 1.3921,
      "step": 57
    },
    {
      "epoch": 0.0012689938847622278,
      "grad_norm": 2.809762477874756,
      "learning_rate": 0.0001814141414141414,
      "loss": 1.1816,
      "step": 58
    },
    {
      "epoch": 0.0012908730896719213,
      "grad_norm": 2.9568984508514404,
      "learning_rate": 0.00018101010101010103,
      "loss": 1.3195,
      "step": 59
    },
    {
      "epoch": 0.0013127522945816149,
      "grad_norm": 3.265488386154175,
      "learning_rate": 0.00018060606060606063,
      "loss": 1.5435,
      "step": 60
    },
    {
      "epoch": 0.0013127522945816149,
      "eval_loss": 1.083312749862671,
      "eval_runtime": 1206.0657,
      "eval_samples_per_second": 0.347,
      "eval_steps_per_second": 0.347,
      "step": 60
    },
    {
      "epoch": 0.0013346314994913085,
      "grad_norm": 4.071633338928223,
      "learning_rate": 0.00018020202020202023,
      "loss": 1.5087,
      "step": 61
    },
    {
      "epoch": 0.0013565107044010022,
      "grad_norm": 2.7864012718200684,
      "learning_rate": 0.0001797979797979798,
      "loss": 1.2848,
      "step": 62
    },
    {
      "epoch": 0.0013783899093106956,
      "grad_norm": 3.2151615619659424,
      "learning_rate": 0.0001793939393939394,
      "loss": 1.5074,
      "step": 63
    },
    {
      "epoch": 0.0014002691142203892,
      "grad_norm": 3.9442641735076904,
      "learning_rate": 0.000178989898989899,
      "loss": 1.2842,
      "step": 64
    },
    {
      "epoch": 0.0014221483191300829,
      "grad_norm": 4.133578300476074,
      "learning_rate": 0.0001785858585858586,
      "loss": 1.4328,
      "step": 65
    },
    {
      "epoch": 0.0014440275240397765,
      "grad_norm": 3.8823354244232178,
      "learning_rate": 0.0001781818181818182,
      "loss": 1.3915,
      "step": 66
    },
    {
      "epoch": 0.00146590672894947,
      "grad_norm": 3.128021001815796,
      "learning_rate": 0.00017777777777777779,
      "loss": 1.2119,
      "step": 67
    },
    {
      "epoch": 0.0014877859338591635,
      "grad_norm": 3.8006370067596436,
      "learning_rate": 0.00017737373737373738,
      "loss": 1.1112,
      "step": 68
    },
    {
      "epoch": 0.0015096651387688572,
      "grad_norm": 3.0185561180114746,
      "learning_rate": 0.00017696969696969698,
      "loss": 1.2158,
      "step": 69
    },
    {
      "epoch": 0.0015315443436785508,
      "grad_norm": 3.7873857021331787,
      "learning_rate": 0.00017656565656565658,
      "loss": 1.1746,
      "step": 70
    },
    {
      "epoch": 0.0015534235485882442,
      "grad_norm": 3.1916403770446777,
      "learning_rate": 0.00017616161616161618,
      "loss": 1.1164,
      "step": 71
    },
    {
      "epoch": 0.0015753027534979379,
      "grad_norm": 3.7392940521240234,
      "learning_rate": 0.00017575757575757578,
      "loss": 0.9906,
      "step": 72
    },
    {
      "epoch": 0.0015971819584076315,
      "grad_norm": 3.4577906131744385,
      "learning_rate": 0.00017535353535353535,
      "loss": 1.2396,
      "step": 73
    },
    {
      "epoch": 0.0016190611633173251,
      "grad_norm": 3.020916700363159,
      "learning_rate": 0.00017494949494949494,
      "loss": 0.9811,
      "step": 74
    },
    {
      "epoch": 0.0016409403682270186,
      "grad_norm": 4.235615253448486,
      "learning_rate": 0.00017454545454545454,
      "loss": 1.1704,
      "step": 75
    },
    {
      "epoch": 0.0016628195731367122,
      "grad_norm": 3.181392192840576,
      "learning_rate": 0.00017414141414141414,
      "loss": 1.2832,
      "step": 76
    },
    {
      "epoch": 0.0016846987780464058,
      "grad_norm": 3.735889196395874,
      "learning_rate": 0.00017373737373737377,
      "loss": 1.0175,
      "step": 77
    },
    {
      "epoch": 0.0017065779829560995,
      "grad_norm": 3.641462564468384,
      "learning_rate": 0.00017333333333333334,
      "loss": 1.0523,
      "step": 78
    },
    {
      "epoch": 0.0017284571878657929,
      "grad_norm": 3.7880380153656006,
      "learning_rate": 0.00017292929292929293,
      "loss": 1.0719,
      "step": 79
    },
    {
      "epoch": 0.0017503363927754865,
      "grad_norm": 4.743274688720703,
      "learning_rate": 0.00017252525252525253,
      "loss": 1.0136,
      "step": 80
    },
    {
      "epoch": 0.0017503363927754865,
      "eval_loss": 1.0778734683990479,
      "eval_runtime": 989.9713,
      "eval_samples_per_second": 0.423,
      "eval_steps_per_second": 0.423,
      "step": 80
    },
    {
      "epoch": 0.0017722155976851802,
      "grad_norm": 3.7915987968444824,
      "learning_rate": 0.00017212121212121213,
      "loss": 0.909,
      "step": 81
    },
    {
      "epoch": 0.0017940948025948738,
      "grad_norm": 4.286474227905273,
      "learning_rate": 0.00017171717171717173,
      "loss": 0.9834,
      "step": 82
    },
    {
      "epoch": 0.0018159740075045672,
      "grad_norm": 4.193707466125488,
      "learning_rate": 0.00017131313131313133,
      "loss": 0.9174,
      "step": 83
    },
    {
      "epoch": 0.0018378532124142608,
      "grad_norm": 3.6510438919067383,
      "learning_rate": 0.0001709090909090909,
      "loss": 0.755,
      "step": 84
    },
    {
      "epoch": 0.0018597324173239545,
      "grad_norm": 3.7754054069519043,
      "learning_rate": 0.00017050505050505052,
      "loss": 0.6863,
      "step": 85
    },
    {
      "epoch": 0.0018816116222336481,
      "grad_norm": 4.033400058746338,
      "learning_rate": 0.00017010101010101012,
      "loss": 0.8197,
      "step": 86
    },
    {
      "epoch": 0.0019034908271433415,
      "grad_norm": 4.03004789352417,
      "learning_rate": 0.00016969696969696972,
      "loss": 0.7402,
      "step": 87
    },
    {
      "epoch": 0.0019253700320530352,
      "grad_norm": 5.7920966148376465,
      "learning_rate": 0.00016929292929292932,
      "loss": 0.7261,
      "step": 88
    },
    {
      "epoch": 0.0019472492369627288,
      "grad_norm": 5.998499393463135,
      "learning_rate": 0.00016888888888888889,
      "loss": 0.6546,
      "step": 89
    },
    {
      "epoch": 0.0019691284418724224,
      "grad_norm": 4.057008743286133,
      "learning_rate": 0.00016848484848484848,
      "loss": 0.6322,
      "step": 90
    },
    {
      "epoch": 0.001991007646782116,
      "grad_norm": 3.8783164024353027,
      "learning_rate": 0.00016808080808080808,
      "loss": 0.5848,
      "step": 91
    },
    {
      "epoch": 0.0020128868516918097,
      "grad_norm": 4.726592063903809,
      "learning_rate": 0.00016767676767676768,
      "loss": 0.5252,
      "step": 92
    },
    {
      "epoch": 0.002034766056601503,
      "grad_norm": 4.214262008666992,
      "learning_rate": 0.00016727272727272728,
      "loss": 0.5345,
      "step": 93
    },
    {
      "epoch": 0.0020566452615111966,
      "grad_norm": 3.671370267868042,
      "learning_rate": 0.00016686868686868688,
      "loss": 0.5801,
      "step": 94
    },
    {
      "epoch": 0.00207852446642089,
      "grad_norm": 3.4273715019226074,
      "learning_rate": 0.00016646464646464647,
      "loss": 0.4902,
      "step": 95
    },
    {
      "epoch": 0.002100403671330584,
      "grad_norm": 4.14777946472168,
      "learning_rate": 0.00016606060606060607,
      "loss": 0.5774,
      "step": 96
    },
    {
      "epoch": 0.0021222828762402775,
      "grad_norm": 4.871885299682617,
      "learning_rate": 0.00016565656565656567,
      "loss": 0.4951,
      "step": 97
    },
    {
      "epoch": 0.002144162081149971,
      "grad_norm": 5.4186692237854,
      "learning_rate": 0.00016525252525252527,
      "loss": 0.5824,
      "step": 98
    },
    {
      "epoch": 0.0021660412860596647,
      "grad_norm": 3.454399347305298,
      "learning_rate": 0.00016484848484848487,
      "loss": 0.431,
      "step": 99
    },
    {
      "epoch": 0.0021879204909693584,
      "grad_norm": 4.710813999176025,
      "learning_rate": 0.00016444444444444444,
      "loss": 0.5383,
      "step": 100
    },
    {
      "epoch": 0.0021879204909693584,
      "eval_loss": 1.08467435836792,
      "eval_runtime": 1157.1673,
      "eval_samples_per_second": 0.362,
      "eval_steps_per_second": 0.362,
      "step": 100
    },
    {
      "epoch": 0.0022097996958790516,
      "grad_norm": 2.853144407272339,
      "learning_rate": 0.00016404040404040403,
      "loss": 1.4011,
      "step": 101
    },
    {
      "epoch": 0.002231678900788745,
      "grad_norm": 2.8249521255493164,
      "learning_rate": 0.00016363636363636366,
      "loss": 1.5769,
      "step": 102
    },
    {
      "epoch": 0.002253558105698439,
      "grad_norm": 2.3686609268188477,
      "learning_rate": 0.00016323232323232326,
      "loss": 1.6052,
      "step": 103
    },
    {
      "epoch": 0.0022754373106081325,
      "grad_norm": 2.9171142578125,
      "learning_rate": 0.00016282828282828283,
      "loss": 1.441,
      "step": 104
    },
    {
      "epoch": 0.002297316515517826,
      "grad_norm": 2.3063971996307373,
      "learning_rate": 0.00016242424242424243,
      "loss": 1.3995,
      "step": 105
    },
    {
      "epoch": 0.0023191957204275197,
      "grad_norm": 3.0006494522094727,
      "learning_rate": 0.00016202020202020202,
      "loss": 1.5298,
      "step": 106
    },
    {
      "epoch": 0.0023410749253372134,
      "grad_norm": 3.122603416442871,
      "learning_rate": 0.00016161616161616162,
      "loss": 1.5058,
      "step": 107
    },
    {
      "epoch": 0.002362954130246907,
      "grad_norm": 2.6224844455718994,
      "learning_rate": 0.00016121212121212122,
      "loss": 1.6639,
      "step": 108
    },
    {
      "epoch": 0.0023848333351566002,
      "grad_norm": 2.936767101287842,
      "learning_rate": 0.00016080808080808082,
      "loss": 1.4193,
      "step": 109
    },
    {
      "epoch": 0.002406712540066294,
      "grad_norm": 2.4151980876922607,
      "learning_rate": 0.0001604040404040404,
      "loss": 1.3721,
      "step": 110
    },
    {
      "epoch": 0.0024285917449759875,
      "grad_norm": 2.1034464836120605,
      "learning_rate": 0.00016,
      "loss": 1.2986,
      "step": 111
    },
    {
      "epoch": 0.002450470949885681,
      "grad_norm": 2.6494686603546143,
      "learning_rate": 0.0001595959595959596,
      "loss": 1.2483,
      "step": 112
    },
    {
      "epoch": 0.0024723501547953748,
      "grad_norm": 2.6644647121429443,
      "learning_rate": 0.0001591919191919192,
      "loss": 1.2991,
      "step": 113
    },
    {
      "epoch": 0.0024942293597050684,
      "grad_norm": 2.961926221847534,
      "learning_rate": 0.0001587878787878788,
      "loss": 1.2564,
      "step": 114
    },
    {
      "epoch": 0.002516108564614762,
      "grad_norm": 2.454287528991699,
      "learning_rate": 0.00015838383838383838,
      "loss": 1.327,
      "step": 115
    },
    {
      "epoch": 0.0025379877695244557,
      "grad_norm": 2.856566905975342,
      "learning_rate": 0.00015797979797979798,
      "loss": 1.0797,
      "step": 116
    },
    {
      "epoch": 0.002559866974434149,
      "grad_norm": 3.308616876602173,
      "learning_rate": 0.00015757575757575757,
      "loss": 1.236,
      "step": 117
    },
    {
      "epoch": 0.0025817461793438425,
      "grad_norm": 3.6130104064941406,
      "learning_rate": 0.00015717171717171717,
      "loss": 1.1725,
      "step": 118
    },
    {
      "epoch": 0.002603625384253536,
      "grad_norm": 3.3420894145965576,
      "learning_rate": 0.0001567676767676768,
      "loss": 1.0999,
      "step": 119
    },
    {
      "epoch": 0.0026255045891632298,
      "grad_norm": 3.749969720840454,
      "learning_rate": 0.00015636363636363637,
      "loss": 1.2844,
      "step": 120
    },
    {
      "epoch": 0.0026255045891632298,
      "eval_loss": 1.074223518371582,
      "eval_runtime": 1285.6475,
      "eval_samples_per_second": 0.326,
      "eval_steps_per_second": 0.326,
      "step": 120
    },
    {
      "epoch": 0.0026473837940729234,
      "grad_norm": 3.2028071880340576,
      "learning_rate": 0.00015595959595959597,
      "loss": 1.0457,
      "step": 121
    },
    {
      "epoch": 0.002669262998982617,
      "grad_norm": 3.0654754638671875,
      "learning_rate": 0.00015555555555555556,
      "loss": 1.0361,
      "step": 122
    },
    {
      "epoch": 0.0026911422038923107,
      "grad_norm": 3.2505292892456055,
      "learning_rate": 0.00015515151515151516,
      "loss": 1.1189,
      "step": 123
    },
    {
      "epoch": 0.0027130214088020043,
      "grad_norm": 3.7952983379364014,
      "learning_rate": 0.00015474747474747476,
      "loss": 1.2202,
      "step": 124
    },
    {
      "epoch": 0.0027349006137116975,
      "grad_norm": 3.5331673622131348,
      "learning_rate": 0.00015434343434343436,
      "loss": 1.0622,
      "step": 125
    },
    {
      "epoch": 0.002756779818621391,
      "grad_norm": 3.791264772415161,
      "learning_rate": 0.00015393939393939393,
      "loss": 1.0674,
      "step": 126
    },
    {
      "epoch": 0.002778659023531085,
      "grad_norm": 3.621905565261841,
      "learning_rate": 0.00015353535353535353,
      "loss": 0.9996,
      "step": 127
    },
    {
      "epoch": 0.0028005382284407784,
      "grad_norm": 3.7815229892730713,
      "learning_rate": 0.00015313131313131315,
      "loss": 1.0512,
      "step": 128
    },
    {
      "epoch": 0.002822417433350472,
      "grad_norm": 3.3097989559173584,
      "learning_rate": 0.00015272727272727275,
      "loss": 1.0251,
      "step": 129
    },
    {
      "epoch": 0.0028442966382601657,
      "grad_norm": 4.104879379272461,
      "learning_rate": 0.00015232323232323235,
      "loss": 0.8195,
      "step": 130
    },
    {
      "epoch": 0.0028661758431698593,
      "grad_norm": 4.4640278816223145,
      "learning_rate": 0.00015191919191919192,
      "loss": 0.7767,
      "step": 131
    },
    {
      "epoch": 0.002888055048079553,
      "grad_norm": 4.035533905029297,
      "learning_rate": 0.00015151515151515152,
      "loss": 0.9547,
      "step": 132
    },
    {
      "epoch": 0.002909934252989246,
      "grad_norm": 3.889815092086792,
      "learning_rate": 0.0001511111111111111,
      "loss": 0.6884,
      "step": 133
    },
    {
      "epoch": 0.00293181345789894,
      "grad_norm": 3.688404083251953,
      "learning_rate": 0.0001507070707070707,
      "loss": 0.9177,
      "step": 134
    },
    {
      "epoch": 0.0029536926628086334,
      "grad_norm": 3.4736905097961426,
      "learning_rate": 0.0001503030303030303,
      "loss": 0.7686,
      "step": 135
    },
    {
      "epoch": 0.002975571867718327,
      "grad_norm": 3.99497652053833,
      "learning_rate": 0.0001498989898989899,
      "loss": 0.8811,
      "step": 136
    },
    {
      "epoch": 0.0029974510726280207,
      "grad_norm": 4.97185754776001,
      "learning_rate": 0.0001494949494949495,
      "loss": 0.8895,
      "step": 137
    },
    {
      "epoch": 0.0030193302775377144,
      "grad_norm": 3.418463945388794,
      "learning_rate": 0.0001490909090909091,
      "loss": 0.5909,
      "step": 138
    },
    {
      "epoch": 0.003041209482447408,
      "grad_norm": 3.456327199935913,
      "learning_rate": 0.0001486868686868687,
      "loss": 0.6432,
      "step": 139
    },
    {
      "epoch": 0.0030630886873571016,
      "grad_norm": 5.200439929962158,
      "learning_rate": 0.0001482828282828283,
      "loss": 0.6963,
      "step": 140
    },
    {
      "epoch": 0.0030630886873571016,
      "eval_loss": 1.0772480964660645,
      "eval_runtime": 1290.3941,
      "eval_samples_per_second": 0.325,
      "eval_steps_per_second": 0.325,
      "step": 140
    },
    {
      "epoch": 0.003084967892266795,
      "grad_norm": 4.516968727111816,
      "learning_rate": 0.0001478787878787879,
      "loss": 0.6698,
      "step": 141
    },
    {
      "epoch": 0.0031068470971764885,
      "grad_norm": 3.0375325679779053,
      "learning_rate": 0.00014747474747474747,
      "loss": 0.5314,
      "step": 142
    },
    {
      "epoch": 0.003128726302086182,
      "grad_norm": 4.42278528213501,
      "learning_rate": 0.00014707070707070706,
      "loss": 0.7276,
      "step": 143
    },
    {
      "epoch": 0.0031506055069958757,
      "grad_norm": 5.266684055328369,
      "learning_rate": 0.00014666666666666666,
      "loss": 0.6405,
      "step": 144
    },
    {
      "epoch": 0.0031724847119055694,
      "grad_norm": 4.321359157562256,
      "learning_rate": 0.0001462626262626263,
      "loss": 0.5172,
      "step": 145
    },
    {
      "epoch": 0.003194363916815263,
      "grad_norm": 4.264169692993164,
      "learning_rate": 0.00014585858585858586,
      "loss": 0.568,
      "step": 146
    },
    {
      "epoch": 0.0032162431217249566,
      "grad_norm": 5.165587902069092,
      "learning_rate": 0.00014545454545454546,
      "loss": 0.4741,
      "step": 147
    },
    {
      "epoch": 0.0032381223266346503,
      "grad_norm": 3.804166555404663,
      "learning_rate": 0.00014505050505050505,
      "loss": 0.4898,
      "step": 148
    },
    {
      "epoch": 0.0032600015315443435,
      "grad_norm": 3.928715229034424,
      "learning_rate": 0.00014464646464646465,
      "loss": 0.4071,
      "step": 149
    },
    {
      "epoch": 0.003281880736454037,
      "grad_norm": 3.7104098796844482,
      "learning_rate": 0.00014424242424242425,
      "loss": 0.5414,
      "step": 150
    },
    {
      "epoch": 0.0033037599413637308,
      "grad_norm": 2.637075185775757,
      "learning_rate": 0.00014383838383838385,
      "loss": 1.5114,
      "step": 151
    },
    {
      "epoch": 0.0033256391462734244,
      "grad_norm": 2.3697123527526855,
      "learning_rate": 0.00014343434343434342,
      "loss": 1.7502,
      "step": 152
    },
    {
      "epoch": 0.003347518351183118,
      "grad_norm": 2.353017807006836,
      "learning_rate": 0.00014303030303030304,
      "loss": 1.2037,
      "step": 153
    },
    {
      "epoch": 0.0033693975560928117,
      "grad_norm": 2.5830557346343994,
      "learning_rate": 0.00014262626262626264,
      "loss": 1.5081,
      "step": 154
    },
    {
      "epoch": 0.0033912767610025053,
      "grad_norm": 3.009612798690796,
      "learning_rate": 0.00014222222222222224,
      "loss": 1.4589,
      "step": 155
    },
    {
      "epoch": 0.003413155965912199,
      "grad_norm": 2.4310996532440186,
      "learning_rate": 0.00014181818181818184,
      "loss": 1.5467,
      "step": 156
    },
    {
      "epoch": 0.003435035170821892,
      "grad_norm": 2.2948780059814453,
      "learning_rate": 0.0001414141414141414,
      "loss": 1.2685,
      "step": 157
    },
    {
      "epoch": 0.0034569143757315858,
      "grad_norm": 2.5796425342559814,
      "learning_rate": 0.000141010101010101,
      "loss": 1.2207,
      "step": 158
    },
    {
      "epoch": 0.0034787935806412794,
      "grad_norm": 2.6796839237213135,
      "learning_rate": 0.0001406060606060606,
      "loss": 1.3855,
      "step": 159
    },
    {
      "epoch": 0.003500672785550973,
      "grad_norm": 2.5676474571228027,
      "learning_rate": 0.0001402020202020202,
      "loss": 1.2576,
      "step": 160
    },
    {
      "epoch": 0.003500672785550973,
      "eval_loss": 1.0610764026641846,
      "eval_runtime": 1293.8927,
      "eval_samples_per_second": 0.324,
      "eval_steps_per_second": 0.324,
      "step": 160
    },
    {
      "epoch": 0.0035225519904606667,
      "grad_norm": 2.26994252204895,
      "learning_rate": 0.0001397979797979798,
      "loss": 1.2167,
      "step": 161
    },
    {
      "epoch": 0.0035444311953703603,
      "grad_norm": 2.664098024368286,
      "learning_rate": 0.0001393939393939394,
      "loss": 1.3953,
      "step": 162
    },
    {
      "epoch": 0.003566310400280054,
      "grad_norm": 3.1288068294525146,
      "learning_rate": 0.000138989898989899,
      "loss": 1.0048,
      "step": 163
    },
    {
      "epoch": 0.0035881896051897476,
      "grad_norm": 3.1782777309417725,
      "learning_rate": 0.0001385858585858586,
      "loss": 1.2208,
      "step": 164
    },
    {
      "epoch": 0.003610068810099441,
      "grad_norm": 3.0507209300994873,
      "learning_rate": 0.0001381818181818182,
      "loss": 0.9869,
      "step": 165
    },
    {
      "epoch": 0.0036319480150091344,
      "grad_norm": 3.098184108734131,
      "learning_rate": 0.0001377777777777778,
      "loss": 1.1221,
      "step": 166
    },
    {
      "epoch": 0.003653827219918828,
      "grad_norm": 2.8293848037719727,
      "learning_rate": 0.0001373737373737374,
      "loss": 1.0732,
      "step": 167
    },
    {
      "epoch": 0.0036757064248285217,
      "grad_norm": 3.264329195022583,
      "learning_rate": 0.00013696969696969696,
      "loss": 1.0445,
      "step": 168
    },
    {
      "epoch": 0.0036975856297382153,
      "grad_norm": 2.9716053009033203,
      "learning_rate": 0.00013656565656565656,
      "loss": 1.106,
      "step": 169
    },
    {
      "epoch": 0.003719464834647909,
      "grad_norm": 2.982933282852173,
      "learning_rate": 0.00013616161616161618,
      "loss": 1.0351,
      "step": 170
    },
    {
      "epoch": 0.0037413440395576026,
      "grad_norm": 3.6501693725585938,
      "learning_rate": 0.00013575757575757578,
      "loss": 1.1997,
      "step": 171
    },
    {
      "epoch": 0.0037632232444672962,
      "grad_norm": 4.151454925537109,
      "learning_rate": 0.00013535353535353538,
      "loss": 1.3112,
      "step": 172
    },
    {
      "epoch": 0.0037851024493769894,
      "grad_norm": 2.9009532928466797,
      "learning_rate": 0.00013494949494949495,
      "loss": 0.9046,
      "step": 173
    },
    {
      "epoch": 0.003806981654286683,
      "grad_norm": 5.228568077087402,
      "learning_rate": 0.00013454545454545455,
      "loss": 1.0388,
      "step": 174
    },
    {
      "epoch": 0.0038288608591963767,
      "grad_norm": 4.2046098709106445,
      "learning_rate": 0.00013414141414141414,
      "loss": 1.2208,
      "step": 175
    },
    {
      "epoch": 0.0038507400641060703,
      "grad_norm": 3.57342529296875,
      "learning_rate": 0.00013373737373737374,
      "loss": 1.1214,
      "step": 176
    },
    {
      "epoch": 0.003872619269015764,
      "grad_norm": 3.3289530277252197,
      "learning_rate": 0.00013333333333333334,
      "loss": 1.0457,
      "step": 177
    },
    {
      "epoch": 0.0038944984739254576,
      "grad_norm": 3.6329686641693115,
      "learning_rate": 0.00013292929292929294,
      "loss": 0.9608,
      "step": 178
    },
    {
      "epoch": 0.003916377678835151,
      "grad_norm": 3.689539909362793,
      "learning_rate": 0.00013252525252525254,
      "loss": 0.9845,
      "step": 179
    },
    {
      "epoch": 0.003938256883744845,
      "grad_norm": 4.17722749710083,
      "learning_rate": 0.00013212121212121213,
      "loss": 0.8909,
      "step": 180
    },
    {
      "epoch": 0.003938256883744845,
      "eval_loss": 1.0644645690917969,
      "eval_runtime": 1198.621,
      "eval_samples_per_second": 0.35,
      "eval_steps_per_second": 0.35,
      "step": 180
    },
    {
      "epoch": 0.003960136088654538,
      "grad_norm": 4.033379077911377,
      "learning_rate": 0.00013171717171717173,
      "loss": 0.7961,
      "step": 181
    },
    {
      "epoch": 0.003982015293564232,
      "grad_norm": 3.9211292266845703,
      "learning_rate": 0.00013131313131313133,
      "loss": 0.8213,
      "step": 182
    },
    {
      "epoch": 0.004003894498473925,
      "grad_norm": 3.5488100051879883,
      "learning_rate": 0.00013090909090909093,
      "loss": 0.8532,
      "step": 183
    },
    {
      "epoch": 0.004025773703383619,
      "grad_norm": 3.95359468460083,
      "learning_rate": 0.0001305050505050505,
      "loss": 0.6689,
      "step": 184
    },
    {
      "epoch": 0.004047652908293313,
      "grad_norm": 3.461029052734375,
      "learning_rate": 0.0001301010101010101,
      "loss": 0.7244,
      "step": 185
    },
    {
      "epoch": 0.004069532113203006,
      "grad_norm": 4.611918926239014,
      "learning_rate": 0.0001296969696969697,
      "loss": 0.8875,
      "step": 186
    },
    {
      "epoch": 0.0040914113181127,
      "grad_norm": 3.4144275188446045,
      "learning_rate": 0.00012929292929292932,
      "loss": 0.7018,
      "step": 187
    },
    {
      "epoch": 0.004113290523022393,
      "grad_norm": 3.6015522480010986,
      "learning_rate": 0.00012888888888888892,
      "loss": 0.5566,
      "step": 188
    },
    {
      "epoch": 0.004135169727932087,
      "grad_norm": 4.2111992835998535,
      "learning_rate": 0.0001284848484848485,
      "loss": 0.6908,
      "step": 189
    },
    {
      "epoch": 0.00415704893284178,
      "grad_norm": 2.803847074508667,
      "learning_rate": 0.00012808080808080809,
      "loss": 0.4326,
      "step": 190
    },
    {
      "epoch": 0.0041789281377514744,
      "grad_norm": 4.123838424682617,
      "learning_rate": 0.00012767676767676768,
      "loss": 0.6215,
      "step": 191
    },
    {
      "epoch": 0.004200807342661168,
      "grad_norm": 3.2144203186035156,
      "learning_rate": 0.00012727272727272728,
      "loss": 0.5967,
      "step": 192
    },
    {
      "epoch": 0.004222686547570861,
      "grad_norm": 7.048199653625488,
      "learning_rate": 0.00012686868686868688,
      "loss": 0.5335,
      "step": 193
    },
    {
      "epoch": 0.004244565752480555,
      "grad_norm": 2.8879921436309814,
      "learning_rate": 0.00012646464646464645,
      "loss": 0.5294,
      "step": 194
    },
    {
      "epoch": 0.004266444957390248,
      "grad_norm": 3.706235647201538,
      "learning_rate": 0.00012606060606060605,
      "loss": 0.4795,
      "step": 195
    },
    {
      "epoch": 0.004288324162299942,
      "grad_norm": 3.955677032470703,
      "learning_rate": 0.00012565656565656567,
      "loss": 0.5636,
      "step": 196
    },
    {
      "epoch": 0.004310203367209635,
      "grad_norm": 5.354634761810303,
      "learning_rate": 0.00012525252525252527,
      "loss": 0.7008,
      "step": 197
    },
    {
      "epoch": 0.0043320825721193295,
      "grad_norm": 3.3354740142822266,
      "learning_rate": 0.00012484848484848487,
      "loss": 0.4364,
      "step": 198
    },
    {
      "epoch": 0.004353961777029023,
      "grad_norm": 3.538856029510498,
      "learning_rate": 0.00012444444444444444,
      "loss": 0.4017,
      "step": 199
    },
    {
      "epoch": 0.004375840981938717,
      "grad_norm": 3.3159778118133545,
      "learning_rate": 0.00012404040404040404,
      "loss": 0.4738,
      "step": 200
    },
    {
      "epoch": 0.004375840981938717,
      "eval_loss": 1.0679080486297607,
      "eval_runtime": 1145.9377,
      "eval_samples_per_second": 0.366,
      "eval_steps_per_second": 0.366,
      "step": 200
    },
    {
      "epoch": 0.00439772018684841,
      "grad_norm": 3.2939202785491943,
      "learning_rate": 0.00012363636363636364,
      "loss": 1.4143,
      "step": 201
    },
    {
      "epoch": 0.004419599391758103,
      "grad_norm": 3.3135740756988525,
      "learning_rate": 0.00012323232323232323,
      "loss": 1.2155,
      "step": 202
    },
    {
      "epoch": 0.004441478596667797,
      "grad_norm": 2.620529890060425,
      "learning_rate": 0.00012282828282828283,
      "loss": 1.3839,
      "step": 203
    },
    {
      "epoch": 0.00446335780157749,
      "grad_norm": 2.5706911087036133,
      "learning_rate": 0.00012242424242424243,
      "loss": 1.6657,
      "step": 204
    },
    {
      "epoch": 0.0044852370064871845,
      "grad_norm": 2.400444984436035,
      "learning_rate": 0.00012202020202020204,
      "loss": 1.3577,
      "step": 205
    },
    {
      "epoch": 0.004507116211396878,
      "grad_norm": 2.6575050354003906,
      "learning_rate": 0.00012161616161616162,
      "loss": 1.6496,
      "step": 206
    },
    {
      "epoch": 0.004528995416306572,
      "grad_norm": 3.0377931594848633,
      "learning_rate": 0.00012121212121212122,
      "loss": 1.3355,
      "step": 207
    },
    {
      "epoch": 0.004550874621216265,
      "grad_norm": 2.6806318759918213,
      "learning_rate": 0.00012080808080808082,
      "loss": 1.5021,
      "step": 208
    },
    {
      "epoch": 0.004572753826125958,
      "grad_norm": 2.872035503387451,
      "learning_rate": 0.0001204040404040404,
      "loss": 1.6519,
      "step": 209
    },
    {
      "epoch": 0.004594633031035652,
      "grad_norm": 2.4138572216033936,
      "learning_rate": 0.00012,
      "loss": 1.074,
      "step": 210
    },
    {
      "epoch": 0.004616512235945345,
      "grad_norm": 2.492382526397705,
      "learning_rate": 0.0001195959595959596,
      "loss": 1.1661,
      "step": 211
    },
    {
      "epoch": 0.0046383914408550395,
      "grad_norm": 2.698227643966675,
      "learning_rate": 0.00011919191919191919,
      "loss": 1.2289,
      "step": 212
    },
    {
      "epoch": 0.004660270645764733,
      "grad_norm": 3.032437801361084,
      "learning_rate": 0.0001187878787878788,
      "loss": 1.3063,
      "step": 213
    },
    {
      "epoch": 0.004682149850674427,
      "grad_norm": 2.7352161407470703,
      "learning_rate": 0.0001183838383838384,
      "loss": 1.2743,
      "step": 214
    },
    {
      "epoch": 0.00470402905558412,
      "grad_norm": 2.593187093734741,
      "learning_rate": 0.00011797979797979799,
      "loss": 1.1189,
      "step": 215
    },
    {
      "epoch": 0.004725908260493814,
      "grad_norm": 2.5180647373199463,
      "learning_rate": 0.00011757575757575758,
      "loss": 1.1729,
      "step": 216
    },
    {
      "epoch": 0.004747787465403507,
      "grad_norm": 2.2748851776123047,
      "learning_rate": 0.00011717171717171717,
      "loss": 0.9969,
      "step": 217
    },
    {
      "epoch": 0.0047696666703132004,
      "grad_norm": 3.162282943725586,
      "learning_rate": 0.00011676767676767677,
      "loss": 1.2882,
      "step": 218
    },
    {
      "epoch": 0.0047915458752228945,
      "grad_norm": 2.801718235015869,
      "learning_rate": 0.00011636363636363636,
      "loss": 1.0909,
      "step": 219
    },
    {
      "epoch": 0.004813425080132588,
      "grad_norm": 2.8023829460144043,
      "learning_rate": 0.00011595959595959596,
      "loss": 0.9968,
      "step": 220
    },
    {
      "epoch": 0.004813425080132588,
      "eval_loss": 1.0632429122924805,
      "eval_runtime": 1197.4521,
      "eval_samples_per_second": 0.35,
      "eval_steps_per_second": 0.35,
      "step": 220
    },
    {
      "epoch": 0.004835304285042282,
      "grad_norm": 3.4666287899017334,
      "learning_rate": 0.00011555555555555555,
      "loss": 1.0609,
      "step": 221
    },
    {
      "epoch": 0.004857183489951975,
      "grad_norm": 2.5582356452941895,
      "learning_rate": 0.00011515151515151516,
      "loss": 1.0922,
      "step": 222
    },
    {
      "epoch": 0.004879062694861669,
      "grad_norm": 2.8699727058410645,
      "learning_rate": 0.00011474747474747476,
      "loss": 0.9726,
      "step": 223
    },
    {
      "epoch": 0.004900941899771362,
      "grad_norm": 2.827749490737915,
      "learning_rate": 0.00011434343434343435,
      "loss": 1.1076,
      "step": 224
    },
    {
      "epoch": 0.0049228211046810555,
      "grad_norm": 3.6897265911102295,
      "learning_rate": 0.00011393939393939394,
      "loss": 1.1165,
      "step": 225
    },
    {
      "epoch": 0.0049447003095907495,
      "grad_norm": 2.9003968238830566,
      "learning_rate": 0.00011353535353535354,
      "loss": 0.8456,
      "step": 226
    },
    {
      "epoch": 0.004966579514500443,
      "grad_norm": 2.9251859188079834,
      "learning_rate": 0.00011313131313131313,
      "loss": 0.9109,
      "step": 227
    },
    {
      "epoch": 0.004988458719410137,
      "grad_norm": 3.38539719581604,
      "learning_rate": 0.00011272727272727272,
      "loss": 0.9815,
      "step": 228
    },
    {
      "epoch": 0.00501033792431983,
      "grad_norm": 5.547202110290527,
      "learning_rate": 0.00011232323232323232,
      "loss": 0.882,
      "step": 229
    },
    {
      "epoch": 0.005032217129229524,
      "grad_norm": 2.929377794265747,
      "learning_rate": 0.00011191919191919193,
      "loss": 0.9478,
      "step": 230
    },
    {
      "epoch": 0.005054096334139217,
      "grad_norm": 3.2285518646240234,
      "learning_rate": 0.00011151515151515153,
      "loss": 0.8624,
      "step": 231
    },
    {
      "epoch": 0.005075975539048911,
      "grad_norm": 3.1948888301849365,
      "learning_rate": 0.00011111111111111112,
      "loss": 0.6526,
      "step": 232
    },
    {
      "epoch": 0.0050978547439586045,
      "grad_norm": 4.207798480987549,
      "learning_rate": 0.00011070707070707071,
      "loss": 0.8527,
      "step": 233
    },
    {
      "epoch": 0.005119733948868298,
      "grad_norm": 3.5254733562469482,
      "learning_rate": 0.00011030303030303031,
      "loss": 0.6861,
      "step": 234
    },
    {
      "epoch": 0.005141613153777992,
      "grad_norm": 3.8179454803466797,
      "learning_rate": 0.0001098989898989899,
      "loss": 0.7844,
      "step": 235
    },
    {
      "epoch": 0.005163492358687685,
      "grad_norm": 4.354806423187256,
      "learning_rate": 0.0001094949494949495,
      "loss": 0.7419,
      "step": 236
    },
    {
      "epoch": 0.005185371563597379,
      "grad_norm": 3.621739387512207,
      "learning_rate": 0.00010909090909090909,
      "loss": 0.672,
      "step": 237
    },
    {
      "epoch": 0.005207250768507072,
      "grad_norm": 3.568056344985962,
      "learning_rate": 0.00010868686868686868,
      "loss": 0.7753,
      "step": 238
    },
    {
      "epoch": 0.005229129973416766,
      "grad_norm": 3.3609976768493652,
      "learning_rate": 0.0001082828282828283,
      "loss": 0.59,
      "step": 239
    },
    {
      "epoch": 0.0052510091783264596,
      "grad_norm": 6.132956027984619,
      "learning_rate": 0.00010787878787878789,
      "loss": 0.9035,
      "step": 240
    },
    {
      "epoch": 0.0052510091783264596,
      "eval_loss": 1.0709129571914673,
      "eval_runtime": 1381.6283,
      "eval_samples_per_second": 0.303,
      "eval_steps_per_second": 0.303,
      "step": 240
    },
    {
      "epoch": 0.005272888383236154,
      "grad_norm": 4.431691646575928,
      "learning_rate": 0.00010747474747474748,
      "loss": 0.5746,
      "step": 241
    },
    {
      "epoch": 0.005294767588145847,
      "grad_norm": 5.465941905975342,
      "learning_rate": 0.00010707070707070708,
      "loss": 0.7262,
      "step": 242
    },
    {
      "epoch": 0.00531664679305554,
      "grad_norm": 2.8218698501586914,
      "learning_rate": 0.00010666666666666667,
      "loss": 0.5764,
      "step": 243
    },
    {
      "epoch": 0.005338525997965234,
      "grad_norm": 3.8575761318206787,
      "learning_rate": 0.00010626262626262626,
      "loss": 0.5009,
      "step": 244
    },
    {
      "epoch": 0.005360405202874927,
      "grad_norm": 3.0821433067321777,
      "learning_rate": 0.00010585858585858586,
      "loss": 0.5416,
      "step": 245
    },
    {
      "epoch": 0.005382284407784621,
      "grad_norm": 6.053490161895752,
      "learning_rate": 0.00010545454545454545,
      "loss": 0.4746,
      "step": 246
    },
    {
      "epoch": 0.005404163612694315,
      "grad_norm": 3.4826810359954834,
      "learning_rate": 0.00010505050505050507,
      "loss": 0.5101,
      "step": 247
    },
    {
      "epoch": 0.005426042817604009,
      "grad_norm": 3.347562313079834,
      "learning_rate": 0.00010464646464646466,
      "loss": 0.4962,
      "step": 248
    },
    {
      "epoch": 0.005447922022513702,
      "grad_norm": 5.368549823760986,
      "learning_rate": 0.00010424242424242425,
      "loss": 0.4277,
      "step": 249
    },
    {
      "epoch": 0.005469801227423395,
      "grad_norm": 3.3195362091064453,
      "learning_rate": 0.00010383838383838385,
      "loss": 0.4181,
      "step": 250
    },
    {
      "epoch": 0.005491680432333089,
      "grad_norm": 2.30576491355896,
      "learning_rate": 0.00010343434343434344,
      "loss": 1.364,
      "step": 251
    },
    {
      "epoch": 0.005513559637242782,
      "grad_norm": 2.0336413383483887,
      "learning_rate": 0.00010303030303030303,
      "loss": 1.5146,
      "step": 252
    },
    {
      "epoch": 0.005535438842152476,
      "grad_norm": 2.359621286392212,
      "learning_rate": 0.00010262626262626263,
      "loss": 1.518,
      "step": 253
    },
    {
      "epoch": 0.00555731804706217,
      "grad_norm": 2.4586262702941895,
      "learning_rate": 0.00010222222222222222,
      "loss": 1.3453,
      "step": 254
    },
    {
      "epoch": 0.005579197251971864,
      "grad_norm": 2.3118057250976562,
      "learning_rate": 0.00010181818181818181,
      "loss": 1.6641,
      "step": 255
    },
    {
      "epoch": 0.005601076456881557,
      "grad_norm": 2.449106216430664,
      "learning_rate": 0.00010141414141414143,
      "loss": 1.2101,
      "step": 256
    },
    {
      "epoch": 0.005622955661791251,
      "grad_norm": 3.664180040359497,
      "learning_rate": 0.00010101010101010102,
      "loss": 1.2509,
      "step": 257
    },
    {
      "epoch": 0.005644834866700944,
      "grad_norm": 2.7715306282043457,
      "learning_rate": 0.00010060606060606062,
      "loss": 1.6926,
      "step": 258
    },
    {
      "epoch": 0.005666714071610637,
      "grad_norm": 2.116490364074707,
      "learning_rate": 0.0001002020202020202,
      "loss": 1.3348,
      "step": 259
    },
    {
      "epoch": 0.005688593276520331,
      "grad_norm": 2.140407085418701,
      "learning_rate": 9.97979797979798e-05,
      "loss": 1.3792,
      "step": 260
    },
    {
      "epoch": 0.005688593276520331,
      "eval_loss": 1.0588370561599731,
      "eval_runtime": 1377.582,
      "eval_samples_per_second": 0.304,
      "eval_steps_per_second": 0.304,
      "step": 260
    },
    {
      "epoch": 0.005710472481430025,
      "grad_norm": 2.4542510509490967,
      "learning_rate": 9.939393939393939e-05,
      "loss": 1.0692,
      "step": 261
    },
    {
      "epoch": 0.005732351686339719,
      "grad_norm": 2.926440477371216,
      "learning_rate": 9.8989898989899e-05,
      "loss": 1.5507,
      "step": 262
    },
    {
      "epoch": 0.005754230891249412,
      "grad_norm": 2.087461471557617,
      "learning_rate": 9.85858585858586e-05,
      "loss": 1.2454,
      "step": 263
    },
    {
      "epoch": 0.005776110096159106,
      "grad_norm": 2.5628600120544434,
      "learning_rate": 9.818181818181818e-05,
      "loss": 1.3279,
      "step": 264
    },
    {
      "epoch": 0.005797989301068799,
      "grad_norm": 2.351067543029785,
      "learning_rate": 9.777777777777778e-05,
      "loss": 1.2194,
      "step": 265
    },
    {
      "epoch": 0.005819868505978492,
      "grad_norm": 3.1711878776550293,
      "learning_rate": 9.737373737373738e-05,
      "loss": 1.1814,
      "step": 266
    },
    {
      "epoch": 0.005841747710888186,
      "grad_norm": 2.097647190093994,
      "learning_rate": 9.696969696969698e-05,
      "loss": 1.0738,
      "step": 267
    },
    {
      "epoch": 0.00586362691579788,
      "grad_norm": 2.5540084838867188,
      "learning_rate": 9.656565656565657e-05,
      "loss": 1.2093,
      "step": 268
    },
    {
      "epoch": 0.005885506120707574,
      "grad_norm": 2.840261220932007,
      "learning_rate": 9.616161616161616e-05,
      "loss": 1.1777,
      "step": 269
    },
    {
      "epoch": 0.005907385325617267,
      "grad_norm": 2.783541679382324,
      "learning_rate": 9.575757575757576e-05,
      "loss": 0.9769,
      "step": 270
    },
    {
      "epoch": 0.005929264530526961,
      "grad_norm": 2.347095251083374,
      "learning_rate": 9.535353535353537e-05,
      "loss": 0.9742,
      "step": 271
    },
    {
      "epoch": 0.005951143735436654,
      "grad_norm": 3.1219632625579834,
      "learning_rate": 9.494949494949495e-05,
      "loss": 1.0454,
      "step": 272
    },
    {
      "epoch": 0.005973022940346348,
      "grad_norm": 3.1900882720947266,
      "learning_rate": 9.454545454545455e-05,
      "loss": 1.0043,
      "step": 273
    },
    {
      "epoch": 0.0059949021452560414,
      "grad_norm": 3.2152068614959717,
      "learning_rate": 9.414141414141415e-05,
      "loss": 0.9145,
      "step": 274
    },
    {
      "epoch": 0.006016781350165735,
      "grad_norm": 2.6236870288848877,
      "learning_rate": 9.373737373737375e-05,
      "loss": 0.977,
      "step": 275
    },
    {
      "epoch": 0.006038660555075429,
      "grad_norm": 2.3707797527313232,
      "learning_rate": 9.333333333333334e-05,
      "loss": 0.8514,
      "step": 276
    },
    {
      "epoch": 0.006060539759985122,
      "grad_norm": 3.5427238941192627,
      "learning_rate": 9.292929292929293e-05,
      "loss": 0.9235,
      "step": 277
    },
    {
      "epoch": 0.006082418964894816,
      "grad_norm": 3.0829925537109375,
      "learning_rate": 9.252525252525253e-05,
      "loss": 0.9899,
      "step": 278
    },
    {
      "epoch": 0.006104298169804509,
      "grad_norm": 2.9868099689483643,
      "learning_rate": 9.212121212121214e-05,
      "loss": 0.93,
      "step": 279
    },
    {
      "epoch": 0.006126177374714203,
      "grad_norm": 2.8247861862182617,
      "learning_rate": 9.171717171717172e-05,
      "loss": 0.9125,
      "step": 280
    },
    {
      "epoch": 0.006126177374714203,
      "eval_loss": 1.0614477396011353,
      "eval_runtime": 1136.8882,
      "eval_samples_per_second": 0.369,
      "eval_steps_per_second": 0.369,
      "step": 280
    },
    {
      "epoch": 0.0061480565796238965,
      "grad_norm": 2.608151912689209,
      "learning_rate": 9.131313131313132e-05,
      "loss": 1.0504,
      "step": 281
    },
    {
      "epoch": 0.00616993578453359,
      "grad_norm": 3.0748093128204346,
      "learning_rate": 9.090909090909092e-05,
      "loss": 0.8232,
      "step": 282
    },
    {
      "epoch": 0.006191814989443284,
      "grad_norm": 2.672926902770996,
      "learning_rate": 9.050505050505052e-05,
      "loss": 0.7816,
      "step": 283
    },
    {
      "epoch": 0.006213694194352977,
      "grad_norm": 3.3022301197052,
      "learning_rate": 9.010101010101011e-05,
      "loss": 0.8068,
      "step": 284
    },
    {
      "epoch": 0.006235573399262671,
      "grad_norm": 2.682406187057495,
      "learning_rate": 8.96969696969697e-05,
      "loss": 0.7335,
      "step": 285
    },
    {
      "epoch": 0.006257452604172364,
      "grad_norm": 3.1794517040252686,
      "learning_rate": 8.92929292929293e-05,
      "loss": 0.7964,
      "step": 286
    },
    {
      "epoch": 0.006279331809082058,
      "grad_norm": 2.9694783687591553,
      "learning_rate": 8.888888888888889e-05,
      "loss": 0.742,
      "step": 287
    },
    {
      "epoch": 0.0063012110139917515,
      "grad_norm": 2.5918123722076416,
      "learning_rate": 8.848484848484849e-05,
      "loss": 0.686,
      "step": 288
    },
    {
      "epoch": 0.0063230902189014455,
      "grad_norm": 3.3616974353790283,
      "learning_rate": 8.808080808080809e-05,
      "loss": 0.6637,
      "step": 289
    },
    {
      "epoch": 0.006344969423811139,
      "grad_norm": 3.826286792755127,
      "learning_rate": 8.767676767676767e-05,
      "loss": 0.7317,
      "step": 290
    },
    {
      "epoch": 0.006366848628720832,
      "grad_norm": 7.805991172790527,
      "learning_rate": 8.727272727272727e-05,
      "loss": 0.5782,
      "step": 291
    },
    {
      "epoch": 0.006388727833630526,
      "grad_norm": 4.547299861907959,
      "learning_rate": 8.686868686868688e-05,
      "loss": 0.7204,
      "step": 292
    },
    {
      "epoch": 0.006410607038540219,
      "grad_norm": 2.7447187900543213,
      "learning_rate": 8.646464646464647e-05,
      "loss": 0.4897,
      "step": 293
    },
    {
      "epoch": 0.006432486243449913,
      "grad_norm": 7.159862041473389,
      "learning_rate": 8.606060606060606e-05,
      "loss": 0.4648,
      "step": 294
    },
    {
      "epoch": 0.0064543654483596065,
      "grad_norm": 4.158013820648193,
      "learning_rate": 8.565656565656566e-05,
      "loss": 0.544,
      "step": 295
    },
    {
      "epoch": 0.0064762446532693006,
      "grad_norm": 4.077907562255859,
      "learning_rate": 8.525252525252526e-05,
      "loss": 0.5169,
      "step": 296
    },
    {
      "epoch": 0.006498123858178994,
      "grad_norm": 3.191100835800171,
      "learning_rate": 8.484848484848486e-05,
      "loss": 0.3231,
      "step": 297
    },
    {
      "epoch": 0.006520003063088687,
      "grad_norm": 3.5922317504882812,
      "learning_rate": 8.444444444444444e-05,
      "loss": 0.6223,
      "step": 298
    },
    {
      "epoch": 0.006541882267998381,
      "grad_norm": 2.6949760913848877,
      "learning_rate": 8.404040404040404e-05,
      "loss": 0.3363,
      "step": 299
    },
    {
      "epoch": 0.006563761472908074,
      "grad_norm": 2.511324882507324,
      "learning_rate": 8.363636363636364e-05,
      "loss": 0.3926,
      "step": 300
    },
    {
      "epoch": 0.006563761472908074,
      "eval_loss": 1.064319372177124,
      "eval_runtime": 1133.5469,
      "eval_samples_per_second": 0.37,
      "eval_steps_per_second": 0.37,
      "step": 300
    },
    {
      "epoch": 0.006585640677817768,
      "grad_norm": 2.3665199279785156,
      "learning_rate": 8.323232323232324e-05,
      "loss": 1.5773,
      "step": 301
    },
    {
      "epoch": 0.0066075198827274615,
      "grad_norm": 2.044283866882324,
      "learning_rate": 8.282828282828283e-05,
      "loss": 1.4232,
      "step": 302
    },
    {
      "epoch": 0.006629399087637156,
      "grad_norm": 2.314277172088623,
      "learning_rate": 8.242424242424243e-05,
      "loss": 1.5121,
      "step": 303
    },
    {
      "epoch": 0.006651278292546849,
      "grad_norm": 2.3792781829833984,
      "learning_rate": 8.202020202020202e-05,
      "loss": 1.3672,
      "step": 304
    },
    {
      "epoch": 0.006673157497456543,
      "grad_norm": 2.2465832233428955,
      "learning_rate": 8.161616161616163e-05,
      "loss": 1.62,
      "step": 305
    },
    {
      "epoch": 0.006695036702366236,
      "grad_norm": 2.2349376678466797,
      "learning_rate": 8.121212121212121e-05,
      "loss": 1.3451,
      "step": 306
    },
    {
      "epoch": 0.006716915907275929,
      "grad_norm": 2.214984655380249,
      "learning_rate": 8.080808080808081e-05,
      "loss": 1.4384,
      "step": 307
    },
    {
      "epoch": 0.006738795112185623,
      "grad_norm": 3.053377389907837,
      "learning_rate": 8.040404040404041e-05,
      "loss": 1.2545,
      "step": 308
    },
    {
      "epoch": 0.0067606743170953165,
      "grad_norm": 2.6540777683258057,
      "learning_rate": 8e-05,
      "loss": 1.2439,
      "step": 309
    },
    {
      "epoch": 0.006782553522005011,
      "grad_norm": 2.687375545501709,
      "learning_rate": 7.95959595959596e-05,
      "loss": 1.2487,
      "step": 310
    },
    {
      "epoch": 0.006804432726914704,
      "grad_norm": 2.9014992713928223,
      "learning_rate": 7.919191919191919e-05,
      "loss": 1.4235,
      "step": 311
    },
    {
      "epoch": 0.006826311931824398,
      "grad_norm": 3.6550610065460205,
      "learning_rate": 7.878787878787879e-05,
      "loss": 1.3203,
      "step": 312
    },
    {
      "epoch": 0.006848191136734091,
      "grad_norm": 2.738222599029541,
      "learning_rate": 7.83838383838384e-05,
      "loss": 1.1757,
      "step": 313
    },
    {
      "epoch": 0.006870070341643784,
      "grad_norm": 2.214082717895508,
      "learning_rate": 7.797979797979798e-05,
      "loss": 1.3675,
      "step": 314
    },
    {
      "epoch": 0.006891949546553478,
      "grad_norm": 2.4620020389556885,
      "learning_rate": 7.757575757575758e-05,
      "loss": 1.295,
      "step": 315
    },
    {
      "epoch": 0.0069138287514631715,
      "grad_norm": 2.63326096534729,
      "learning_rate": 7.717171717171718e-05,
      "loss": 1.1438,
      "step": 316
    },
    {
      "epoch": 0.006935707956372866,
      "grad_norm": 3.170936107635498,
      "learning_rate": 7.676767676767676e-05,
      "loss": 1.1836,
      "step": 317
    },
    {
      "epoch": 0.006957587161282559,
      "grad_norm": 2.4681313037872314,
      "learning_rate": 7.636363636363637e-05,
      "loss": 1.2203,
      "step": 318
    },
    {
      "epoch": 0.006979466366192253,
      "grad_norm": 2.4116222858428955,
      "learning_rate": 7.595959595959596e-05,
      "loss": 1.0973,
      "step": 319
    },
    {
      "epoch": 0.007001345571101946,
      "grad_norm": 2.820439577102661,
      "learning_rate": 7.555555555555556e-05,
      "loss": 1.0853,
      "step": 320
    },
    {
      "epoch": 0.007001345571101946,
      "eval_loss": 1.0542235374450684,
      "eval_runtime": 1126.0181,
      "eval_samples_per_second": 0.372,
      "eval_steps_per_second": 0.372,
      "step": 320
    },
    {
      "epoch": 0.00702322477601164,
      "grad_norm": 2.7022147178649902,
      "learning_rate": 7.515151515151515e-05,
      "loss": 1.0392,
      "step": 321
    },
    {
      "epoch": 0.007045103980921333,
      "grad_norm": 3.6522469520568848,
      "learning_rate": 7.474747474747475e-05,
      "loss": 1.0714,
      "step": 322
    },
    {
      "epoch": 0.0070669831858310266,
      "grad_norm": 2.5931456089019775,
      "learning_rate": 7.434343434343435e-05,
      "loss": 1.054,
      "step": 323
    },
    {
      "epoch": 0.007088862390740721,
      "grad_norm": 2.7494957447052,
      "learning_rate": 7.393939393939395e-05,
      "loss": 0.9495,
      "step": 324
    },
    {
      "epoch": 0.007110741595650414,
      "grad_norm": 2.8037216663360596,
      "learning_rate": 7.353535353535353e-05,
      "loss": 0.9763,
      "step": 325
    },
    {
      "epoch": 0.007132620800560108,
      "grad_norm": 2.9409546852111816,
      "learning_rate": 7.313131313131314e-05,
      "loss": 1.1201,
      "step": 326
    },
    {
      "epoch": 0.007154500005469801,
      "grad_norm": 2.6585135459899902,
      "learning_rate": 7.272727272727273e-05,
      "loss": 1.0221,
      "step": 327
    },
    {
      "epoch": 0.007176379210379495,
      "grad_norm": 2.3767690658569336,
      "learning_rate": 7.232323232323233e-05,
      "loss": 0.9632,
      "step": 328
    },
    {
      "epoch": 0.007198258415289188,
      "grad_norm": 2.531965732574463,
      "learning_rate": 7.191919191919192e-05,
      "loss": 0.8375,
      "step": 329
    },
    {
      "epoch": 0.007220137620198882,
      "grad_norm": 2.479558229446411,
      "learning_rate": 7.151515151515152e-05,
      "loss": 1.2294,
      "step": 330
    },
    {
      "epoch": 0.007242016825108576,
      "grad_norm": 2.6898159980773926,
      "learning_rate": 7.111111111111112e-05,
      "loss": 1.0016,
      "step": 331
    },
    {
      "epoch": 0.007263896030018269,
      "grad_norm": 2.984156847000122,
      "learning_rate": 7.07070707070707e-05,
      "loss": 0.8735,
      "step": 332
    },
    {
      "epoch": 0.007285775234927963,
      "grad_norm": 4.829197883605957,
      "learning_rate": 7.03030303030303e-05,
      "loss": 0.8535,
      "step": 333
    },
    {
      "epoch": 0.007307654439837656,
      "grad_norm": 3.0355377197265625,
      "learning_rate": 6.98989898989899e-05,
      "loss": 1.051,
      "step": 334
    },
    {
      "epoch": 0.00732953364474735,
      "grad_norm": 3.3984897136688232,
      "learning_rate": 6.94949494949495e-05,
      "loss": 0.8846,
      "step": 335
    },
    {
      "epoch": 0.007351412849657043,
      "grad_norm": 3.6512601375579834,
      "learning_rate": 6.90909090909091e-05,
      "loss": 0.7291,
      "step": 336
    },
    {
      "epoch": 0.0073732920545667375,
      "grad_norm": 2.7994391918182373,
      "learning_rate": 6.86868686868687e-05,
      "loss": 0.7437,
      "step": 337
    },
    {
      "epoch": 0.007395171259476431,
      "grad_norm": 3.1337954998016357,
      "learning_rate": 6.828282828282828e-05,
      "loss": 0.7388,
      "step": 338
    },
    {
      "epoch": 0.007417050464386124,
      "grad_norm": 2.74843168258667,
      "learning_rate": 6.787878787878789e-05,
      "loss": 0.5403,
      "step": 339
    },
    {
      "epoch": 0.007438929669295818,
      "grad_norm": 3.5722415447235107,
      "learning_rate": 6.747474747474747e-05,
      "loss": 0.8237,
      "step": 340
    },
    {
      "epoch": 0.007438929669295818,
      "eval_loss": 1.0628308057785034,
      "eval_runtime": 1126.1405,
      "eval_samples_per_second": 0.372,
      "eval_steps_per_second": 0.372,
      "step": 340
    },
    {
      "epoch": 0.007460808874205511,
      "grad_norm": 4.262402057647705,
      "learning_rate": 6.707070707070707e-05,
      "loss": 0.6582,
      "step": 341
    },
    {
      "epoch": 0.007482688079115205,
      "grad_norm": 3.7575623989105225,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.5457,
      "step": 342
    },
    {
      "epoch": 0.007504567284024898,
      "grad_norm": 5.9597578048706055,
      "learning_rate": 6.626262626262627e-05,
      "loss": 0.5282,
      "step": 343
    },
    {
      "epoch": 0.0075264464889345925,
      "grad_norm": 3.8190314769744873,
      "learning_rate": 6.585858585858587e-05,
      "loss": 0.645,
      "step": 344
    },
    {
      "epoch": 0.007548325693844286,
      "grad_norm": 4.631303310394287,
      "learning_rate": 6.545454545454546e-05,
      "loss": 0.6021,
      "step": 345
    },
    {
      "epoch": 0.007570204898753979,
      "grad_norm": 3.280122756958008,
      "learning_rate": 6.505050505050505e-05,
      "loss": 0.5675,
      "step": 346
    },
    {
      "epoch": 0.007592084103663673,
      "grad_norm": 3.626063585281372,
      "learning_rate": 6.464646464646466e-05,
      "loss": 0.5506,
      "step": 347
    },
    {
      "epoch": 0.007613963308573366,
      "grad_norm": 3.425227403640747,
      "learning_rate": 6.424242424242424e-05,
      "loss": 0.3604,
      "step": 348
    },
    {
      "epoch": 0.00763584251348306,
      "grad_norm": 2.098273992538452,
      "learning_rate": 6.383838383838384e-05,
      "loss": 0.3698,
      "step": 349
    },
    {
      "epoch": 0.007657721718392753,
      "grad_norm": 3.192187786102295,
      "learning_rate": 6.343434343434344e-05,
      "loss": 0.3964,
      "step": 350
    },
    {
      "epoch": 0.0076796009233024475,
      "grad_norm": 1.9067248106002808,
      "learning_rate": 6.303030303030302e-05,
      "loss": 1.7257,
      "step": 351
    },
    {
      "epoch": 0.007701480128212141,
      "grad_norm": 2.385531425476074,
      "learning_rate": 6.262626262626264e-05,
      "loss": 1.6306,
      "step": 352
    },
    {
      "epoch": 0.007723359333121835,
      "grad_norm": 1.9323674440383911,
      "learning_rate": 6.222222222222222e-05,
      "loss": 1.4108,
      "step": 353
    },
    {
      "epoch": 0.007745238538031528,
      "grad_norm": 2.3959529399871826,
      "learning_rate": 6.181818181818182e-05,
      "loss": 1.7358,
      "step": 354
    },
    {
      "epoch": 0.007767117742941221,
      "grad_norm": 2.200993537902832,
      "learning_rate": 6.141414141414142e-05,
      "loss": 1.6272,
      "step": 355
    },
    {
      "epoch": 0.007788996947850915,
      "grad_norm": 2.177061080932617,
      "learning_rate": 6.101010101010102e-05,
      "loss": 1.2843,
      "step": 356
    },
    {
      "epoch": 0.007810876152760608,
      "grad_norm": 1.9225434064865112,
      "learning_rate": 6.060606060606061e-05,
      "loss": 1.1873,
      "step": 357
    },
    {
      "epoch": 0.007832755357670302,
      "grad_norm": 2.0962164402008057,
      "learning_rate": 6.02020202020202e-05,
      "loss": 1.5584,
      "step": 358
    },
    {
      "epoch": 0.007854634562579996,
      "grad_norm": 2.5016815662384033,
      "learning_rate": 5.97979797979798e-05,
      "loss": 1.3339,
      "step": 359
    },
    {
      "epoch": 0.00787651376748969,
      "grad_norm": 2.6286799907684326,
      "learning_rate": 5.93939393939394e-05,
      "loss": 1.5063,
      "step": 360
    },
    {
      "epoch": 0.00787651376748969,
      "eval_loss": 1.058814287185669,
      "eval_runtime": 1126.8044,
      "eval_samples_per_second": 0.372,
      "eval_steps_per_second": 0.372,
      "step": 360
    },
    {
      "epoch": 0.007898392972399384,
      "grad_norm": 2.5376617908477783,
      "learning_rate": 5.8989898989898996e-05,
      "loss": 1.235,
      "step": 361
    },
    {
      "epoch": 0.007920272177309076,
      "grad_norm": 3.129783868789673,
      "learning_rate": 5.858585858585859e-05,
      "loss": 1.3544,
      "step": 362
    },
    {
      "epoch": 0.00794215138221877,
      "grad_norm": 3.018368721008301,
      "learning_rate": 5.818181818181818e-05,
      "loss": 1.3913,
      "step": 363
    },
    {
      "epoch": 0.007964030587128464,
      "grad_norm": 2.5278642177581787,
      "learning_rate": 5.7777777777777776e-05,
      "loss": 1.1855,
      "step": 364
    },
    {
      "epoch": 0.007985909792038157,
      "grad_norm": 3.726611614227295,
      "learning_rate": 5.737373737373738e-05,
      "loss": 1.3958,
      "step": 365
    },
    {
      "epoch": 0.00800778899694785,
      "grad_norm": 2.44836163520813,
      "learning_rate": 5.696969696969697e-05,
      "loss": 0.9616,
      "step": 366
    },
    {
      "epoch": 0.008029668201857545,
      "grad_norm": 2.327892303466797,
      "learning_rate": 5.6565656565656563e-05,
      "loss": 1.0815,
      "step": 367
    },
    {
      "epoch": 0.008051547406767239,
      "grad_norm": 2.8930983543395996,
      "learning_rate": 5.616161616161616e-05,
      "loss": 1.1644,
      "step": 368
    },
    {
      "epoch": 0.008073426611676931,
      "grad_norm": 2.6649205684661865,
      "learning_rate": 5.5757575757575766e-05,
      "loss": 1.3699,
      "step": 369
    },
    {
      "epoch": 0.008095305816586625,
      "grad_norm": 2.495908498764038,
      "learning_rate": 5.535353535353536e-05,
      "loss": 0.973,
      "step": 370
    },
    {
      "epoch": 0.00811718502149632,
      "grad_norm": 2.8452885150909424,
      "learning_rate": 5.494949494949495e-05,
      "loss": 1.0671,
      "step": 371
    },
    {
      "epoch": 0.008139064226406012,
      "grad_norm": 2.9397976398468018,
      "learning_rate": 5.4545454545454546e-05,
      "loss": 1.0134,
      "step": 372
    },
    {
      "epoch": 0.008160943431315706,
      "grad_norm": 3.0953664779663086,
      "learning_rate": 5.414141414141415e-05,
      "loss": 0.967,
      "step": 373
    },
    {
      "epoch": 0.0081828226362254,
      "grad_norm": 2.514328718185425,
      "learning_rate": 5.373737373737374e-05,
      "loss": 1.0642,
      "step": 374
    },
    {
      "epoch": 0.008204701841135094,
      "grad_norm": 2.597201108932495,
      "learning_rate": 5.333333333333333e-05,
      "loss": 1.1634,
      "step": 375
    },
    {
      "epoch": 0.008226581046044786,
      "grad_norm": 2.565070152282715,
      "learning_rate": 5.292929292929293e-05,
      "loss": 1.034,
      "step": 376
    },
    {
      "epoch": 0.00824846025095448,
      "grad_norm": 3.029691696166992,
      "learning_rate": 5.2525252525252536e-05,
      "loss": 0.9234,
      "step": 377
    },
    {
      "epoch": 0.008270339455864174,
      "grad_norm": 3.017089605331421,
      "learning_rate": 5.212121212121213e-05,
      "loss": 1.0586,
      "step": 378
    },
    {
      "epoch": 0.008292218660773867,
      "grad_norm": 3.2232024669647217,
      "learning_rate": 5.171717171717172e-05,
      "loss": 0.904,
      "step": 379
    },
    {
      "epoch": 0.00831409786568356,
      "grad_norm": 2.943955898284912,
      "learning_rate": 5.1313131313131316e-05,
      "loss": 1.0478,
      "step": 380
    },
    {
      "epoch": 0.00831409786568356,
      "eval_loss": 1.0584824085235596,
      "eval_runtime": 1125.8919,
      "eval_samples_per_second": 0.372,
      "eval_steps_per_second": 0.372,
      "step": 380
    },
    {
      "epoch": 0.008335977070593255,
      "grad_norm": 4.526090145111084,
      "learning_rate": 5.090909090909091e-05,
      "loss": 0.8801,
      "step": 381
    },
    {
      "epoch": 0.008357856275502949,
      "grad_norm": 4.359368801116943,
      "learning_rate": 5.050505050505051e-05,
      "loss": 0.9915,
      "step": 382
    },
    {
      "epoch": 0.008379735480412641,
      "grad_norm": 3.318157911300659,
      "learning_rate": 5.01010101010101e-05,
      "loss": 0.7019,
      "step": 383
    },
    {
      "epoch": 0.008401614685322335,
      "grad_norm": 3.7887117862701416,
      "learning_rate": 4.9696969696969694e-05,
      "loss": 0.722,
      "step": 384
    },
    {
      "epoch": 0.00842349389023203,
      "grad_norm": 3.098832607269287,
      "learning_rate": 4.92929292929293e-05,
      "loss": 0.6054,
      "step": 385
    },
    {
      "epoch": 0.008445373095141722,
      "grad_norm": 2.985724687576294,
      "learning_rate": 4.888888888888889e-05,
      "loss": 0.8249,
      "step": 386
    },
    {
      "epoch": 0.008467252300051416,
      "grad_norm": 3.0541269779205322,
      "learning_rate": 4.848484848484849e-05,
      "loss": 0.6253,
      "step": 387
    },
    {
      "epoch": 0.00848913150496111,
      "grad_norm": 3.8758230209350586,
      "learning_rate": 4.808080808080808e-05,
      "loss": 0.9013,
      "step": 388
    },
    {
      "epoch": 0.008511010709870804,
      "grad_norm": 3.254669666290283,
      "learning_rate": 4.7676767676767684e-05,
      "loss": 0.6832,
      "step": 389
    },
    {
      "epoch": 0.008532889914780496,
      "grad_norm": 2.850383758544922,
      "learning_rate": 4.7272727272727275e-05,
      "loss": 0.5728,
      "step": 390
    },
    {
      "epoch": 0.00855476911969019,
      "grad_norm": 3.0792276859283447,
      "learning_rate": 4.686868686868687e-05,
      "loss": 0.7791,
      "step": 391
    },
    {
      "epoch": 0.008576648324599884,
      "grad_norm": 3.7406270503997803,
      "learning_rate": 4.6464646464646464e-05,
      "loss": 0.7112,
      "step": 392
    },
    {
      "epoch": 0.008598527529509578,
      "grad_norm": 4.668735980987549,
      "learning_rate": 4.606060606060607e-05,
      "loss": 0.5237,
      "step": 393
    },
    {
      "epoch": 0.00862040673441927,
      "grad_norm": 3.740192174911499,
      "learning_rate": 4.565656565656566e-05,
      "loss": 0.5332,
      "step": 394
    },
    {
      "epoch": 0.008642285939328965,
      "grad_norm": 3.5966694355010986,
      "learning_rate": 4.525252525252526e-05,
      "loss": 0.4502,
      "step": 395
    },
    {
      "epoch": 0.008664165144238659,
      "grad_norm": 3.3049585819244385,
      "learning_rate": 4.484848484848485e-05,
      "loss": 0.6506,
      "step": 396
    },
    {
      "epoch": 0.008686044349148351,
      "grad_norm": 3.8160126209259033,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 0.3845,
      "step": 397
    },
    {
      "epoch": 0.008707923554058045,
      "grad_norm": 3.1983022689819336,
      "learning_rate": 4.4040404040404044e-05,
      "loss": 0.4026,
      "step": 398
    },
    {
      "epoch": 0.00872980275896774,
      "grad_norm": 3.895946979522705,
      "learning_rate": 4.3636363636363636e-05,
      "loss": 0.3918,
      "step": 399
    },
    {
      "epoch": 0.008751681963877433,
      "grad_norm": 4.417733192443848,
      "learning_rate": 4.3232323232323234e-05,
      "loss": 0.4377,
      "step": 400
    },
    {
      "epoch": 0.008751681963877433,
      "eval_loss": 1.061728596687317,
      "eval_runtime": 1154.0533,
      "eval_samples_per_second": 0.363,
      "eval_steps_per_second": 0.363,
      "step": 400
    },
    {
      "epoch": 0.008773561168787126,
      "grad_norm": 1.8909755945205688,
      "learning_rate": 4.282828282828283e-05,
      "loss": 1.6193,
      "step": 401
    },
    {
      "epoch": 0.00879544037369682,
      "grad_norm": 2.021498203277588,
      "learning_rate": 4.242424242424243e-05,
      "loss": 1.6536,
      "step": 402
    },
    {
      "epoch": 0.008817319578606514,
      "grad_norm": 2.0645227432250977,
      "learning_rate": 4.202020202020202e-05,
      "loss": 1.5661,
      "step": 403
    },
    {
      "epoch": 0.008839198783516206,
      "grad_norm": 3.4937970638275146,
      "learning_rate": 4.161616161616162e-05,
      "loss": 1.5083,
      "step": 404
    },
    {
      "epoch": 0.0088610779884259,
      "grad_norm": 2.45450758934021,
      "learning_rate": 4.1212121212121216e-05,
      "loss": 1.2853,
      "step": 405
    },
    {
      "epoch": 0.008882957193335594,
      "grad_norm": 2.4743194580078125,
      "learning_rate": 4.0808080808080814e-05,
      "loss": 1.3852,
      "step": 406
    },
    {
      "epoch": 0.008904836398245288,
      "grad_norm": 2.850428581237793,
      "learning_rate": 4.0404040404040405e-05,
      "loss": 1.201,
      "step": 407
    },
    {
      "epoch": 0.00892671560315498,
      "grad_norm": 2.783496141433716,
      "learning_rate": 4e-05,
      "loss": 1.361,
      "step": 408
    },
    {
      "epoch": 0.008948594808064675,
      "grad_norm": 2.366154909133911,
      "learning_rate": 3.9595959595959594e-05,
      "loss": 1.3886,
      "step": 409
    },
    {
      "epoch": 0.008970474012974369,
      "grad_norm": 2.717778205871582,
      "learning_rate": 3.91919191919192e-05,
      "loss": 1.1507,
      "step": 410
    },
    {
      "epoch": 0.008992353217884061,
      "grad_norm": 2.9946999549865723,
      "learning_rate": 3.878787878787879e-05,
      "loss": 1.0793,
      "step": 411
    },
    {
      "epoch": 0.009014232422793755,
      "grad_norm": 2.6511268615722656,
      "learning_rate": 3.838383838383838e-05,
      "loss": 1.1405,
      "step": 412
    },
    {
      "epoch": 0.00903611162770345,
      "grad_norm": 2.9845776557922363,
      "learning_rate": 3.797979797979798e-05,
      "loss": 1.1691,
      "step": 413
    },
    {
      "epoch": 0.009057990832613144,
      "grad_norm": 2.4852242469787598,
      "learning_rate": 3.757575757575758e-05,
      "loss": 1.0895,
      "step": 414
    },
    {
      "epoch": 0.009079870037522836,
      "grad_norm": 3.339536666870117,
      "learning_rate": 3.7171717171717175e-05,
      "loss": 1.1938,
      "step": 415
    },
    {
      "epoch": 0.00910174924243253,
      "grad_norm": 1.992193341255188,
      "learning_rate": 3.6767676767676766e-05,
      "loss": 1.1645,
      "step": 416
    },
    {
      "epoch": 0.009123628447342224,
      "grad_norm": 5.612308025360107,
      "learning_rate": 3.6363636363636364e-05,
      "loss": 1.1228,
      "step": 417
    },
    {
      "epoch": 0.009145507652251916,
      "grad_norm": 2.836247682571411,
      "learning_rate": 3.595959595959596e-05,
      "loss": 1.1992,
      "step": 418
    },
    {
      "epoch": 0.00916738685716161,
      "grad_norm": 2.7728872299194336,
      "learning_rate": 3.555555555555556e-05,
      "loss": 1.0511,
      "step": 419
    },
    {
      "epoch": 0.009189266062071304,
      "grad_norm": 3.0089898109436035,
      "learning_rate": 3.515151515151515e-05,
      "loss": 1.2261,
      "step": 420
    },
    {
      "epoch": 0.009189266062071304,
      "eval_loss": 1.0582693815231323,
      "eval_runtime": 1153.8889,
      "eval_samples_per_second": 0.363,
      "eval_steps_per_second": 0.363,
      "step": 420
    },
    {
      "epoch": 0.009211145266980999,
      "grad_norm": 2.5044634342193604,
      "learning_rate": 3.474747474747475e-05,
      "loss": 1.1661,
      "step": 421
    },
    {
      "epoch": 0.00923302447189069,
      "grad_norm": 2.8743393421173096,
      "learning_rate": 3.434343434343435e-05,
      "loss": 0.967,
      "step": 422
    },
    {
      "epoch": 0.009254903676800385,
      "grad_norm": 3.4270005226135254,
      "learning_rate": 3.3939393939393945e-05,
      "loss": 1.186,
      "step": 423
    },
    {
      "epoch": 0.009276782881710079,
      "grad_norm": 3.0980911254882812,
      "learning_rate": 3.3535353535353536e-05,
      "loss": 1.0907,
      "step": 424
    },
    {
      "epoch": 0.009298662086619773,
      "grad_norm": 2.8450021743774414,
      "learning_rate": 3.3131313131313134e-05,
      "loss": 1.0905,
      "step": 425
    },
    {
      "epoch": 0.009320541291529465,
      "grad_norm": 3.0503838062286377,
      "learning_rate": 3.272727272727273e-05,
      "loss": 0.9707,
      "step": 426
    },
    {
      "epoch": 0.00934242049643916,
      "grad_norm": 3.6757235527038574,
      "learning_rate": 3.232323232323233e-05,
      "loss": 0.8985,
      "step": 427
    },
    {
      "epoch": 0.009364299701348854,
      "grad_norm": 3.268986225128174,
      "learning_rate": 3.191919191919192e-05,
      "loss": 0.9475,
      "step": 428
    },
    {
      "epoch": 0.009386178906258546,
      "grad_norm": 3.388612985610962,
      "learning_rate": 3.151515151515151e-05,
      "loss": 0.9689,
      "step": 429
    },
    {
      "epoch": 0.00940805811116824,
      "grad_norm": 3.3108317852020264,
      "learning_rate": 3.111111111111111e-05,
      "loss": 0.818,
      "step": 430
    },
    {
      "epoch": 0.009429937316077934,
      "grad_norm": 3.7734203338623047,
      "learning_rate": 3.070707070707071e-05,
      "loss": 0.9362,
      "step": 431
    },
    {
      "epoch": 0.009451816520987628,
      "grad_norm": 3.414229154586792,
      "learning_rate": 3.0303030303030306e-05,
      "loss": 0.9608,
      "step": 432
    },
    {
      "epoch": 0.00947369572589732,
      "grad_norm": 3.319833993911743,
      "learning_rate": 2.98989898989899e-05,
      "loss": 0.6165,
      "step": 433
    },
    {
      "epoch": 0.009495574930807014,
      "grad_norm": 2.7725861072540283,
      "learning_rate": 2.9494949494949498e-05,
      "loss": 0.8539,
      "step": 434
    },
    {
      "epoch": 0.009517454135716709,
      "grad_norm": 2.96895694732666,
      "learning_rate": 2.909090909090909e-05,
      "loss": 0.6363,
      "step": 435
    },
    {
      "epoch": 0.009539333340626401,
      "grad_norm": 3.8536930084228516,
      "learning_rate": 2.868686868686869e-05,
      "loss": 0.757,
      "step": 436
    },
    {
      "epoch": 0.009561212545536095,
      "grad_norm": 3.202620029449463,
      "learning_rate": 2.8282828282828282e-05,
      "loss": 0.7451,
      "step": 437
    },
    {
      "epoch": 0.009583091750445789,
      "grad_norm": 2.8805456161499023,
      "learning_rate": 2.7878787878787883e-05,
      "loss": 0.6943,
      "step": 438
    },
    {
      "epoch": 0.009604970955355483,
      "grad_norm": 3.5715019702911377,
      "learning_rate": 2.7474747474747474e-05,
      "loss": 0.717,
      "step": 439
    },
    {
      "epoch": 0.009626850160265175,
      "grad_norm": 3.8024115562438965,
      "learning_rate": 2.7070707070707075e-05,
      "loss": 0.8258,
      "step": 440
    },
    {
      "epoch": 0.009626850160265175,
      "eval_loss": 1.0608880519866943,
      "eval_runtime": 1152.5736,
      "eval_samples_per_second": 0.364,
      "eval_steps_per_second": 0.364,
      "step": 440
    },
    {
      "epoch": 0.00964872936517487,
      "grad_norm": 3.579216718673706,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.7631,
      "step": 441
    },
    {
      "epoch": 0.009670608570084564,
      "grad_norm": 3.139361619949341,
      "learning_rate": 2.6262626262626268e-05,
      "loss": 0.6531,
      "step": 442
    },
    {
      "epoch": 0.009692487774994256,
      "grad_norm": 3.6172993183135986,
      "learning_rate": 2.585858585858586e-05,
      "loss": 0.6209,
      "step": 443
    },
    {
      "epoch": 0.00971436697990395,
      "grad_norm": 3.2847037315368652,
      "learning_rate": 2.5454545454545454e-05,
      "loss": 0.5963,
      "step": 444
    },
    {
      "epoch": 0.009736246184813644,
      "grad_norm": 3.659386157989502,
      "learning_rate": 2.505050505050505e-05,
      "loss": 0.7072,
      "step": 445
    },
    {
      "epoch": 0.009758125389723338,
      "grad_norm": 3.167666435241699,
      "learning_rate": 2.464646464646465e-05,
      "loss": 0.5781,
      "step": 446
    },
    {
      "epoch": 0.00978000459463303,
      "grad_norm": 3.474196195602417,
      "learning_rate": 2.4242424242424244e-05,
      "loss": 0.3468,
      "step": 447
    },
    {
      "epoch": 0.009801883799542725,
      "grad_norm": 6.068548202514648,
      "learning_rate": 2.3838383838383842e-05,
      "loss": 0.6014,
      "step": 448
    },
    {
      "epoch": 0.009823763004452419,
      "grad_norm": 3.263716220855713,
      "learning_rate": 2.3434343434343436e-05,
      "loss": 0.4576,
      "step": 449
    },
    {
      "epoch": 0.009845642209362111,
      "grad_norm": 2.606013536453247,
      "learning_rate": 2.3030303030303034e-05,
      "loss": 0.3691,
      "step": 450
    },
    {
      "epoch": 0.009867521414271805,
      "grad_norm": 1.8506062030792236,
      "learning_rate": 2.262626262626263e-05,
      "loss": 1.7656,
      "step": 451
    },
    {
      "epoch": 0.009889400619181499,
      "grad_norm": 1.986030101776123,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 1.594,
      "step": 452
    },
    {
      "epoch": 0.009911279824091193,
      "grad_norm": 1.983208417892456,
      "learning_rate": 2.1818181818181818e-05,
      "loss": 1.5195,
      "step": 453
    },
    {
      "epoch": 0.009933159029000885,
      "grad_norm": 2.0429494380950928,
      "learning_rate": 2.1414141414141416e-05,
      "loss": 1.5082,
      "step": 454
    },
    {
      "epoch": 0.00995503823391058,
      "grad_norm": 2.5082757472991943,
      "learning_rate": 2.101010101010101e-05,
      "loss": 1.4408,
      "step": 455
    },
    {
      "epoch": 0.009976917438820274,
      "grad_norm": 2.0133697986602783,
      "learning_rate": 2.0606060606060608e-05,
      "loss": 1.0878,
      "step": 456
    },
    {
      "epoch": 0.009998796643729968,
      "grad_norm": 2.173503875732422,
      "learning_rate": 2.0202020202020203e-05,
      "loss": 1.1453,
      "step": 457
    },
    {
      "epoch": 0.01002067584863966,
      "grad_norm": 2.6416826248168945,
      "learning_rate": 1.9797979797979797e-05,
      "loss": 1.3531,
      "step": 458
    },
    {
      "epoch": 0.010042555053549354,
      "grad_norm": 2.166341543197632,
      "learning_rate": 1.9393939393939395e-05,
      "loss": 1.0057,
      "step": 459
    },
    {
      "epoch": 0.010064434258459048,
      "grad_norm": 2.2690532207489014,
      "learning_rate": 1.898989898989899e-05,
      "loss": 1.2631,
      "step": 460
    },
    {
      "epoch": 0.010064434258459048,
      "eval_loss": 1.0579965114593506,
      "eval_runtime": 1126.4556,
      "eval_samples_per_second": 0.372,
      "eval_steps_per_second": 0.372,
      "step": 460
    },
    {
      "epoch": 0.01008631346336874,
      "grad_norm": 3.1414246559143066,
      "learning_rate": 1.8585858585858588e-05,
      "loss": 1.2703,
      "step": 461
    },
    {
      "epoch": 0.010108192668278435,
      "grad_norm": 2.673893451690674,
      "learning_rate": 1.8181818181818182e-05,
      "loss": 1.1916,
      "step": 462
    },
    {
      "epoch": 0.010130071873188129,
      "grad_norm": 2.4277358055114746,
      "learning_rate": 1.777777777777778e-05,
      "loss": 1.154,
      "step": 463
    },
    {
      "epoch": 0.010151951078097823,
      "grad_norm": 2.519286632537842,
      "learning_rate": 1.7373737373737375e-05,
      "loss": 1.1732,
      "step": 464
    },
    {
      "epoch": 0.010173830283007515,
      "grad_norm": 2.6941003799438477,
      "learning_rate": 1.6969696969696972e-05,
      "loss": 1.2328,
      "step": 465
    },
    {
      "epoch": 0.010195709487917209,
      "grad_norm": 3.4766077995300293,
      "learning_rate": 1.6565656565656567e-05,
      "loss": 1.181,
      "step": 466
    },
    {
      "epoch": 0.010217588692826903,
      "grad_norm": 2.8215789794921875,
      "learning_rate": 1.6161616161616165e-05,
      "loss": 0.8892,
      "step": 467
    },
    {
      "epoch": 0.010239467897736595,
      "grad_norm": 2.7106833457946777,
      "learning_rate": 1.5757575757575756e-05,
      "loss": 1.0561,
      "step": 468
    },
    {
      "epoch": 0.01026134710264629,
      "grad_norm": 2.6014740467071533,
      "learning_rate": 1.5353535353535354e-05,
      "loss": 1.088,
      "step": 469
    },
    {
      "epoch": 0.010283226307555984,
      "grad_norm": 2.702737808227539,
      "learning_rate": 1.494949494949495e-05,
      "loss": 0.8211,
      "step": 470
    },
    {
      "epoch": 0.010305105512465678,
      "grad_norm": 3.2528674602508545,
      "learning_rate": 1.4545454545454545e-05,
      "loss": 1.1465,
      "step": 471
    },
    {
      "epoch": 0.01032698471737537,
      "grad_norm": 2.822068214416504,
      "learning_rate": 1.4141414141414141e-05,
      "loss": 1.0342,
      "step": 472
    },
    {
      "epoch": 0.010348863922285064,
      "grad_norm": 2.8883211612701416,
      "learning_rate": 1.3737373737373737e-05,
      "loss": 0.9038,
      "step": 473
    },
    {
      "epoch": 0.010370743127194758,
      "grad_norm": 3.1304938793182373,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 1.1129,
      "step": 474
    },
    {
      "epoch": 0.01039262233210445,
      "grad_norm": 3.5152394771575928,
      "learning_rate": 1.292929292929293e-05,
      "loss": 1.0525,
      "step": 475
    },
    {
      "epoch": 0.010414501537014145,
      "grad_norm": 3.350491523742676,
      "learning_rate": 1.2525252525252526e-05,
      "loss": 0.9418,
      "step": 476
    },
    {
      "epoch": 0.010436380741923839,
      "grad_norm": 3.557809829711914,
      "learning_rate": 1.2121212121212122e-05,
      "loss": 0.7795,
      "step": 477
    },
    {
      "epoch": 0.010458259946833533,
      "grad_norm": 2.787726879119873,
      "learning_rate": 1.1717171717171718e-05,
      "loss": 0.9956,
      "step": 478
    },
    {
      "epoch": 0.010480139151743225,
      "grad_norm": 3.391378402709961,
      "learning_rate": 1.1313131313131314e-05,
      "loss": 0.8999,
      "step": 479
    },
    {
      "epoch": 0.010502018356652919,
      "grad_norm": 3.590916395187378,
      "learning_rate": 1.0909090909090909e-05,
      "loss": 0.8525,
      "step": 480
    },
    {
      "epoch": 0.010502018356652919,
      "eval_loss": 1.0576767921447754,
      "eval_runtime": 1126.1204,
      "eval_samples_per_second": 0.372,
      "eval_steps_per_second": 0.372,
      "step": 480
    }
  ],
  "logging_steps": 1,
  "max_steps": 500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 40,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.038589835551539e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
