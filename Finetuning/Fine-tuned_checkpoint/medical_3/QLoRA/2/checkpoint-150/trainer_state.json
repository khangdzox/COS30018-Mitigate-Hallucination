{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.012408102490926574,
  "eval_steps": 500,
  "global_step": 150,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 8.272068327284383e-05,
      "grad_norm": 1.5043562650680542,
      "learning_rate": 0.0001,
      "loss": 1.9595,
      "step": 1
    },
    {
      "epoch": 0.00016544136654568766,
      "grad_norm": 1.8306677341461182,
      "learning_rate": 9.990094071072877e-05,
      "loss": 2.3019,
      "step": 2
    },
    {
      "epoch": 0.0002481620498185315,
      "grad_norm": 1.7613961696624756,
      "learning_rate": 9.960415535262671e-05,
      "loss": 2.2783,
      "step": 3
    },
    {
      "epoch": 0.0003308827330913753,
      "grad_norm": 1.9991271495819092,
      "learning_rate": 9.91108198995594e-05,
      "loss": 2.0563,
      "step": 4
    },
    {
      "epoch": 0.00041360341636421916,
      "grad_norm": NaN,
      "learning_rate": 9.91108198995594e-05,
      "loss": 2.2947,
      "step": 5
    },
    {
      "epoch": 0.000496324099637063,
      "grad_norm": 2.020979166030884,
      "learning_rate": 9.842288912990096e-05,
      "loss": 2.2119,
      "step": 6
    },
    {
      "epoch": 0.0005790447829099068,
      "grad_norm": 2.107518434524536,
      "learning_rate": 9.754308888097583e-05,
      "loss": 2.0839,
      "step": 7
    },
    {
      "epoch": 0.0006617654661827506,
      "grad_norm": 2.0635111331939697,
      "learning_rate": 9.647490524827834e-05,
      "loss": 2.0968,
      "step": 8
    },
    {
      "epoch": 0.0007444861494555945,
      "grad_norm": 2.1375210285186768,
      "learning_rate": 9.522257077226717e-05,
      "loss": 2.1344,
      "step": 9
    },
    {
      "epoch": 0.0008272068327284383,
      "grad_norm": 2.2170939445495605,
      "learning_rate": 9.379104766746722e-05,
      "loss": 1.7956,
      "step": 10
    },
    {
      "epoch": 0.0009099275160012822,
      "grad_norm": 2.70408034324646,
      "learning_rate": 9.2186008160332e-05,
      "loss": 1.772,
      "step": 11
    },
    {
      "epoch": 0.000992648199274126,
      "grad_norm": 2.7873737812042236,
      "learning_rate": 9.041381201377468e-05,
      "loss": 1.84,
      "step": 12
    },
    {
      "epoch": 0.00107536888254697,
      "grad_norm": 4.790360927581787,
      "learning_rate": 8.848148132742431e-05,
      "loss": 1.8058,
      "step": 13
    },
    {
      "epoch": 0.0011580895658198137,
      "grad_norm": 3.945890426635742,
      "learning_rate": 8.6396672713458e-05,
      "loss": 1.7737,
      "step": 14
    },
    {
      "epoch": 0.0012408102490926575,
      "grad_norm": 2.960235595703125,
      "learning_rate": 8.416764695825834e-05,
      "loss": 1.543,
      "step": 15
    },
    {
      "epoch": 0.0013235309323655013,
      "grad_norm": 3.493924856185913,
      "learning_rate": 8.180323629010848e-05,
      "loss": 1.3756,
      "step": 16
    },
    {
      "epoch": 0.0014062516156383453,
      "grad_norm": 5.751547336578369,
      "learning_rate": 7.93128093826217e-05,
      "loss": 1.4768,
      "step": 17
    },
    {
      "epoch": 0.001488972298911189,
      "grad_norm": 3.678823471069336,
      "learning_rate": 7.670623423257548e-05,
      "loss": 1.5063,
      "step": 18
    },
    {
      "epoch": 0.0015716929821840329,
      "grad_norm": 2.7539215087890625,
      "learning_rate": 7.399383905924165e-05,
      "loss": 1.4013,
      "step": 19
    },
    {
      "epoch": 0.0016544136654568766,
      "grad_norm": 2.9167375564575195,
      "learning_rate": 7.118637138014396e-05,
      "loss": 1.3577,
      "step": 20
    },
    {
      "epoch": 0.0017371343487297204,
      "grad_norm": 2.256544589996338,
      "learning_rate": 6.829495542540013e-05,
      "loss": 1.2725,
      "step": 21
    },
    {
      "epoch": 0.0018198550320025644,
      "grad_norm": 2.7758729457855225,
      "learning_rate": 6.533104805938873e-05,
      "loss": 1.3249,
      "step": 22
    },
    {
      "epoch": 0.0019025757152754082,
      "grad_norm": 3.7425026893615723,
      "learning_rate": 6.230639338439549e-05,
      "loss": 1.2934,
      "step": 23
    },
    {
      "epoch": 0.001985296398548252,
      "grad_norm": 5.221314907073975,
      "learning_rate": 5.923297620611623e-05,
      "loss": 1.1909,
      "step": 24
    },
    {
      "epoch": 0.002068017081821096,
      "grad_norm": 2.2888824939727783,
      "learning_rate": 5.6122974545403525e-05,
      "loss": 1.2838,
      "step": 25
    },
    {
      "epoch": 0.00215073776509394,
      "grad_norm": 2.5191752910614014,
      "learning_rate": 5.298871138442307e-05,
      "loss": 1.1959,
      "step": 26
    },
    {
      "epoch": 0.0022334584483667836,
      "grad_norm": 2.24515962600708,
      "learning_rate": 4.984260583841951e-05,
      "loss": 1.1333,
      "step": 27
    },
    {
      "epoch": 0.0023161791316396274,
      "grad_norm": 2.2987701892852783,
      "learning_rate": 4.6697123946567227e-05,
      "loss": 1.1369,
      "step": 28
    },
    {
      "epoch": 0.002398899814912471,
      "grad_norm": 3.235107660293579,
      "learning_rate": 4.356472927689109e-05,
      "loss": 1.0308,
      "step": 29
    },
    {
      "epoch": 0.002481620498185315,
      "grad_norm": 3.092144250869751,
      "learning_rate": 4.045783354097893e-05,
      "loss": 1.1718,
      "step": 30
    },
    {
      "epoch": 0.0025643411814581588,
      "grad_norm": 2.702939510345459,
      "learning_rate": 3.73887474141683e-05,
      "loss": 0.9858,
      "step": 31
    },
    {
      "epoch": 0.0026470618647310025,
      "grad_norm": 3.1886661052703857,
      "learning_rate": 3.436963175607658e-05,
      "loss": 1.0431,
      "step": 32
    },
    {
      "epoch": 0.0027297825480038463,
      "grad_norm": 2.6861298084259033,
      "learning_rate": 3.1412449424756474e-05,
      "loss": 1.2254,
      "step": 33
    },
    {
      "epoch": 0.0028125032312766906,
      "grad_norm": 2.322610855102539,
      "learning_rate": 2.8528917875407433e-05,
      "loss": 1.2049,
      "step": 34
    },
    {
      "epoch": 0.0028952239145495343,
      "grad_norm": 3.3600270748138428,
      "learning_rate": 2.5730462731464273e-05,
      "loss": 1.0007,
      "step": 35
    },
    {
      "epoch": 0.002977944597822378,
      "grad_norm": 3.2248644828796387,
      "learning_rate": 2.3028172512031604e-05,
      "loss": 1.0582,
      "step": 36
    },
    {
      "epoch": 0.003060665281095222,
      "grad_norm": 2.5334842205047607,
      "learning_rate": 2.0432754695051136e-05,
      "loss": 0.9331,
      "step": 37
    },
    {
      "epoch": 0.0031433859643680657,
      "grad_norm": 2.458400249481201,
      "learning_rate": 1.795449329029531e-05,
      "loss": 0.9865,
      "step": 38
    },
    {
      "epoch": 0.0032261066476409095,
      "grad_norm": 2.4662415981292725,
      "learning_rate": 1.560320809029948e-05,
      "loss": 0.9434,
      "step": 39
    },
    {
      "epoch": 0.0033088273309137533,
      "grad_norm": 2.526129722595215,
      "learning_rate": 1.3388215760695083e-05,
      "loss": 1.0176,
      "step": 40
    },
    {
      "epoch": 0.003391548014186597,
      "grad_norm": 2.6398019790649414,
      "learning_rate": 1.1318292924118601e-05,
      "loss": 0.8116,
      "step": 41
    },
    {
      "epoch": 0.003474268697459441,
      "grad_norm": 3.184659481048584,
      "learning_rate": 9.401641383971477e-06,
      "loss": 0.8309,
      "step": 42
    },
    {
      "epoch": 0.0035569893807322847,
      "grad_norm": 3.0133354663848877,
      "learning_rate": 7.64585562582767e-06,
      "loss": 0.8475,
      "step": 43
    },
    {
      "epoch": 0.003639710064005129,
      "grad_norm": 2.968069314956665,
      "learning_rate": 6.057892725259717e-06,
      "loss": 0.7816,
      "step": 44
    },
    {
      "epoch": 0.0037224307472779727,
      "grad_norm": 2.7015604972839355,
      "learning_rate": 4.644044781320422e-06,
      "loss": 0.7451,
      "step": 45
    },
    {
      "epoch": 0.0038051514305508165,
      "grad_norm": 2.995434284210205,
      "learning_rate": 3.4099139849083307e-06,
      "loss": 0.8383,
      "step": 46
    },
    {
      "epoch": 0.0038878721138236602,
      "grad_norm": 3.2848455905914307,
      "learning_rate": 2.360390420805869e-06,
      "loss": 0.7847,
      "step": 47
    },
    {
      "epoch": 0.003970592797096504,
      "grad_norm": 3.653118133544922,
      "learning_rate": 1.499632691346381e-06,
      "loss": 0.84,
      "step": 48
    },
    {
      "epoch": 0.004053313480369348,
      "grad_norm": 3.5103344917297363,
      "learning_rate": 8.31051438486441e-07,
      "loss": 0.8116,
      "step": 49
    },
    {
      "epoch": 0.004136034163642192,
      "grad_norm": 4.318592071533203,
      "learning_rate": 3.572958295752049e-07,
      "loss": 0.8914,
      "step": 50
    },
    {
      "epoch": 0.004218754846915035,
      "grad_norm": 2.5561983585357666,
      "learning_rate": 8.02430603689397e-08,
      "loss": 1.4523,
      "step": 51
    },
    {
      "epoch": 0.00430147553018788,
      "grad_norm": 2.3827123641967773,
      "learning_rate": 9.999900908311602e-05,
      "loss": 1.2889,
      "step": 52
    },
    {
      "epoch": 0.004384196213460723,
      "grad_norm": 2.10959529876709,
      "learning_rate": 9.988014657443941e-05,
      "loss": 1.381,
      "step": 53
    },
    {
      "epoch": 0.004466916896733567,
      "grad_norm": 2.2905192375183105,
      "learning_rate": 9.956364039102642e-05,
      "loss": 1.2601,
      "step": 54
    },
    {
      "epoch": 0.0045496375800064106,
      "grad_norm": 2.2840139865875244,
      "learning_rate": 9.905074464798024e-05,
      "loss": 1.362,
      "step": 55
    },
    {
      "epoch": 0.004632358263279255,
      "grad_norm": 2.5473103523254395,
      "learning_rate": 9.83434916288119e-05,
      "loss": 1.4671,
      "step": 56
    },
    {
      "epoch": 0.004715078946552098,
      "grad_norm": 2.331167697906494,
      "learning_rate": 9.744468373277797e-05,
      "loss": 1.3009,
      "step": 57
    },
    {
      "epoch": 0.004797799629824942,
      "grad_norm": 2.356964111328125,
      "learning_rate": 9.635788237073334e-05,
      "loss": 1.3004,
      "step": 58
    },
    {
      "epoch": 0.004880520313097787,
      "grad_norm": 1.935747504234314,
      "learning_rate": 9.508739385349812e-05,
      "loss": 1.279,
      "step": 59
    },
    {
      "epoch": 0.00496324099637063,
      "grad_norm": 2.097308397293091,
      "learning_rate": 9.363825232865413e-05,
      "loss": 1.2061,
      "step": 60
    },
    {
      "epoch": 0.005045961679643474,
      "grad_norm": 2.7191247940063477,
      "learning_rate": 9.201619983338154e-05,
      "loss": 1.3142,
      "step": 61
    },
    {
      "epoch": 0.0051286823629163175,
      "grad_norm": 2.8408358097076416,
      "learning_rate": 9.0227663542374e-05,
      "loss": 1.3354,
      "step": 62
    },
    {
      "epoch": 0.005211403046189162,
      "grad_norm": 2.20768666267395,
      "learning_rate": 8.827973030098448e-05,
      "loss": 1.3382,
      "step": 63
    },
    {
      "epoch": 0.005294123729462005,
      "grad_norm": 2.572021007537842,
      "learning_rate": 8.618011854451056e-05,
      "loss": 1.1975,
      "step": 64
    },
    {
      "epoch": 0.005376844412734849,
      "grad_norm": 2.387615203857422,
      "learning_rate": 8.39371477148859e-05,
      "loss": 1.3162,
      "step": 65
    },
    {
      "epoch": 0.005459565096007693,
      "grad_norm": 2.2671873569488525,
      "learning_rate": 8.155970529596007e-05,
      "loss": 1.3249,
      "step": 66
    },
    {
      "epoch": 0.005542285779280537,
      "grad_norm": 2.1475541591644287,
      "learning_rate": 7.905721159798513e-05,
      "loss": 1.2075,
      "step": 67
    },
    {
      "epoch": 0.005625006462553381,
      "grad_norm": 2.0540103912353516,
      "learning_rate": 7.64395824308462e-05,
      "loss": 1.2061,
      "step": 68
    },
    {
      "epoch": 0.0057077271458262245,
      "grad_norm": 2.283553123474121,
      "learning_rate": 7.371718981393815e-05,
      "loss": 1.2781,
      "step": 69
    },
    {
      "epoch": 0.005790447829099069,
      "grad_norm": 2.343355655670166,
      "learning_rate": 7.090082087837091e-05,
      "loss": 1.0859,
      "step": 70
    },
    {
      "epoch": 0.005873168512371912,
      "grad_norm": 2.1701223850250244,
      "learning_rate": 6.80016351243478e-05,
      "loss": 1.1756,
      "step": 71
    },
    {
      "epoch": 0.005955889195644756,
      "grad_norm": 2.408067464828491,
      "learning_rate": 6.503112020307916e-05,
      "loss": 1.1072,
      "step": 72
    },
    {
      "epoch": 0.0060386098789176,
      "grad_norm": 2.0953032970428467,
      "learning_rate": 6.200104639843982e-05,
      "loss": 1.1853,
      "step": 73
    },
    {
      "epoch": 0.006121330562190444,
      "grad_norm": 2.4414994716644287,
      "learning_rate": 5.8923419988730864e-05,
      "loss": 1.2798,
      "step": 74
    },
    {
      "epoch": 0.006204051245463287,
      "grad_norm": 2.2131426334381104,
      "learning_rate": 5.5810435673343806e-05,
      "loss": 0.9928,
      "step": 75
    },
    {
      "epoch": 0.006286771928736131,
      "grad_norm": 2.310889720916748,
      "learning_rate": 5.267442825283045e-05,
      "loss": 0.9607,
      "step": 76
    },
    {
      "epoch": 0.006369492612008975,
      "grad_norm": 2.5549275875091553,
      "learning_rate": 4.95278237538398e-05,
      "loss": 1.1821,
      "step": 77
    },
    {
      "epoch": 0.006452213295281819,
      "grad_norm": 2.448997974395752,
      "learning_rate": 4.638309019258337e-05,
      "loss": 1.0145,
      "step": 78
    },
    {
      "epoch": 0.006534933978554663,
      "grad_norm": 2.943434953689575,
      "learning_rate": 4.32526881719222e-05,
      "loss": 1.0639,
      "step": 79
    },
    {
      "epoch": 0.006617654661827507,
      "grad_norm": 2.867375373840332,
      "learning_rate": 4.0149021507828265e-05,
      "loss": 1.2538,
      "step": 80
    },
    {
      "epoch": 0.006700375345100351,
      "grad_norm": 2.579162836074829,
      "learning_rate": 3.708438808085668e-05,
      "loss": 1.0396,
      "step": 81
    },
    {
      "epoch": 0.006783096028373194,
      "grad_norm": 3.0243701934814453,
      "learning_rate": 3.407093110737371e-05,
      "loss": 1.0774,
      "step": 82
    },
    {
      "epoch": 0.006865816711646038,
      "grad_norm": 2.844428300857544,
      "learning_rate": 3.112059102362092e-05,
      "loss": 1.1524,
      "step": 83
    },
    {
      "epoch": 0.006948537394918882,
      "grad_norm": 2.923142910003662,
      "learning_rate": 2.8245058173270622e-05,
      "loss": 1.0971,
      "step": 84
    },
    {
      "epoch": 0.007031258078191726,
      "grad_norm": 2.740069627761841,
      "learning_rate": 2.5455726485939992e-05,
      "loss": 0.9025,
      "step": 85
    },
    {
      "epoch": 0.007113978761464569,
      "grad_norm": 2.2679755687713623,
      "learning_rate": 2.2763648330208688e-05,
      "loss": 1.0079,
      "step": 86
    },
    {
      "epoch": 0.0071966994447374135,
      "grad_norm": 2.9116687774658203,
      "learning_rate": 2.0179490720027355e-05,
      "loss": 1.1112,
      "step": 87
    },
    {
      "epoch": 0.007279420128010258,
      "grad_norm": 3.0605356693267822,
      "learning_rate": 1.7713493048045294e-05,
      "loss": 0.8803,
      "step": 88
    },
    {
      "epoch": 0.007362140811283101,
      "grad_norm": 2.7812325954437256,
      "learning_rate": 1.5375426513331636e-05,
      "loss": 0.9146,
      "step": 89
    },
    {
      "epoch": 0.007444861494555945,
      "grad_norm": 2.5753121376037598,
      "learning_rate": 1.317455540425439e-05,
      "loss": 0.8648,
      "step": 90
    },
    {
      "epoch": 0.007527582177828789,
      "grad_norm": 2.8104820251464844,
      "learning_rate": 1.111960038992717e-05,
      "loss": 0.8141,
      "step": 91
    },
    {
      "epoch": 0.007610302861101633,
      "grad_norm": 2.6657049655914307,
      "learning_rate": 9.218703965678204e-06,
      "loss": 0.8739,
      "step": 92
    },
    {
      "epoch": 0.007693023544374476,
      "grad_norm": 2.970731496810913,
      "learning_rate": 7.4793981894580255e-06,
      "loss": 0.9819,
      "step": 93
    },
    {
      "epoch": 0.0077757442276473205,
      "grad_norm": 2.904897928237915,
      "learning_rate": 5.90857483702732e-06,
      "loss": 0.8088,
      "step": 94
    },
    {
      "epoch": 0.007858464910920165,
      "grad_norm": 2.650369644165039,
      "learning_rate": 4.5124580941806215e-06,
      "loss": 0.6195,
      "step": 95
    },
    {
      "epoch": 0.007941185594193007,
      "grad_norm": 4.781639099121094,
      "learning_rate": 3.296579894209356e-06,
      "loss": 0.8532,
      "step": 96
    },
    {
      "epoch": 0.008023906277465851,
      "grad_norm": 2.783912420272827,
      "learning_rate": 2.265757998326712e-06,
      "loss": 0.9055,
      "step": 97
    },
    {
      "epoch": 0.008106626960738696,
      "grad_norm": 3.131596088409424,
      "learning_rate": 1.42407690590754e-06,
      "loss": 0.7204,
      "step": 98
    },
    {
      "epoch": 0.00818934764401154,
      "grad_norm": 3.6918785572052,
      "learning_rate": 7.748716701841685e-07,
      "loss": 0.7299,
      "step": 99
    },
    {
      "epoch": 0.008272068327284384,
      "grad_norm": 4.611267566680908,
      "learning_rate": 3.207146835262742e-07,
      "loss": 0.794,
      "step": 100
    },
    {
      "epoch": 0.008354789010557227,
      "grad_norm": 2.771995782852173,
      "learning_rate": 6.340548466648443e-08,
      "loss": 1.397,
      "step": 101
    },
    {
      "epoch": 0.00843750969383007,
      "grad_norm": 2.3782052993774414,
      "learning_rate": 9.999603637174071e-05,
      "loss": 1.423,
      "step": 102
    },
    {
      "epoch": 0.008520230377102915,
      "grad_norm": 2.6509506702423096,
      "learning_rate": 9.985737535497337e-05,
      "loss": 1.3263,
      "step": 103
    },
    {
      "epoch": 0.00860295106037576,
      "grad_norm": 2.2445216178894043,
      "learning_rate": 9.952116089150232e-05,
      "loss": 1.2381,
      "step": 104
    },
    {
      "epoch": 0.008685671743648602,
      "grad_norm": 2.2412264347076416,
      "learning_rate": 9.898872518795932e-05,
      "loss": 1.2007,
      "step": 105
    },
    {
      "epoch": 0.008768392426921446,
      "grad_norm": 1.9940555095672607,
      "learning_rate": 9.82621779524394e-05,
      "loss": 1.2681,
      "step": 106
    },
    {
      "epoch": 0.00885111311019429,
      "grad_norm": 2.5410356521606445,
      "learning_rate": 9.734439803505347e-05,
      "loss": 1.509,
      "step": 107
    },
    {
      "epoch": 0.008933833793467134,
      "grad_norm": 2.2012619972229004,
      "learning_rate": 9.623902202085444e-05,
      "loss": 1.2496,
      "step": 108
    },
    {
      "epoch": 0.009016554476739979,
      "grad_norm": 2.2244791984558105,
      "learning_rate": 9.495042982033609e-05,
      "loss": 1.4243,
      "step": 109
    },
    {
      "epoch": 0.009099275160012821,
      "grad_norm": 2.669168710708618,
      "learning_rate": 9.348372731460023e-05,
      "loss": 1.246,
      "step": 110
    },
    {
      "epoch": 0.009181995843285665,
      "grad_norm": 2.6673929691314697,
      "learning_rate": 9.184472612395842e-05,
      "loss": 1.2872,
      "step": 111
    },
    {
      "epoch": 0.00926471652655851,
      "grad_norm": 2.4134464263916016,
      "learning_rate": 9.003992058013302e-05,
      "loss": 1.3421,
      "step": 112
    },
    {
      "epoch": 0.009347437209831354,
      "grad_norm": 2.201627731323242,
      "learning_rate": 8.807646199330182e-05,
      "loss": 1.1109,
      "step": 113
    },
    {
      "epoch": 0.009430157893104196,
      "grad_norm": 2.15411114692688,
      "learning_rate": 8.596213031594991e-05,
      "loss": 1.1253,
      "step": 114
    },
    {
      "epoch": 0.00951287857637704,
      "grad_norm": 2.007629156112671,
      "learning_rate": 8.370530331580688e-05,
      "loss": 1.1167,
      "step": 115
    },
    {
      "epoch": 0.009595599259649885,
      "grad_norm": 2.1921730041503906,
      "learning_rate": 8.131492338001839e-05,
      "loss": 1.1805,
      "step": 116
    },
    {
      "epoch": 0.009678319942922729,
      "grad_norm": 2.222010850906372,
      "learning_rate": 7.880046208208559e-05,
      "loss": 1.3211,
      "step": 117
    },
    {
      "epoch": 0.009761040626195573,
      "grad_norm": 2.0497751235961914,
      "learning_rate": 7.617188265197146e-05,
      "loss": 1.0985,
      "step": 118
    },
    {
      "epoch": 0.009843761309468416,
      "grad_norm": 2.3346052169799805,
      "learning_rate": 7.343960049808156e-05,
      "loss": 1.2575,
      "step": 119
    },
    {
      "epoch": 0.00992648199274126,
      "grad_norm": 2.614447593688965,
      "learning_rate": 7.0614441937546e-05,
      "loss": 1.2215,
      "step": 120
    },
    {
      "epoch": 0.010009202676014104,
      "grad_norm": 2.486799716949463,
      "learning_rate": 6.770760129832811e-05,
      "loss": 1.1584,
      "step": 121
    },
    {
      "epoch": 0.010091923359286948,
      "grad_norm": 2.520561456680298,
      "learning_rate": 6.473059656313786e-05,
      "loss": 1.3498,
      "step": 122
    },
    {
      "epoch": 0.01017464404255979,
      "grad_norm": 2.3904736042022705,
      "learning_rate": 6.169522373090412e-05,
      "loss": 1.2706,
      "step": 123
    },
    {
      "epoch": 0.010257364725832635,
      "grad_norm": 2.8503761291503906,
      "learning_rate": 5.8613510076644384e-05,
      "loss": 0.9887,
      "step": 124
    },
    {
      "epoch": 0.01034008540910548,
      "grad_norm": 2.2997007369995117,
      "learning_rate": 5.5497666494931654e-05,
      "loss": 0.9693,
      "step": 125
    },
    {
      "epoch": 0.010422806092378323,
      "grad_norm": 2.5005884170532227,
      "learning_rate": 5.236003911579347e-05,
      "loss": 1.0824,
      "step": 126
    },
    {
      "epoch": 0.010505526775651168,
      "grad_norm": 2.3971927165985107,
      "learning_rate": 4.921306038475677e-05,
      "loss": 0.9908,
      "step": 127
    },
    {
      "epoch": 0.01058824745892401,
      "grad_norm": 4.0476813316345215,
      "learning_rate": 4.606919980087942e-05,
      "loss": 0.9239,
      "step": 128
    },
    {
      "epoch": 0.010670968142196854,
      "grad_norm": 2.59497332572937,
      "learning_rate": 4.294091450796183e-05,
      "loss": 1.0633,
      "step": 129
    },
    {
      "epoch": 0.010753688825469699,
      "grad_norm": 2.439748764038086,
      "learning_rate": 3.984059993471401e-05,
      "loss": 1.0287,
      "step": 130
    },
    {
      "epoch": 0.010836409508742543,
      "grad_norm": 2.7451720237731934,
      "learning_rate": 3.678054067946184e-05,
      "loss": 1.1215,
      "step": 131
    },
    {
      "epoch": 0.010919130192015385,
      "grad_norm": 2.973689317703247,
      "learning_rate": 3.3772861834003287e-05,
      "loss": 1.0658,
      "step": 132
    },
    {
      "epoch": 0.01100185087528823,
      "grad_norm": 2.825209379196167,
      "learning_rate": 3.0829480939489994e-05,
      "loss": 0.9965,
      "step": 133
    },
    {
      "epoch": 0.011084571558561074,
      "grad_norm": 2.522503137588501,
      "learning_rate": 2.796206076470044e-05,
      "loss": 1.0821,
      "step": 134
    },
    {
      "epoch": 0.011167292241833918,
      "grad_norm": 2.9113543033599854,
      "learning_rate": 2.5181963093817003e-05,
      "loss": 0.9131,
      "step": 135
    },
    {
      "epoch": 0.011250012925106762,
      "grad_norm": 2.325521945953369,
      "learning_rate": 2.2500203706814856e-05,
      "loss": 0.9088,
      "step": 136
    },
    {
      "epoch": 0.011332733608379605,
      "grad_norm": 2.287277936935425,
      "learning_rate": 1.992740873084903e-05,
      "loss": 0.8602,
      "step": 137
    },
    {
      "epoch": 0.011415454291652449,
      "grad_norm": 3.2943623065948486,
      "learning_rate": 1.7473772535589804e-05,
      "loss": 1.0394,
      "step": 138
    },
    {
      "epoch": 0.011498174974925293,
      "grad_norm": 2.971130132675171,
      "learning_rate": 1.5149017339342591e-05,
      "loss": 0.9484,
      "step": 139
    },
    {
      "epoch": 0.011580895658198137,
      "grad_norm": 2.681934118270874,
      "learning_rate": 1.2962354686006084e-05,
      "loss": 0.7826,
      "step": 140
    },
    {
      "epoch": 0.01166361634147098,
      "grad_norm": 2.515597105026245,
      "learning_rate": 1.0922448945512998e-05,
      "loss": 0.86,
      "step": 141
    },
    {
      "epoch": 0.011746337024743824,
      "grad_norm": 2.818406105041504,
      "learning_rate": 9.03738298237657e-06,
      "loss": 0.735,
      "step": 142
    },
    {
      "epoch": 0.011829057708016668,
      "grad_norm": 2.4099717140197754,
      "learning_rate": 7.3146261283784104e-06,
      "loss": 0.7519,
      "step": 143
    },
    {
      "epoch": 0.011911778391289513,
      "grad_norm": 2.9326817989349365,
      "learning_rate": 5.761004586300211e-06,
      "loss": 0.6674,
      "step": 144
    },
    {
      "epoch": 0.011994499074562355,
      "grad_norm": 2.74967885017395,
      "learning_rate": 4.382674381972224e-06,
      "loss": 0.7392,
      "step": 145
    },
    {
      "epoch": 0.0120772197578352,
      "grad_norm": 3.021615982055664,
      "learning_rate": 3.1850969718112576e-06,
      "loss": 0.6447,
      "step": 146
    },
    {
      "epoch": 0.012159940441108043,
      "grad_norm": 3.6869380474090576,
      "learning_rate": 2.1730176025012816e-06,
      "loss": 0.7046,
      "step": 147
    },
    {
      "epoch": 0.012242661124380888,
      "grad_norm": 4.108006954193115,
      "learning_rate": 1.3504465085626528e-06,
      "loss": 0.6697,
      "step": 148
    },
    {
      "epoch": 0.012325381807653732,
      "grad_norm": 2.948345422744751,
      "learning_rate": 7.206430223130223e-07,
      "loss": 0.6251,
      "step": 149
    },
    {
      "epoch": 0.012408102490926574,
      "grad_norm": 2.8484694957733154,
      "learning_rate": 2.861026591815086e-07,
      "loss": 0.5892,
      "step": 150
    }
  ],
  "logging_steps": 1,
  "max_steps": 500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8683172536246272.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
