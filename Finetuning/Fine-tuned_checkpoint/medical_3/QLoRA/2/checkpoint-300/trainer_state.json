{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.02481620498185315,
  "eval_steps": 500,
  "global_step": 300,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 8.272068327284383e-05,
      "grad_norm": 1.5043562650680542,
      "learning_rate": 0.0001,
      "loss": 1.9595,
      "step": 1
    },
    {
      "epoch": 0.00016544136654568766,
      "grad_norm": 1.8306677341461182,
      "learning_rate": 9.990094071072877e-05,
      "loss": 2.3019,
      "step": 2
    },
    {
      "epoch": 0.0002481620498185315,
      "grad_norm": 1.7613961696624756,
      "learning_rate": 9.960415535262671e-05,
      "loss": 2.2783,
      "step": 3
    },
    {
      "epoch": 0.0003308827330913753,
      "grad_norm": 1.9991271495819092,
      "learning_rate": 9.91108198995594e-05,
      "loss": 2.0563,
      "step": 4
    },
    {
      "epoch": 0.00041360341636421916,
      "grad_norm": NaN,
      "learning_rate": 9.91108198995594e-05,
      "loss": 2.2947,
      "step": 5
    },
    {
      "epoch": 0.000496324099637063,
      "grad_norm": 2.020979166030884,
      "learning_rate": 9.842288912990096e-05,
      "loss": 2.2119,
      "step": 6
    },
    {
      "epoch": 0.0005790447829099068,
      "grad_norm": 2.107518434524536,
      "learning_rate": 9.754308888097583e-05,
      "loss": 2.0839,
      "step": 7
    },
    {
      "epoch": 0.0006617654661827506,
      "grad_norm": 2.0635111331939697,
      "learning_rate": 9.647490524827834e-05,
      "loss": 2.0968,
      "step": 8
    },
    {
      "epoch": 0.0007444861494555945,
      "grad_norm": 2.1375210285186768,
      "learning_rate": 9.522257077226717e-05,
      "loss": 2.1344,
      "step": 9
    },
    {
      "epoch": 0.0008272068327284383,
      "grad_norm": 2.2170939445495605,
      "learning_rate": 9.379104766746722e-05,
      "loss": 1.7956,
      "step": 10
    },
    {
      "epoch": 0.0009099275160012822,
      "grad_norm": 2.70408034324646,
      "learning_rate": 9.2186008160332e-05,
      "loss": 1.772,
      "step": 11
    },
    {
      "epoch": 0.000992648199274126,
      "grad_norm": 2.7873737812042236,
      "learning_rate": 9.041381201377468e-05,
      "loss": 1.84,
      "step": 12
    },
    {
      "epoch": 0.00107536888254697,
      "grad_norm": 4.790360927581787,
      "learning_rate": 8.848148132742431e-05,
      "loss": 1.8058,
      "step": 13
    },
    {
      "epoch": 0.0011580895658198137,
      "grad_norm": 3.945890426635742,
      "learning_rate": 8.6396672713458e-05,
      "loss": 1.7737,
      "step": 14
    },
    {
      "epoch": 0.0012408102490926575,
      "grad_norm": 2.960235595703125,
      "learning_rate": 8.416764695825834e-05,
      "loss": 1.543,
      "step": 15
    },
    {
      "epoch": 0.0013235309323655013,
      "grad_norm": 3.493924856185913,
      "learning_rate": 8.180323629010848e-05,
      "loss": 1.3756,
      "step": 16
    },
    {
      "epoch": 0.0014062516156383453,
      "grad_norm": 5.751547336578369,
      "learning_rate": 7.93128093826217e-05,
      "loss": 1.4768,
      "step": 17
    },
    {
      "epoch": 0.001488972298911189,
      "grad_norm": 3.678823471069336,
      "learning_rate": 7.670623423257548e-05,
      "loss": 1.5063,
      "step": 18
    },
    {
      "epoch": 0.0015716929821840329,
      "grad_norm": 2.7539215087890625,
      "learning_rate": 7.399383905924165e-05,
      "loss": 1.4013,
      "step": 19
    },
    {
      "epoch": 0.0016544136654568766,
      "grad_norm": 2.9167375564575195,
      "learning_rate": 7.118637138014396e-05,
      "loss": 1.3577,
      "step": 20
    },
    {
      "epoch": 0.0017371343487297204,
      "grad_norm": 2.256544589996338,
      "learning_rate": 6.829495542540013e-05,
      "loss": 1.2725,
      "step": 21
    },
    {
      "epoch": 0.0018198550320025644,
      "grad_norm": 2.7758729457855225,
      "learning_rate": 6.533104805938873e-05,
      "loss": 1.3249,
      "step": 22
    },
    {
      "epoch": 0.0019025757152754082,
      "grad_norm": 3.7425026893615723,
      "learning_rate": 6.230639338439549e-05,
      "loss": 1.2934,
      "step": 23
    },
    {
      "epoch": 0.001985296398548252,
      "grad_norm": 5.221314907073975,
      "learning_rate": 5.923297620611623e-05,
      "loss": 1.1909,
      "step": 24
    },
    {
      "epoch": 0.002068017081821096,
      "grad_norm": 2.2888824939727783,
      "learning_rate": 5.6122974545403525e-05,
      "loss": 1.2838,
      "step": 25
    },
    {
      "epoch": 0.00215073776509394,
      "grad_norm": 2.5191752910614014,
      "learning_rate": 5.298871138442307e-05,
      "loss": 1.1959,
      "step": 26
    },
    {
      "epoch": 0.0022334584483667836,
      "grad_norm": 2.24515962600708,
      "learning_rate": 4.984260583841951e-05,
      "loss": 1.1333,
      "step": 27
    },
    {
      "epoch": 0.0023161791316396274,
      "grad_norm": 2.2987701892852783,
      "learning_rate": 4.6697123946567227e-05,
      "loss": 1.1369,
      "step": 28
    },
    {
      "epoch": 0.002398899814912471,
      "grad_norm": 3.235107660293579,
      "learning_rate": 4.356472927689109e-05,
      "loss": 1.0308,
      "step": 29
    },
    {
      "epoch": 0.002481620498185315,
      "grad_norm": 3.092144250869751,
      "learning_rate": 4.045783354097893e-05,
      "loss": 1.1718,
      "step": 30
    },
    {
      "epoch": 0.0025643411814581588,
      "grad_norm": 2.702939510345459,
      "learning_rate": 3.73887474141683e-05,
      "loss": 0.9858,
      "step": 31
    },
    {
      "epoch": 0.0026470618647310025,
      "grad_norm": 3.1886661052703857,
      "learning_rate": 3.436963175607658e-05,
      "loss": 1.0431,
      "step": 32
    },
    {
      "epoch": 0.0027297825480038463,
      "grad_norm": 2.6861298084259033,
      "learning_rate": 3.1412449424756474e-05,
      "loss": 1.2254,
      "step": 33
    },
    {
      "epoch": 0.0028125032312766906,
      "grad_norm": 2.322610855102539,
      "learning_rate": 2.8528917875407433e-05,
      "loss": 1.2049,
      "step": 34
    },
    {
      "epoch": 0.0028952239145495343,
      "grad_norm": 3.3600270748138428,
      "learning_rate": 2.5730462731464273e-05,
      "loss": 1.0007,
      "step": 35
    },
    {
      "epoch": 0.002977944597822378,
      "grad_norm": 3.2248644828796387,
      "learning_rate": 2.3028172512031604e-05,
      "loss": 1.0582,
      "step": 36
    },
    {
      "epoch": 0.003060665281095222,
      "grad_norm": 2.5334842205047607,
      "learning_rate": 2.0432754695051136e-05,
      "loss": 0.9331,
      "step": 37
    },
    {
      "epoch": 0.0031433859643680657,
      "grad_norm": 2.458400249481201,
      "learning_rate": 1.795449329029531e-05,
      "loss": 0.9865,
      "step": 38
    },
    {
      "epoch": 0.0032261066476409095,
      "grad_norm": 2.4662415981292725,
      "learning_rate": 1.560320809029948e-05,
      "loss": 0.9434,
      "step": 39
    },
    {
      "epoch": 0.0033088273309137533,
      "grad_norm": 2.526129722595215,
      "learning_rate": 1.3388215760695083e-05,
      "loss": 1.0176,
      "step": 40
    },
    {
      "epoch": 0.003391548014186597,
      "grad_norm": 2.6398019790649414,
      "learning_rate": 1.1318292924118601e-05,
      "loss": 0.8116,
      "step": 41
    },
    {
      "epoch": 0.003474268697459441,
      "grad_norm": 3.184659481048584,
      "learning_rate": 9.401641383971477e-06,
      "loss": 0.8309,
      "step": 42
    },
    {
      "epoch": 0.0035569893807322847,
      "grad_norm": 3.0133354663848877,
      "learning_rate": 7.64585562582767e-06,
      "loss": 0.8475,
      "step": 43
    },
    {
      "epoch": 0.003639710064005129,
      "grad_norm": 2.968069314956665,
      "learning_rate": 6.057892725259717e-06,
      "loss": 0.7816,
      "step": 44
    },
    {
      "epoch": 0.0037224307472779727,
      "grad_norm": 2.7015604972839355,
      "learning_rate": 4.644044781320422e-06,
      "loss": 0.7451,
      "step": 45
    },
    {
      "epoch": 0.0038051514305508165,
      "grad_norm": 2.995434284210205,
      "learning_rate": 3.4099139849083307e-06,
      "loss": 0.8383,
      "step": 46
    },
    {
      "epoch": 0.0038878721138236602,
      "grad_norm": 3.2848455905914307,
      "learning_rate": 2.360390420805869e-06,
      "loss": 0.7847,
      "step": 47
    },
    {
      "epoch": 0.003970592797096504,
      "grad_norm": 3.653118133544922,
      "learning_rate": 1.499632691346381e-06,
      "loss": 0.84,
      "step": 48
    },
    {
      "epoch": 0.004053313480369348,
      "grad_norm": 3.5103344917297363,
      "learning_rate": 8.31051438486441e-07,
      "loss": 0.8116,
      "step": 49
    },
    {
      "epoch": 0.004136034163642192,
      "grad_norm": 4.318592071533203,
      "learning_rate": 3.572958295752049e-07,
      "loss": 0.8914,
      "step": 50
    },
    {
      "epoch": 0.004218754846915035,
      "grad_norm": 2.5561983585357666,
      "learning_rate": 8.02430603689397e-08,
      "loss": 1.4523,
      "step": 51
    },
    {
      "epoch": 0.00430147553018788,
      "grad_norm": 2.3827123641967773,
      "learning_rate": 9.999900908311602e-05,
      "loss": 1.2889,
      "step": 52
    },
    {
      "epoch": 0.004384196213460723,
      "grad_norm": 2.10959529876709,
      "learning_rate": 9.988014657443941e-05,
      "loss": 1.381,
      "step": 53
    },
    {
      "epoch": 0.004466916896733567,
      "grad_norm": 2.2905192375183105,
      "learning_rate": 9.956364039102642e-05,
      "loss": 1.2601,
      "step": 54
    },
    {
      "epoch": 0.0045496375800064106,
      "grad_norm": 2.2840139865875244,
      "learning_rate": 9.905074464798024e-05,
      "loss": 1.362,
      "step": 55
    },
    {
      "epoch": 0.004632358263279255,
      "grad_norm": 2.5473103523254395,
      "learning_rate": 9.83434916288119e-05,
      "loss": 1.4671,
      "step": 56
    },
    {
      "epoch": 0.004715078946552098,
      "grad_norm": 2.331167697906494,
      "learning_rate": 9.744468373277797e-05,
      "loss": 1.3009,
      "step": 57
    },
    {
      "epoch": 0.004797799629824942,
      "grad_norm": 2.356964111328125,
      "learning_rate": 9.635788237073334e-05,
      "loss": 1.3004,
      "step": 58
    },
    {
      "epoch": 0.004880520313097787,
      "grad_norm": 1.935747504234314,
      "learning_rate": 9.508739385349812e-05,
      "loss": 1.279,
      "step": 59
    },
    {
      "epoch": 0.00496324099637063,
      "grad_norm": 2.097308397293091,
      "learning_rate": 9.363825232865413e-05,
      "loss": 1.2061,
      "step": 60
    },
    {
      "epoch": 0.005045961679643474,
      "grad_norm": 2.7191247940063477,
      "learning_rate": 9.201619983338154e-05,
      "loss": 1.3142,
      "step": 61
    },
    {
      "epoch": 0.0051286823629163175,
      "grad_norm": 2.8408358097076416,
      "learning_rate": 9.0227663542374e-05,
      "loss": 1.3354,
      "step": 62
    },
    {
      "epoch": 0.005211403046189162,
      "grad_norm": 2.20768666267395,
      "learning_rate": 8.827973030098448e-05,
      "loss": 1.3382,
      "step": 63
    },
    {
      "epoch": 0.005294123729462005,
      "grad_norm": 2.572021007537842,
      "learning_rate": 8.618011854451056e-05,
      "loss": 1.1975,
      "step": 64
    },
    {
      "epoch": 0.005376844412734849,
      "grad_norm": 2.387615203857422,
      "learning_rate": 8.39371477148859e-05,
      "loss": 1.3162,
      "step": 65
    },
    {
      "epoch": 0.005459565096007693,
      "grad_norm": 2.2671873569488525,
      "learning_rate": 8.155970529596007e-05,
      "loss": 1.3249,
      "step": 66
    },
    {
      "epoch": 0.005542285779280537,
      "grad_norm": 2.1475541591644287,
      "learning_rate": 7.905721159798513e-05,
      "loss": 1.2075,
      "step": 67
    },
    {
      "epoch": 0.005625006462553381,
      "grad_norm": 2.0540103912353516,
      "learning_rate": 7.64395824308462e-05,
      "loss": 1.2061,
      "step": 68
    },
    {
      "epoch": 0.0057077271458262245,
      "grad_norm": 2.283553123474121,
      "learning_rate": 7.371718981393815e-05,
      "loss": 1.2781,
      "step": 69
    },
    {
      "epoch": 0.005790447829099069,
      "grad_norm": 2.343355655670166,
      "learning_rate": 7.090082087837091e-05,
      "loss": 1.0859,
      "step": 70
    },
    {
      "epoch": 0.005873168512371912,
      "grad_norm": 2.1701223850250244,
      "learning_rate": 6.80016351243478e-05,
      "loss": 1.1756,
      "step": 71
    },
    {
      "epoch": 0.005955889195644756,
      "grad_norm": 2.408067464828491,
      "learning_rate": 6.503112020307916e-05,
      "loss": 1.1072,
      "step": 72
    },
    {
      "epoch": 0.0060386098789176,
      "grad_norm": 2.0953032970428467,
      "learning_rate": 6.200104639843982e-05,
      "loss": 1.1853,
      "step": 73
    },
    {
      "epoch": 0.006121330562190444,
      "grad_norm": 2.4414994716644287,
      "learning_rate": 5.8923419988730864e-05,
      "loss": 1.2798,
      "step": 74
    },
    {
      "epoch": 0.006204051245463287,
      "grad_norm": 2.2131426334381104,
      "learning_rate": 5.5810435673343806e-05,
      "loss": 0.9928,
      "step": 75
    },
    {
      "epoch": 0.006286771928736131,
      "grad_norm": 2.310889720916748,
      "learning_rate": 5.267442825283045e-05,
      "loss": 0.9607,
      "step": 76
    },
    {
      "epoch": 0.006369492612008975,
      "grad_norm": 2.5549275875091553,
      "learning_rate": 4.95278237538398e-05,
      "loss": 1.1821,
      "step": 77
    },
    {
      "epoch": 0.006452213295281819,
      "grad_norm": 2.448997974395752,
      "learning_rate": 4.638309019258337e-05,
      "loss": 1.0145,
      "step": 78
    },
    {
      "epoch": 0.006534933978554663,
      "grad_norm": 2.943434953689575,
      "learning_rate": 4.32526881719222e-05,
      "loss": 1.0639,
      "step": 79
    },
    {
      "epoch": 0.006617654661827507,
      "grad_norm": 2.867375373840332,
      "learning_rate": 4.0149021507828265e-05,
      "loss": 1.2538,
      "step": 80
    },
    {
      "epoch": 0.006700375345100351,
      "grad_norm": 2.579162836074829,
      "learning_rate": 3.708438808085668e-05,
      "loss": 1.0396,
      "step": 81
    },
    {
      "epoch": 0.006783096028373194,
      "grad_norm": 3.0243701934814453,
      "learning_rate": 3.407093110737371e-05,
      "loss": 1.0774,
      "step": 82
    },
    {
      "epoch": 0.006865816711646038,
      "grad_norm": 2.844428300857544,
      "learning_rate": 3.112059102362092e-05,
      "loss": 1.1524,
      "step": 83
    },
    {
      "epoch": 0.006948537394918882,
      "grad_norm": 2.923142910003662,
      "learning_rate": 2.8245058173270622e-05,
      "loss": 1.0971,
      "step": 84
    },
    {
      "epoch": 0.007031258078191726,
      "grad_norm": 2.740069627761841,
      "learning_rate": 2.5455726485939992e-05,
      "loss": 0.9025,
      "step": 85
    },
    {
      "epoch": 0.007113978761464569,
      "grad_norm": 2.2679755687713623,
      "learning_rate": 2.2763648330208688e-05,
      "loss": 1.0079,
      "step": 86
    },
    {
      "epoch": 0.0071966994447374135,
      "grad_norm": 2.9116687774658203,
      "learning_rate": 2.0179490720027355e-05,
      "loss": 1.1112,
      "step": 87
    },
    {
      "epoch": 0.007279420128010258,
      "grad_norm": 3.0605356693267822,
      "learning_rate": 1.7713493048045294e-05,
      "loss": 0.8803,
      "step": 88
    },
    {
      "epoch": 0.007362140811283101,
      "grad_norm": 2.7812325954437256,
      "learning_rate": 1.5375426513331636e-05,
      "loss": 0.9146,
      "step": 89
    },
    {
      "epoch": 0.007444861494555945,
      "grad_norm": 2.5753121376037598,
      "learning_rate": 1.317455540425439e-05,
      "loss": 0.8648,
      "step": 90
    },
    {
      "epoch": 0.007527582177828789,
      "grad_norm": 2.8104820251464844,
      "learning_rate": 1.111960038992717e-05,
      "loss": 0.8141,
      "step": 91
    },
    {
      "epoch": 0.007610302861101633,
      "grad_norm": 2.6657049655914307,
      "learning_rate": 9.218703965678204e-06,
      "loss": 0.8739,
      "step": 92
    },
    {
      "epoch": 0.007693023544374476,
      "grad_norm": 2.970731496810913,
      "learning_rate": 7.4793981894580255e-06,
      "loss": 0.9819,
      "step": 93
    },
    {
      "epoch": 0.0077757442276473205,
      "grad_norm": 2.904897928237915,
      "learning_rate": 5.90857483702732e-06,
      "loss": 0.8088,
      "step": 94
    },
    {
      "epoch": 0.007858464910920165,
      "grad_norm": 2.650369644165039,
      "learning_rate": 4.5124580941806215e-06,
      "loss": 0.6195,
      "step": 95
    },
    {
      "epoch": 0.007941185594193007,
      "grad_norm": 4.781639099121094,
      "learning_rate": 3.296579894209356e-06,
      "loss": 0.8532,
      "step": 96
    },
    {
      "epoch": 0.008023906277465851,
      "grad_norm": 2.783912420272827,
      "learning_rate": 2.265757998326712e-06,
      "loss": 0.9055,
      "step": 97
    },
    {
      "epoch": 0.008106626960738696,
      "grad_norm": 3.131596088409424,
      "learning_rate": 1.42407690590754e-06,
      "loss": 0.7204,
      "step": 98
    },
    {
      "epoch": 0.00818934764401154,
      "grad_norm": 3.6918785572052,
      "learning_rate": 7.748716701841685e-07,
      "loss": 0.7299,
      "step": 99
    },
    {
      "epoch": 0.008272068327284384,
      "grad_norm": 4.611267566680908,
      "learning_rate": 3.207146835262742e-07,
      "loss": 0.794,
      "step": 100
    },
    {
      "epoch": 0.008354789010557227,
      "grad_norm": 2.771995782852173,
      "learning_rate": 6.340548466648443e-08,
      "loss": 1.397,
      "step": 101
    },
    {
      "epoch": 0.00843750969383007,
      "grad_norm": 2.3782052993774414,
      "learning_rate": 9.999603637174071e-05,
      "loss": 1.423,
      "step": 102
    },
    {
      "epoch": 0.008520230377102915,
      "grad_norm": 2.6509506702423096,
      "learning_rate": 9.985737535497337e-05,
      "loss": 1.3263,
      "step": 103
    },
    {
      "epoch": 0.00860295106037576,
      "grad_norm": 2.2445216178894043,
      "learning_rate": 9.952116089150232e-05,
      "loss": 1.2381,
      "step": 104
    },
    {
      "epoch": 0.008685671743648602,
      "grad_norm": 2.2412264347076416,
      "learning_rate": 9.898872518795932e-05,
      "loss": 1.2007,
      "step": 105
    },
    {
      "epoch": 0.008768392426921446,
      "grad_norm": 1.9940555095672607,
      "learning_rate": 9.82621779524394e-05,
      "loss": 1.2681,
      "step": 106
    },
    {
      "epoch": 0.00885111311019429,
      "grad_norm": 2.5410356521606445,
      "learning_rate": 9.734439803505347e-05,
      "loss": 1.509,
      "step": 107
    },
    {
      "epoch": 0.008933833793467134,
      "grad_norm": 2.2012619972229004,
      "learning_rate": 9.623902202085444e-05,
      "loss": 1.2496,
      "step": 108
    },
    {
      "epoch": 0.009016554476739979,
      "grad_norm": 2.2244791984558105,
      "learning_rate": 9.495042982033609e-05,
      "loss": 1.4243,
      "step": 109
    },
    {
      "epoch": 0.009099275160012821,
      "grad_norm": 2.669168710708618,
      "learning_rate": 9.348372731460023e-05,
      "loss": 1.246,
      "step": 110
    },
    {
      "epoch": 0.009181995843285665,
      "grad_norm": 2.6673929691314697,
      "learning_rate": 9.184472612395842e-05,
      "loss": 1.2872,
      "step": 111
    },
    {
      "epoch": 0.00926471652655851,
      "grad_norm": 2.4134464263916016,
      "learning_rate": 9.003992058013302e-05,
      "loss": 1.3421,
      "step": 112
    },
    {
      "epoch": 0.009347437209831354,
      "grad_norm": 2.201627731323242,
      "learning_rate": 8.807646199330182e-05,
      "loss": 1.1109,
      "step": 113
    },
    {
      "epoch": 0.009430157893104196,
      "grad_norm": 2.15411114692688,
      "learning_rate": 8.596213031594991e-05,
      "loss": 1.1253,
      "step": 114
    },
    {
      "epoch": 0.00951287857637704,
      "grad_norm": 2.007629156112671,
      "learning_rate": 8.370530331580688e-05,
      "loss": 1.1167,
      "step": 115
    },
    {
      "epoch": 0.009595599259649885,
      "grad_norm": 2.1921730041503906,
      "learning_rate": 8.131492338001839e-05,
      "loss": 1.1805,
      "step": 116
    },
    {
      "epoch": 0.009678319942922729,
      "grad_norm": 2.222010850906372,
      "learning_rate": 7.880046208208559e-05,
      "loss": 1.3211,
      "step": 117
    },
    {
      "epoch": 0.009761040626195573,
      "grad_norm": 2.0497751235961914,
      "learning_rate": 7.617188265197146e-05,
      "loss": 1.0985,
      "step": 118
    },
    {
      "epoch": 0.009843761309468416,
      "grad_norm": 2.3346052169799805,
      "learning_rate": 7.343960049808156e-05,
      "loss": 1.2575,
      "step": 119
    },
    {
      "epoch": 0.00992648199274126,
      "grad_norm": 2.614447593688965,
      "learning_rate": 7.0614441937546e-05,
      "loss": 1.2215,
      "step": 120
    },
    {
      "epoch": 0.010009202676014104,
      "grad_norm": 2.486799716949463,
      "learning_rate": 6.770760129832811e-05,
      "loss": 1.1584,
      "step": 121
    },
    {
      "epoch": 0.010091923359286948,
      "grad_norm": 2.520561456680298,
      "learning_rate": 6.473059656313786e-05,
      "loss": 1.3498,
      "step": 122
    },
    {
      "epoch": 0.01017464404255979,
      "grad_norm": 2.3904736042022705,
      "learning_rate": 6.169522373090412e-05,
      "loss": 1.2706,
      "step": 123
    },
    {
      "epoch": 0.010257364725832635,
      "grad_norm": 2.8503761291503906,
      "learning_rate": 5.8613510076644384e-05,
      "loss": 0.9887,
      "step": 124
    },
    {
      "epoch": 0.01034008540910548,
      "grad_norm": 2.2997007369995117,
      "learning_rate": 5.5497666494931654e-05,
      "loss": 0.9693,
      "step": 125
    },
    {
      "epoch": 0.010422806092378323,
      "grad_norm": 2.5005884170532227,
      "learning_rate": 5.236003911579347e-05,
      "loss": 1.0824,
      "step": 126
    },
    {
      "epoch": 0.010505526775651168,
      "grad_norm": 2.3971927165985107,
      "learning_rate": 4.921306038475677e-05,
      "loss": 0.9908,
      "step": 127
    },
    {
      "epoch": 0.01058824745892401,
      "grad_norm": 4.0476813316345215,
      "learning_rate": 4.606919980087942e-05,
      "loss": 0.9239,
      "step": 128
    },
    {
      "epoch": 0.010670968142196854,
      "grad_norm": 2.59497332572937,
      "learning_rate": 4.294091450796183e-05,
      "loss": 1.0633,
      "step": 129
    },
    {
      "epoch": 0.010753688825469699,
      "grad_norm": 2.439748764038086,
      "learning_rate": 3.984059993471401e-05,
      "loss": 1.0287,
      "step": 130
    },
    {
      "epoch": 0.010836409508742543,
      "grad_norm": 2.7451720237731934,
      "learning_rate": 3.678054067946184e-05,
      "loss": 1.1215,
      "step": 131
    },
    {
      "epoch": 0.010919130192015385,
      "grad_norm": 2.973689317703247,
      "learning_rate": 3.3772861834003287e-05,
      "loss": 1.0658,
      "step": 132
    },
    {
      "epoch": 0.01100185087528823,
      "grad_norm": 2.825209379196167,
      "learning_rate": 3.0829480939489994e-05,
      "loss": 0.9965,
      "step": 133
    },
    {
      "epoch": 0.011084571558561074,
      "grad_norm": 2.522503137588501,
      "learning_rate": 2.796206076470044e-05,
      "loss": 1.0821,
      "step": 134
    },
    {
      "epoch": 0.011167292241833918,
      "grad_norm": 2.9113543033599854,
      "learning_rate": 2.5181963093817003e-05,
      "loss": 0.9131,
      "step": 135
    },
    {
      "epoch": 0.011250012925106762,
      "grad_norm": 2.325521945953369,
      "learning_rate": 2.2500203706814856e-05,
      "loss": 0.9088,
      "step": 136
    },
    {
      "epoch": 0.011332733608379605,
      "grad_norm": 2.287277936935425,
      "learning_rate": 1.992740873084903e-05,
      "loss": 0.8602,
      "step": 137
    },
    {
      "epoch": 0.011415454291652449,
      "grad_norm": 3.2943623065948486,
      "learning_rate": 1.7473772535589804e-05,
      "loss": 1.0394,
      "step": 138
    },
    {
      "epoch": 0.011498174974925293,
      "grad_norm": 2.971130132675171,
      "learning_rate": 1.5149017339342591e-05,
      "loss": 0.9484,
      "step": 139
    },
    {
      "epoch": 0.011580895658198137,
      "grad_norm": 2.681934118270874,
      "learning_rate": 1.2962354686006084e-05,
      "loss": 0.7826,
      "step": 140
    },
    {
      "epoch": 0.01166361634147098,
      "grad_norm": 2.515597105026245,
      "learning_rate": 1.0922448945512998e-05,
      "loss": 0.86,
      "step": 141
    },
    {
      "epoch": 0.011746337024743824,
      "grad_norm": 2.818406105041504,
      "learning_rate": 9.03738298237657e-06,
      "loss": 0.735,
      "step": 142
    },
    {
      "epoch": 0.011829057708016668,
      "grad_norm": 2.4099717140197754,
      "learning_rate": 7.3146261283784104e-06,
      "loss": 0.7519,
      "step": 143
    },
    {
      "epoch": 0.011911778391289513,
      "grad_norm": 2.9326817989349365,
      "learning_rate": 5.761004586300211e-06,
      "loss": 0.6674,
      "step": 144
    },
    {
      "epoch": 0.011994499074562355,
      "grad_norm": 2.74967885017395,
      "learning_rate": 4.382674381972224e-06,
      "loss": 0.7392,
      "step": 145
    },
    {
      "epoch": 0.0120772197578352,
      "grad_norm": 3.021615982055664,
      "learning_rate": 3.1850969718112576e-06,
      "loss": 0.6447,
      "step": 146
    },
    {
      "epoch": 0.012159940441108043,
      "grad_norm": 3.6869380474090576,
      "learning_rate": 2.1730176025012816e-06,
      "loss": 0.7046,
      "step": 147
    },
    {
      "epoch": 0.012242661124380888,
      "grad_norm": 4.108006954193115,
      "learning_rate": 1.3504465085626528e-06,
      "loss": 0.6697,
      "step": 148
    },
    {
      "epoch": 0.012325381807653732,
      "grad_norm": 2.948345422744751,
      "learning_rate": 7.206430223130223e-07,
      "loss": 0.6251,
      "step": 149
    },
    {
      "epoch": 0.012408102490926574,
      "grad_norm": 2.8484694957733154,
      "learning_rate": 2.861026591815086e-07,
      "loss": 0.5892,
      "step": 150
    },
    {
      "epoch": 0.012490823174199419,
      "grad_norm": 2.524045705795288,
      "learning_rate": 4.8547229549383844e-08,
      "loss": 1.3677,
      "step": 151
    },
    {
      "epoch": 0.012573543857472263,
      "grad_norm": 2.48502254486084,
      "learning_rate": 9.999108198370249e-05,
      "loss": 1.2659,
      "step": 152
    },
    {
      "epoch": 0.012656264540745107,
      "grad_norm": 2.2425994873046875,
      "learning_rate": 9.983262795490613e-05,
      "loss": 1.2521,
      "step": 153
    },
    {
      "epoch": 0.01273898522401795,
      "grad_norm": 2.202578544616699,
      "learning_rate": 9.947671853780054e-05,
      "loss": 1.1643,
      "step": 154
    },
    {
      "epoch": 0.012821705907290794,
      "grad_norm": 2.3758859634399414,
      "learning_rate": 9.892476397774186e-05,
      "loss": 1.2838,
      "step": 155
    },
    {
      "epoch": 0.012904426590563638,
      "grad_norm": 2.1414027214050293,
      "learning_rate": 9.817895132378725e-05,
      "loss": 1.2323,
      "step": 156
    },
    {
      "epoch": 0.012987147273836482,
      "grad_norm": 2.2603063583374023,
      "learning_rate": 9.724223576279395e-05,
      "loss": 1.2382,
      "step": 157
    },
    {
      "epoch": 0.013069867957109326,
      "grad_norm": 2.0892608165740967,
      "learning_rate": 9.611832890987079e-05,
      "loss": 1.2185,
      "step": 158
    },
    {
      "epoch": 0.013152588640382169,
      "grad_norm": 2.3265786170959473,
      "learning_rate": 9.481168410158006e-05,
      "loss": 1.2997,
      "step": 159
    },
    {
      "epoch": 0.013235309323655013,
      "grad_norm": 2.819876194000244,
      "learning_rate": 9.332747875016332e-05,
      "loss": 1.3137,
      "step": 160
    },
    {
      "epoch": 0.013318030006927857,
      "grad_norm": 2.8405160903930664,
      "learning_rate": 9.16715938287104e-05,
      "loss": 1.3081,
      "step": 161
    },
    {
      "epoch": 0.013400750690200702,
      "grad_norm": 2.6705682277679443,
      "learning_rate": 8.985059056855862e-05,
      "loss": 1.0704,
      "step": 162
    },
    {
      "epoch": 0.013483471373473544,
      "grad_norm": 2.5349082946777344,
      "learning_rate": 8.787168446125638e-05,
      "loss": 1.1009,
      "step": 163
    },
    {
      "epoch": 0.013566192056746388,
      "grad_norm": 2.3761534690856934,
      "learning_rate": 8.574271666810467e-05,
      "loss": 1.3948,
      "step": 164
    },
    {
      "epoch": 0.013648912740019233,
      "grad_norm": 2.0366649627685547,
      "learning_rate": 8.347212295056239e-05,
      "loss": 1.3042,
      "step": 165
    },
    {
      "epoch": 0.013731633423292077,
      "grad_norm": 2.5665273666381836,
      "learning_rate": 8.106890024462485e-05,
      "loss": 1.1279,
      "step": 166
    },
    {
      "epoch": 0.013814354106564921,
      "grad_norm": 2.2277767658233643,
      "learning_rate": 7.854257101162037e-05,
      "loss": 1.0276,
      "step": 167
    },
    {
      "epoch": 0.013897074789837763,
      "grad_norm": 2.6040496826171875,
      "learning_rate": 7.590314550668051e-05,
      "loss": 1.2547,
      "step": 168
    },
    {
      "epoch": 0.013979795473110608,
      "grad_norm": 2.412614583969116,
      "learning_rate": 7.316108211438945e-05,
      "loss": 1.1366,
      "step": 169
    },
    {
      "epoch": 0.014062516156383452,
      "grad_norm": 2.7698814868927,
      "learning_rate": 7.032724590877824e-05,
      "loss": 1.1467,
      "step": 170
    },
    {
      "epoch": 0.014145236839656296,
      "grad_norm": 2.352604866027832,
      "learning_rate": 6.741286560186435e-05,
      "loss": 1.0439,
      "step": 171
    },
    {
      "epoch": 0.014227957522929139,
      "grad_norm": 2.732985258102417,
      "learning_rate": 6.442948905132262e-05,
      "loss": 1.1244,
      "step": 172
    },
    {
      "epoch": 0.014310678206201983,
      "grad_norm": 2.4111437797546387,
      "learning_rate": 6.13889375035821e-05,
      "loss": 0.9955,
      "step": 173
    },
    {
      "epoch": 0.014393398889474827,
      "grad_norm": 2.541710615158081,
      "learning_rate": 5.830325875365522e-05,
      "loss": 1.1219,
      "step": 174
    },
    {
      "epoch": 0.014476119572747671,
      "grad_norm": 2.568457841873169,
      "learning_rate": 5.518467940729737e-05,
      "loss": 1.0404,
      "step": 175
    },
    {
      "epoch": 0.014558840256020516,
      "grad_norm": 2.3697330951690674,
      "learning_rate": 5.2045556434652085e-05,
      "loss": 1.1134,
      "step": 176
    },
    {
      "epoch": 0.014641560939293358,
      "grad_norm": 3.043905258178711,
      "learning_rate": 4.889832820734363e-05,
      "loss": 0.8997,
      "step": 177
    },
    {
      "epoch": 0.014724281622566202,
      "grad_norm": 2.69205379486084,
      "learning_rate": 4.575546521302681e-05,
      "loss": 1.125,
      "step": 178
    },
    {
      "epoch": 0.014807002305839046,
      "grad_norm": 2.6640970706939697,
      "learning_rate": 4.2629420642681306e-05,
      "loss": 1.0608,
      "step": 179
    },
    {
      "epoch": 0.01488972298911189,
      "grad_norm": 3.0289556980133057,
      "learning_rate": 3.953258104644192e-05,
      "loss": 1.0332,
      "step": 180
    },
    {
      "epoch": 0.014972443672384733,
      "grad_norm": 3.0815601348876953,
      "learning_rate": 3.647721725348435e-05,
      "loss": 0.97,
      "step": 181
    },
    {
      "epoch": 0.015055164355657577,
      "grad_norm": 2.9323689937591553,
      "learning_rate": 3.3475435750440356e-05,
      "loss": 1.0372,
      "step": 182
    },
    {
      "epoch": 0.015137885038930422,
      "grad_norm": 2.852897882461548,
      "learning_rate": 3.0539130710999486e-05,
      "loss": 0.9374,
      "step": 183
    },
    {
      "epoch": 0.015220605722203266,
      "grad_norm": 2.8303334712982178,
      "learning_rate": 2.7679936866773376e-05,
      "loss": 0.8885,
      "step": 184
    },
    {
      "epoch": 0.01530332640547611,
      "grad_norm": 3.060784339904785,
      "learning_rate": 2.490918340616586e-05,
      "loss": 0.9998,
      "step": 185
    },
    {
      "epoch": 0.015386047088748953,
      "grad_norm": 2.965430736541748,
      "learning_rate": 2.22378490839192e-05,
      "loss": 0.839,
      "step": 186
    },
    {
      "epoch": 0.015468767772021797,
      "grad_norm": 2.8629958629608154,
      "learning_rate": 1.9676518719207977e-05,
      "loss": 0.9819,
      "step": 187
    },
    {
      "epoch": 0.015551488455294641,
      "grad_norm": 2.74807071685791,
      "learning_rate": 1.723534125465302e-05,
      "loss": 0.7902,
      "step": 188
    },
    {
      "epoch": 0.015634209138567483,
      "grad_norm": 2.879805088043213,
      "learning_rate": 1.4923989542439159e-05,
      "loss": 0.7908,
      "step": 189
    },
    {
      "epoch": 0.01571692982184033,
      "grad_norm": 3.01045560836792,
      "learning_rate": 1.2751622016881199e-05,
      "loss": 0.9525,
      "step": 190
    },
    {
      "epoch": 0.015799650505113172,
      "grad_norm": 2.830258846282959,
      "learning_rate": 1.0726846405303754e-05,
      "loss": 0.875,
      "step": 191
    },
    {
      "epoch": 0.015882371188386014,
      "grad_norm": 3.210082530975342,
      "learning_rate": 8.857685621027578e-06,
      "loss": 0.8692,
      "step": 192
    },
    {
      "epoch": 0.01596509187165886,
      "grad_norm": 2.9460597038269043,
      "learning_rate": 7.1515459736055505e-06,
      "loss": 0.6706,
      "step": 193
    },
    {
      "epoch": 0.016047812554931703,
      "grad_norm": 2.5167949199676514,
      "learning_rate": 5.615187822272588e-06,
      "loss": 0.644,
      "step": 194
    },
    {
      "epoch": 0.01613053323820455,
      "grad_norm": 3.235827922821045,
      "learning_rate": 4.254698788890116e-06,
      "loss": 0.8087,
      "step": 195
    },
    {
      "epoch": 0.01621325392147739,
      "grad_norm": 3.202122926712036,
      "learning_rate": 3.0754696365265123e-06,
      "loss": 0.7054,
      "step": 196
    },
    {
      "epoch": 0.016295974604750234,
      "grad_norm": 3.0584912300109863,
      "learning_rate": 2.082172909250568e-06,
      "loss": 0.6067,
      "step": 197
    },
    {
      "epoch": 0.01637869528802308,
      "grad_norm": 3.32216739654541,
      "learning_rate": 1.2787444177759123e-06,
      "loss": 0.6159,
      "step": 198
    },
    {
      "epoch": 0.016461415971295922,
      "grad_norm": 3.372765302658081,
      "learning_rate": 6.683676443163256e-07,
      "loss": 0.599,
      "step": 199
    },
    {
      "epoch": 0.016544136654568768,
      "grad_norm": 5.333384037017822,
      "learning_rate": 2.534611284465083e-07,
      "loss": 0.6941,
      "step": 200
    },
    {
      "epoch": 0.01662685733784161,
      "grad_norm": 2.5300095081329346,
      "learning_rate": 3.566888394948009e-08,
      "loss": 1.2561,
      "step": 201
    },
    {
      "epoch": 0.016709578021114453,
      "grad_norm": 2.4682185649871826,
      "learning_rate": 9.998414611537681e-05,
      "loss": 1.1351,
      "step": 202
    },
    {
      "epoch": 0.0167922987043873,
      "grad_norm": 2.8092005252838135,
      "learning_rate": 9.980590535514233e-05,
      "loss": 1.0876,
      "step": 203
    },
    {
      "epoch": 0.01687501938766014,
      "grad_norm": 2.6164400577545166,
      "learning_rate": 9.943031509146825e-05,
      "loss": 1.2216,
      "step": 204
    },
    {
      "epoch": 0.016957740070932988,
      "grad_norm": 2.450028657913208,
      "learning_rate": 9.885886355253758e-05,
      "loss": 1.3142,
      "step": 205
    },
    {
      "epoch": 0.01704046075420583,
      "grad_norm": 2.217843532562256,
      "learning_rate": 9.809381504168231e-05,
      "loss": 1.1277,
      "step": 206
    },
    {
      "epoch": 0.017123181437478673,
      "grad_norm": 2.1484124660491943,
      "learning_rate": 9.713820096537225e-05,
      "loss": 1.1206,
      "step": 207
    },
    {
      "epoch": 0.01720590212075152,
      "grad_norm": 2.595496416091919,
      "learning_rate": 9.599580782165597e-05,
      "loss": 1.3468,
      "step": 208
    },
    {
      "epoch": 0.01728862280402436,
      "grad_norm": 2.609548568725586,
      "learning_rate": 9.46711621966489e-05,
      "loss": 1.3127,
      "step": 209
    },
    {
      "epoch": 0.017371343487297203,
      "grad_norm": 2.9885194301605225,
      "learning_rate": 9.31695128285171e-05,
      "loss": 1.2066,
      "step": 210
    },
    {
      "epoch": 0.01745406417057005,
      "grad_norm": 2.1710734367370605,
      "learning_rate": 9.149680981002609e-05,
      "loss": 1.0917,
      "step": 211
    },
    {
      "epoch": 0.017536784853842892,
      "grad_norm": 2.234113931655884,
      "learning_rate": 8.965968101206296e-05,
      "loss": 1.2525,
      "step": 212
    },
    {
      "epoch": 0.017619505537115738,
      "grad_norm": 2.363409996032715,
      "learning_rate": 8.766540582154863e-05,
      "loss": 1.1802,
      "step": 213
    },
    {
      "epoch": 0.01770222622038858,
      "grad_norm": 2.2879207134246826,
      "learning_rate": 8.552188629780247e-05,
      "loss": 0.9453,
      "step": 214
    },
    {
      "epoch": 0.017784946903661423,
      "grad_norm": 2.645390510559082,
      "learning_rate": 8.323761586164693e-05,
      "loss": 1.054,
      "step": 215
    },
    {
      "epoch": 0.01786766758693427,
      "grad_norm": 2.131753921508789,
      "learning_rate": 8.082164564131841e-05,
      "loss": 1.0289,
      "step": 216
    },
    {
      "epoch": 0.01795038827020711,
      "grad_norm": 2.435033082962036,
      "learning_rate": 7.828354860853404e-05,
      "loss": 1.163,
      "step": 217
    },
    {
      "epoch": 0.018033108953479957,
      "grad_norm": 2.314852237701416,
      "learning_rate": 7.563338164682037e-05,
      "loss": 1.1395,
      "step": 218
    },
    {
      "epoch": 0.0181158296367528,
      "grad_norm": 2.7291159629821777,
      "learning_rate": 7.28816457024046e-05,
      "loss": 1.275,
      "step": 219
    },
    {
      "epoch": 0.018198550320025642,
      "grad_norm": 2.5757625102996826,
      "learning_rate": 7.003924417556351e-05,
      "loss": 1.0547,
      "step": 220
    },
    {
      "epoch": 0.018281271003298488,
      "grad_norm": 2.4381117820739746,
      "learning_rate": 6.711743971729971e-05,
      "loss": 1.1921,
      "step": 221
    },
    {
      "epoch": 0.01836399168657133,
      "grad_norm": 2.81893253326416,
      "learning_rate": 6.412780960253438e-05,
      "loss": 1.0369,
      "step": 222
    },
    {
      "epoch": 0.018446712369844177,
      "grad_norm": 2.595392942428589,
      "learning_rate": 6.108219985664158e-05,
      "loss": 1.0805,
      "step": 223
    },
    {
      "epoch": 0.01852943305311702,
      "grad_norm": 2.269163131713867,
      "learning_rate": 5.799267831709434e-05,
      "loss": 1.0698,
      "step": 224
    },
    {
      "epoch": 0.01861215373638986,
      "grad_norm": 2.4764816761016846,
      "learning_rate": 5.487148681620865e-05,
      "loss": 0.9098,
      "step": 225
    },
    {
      "epoch": 0.018694874419662708,
      "grad_norm": 2.301236391067505,
      "learning_rate": 5.1730992674454505e-05,
      "loss": 1.1335,
      "step": 226
    },
    {
      "epoch": 0.01877759510293555,
      "grad_norm": 2.4862706661224365,
      "learning_rate": 4.858363969653776e-05,
      "loss": 0.9138,
      "step": 227
    },
    {
      "epoch": 0.018860315786208393,
      "grad_norm": 3.0216848850250244,
      "learning_rate": 4.544189886442167e-05,
      "loss": 1.1821,
      "step": 228
    },
    {
      "epoch": 0.01894303646948124,
      "grad_norm": 2.7798476219177246,
      "learning_rate": 4.2318218922662114e-05,
      "loss": 0.9194,
      "step": 229
    },
    {
      "epoch": 0.01902575715275408,
      "grad_norm": 2.5338101387023926,
      "learning_rate": 3.922497705185689e-05,
      "loss": 0.9455,
      "step": 230
    },
    {
      "epoch": 0.019108477836026927,
      "grad_norm": 2.7410335540771484,
      "learning_rate": 3.6174429825656644e-05,
      "loss": 1.0465,
      "step": 231
    },
    {
      "epoch": 0.01919119851929977,
      "grad_norm": 2.6409614086151123,
      "learning_rate": 3.317866464566598e-05,
      "loss": 0.9104,
      "step": 232
    },
    {
      "epoch": 0.019273919202572612,
      "grad_norm": 2.705345392227173,
      "learning_rate": 3.0249551846667207e-05,
      "loss": 0.9821,
      "step": 233
    },
    {
      "epoch": 0.019356639885845458,
      "grad_norm": 3.0736594200134277,
      "learning_rate": 2.7398697661942585e-05,
      "loss": 0.9321,
      "step": 234
    },
    {
      "epoch": 0.0194393605691183,
      "grad_norm": 3.1172425746917725,
      "learning_rate": 2.4637398235066473e-05,
      "loss": 0.9219,
      "step": 235
    },
    {
      "epoch": 0.019522081252391146,
      "grad_norm": 2.750128984451294,
      "learning_rate": 2.1976594860386614e-05,
      "loss": 0.9844,
      "step": 236
    },
    {
      "epoch": 0.01960480193566399,
      "grad_norm": 2.741159439086914,
      "learning_rate": 1.9426830629550242e-05,
      "loss": 0.8346,
      "step": 237
    },
    {
      "epoch": 0.01968752261893683,
      "grad_norm": 2.798370361328125,
      "learning_rate": 1.6998208655858205e-05,
      "loss": 0.7598,
      "step": 238
    },
    {
      "epoch": 0.019770243302209677,
      "grad_norm": 3.230220317840576,
      "learning_rate": 1.4700352041975197e-05,
      "loss": 0.8265,
      "step": 239
    },
    {
      "epoch": 0.01985296398548252,
      "grad_norm": 2.70322847366333,
      "learning_rate": 1.2542365749622064e-05,
      "loss": 0.6773,
      "step": 240
    },
    {
      "epoch": 0.019935684668755362,
      "grad_norm": 3.1857683658599854,
      "learning_rate": 1.0532800522333897e-05,
      "loss": 0.7911,
      "step": 241
    },
    {
      "epoch": 0.020018405352028208,
      "grad_norm": 3.773155450820923,
      "learning_rate": 8.679619004237161e-06,
      "loss": 0.7107,
      "step": 242
    },
    {
      "epoch": 0.02010112603530105,
      "grad_norm": 2.9136664867401123,
      "learning_rate": 6.990164189094622e-06,
      "loss": 0.6453,
      "step": 243
    },
    {
      "epoch": 0.020183846718573897,
      "grad_norm": 2.9928243160247803,
      "learning_rate": 5.4711303246361144e-06,
      "loss": 0.4764,
      "step": 244
    },
    {
      "epoch": 0.02026656740184674,
      "grad_norm": 3.4420974254608154,
      "learning_rate": 4.128536387461323e-06,
      "loss": 0.6468,
      "step": 245
    },
    {
      "epoch": 0.02034928808511958,
      "grad_norm": 2.91145396232605,
      "learning_rate": 2.9677022336181747e-06,
      "loss": 0.6066,
      "step": 246
    },
    {
      "epoch": 0.020432008768392428,
      "grad_norm": 4.40066385269165,
      "learning_rate": 1.9932275193561945e-06,
      "loss": 0.6318,
      "step": 247
    },
    {
      "epoch": 0.02051472945166527,
      "grad_norm": 3.170173406600952,
      "learning_rate": 1.208973475579761e-06,
      "loss": 0.6427,
      "step": 248
    },
    {
      "epoch": 0.020597450134938116,
      "grad_norm": 3.0413131713867188,
      "learning_rate": 6.180476082162546e-07,
      "loss": 0.5688,
      "step": 249
    },
    {
      "epoch": 0.02068017081821096,
      "grad_norm": NaN,
      "learning_rate": 6.180476082162546e-07,
      "loss": 0.6405,
      "step": 250
    },
    {
      "epoch": 0.0207628915014838,
      "grad_norm": 2.3715901374816895,
      "learning_rate": 2.2279138512301123e-07,
      "loss": 1.4623,
      "step": 251
    },
    {
      "epoch": 0.020845612184756647,
      "grad_norm": 2.7274351119995117,
      "learning_rate": 2.4770958321568283e-08,
      "loss": 1.4284,
      "step": 252
    },
    {
      "epoch": 0.02092833286802949,
      "grad_norm": 2.809828758239746,
      "learning_rate": 9.997522904167844e-05,
      "loss": 1.2725,
      "step": 253
    },
    {
      "epoch": 0.021011053551302335,
      "grad_norm": 2.6403815746307373,
      "learning_rate": 9.9777208614877e-05,
      "loss": 1.1109,
      "step": 254
    },
    {
      "epoch": 0.021093774234575178,
      "grad_norm": 2.368431329727173,
      "learning_rate": 9.938195239178375e-05,
      "loss": 1.3375,
      "step": 255
    },
    {
      "epoch": 0.02117649491784802,
      "grad_norm": 2.195054769515991,
      "learning_rate": 9.879102652442027e-05,
      "loss": 1.1981,
      "step": 256
    },
    {
      "epoch": 0.021259215601120866,
      "grad_norm": 2.3776161670684814,
      "learning_rate": 9.800677248064385e-05,
      "loss": 1.1693,
      "step": 257
    },
    {
      "epoch": 0.02134193628439371,
      "grad_norm": 2.058454751968384,
      "learning_rate": 9.703229776638188e-05,
      "loss": 1.3086,
      "step": 258
    },
    {
      "epoch": 0.02142465696766655,
      "grad_norm": 2.2017621994018555,
      "learning_rate": 9.587146361253868e-05,
      "loss": 1.1761,
      "step": 259
    },
    {
      "epoch": 0.021507377650939397,
      "grad_norm": 2.255263328552246,
      "learning_rate": 9.45288696753639e-05,
      "loss": 1.0767,
      "step": 260
    },
    {
      "epoch": 0.02159009833421224,
      "grad_norm": 2.5887086391448975,
      "learning_rate": 9.300983581090545e-05,
      "loss": 1.2723,
      "step": 261
    },
    {
      "epoch": 0.021672819017485086,
      "grad_norm": 2.5761420726776123,
      "learning_rate": 9.132038099576292e-05,
      "loss": 1.2486,
      "step": 262
    },
    {
      "epoch": 0.021755539700757928,
      "grad_norm": 2.341066360473633,
      "learning_rate": 8.94671994776661e-05,
      "loss": 1.2789,
      "step": 263
    },
    {
      "epoch": 0.02183826038403077,
      "grad_norm": 2.3681387901306152,
      "learning_rate": 8.745763425037805e-05,
      "loss": 1.2464,
      "step": 264
    },
    {
      "epoch": 0.021920981067303617,
      "grad_norm": 2.1590607166290283,
      "learning_rate": 8.52996479580249e-05,
      "loss": 1.0784,
      "step": 265
    },
    {
      "epoch": 0.02200370175057646,
      "grad_norm": 2.093672037124634,
      "learning_rate": 8.300179134414191e-05,
      "loss": 1.0477,
      "step": 266
    },
    {
      "epoch": 0.022086422433849305,
      "grad_norm": 2.8812665939331055,
      "learning_rate": 8.057316937044977e-05,
      "loss": 1.1635,
      "step": 267
    },
    {
      "epoch": 0.022169143117122148,
      "grad_norm": 2.449533700942993,
      "learning_rate": 7.80234051396134e-05,
      "loss": 1.0299,
      "step": 268
    },
    {
      "epoch": 0.02225186380039499,
      "grad_norm": 2.361886739730835,
      "learning_rate": 7.536260176493354e-05,
      "loss": 1.0585,
      "step": 269
    },
    {
      "epoch": 0.022334584483667836,
      "grad_norm": 2.5262885093688965,
      "learning_rate": 7.260130233805741e-05,
      "loss": 1.1755,
      "step": 270
    },
    {
      "epoch": 0.02241730516694068,
      "grad_norm": 2.671877384185791,
      "learning_rate": 6.97504481533328e-05,
      "loss": 1.3019,
      "step": 271
    },
    {
      "epoch": 0.022500025850213524,
      "grad_norm": 2.190485715866089,
      "learning_rate": 6.682133535433403e-05,
      "loss": 1.2444,
      "step": 272
    },
    {
      "epoch": 0.022582746533486367,
      "grad_norm": 2.474337577819824,
      "learning_rate": 6.382557017434337e-05,
      "loss": 1.173,
      "step": 273
    },
    {
      "epoch": 0.02266546721675921,
      "grad_norm": 2.6360442638397217,
      "learning_rate": 6.077502294814312e-05,
      "loss": 1.0191,
      "step": 274
    },
    {
      "epoch": 0.022748187900032055,
      "grad_norm": 2.6871681213378906,
      "learning_rate": 5.7681781077337905e-05,
      "loss": 1.1115,
      "step": 275
    },
    {
      "epoch": 0.022830908583304898,
      "grad_norm": 2.2699544429779053,
      "learning_rate": 5.455810113557834e-05,
      "loss": 0.8804,
      "step": 276
    },
    {
      "epoch": 0.02291362926657774,
      "grad_norm": 2.9938695430755615,
      "learning_rate": 5.1416360303462254e-05,
      "loss": 0.9694,
      "step": 277
    },
    {
      "epoch": 0.022996349949850586,
      "grad_norm": 2.4285812377929688,
      "learning_rate": 4.8269007325545506e-05,
      "loss": 0.9185,
      "step": 278
    },
    {
      "epoch": 0.02307907063312343,
      "grad_norm": 2.3933985233306885,
      "learning_rate": 4.512851318379135e-05,
      "loss": 1.0215,
      "step": 279
    },
    {
      "epoch": 0.023161791316396275,
      "grad_norm": 2.7737534046173096,
      "learning_rate": 4.2007321682905675e-05,
      "loss": 0.9499,
      "step": 280
    },
    {
      "epoch": 0.023244511999669117,
      "grad_norm": 2.1060075759887695,
      "learning_rate": 3.891780014335844e-05,
      "loss": 0.9305,
      "step": 281
    },
    {
      "epoch": 0.02332723268294196,
      "grad_norm": 2.482893943786621,
      "learning_rate": 3.587219039746564e-05,
      "loss": 0.9734,
      "step": 282
    },
    {
      "epoch": 0.023409953366214806,
      "grad_norm": 2.7937533855438232,
      "learning_rate": 3.2882560282700295e-05,
      "loss": 1.0373,
      "step": 283
    },
    {
      "epoch": 0.023492674049487648,
      "grad_norm": 3.8134541511535645,
      "learning_rate": 2.9960755824436503e-05,
      "loss": 0.9984,
      "step": 284
    },
    {
      "epoch": 0.023575394732760494,
      "grad_norm": 2.7384705543518066,
      "learning_rate": 2.7118354297595412e-05,
      "loss": 0.9816,
      "step": 285
    },
    {
      "epoch": 0.023658115416033337,
      "grad_norm": 2.9398815631866455,
      "learning_rate": 2.4366618353179644e-05,
      "loss": 0.8315,
      "step": 286
    },
    {
      "epoch": 0.02374083609930618,
      "grad_norm": 2.3722190856933594,
      "learning_rate": 2.1716451391465974e-05,
      "loss": 0.7387,
      "step": 287
    },
    {
      "epoch": 0.023823556782579025,
      "grad_norm": 2.8505733013153076,
      "learning_rate": 1.91783543586816e-05,
      "loss": 0.9551,
      "step": 288
    },
    {
      "epoch": 0.023906277465851868,
      "grad_norm": 2.746326208114624,
      "learning_rate": 1.6762384138353078e-05,
      "loss": 0.8709,
      "step": 289
    },
    {
      "epoch": 0.02398899814912471,
      "grad_norm": 2.702108383178711,
      "learning_rate": 1.4478113702197539e-05,
      "loss": 0.8949,
      "step": 290
    },
    {
      "epoch": 0.024071718832397556,
      "grad_norm": 2.9554717540740967,
      "learning_rate": 1.233459417845138e-05,
      "loss": 0.8291,
      "step": 291
    },
    {
      "epoch": 0.0241544395156704,
      "grad_norm": 3.089277982711792,
      "learning_rate": 1.0340318987937031e-05,
      "loss": 0.792,
      "step": 292
    },
    {
      "epoch": 0.024237160198943244,
      "grad_norm": 2.8191988468170166,
      "learning_rate": 8.503190189973914e-06,
      "loss": 0.6917,
      "step": 293
    },
    {
      "epoch": 0.024319880882216087,
      "grad_norm": 3.628286838531494,
      "learning_rate": 6.830487171482913e-06,
      "loss": 0.6827,
      "step": 294
    },
    {
      "epoch": 0.02440260156548893,
      "grad_norm": 3.5429203510284424,
      "learning_rate": 5.328837803351044e-06,
      "loss": 0.6924,
      "step": 295
    },
    {
      "epoch": 0.024485322248761775,
      "grad_norm": 3.6406972408294678,
      "learning_rate": 4.004192178344041e-06,
      "loss": 0.6184,
      "step": 296
    },
    {
      "epoch": 0.024568042932034618,
      "grad_norm": 3.4887325763702393,
      "learning_rate": 2.8617990346277547e-06,
      "loss": 0.6895,
      "step": 297
    },
    {
      "epoch": 0.024650763615307464,
      "grad_norm": 3.049867868423462,
      "learning_rate": 1.9061849583176527e-06,
      "loss": 0.607,
      "step": 298
    },
    {
      "epoch": 0.024733484298580306,
      "grad_norm": 3.2983977794647217,
      "learning_rate": 1.1411364474624043e-06,
      "loss": 0.7239,
      "step": 299
    },
    {
      "epoch": 0.02481620498185315,
      "grad_norm": 2.878427505493164,
      "learning_rate": 5.696849085317424e-07,
      "loss": 0.6022,
      "step": 300
    }
  ],
  "logging_steps": 1,
  "max_steps": 500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.7442147008176128e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
