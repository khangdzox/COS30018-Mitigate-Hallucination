{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.02318494965553789,
  "eval_steps": 500,
  "global_step": 350,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 6.624271330153684e-05,
      "grad_norm": 1.3809800148010254,
      "learning_rate": 1.6e-06,
      "loss": 2.2689,
      "step": 1
    },
    {
      "epoch": 0.00013248542660307367,
      "grad_norm": 1.6996265649795532,
      "learning_rate": 3.2e-06,
      "loss": 2.3392,
      "step": 2
    },
    {
      "epoch": 0.0001987281399046105,
      "grad_norm": 1.5995228290557861,
      "learning_rate": 4.8e-06,
      "loss": 2.327,
      "step": 3
    },
    {
      "epoch": 0.00026497085320614734,
      "grad_norm": 1.2807488441467285,
      "learning_rate": 6.4e-06,
      "loss": 2.2282,
      "step": 4
    },
    {
      "epoch": 0.00033121356650768415,
      "grad_norm": 1.7918407917022705,
      "learning_rate": 8e-06,
      "loss": 2.3296,
      "step": 5
    },
    {
      "epoch": 0.000397456279809221,
      "grad_norm": 2.1889030933380127,
      "learning_rate": 7.999919440291625e-06,
      "loss": 2.4252,
      "step": 6
    },
    {
      "epoch": 0.0004636989931107578,
      "grad_norm": 1.4946634769439697,
      "learning_rate": 7.999677764411437e-06,
      "loss": 2.2345,
      "step": 7
    },
    {
      "epoch": 0.0005299417064122947,
      "grad_norm": 1.4097577333450317,
      "learning_rate": 7.999274982094103e-06,
      "loss": 2.2835,
      "step": 8
    },
    {
      "epoch": 0.0005961844197138314,
      "grad_norm": 1.5491282939910889,
      "learning_rate": 7.998711109563636e-06,
      "loss": 2.3787,
      "step": 9
    },
    {
      "epoch": 0.0006624271330153683,
      "grad_norm": 1.5071535110473633,
      "learning_rate": 7.99798616953274e-06,
      "loss": 2.084,
      "step": 10
    },
    {
      "epoch": 0.0007286698463169052,
      "grad_norm": 1.3594766855239868,
      "learning_rate": 7.997100191201894e-06,
      "loss": 2.2995,
      "step": 11
    },
    {
      "epoch": 0.000794912559618442,
      "grad_norm": 1.4487510919570923,
      "learning_rate": 7.996053210258175e-06,
      "loss": 2.5685,
      "step": 12
    },
    {
      "epoch": 0.0008611552729199788,
      "grad_norm": 1.9860165119171143,
      "learning_rate": 7.994845268873824e-06,
      "loss": 2.5732,
      "step": 13
    },
    {
      "epoch": 0.0009273979862215156,
      "grad_norm": 1.3641774654388428,
      "learning_rate": 7.993476415704541e-06,
      "loss": 2.2761,
      "step": 14
    },
    {
      "epoch": 0.0009936406995230524,
      "grad_norm": 1.6011816263198853,
      "learning_rate": 7.991946705887537e-06,
      "loss": 2.2512,
      "step": 15
    },
    {
      "epoch": 0.0010598834128245894,
      "grad_norm": 1.6379209756851196,
      "learning_rate": 7.990256201039296e-06,
      "loss": 2.3967,
      "step": 16
    },
    {
      "epoch": 0.0011261261261261261,
      "grad_norm": 1.312429428100586,
      "learning_rate": 7.988404969253109e-06,
      "loss": 2.0369,
      "step": 17
    },
    {
      "epoch": 0.0011923688394276629,
      "grad_norm": 1.5695819854736328,
      "learning_rate": 7.986393085096323e-06,
      "loss": 2.2738,
      "step": 18
    },
    {
      "epoch": 0.0012586115527291998,
      "grad_norm": 1.7151944637298584,
      "learning_rate": 7.984220629607335e-06,
      "loss": 2.2317,
      "step": 19
    },
    {
      "epoch": 0.0013248542660307366,
      "grad_norm": 1.3730355501174927,
      "learning_rate": 7.981887690292338e-06,
      "loss": 2.2022,
      "step": 20
    },
    {
      "epoch": 0.0013910969793322733,
      "grad_norm": 1.6174428462982178,
      "learning_rate": 7.979394361121788e-06,
      "loss": 2.4936,
      "step": 21
    },
    {
      "epoch": 0.0014573396926338103,
      "grad_norm": 1.417569637298584,
      "learning_rate": 7.976740742526617e-06,
      "loss": 2.1494,
      "step": 22
    },
    {
      "epoch": 0.001523582405935347,
      "grad_norm": 1.4038379192352295,
      "learning_rate": 7.973926941394201e-06,
      "loss": 2.2114,
      "step": 23
    },
    {
      "epoch": 0.001589825119236884,
      "grad_norm": 1.4127357006072998,
      "learning_rate": 7.970953071064033e-06,
      "loss": 2.2311,
      "step": 24
    },
    {
      "epoch": 0.0016560678325384208,
      "grad_norm": 1.5051912069320679,
      "learning_rate": 7.96781925132318e-06,
      "loss": 2.215,
      "step": 25
    },
    {
      "epoch": 0.0017223105458399576,
      "grad_norm": 1.38594651222229,
      "learning_rate": 7.964525608401445e-06,
      "loss": 2.1734,
      "step": 26
    },
    {
      "epoch": 0.0017885532591414945,
      "grad_norm": 1.5026637315750122,
      "learning_rate": 7.96107227496628e-06,
      "loss": 2.256,
      "step": 27
    },
    {
      "epoch": 0.0018547959724430313,
      "grad_norm": 1.3919044733047485,
      "learning_rate": 7.957459390117459e-06,
      "loss": 2.1654,
      "step": 28
    },
    {
      "epoch": 0.001921038685744568,
      "grad_norm": 1.4173874855041504,
      "learning_rate": 7.953687099381448e-06,
      "loss": 2.3061,
      "step": 29
    },
    {
      "epoch": 0.001987281399046105,
      "grad_norm": 1.6002627611160278,
      "learning_rate": 7.949755554705577e-06,
      "loss": 2.2258,
      "step": 30
    },
    {
      "epoch": 0.0020535241123476418,
      "grad_norm": 1.5717726945877075,
      "learning_rate": 7.945664914451887e-06,
      "loss": 2.2467,
      "step": 31
    },
    {
      "epoch": 0.0021197668256491787,
      "grad_norm": 1.6920794248580933,
      "learning_rate": 7.941415343390771e-06,
      "loss": 2.1356,
      "step": 32
    },
    {
      "epoch": 0.0021860095389507153,
      "grad_norm": 1.4630272388458252,
      "learning_rate": 7.937007012694335e-06,
      "loss": 2.2694,
      "step": 33
    },
    {
      "epoch": 0.0022522522522522522,
      "grad_norm": 1.5543068647384644,
      "learning_rate": 7.932440099929493e-06,
      "loss": 2.2479,
      "step": 34
    },
    {
      "epoch": 0.002318494965553789,
      "grad_norm": 1.2894253730773926,
      "learning_rate": 7.927714789050827e-06,
      "loss": 2.0509,
      "step": 35
    },
    {
      "epoch": 0.0023847376788553257,
      "grad_norm": 1.5814480781555176,
      "learning_rate": 7.922831270393169e-06,
      "loss": 2.1374,
      "step": 36
    },
    {
      "epoch": 0.0024509803921568627,
      "grad_norm": 1.5754947662353516,
      "learning_rate": 7.917789740663941e-06,
      "loss": 1.9903,
      "step": 37
    },
    {
      "epoch": 0.0025172231054583997,
      "grad_norm": 1.5083675384521484,
      "learning_rate": 7.912590402935222e-06,
      "loss": 2.2261,
      "step": 38
    },
    {
      "epoch": 0.0025834658187599362,
      "grad_norm": 1.5683575868606567,
      "learning_rate": 7.90723346663558e-06,
      "loss": 2.1183,
      "step": 39
    },
    {
      "epoch": 0.002649708532061473,
      "grad_norm": 1.5738838911056519,
      "learning_rate": 7.901719147541628e-06,
      "loss": 2.2268,
      "step": 40
    },
    {
      "epoch": 0.00271595124536301,
      "grad_norm": 1.6617158651351929,
      "learning_rate": 7.896047667769334e-06,
      "loss": 2.1754,
      "step": 41
    },
    {
      "epoch": 0.0027821939586645467,
      "grad_norm": 1.6001683473587036,
      "learning_rate": 7.890219255765076e-06,
      "loss": 1.8818,
      "step": 42
    },
    {
      "epoch": 0.0028484366719660837,
      "grad_norm": 1.7564657926559448,
      "learning_rate": 7.88423414629644e-06,
      "loss": 2.0247,
      "step": 43
    },
    {
      "epoch": 0.0029146793852676206,
      "grad_norm": 1.3842233419418335,
      "learning_rate": 7.878092580442764e-06,
      "loss": 1.9519,
      "step": 44
    },
    {
      "epoch": 0.0029809220985691576,
      "grad_norm": 1.5279793739318848,
      "learning_rate": 7.871794805585425e-06,
      "loss": 2.1173,
      "step": 45
    },
    {
      "epoch": 0.003047164811870694,
      "grad_norm": 1.4517604112625122,
      "learning_rate": 7.865341075397873e-06,
      "loss": 2.0558,
      "step": 46
    },
    {
      "epoch": 0.003113407525172231,
      "grad_norm": 1.494954228401184,
      "learning_rate": 7.858731649835423e-06,
      "loss": 2.0556,
      "step": 47
    },
    {
      "epoch": 0.003179650238473768,
      "grad_norm": 1.8161464929580688,
      "learning_rate": 7.85196679512477e-06,
      "loss": 2.1374,
      "step": 48
    },
    {
      "epoch": 0.0032458929517753046,
      "grad_norm": 1.441933274269104,
      "learning_rate": 7.845046783753275e-06,
      "loss": 2.1359,
      "step": 49
    },
    {
      "epoch": 0.0033121356650768416,
      "grad_norm": 1.527364730834961,
      "learning_rate": 7.837971894457989e-06,
      "loss": 1.8756,
      "step": 50
    },
    {
      "epoch": 0.0033783783783783786,
      "grad_norm": 1.513611078262329,
      "learning_rate": 7.83074241221442e-06,
      "loss": 2.2051,
      "step": 51
    },
    {
      "epoch": 0.003444621091679915,
      "grad_norm": 1.461153507232666,
      "learning_rate": 7.82335862822506e-06,
      "loss": 2.1731,
      "step": 52
    },
    {
      "epoch": 0.003510863804981452,
      "grad_norm": 1.1880441904067993,
      "learning_rate": 7.81582083990765e-06,
      "loss": 1.9915,
      "step": 53
    },
    {
      "epoch": 0.003577106518282989,
      "grad_norm": 1.3948886394500732,
      "learning_rate": 7.808129350883205e-06,
      "loss": 2.0217,
      "step": 54
    },
    {
      "epoch": 0.0036433492315845256,
      "grad_norm": 1.4726269245147705,
      "learning_rate": 7.800284470963781e-06,
      "loss": 2.0647,
      "step": 55
    },
    {
      "epoch": 0.0037095919448860626,
      "grad_norm": 1.6807737350463867,
      "learning_rate": 7.792286516139997e-06,
      "loss": 2.1367,
      "step": 56
    },
    {
      "epoch": 0.0037758346581875995,
      "grad_norm": 1.352622628211975,
      "learning_rate": 7.784135808568308e-06,
      "loss": 2.1061,
      "step": 57
    },
    {
      "epoch": 0.003842077371489136,
      "grad_norm": 1.3320592641830444,
      "learning_rate": 7.775832676558026e-06,
      "loss": 2.0101,
      "step": 58
    },
    {
      "epoch": 0.003908320084790673,
      "grad_norm": 1.4262548685073853,
      "learning_rate": 7.767377454558098e-06,
      "loss": 2.0038,
      "step": 59
    },
    {
      "epoch": 0.00397456279809221,
      "grad_norm": 1.8328378200531006,
      "learning_rate": 7.758770483143633e-06,
      "loss": 1.9436,
      "step": 60
    },
    {
      "epoch": 0.0040408055113937465,
      "grad_norm": 1.6064929962158203,
      "learning_rate": 7.750012109002185e-06,
      "loss": 1.986,
      "step": 61
    },
    {
      "epoch": 0.0041070482246952835,
      "grad_norm": 2.126375675201416,
      "learning_rate": 7.741102684919786e-06,
      "loss": 2.0367,
      "step": 62
    },
    {
      "epoch": 0.0041732909379968205,
      "grad_norm": 1.3339564800262451,
      "learning_rate": 7.73204256976674e-06,
      "loss": 1.9175,
      "step": 63
    },
    {
      "epoch": 0.0042395336512983575,
      "grad_norm": 1.377361536026001,
      "learning_rate": 7.722832128483164e-06,
      "loss": 2.0844,
      "step": 64
    },
    {
      "epoch": 0.004305776364599894,
      "grad_norm": 1.5128929615020752,
      "learning_rate": 7.71347173206429e-06,
      "loss": 1.9946,
      "step": 65
    },
    {
      "epoch": 0.0043720190779014305,
      "grad_norm": 1.56609308719635,
      "learning_rate": 7.70396175754552e-06,
      "loss": 2.0313,
      "step": 66
    },
    {
      "epoch": 0.0044382617912029675,
      "grad_norm": 1.9883188009262085,
      "learning_rate": 7.694302587987244e-06,
      "loss": 1.9522,
      "step": 67
    },
    {
      "epoch": 0.0045045045045045045,
      "grad_norm": 1.5238457918167114,
      "learning_rate": 7.6844946124594e-06,
      "loss": 1.8802,
      "step": 68
    },
    {
      "epoch": 0.0045707472178060414,
      "grad_norm": 1.6092586517333984,
      "learning_rate": 7.674538226025814e-06,
      "loss": 1.8693,
      "step": 69
    },
    {
      "epoch": 0.004636989931107578,
      "grad_norm": 1.7429418563842773,
      "learning_rate": 7.664433829728277e-06,
      "loss": 2.1075,
      "step": 70
    },
    {
      "epoch": 0.004703232644409115,
      "grad_norm": 2.038536787033081,
      "learning_rate": 7.654181830570403e-06,
      "loss": 2.0714,
      "step": 71
    },
    {
      "epoch": 0.0047694753577106515,
      "grad_norm": 1.7516038417816162,
      "learning_rate": 7.64378264150122e-06,
      "loss": 2.2597,
      "step": 72
    },
    {
      "epoch": 0.0048357180710121885,
      "grad_norm": 1.6387555599212646,
      "learning_rate": 7.633236681398548e-06,
      "loss": 2.0844,
      "step": 73
    },
    {
      "epoch": 0.004901960784313725,
      "grad_norm": 2.247476577758789,
      "learning_rate": 7.622544375052123e-06,
      "loss": 1.8272,
      "step": 74
    },
    {
      "epoch": 0.004968203497615262,
      "grad_norm": 1.9581820964813232,
      "learning_rate": 7.611706153146485e-06,
      "loss": 1.9989,
      "step": 75
    },
    {
      "epoch": 0.005034446210916799,
      "grad_norm": 1.587908148765564,
      "learning_rate": 7.600722452243631e-06,
      "loss": 1.9356,
      "step": 76
    },
    {
      "epoch": 0.005100688924218336,
      "grad_norm": 2.135432720184326,
      "learning_rate": 7.589593714765433e-06,
      "loss": 1.7661,
      "step": 77
    },
    {
      "epoch": 0.0051669316375198724,
      "grad_norm": 2.2530341148376465,
      "learning_rate": 7.578320388975815e-06,
      "loss": 1.9583,
      "step": 78
    },
    {
      "epoch": 0.005233174350821409,
      "grad_norm": 1.563417911529541,
      "learning_rate": 7.566902928962693e-06,
      "loss": 1.859,
      "step": 79
    },
    {
      "epoch": 0.005299417064122946,
      "grad_norm": 1.8454619646072388,
      "learning_rate": 7.555341794619694e-06,
      "loss": 1.9772,
      "step": 80
    },
    {
      "epoch": 0.005365659777424483,
      "grad_norm": 1.5799814462661743,
      "learning_rate": 7.543637451627622e-06,
      "loss": 1.7329,
      "step": 81
    },
    {
      "epoch": 0.00543190249072602,
      "grad_norm": 1.46219003200531,
      "learning_rate": 7.531790371435707e-06,
      "loss": 1.9172,
      "step": 82
    },
    {
      "epoch": 0.005498145204027557,
      "grad_norm": 1.6360783576965332,
      "learning_rate": 7.519801031242613e-06,
      "loss": 1.8636,
      "step": 83
    },
    {
      "epoch": 0.005564387917329093,
      "grad_norm": 1.597049593925476,
      "learning_rate": 7.5076699139772115e-06,
      "loss": 1.855,
      "step": 84
    },
    {
      "epoch": 0.00563063063063063,
      "grad_norm": 1.5641920566558838,
      "learning_rate": 7.49539750827914e-06,
      "loss": 1.6662,
      "step": 85
    },
    {
      "epoch": 0.005696873343932167,
      "grad_norm": 1.6213308572769165,
      "learning_rate": 7.4829843084791085e-06,
      "loss": 2.0456,
      "step": 86
    },
    {
      "epoch": 0.005763116057233704,
      "grad_norm": 1.7253029346466064,
      "learning_rate": 7.470430814578996e-06,
      "loss": 1.9573,
      "step": 87
    },
    {
      "epoch": 0.005829358770535241,
      "grad_norm": 1.654067039489746,
      "learning_rate": 7.457737532231707e-06,
      "loss": 1.7104,
      "step": 88
    },
    {
      "epoch": 0.005895601483836778,
      "grad_norm": 1.8026683330535889,
      "learning_rate": 7.4449049727208025e-06,
      "loss": 1.671,
      "step": 89
    },
    {
      "epoch": 0.005961844197138315,
      "grad_norm": 1.6492748260498047,
      "learning_rate": 7.431933652939908e-06,
      "loss": 1.838,
      "step": 90
    },
    {
      "epoch": 0.006028086910439851,
      "grad_norm": 1.5996257066726685,
      "learning_rate": 7.418824095371894e-06,
      "loss": 1.7762,
      "step": 91
    },
    {
      "epoch": 0.006094329623741388,
      "grad_norm": 1.8772085905075073,
      "learning_rate": 7.405576828067827e-06,
      "loss": 1.8575,
      "step": 92
    },
    {
      "epoch": 0.006160572337042925,
      "grad_norm": 1.541295051574707,
      "learning_rate": 7.392192384625703e-06,
      "loss": 1.7763,
      "step": 93
    },
    {
      "epoch": 0.006226815050344462,
      "grad_norm": 1.6223118305206299,
      "learning_rate": 7.378671304168954e-06,
      "loss": 1.8418,
      "step": 94
    },
    {
      "epoch": 0.006293057763645999,
      "grad_norm": 1.9726598262786865,
      "learning_rate": 7.365014131324725e-06,
      "loss": 1.7748,
      "step": 95
    },
    {
      "epoch": 0.006359300476947536,
      "grad_norm": 1.6706812381744385,
      "learning_rate": 7.3512214162019485e-06,
      "loss": 1.6976,
      "step": 96
    },
    {
      "epoch": 0.006425543190249072,
      "grad_norm": 1.681087851524353,
      "learning_rate": 7.337293714369181e-06,
      "loss": 1.7339,
      "step": 97
    },
    {
      "epoch": 0.006491785903550609,
      "grad_norm": 1.4435925483703613,
      "learning_rate": 7.323231586832218e-06,
      "loss": 1.7383,
      "step": 98
    },
    {
      "epoch": 0.006558028616852146,
      "grad_norm": 1.7204086780548096,
      "learning_rate": 7.309035600011508e-06,
      "loss": 1.8086,
      "step": 99
    },
    {
      "epoch": 0.006624271330153683,
      "grad_norm": 2.0236806869506836,
      "learning_rate": 7.294706325719331e-06,
      "loss": 1.6184,
      "step": 100
    },
    {
      "epoch": 0.00669051404345522,
      "grad_norm": 2.0204360485076904,
      "learning_rate": 7.2802443411367645e-06,
      "loss": 1.5611,
      "step": 101
    },
    {
      "epoch": 0.006756756756756757,
      "grad_norm": 1.9645452499389648,
      "learning_rate": 7.2656502287904385e-06,
      "loss": 1.5475,
      "step": 102
    },
    {
      "epoch": 0.006822999470058293,
      "grad_norm": 2.3258585929870605,
      "learning_rate": 7.250924576529072e-06,
      "loss": 1.5633,
      "step": 103
    },
    {
      "epoch": 0.00688924218335983,
      "grad_norm": 1.7984058856964111,
      "learning_rate": 7.2360679774997895e-06,
      "loss": 1.508,
      "step": 104
    },
    {
      "epoch": 0.006955484896661367,
      "grad_norm": 1.6849430799484253,
      "learning_rate": 7.2210810301242345e-06,
      "loss": 1.7795,
      "step": 105
    },
    {
      "epoch": 0.007021727609962904,
      "grad_norm": 1.7991485595703125,
      "learning_rate": 7.20596433807446e-06,
      "loss": 1.4086,
      "step": 106
    },
    {
      "epoch": 0.007087970323264441,
      "grad_norm": 1.8393315076828003,
      "learning_rate": 7.190718510248621e-06,
      "loss": 1.6975,
      "step": 107
    },
    {
      "epoch": 0.007154213036565978,
      "grad_norm": 2.235084295272827,
      "learning_rate": 7.1753441607464374e-06,
      "loss": 1.4958,
      "step": 108
    },
    {
      "epoch": 0.007220455749867514,
      "grad_norm": 1.9484431743621826,
      "learning_rate": 7.159841908844464e-06,
      "loss": 1.7267,
      "step": 109
    },
    {
      "epoch": 0.007286698463169051,
      "grad_norm": 2.0825114250183105,
      "learning_rate": 7.1442123789711495e-06,
      "loss": 1.4566,
      "step": 110
    },
    {
      "epoch": 0.007352941176470588,
      "grad_norm": 1.818530797958374,
      "learning_rate": 7.128456200681677e-06,
      "loss": 1.5132,
      "step": 111
    },
    {
      "epoch": 0.007419183889772125,
      "grad_norm": 1.7386236190795898,
      "learning_rate": 7.11257400863261e-06,
      "loss": 1.5466,
      "step": 112
    },
    {
      "epoch": 0.007485426603073662,
      "grad_norm": 1.5583211183547974,
      "learning_rate": 7.09656644255633e-06,
      "loss": 1.5296,
      "step": 113
    },
    {
      "epoch": 0.007551669316375199,
      "grad_norm": 1.7485840320587158,
      "learning_rate": 7.080434147235262e-06,
      "loss": 1.4539,
      "step": 114
    },
    {
      "epoch": 0.007617912029676735,
      "grad_norm": 1.9540667533874512,
      "learning_rate": 7.064177772475912e-06,
      "loss": 1.6367,
      "step": 115
    },
    {
      "epoch": 0.007684154742978272,
      "grad_norm": 2.2393558025360107,
      "learning_rate": 7.047797973082684e-06,
      "loss": 1.6667,
      "step": 116
    },
    {
      "epoch": 0.007750397456279809,
      "grad_norm": 3.825916051864624,
      "learning_rate": 7.031295408831507e-06,
      "loss": 1.2721,
      "step": 117
    },
    {
      "epoch": 0.007816640169581345,
      "grad_norm": 2.4767422676086426,
      "learning_rate": 7.014670744443266e-06,
      "loss": 1.4735,
      "step": 118
    },
    {
      "epoch": 0.007882882882882882,
      "grad_norm": 2.665492057800293,
      "learning_rate": 6.997924649557016e-06,
      "loss": 1.4848,
      "step": 119
    },
    {
      "epoch": 0.00794912559618442,
      "grad_norm": 2.5010271072387695,
      "learning_rate": 6.981057798703018e-06,
      "loss": 1.6203,
      "step": 120
    },
    {
      "epoch": 0.008015368309485956,
      "grad_norm": 3.238929271697998,
      "learning_rate": 6.964070871275567e-06,
      "loss": 1.574,
      "step": 121
    },
    {
      "epoch": 0.008081611022787493,
      "grad_norm": 3.292484998703003,
      "learning_rate": 6.946964551505619e-06,
      "loss": 1.6163,
      "step": 122
    },
    {
      "epoch": 0.00814785373608903,
      "grad_norm": NaN,
      "learning_rate": 6.946964551505619e-06,
      "loss": 1.5872,
      "step": 123
    },
    {
      "epoch": 0.008214096449390567,
      "grad_norm": 2.9584553241729736,
      "learning_rate": 6.929739528433243e-06,
      "loss": 1.59,
      "step": 124
    },
    {
      "epoch": 0.008280339162692104,
      "grad_norm": 1.7878342866897583,
      "learning_rate": 6.912396495879856e-06,
      "loss": 1.5795,
      "step": 125
    },
    {
      "epoch": 0.008346581875993641,
      "grad_norm": 1.8227241039276123,
      "learning_rate": 6.89493615242028e-06,
      "loss": 1.555,
      "step": 126
    },
    {
      "epoch": 0.008412824589295178,
      "grad_norm": 2.7873735427856445,
      "learning_rate": 6.877359201354604e-06,
      "loss": 1.4398,
      "step": 127
    },
    {
      "epoch": 0.008479067302596715,
      "grad_norm": 2.862027406692505,
      "learning_rate": 6.859666350679854e-06,
      "loss": 1.3716,
      "step": 128
    },
    {
      "epoch": 0.008545310015898252,
      "grad_norm": 2.653536558151245,
      "learning_rate": 6.8418583130614755e-06,
      "loss": 1.4388,
      "step": 129
    },
    {
      "epoch": 0.008611552729199789,
      "grad_norm": 3.4832093715667725,
      "learning_rate": 6.823935805804625e-06,
      "loss": 1.5748,
      "step": 130
    },
    {
      "epoch": 0.008677795442501324,
      "grad_norm": 2.699143886566162,
      "learning_rate": 6.805899550825285e-06,
      "loss": 1.4983,
      "step": 131
    },
    {
      "epoch": 0.008744038155802861,
      "grad_norm": 1.9413812160491943,
      "learning_rate": 6.787750274621174e-06,
      "loss": 1.5618,
      "step": 132
    },
    {
      "epoch": 0.008810280869104398,
      "grad_norm": 2.7908928394317627,
      "learning_rate": 6.7694887082424906e-06,
      "loss": 1.4284,
      "step": 133
    },
    {
      "epoch": 0.008876523582405935,
      "grad_norm": 1.7990185022354126,
      "learning_rate": 6.751115587262468e-06,
      "loss": 1.5476,
      "step": 134
    },
    {
      "epoch": 0.008942766295707472,
      "grad_norm": 1.7922929525375366,
      "learning_rate": 6.732631651747738e-06,
      "loss": 1.2363,
      "step": 135
    },
    {
      "epoch": 0.009009009009009009,
      "grad_norm": 1.8143641948699951,
      "learning_rate": 6.714037646228529e-06,
      "loss": 1.6721,
      "step": 136
    },
    {
      "epoch": 0.009075251722310546,
      "grad_norm": 2.1780283451080322,
      "learning_rate": 6.695334319668671e-06,
      "loss": 1.4645,
      "step": 137
    },
    {
      "epoch": 0.009141494435612083,
      "grad_norm": 2.496548891067505,
      "learning_rate": 6.676522425435432e-06,
      "loss": 1.3946,
      "step": 138
    },
    {
      "epoch": 0.00920773714891362,
      "grad_norm": 1.4481409788131714,
      "learning_rate": 6.657602721269169e-06,
      "loss": 1.4979,
      "step": 139
    },
    {
      "epoch": 0.009273979862215157,
      "grad_norm": 2.527867078781128,
      "learning_rate": 6.638575969252805e-06,
      "loss": 1.4871,
      "step": 140
    },
    {
      "epoch": 0.009340222575516694,
      "grad_norm": 2.6403379440307617,
      "learning_rate": 6.619442935781141e-06,
      "loss": 1.3464,
      "step": 141
    },
    {
      "epoch": 0.00940646528881823,
      "grad_norm": 2.256847381591797,
      "learning_rate": 6.600204391529969e-06,
      "loss": 1.1933,
      "step": 142
    },
    {
      "epoch": 0.009472708002119766,
      "grad_norm": 1.8411595821380615,
      "learning_rate": 6.580861111425051e-06,
      "loss": 1.3258,
      "step": 143
    },
    {
      "epoch": 0.009538950715421303,
      "grad_norm": 1.7218337059020996,
      "learning_rate": 6.561413874610889e-06,
      "loss": 1.298,
      "step": 144
    },
    {
      "epoch": 0.00960519342872284,
      "grad_norm": 1.7238742113113403,
      "learning_rate": 6.5418634644193444e-06,
      "loss": 1.5168,
      "step": 145
    },
    {
      "epoch": 0.009671436142024377,
      "grad_norm": 1.6658053398132324,
      "learning_rate": 6.52221066833809e-06,
      "loss": 1.3247,
      "step": 146
    },
    {
      "epoch": 0.009737678855325914,
      "grad_norm": 2.6103458404541016,
      "learning_rate": 6.502456277978885e-06,
      "loss": 1.4333,
      "step": 147
    },
    {
      "epoch": 0.00980392156862745,
      "grad_norm": 1.6054779291152954,
      "learning_rate": 6.4826010890456945e-06,
      "loss": 1.5075,
      "step": 148
    },
    {
      "epoch": 0.009870164281928988,
      "grad_norm": 1.3823165893554688,
      "learning_rate": 6.462645901302632e-06,
      "loss": 1.4773,
      "step": 149
    },
    {
      "epoch": 0.009936406995230525,
      "grad_norm": 1.5936436653137207,
      "learning_rate": 6.442591518541753e-06,
      "loss": 1.3436,
      "step": 150
    },
    {
      "epoch": 0.010002649708532062,
      "grad_norm": 2.018528699874878,
      "learning_rate": 6.422438748550666e-06,
      "loss": 1.3298,
      "step": 151
    },
    {
      "epoch": 0.010068892421833599,
      "grad_norm": 1.7952768802642822,
      "learning_rate": 6.402188403080012e-06,
      "loss": 1.3543,
      "step": 152
    },
    {
      "epoch": 0.010135135135135136,
      "grad_norm": 2.5104212760925293,
      "learning_rate": 6.381841297810752e-06,
      "loss": 1.3093,
      "step": 153
    },
    {
      "epoch": 0.010201377848436673,
      "grad_norm": 1.6849403381347656,
      "learning_rate": 6.361398252321319e-06,
      "loss": 1.3147,
      "step": 154
    },
    {
      "epoch": 0.01026762056173821,
      "grad_norm": 3.842738628387451,
      "learning_rate": 6.340860090054606e-06,
      "loss": 1.273,
      "step": 155
    },
    {
      "epoch": 0.010333863275039745,
      "grad_norm": 1.5389838218688965,
      "learning_rate": 6.3202276382847925e-06,
      "loss": 1.3782,
      "step": 156
    },
    {
      "epoch": 0.010400105988341282,
      "grad_norm": 2.232748508453369,
      "learning_rate": 6.299501728084029e-06,
      "loss": 1.2653,
      "step": 157
    },
    {
      "epoch": 0.010466348701642819,
      "grad_norm": 1.8020761013031006,
      "learning_rate": 6.2786831942889555e-06,
      "loss": 1.3698,
      "step": 158
    },
    {
      "epoch": 0.010532591414944356,
      "grad_norm": 2.1255156993865967,
      "learning_rate": 6.257772875467077e-06,
      "loss": 1.1725,
      "step": 159
    },
    {
      "epoch": 0.010598834128245893,
      "grad_norm": 1.5517891645431519,
      "learning_rate": 6.2367716138829865e-06,
      "loss": 1.2552,
      "step": 160
    },
    {
      "epoch": 0.01066507684154743,
      "grad_norm": 2.0337297916412354,
      "learning_rate": 6.215680255464441e-06,
      "loss": 1.5028,
      "step": 161
    },
    {
      "epoch": 0.010731319554848967,
      "grad_norm": 2.0723936557769775,
      "learning_rate": 6.1944996497682805e-06,
      "loss": 1.3595,
      "step": 162
    },
    {
      "epoch": 0.010797562268150504,
      "grad_norm": 1.5878738164901733,
      "learning_rate": 6.173230649946212e-06,
      "loss": 1.4805,
      "step": 163
    },
    {
      "epoch": 0.01086380498145204,
      "grad_norm": 2.0891520977020264,
      "learning_rate": 6.15187411271045e-06,
      "loss": 1.3097,
      "step": 164
    },
    {
      "epoch": 0.010930047694753578,
      "grad_norm": 1.901249885559082,
      "learning_rate": 6.130430898299199e-06,
      "loss": 1.3253,
      "step": 165
    },
    {
      "epoch": 0.010996290408055115,
      "grad_norm": 1.7920233011245728,
      "learning_rate": 6.108901870442009e-06,
      "loss": 1.2322,
      "step": 166
    },
    {
      "epoch": 0.011062533121356652,
      "grad_norm": 2.5755906105041504,
      "learning_rate": 6.087287896324984e-06,
      "loss": 1.2759,
      "step": 167
    },
    {
      "epoch": 0.011128775834658187,
      "grad_norm": 3.831387519836426,
      "learning_rate": 6.065589846555847e-06,
      "loss": 1.3743,
      "step": 168
    },
    {
      "epoch": 0.011195018547959724,
      "grad_norm": 1.4799463748931885,
      "learning_rate": 6.043808595128882e-06,
      "loss": 1.3908,
      "step": 169
    },
    {
      "epoch": 0.01126126126126126,
      "grad_norm": 1.379105567932129,
      "learning_rate": 6.021945019389719e-06,
      "loss": 1.2291,
      "step": 170
    },
    {
      "epoch": 0.011327503974562798,
      "grad_norm": 1.4653363227844238,
      "learning_rate": 6e-06,
      "loss": 1.2752,
      "step": 171
    },
    {
      "epoch": 0.011393746687864335,
      "grad_norm": 1.7727776765823364,
      "learning_rate": 5.977974420901907e-06,
      "loss": 1.3549,
      "step": 172
    },
    {
      "epoch": 0.011459989401165872,
      "grad_norm": 2.440699815750122,
      "learning_rate": 5.955869169282555e-06,
      "loss": 1.2569,
      "step": 173
    },
    {
      "epoch": 0.011526232114467409,
      "grad_norm": 1.3747639656066895,
      "learning_rate": 5.933685135538254e-06,
      "loss": 1.3064,
      "step": 174
    },
    {
      "epoch": 0.011592474827768946,
      "grad_norm": 2.218384027481079,
      "learning_rate": 5.9114232132386525e-06,
      "loss": 1.5589,
      "step": 175
    },
    {
      "epoch": 0.011658717541070483,
      "grad_norm": 3.011122465133667,
      "learning_rate": 5.889084299090731e-06,
      "loss": 1.4526,
      "step": 176
    },
    {
      "epoch": 0.01172496025437202,
      "grad_norm": 3.2744851112365723,
      "learning_rate": 5.866669292902695e-06,
      "loss": 1.4351,
      "step": 177
    },
    {
      "epoch": 0.011791202967673556,
      "grad_norm": 1.2164044380187988,
      "learning_rate": 5.844179097547725e-06,
      "loss": 1.3979,
      "step": 178
    },
    {
      "epoch": 0.011857445680975093,
      "grad_norm": 1.5288445949554443,
      "learning_rate": 5.821614618927612e-06,
      "loss": 1.2134,
      "step": 179
    },
    {
      "epoch": 0.01192368839427663,
      "grad_norm": 2.2040162086486816,
      "learning_rate": 5.798976765936263e-06,
      "loss": 1.1057,
      "step": 180
    },
    {
      "epoch": 0.011989931107578166,
      "grad_norm": 2.2756924629211426,
      "learning_rate": 5.7762664504230965e-06,
      "loss": 1.3256,
      "step": 181
    },
    {
      "epoch": 0.012056173820879703,
      "grad_norm": 2.204287528991699,
      "learning_rate": 5.753484587156309e-06,
      "loss": 1.3739,
      "step": 182
    },
    {
      "epoch": 0.01212241653418124,
      "grad_norm": 3.7600908279418945,
      "learning_rate": 5.730632093786033e-06,
      "loss": 1.1522,
      "step": 183
    },
    {
      "epoch": 0.012188659247482777,
      "grad_norm": 1.823350429534912,
      "learning_rate": 5.707709890807367e-06,
      "loss": 1.4389,
      "step": 184
    },
    {
      "epoch": 0.012254901960784314,
      "grad_norm": 2.0803017616271973,
      "learning_rate": 5.684718901523307e-06,
      "loss": 1.3768,
      "step": 185
    },
    {
      "epoch": 0.01232114467408585,
      "grad_norm": 3.3589093685150146,
      "learning_rate": 5.661660052007546e-06,
      "loss": 1.3031,
      "step": 186
    },
    {
      "epoch": 0.012387387387387387,
      "grad_norm": 2.364274263381958,
      "learning_rate": 5.6385342710671805e-06,
      "loss": 1.2982,
      "step": 187
    },
    {
      "epoch": 0.012453630100688924,
      "grad_norm": 1.8821864128112793,
      "learning_rate": 5.6153424902053e-06,
      "loss": 1.2083,
      "step": 188
    },
    {
      "epoch": 0.012519872813990461,
      "grad_norm": 1.421647310256958,
      "learning_rate": 5.59208564358345e-06,
      "loss": 1.2398,
      "step": 189
    },
    {
      "epoch": 0.012586115527291998,
      "grad_norm": 1.3718851804733276,
      "learning_rate": 5.568764667984022e-06,
      "loss": 1.2555,
      "step": 190
    },
    {
      "epoch": 0.012652358240593535,
      "grad_norm": 1.5927937030792236,
      "learning_rate": 5.545380502772514e-06,
      "loss": 1.49,
      "step": 191
    },
    {
      "epoch": 0.012718600953895072,
      "grad_norm": 3.1337080001831055,
      "learning_rate": 5.521934089859692e-06,
      "loss": 1.1812,
      "step": 192
    },
    {
      "epoch": 0.012784843667196608,
      "grad_norm": 1.3573287725448608,
      "learning_rate": 5.498426373663648e-06,
      "loss": 1.2803,
      "step": 193
    },
    {
      "epoch": 0.012851086380498145,
      "grad_norm": 1.7724398374557495,
      "learning_rate": 5.474858301071763e-06,
      "loss": 1.5124,
      "step": 194
    },
    {
      "epoch": 0.012917329093799682,
      "grad_norm": 2.0444514751434326,
      "learning_rate": 5.451230821402563e-06,
      "loss": 1.6455,
      "step": 195
    },
    {
      "epoch": 0.012983571807101218,
      "grad_norm": 1.3444105386734009,
      "learning_rate": 5.427544886367487e-06,
      "loss": 1.2388,
      "step": 196
    },
    {
      "epoch": 0.013049814520402755,
      "grad_norm": 1.6062041521072388,
      "learning_rate": 5.403801450032543e-06,
      "loss": 1.289,
      "step": 197
    },
    {
      "epoch": 0.013116057233704292,
      "grad_norm": 1.1947040557861328,
      "learning_rate": 5.380001468779882e-06,
      "loss": 1.2296,
      "step": 198
    },
    {
      "epoch": 0.01318229994700583,
      "grad_norm": 1.3763694763183594,
      "learning_rate": 5.356145901269282e-06,
      "loss": 1.3367,
      "step": 199
    },
    {
      "epoch": 0.013248542660307366,
      "grad_norm": 1.4768248796463013,
      "learning_rate": 5.3322357083995225e-06,
      "loss": 1.2805,
      "step": 200
    },
    {
      "epoch": 0.013314785373608903,
      "grad_norm": 1.224830150604248,
      "learning_rate": 5.308271853269687e-06,
      "loss": 1.2329,
      "step": 201
    },
    {
      "epoch": 0.01338102808691044,
      "grad_norm": 1.2063672542572021,
      "learning_rate": 5.284255301140363e-06,
      "loss": 1.3682,
      "step": 202
    },
    {
      "epoch": 0.013447270800211977,
      "grad_norm": 1.7131750583648682,
      "learning_rate": 5.260187019394771e-06,
      "loss": 1.3196,
      "step": 203
    },
    {
      "epoch": 0.013513513513513514,
      "grad_norm": 1.2872443199157715,
      "learning_rate": 5.236067977499789e-06,
      "loss": 1.1738,
      "step": 204
    },
    {
      "epoch": 0.01357975622681505,
      "grad_norm": 1.735216736793518,
      "learning_rate": 5.211899146966909e-06,
      "loss": 1.4084,
      "step": 205
    },
    {
      "epoch": 0.013645998940116586,
      "grad_norm": 1.9439547061920166,
      "learning_rate": 5.1876815013131e-06,
      "loss": 1.412,
      "step": 206
    },
    {
      "epoch": 0.013712241653418123,
      "grad_norm": 1.3556679487228394,
      "learning_rate": 5.163416016021596e-06,
      "loss": 1.2289,
      "step": 207
    },
    {
      "epoch": 0.01377848436671966,
      "grad_norm": 1.3972656726837158,
      "learning_rate": 5.139103668502609e-06,
      "loss": 1.4548,
      "step": 208
    },
    {
      "epoch": 0.013844727080021197,
      "grad_norm": 1.1724647283554077,
      "learning_rate": 5.114745438053951e-06,
      "loss": 1.244,
      "step": 209
    },
    {
      "epoch": 0.013910969793322734,
      "grad_norm": 3.1828484535217285,
      "learning_rate": 5.090342305821591e-06,
      "loss": 1.3225,
      "step": 210
    },
    {
      "epoch": 0.013977212506624271,
      "grad_norm": 2.1090409755706787,
      "learning_rate": 5.065895254760139e-06,
      "loss": 1.236,
      "step": 211
    },
    {
      "epoch": 0.014043455219925808,
      "grad_norm": 1.462581992149353,
      "learning_rate": 5.041405269593248e-06,
      "loss": 1.1058,
      "step": 212
    },
    {
      "epoch": 0.014109697933227345,
      "grad_norm": 1.267865777015686,
      "learning_rate": 5.0168733367739484e-06,
      "loss": 1.3343,
      "step": 213
    },
    {
      "epoch": 0.014175940646528882,
      "grad_norm": 2.536048173904419,
      "learning_rate": 4.992300444444916e-06,
      "loss": 1.0178,
      "step": 214
    },
    {
      "epoch": 0.01424218335983042,
      "grad_norm": 1.3153902292251587,
      "learning_rate": 4.96768758239867e-06,
      "loss": 1.3405,
      "step": 215
    },
    {
      "epoch": 0.014308426073131956,
      "grad_norm": 1.201095461845398,
      "learning_rate": 4.943035742037709e-06,
      "loss": 1.0434,
      "step": 216
    },
    {
      "epoch": 0.014374668786433493,
      "grad_norm": 1.4584801197052002,
      "learning_rate": 4.918345916334564e-06,
      "loss": 1.4437,
      "step": 217
    },
    {
      "epoch": 0.014440911499735028,
      "grad_norm": 2.2231831550598145,
      "learning_rate": 4.8936190997918165e-06,
      "loss": 1.327,
      "step": 218
    },
    {
      "epoch": 0.014507154213036565,
      "grad_norm": 1.788141131401062,
      "learning_rate": 4.868856288402031e-06,
      "loss": 1.3641,
      "step": 219
    },
    {
      "epoch": 0.014573396926338102,
      "grad_norm": 1.4066483974456787,
      "learning_rate": 4.844058479607639e-06,
      "loss": 1.2223,
      "step": 220
    },
    {
      "epoch": 0.01463963963963964,
      "grad_norm": 1.7918614149093628,
      "learning_rate": 4.819226672260763e-06,
      "loss": 1.2742,
      "step": 221
    },
    {
      "epoch": 0.014705882352941176,
      "grad_norm": 1.3476145267486572,
      "learning_rate": 4.794361866582981e-06,
      "loss": 1.2808,
      "step": 222
    },
    {
      "epoch": 0.014772125066242713,
      "grad_norm": 1.4770187139511108,
      "learning_rate": 4.769465064125044e-06,
      "loss": 1.3287,
      "step": 223
    },
    {
      "epoch": 0.01483836777954425,
      "grad_norm": 1.3795844316482544,
      "learning_rate": 4.74453726772652e-06,
      "loss": 1.3302,
      "step": 224
    },
    {
      "epoch": 0.014904610492845787,
      "grad_norm": 1.9444994926452637,
      "learning_rate": 4.719579481475415e-06,
      "loss": 1.1362,
      "step": 225
    },
    {
      "epoch": 0.014970853206147324,
      "grad_norm": 1.6303532123565674,
      "learning_rate": 4.694592710667722e-06,
      "loss": 1.0958,
      "step": 226
    },
    {
      "epoch": 0.015037095919448861,
      "grad_norm": 1.5297889709472656,
      "learning_rate": 4.669577961766923e-06,
      "loss": 1.4482,
      "step": 227
    },
    {
      "epoch": 0.015103338632750398,
      "grad_norm": 1.9993454217910767,
      "learning_rate": 4.644536242363457e-06,
      "loss": 1.055,
      "step": 228
    },
    {
      "epoch": 0.015169581346051935,
      "grad_norm": 1.3872379064559937,
      "learning_rate": 4.6194685611341295e-06,
      "loss": 1.2687,
      "step": 229
    },
    {
      "epoch": 0.01523582405935347,
      "grad_norm": 1.7411428689956665,
      "learning_rate": 4.594375927801486e-06,
      "loss": 1.1685,
      "step": 230
    },
    {
      "epoch": 0.015302066772655007,
      "grad_norm": 2.000112295150757,
      "learning_rate": 4.569259353093141e-06,
      "loss": 1.3932,
      "step": 231
    },
    {
      "epoch": 0.015368309485956544,
      "grad_norm": 1.565459966659546,
      "learning_rate": 4.544119848701057e-06,
      "loss": 1.2877,
      "step": 232
    },
    {
      "epoch": 0.015434552199258081,
      "grad_norm": 1.9830831289291382,
      "learning_rate": 4.518958427240807e-06,
      "loss": 1.3507,
      "step": 233
    },
    {
      "epoch": 0.015500794912559618,
      "grad_norm": 1.4546399116516113,
      "learning_rate": 4.493776102210779e-06,
      "loss": 1.3231,
      "step": 234
    },
    {
      "epoch": 0.015567037625861155,
      "grad_norm": 1.391022801399231,
      "learning_rate": 4.468573887951353e-06,
      "loss": 1.1008,
      "step": 235
    },
    {
      "epoch": 0.01563328033916269,
      "grad_norm": 3.415111541748047,
      "learning_rate": 4.443352799604044e-06,
      "loss": 1.2195,
      "step": 236
    },
    {
      "epoch": 0.015699523052464227,
      "grad_norm": 1.706656813621521,
      "learning_rate": 4.418113853070614e-06,
      "loss": 1.2021,
      "step": 237
    },
    {
      "epoch": 0.015765765765765764,
      "grad_norm": 1.4041047096252441,
      "learning_rate": 4.3928580649721484e-06,
      "loss": 1.2098,
      "step": 238
    },
    {
      "epoch": 0.0158320084790673,
      "grad_norm": 1.3230684995651245,
      "learning_rate": 4.36758645260811e-06,
      "loss": 1.4079,
      "step": 239
    },
    {
      "epoch": 0.01589825119236884,
      "grad_norm": 2.044865608215332,
      "learning_rate": 4.342300033915359e-06,
      "loss": 1.1009,
      "step": 240
    },
    {
      "epoch": 0.015964493905670375,
      "grad_norm": 1.38749098777771,
      "learning_rate": 4.3169998274271535e-06,
      "loss": 1.3245,
      "step": 241
    },
    {
      "epoch": 0.016030736618971912,
      "grad_norm": 1.4479832649230957,
      "learning_rate": 4.2916868522321235e-06,
      "loss": 1.1982,
      "step": 242
    },
    {
      "epoch": 0.01609697933227345,
      "grad_norm": 1.4716013669967651,
      "learning_rate": 4.266362127933216e-06,
      "loss": 1.2045,
      "step": 243
    },
    {
      "epoch": 0.016163222045574986,
      "grad_norm": 1.4030916690826416,
      "learning_rate": 4.241026674606634e-06,
      "loss": 1.1613,
      "step": 244
    },
    {
      "epoch": 0.016229464758876523,
      "grad_norm": 1.382049322128296,
      "learning_rate": 4.215681512760744e-06,
      "loss": 1.2539,
      "step": 245
    },
    {
      "epoch": 0.01629570747217806,
      "grad_norm": 2.0226855278015137,
      "learning_rate": 4.1903276632949695e-06,
      "loss": 1.1758,
      "step": 246
    },
    {
      "epoch": 0.016361950185479597,
      "grad_norm": 1.5458041429519653,
      "learning_rate": 4.164966147458668e-06,
      "loss": 1.1003,
      "step": 247
    },
    {
      "epoch": 0.016428192898781134,
      "grad_norm": 1.4935489892959595,
      "learning_rate": 4.139597986810004e-06,
      "loss": 1.2309,
      "step": 248
    },
    {
      "epoch": 0.01649443561208267,
      "grad_norm": 1.4075912237167358,
      "learning_rate": 4.1142242031747846e-06,
      "loss": 1.067,
      "step": 249
    },
    {
      "epoch": 0.016560678325384208,
      "grad_norm": 1.9693472385406494,
      "learning_rate": 4.088845818605317e-06,
      "loss": 1.3112,
      "step": 250
    },
    {
      "epoch": 0.016626921038685745,
      "grad_norm": 1.9526431560516357,
      "learning_rate": 4.063463855339232e-06,
      "loss": 1.4849,
      "step": 251
    },
    {
      "epoch": 0.016693163751987282,
      "grad_norm": 2.3290228843688965,
      "learning_rate": 4.038079335758307e-06,
      "loss": 1.4803,
      "step": 252
    },
    {
      "epoch": 0.01675940646528882,
      "grad_norm": 4.954296588897705,
      "learning_rate": 4.012693282347289e-06,
      "loss": 1.1239,
      "step": 253
    },
    {
      "epoch": 0.016825649178590356,
      "grad_norm": 1.8906598091125488,
      "learning_rate": 3.9873067176527105e-06,
      "loss": 1.5194,
      "step": 254
    },
    {
      "epoch": 0.016891891891891893,
      "grad_norm": 1.4072355031967163,
      "learning_rate": 3.961920664241693e-06,
      "loss": 1.2513,
      "step": 255
    },
    {
      "epoch": 0.01695813460519343,
      "grad_norm": 1.3615152835845947,
      "learning_rate": 3.936536144660768e-06,
      "loss": 1.1692,
      "step": 256
    },
    {
      "epoch": 0.017024377318494967,
      "grad_norm": 1.7175419330596924,
      "learning_rate": 3.911154181394682e-06,
      "loss": 1.3549,
      "step": 257
    },
    {
      "epoch": 0.017090620031796504,
      "grad_norm": 1.2949954271316528,
      "learning_rate": 3.885775796825215e-06,
      "loss": 1.2295,
      "step": 258
    },
    {
      "epoch": 0.01715686274509804,
      "grad_norm": 1.878619909286499,
      "learning_rate": 3.860402013189997e-06,
      "loss": 1.1807,
      "step": 259
    },
    {
      "epoch": 0.017223105458399578,
      "grad_norm": 1.796268105506897,
      "learning_rate": 3.835033852541331e-06,
      "loss": 0.9546,
      "step": 260
    },
    {
      "epoch": 0.01728934817170111,
      "grad_norm": 1.6335581541061401,
      "learning_rate": 3.8096723367050306e-06,
      "loss": 1.2619,
      "step": 261
    },
    {
      "epoch": 0.017355590885002648,
      "grad_norm": 1.4860516786575317,
      "learning_rate": 3.7843184872392565e-06,
      "loss": 1.1616,
      "step": 262
    },
    {
      "epoch": 0.017421833598304185,
      "grad_norm": 1.599025845527649,
      "learning_rate": 3.7589733253933664e-06,
      "loss": 1.5676,
      "step": 263
    },
    {
      "epoch": 0.017488076311605722,
      "grad_norm": 1.3899834156036377,
      "learning_rate": 3.733637872066784e-06,
      "loss": 1.4443,
      "step": 264
    },
    {
      "epoch": 0.01755431902490726,
      "grad_norm": 2.0901875495910645,
      "learning_rate": 3.7083131477678774e-06,
      "loss": 1.2153,
      "step": 265
    },
    {
      "epoch": 0.017620561738208796,
      "grad_norm": 2.0216593742370605,
      "learning_rate": 3.6830001725728453e-06,
      "loss": 1.2336,
      "step": 266
    },
    {
      "epoch": 0.017686804451510333,
      "grad_norm": 1.5134528875350952,
      "learning_rate": 3.657699966084642e-06,
      "loss": 1.1403,
      "step": 267
    },
    {
      "epoch": 0.01775304716481187,
      "grad_norm": 1.2931060791015625,
      "learning_rate": 3.6324135473918902e-06,
      "loss": 1.2235,
      "step": 268
    },
    {
      "epoch": 0.017819289878113407,
      "grad_norm": 1.2610539197921753,
      "learning_rate": 3.607141935027851e-06,
      "loss": 1.2887,
      "step": 269
    },
    {
      "epoch": 0.017885532591414944,
      "grad_norm": 1.340107798576355,
      "learning_rate": 3.5818861469293865e-06,
      "loss": 1.3224,
      "step": 270
    },
    {
      "epoch": 0.01795177530471648,
      "grad_norm": 1.6251693964004517,
      "learning_rate": 3.556647200395956e-06,
      "loss": 1.3576,
      "step": 271
    },
    {
      "epoch": 0.018018018018018018,
      "grad_norm": 1.6691228151321411,
      "learning_rate": 3.531426112048647e-06,
      "loss": 1.3468,
      "step": 272
    },
    {
      "epoch": 0.018084260731319555,
      "grad_norm": 1.4249697923660278,
      "learning_rate": 3.506223897789221e-06,
      "loss": 1.2296,
      "step": 273
    },
    {
      "epoch": 0.018150503444621092,
      "grad_norm": 1.3582615852355957,
      "learning_rate": 3.4810415727591925e-06,
      "loss": 1.0313,
      "step": 274
    },
    {
      "epoch": 0.01821674615792263,
      "grad_norm": 1.7276455163955688,
      "learning_rate": 3.455880151298944e-06,
      "loss": 1.3403,
      "step": 275
    },
    {
      "epoch": 0.018282988871224166,
      "grad_norm": 2.3855128288269043,
      "learning_rate": 3.4307406469068596e-06,
      "loss": 1.1423,
      "step": 276
    },
    {
      "epoch": 0.018349231584525703,
      "grad_norm": 1.6848241090774536,
      "learning_rate": 3.4056240721985137e-06,
      "loss": 1.287,
      "step": 277
    },
    {
      "epoch": 0.01841547429782724,
      "grad_norm": 1.5040799379348755,
      "learning_rate": 3.380531438865871e-06,
      "loss": 1.1317,
      "step": 278
    },
    {
      "epoch": 0.018481717011128777,
      "grad_norm": 1.4276741743087769,
      "learning_rate": 3.355463757636544e-06,
      "loss": 1.1692,
      "step": 279
    },
    {
      "epoch": 0.018547959724430314,
      "grad_norm": 1.6240299940109253,
      "learning_rate": 3.3304220382330772e-06,
      "loss": 1.2061,
      "step": 280
    },
    {
      "epoch": 0.01861420243773185,
      "grad_norm": 2.188204526901245,
      "learning_rate": 3.3054072893322785e-06,
      "loss": 1.2133,
      "step": 281
    },
    {
      "epoch": 0.018680445151033388,
      "grad_norm": 1.633481502532959,
      "learning_rate": 3.2804205185245845e-06,
      "loss": 1.2428,
      "step": 282
    },
    {
      "epoch": 0.018746687864334925,
      "grad_norm": 1.2672547101974487,
      "learning_rate": 3.2554627322734797e-06,
      "loss": 1.3098,
      "step": 283
    },
    {
      "epoch": 0.01881293057763646,
      "grad_norm": 1.2917547225952148,
      "learning_rate": 3.2305349358749575e-06,
      "loss": 1.3687,
      "step": 284
    },
    {
      "epoch": 0.018879173290938,
      "grad_norm": 1.2951282262802124,
      "learning_rate": 3.205638133417019e-06,
      "loss": 1.2052,
      "step": 285
    },
    {
      "epoch": 0.018945416004239532,
      "grad_norm": 1.6143862009048462,
      "learning_rate": 3.180773327739238e-06,
      "loss": 1.3212,
      "step": 286
    },
    {
      "epoch": 0.01901165871754107,
      "grad_norm": 1.8697991371154785,
      "learning_rate": 3.1559415203923613e-06,
      "loss": 1.3808,
      "step": 287
    },
    {
      "epoch": 0.019077901430842606,
      "grad_norm": 1.6682116985321045,
      "learning_rate": 3.1311437115979695e-06,
      "loss": 1.3616,
      "step": 288
    },
    {
      "epoch": 0.019144144144144143,
      "grad_norm": 1.7266913652420044,
      "learning_rate": 3.106380900208183e-06,
      "loss": 1.397,
      "step": 289
    },
    {
      "epoch": 0.01921038685744568,
      "grad_norm": 1.3861616849899292,
      "learning_rate": 3.081654083665435e-06,
      "loss": 1.2409,
      "step": 290
    },
    {
      "epoch": 0.019276629570747217,
      "grad_norm": 1.1060028076171875,
      "learning_rate": 3.0569642579622904e-06,
      "loss": 1.3521,
      "step": 291
    },
    {
      "epoch": 0.019342872284048754,
      "grad_norm": 1.2173022031784058,
      "learning_rate": 3.0323124176013297e-06,
      "loss": 1.3062,
      "step": 292
    },
    {
      "epoch": 0.01940911499735029,
      "grad_norm": 1.6997581720352173,
      "learning_rate": 3.007699555555086e-06,
      "loss": 1.2868,
      "step": 293
    },
    {
      "epoch": 0.019475357710651828,
      "grad_norm": 1.3532530069351196,
      "learning_rate": 2.983126663226053e-06,
      "loss": 1.3182,
      "step": 294
    },
    {
      "epoch": 0.019541600423953365,
      "grad_norm": 1.4078339338302612,
      "learning_rate": 2.9585947304067517e-06,
      "loss": 1.3424,
      "step": 295
    },
    {
      "epoch": 0.0196078431372549,
      "grad_norm": 2.1838490962982178,
      "learning_rate": 2.9341047452398604e-06,
      "loss": 1.2265,
      "step": 296
    },
    {
      "epoch": 0.01967408585055644,
      "grad_norm": 1.3118315935134888,
      "learning_rate": 2.909657694178409e-06,
      "loss": 1.0322,
      "step": 297
    },
    {
      "epoch": 0.019740328563857976,
      "grad_norm": 1.4210970401763916,
      "learning_rate": 2.8852545619460494e-06,
      "loss": 1.2027,
      "step": 298
    },
    {
      "epoch": 0.019806571277159513,
      "grad_norm": 1.6617122888565063,
      "learning_rate": 2.860896331497391e-06,
      "loss": 1.0512,
      "step": 299
    },
    {
      "epoch": 0.01987281399046105,
      "grad_norm": 1.3214839696884155,
      "learning_rate": 2.8365839839784036e-06,
      "loss": 0.991,
      "step": 300
    },
    {
      "epoch": 0.019939056703762587,
      "grad_norm": 1.2167725563049316,
      "learning_rate": 2.812318498686902e-06,
      "loss": 1.0441,
      "step": 301
    },
    {
      "epoch": 0.020005299417064124,
      "grad_norm": 2.257091522216797,
      "learning_rate": 2.788100853033091e-06,
      "loss": 1.3082,
      "step": 302
    },
    {
      "epoch": 0.02007154213036566,
      "grad_norm": 1.6888138055801392,
      "learning_rate": 2.7639320225002105e-06,
      "loss": 1.3066,
      "step": 303
    },
    {
      "epoch": 0.020137784843667197,
      "grad_norm": 1.6195621490478516,
      "learning_rate": 2.7398129806052293e-06,
      "loss": 1.1968,
      "step": 304
    },
    {
      "epoch": 0.020204027556968734,
      "grad_norm": 1.6046000719070435,
      "learning_rate": 2.7157446988596366e-06,
      "loss": 1.1624,
      "step": 305
    },
    {
      "epoch": 0.02027027027027027,
      "grad_norm": 1.3258249759674072,
      "learning_rate": 2.6917281467303133e-06,
      "loss": 1.4881,
      "step": 306
    },
    {
      "epoch": 0.02033651298357181,
      "grad_norm": 1.6555423736572266,
      "learning_rate": 2.667764291600477e-06,
      "loss": 0.9635,
      "step": 307
    },
    {
      "epoch": 0.020402755696873345,
      "grad_norm": 1.864563226699829,
      "learning_rate": 2.643854098730717e-06,
      "loss": 1.4541,
      "step": 308
    },
    {
      "epoch": 0.020468998410174882,
      "grad_norm": 1.2398879528045654,
      "learning_rate": 2.6199985312201182e-06,
      "loss": 1.203,
      "step": 309
    },
    {
      "epoch": 0.02053524112347642,
      "grad_norm": 1.4347963333129883,
      "learning_rate": 2.5961985499674585e-06,
      "loss": 1.1352,
      "step": 310
    },
    {
      "epoch": 0.020601483836777953,
      "grad_norm": 1.9964945316314697,
      "learning_rate": 2.5724551136325126e-06,
      "loss": 1.2777,
      "step": 311
    },
    {
      "epoch": 0.02066772655007949,
      "grad_norm": 1.9126611948013306,
      "learning_rate": 2.5487691785974363e-06,
      "loss": 1.0851,
      "step": 312
    },
    {
      "epoch": 0.020733969263381027,
      "grad_norm": 1.4650094509124756,
      "learning_rate": 2.5251416989282376e-06,
      "loss": 1.35,
      "step": 313
    },
    {
      "epoch": 0.020800211976682564,
      "grad_norm": 1.8302819728851318,
      "learning_rate": 2.5015736263363514e-06,
      "loss": 1.1881,
      "step": 314
    },
    {
      "epoch": 0.0208664546899841,
      "grad_norm": 1.6389906406402588,
      "learning_rate": 2.478065910140308e-06,
      "loss": 1.2417,
      "step": 315
    },
    {
      "epoch": 0.020932697403285638,
      "grad_norm": 1.7632144689559937,
      "learning_rate": 2.4546194972274847e-06,
      "loss": 1.2416,
      "step": 316
    },
    {
      "epoch": 0.020998940116587175,
      "grad_norm": 1.511499047279358,
      "learning_rate": 2.4312353320159775e-06,
      "loss": 1.1611,
      "step": 317
    },
    {
      "epoch": 0.02106518282988871,
      "grad_norm": 1.5187915563583374,
      "learning_rate": 2.407914356416552e-06,
      "loss": 1.4229,
      "step": 318
    },
    {
      "epoch": 0.02113142554319025,
      "grad_norm": 1.3169023990631104,
      "learning_rate": 2.3846575097947013e-06,
      "loss": 0.9345,
      "step": 319
    },
    {
      "epoch": 0.021197668256491786,
      "grad_norm": 1.2842246294021606,
      "learning_rate": 2.3614657289328187e-06,
      "loss": 1.2915,
      "step": 320
    },
    {
      "epoch": 0.021263910969793322,
      "grad_norm": 1.3851051330566406,
      "learning_rate": 2.3383399479924544e-06,
      "loss": 1.2818,
      "step": 321
    },
    {
      "epoch": 0.02133015368309486,
      "grad_norm": 1.7056126594543457,
      "learning_rate": 2.3152810984766935e-06,
      "loss": 1.2732,
      "step": 322
    },
    {
      "epoch": 0.021396396396396396,
      "grad_norm": 1.724444031715393,
      "learning_rate": 2.292290109192633e-06,
      "loss": 1.0232,
      "step": 323
    },
    {
      "epoch": 0.021462639109697933,
      "grad_norm": 1.3469338417053223,
      "learning_rate": 2.269367906213966e-06,
      "loss": 1.3024,
      "step": 324
    },
    {
      "epoch": 0.02152888182299947,
      "grad_norm": 1.5174922943115234,
      "learning_rate": 2.2465154128436897e-06,
      "loss": 1.3031,
      "step": 325
    },
    {
      "epoch": 0.021595124536301007,
      "grad_norm": 1.4168879985809326,
      "learning_rate": 2.223733549576903e-06,
      "loss": 1.0935,
      "step": 326
    },
    {
      "epoch": 0.021661367249602544,
      "grad_norm": 1.800870656967163,
      "learning_rate": 2.2010232340637375e-06,
      "loss": 1.2435,
      "step": 327
    },
    {
      "epoch": 0.02172760996290408,
      "grad_norm": 1.2866805791854858,
      "learning_rate": 2.178385381072389e-06,
      "loss": 1.2943,
      "step": 328
    },
    {
      "epoch": 0.021793852676205618,
      "grad_norm": 1.3019936084747314,
      "learning_rate": 2.1558209024522758e-06,
      "loss": 1.2858,
      "step": 329
    },
    {
      "epoch": 0.021860095389507155,
      "grad_norm": 1.4852832555770874,
      "learning_rate": 2.133330707097305e-06,
      "loss": 1.3392,
      "step": 330
    },
    {
      "epoch": 0.021926338102808692,
      "grad_norm": 2.0428571701049805,
      "learning_rate": 2.1109157009092697e-06,
      "loss": 1.2894,
      "step": 331
    },
    {
      "epoch": 0.02199258081611023,
      "grad_norm": 1.2568470239639282,
      "learning_rate": 2.088576786761348e-06,
      "loss": 1.1727,
      "step": 332
    },
    {
      "epoch": 0.022058823529411766,
      "grad_norm": 1.3629928827285767,
      "learning_rate": 2.066314864461744e-06,
      "loss": 1.1948,
      "step": 333
    },
    {
      "epoch": 0.022125066242713303,
      "grad_norm": 1.4920531511306763,
      "learning_rate": 2.044130830717445e-06,
      "loss": 1.3161,
      "step": 334
    },
    {
      "epoch": 0.02219130895601484,
      "grad_norm": 1.2297852039337158,
      "learning_rate": 2.022025579098093e-06,
      "loss": 1.2762,
      "step": 335
    },
    {
      "epoch": 0.022257551669316374,
      "grad_norm": 2.0344085693359375,
      "learning_rate": 2.0000000000000008e-06,
      "loss": 1.0597,
      "step": 336
    },
    {
      "epoch": 0.02232379438261791,
      "grad_norm": 1.6762233972549438,
      "learning_rate": 1.9780549806102825e-06,
      "loss": 1.4308,
      "step": 337
    },
    {
      "epoch": 0.022390037095919448,
      "grad_norm": 1.475631833076477,
      "learning_rate": 1.956191404871118e-06,
      "loss": 1.3064,
      "step": 338
    },
    {
      "epoch": 0.022456279809220984,
      "grad_norm": 1.4772073030471802,
      "learning_rate": 1.9344101534441527e-06,
      "loss": 1.1896,
      "step": 339
    },
    {
      "epoch": 0.02252252252252252,
      "grad_norm": 2.68656849861145,
      "learning_rate": 1.912712103675017e-06,
      "loss": 1.2935,
      "step": 340
    },
    {
      "epoch": 0.02258876523582406,
      "grad_norm": 1.4358928203582764,
      "learning_rate": 1.89109812955799e-06,
      "loss": 1.1697,
      "step": 341
    },
    {
      "epoch": 0.022655007949125595,
      "grad_norm": 1.623427391052246,
      "learning_rate": 1.8695691017008004e-06,
      "loss": 1.1877,
      "step": 342
    },
    {
      "epoch": 0.022721250662427132,
      "grad_norm": 1.886601448059082,
      "learning_rate": 1.8481258872895503e-06,
      "loss": 1.1573,
      "step": 343
    },
    {
      "epoch": 0.02278749337572867,
      "grad_norm": 1.7660505771636963,
      "learning_rate": 1.8267693500537887e-06,
      "loss": 1.3136,
      "step": 344
    },
    {
      "epoch": 0.022853736089030206,
      "grad_norm": 1.5523817539215088,
      "learning_rate": 1.8055003502317212e-06,
      "loss": 1.2552,
      "step": 345
    },
    {
      "epoch": 0.022919978802331743,
      "grad_norm": 2.0625879764556885,
      "learning_rate": 1.7843197445355589e-06,
      "loss": 1.1959,
      "step": 346
    },
    {
      "epoch": 0.02298622151563328,
      "grad_norm": 1.4445204734802246,
      "learning_rate": 1.7632283861170131e-06,
      "loss": 1.3423,
      "step": 347
    },
    {
      "epoch": 0.023052464228934817,
      "grad_norm": 2.4486324787139893,
      "learning_rate": 1.742227124532924e-06,
      "loss": 1.1887,
      "step": 348
    },
    {
      "epoch": 0.023118706942236354,
      "grad_norm": 1.6809461116790771,
      "learning_rate": 1.7213168057110448e-06,
      "loss": 1.4842,
      "step": 349
    },
    {
      "epoch": 0.02318494965553789,
      "grad_norm": 1.3646929264068604,
      "learning_rate": 1.700498271915971e-06,
      "loss": 1.241,
      "step": 350
    }
  ],
  "logging_steps": 1,
  "max_steps": 500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.285441012175667e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
