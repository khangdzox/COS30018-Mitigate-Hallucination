{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.010598834128245893,
  "eval_steps": 500,
  "global_step": 160,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 6.624271330153684e-05,
      "grad_norm": 1.3809800148010254,
      "learning_rate": 1.6e-06,
      "loss": 2.2689,
      "step": 1
    },
    {
      "epoch": 0.00013248542660307367,
      "grad_norm": 1.6996265649795532,
      "learning_rate": 3.2e-06,
      "loss": 2.3392,
      "step": 2
    },
    {
      "epoch": 0.0001987281399046105,
      "grad_norm": 1.5995228290557861,
      "learning_rate": 4.8e-06,
      "loss": 2.327,
      "step": 3
    },
    {
      "epoch": 0.00026497085320614734,
      "grad_norm": 1.2807488441467285,
      "learning_rate": 6.4e-06,
      "loss": 2.2282,
      "step": 4
    },
    {
      "epoch": 0.00033121356650768415,
      "grad_norm": 1.7918407917022705,
      "learning_rate": 8e-06,
      "loss": 2.3296,
      "step": 5
    },
    {
      "epoch": 0.000397456279809221,
      "grad_norm": 2.1889030933380127,
      "learning_rate": 7.999919440291625e-06,
      "loss": 2.4252,
      "step": 6
    },
    {
      "epoch": 0.0004636989931107578,
      "grad_norm": 1.4946634769439697,
      "learning_rate": 7.999677764411437e-06,
      "loss": 2.2345,
      "step": 7
    },
    {
      "epoch": 0.0005299417064122947,
      "grad_norm": 1.4097577333450317,
      "learning_rate": 7.999274982094103e-06,
      "loss": 2.2835,
      "step": 8
    },
    {
      "epoch": 0.0005961844197138314,
      "grad_norm": 1.5491282939910889,
      "learning_rate": 7.998711109563636e-06,
      "loss": 2.3787,
      "step": 9
    },
    {
      "epoch": 0.0006624271330153683,
      "grad_norm": 1.5071535110473633,
      "learning_rate": 7.99798616953274e-06,
      "loss": 2.084,
      "step": 10
    },
    {
      "epoch": 0.0007286698463169052,
      "grad_norm": 1.3594766855239868,
      "learning_rate": 7.997100191201894e-06,
      "loss": 2.2995,
      "step": 11
    },
    {
      "epoch": 0.000794912559618442,
      "grad_norm": 1.4487510919570923,
      "learning_rate": 7.996053210258175e-06,
      "loss": 2.5685,
      "step": 12
    },
    {
      "epoch": 0.0008611552729199788,
      "grad_norm": 1.9860165119171143,
      "learning_rate": 7.994845268873824e-06,
      "loss": 2.5732,
      "step": 13
    },
    {
      "epoch": 0.0009273979862215156,
      "grad_norm": 1.3641774654388428,
      "learning_rate": 7.993476415704541e-06,
      "loss": 2.2761,
      "step": 14
    },
    {
      "epoch": 0.0009936406995230524,
      "grad_norm": 1.6011816263198853,
      "learning_rate": 7.991946705887537e-06,
      "loss": 2.2512,
      "step": 15
    },
    {
      "epoch": 0.0010598834128245894,
      "grad_norm": 1.6379209756851196,
      "learning_rate": 7.990256201039296e-06,
      "loss": 2.3967,
      "step": 16
    },
    {
      "epoch": 0.0011261261261261261,
      "grad_norm": 1.312429428100586,
      "learning_rate": 7.988404969253109e-06,
      "loss": 2.0369,
      "step": 17
    },
    {
      "epoch": 0.0011923688394276629,
      "grad_norm": 1.5695819854736328,
      "learning_rate": 7.986393085096323e-06,
      "loss": 2.2738,
      "step": 18
    },
    {
      "epoch": 0.0012586115527291998,
      "grad_norm": 1.7151944637298584,
      "learning_rate": 7.984220629607335e-06,
      "loss": 2.2317,
      "step": 19
    },
    {
      "epoch": 0.0013248542660307366,
      "grad_norm": 1.3730355501174927,
      "learning_rate": 7.981887690292338e-06,
      "loss": 2.2022,
      "step": 20
    },
    {
      "epoch": 0.0013910969793322733,
      "grad_norm": 1.6174428462982178,
      "learning_rate": 7.979394361121788e-06,
      "loss": 2.4936,
      "step": 21
    },
    {
      "epoch": 0.0014573396926338103,
      "grad_norm": 1.417569637298584,
      "learning_rate": 7.976740742526617e-06,
      "loss": 2.1494,
      "step": 22
    },
    {
      "epoch": 0.001523582405935347,
      "grad_norm": 1.4038379192352295,
      "learning_rate": 7.973926941394201e-06,
      "loss": 2.2114,
      "step": 23
    },
    {
      "epoch": 0.001589825119236884,
      "grad_norm": 1.4127357006072998,
      "learning_rate": 7.970953071064033e-06,
      "loss": 2.2311,
      "step": 24
    },
    {
      "epoch": 0.0016560678325384208,
      "grad_norm": 1.5051912069320679,
      "learning_rate": 7.96781925132318e-06,
      "loss": 2.215,
      "step": 25
    },
    {
      "epoch": 0.0017223105458399576,
      "grad_norm": 1.38594651222229,
      "learning_rate": 7.964525608401445e-06,
      "loss": 2.1734,
      "step": 26
    },
    {
      "epoch": 0.0017885532591414945,
      "grad_norm": 1.5026637315750122,
      "learning_rate": 7.96107227496628e-06,
      "loss": 2.256,
      "step": 27
    },
    {
      "epoch": 0.0018547959724430313,
      "grad_norm": 1.3919044733047485,
      "learning_rate": 7.957459390117459e-06,
      "loss": 2.1654,
      "step": 28
    },
    {
      "epoch": 0.001921038685744568,
      "grad_norm": 1.4173874855041504,
      "learning_rate": 7.953687099381448e-06,
      "loss": 2.3061,
      "step": 29
    },
    {
      "epoch": 0.001987281399046105,
      "grad_norm": 1.6002627611160278,
      "learning_rate": 7.949755554705577e-06,
      "loss": 2.2258,
      "step": 30
    },
    {
      "epoch": 0.0020535241123476418,
      "grad_norm": 1.5717726945877075,
      "learning_rate": 7.945664914451887e-06,
      "loss": 2.2467,
      "step": 31
    },
    {
      "epoch": 0.0021197668256491787,
      "grad_norm": 1.6920794248580933,
      "learning_rate": 7.941415343390771e-06,
      "loss": 2.1356,
      "step": 32
    },
    {
      "epoch": 0.0021860095389507153,
      "grad_norm": 1.4630272388458252,
      "learning_rate": 7.937007012694335e-06,
      "loss": 2.2694,
      "step": 33
    },
    {
      "epoch": 0.0022522522522522522,
      "grad_norm": 1.5543068647384644,
      "learning_rate": 7.932440099929493e-06,
      "loss": 2.2479,
      "step": 34
    },
    {
      "epoch": 0.002318494965553789,
      "grad_norm": 1.2894253730773926,
      "learning_rate": 7.927714789050827e-06,
      "loss": 2.0509,
      "step": 35
    },
    {
      "epoch": 0.0023847376788553257,
      "grad_norm": 1.5814480781555176,
      "learning_rate": 7.922831270393169e-06,
      "loss": 2.1374,
      "step": 36
    },
    {
      "epoch": 0.0024509803921568627,
      "grad_norm": 1.5754947662353516,
      "learning_rate": 7.917789740663941e-06,
      "loss": 1.9903,
      "step": 37
    },
    {
      "epoch": 0.0025172231054583997,
      "grad_norm": 1.5083675384521484,
      "learning_rate": 7.912590402935222e-06,
      "loss": 2.2261,
      "step": 38
    },
    {
      "epoch": 0.0025834658187599362,
      "grad_norm": 1.5683575868606567,
      "learning_rate": 7.90723346663558e-06,
      "loss": 2.1183,
      "step": 39
    },
    {
      "epoch": 0.002649708532061473,
      "grad_norm": 1.5738838911056519,
      "learning_rate": 7.901719147541628e-06,
      "loss": 2.2268,
      "step": 40
    },
    {
      "epoch": 0.00271595124536301,
      "grad_norm": 1.6617158651351929,
      "learning_rate": 7.896047667769334e-06,
      "loss": 2.1754,
      "step": 41
    },
    {
      "epoch": 0.0027821939586645467,
      "grad_norm": 1.6001683473587036,
      "learning_rate": 7.890219255765076e-06,
      "loss": 1.8818,
      "step": 42
    },
    {
      "epoch": 0.0028484366719660837,
      "grad_norm": 1.7564657926559448,
      "learning_rate": 7.88423414629644e-06,
      "loss": 2.0247,
      "step": 43
    },
    {
      "epoch": 0.0029146793852676206,
      "grad_norm": 1.3842233419418335,
      "learning_rate": 7.878092580442764e-06,
      "loss": 1.9519,
      "step": 44
    },
    {
      "epoch": 0.0029809220985691576,
      "grad_norm": 1.5279793739318848,
      "learning_rate": 7.871794805585425e-06,
      "loss": 2.1173,
      "step": 45
    },
    {
      "epoch": 0.003047164811870694,
      "grad_norm": 1.4517604112625122,
      "learning_rate": 7.865341075397873e-06,
      "loss": 2.0558,
      "step": 46
    },
    {
      "epoch": 0.003113407525172231,
      "grad_norm": 1.494954228401184,
      "learning_rate": 7.858731649835423e-06,
      "loss": 2.0556,
      "step": 47
    },
    {
      "epoch": 0.003179650238473768,
      "grad_norm": 1.8161464929580688,
      "learning_rate": 7.85196679512477e-06,
      "loss": 2.1374,
      "step": 48
    },
    {
      "epoch": 0.0032458929517753046,
      "grad_norm": 1.441933274269104,
      "learning_rate": 7.845046783753275e-06,
      "loss": 2.1359,
      "step": 49
    },
    {
      "epoch": 0.0033121356650768416,
      "grad_norm": 1.527364730834961,
      "learning_rate": 7.837971894457989e-06,
      "loss": 1.8756,
      "step": 50
    },
    {
      "epoch": 0.0033783783783783786,
      "grad_norm": 1.513611078262329,
      "learning_rate": 7.83074241221442e-06,
      "loss": 2.2051,
      "step": 51
    },
    {
      "epoch": 0.003444621091679915,
      "grad_norm": 1.461153507232666,
      "learning_rate": 7.82335862822506e-06,
      "loss": 2.1731,
      "step": 52
    },
    {
      "epoch": 0.003510863804981452,
      "grad_norm": 1.1880441904067993,
      "learning_rate": 7.81582083990765e-06,
      "loss": 1.9915,
      "step": 53
    },
    {
      "epoch": 0.003577106518282989,
      "grad_norm": 1.3948886394500732,
      "learning_rate": 7.808129350883205e-06,
      "loss": 2.0217,
      "step": 54
    },
    {
      "epoch": 0.0036433492315845256,
      "grad_norm": 1.4726269245147705,
      "learning_rate": 7.800284470963781e-06,
      "loss": 2.0647,
      "step": 55
    },
    {
      "epoch": 0.0037095919448860626,
      "grad_norm": 1.6807737350463867,
      "learning_rate": 7.792286516139997e-06,
      "loss": 2.1367,
      "step": 56
    },
    {
      "epoch": 0.0037758346581875995,
      "grad_norm": 1.352622628211975,
      "learning_rate": 7.784135808568308e-06,
      "loss": 2.1061,
      "step": 57
    },
    {
      "epoch": 0.003842077371489136,
      "grad_norm": 1.3320592641830444,
      "learning_rate": 7.775832676558026e-06,
      "loss": 2.0101,
      "step": 58
    },
    {
      "epoch": 0.003908320084790673,
      "grad_norm": 1.4262548685073853,
      "learning_rate": 7.767377454558098e-06,
      "loss": 2.0038,
      "step": 59
    },
    {
      "epoch": 0.00397456279809221,
      "grad_norm": 1.8328378200531006,
      "learning_rate": 7.758770483143633e-06,
      "loss": 1.9436,
      "step": 60
    },
    {
      "epoch": 0.0040408055113937465,
      "grad_norm": 1.6064929962158203,
      "learning_rate": 7.750012109002185e-06,
      "loss": 1.986,
      "step": 61
    },
    {
      "epoch": 0.0041070482246952835,
      "grad_norm": 2.126375675201416,
      "learning_rate": 7.741102684919786e-06,
      "loss": 2.0367,
      "step": 62
    },
    {
      "epoch": 0.0041732909379968205,
      "grad_norm": 1.3339564800262451,
      "learning_rate": 7.73204256976674e-06,
      "loss": 1.9175,
      "step": 63
    },
    {
      "epoch": 0.0042395336512983575,
      "grad_norm": 1.377361536026001,
      "learning_rate": 7.722832128483164e-06,
      "loss": 2.0844,
      "step": 64
    },
    {
      "epoch": 0.004305776364599894,
      "grad_norm": 1.5128929615020752,
      "learning_rate": 7.71347173206429e-06,
      "loss": 1.9946,
      "step": 65
    },
    {
      "epoch": 0.0043720190779014305,
      "grad_norm": 1.56609308719635,
      "learning_rate": 7.70396175754552e-06,
      "loss": 2.0313,
      "step": 66
    },
    {
      "epoch": 0.0044382617912029675,
      "grad_norm": 1.9883188009262085,
      "learning_rate": 7.694302587987244e-06,
      "loss": 1.9522,
      "step": 67
    },
    {
      "epoch": 0.0045045045045045045,
      "grad_norm": 1.5238457918167114,
      "learning_rate": 7.6844946124594e-06,
      "loss": 1.8802,
      "step": 68
    },
    {
      "epoch": 0.0045707472178060414,
      "grad_norm": 1.6092586517333984,
      "learning_rate": 7.674538226025814e-06,
      "loss": 1.8693,
      "step": 69
    },
    {
      "epoch": 0.004636989931107578,
      "grad_norm": 1.7429418563842773,
      "learning_rate": 7.664433829728277e-06,
      "loss": 2.1075,
      "step": 70
    },
    {
      "epoch": 0.004703232644409115,
      "grad_norm": 2.038536787033081,
      "learning_rate": 7.654181830570403e-06,
      "loss": 2.0714,
      "step": 71
    },
    {
      "epoch": 0.0047694753577106515,
      "grad_norm": 1.7516038417816162,
      "learning_rate": 7.64378264150122e-06,
      "loss": 2.2597,
      "step": 72
    },
    {
      "epoch": 0.0048357180710121885,
      "grad_norm": 1.6387555599212646,
      "learning_rate": 7.633236681398548e-06,
      "loss": 2.0844,
      "step": 73
    },
    {
      "epoch": 0.004901960784313725,
      "grad_norm": 2.247476577758789,
      "learning_rate": 7.622544375052123e-06,
      "loss": 1.8272,
      "step": 74
    },
    {
      "epoch": 0.004968203497615262,
      "grad_norm": 1.9581820964813232,
      "learning_rate": 7.611706153146485e-06,
      "loss": 1.9989,
      "step": 75
    },
    {
      "epoch": 0.005034446210916799,
      "grad_norm": 1.587908148765564,
      "learning_rate": 7.600722452243631e-06,
      "loss": 1.9356,
      "step": 76
    },
    {
      "epoch": 0.005100688924218336,
      "grad_norm": 2.135432720184326,
      "learning_rate": 7.589593714765433e-06,
      "loss": 1.7661,
      "step": 77
    },
    {
      "epoch": 0.0051669316375198724,
      "grad_norm": 2.2530341148376465,
      "learning_rate": 7.578320388975815e-06,
      "loss": 1.9583,
      "step": 78
    },
    {
      "epoch": 0.005233174350821409,
      "grad_norm": 1.563417911529541,
      "learning_rate": 7.566902928962693e-06,
      "loss": 1.859,
      "step": 79
    },
    {
      "epoch": 0.005299417064122946,
      "grad_norm": 1.8454619646072388,
      "learning_rate": 7.555341794619694e-06,
      "loss": 1.9772,
      "step": 80
    },
    {
      "epoch": 0.005365659777424483,
      "grad_norm": 1.5799814462661743,
      "learning_rate": 7.543637451627622e-06,
      "loss": 1.7329,
      "step": 81
    },
    {
      "epoch": 0.00543190249072602,
      "grad_norm": 1.46219003200531,
      "learning_rate": 7.531790371435707e-06,
      "loss": 1.9172,
      "step": 82
    },
    {
      "epoch": 0.005498145204027557,
      "grad_norm": 1.6360783576965332,
      "learning_rate": 7.519801031242613e-06,
      "loss": 1.8636,
      "step": 83
    },
    {
      "epoch": 0.005564387917329093,
      "grad_norm": 1.597049593925476,
      "learning_rate": 7.5076699139772115e-06,
      "loss": 1.855,
      "step": 84
    },
    {
      "epoch": 0.00563063063063063,
      "grad_norm": 1.5641920566558838,
      "learning_rate": 7.49539750827914e-06,
      "loss": 1.6662,
      "step": 85
    },
    {
      "epoch": 0.005696873343932167,
      "grad_norm": 1.6213308572769165,
      "learning_rate": 7.4829843084791085e-06,
      "loss": 2.0456,
      "step": 86
    },
    {
      "epoch": 0.005763116057233704,
      "grad_norm": 1.7253029346466064,
      "learning_rate": 7.470430814578996e-06,
      "loss": 1.9573,
      "step": 87
    },
    {
      "epoch": 0.005829358770535241,
      "grad_norm": 1.654067039489746,
      "learning_rate": 7.457737532231707e-06,
      "loss": 1.7104,
      "step": 88
    },
    {
      "epoch": 0.005895601483836778,
      "grad_norm": 1.8026683330535889,
      "learning_rate": 7.4449049727208025e-06,
      "loss": 1.671,
      "step": 89
    },
    {
      "epoch": 0.005961844197138315,
      "grad_norm": 1.6492748260498047,
      "learning_rate": 7.431933652939908e-06,
      "loss": 1.838,
      "step": 90
    },
    {
      "epoch": 0.006028086910439851,
      "grad_norm": 1.5996257066726685,
      "learning_rate": 7.418824095371894e-06,
      "loss": 1.7762,
      "step": 91
    },
    {
      "epoch": 0.006094329623741388,
      "grad_norm": 1.8772085905075073,
      "learning_rate": 7.405576828067827e-06,
      "loss": 1.8575,
      "step": 92
    },
    {
      "epoch": 0.006160572337042925,
      "grad_norm": 1.541295051574707,
      "learning_rate": 7.392192384625703e-06,
      "loss": 1.7763,
      "step": 93
    },
    {
      "epoch": 0.006226815050344462,
      "grad_norm": 1.6223118305206299,
      "learning_rate": 7.378671304168954e-06,
      "loss": 1.8418,
      "step": 94
    },
    {
      "epoch": 0.006293057763645999,
      "grad_norm": 1.9726598262786865,
      "learning_rate": 7.365014131324725e-06,
      "loss": 1.7748,
      "step": 95
    },
    {
      "epoch": 0.006359300476947536,
      "grad_norm": 1.6706812381744385,
      "learning_rate": 7.3512214162019485e-06,
      "loss": 1.6976,
      "step": 96
    },
    {
      "epoch": 0.006425543190249072,
      "grad_norm": 1.681087851524353,
      "learning_rate": 7.337293714369181e-06,
      "loss": 1.7339,
      "step": 97
    },
    {
      "epoch": 0.006491785903550609,
      "grad_norm": 1.4435925483703613,
      "learning_rate": 7.323231586832218e-06,
      "loss": 1.7383,
      "step": 98
    },
    {
      "epoch": 0.006558028616852146,
      "grad_norm": 1.7204086780548096,
      "learning_rate": 7.309035600011508e-06,
      "loss": 1.8086,
      "step": 99
    },
    {
      "epoch": 0.006624271330153683,
      "grad_norm": 2.0236806869506836,
      "learning_rate": 7.294706325719331e-06,
      "loss": 1.6184,
      "step": 100
    },
    {
      "epoch": 0.00669051404345522,
      "grad_norm": 2.0204360485076904,
      "learning_rate": 7.2802443411367645e-06,
      "loss": 1.5611,
      "step": 101
    },
    {
      "epoch": 0.006756756756756757,
      "grad_norm": 1.9645452499389648,
      "learning_rate": 7.2656502287904385e-06,
      "loss": 1.5475,
      "step": 102
    },
    {
      "epoch": 0.006822999470058293,
      "grad_norm": 2.3258585929870605,
      "learning_rate": 7.250924576529072e-06,
      "loss": 1.5633,
      "step": 103
    },
    {
      "epoch": 0.00688924218335983,
      "grad_norm": 1.7984058856964111,
      "learning_rate": 7.2360679774997895e-06,
      "loss": 1.508,
      "step": 104
    },
    {
      "epoch": 0.006955484896661367,
      "grad_norm": 1.6849430799484253,
      "learning_rate": 7.2210810301242345e-06,
      "loss": 1.7795,
      "step": 105
    },
    {
      "epoch": 0.007021727609962904,
      "grad_norm": 1.7991485595703125,
      "learning_rate": 7.20596433807446e-06,
      "loss": 1.4086,
      "step": 106
    },
    {
      "epoch": 0.007087970323264441,
      "grad_norm": 1.8393315076828003,
      "learning_rate": 7.190718510248621e-06,
      "loss": 1.6975,
      "step": 107
    },
    {
      "epoch": 0.007154213036565978,
      "grad_norm": 2.235084295272827,
      "learning_rate": 7.1753441607464374e-06,
      "loss": 1.4958,
      "step": 108
    },
    {
      "epoch": 0.007220455749867514,
      "grad_norm": 1.9484431743621826,
      "learning_rate": 7.159841908844464e-06,
      "loss": 1.7267,
      "step": 109
    },
    {
      "epoch": 0.007286698463169051,
      "grad_norm": 2.0825114250183105,
      "learning_rate": 7.1442123789711495e-06,
      "loss": 1.4566,
      "step": 110
    },
    {
      "epoch": 0.007352941176470588,
      "grad_norm": 1.818530797958374,
      "learning_rate": 7.128456200681677e-06,
      "loss": 1.5132,
      "step": 111
    },
    {
      "epoch": 0.007419183889772125,
      "grad_norm": 1.7386236190795898,
      "learning_rate": 7.11257400863261e-06,
      "loss": 1.5466,
      "step": 112
    },
    {
      "epoch": 0.007485426603073662,
      "grad_norm": 1.5583211183547974,
      "learning_rate": 7.09656644255633e-06,
      "loss": 1.5296,
      "step": 113
    },
    {
      "epoch": 0.007551669316375199,
      "grad_norm": 1.7485840320587158,
      "learning_rate": 7.080434147235262e-06,
      "loss": 1.4539,
      "step": 114
    },
    {
      "epoch": 0.007617912029676735,
      "grad_norm": 1.9540667533874512,
      "learning_rate": 7.064177772475912e-06,
      "loss": 1.6367,
      "step": 115
    },
    {
      "epoch": 0.007684154742978272,
      "grad_norm": 2.2393558025360107,
      "learning_rate": 7.047797973082684e-06,
      "loss": 1.6667,
      "step": 116
    },
    {
      "epoch": 0.007750397456279809,
      "grad_norm": 3.825916051864624,
      "learning_rate": 7.031295408831507e-06,
      "loss": 1.2721,
      "step": 117
    },
    {
      "epoch": 0.007816640169581345,
      "grad_norm": 2.4767422676086426,
      "learning_rate": 7.014670744443266e-06,
      "loss": 1.4735,
      "step": 118
    },
    {
      "epoch": 0.007882882882882882,
      "grad_norm": 2.665492057800293,
      "learning_rate": 6.997924649557016e-06,
      "loss": 1.4848,
      "step": 119
    },
    {
      "epoch": 0.00794912559618442,
      "grad_norm": 2.5010271072387695,
      "learning_rate": 6.981057798703018e-06,
      "loss": 1.6203,
      "step": 120
    },
    {
      "epoch": 0.008015368309485956,
      "grad_norm": 3.238929271697998,
      "learning_rate": 6.964070871275567e-06,
      "loss": 1.574,
      "step": 121
    },
    {
      "epoch": 0.008081611022787493,
      "grad_norm": 3.292484998703003,
      "learning_rate": 6.946964551505619e-06,
      "loss": 1.6163,
      "step": 122
    },
    {
      "epoch": 0.00814785373608903,
      "grad_norm": NaN,
      "learning_rate": 6.946964551505619e-06,
      "loss": 1.5872,
      "step": 123
    },
    {
      "epoch": 0.008214096449390567,
      "grad_norm": 2.9584553241729736,
      "learning_rate": 6.929739528433243e-06,
      "loss": 1.59,
      "step": 124
    },
    {
      "epoch": 0.008280339162692104,
      "grad_norm": 1.7878342866897583,
      "learning_rate": 6.912396495879856e-06,
      "loss": 1.5795,
      "step": 125
    },
    {
      "epoch": 0.008346581875993641,
      "grad_norm": 1.8227241039276123,
      "learning_rate": 6.89493615242028e-06,
      "loss": 1.555,
      "step": 126
    },
    {
      "epoch": 0.008412824589295178,
      "grad_norm": 2.7873735427856445,
      "learning_rate": 6.877359201354604e-06,
      "loss": 1.4398,
      "step": 127
    },
    {
      "epoch": 0.008479067302596715,
      "grad_norm": 2.862027406692505,
      "learning_rate": 6.859666350679854e-06,
      "loss": 1.3716,
      "step": 128
    },
    {
      "epoch": 0.008545310015898252,
      "grad_norm": 2.653536558151245,
      "learning_rate": 6.8418583130614755e-06,
      "loss": 1.4388,
      "step": 129
    },
    {
      "epoch": 0.008611552729199789,
      "grad_norm": 3.4832093715667725,
      "learning_rate": 6.823935805804625e-06,
      "loss": 1.5748,
      "step": 130
    },
    {
      "epoch": 0.008677795442501324,
      "grad_norm": 2.699143886566162,
      "learning_rate": 6.805899550825285e-06,
      "loss": 1.4983,
      "step": 131
    },
    {
      "epoch": 0.008744038155802861,
      "grad_norm": 1.9413812160491943,
      "learning_rate": 6.787750274621174e-06,
      "loss": 1.5618,
      "step": 132
    },
    {
      "epoch": 0.008810280869104398,
      "grad_norm": 2.7908928394317627,
      "learning_rate": 6.7694887082424906e-06,
      "loss": 1.4284,
      "step": 133
    },
    {
      "epoch": 0.008876523582405935,
      "grad_norm": 1.7990185022354126,
      "learning_rate": 6.751115587262468e-06,
      "loss": 1.5476,
      "step": 134
    },
    {
      "epoch": 0.008942766295707472,
      "grad_norm": 1.7922929525375366,
      "learning_rate": 6.732631651747738e-06,
      "loss": 1.2363,
      "step": 135
    },
    {
      "epoch": 0.009009009009009009,
      "grad_norm": 1.8143641948699951,
      "learning_rate": 6.714037646228529e-06,
      "loss": 1.6721,
      "step": 136
    },
    {
      "epoch": 0.009075251722310546,
      "grad_norm": 2.1780283451080322,
      "learning_rate": 6.695334319668671e-06,
      "loss": 1.4645,
      "step": 137
    },
    {
      "epoch": 0.009141494435612083,
      "grad_norm": 2.496548891067505,
      "learning_rate": 6.676522425435432e-06,
      "loss": 1.3946,
      "step": 138
    },
    {
      "epoch": 0.00920773714891362,
      "grad_norm": 1.4481409788131714,
      "learning_rate": 6.657602721269169e-06,
      "loss": 1.4979,
      "step": 139
    },
    {
      "epoch": 0.009273979862215157,
      "grad_norm": 2.527867078781128,
      "learning_rate": 6.638575969252805e-06,
      "loss": 1.4871,
      "step": 140
    },
    {
      "epoch": 0.009340222575516694,
      "grad_norm": 2.6403379440307617,
      "learning_rate": 6.619442935781141e-06,
      "loss": 1.3464,
      "step": 141
    },
    {
      "epoch": 0.00940646528881823,
      "grad_norm": 2.256847381591797,
      "learning_rate": 6.600204391529969e-06,
      "loss": 1.1933,
      "step": 142
    },
    {
      "epoch": 0.009472708002119766,
      "grad_norm": 1.8411595821380615,
      "learning_rate": 6.580861111425051e-06,
      "loss": 1.3258,
      "step": 143
    },
    {
      "epoch": 0.009538950715421303,
      "grad_norm": 1.7218337059020996,
      "learning_rate": 6.561413874610889e-06,
      "loss": 1.298,
      "step": 144
    },
    {
      "epoch": 0.00960519342872284,
      "grad_norm": 1.7238742113113403,
      "learning_rate": 6.5418634644193444e-06,
      "loss": 1.5168,
      "step": 145
    },
    {
      "epoch": 0.009671436142024377,
      "grad_norm": 1.6658053398132324,
      "learning_rate": 6.52221066833809e-06,
      "loss": 1.3247,
      "step": 146
    },
    {
      "epoch": 0.009737678855325914,
      "grad_norm": 2.6103458404541016,
      "learning_rate": 6.502456277978885e-06,
      "loss": 1.4333,
      "step": 147
    },
    {
      "epoch": 0.00980392156862745,
      "grad_norm": 1.6054779291152954,
      "learning_rate": 6.4826010890456945e-06,
      "loss": 1.5075,
      "step": 148
    },
    {
      "epoch": 0.009870164281928988,
      "grad_norm": 1.3823165893554688,
      "learning_rate": 6.462645901302632e-06,
      "loss": 1.4773,
      "step": 149
    },
    {
      "epoch": 0.009936406995230525,
      "grad_norm": 1.5936436653137207,
      "learning_rate": 6.442591518541753e-06,
      "loss": 1.3436,
      "step": 150
    },
    {
      "epoch": 0.010002649708532062,
      "grad_norm": 2.018528699874878,
      "learning_rate": 6.422438748550666e-06,
      "loss": 1.3298,
      "step": 151
    },
    {
      "epoch": 0.010068892421833599,
      "grad_norm": 1.7952768802642822,
      "learning_rate": 6.402188403080012e-06,
      "loss": 1.3543,
      "step": 152
    },
    {
      "epoch": 0.010135135135135136,
      "grad_norm": 2.5104212760925293,
      "learning_rate": 6.381841297810752e-06,
      "loss": 1.3093,
      "step": 153
    },
    {
      "epoch": 0.010201377848436673,
      "grad_norm": 1.6849403381347656,
      "learning_rate": 6.361398252321319e-06,
      "loss": 1.3147,
      "step": 154
    },
    {
      "epoch": 0.01026762056173821,
      "grad_norm": 3.842738628387451,
      "learning_rate": 6.340860090054606e-06,
      "loss": 1.273,
      "step": 155
    },
    {
      "epoch": 0.010333863275039745,
      "grad_norm": 1.5389838218688965,
      "learning_rate": 6.3202276382847925e-06,
      "loss": 1.3782,
      "step": 156
    },
    {
      "epoch": 0.010400105988341282,
      "grad_norm": 2.232748508453369,
      "learning_rate": 6.299501728084029e-06,
      "loss": 1.2653,
      "step": 157
    },
    {
      "epoch": 0.010466348701642819,
      "grad_norm": 1.8020761013031006,
      "learning_rate": 6.2786831942889555e-06,
      "loss": 1.3698,
      "step": 158
    },
    {
      "epoch": 0.010532591414944356,
      "grad_norm": 2.1255156993865967,
      "learning_rate": 6.257772875467077e-06,
      "loss": 1.1725,
      "step": 159
    },
    {
      "epoch": 0.010598834128245893,
      "grad_norm": 1.5517891645431519,
      "learning_rate": 6.2367716138829865e-06,
      "loss": 1.2552,
      "step": 160
    }
  ],
  "logging_steps": 1,
  "max_steps": 500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0447914486325248e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
