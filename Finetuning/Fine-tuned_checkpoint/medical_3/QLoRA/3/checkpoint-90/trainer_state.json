{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.005961844197138315,
  "eval_steps": 500,
  "global_step": 90,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 6.624271330153684e-05,
      "grad_norm": 1.3809800148010254,
      "learning_rate": 1.6e-06,
      "loss": 2.2689,
      "step": 1
    },
    {
      "epoch": 0.00013248542660307367,
      "grad_norm": 1.6996265649795532,
      "learning_rate": 3.2e-06,
      "loss": 2.3392,
      "step": 2
    },
    {
      "epoch": 0.0001987281399046105,
      "grad_norm": 1.5995228290557861,
      "learning_rate": 4.8e-06,
      "loss": 2.327,
      "step": 3
    },
    {
      "epoch": 0.00026497085320614734,
      "grad_norm": 1.2807488441467285,
      "learning_rate": 6.4e-06,
      "loss": 2.2282,
      "step": 4
    },
    {
      "epoch": 0.00033121356650768415,
      "grad_norm": 1.7918407917022705,
      "learning_rate": 8e-06,
      "loss": 2.3296,
      "step": 5
    },
    {
      "epoch": 0.000397456279809221,
      "grad_norm": 2.1889030933380127,
      "learning_rate": 7.999919440291625e-06,
      "loss": 2.4252,
      "step": 6
    },
    {
      "epoch": 0.0004636989931107578,
      "grad_norm": 1.4946634769439697,
      "learning_rate": 7.999677764411437e-06,
      "loss": 2.2345,
      "step": 7
    },
    {
      "epoch": 0.0005299417064122947,
      "grad_norm": 1.4097577333450317,
      "learning_rate": 7.999274982094103e-06,
      "loss": 2.2835,
      "step": 8
    },
    {
      "epoch": 0.0005961844197138314,
      "grad_norm": 1.5491282939910889,
      "learning_rate": 7.998711109563636e-06,
      "loss": 2.3787,
      "step": 9
    },
    {
      "epoch": 0.0006624271330153683,
      "grad_norm": 1.5071535110473633,
      "learning_rate": 7.99798616953274e-06,
      "loss": 2.084,
      "step": 10
    },
    {
      "epoch": 0.0007286698463169052,
      "grad_norm": 1.3594766855239868,
      "learning_rate": 7.997100191201894e-06,
      "loss": 2.2995,
      "step": 11
    },
    {
      "epoch": 0.000794912559618442,
      "grad_norm": 1.4487510919570923,
      "learning_rate": 7.996053210258175e-06,
      "loss": 2.5685,
      "step": 12
    },
    {
      "epoch": 0.0008611552729199788,
      "grad_norm": 1.9860165119171143,
      "learning_rate": 7.994845268873824e-06,
      "loss": 2.5732,
      "step": 13
    },
    {
      "epoch": 0.0009273979862215156,
      "grad_norm": 1.3641774654388428,
      "learning_rate": 7.993476415704541e-06,
      "loss": 2.2761,
      "step": 14
    },
    {
      "epoch": 0.0009936406995230524,
      "grad_norm": 1.6011816263198853,
      "learning_rate": 7.991946705887537e-06,
      "loss": 2.2512,
      "step": 15
    },
    {
      "epoch": 0.0010598834128245894,
      "grad_norm": 1.6379209756851196,
      "learning_rate": 7.990256201039296e-06,
      "loss": 2.3967,
      "step": 16
    },
    {
      "epoch": 0.0011261261261261261,
      "grad_norm": 1.312429428100586,
      "learning_rate": 7.988404969253109e-06,
      "loss": 2.0369,
      "step": 17
    },
    {
      "epoch": 0.0011923688394276629,
      "grad_norm": 1.5695819854736328,
      "learning_rate": 7.986393085096323e-06,
      "loss": 2.2738,
      "step": 18
    },
    {
      "epoch": 0.0012586115527291998,
      "grad_norm": 1.7151944637298584,
      "learning_rate": 7.984220629607335e-06,
      "loss": 2.2317,
      "step": 19
    },
    {
      "epoch": 0.0013248542660307366,
      "grad_norm": 1.3730355501174927,
      "learning_rate": 7.981887690292338e-06,
      "loss": 2.2022,
      "step": 20
    },
    {
      "epoch": 0.0013910969793322733,
      "grad_norm": 1.6174428462982178,
      "learning_rate": 7.979394361121788e-06,
      "loss": 2.4936,
      "step": 21
    },
    {
      "epoch": 0.0014573396926338103,
      "grad_norm": 1.417569637298584,
      "learning_rate": 7.976740742526617e-06,
      "loss": 2.1494,
      "step": 22
    },
    {
      "epoch": 0.001523582405935347,
      "grad_norm": 1.4038379192352295,
      "learning_rate": 7.973926941394201e-06,
      "loss": 2.2114,
      "step": 23
    },
    {
      "epoch": 0.001589825119236884,
      "grad_norm": 1.4127357006072998,
      "learning_rate": 7.970953071064033e-06,
      "loss": 2.2311,
      "step": 24
    },
    {
      "epoch": 0.0016560678325384208,
      "grad_norm": 1.5051912069320679,
      "learning_rate": 7.96781925132318e-06,
      "loss": 2.215,
      "step": 25
    },
    {
      "epoch": 0.0017223105458399576,
      "grad_norm": 1.38594651222229,
      "learning_rate": 7.964525608401445e-06,
      "loss": 2.1734,
      "step": 26
    },
    {
      "epoch": 0.0017885532591414945,
      "grad_norm": 1.5026637315750122,
      "learning_rate": 7.96107227496628e-06,
      "loss": 2.256,
      "step": 27
    },
    {
      "epoch": 0.0018547959724430313,
      "grad_norm": 1.3919044733047485,
      "learning_rate": 7.957459390117459e-06,
      "loss": 2.1654,
      "step": 28
    },
    {
      "epoch": 0.001921038685744568,
      "grad_norm": 1.4173874855041504,
      "learning_rate": 7.953687099381448e-06,
      "loss": 2.3061,
      "step": 29
    },
    {
      "epoch": 0.001987281399046105,
      "grad_norm": 1.6002627611160278,
      "learning_rate": 7.949755554705577e-06,
      "loss": 2.2258,
      "step": 30
    },
    {
      "epoch": 0.0020535241123476418,
      "grad_norm": 1.5717726945877075,
      "learning_rate": 7.945664914451887e-06,
      "loss": 2.2467,
      "step": 31
    },
    {
      "epoch": 0.0021197668256491787,
      "grad_norm": 1.6920794248580933,
      "learning_rate": 7.941415343390771e-06,
      "loss": 2.1356,
      "step": 32
    },
    {
      "epoch": 0.0021860095389507153,
      "grad_norm": 1.4630272388458252,
      "learning_rate": 7.937007012694335e-06,
      "loss": 2.2694,
      "step": 33
    },
    {
      "epoch": 0.0022522522522522522,
      "grad_norm": 1.5543068647384644,
      "learning_rate": 7.932440099929493e-06,
      "loss": 2.2479,
      "step": 34
    },
    {
      "epoch": 0.002318494965553789,
      "grad_norm": 1.2894253730773926,
      "learning_rate": 7.927714789050827e-06,
      "loss": 2.0509,
      "step": 35
    },
    {
      "epoch": 0.0023847376788553257,
      "grad_norm": 1.5814480781555176,
      "learning_rate": 7.922831270393169e-06,
      "loss": 2.1374,
      "step": 36
    },
    {
      "epoch": 0.0024509803921568627,
      "grad_norm": 1.5754947662353516,
      "learning_rate": 7.917789740663941e-06,
      "loss": 1.9903,
      "step": 37
    },
    {
      "epoch": 0.0025172231054583997,
      "grad_norm": 1.5083675384521484,
      "learning_rate": 7.912590402935222e-06,
      "loss": 2.2261,
      "step": 38
    },
    {
      "epoch": 0.0025834658187599362,
      "grad_norm": 1.5683575868606567,
      "learning_rate": 7.90723346663558e-06,
      "loss": 2.1183,
      "step": 39
    },
    {
      "epoch": 0.002649708532061473,
      "grad_norm": 1.5738838911056519,
      "learning_rate": 7.901719147541628e-06,
      "loss": 2.2268,
      "step": 40
    },
    {
      "epoch": 0.00271595124536301,
      "grad_norm": 1.6617158651351929,
      "learning_rate": 7.896047667769334e-06,
      "loss": 2.1754,
      "step": 41
    },
    {
      "epoch": 0.0027821939586645467,
      "grad_norm": 1.6001683473587036,
      "learning_rate": 7.890219255765076e-06,
      "loss": 1.8818,
      "step": 42
    },
    {
      "epoch": 0.0028484366719660837,
      "grad_norm": 1.7564657926559448,
      "learning_rate": 7.88423414629644e-06,
      "loss": 2.0247,
      "step": 43
    },
    {
      "epoch": 0.0029146793852676206,
      "grad_norm": 1.3842233419418335,
      "learning_rate": 7.878092580442764e-06,
      "loss": 1.9519,
      "step": 44
    },
    {
      "epoch": 0.0029809220985691576,
      "grad_norm": 1.5279793739318848,
      "learning_rate": 7.871794805585425e-06,
      "loss": 2.1173,
      "step": 45
    },
    {
      "epoch": 0.003047164811870694,
      "grad_norm": 1.4517604112625122,
      "learning_rate": 7.865341075397873e-06,
      "loss": 2.0558,
      "step": 46
    },
    {
      "epoch": 0.003113407525172231,
      "grad_norm": 1.494954228401184,
      "learning_rate": 7.858731649835423e-06,
      "loss": 2.0556,
      "step": 47
    },
    {
      "epoch": 0.003179650238473768,
      "grad_norm": 1.8161464929580688,
      "learning_rate": 7.85196679512477e-06,
      "loss": 2.1374,
      "step": 48
    },
    {
      "epoch": 0.0032458929517753046,
      "grad_norm": 1.441933274269104,
      "learning_rate": 7.845046783753275e-06,
      "loss": 2.1359,
      "step": 49
    },
    {
      "epoch": 0.0033121356650768416,
      "grad_norm": 1.527364730834961,
      "learning_rate": 7.837971894457989e-06,
      "loss": 1.8756,
      "step": 50
    },
    {
      "epoch": 0.0033783783783783786,
      "grad_norm": 1.513611078262329,
      "learning_rate": 7.83074241221442e-06,
      "loss": 2.2051,
      "step": 51
    },
    {
      "epoch": 0.003444621091679915,
      "grad_norm": 1.461153507232666,
      "learning_rate": 7.82335862822506e-06,
      "loss": 2.1731,
      "step": 52
    },
    {
      "epoch": 0.003510863804981452,
      "grad_norm": 1.1880441904067993,
      "learning_rate": 7.81582083990765e-06,
      "loss": 1.9915,
      "step": 53
    },
    {
      "epoch": 0.003577106518282989,
      "grad_norm": 1.3948886394500732,
      "learning_rate": 7.808129350883205e-06,
      "loss": 2.0217,
      "step": 54
    },
    {
      "epoch": 0.0036433492315845256,
      "grad_norm": 1.4726269245147705,
      "learning_rate": 7.800284470963781e-06,
      "loss": 2.0647,
      "step": 55
    },
    {
      "epoch": 0.0037095919448860626,
      "grad_norm": 1.6807737350463867,
      "learning_rate": 7.792286516139997e-06,
      "loss": 2.1367,
      "step": 56
    },
    {
      "epoch": 0.0037758346581875995,
      "grad_norm": 1.352622628211975,
      "learning_rate": 7.784135808568308e-06,
      "loss": 2.1061,
      "step": 57
    },
    {
      "epoch": 0.003842077371489136,
      "grad_norm": 1.3320592641830444,
      "learning_rate": 7.775832676558026e-06,
      "loss": 2.0101,
      "step": 58
    },
    {
      "epoch": 0.003908320084790673,
      "grad_norm": 1.4262548685073853,
      "learning_rate": 7.767377454558098e-06,
      "loss": 2.0038,
      "step": 59
    },
    {
      "epoch": 0.00397456279809221,
      "grad_norm": 1.8328378200531006,
      "learning_rate": 7.758770483143633e-06,
      "loss": 1.9436,
      "step": 60
    },
    {
      "epoch": 0.0040408055113937465,
      "grad_norm": 1.6064929962158203,
      "learning_rate": 7.750012109002185e-06,
      "loss": 1.986,
      "step": 61
    },
    {
      "epoch": 0.0041070482246952835,
      "grad_norm": 2.126375675201416,
      "learning_rate": 7.741102684919786e-06,
      "loss": 2.0367,
      "step": 62
    },
    {
      "epoch": 0.0041732909379968205,
      "grad_norm": 1.3339564800262451,
      "learning_rate": 7.73204256976674e-06,
      "loss": 1.9175,
      "step": 63
    },
    {
      "epoch": 0.0042395336512983575,
      "grad_norm": 1.377361536026001,
      "learning_rate": 7.722832128483164e-06,
      "loss": 2.0844,
      "step": 64
    },
    {
      "epoch": 0.004305776364599894,
      "grad_norm": 1.5128929615020752,
      "learning_rate": 7.71347173206429e-06,
      "loss": 1.9946,
      "step": 65
    },
    {
      "epoch": 0.0043720190779014305,
      "grad_norm": 1.56609308719635,
      "learning_rate": 7.70396175754552e-06,
      "loss": 2.0313,
      "step": 66
    },
    {
      "epoch": 0.0044382617912029675,
      "grad_norm": 1.9883188009262085,
      "learning_rate": 7.694302587987244e-06,
      "loss": 1.9522,
      "step": 67
    },
    {
      "epoch": 0.0045045045045045045,
      "grad_norm": 1.5238457918167114,
      "learning_rate": 7.6844946124594e-06,
      "loss": 1.8802,
      "step": 68
    },
    {
      "epoch": 0.0045707472178060414,
      "grad_norm": 1.6092586517333984,
      "learning_rate": 7.674538226025814e-06,
      "loss": 1.8693,
      "step": 69
    },
    {
      "epoch": 0.004636989931107578,
      "grad_norm": 1.7429418563842773,
      "learning_rate": 7.664433829728277e-06,
      "loss": 2.1075,
      "step": 70
    },
    {
      "epoch": 0.004703232644409115,
      "grad_norm": 2.038536787033081,
      "learning_rate": 7.654181830570403e-06,
      "loss": 2.0714,
      "step": 71
    },
    {
      "epoch": 0.0047694753577106515,
      "grad_norm": 1.7516038417816162,
      "learning_rate": 7.64378264150122e-06,
      "loss": 2.2597,
      "step": 72
    },
    {
      "epoch": 0.0048357180710121885,
      "grad_norm": 1.6387555599212646,
      "learning_rate": 7.633236681398548e-06,
      "loss": 2.0844,
      "step": 73
    },
    {
      "epoch": 0.004901960784313725,
      "grad_norm": 2.247476577758789,
      "learning_rate": 7.622544375052123e-06,
      "loss": 1.8272,
      "step": 74
    },
    {
      "epoch": 0.004968203497615262,
      "grad_norm": 1.9581820964813232,
      "learning_rate": 7.611706153146485e-06,
      "loss": 1.9989,
      "step": 75
    },
    {
      "epoch": 0.005034446210916799,
      "grad_norm": 1.587908148765564,
      "learning_rate": 7.600722452243631e-06,
      "loss": 1.9356,
      "step": 76
    },
    {
      "epoch": 0.005100688924218336,
      "grad_norm": 2.135432720184326,
      "learning_rate": 7.589593714765433e-06,
      "loss": 1.7661,
      "step": 77
    },
    {
      "epoch": 0.0051669316375198724,
      "grad_norm": 2.2530341148376465,
      "learning_rate": 7.578320388975815e-06,
      "loss": 1.9583,
      "step": 78
    },
    {
      "epoch": 0.005233174350821409,
      "grad_norm": 1.563417911529541,
      "learning_rate": 7.566902928962693e-06,
      "loss": 1.859,
      "step": 79
    },
    {
      "epoch": 0.005299417064122946,
      "grad_norm": 1.8454619646072388,
      "learning_rate": 7.555341794619694e-06,
      "loss": 1.9772,
      "step": 80
    },
    {
      "epoch": 0.005365659777424483,
      "grad_norm": 1.5799814462661743,
      "learning_rate": 7.543637451627622e-06,
      "loss": 1.7329,
      "step": 81
    },
    {
      "epoch": 0.00543190249072602,
      "grad_norm": 1.46219003200531,
      "learning_rate": 7.531790371435707e-06,
      "loss": 1.9172,
      "step": 82
    },
    {
      "epoch": 0.005498145204027557,
      "grad_norm": 1.6360783576965332,
      "learning_rate": 7.519801031242613e-06,
      "loss": 1.8636,
      "step": 83
    },
    {
      "epoch": 0.005564387917329093,
      "grad_norm": 1.597049593925476,
      "learning_rate": 7.5076699139772115e-06,
      "loss": 1.855,
      "step": 84
    },
    {
      "epoch": 0.00563063063063063,
      "grad_norm": 1.5641920566558838,
      "learning_rate": 7.49539750827914e-06,
      "loss": 1.6662,
      "step": 85
    },
    {
      "epoch": 0.005696873343932167,
      "grad_norm": 1.6213308572769165,
      "learning_rate": 7.4829843084791085e-06,
      "loss": 2.0456,
      "step": 86
    },
    {
      "epoch": 0.005763116057233704,
      "grad_norm": 1.7253029346466064,
      "learning_rate": 7.470430814578996e-06,
      "loss": 1.9573,
      "step": 87
    },
    {
      "epoch": 0.005829358770535241,
      "grad_norm": 1.654067039489746,
      "learning_rate": 7.457737532231707e-06,
      "loss": 1.7104,
      "step": 88
    },
    {
      "epoch": 0.005895601483836778,
      "grad_norm": 1.8026683330535889,
      "learning_rate": 7.4449049727208025e-06,
      "loss": 1.671,
      "step": 89
    },
    {
      "epoch": 0.005961844197138315,
      "grad_norm": 1.6492748260498047,
      "learning_rate": 7.431933652939908e-06,
      "loss": 1.838,
      "step": 90
    }
  ],
  "logging_steps": 1,
  "max_steps": 500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5863213864304640.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
