{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.0037224307472779727,
  "eval_steps": 500,
  "global_step": 90,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 4.1360341636421915e-05,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 1.7031,
      "step": 1
    },
    {
      "epoch": 8.272068327284383e-05,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.3213,
      "step": 2
    },
    {
      "epoch": 0.00012408102490926574,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.419,
      "step": 3
    },
    {
      "epoch": 0.00016544136654568766,
      "grad_norm": 4.164985656738281,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 2.0759,
      "step": 4
    },
    {
      "epoch": 0.00020680170818210958,
      "grad_norm": 4.716665744781494,
      "learning_rate": 4.000000000000001e-06,
      "loss": 2.3375,
      "step": 5
    },
    {
      "epoch": 0.0002481620498185315,
      "grad_norm": 4.337313175201416,
      "learning_rate": 6e-06,
      "loss": 2.204,
      "step": 6
    },
    {
      "epoch": 0.0002895223914549534,
      "grad_norm": 4.612483978271484,
      "learning_rate": 8.000000000000001e-06,
      "loss": 2.2095,
      "step": 7
    },
    {
      "epoch": 0.0003308827330913753,
      "grad_norm": 4.668890953063965,
      "learning_rate": 1e-05,
      "loss": 2.4594,
      "step": 8
    },
    {
      "epoch": 0.00037224307472779727,
      "grad_norm": 6.125282287597656,
      "learning_rate": 9.997762161417517e-06,
      "loss": 2.3116,
      "step": 9
    },
    {
      "epoch": 0.00041360341636421916,
      "grad_norm": 5.5043721199035645,
      "learning_rate": 9.991050648838676e-06,
      "loss": 2.1727,
      "step": 10
    },
    {
      "epoch": 0.0004549637580006411,
      "grad_norm": 4.275330543518066,
      "learning_rate": 9.979871469976197e-06,
      "loss": 2.1825,
      "step": 11
    },
    {
      "epoch": 0.000496324099637063,
      "grad_norm": 6.516251564025879,
      "learning_rate": 9.964234631709188e-06,
      "loss": 2.4677,
      "step": 12
    },
    {
      "epoch": 0.000537684441273485,
      "grad_norm": NaN,
      "learning_rate": 9.964234631709188e-06,
      "loss": 2.6173,
      "step": 13
    },
    {
      "epoch": 0.0005790447829099068,
      "grad_norm": 5.082197189331055,
      "learning_rate": 9.944154131125643e-06,
      "loss": 2.5272,
      "step": 14
    },
    {
      "epoch": 0.0006204051245463287,
      "grad_norm": 4.449840545654297,
      "learning_rate": 9.91964794299315e-06,
      "loss": 2.4029,
      "step": 15
    },
    {
      "epoch": 0.0006617654661827506,
      "grad_norm": 5.338963031768799,
      "learning_rate": 9.890738003669029e-06,
      "loss": 2.4036,
      "step": 16
    },
    {
      "epoch": 0.0007031258078191726,
      "grad_norm": 6.106631278991699,
      "learning_rate": 9.857450191464337e-06,
      "loss": 2.1675,
      "step": 17
    },
    {
      "epoch": 0.0007444861494555945,
      "grad_norm": 4.758635997772217,
      "learning_rate": 9.819814303479268e-06,
      "loss": 2.4686,
      "step": 18
    },
    {
      "epoch": 0.0007858464910920164,
      "grad_norm": 4.870220184326172,
      "learning_rate": 9.777864028930705e-06,
      "loss": 2.1887,
      "step": 19
    },
    {
      "epoch": 0.0008272068327284383,
      "grad_norm": 5.462088108062744,
      "learning_rate": 9.731636918995821e-06,
      "loss": 2.2799,
      "step": 20
    },
    {
      "epoch": 0.0008685671743648602,
      "grad_norm": 4.986709117889404,
      "learning_rate": 9.681174353198687e-06,
      "loss": 2.2636,
      "step": 21
    },
    {
      "epoch": 0.0009099275160012822,
      "grad_norm": 5.120896339416504,
      "learning_rate": 9.626521502369984e-06,
      "loss": 2.1694,
      "step": 22
    },
    {
      "epoch": 0.0009512878576377041,
      "grad_norm": 5.6264824867248535,
      "learning_rate": 9.567727288213005e-06,
      "loss": 2.38,
      "step": 23
    },
    {
      "epoch": 0.000992648199274126,
      "grad_norm": 5.71195125579834,
      "learning_rate": 9.504844339512096e-06,
      "loss": 2.3381,
      "step": 24
    },
    {
      "epoch": 0.001034008540910548,
      "grad_norm": 5.804052829742432,
      "learning_rate": 9.437928945022772e-06,
      "loss": 2.3767,
      "step": 25
    },
    {
      "epoch": 0.00107536888254697,
      "grad_norm": 5.318484306335449,
      "learning_rate": 9.36704100308565e-06,
      "loss": 2.1412,
      "step": 26
    },
    {
      "epoch": 0.0011167292241833918,
      "grad_norm": 5.365713596343994,
      "learning_rate": 9.292243968009332e-06,
      "loss": 2.1423,
      "step": 27
    },
    {
      "epoch": 0.0011580895658198137,
      "grad_norm": 5.735811233520508,
      "learning_rate": 9.213604793270196e-06,
      "loss": 2.265,
      "step": 28
    },
    {
      "epoch": 0.0011994499074562356,
      "grad_norm": 6.017764091491699,
      "learning_rate": 9.131193871579975e-06,
      "loss": 2.2229,
      "step": 29
    },
    {
      "epoch": 0.0012408102490926575,
      "grad_norm": 6.0233001708984375,
      "learning_rate": 9.045084971874738e-06,
      "loss": 2.4513,
      "step": 30
    },
    {
      "epoch": 0.0012821705907290794,
      "grad_norm": 6.609781265258789,
      "learning_rate": 8.955355173281709e-06,
      "loss": 2.1989,
      "step": 31
    },
    {
      "epoch": 0.0013235309323655013,
      "grad_norm": 6.584712982177734,
      "learning_rate": 8.862084796122998e-06,
      "loss": 2.1453,
      "step": 32
    },
    {
      "epoch": 0.0013648912740019232,
      "grad_norm": 6.2977142333984375,
      "learning_rate": 8.765357330018056e-06,
      "loss": 2.3431,
      "step": 33
    },
    {
      "epoch": 0.0014062516156383453,
      "grad_norm": 6.398581504821777,
      "learning_rate": 8.665259359149132e-06,
      "loss": 2.2468,
      "step": 34
    },
    {
      "epoch": 0.0014476119572747672,
      "grad_norm": 6.20195198059082,
      "learning_rate": 8.561880484756724e-06,
      "loss": 2.1796,
      "step": 35
    },
    {
      "epoch": 0.001488972298911189,
      "grad_norm": 6.499197483062744,
      "learning_rate": 8.455313244934324e-06,
      "loss": 2.1077,
      "step": 36
    },
    {
      "epoch": 0.001530332640547611,
      "grad_norm": 6.689631938934326,
      "learning_rate": 8.345653031794292e-06,
      "loss": 2.3255,
      "step": 37
    },
    {
      "epoch": 0.0015716929821840329,
      "grad_norm": 6.973773002624512,
      "learning_rate": 8.232998006078998e-06,
      "loss": 2.1991,
      "step": 38
    },
    {
      "epoch": 0.0016130533238204548,
      "grad_norm": 6.864548206329346,
      "learning_rate": 8.117449009293668e-06,
      "loss": 2.1288,
      "step": 39
    },
    {
      "epoch": 0.0016544136654568766,
      "grad_norm": 7.341221809387207,
      "learning_rate": 7.99910947343957e-06,
      "loss": 2.4229,
      "step": 40
    },
    {
      "epoch": 0.0016957740070932985,
      "grad_norm": 8.320141792297363,
      "learning_rate": 7.87808532842837e-06,
      "loss": 2.2499,
      "step": 41
    },
    {
      "epoch": 0.0017371343487297204,
      "grad_norm": 7.649667739868164,
      "learning_rate": 7.754484907260513e-06,
      "loss": 2.1408,
      "step": 42
    },
    {
      "epoch": 0.0017784946903661423,
      "grad_norm": 8.327337265014648,
      "learning_rate": 7.628418849052523e-06,
      "loss": 2.1506,
      "step": 43
    },
    {
      "epoch": 0.0018198550320025644,
      "grad_norm": 7.898609638214111,
      "learning_rate": 7.500000000000001e-06,
      "loss": 2.1943,
      "step": 44
    },
    {
      "epoch": 0.0018612153736389863,
      "grad_norm": 7.830791473388672,
      "learning_rate": 7.369343312364994e-06,
      "loss": 2.2099,
      "step": 45
    },
    {
      "epoch": 0.0019025757152754082,
      "grad_norm": 8.45985221862793,
      "learning_rate": 7.236565741578163e-06,
      "loss": 2.1548,
      "step": 46
    },
    {
      "epoch": 0.0019439360569118301,
      "grad_norm": 8.227968215942383,
      "learning_rate": 7.101786141547829e-06,
      "loss": 2.2727,
      "step": 47
    },
    {
      "epoch": 0.001985296398548252,
      "grad_norm": 8.42605209350586,
      "learning_rate": 6.965125158269619e-06,
      "loss": 2.1907,
      "step": 48
    },
    {
      "epoch": 0.002026656740184674,
      "grad_norm": 8.992300033569336,
      "learning_rate": 6.8267051218319766e-06,
      "loss": 2.5127,
      "step": 49
    },
    {
      "epoch": 0.002068017081821096,
      "grad_norm": 10.033496856689453,
      "learning_rate": 6.686649936914151e-06,
      "loss": 2.4304,
      "step": 50
    },
    {
      "epoch": 0.0021093774234575177,
      "grad_norm": 4.379824638366699,
      "learning_rate": 6.545084971874738e-06,
      "loss": 2.0051,
      "step": 51
    },
    {
      "epoch": 0.00215073776509394,
      "grad_norm": 4.529748916625977,
      "learning_rate": 6.402136946530014e-06,
      "loss": 1.8965,
      "step": 52
    },
    {
      "epoch": 0.0021920981067303615,
      "grad_norm": 4.544522762298584,
      "learning_rate": 6.257933818722544e-06,
      "loss": 2.365,
      "step": 53
    },
    {
      "epoch": 0.0022334584483667836,
      "grad_norm": 5.609203815460205,
      "learning_rate": 6.112604669781572e-06,
      "loss": 1.9515,
      "step": 54
    },
    {
      "epoch": 0.0022748187900032053,
      "grad_norm": 5.310708522796631,
      "learning_rate": 5.9662795889777666e-06,
      "loss": 2.2309,
      "step": 55
    },
    {
      "epoch": 0.0023161791316396274,
      "grad_norm": 5.072017669677734,
      "learning_rate": 5.819089557075689e-06,
      "loss": 2.087,
      "step": 56
    },
    {
      "epoch": 0.002357539473276049,
      "grad_norm": 5.72014045715332,
      "learning_rate": 5.671166329088278e-06,
      "loss": 2.54,
      "step": 57
    },
    {
      "epoch": 0.002398899814912471,
      "grad_norm": 5.251782417297363,
      "learning_rate": 5.522642316338268e-06,
      "loss": 2.154,
      "step": 58
    },
    {
      "epoch": 0.0024402601565488933,
      "grad_norm": 5.470980167388916,
      "learning_rate": 5.373650467932122e-06,
      "loss": 2.0916,
      "step": 59
    },
    {
      "epoch": 0.002481620498185315,
      "grad_norm": 5.515152931213379,
      "learning_rate": 5.224324151752575e-06,
      "loss": 2.238,
      "step": 60
    },
    {
      "epoch": 0.002522980839821737,
      "grad_norm": 4.576385974884033,
      "learning_rate": 5.074797035076319e-06,
      "loss": 2.0014,
      "step": 61
    },
    {
      "epoch": 0.0025643411814581588,
      "grad_norm": 5.341975688934326,
      "learning_rate": 4.9252029649236835e-06,
      "loss": 1.9296,
      "step": 62
    },
    {
      "epoch": 0.002605701523094581,
      "grad_norm": 6.034480094909668,
      "learning_rate": 4.775675848247427e-06,
      "loss": 2.1047,
      "step": 63
    },
    {
      "epoch": 0.0026470618647310025,
      "grad_norm": 5.70397424697876,
      "learning_rate": 4.626349532067879e-06,
      "loss": 2.0329,
      "step": 64
    },
    {
      "epoch": 0.0026884222063674247,
      "grad_norm": 5.571841716766357,
      "learning_rate": 4.477357683661734e-06,
      "loss": 1.969,
      "step": 65
    },
    {
      "epoch": 0.0027297825480038463,
      "grad_norm": 7.667943477630615,
      "learning_rate": 4.3288336709117246e-06,
      "loss": 2.2417,
      "step": 66
    },
    {
      "epoch": 0.0027711428896402684,
      "grad_norm": 5.746097564697266,
      "learning_rate": 4.180910442924312e-06,
      "loss": 2.1811,
      "step": 67
    },
    {
      "epoch": 0.0028125032312766906,
      "grad_norm": 5.580810546875,
      "learning_rate": 4.033720411022235e-06,
      "loss": 2.0203,
      "step": 68
    },
    {
      "epoch": 0.0028538635729131122,
      "grad_norm": 5.511772155761719,
      "learning_rate": 3.887395330218429e-06,
      "loss": 2.0284,
      "step": 69
    },
    {
      "epoch": 0.0028952239145495343,
      "grad_norm": 5.374830722808838,
      "learning_rate": 3.7420661812774577e-06,
      "loss": 1.9312,
      "step": 70
    },
    {
      "epoch": 0.002936584256185956,
      "grad_norm": 5.854179382324219,
      "learning_rate": 3.5978630534699873e-06,
      "loss": 2.0589,
      "step": 71
    },
    {
      "epoch": 0.002977944597822378,
      "grad_norm": 6.205358505249023,
      "learning_rate": 3.4549150281252635e-06,
      "loss": 1.9499,
      "step": 72
    },
    {
      "epoch": 0.0030193049394588,
      "grad_norm": 6.435380458831787,
      "learning_rate": 3.3133500630858507e-06,
      "loss": 2.2904,
      "step": 73
    },
    {
      "epoch": 0.003060665281095222,
      "grad_norm": 6.161814212799072,
      "learning_rate": 3.173294878168025e-06,
      "loss": 2.1745,
      "step": 74
    },
    {
      "epoch": 0.0031020256227316436,
      "grad_norm": 6.130124092102051,
      "learning_rate": 3.0348748417303826e-06,
      "loss": 2.0112,
      "step": 75
    },
    {
      "epoch": 0.0031433859643680657,
      "grad_norm": 5.8785295486450195,
      "learning_rate": 2.8982138584521734e-06,
      "loss": 1.8818,
      "step": 76
    },
    {
      "epoch": 0.0031847463060044874,
      "grad_norm": 6.291286468505859,
      "learning_rate": 2.7634342584218364e-06,
      "loss": 1.9304,
      "step": 77
    },
    {
      "epoch": 0.0032261066476409095,
      "grad_norm": 7.331194877624512,
      "learning_rate": 2.6306566876350072e-06,
      "loss": 2.0353,
      "step": 78
    },
    {
      "epoch": 0.0032674669892773316,
      "grad_norm": 6.449727535247803,
      "learning_rate": 2.5000000000000015e-06,
      "loss": 1.7624,
      "step": 79
    },
    {
      "epoch": 0.0033088273309137533,
      "grad_norm": 6.803339004516602,
      "learning_rate": 2.371581150947476e-06,
      "loss": 2.1373,
      "step": 80
    },
    {
      "epoch": 0.0033501876725501754,
      "grad_norm": 6.576654434204102,
      "learning_rate": 2.245515092739488e-06,
      "loss": 1.8231,
      "step": 81
    },
    {
      "epoch": 0.003391548014186597,
      "grad_norm": 6.311235427856445,
      "learning_rate": 2.1219146715716332e-06,
      "loss": 1.9263,
      "step": 82
    },
    {
      "epoch": 0.003432908355823019,
      "grad_norm": 6.848902225494385,
      "learning_rate": 2.0008905265604316e-06,
      "loss": 2.3057,
      "step": 83
    },
    {
      "epoch": 0.003474268697459441,
      "grad_norm": 6.928767681121826,
      "learning_rate": 1.8825509907063328e-06,
      "loss": 2.2425,
      "step": 84
    },
    {
      "epoch": 0.003515629039095863,
      "grad_norm": 7.622620105743408,
      "learning_rate": 1.7670019939210025e-06,
      "loss": 2.1302,
      "step": 85
    },
    {
      "epoch": 0.0035569893807322847,
      "grad_norm": 7.2634358406066895,
      "learning_rate": 1.6543469682057105e-06,
      "loss": 2.1236,
      "step": 86
    },
    {
      "epoch": 0.0035983497223687068,
      "grad_norm": 7.554080963134766,
      "learning_rate": 1.5446867550656774e-06,
      "loss": 1.9996,
      "step": 87
    },
    {
      "epoch": 0.003639710064005129,
      "grad_norm": 7.214341163635254,
      "learning_rate": 1.438119515243277e-06,
      "loss": 1.8336,
      "step": 88
    },
    {
      "epoch": 0.0036810704056415506,
      "grad_norm": 6.677700996398926,
      "learning_rate": 1.3347406408508702e-06,
      "loss": 1.9391,
      "step": 89
    },
    {
      "epoch": 0.0037224307472779727,
      "grad_norm": 7.255701065063477,
      "learning_rate": 1.234642669981946e-06,
      "loss": 1.9219,
      "step": 90
    }
  ],
  "logging_steps": 1,
  "max_steps": 110,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2699989855690752.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
