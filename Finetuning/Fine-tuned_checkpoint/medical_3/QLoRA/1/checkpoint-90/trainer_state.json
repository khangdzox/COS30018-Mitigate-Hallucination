{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.002517834662190516,
  "eval_steps": 500,
  "global_step": 90,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 2.7975940691005735e-05,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.0912,
      "step": 1
    },
    {
      "epoch": 5.595188138201147e-05,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.3688,
      "step": 2
    },
    {
      "epoch": 8.39278220730172e-05,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 1.7056,
      "step": 3
    },
    {
      "epoch": 0.00011190376276402294,
      "grad_norm": 3.783850908279419,
      "learning_rate": 6e-06,
      "loss": 2.0922,
      "step": 4
    },
    {
      "epoch": 0.00013987970345502866,
      "grad_norm": 3.8983192443847656,
      "learning_rate": 1.2e-05,
      "loss": 2.0968,
      "step": 5
    },
    {
      "epoch": 0.0001678556441460344,
      "grad_norm": 3.9126274585723877,
      "learning_rate": 1.8e-05,
      "loss": 2.5119,
      "step": 6
    },
    {
      "epoch": 0.00019583158483704014,
      "grad_norm": 4.3431172370910645,
      "learning_rate": 2.4e-05,
      "loss": 2.1283,
      "step": 7
    },
    {
      "epoch": 0.00022380752552804588,
      "grad_norm": 5.04906702041626,
      "learning_rate": 3e-05,
      "loss": 2.4234,
      "step": 8
    },
    {
      "epoch": 0.0002517834662190516,
      "grad_norm": 4.506409645080566,
      "learning_rate": 2.9999697901093597e-05,
      "loss": 2.1856,
      "step": 9
    },
    {
      "epoch": 0.00027975940691005733,
      "grad_norm": 4.346327781677246,
      "learning_rate": 2.9998791616542892e-05,
      "loss": 2.2305,
      "step": 10
    },
    {
      "epoch": 0.00030773534760106307,
      "grad_norm": 4.162479400634766,
      "learning_rate": 2.9997281182852888e-05,
      "loss": 2.0137,
      "step": 11
    },
    {
      "epoch": 0.0003357112882920688,
      "grad_norm": 4.628355503082275,
      "learning_rate": 2.9995166660863637e-05,
      "loss": 2.0176,
      "step": 12
    },
    {
      "epoch": 0.00036368722898307454,
      "grad_norm": 5.497799396514893,
      "learning_rate": 2.999244813574778e-05,
      "loss": 2.1839,
      "step": 13
    },
    {
      "epoch": 0.0003916631696740803,
      "grad_norm": 4.628487586975098,
      "learning_rate": 2.9989125717007107e-05,
      "loss": 2.1463,
      "step": 14
    },
    {
      "epoch": 0.000419639110365086,
      "grad_norm": 5.733229637145996,
      "learning_rate": 2.9985199538468158e-05,
      "loss": 2.205,
      "step": 15
    },
    {
      "epoch": 0.00044761505105609176,
      "grad_norm": 7.184881210327148,
      "learning_rate": 2.998066975827684e-05,
      "loss": 2.2628,
      "step": 16
    },
    {
      "epoch": 0.0004755909917470975,
      "grad_norm": 4.9925537109375,
      "learning_rate": 2.9975536558892034e-05,
      "loss": 2.2355,
      "step": 17
    },
    {
      "epoch": 0.0005035669324381032,
      "grad_norm": 6.217339038848877,
      "learning_rate": 2.9969800147078265e-05,
      "loss": 2.2262,
      "step": 18
    },
    {
      "epoch": 0.0005315428731291089,
      "grad_norm": 5.992177486419678,
      "learning_rate": 2.9963460753897364e-05,
      "loss": 2.2691,
      "step": 19
    },
    {
      "epoch": 0.0005595188138201147,
      "grad_norm": 6.715956211090088,
      "learning_rate": 2.995651863469916e-05,
      "loss": 2.1157,
      "step": 20
    },
    {
      "epoch": 0.0005874947545111204,
      "grad_norm": 5.827864646911621,
      "learning_rate": 2.994897406911121e-05,
      "loss": 2.2117,
      "step": 21
    },
    {
      "epoch": 0.0006154706952021261,
      "grad_norm": 6.2691650390625,
      "learning_rate": 2.994082736102751e-05,
      "loss": 1.9871,
      "step": 22
    },
    {
      "epoch": 0.0006434466358931319,
      "grad_norm": 5.677264213562012,
      "learning_rate": 2.993207883859627e-05,
      "loss": 2.0584,
      "step": 23
    },
    {
      "epoch": 0.0006714225765841376,
      "grad_norm": 5.883533477783203,
      "learning_rate": 2.9922728854206704e-05,
      "loss": 1.9854,
      "step": 24
    },
    {
      "epoch": 0.0006993985172751433,
      "grad_norm": 6.103875160217285,
      "learning_rate": 2.991277778447482e-05,
      "loss": 2.1074,
      "step": 25
    },
    {
      "epoch": 0.0007273744579661491,
      "grad_norm": 5.402991771697998,
      "learning_rate": 2.9902226030228252e-05,
      "loss": 2.0998,
      "step": 26
    },
    {
      "epoch": 0.0007553503986571548,
      "grad_norm": 5.308849334716797,
      "learning_rate": 2.989107401649013e-05,
      "loss": 2.0289,
      "step": 27
    },
    {
      "epoch": 0.0007833263393481606,
      "grad_norm": 5.444468021392822,
      "learning_rate": 2.9879322192461932e-05,
      "loss": 2.1144,
      "step": 28
    },
    {
      "epoch": 0.0008113022800391663,
      "grad_norm": 6.0809736251831055,
      "learning_rate": 2.986697103150542e-05,
      "loss": 2.0215,
      "step": 29
    },
    {
      "epoch": 0.000839278220730172,
      "grad_norm": 6.157512664794922,
      "learning_rate": 2.9854021031123555e-05,
      "loss": 1.9675,
      "step": 30
    },
    {
      "epoch": 0.0008672541614211778,
      "grad_norm": 6.444407939910889,
      "learning_rate": 2.984047271294047e-05,
      "loss": 1.7704,
      "step": 31
    },
    {
      "epoch": 0.0008952301021121835,
      "grad_norm": 6.372190952301025,
      "learning_rate": 2.9826326622680436e-05,
      "loss": 2.0241,
      "step": 32
    },
    {
      "epoch": 0.0009232060428031893,
      "grad_norm": 6.4107537269592285,
      "learning_rate": 2.9811583330145915e-05,
      "loss": 1.9214,
      "step": 33
    },
    {
      "epoch": 0.000951181983494195,
      "grad_norm": 7.94268274307251,
      "learning_rate": 2.9796243429194578e-05,
      "loss": 1.9085,
      "step": 34
    },
    {
      "epoch": 0.0009791579241852006,
      "grad_norm": 8.443192481994629,
      "learning_rate": 2.9780307537715396e-05,
      "loss": 1.9179,
      "step": 35
    },
    {
      "epoch": 0.0010071338648762064,
      "grad_norm": 7.704146385192871,
      "learning_rate": 2.9763776297603758e-05,
      "loss": 2.0051,
      "step": 36
    },
    {
      "epoch": 0.001035109805567212,
      "grad_norm": NaN,
      "learning_rate": 2.9763776297603758e-05,
      "loss": 1.8298,
      "step": 37
    },
    {
      "epoch": 0.0010630857462582178,
      "grad_norm": 7.468281269073486,
      "learning_rate": 2.97466503747356e-05,
      "loss": 1.5523,
      "step": 38
    },
    {
      "epoch": 0.0010910616869492236,
      "grad_norm": 7.660518646240234,
      "learning_rate": 2.97289304589406e-05,
      "loss": 1.6366,
      "step": 39
    },
    {
      "epoch": 0.0011190376276402293,
      "grad_norm": 8.517303466796875,
      "learning_rate": 2.9710617263974385e-05,
      "loss": 1.788,
      "step": 40
    },
    {
      "epoch": 0.001147013568331235,
      "grad_norm": 8.836347579956055,
      "learning_rate": 2.9691711527489777e-05,
      "loss": 1.5528,
      "step": 41
    },
    {
      "epoch": 0.0011749895090222408,
      "grad_norm": 9.64724349975586,
      "learning_rate": 2.9672214011007087e-05,
      "loss": 1.7166,
      "step": 42
    },
    {
      "epoch": 0.0012029654497132465,
      "grad_norm": 8.415793418884277,
      "learning_rate": 2.9652125499883428e-05,
      "loss": 1.5261,
      "step": 43
    },
    {
      "epoch": 0.0012309413904042523,
      "grad_norm": 9.289592742919922,
      "learning_rate": 2.963144680328111e-05,
      "loss": 1.5398,
      "step": 44
    },
    {
      "epoch": 0.001258917331095258,
      "grad_norm": 9.380474090576172,
      "learning_rate": 2.9610178754135005e-05,
      "loss": 1.6714,
      "step": 45
    },
    {
      "epoch": 0.0012868932717862637,
      "grad_norm": 9.136528015136719,
      "learning_rate": 2.9588322209119037e-05,
      "loss": 1.3726,
      "step": 46
    },
    {
      "epoch": 0.0013148692124772695,
      "grad_norm": 10.976570129394531,
      "learning_rate": 2.9565878048611655e-05,
      "loss": 1.4764,
      "step": 47
    },
    {
      "epoch": 0.0013428451531682752,
      "grad_norm": 13.07079792022705,
      "learning_rate": 2.954284717666037e-05,
      "loss": 1.2505,
      "step": 48
    },
    {
      "epoch": 0.001370821093859281,
      "grad_norm": 11.639213562011719,
      "learning_rate": 2.9519230520945346e-05,
      "loss": 1.2771,
      "step": 49
    },
    {
      "epoch": 0.0013987970345502867,
      "grad_norm": 14.19975471496582,
      "learning_rate": 2.9495029032742025e-05,
      "loss": 1.3922,
      "step": 50
    },
    {
      "epoch": 0.0014267729752412924,
      "grad_norm": 4.525470733642578,
      "learning_rate": 2.9470243686882838e-05,
      "loss": 1.8655,
      "step": 51
    },
    {
      "epoch": 0.0014547489159322982,
      "grad_norm": 5.483293056488037,
      "learning_rate": 2.9444875481717888e-05,
      "loss": 1.5625,
      "step": 52
    },
    {
      "epoch": 0.001482724856623304,
      "grad_norm": 5.020266532897949,
      "learning_rate": 2.9418925439074784e-05,
      "loss": 1.783,
      "step": 53
    },
    {
      "epoch": 0.0015107007973143096,
      "grad_norm": 5.828213214874268,
      "learning_rate": 2.939239460421746e-05,
      "loss": 1.6158,
      "step": 54
    },
    {
      "epoch": 0.0015386767380053154,
      "grad_norm": 5.895217418670654,
      "learning_rate": 2.936528404580408e-05,
      "loss": 1.6217,
      "step": 55
    },
    {
      "epoch": 0.0015666526786963211,
      "grad_norm": 5.307732105255127,
      "learning_rate": 2.9337594855843976e-05,
      "loss": 1.6712,
      "step": 56
    },
    {
      "epoch": 0.0015946286193873269,
      "grad_norm": 5.299658298492432,
      "learning_rate": 2.930932814965369e-05,
      "loss": 1.5082,
      "step": 57
    },
    {
      "epoch": 0.0016226045600783326,
      "grad_norm": 6.314249038696289,
      "learning_rate": 2.9280485065812025e-05,
      "loss": 1.7292,
      "step": 58
    },
    {
      "epoch": 0.0016505805007693383,
      "grad_norm": 7.312399387359619,
      "learning_rate": 2.925106676611418e-05,
      "loss": 1.9197,
      "step": 59
    },
    {
      "epoch": 0.001678556441460344,
      "grad_norm": 6.54948616027832,
      "learning_rate": 2.9221074435524995e-05,
      "loss": 1.4763,
      "step": 60
    },
    {
      "epoch": 0.0017065323821513498,
      "grad_norm": 6.718735694885254,
      "learning_rate": 2.9190509282131156e-05,
      "loss": 1.4647,
      "step": 61
    },
    {
      "epoch": 0.0017345083228423556,
      "grad_norm": 6.8831892013549805,
      "learning_rate": 2.91593725370926e-05,
      "loss": 1.7518,
      "step": 62
    },
    {
      "epoch": 0.0017624842635333613,
      "grad_norm": 6.049627304077148,
      "learning_rate": 2.9127665454592872e-05,
      "loss": 1.5605,
      "step": 63
    },
    {
      "epoch": 0.001790460204224367,
      "grad_norm": 6.809024333953857,
      "learning_rate": 2.9095389311788626e-05,
      "loss": 1.589,
      "step": 64
    },
    {
      "epoch": 0.0018184361449153728,
      "grad_norm": 7.937854290008545,
      "learning_rate": 2.9062545408758193e-05,
      "loss": 1.5983,
      "step": 65
    },
    {
      "epoch": 0.0018464120856063785,
      "grad_norm": 6.928554534912109,
      "learning_rate": 2.90291350684492e-05,
      "loss": 1.4824,
      "step": 66
    },
    {
      "epoch": 0.0018743880262973842,
      "grad_norm": 6.012258529663086,
      "learning_rate": 2.899515963662528e-05,
      "loss": 1.3995,
      "step": 67
    },
    {
      "epoch": 0.00190236396698839,
      "grad_norm": 8.374756813049316,
      "learning_rate": 2.8960620481811866e-05,
      "loss": 1.5199,
      "step": 68
    },
    {
      "epoch": 0.0019303399076793957,
      "grad_norm": 6.462904453277588,
      "learning_rate": 2.892551899524109e-05,
      "loss": 1.3518,
      "step": 69
    },
    {
      "epoch": 0.0019583158483704012,
      "grad_norm": 7.308802127838135,
      "learning_rate": 2.8889856590795705e-05,
      "loss": 1.6546,
      "step": 70
    },
    {
      "epoch": 0.001986291789061407,
      "grad_norm": 7.1754326820373535,
      "learning_rate": 2.8853634704952167e-05,
      "loss": 1.4177,
      "step": 71
    },
    {
      "epoch": 0.0020142677297524127,
      "grad_norm": 7.904433727264404,
      "learning_rate": 2.8816854796722754e-05,
      "loss": 1.3452,
      "step": 72
    },
    {
      "epoch": 0.0020422436704434187,
      "grad_norm": 6.5022873878479,
      "learning_rate": 2.8779518347596803e-05,
      "loss": 1.3065,
      "step": 73
    },
    {
      "epoch": 0.002070219611134424,
      "grad_norm": 6.062945365905762,
      "learning_rate": 2.8741626861481043e-05,
      "loss": 1.4336,
      "step": 74
    },
    {
      "epoch": 0.00209819555182543,
      "grad_norm": 7.08620548248291,
      "learning_rate": 2.8703181864639013e-05,
      "loss": 1.3526,
      "step": 75
    },
    {
      "epoch": 0.0021261714925164357,
      "grad_norm": 6.188721656799316,
      "learning_rate": 2.8664184905629577e-05,
      "loss": 1.2484,
      "step": 76
    },
    {
      "epoch": 0.0021541474332074416,
      "grad_norm": 6.952242374420166,
      "learning_rate": 2.8624637555244556e-05,
      "loss": 1.3292,
      "step": 77
    },
    {
      "epoch": 0.002182123373898447,
      "grad_norm": 9.066784858703613,
      "learning_rate": 2.8584541406445462e-05,
      "loss": 1.5459,
      "step": 78
    },
    {
      "epoch": 0.002210099314589453,
      "grad_norm": 7.1990790367126465,
      "learning_rate": 2.8543898074299322e-05,
      "loss": 1.1734,
      "step": 79
    },
    {
      "epoch": 0.0022380752552804586,
      "grad_norm": 7.529713153839111,
      "learning_rate": 2.8502709195913617e-05,
      "loss": 1.4157,
      "step": 80
    },
    {
      "epoch": 0.0022660511959714646,
      "grad_norm": 7.110294818878174,
      "learning_rate": 2.8460976430370375e-05,
      "loss": 1.3715,
      "step": 81
    },
    {
      "epoch": 0.00229402713666247,
      "grad_norm": 7.899359226226807,
      "learning_rate": 2.8418701458659307e-05,
      "loss": 1.144,
      "step": 82
    },
    {
      "epoch": 0.002322003077353476,
      "grad_norm": 8.898456573486328,
      "learning_rate": 2.83758859836101e-05,
      "loss": 1.2892,
      "step": 83
    },
    {
      "epoch": 0.0023499790180444816,
      "grad_norm": 7.804157257080078,
      "learning_rate": 2.8332531729823853e-05,
      "loss": 1.2691,
      "step": 84
    },
    {
      "epoch": 0.0023779549587354875,
      "grad_norm": 8.660775184631348,
      "learning_rate": 2.8288640443603587e-05,
      "loss": 1.2027,
      "step": 85
    },
    {
      "epoch": 0.002405930899426493,
      "grad_norm": 7.666698932647705,
      "learning_rate": 2.8244213892883907e-05,
      "loss": 1.2985,
      "step": 86
    },
    {
      "epoch": 0.002433906840117499,
      "grad_norm": 9.67064094543457,
      "learning_rate": 2.81992538671598e-05,
      "loss": 1.0268,
      "step": 87
    },
    {
      "epoch": 0.0024618827808085045,
      "grad_norm": 8.72736644744873,
      "learning_rate": 2.8153762177414545e-05,
      "loss": 0.9557,
      "step": 88
    },
    {
      "epoch": 0.0024898587214995105,
      "grad_norm": 8.585872650146484,
      "learning_rate": 2.8107740656046775e-05,
      "loss": 1.0785,
      "step": 89
    },
    {
      "epoch": 0.002517834662190516,
      "grad_norm": 11.038117408752441,
      "learning_rate": 2.8061191156796658e-05,
      "loss": 1.2215,
      "step": 90
    }
  ],
  "logging_steps": 1,
  "max_steps": 500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3407984593772544.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
