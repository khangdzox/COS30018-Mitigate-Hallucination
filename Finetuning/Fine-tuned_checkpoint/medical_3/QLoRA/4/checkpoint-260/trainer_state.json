{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.017223105458399578,
  "eval_steps": 500,
  "global_step": 260,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 6.624271330153684e-05,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.2689,
      "step": 1
    },
    {
      "epoch": 0.00013248542660307367,
      "grad_norm": 3.935640573501587,
      "learning_rate": 2e-05,
      "loss": 2.3392,
      "step": 2
    },
    {
      "epoch": 0.0001987281399046105,
      "grad_norm": NaN,
      "learning_rate": 2e-05,
      "loss": 2.328,
      "step": 3
    },
    {
      "epoch": 0.00026497085320614734,
      "grad_norm": 3.172327995300293,
      "learning_rate": 4e-05,
      "loss": 2.23,
      "step": 4
    },
    {
      "epoch": 0.00033121356650768415,
      "grad_norm": 4.042605876922607,
      "learning_rate": 6e-05,
      "loss": 2.324,
      "step": 5
    },
    {
      "epoch": 0.000397456279809221,
      "grad_norm": 4.5806450843811035,
      "learning_rate": 8e-05,
      "loss": 2.3925,
      "step": 6
    },
    {
      "epoch": 0.0004636989931107578,
      "grad_norm": 3.348752498626709,
      "learning_rate": 0.0001,
      "loss": 2.1809,
      "step": 7
    },
    {
      "epoch": 0.0005299417064122947,
      "grad_norm": 3.5855343341827393,
      "learning_rate": 9.999899300364532e-05,
      "loss": 2.181,
      "step": 8
    },
    {
      "epoch": 0.0005961844197138314,
      "grad_norm": 3.9782562255859375,
      "learning_rate": 9.999597205514297e-05,
      "loss": 2.2029,
      "step": 9
    },
    {
      "epoch": 0.0006624271330153683,
      "grad_norm": 3.451857089996338,
      "learning_rate": 9.99909372761763e-05,
      "loss": 1.825,
      "step": 10
    },
    {
      "epoch": 0.0007286698463169052,
      "grad_norm": 3.3541646003723145,
      "learning_rate": 9.998388886954547e-05,
      "loss": 1.9844,
      "step": 11
    },
    {
      "epoch": 0.000794912559618442,
      "grad_norm": 4.102404594421387,
      "learning_rate": 9.997482711915927e-05,
      "loss": 2.1533,
      "step": 12
    },
    {
      "epoch": 0.0008611552729199788,
      "grad_norm": 5.836984634399414,
      "learning_rate": 9.996375239002369e-05,
      "loss": 1.9529,
      "step": 13
    },
    {
      "epoch": 0.0009273979862215156,
      "grad_norm": 4.671607971191406,
      "learning_rate": 9.99506651282272e-05,
      "loss": 1.7419,
      "step": 14
    },
    {
      "epoch": 0.0009936406995230524,
      "grad_norm": 6.9419403076171875,
      "learning_rate": 9.993556586092281e-05,
      "loss": 1.6356,
      "step": 15
    },
    {
      "epoch": 0.0010598834128245894,
      "grad_norm": 5.967659950256348,
      "learning_rate": 9.991845519630678e-05,
      "loss": 1.6832,
      "step": 16
    },
    {
      "epoch": 0.0011261261261261261,
      "grad_norm": 6.001219272613525,
      "learning_rate": 9.989933382359422e-05,
      "loss": 1.3467,
      "step": 17
    },
    {
      "epoch": 0.0011923688394276629,
      "grad_norm": 7.259174823760986,
      "learning_rate": 9.987820251299122e-05,
      "loss": 1.3676,
      "step": 18
    },
    {
      "epoch": 0.0012586115527291998,
      "grad_norm": 7.373693943023682,
      "learning_rate": 9.985506211566388e-05,
      "loss": 1.2509,
      "step": 19
    },
    {
      "epoch": 0.0013248542660307366,
      "grad_norm": 7.179003715515137,
      "learning_rate": 9.982991356370404e-05,
      "loss": 1.3077,
      "step": 20
    },
    {
      "epoch": 0.0013910969793322733,
      "grad_norm": 4.5587239265441895,
      "learning_rate": 9.98027578700917e-05,
      "loss": 1.5136,
      "step": 21
    },
    {
      "epoch": 0.0014573396926338103,
      "grad_norm": 4.268433570861816,
      "learning_rate": 9.977359612865423e-05,
      "loss": 1.2645,
      "step": 22
    },
    {
      "epoch": 0.001523582405935347,
      "grad_norm": 4.881389141082764,
      "learning_rate": 9.974242951402235e-05,
      "loss": 1.3434,
      "step": 23
    },
    {
      "epoch": 0.001589825119236884,
      "grad_norm": 4.863972187042236,
      "learning_rate": 9.970925928158274e-05,
      "loss": 1.3195,
      "step": 24
    },
    {
      "epoch": 0.0016560678325384208,
      "grad_norm": 17.17622947692871,
      "learning_rate": 9.967408676742751e-05,
      "loss": 1.3572,
      "step": 25
    },
    {
      "epoch": 0.0017223105458399576,
      "grad_norm": 3.2035300731658936,
      "learning_rate": 9.963691338830044e-05,
      "loss": 1.2448,
      "step": 26
    },
    {
      "epoch": 0.0017885532591414945,
      "grad_norm": 3.4512383937835693,
      "learning_rate": 9.959774064153977e-05,
      "loss": 1.2655,
      "step": 27
    },
    {
      "epoch": 0.0018547959724430313,
      "grad_norm": 2.893615245819092,
      "learning_rate": 9.955657010501806e-05,
      "loss": 1.232,
      "step": 28
    },
    {
      "epoch": 0.001921038685744568,
      "grad_norm": 3.7472167015075684,
      "learning_rate": 9.951340343707852e-05,
      "loss": 1.4194,
      "step": 29
    },
    {
      "epoch": 0.001987281399046105,
      "grad_norm": 3.8347768783569336,
      "learning_rate": 9.946824237646824e-05,
      "loss": 1.2648,
      "step": 30
    },
    {
      "epoch": 0.0020535241123476418,
      "grad_norm": 3.21195125579834,
      "learning_rate": 9.942108874226811e-05,
      "loss": 1.3137,
      "step": 31
    },
    {
      "epoch": 0.0021197668256491787,
      "grad_norm": 3.4692153930664062,
      "learning_rate": 9.937194443381972e-05,
      "loss": 1.1838,
      "step": 32
    },
    {
      "epoch": 0.0021860095389507153,
      "grad_norm": 3.0090458393096924,
      "learning_rate": 9.93208114306486e-05,
      "loss": 1.3544,
      "step": 33
    },
    {
      "epoch": 0.0022522522522522522,
      "grad_norm": 3.2744686603546143,
      "learning_rate": 9.926769179238466e-05,
      "loss": 1.3004,
      "step": 34
    },
    {
      "epoch": 0.002318494965553789,
      "grad_norm": 2.9766061305999756,
      "learning_rate": 9.921258765867919e-05,
      "loss": 1.2653,
      "step": 35
    },
    {
      "epoch": 0.0023847376788553257,
      "grad_norm": 3.0921878814697266,
      "learning_rate": 9.915550124911866e-05,
      "loss": 1.1329,
      "step": 36
    },
    {
      "epoch": 0.0024509803921568627,
      "grad_norm": 2.7303004264831543,
      "learning_rate": 9.909643486313533e-05,
      "loss": 1.089,
      "step": 37
    },
    {
      "epoch": 0.0025172231054583997,
      "grad_norm": 3.438199758529663,
      "learning_rate": 9.903539087991462e-05,
      "loss": 1.3225,
      "step": 38
    },
    {
      "epoch": 0.0025834658187599362,
      "grad_norm": 2.709054470062256,
      "learning_rate": 9.897237175829926e-05,
      "loss": 1.1871,
      "step": 39
    },
    {
      "epoch": 0.002649708532061473,
      "grad_norm": 2.9012386798858643,
      "learning_rate": 9.890738003669029e-05,
      "loss": 1.2922,
      "step": 40
    },
    {
      "epoch": 0.00271595124536301,
      "grad_norm": 3.2810704708099365,
      "learning_rate": 9.884041833294476e-05,
      "loss": 1.1932,
      "step": 41
    },
    {
      "epoch": 0.0027821939586645467,
      "grad_norm": 2.953472852706909,
      "learning_rate": 9.877148934427037e-05,
      "loss": 0.9832,
      "step": 42
    },
    {
      "epoch": 0.0028484366719660837,
      "grad_norm": 3.304927349090576,
      "learning_rate": 9.870059584711668e-05,
      "loss": 0.967,
      "step": 43
    },
    {
      "epoch": 0.0029146793852676206,
      "grad_norm": 3.002321720123291,
      "learning_rate": 9.862774069706346e-05,
      "loss": 1.1302,
      "step": 44
    },
    {
      "epoch": 0.0029809220985691576,
      "grad_norm": 3.673306465148926,
      "learning_rate": 9.855292682870551e-05,
      "loss": 1.1649,
      "step": 45
    },
    {
      "epoch": 0.003047164811870694,
      "grad_norm": 3.451751232147217,
      "learning_rate": 9.847615725553456e-05,
      "loss": 1.1961,
      "step": 46
    },
    {
      "epoch": 0.003113407525172231,
      "grad_norm": 3.4889540672302246,
      "learning_rate": 9.839743506981782e-05,
      "loss": 1.1033,
      "step": 47
    },
    {
      "epoch": 0.003179650238473768,
      "grad_norm": 3.8954901695251465,
      "learning_rate": 9.831676344247342e-05,
      "loss": 1.0559,
      "step": 48
    },
    {
      "epoch": 0.0032458929517753046,
      "grad_norm": 3.8720455169677734,
      "learning_rate": 9.82341456229428e-05,
      "loss": 1.2445,
      "step": 49
    },
    {
      "epoch": 0.0033121356650768416,
      "grad_norm": 2.5133635997772217,
      "learning_rate": 9.814958493905963e-05,
      "loss": 0.9382,
      "step": 50
    },
    {
      "epoch": 0.0033783783783783786,
      "grad_norm": 3.5231356620788574,
      "learning_rate": 9.806308479691595e-05,
      "loss": 1.2273,
      "step": 51
    },
    {
      "epoch": 0.003444621091679915,
      "grad_norm": 3.230523109436035,
      "learning_rate": 9.797464868072488e-05,
      "loss": 1.2495,
      "step": 52
    },
    {
      "epoch": 0.003510863804981452,
      "grad_norm": 3.12392520904541,
      "learning_rate": 9.788428015268027e-05,
      "loss": 1.2135,
      "step": 53
    },
    {
      "epoch": 0.003577106518282989,
      "grad_norm": 3.016922950744629,
      "learning_rate": 9.779198285281325e-05,
      "loss": 1.1439,
      "step": 54
    },
    {
      "epoch": 0.0036433492315845256,
      "grad_norm": 3.000917673110962,
      "learning_rate": 9.769776049884563e-05,
      "loss": 1.1964,
      "step": 55
    },
    {
      "epoch": 0.0037095919448860626,
      "grad_norm": 3.609166145324707,
      "learning_rate": 9.760161688604008e-05,
      "loss": 1.0649,
      "step": 56
    },
    {
      "epoch": 0.0037758346581875995,
      "grad_norm": 3.220571517944336,
      "learning_rate": 9.750355588704727e-05,
      "loss": 1.2922,
      "step": 57
    },
    {
      "epoch": 0.003842077371489136,
      "grad_norm": 3.3062751293182373,
      "learning_rate": 9.740358145174998e-05,
      "loss": 1.2174,
      "step": 58
    },
    {
      "epoch": 0.003908320084790673,
      "grad_norm": 3.251234769821167,
      "learning_rate": 9.730169760710386e-05,
      "loss": 1.2926,
      "step": 59
    },
    {
      "epoch": 0.00397456279809221,
      "grad_norm": 3.6804213523864746,
      "learning_rate": 9.719790845697533e-05,
      "loss": 0.9211,
      "step": 60
    },
    {
      "epoch": 0.0040408055113937465,
      "grad_norm": 3.72613787651062,
      "learning_rate": 9.709221818197624e-05,
      "loss": 1.13,
      "step": 61
    },
    {
      "epoch": 0.0041070482246952835,
      "grad_norm": 3.939169406890869,
      "learning_rate": 9.698463103929542e-05,
      "loss": 0.9656,
      "step": 62
    },
    {
      "epoch": 0.0041732909379968205,
      "grad_norm": 3.0876150131225586,
      "learning_rate": 9.687515136252731e-05,
      "loss": 1.1424,
      "step": 63
    },
    {
      "epoch": 0.0042395336512983575,
      "grad_norm": 3.3746180534362793,
      "learning_rate": 9.676378356149734e-05,
      "loss": 1.2594,
      "step": 64
    },
    {
      "epoch": 0.004305776364599894,
      "grad_norm": 4.579237461090088,
      "learning_rate": 9.665053212208426e-05,
      "loss": 1.1548,
      "step": 65
    },
    {
      "epoch": 0.0043720190779014305,
      "grad_norm": 3.825326919555664,
      "learning_rate": 9.653540160603956e-05,
      "loss": 1.2272,
      "step": 66
    },
    {
      "epoch": 0.0044382617912029675,
      "grad_norm": 3.9778435230255127,
      "learning_rate": 9.641839665080363e-05,
      "loss": 1.0409,
      "step": 67
    },
    {
      "epoch": 0.0045045045045045045,
      "grad_norm": 3.0924837589263916,
      "learning_rate": 9.629952196931901e-05,
      "loss": 1.0821,
      "step": 68
    },
    {
      "epoch": 0.0045707472178060414,
      "grad_norm": 3.241750955581665,
      "learning_rate": 9.617878234984055e-05,
      "loss": 1.0478,
      "step": 69
    },
    {
      "epoch": 0.004636989931107578,
      "grad_norm": 3.575681209564209,
      "learning_rate": 9.60561826557425e-05,
      "loss": 1.3172,
      "step": 70
    },
    {
      "epoch": 0.004703232644409115,
      "grad_norm": 3.798192262649536,
      "learning_rate": 9.593172782532268e-05,
      "loss": 1.1609,
      "step": 71
    },
    {
      "epoch": 0.0047694753577106515,
      "grad_norm": 3.1822009086608887,
      "learning_rate": 9.580542287160348e-05,
      "loss": 1.4767,
      "step": 72
    },
    {
      "epoch": 0.0048357180710121885,
      "grad_norm": 3.501284122467041,
      "learning_rate": 9.567727288213005e-05,
      "loss": 1.4139,
      "step": 73
    },
    {
      "epoch": 0.004901960784313725,
      "grad_norm": 2.887429714202881,
      "learning_rate": 9.554728301876526e-05,
      "loss": 1.0316,
      "step": 74
    },
    {
      "epoch": 0.004968203497615262,
      "grad_norm": 3.154911518096924,
      "learning_rate": 9.541545851748186e-05,
      "loss": 1.19,
      "step": 75
    },
    {
      "epoch": 0.005034446210916799,
      "grad_norm": 2.869253158569336,
      "learning_rate": 9.528180468815155e-05,
      "loss": 1.2745,
      "step": 76
    },
    {
      "epoch": 0.005100688924218336,
      "grad_norm": 3.471853017807007,
      "learning_rate": 9.514632691433107e-05,
      "loss": 1.0286,
      "step": 77
    },
    {
      "epoch": 0.0051669316375198724,
      "grad_norm": 3.389716148376465,
      "learning_rate": 9.50090306530454e-05,
      "loss": 1.0855,
      "step": 78
    },
    {
      "epoch": 0.005233174350821409,
      "grad_norm": 3.243983030319214,
      "learning_rate": 9.486992143456792e-05,
      "loss": 1.1931,
      "step": 79
    },
    {
      "epoch": 0.005299417064122946,
      "grad_norm": 3.344263792037964,
      "learning_rate": 9.472900486219769e-05,
      "loss": 1.2749,
      "step": 80
    },
    {
      "epoch": 0.005365659777424483,
      "grad_norm": 3.2355079650878906,
      "learning_rate": 9.458628661203367e-05,
      "loss": 0.9771,
      "step": 81
    },
    {
      "epoch": 0.00543190249072602,
      "grad_norm": 3.1826441287994385,
      "learning_rate": 9.444177243274618e-05,
      "loss": 1.244,
      "step": 82
    },
    {
      "epoch": 0.005498145204027557,
      "grad_norm": 3.1203207969665527,
      "learning_rate": 9.429546814534529e-05,
      "loss": 1.0937,
      "step": 83
    },
    {
      "epoch": 0.005564387917329093,
      "grad_norm": 3.6420516967773438,
      "learning_rate": 9.414737964294636e-05,
      "loss": 1.0575,
      "step": 84
    },
    {
      "epoch": 0.00563063063063063,
      "grad_norm": 2.949049949645996,
      "learning_rate": 9.399751289053267e-05,
      "loss": 0.9306,
      "step": 85
    },
    {
      "epoch": 0.005696873343932167,
      "grad_norm": 4.718625545501709,
      "learning_rate": 9.384587392471515e-05,
      "loss": 1.3255,
      "step": 86
    },
    {
      "epoch": 0.005763116057233704,
      "grad_norm": 3.3518567085266113,
      "learning_rate": 9.369246885348926e-05,
      "loss": 1.2166,
      "step": 87
    },
    {
      "epoch": 0.005829358770535241,
      "grad_norm": 4.522928714752197,
      "learning_rate": 9.353730385598887e-05,
      "loss": 1.0191,
      "step": 88
    },
    {
      "epoch": 0.005895601483836778,
      "grad_norm": 3.1650099754333496,
      "learning_rate": 9.338038518223747e-05,
      "loss": 0.8991,
      "step": 89
    },
    {
      "epoch": 0.005961844197138315,
      "grad_norm": 4.137946128845215,
      "learning_rate": 9.322171915289635e-05,
      "loss": 1.1083,
      "step": 90
    },
    {
      "epoch": 0.006028086910439851,
      "grad_norm": 3.100019931793213,
      "learning_rate": 9.306131215901003e-05,
      "loss": 1.1354,
      "step": 91
    },
    {
      "epoch": 0.006094329623741388,
      "grad_norm": 4.174485206604004,
      "learning_rate": 9.289917066174886e-05,
      "loss": 1.1759,
      "step": 92
    },
    {
      "epoch": 0.006160572337042925,
      "grad_norm": 2.924689531326294,
      "learning_rate": 9.273530119214868e-05,
      "loss": 1.1953,
      "step": 93
    },
    {
      "epoch": 0.006226815050344462,
      "grad_norm": 3.355476140975952,
      "learning_rate": 9.256971035084785e-05,
      "loss": 1.233,
      "step": 94
    },
    {
      "epoch": 0.006293057763645999,
      "grad_norm": 3.584958553314209,
      "learning_rate": 9.24024048078213e-05,
      "loss": 0.9755,
      "step": 95
    },
    {
      "epoch": 0.006359300476947536,
      "grad_norm": 2.8995909690856934,
      "learning_rate": 9.223339130211192e-05,
      "loss": 1.0963,
      "step": 96
    },
    {
      "epoch": 0.006425543190249072,
      "grad_norm": 3.1801187992095947,
      "learning_rate": 9.206267664155907e-05,
      "loss": 1.1805,
      "step": 97
    },
    {
      "epoch": 0.006491785903550609,
      "grad_norm": 2.646547794342041,
      "learning_rate": 9.189026770252436e-05,
      "loss": 1.259,
      "step": 98
    },
    {
      "epoch": 0.006558028616852146,
      "grad_norm": 3.100245952606201,
      "learning_rate": 9.171617142961477e-05,
      "loss": 1.2082,
      "step": 99
    },
    {
      "epoch": 0.006624271330153683,
      "grad_norm": 3.4374091625213623,
      "learning_rate": 9.154039483540273e-05,
      "loss": 0.9978,
      "step": 100
    },
    {
      "epoch": 0.00669051404345522,
      "grad_norm": 2.7903010845184326,
      "learning_rate": 9.136294500014386e-05,
      "loss": 1.043,
      "step": 101
    },
    {
      "epoch": 0.006756756756756757,
      "grad_norm": 3.8611390590667725,
      "learning_rate": 9.118382907149165e-05,
      "loss": 1.001,
      "step": 102
    },
    {
      "epoch": 0.006822999470058293,
      "grad_norm": 3.37994384765625,
      "learning_rate": 9.100305426420956e-05,
      "loss": 0.9132,
      "step": 103
    },
    {
      "epoch": 0.00688924218335983,
      "grad_norm": 2.945674180984497,
      "learning_rate": 9.082062785988049e-05,
      "loss": 0.9454,
      "step": 104
    },
    {
      "epoch": 0.006955484896661367,
      "grad_norm": 3.008054733276367,
      "learning_rate": 9.06365572066134e-05,
      "loss": 1.3524,
      "step": 105
    },
    {
      "epoch": 0.007021727609962904,
      "grad_norm": 3.6604371070861816,
      "learning_rate": 9.045084971874738e-05,
      "loss": 0.9391,
      "step": 106
    },
    {
      "epoch": 0.007087970323264441,
      "grad_norm": 3.0335724353790283,
      "learning_rate": 9.026351287655294e-05,
      "loss": 1.1299,
      "step": 107
    },
    {
      "epoch": 0.007154213036565978,
      "grad_norm": 4.3615593910217285,
      "learning_rate": 9.007455422593077e-05,
      "loss": 0.8973,
      "step": 108
    },
    {
      "epoch": 0.007220455749867514,
      "grad_norm": 3.213595151901245,
      "learning_rate": 8.988398137810777e-05,
      "loss": 1.1593,
      "step": 109
    },
    {
      "epoch": 0.007286698463169051,
      "grad_norm": 2.679920196533203,
      "learning_rate": 8.969180200933047e-05,
      "loss": 0.9451,
      "step": 110
    },
    {
      "epoch": 0.007352941176470588,
      "grad_norm": 3.1394662857055664,
      "learning_rate": 8.949802386055581e-05,
      "loss": 1.0178,
      "step": 111
    },
    {
      "epoch": 0.007419183889772125,
      "grad_norm": 2.8241398334503174,
      "learning_rate": 8.930265473713938e-05,
      "loss": 1.0239,
      "step": 112
    },
    {
      "epoch": 0.007485426603073662,
      "grad_norm": 2.8220155239105225,
      "learning_rate": 8.910570250852097e-05,
      "loss": 1.1167,
      "step": 113
    },
    {
      "epoch": 0.007551669316375199,
      "grad_norm": 3.1044211387634277,
      "learning_rate": 8.890717510790763e-05,
      "loss": 1.0398,
      "step": 114
    },
    {
      "epoch": 0.007617912029676735,
      "grad_norm": 3.0132229328155518,
      "learning_rate": 8.870708053195413e-05,
      "loss": 1.1759,
      "step": 115
    },
    {
      "epoch": 0.007684154742978272,
      "grad_norm": 2.792991876602173,
      "learning_rate": 8.850542684044078e-05,
      "loss": 1.2857,
      "step": 116
    },
    {
      "epoch": 0.007750397456279809,
      "grad_norm": 3.386343479156494,
      "learning_rate": 8.83022221559489e-05,
      "loss": 0.833,
      "step": 117
    },
    {
      "epoch": 0.007816640169581345,
      "grad_norm": 4.85018253326416,
      "learning_rate": 8.809747466353356e-05,
      "loss": 0.9716,
      "step": 118
    },
    {
      "epoch": 0.007882882882882882,
      "grad_norm": 3.574420928955078,
      "learning_rate": 8.789119261039385e-05,
      "loss": 0.9761,
      "step": 119
    },
    {
      "epoch": 0.00794912559618442,
      "grad_norm": 3.2426564693450928,
      "learning_rate": 8.768338430554082e-05,
      "loss": 1.1927,
      "step": 120
    },
    {
      "epoch": 0.008015368309485956,
      "grad_norm": 2.8880693912506104,
      "learning_rate": 8.74740581194627e-05,
      "loss": 1.1797,
      "step": 121
    },
    {
      "epoch": 0.008081611022787493,
      "grad_norm": 3.677746534347534,
      "learning_rate": 8.726322248378774e-05,
      "loss": 1.1132,
      "step": 122
    },
    {
      "epoch": 0.00814785373608903,
      "grad_norm": 4.071290493011475,
      "learning_rate": 8.705088589094459e-05,
      "loss": 1.2054,
      "step": 123
    },
    {
      "epoch": 0.008214096449390567,
      "grad_norm": 3.2814810276031494,
      "learning_rate": 8.683705689382024e-05,
      "loss": 1.1367,
      "step": 124
    },
    {
      "epoch": 0.008280339162692104,
      "grad_norm": 3.17580509185791,
      "learning_rate": 8.662174410541555e-05,
      "loss": 1.2001,
      "step": 125
    },
    {
      "epoch": 0.008346581875993641,
      "grad_norm": 2.702069044113159,
      "learning_rate": 8.640495619849821e-05,
      "loss": 1.1966,
      "step": 126
    },
    {
      "epoch": 0.008412824589295178,
      "grad_norm": 3.792174816131592,
      "learning_rate": 8.618670190525352e-05,
      "loss": 0.9687,
      "step": 127
    },
    {
      "epoch": 0.008479067302596715,
      "grad_norm": 3.31939959526062,
      "learning_rate": 8.596699001693255e-05,
      "loss": 0.92,
      "step": 128
    },
    {
      "epoch": 0.008545310015898252,
      "grad_norm": 3.4189999103546143,
      "learning_rate": 8.574582938349817e-05,
      "loss": 1.0359,
      "step": 129
    },
    {
      "epoch": 0.008611552729199789,
      "grad_norm": 4.046175956726074,
      "learning_rate": 8.552322891326846e-05,
      "loss": 1.1378,
      "step": 130
    },
    {
      "epoch": 0.008677795442501324,
      "grad_norm": 2.386441230773926,
      "learning_rate": 8.529919757255783e-05,
      "loss": 1.1806,
      "step": 131
    },
    {
      "epoch": 0.008744038155802861,
      "grad_norm": 3.603132724761963,
      "learning_rate": 8.507374438531607e-05,
      "loss": 1.1926,
      "step": 132
    },
    {
      "epoch": 0.008810280869104398,
      "grad_norm": 3.1990151405334473,
      "learning_rate": 8.484687843276467e-05,
      "loss": 1.039,
      "step": 133
    },
    {
      "epoch": 0.008876523582405935,
      "grad_norm": 3.0033857822418213,
      "learning_rate": 8.461860885303114e-05,
      "loss": 1.2236,
      "step": 134
    },
    {
      "epoch": 0.008942766295707472,
      "grad_norm": 2.9489827156066895,
      "learning_rate": 8.438894484078086e-05,
      "loss": 0.8662,
      "step": 135
    },
    {
      "epoch": 0.009009009009009009,
      "grad_norm": 3.539790153503418,
      "learning_rate": 8.415789564684673e-05,
      "loss": 1.3331,
      "step": 136
    },
    {
      "epoch": 0.009075251722310546,
      "grad_norm": 2.9523866176605225,
      "learning_rate": 8.392547057785661e-05,
      "loss": 1.1534,
      "step": 137
    },
    {
      "epoch": 0.009141494435612083,
      "grad_norm": 4.424468994140625,
      "learning_rate": 8.369167899585841e-05,
      "loss": 1.02,
      "step": 138
    },
    {
      "epoch": 0.00920773714891362,
      "grad_norm": 2.509406805038452,
      "learning_rate": 8.345653031794292e-05,
      "loss": 1.2234,
      "step": 139
    },
    {
      "epoch": 0.009273979862215157,
      "grad_norm": 3.0897953510284424,
      "learning_rate": 8.322003401586462e-05,
      "loss": 1.1343,
      "step": 140
    },
    {
      "epoch": 0.009340222575516694,
      "grad_norm": 3.286202907562256,
      "learning_rate": 8.298219961566007e-05,
      "loss": 1.0186,
      "step": 141
    },
    {
      "epoch": 0.00940646528881823,
      "grad_norm": 3.356113910675049,
      "learning_rate": 8.274303669726426e-05,
      "loss": 0.8784,
      "step": 142
    },
    {
      "epoch": 0.009472708002119766,
      "grad_norm": 3.281726360321045,
      "learning_rate": 8.250255489412463e-05,
      "loss": 1.0292,
      "step": 143
    },
    {
      "epoch": 0.009538950715421303,
      "grad_norm": 3.2486982345581055,
      "learning_rate": 8.226076389281316e-05,
      "loss": 0.9885,
      "step": 144
    },
    {
      "epoch": 0.00960519342872284,
      "grad_norm": 3.43896746635437,
      "learning_rate": 8.201767343263612e-05,
      "loss": 1.199,
      "step": 145
    },
    {
      "epoch": 0.009671436142024377,
      "grad_norm": 2.685864210128784,
      "learning_rate": 8.177329330524182e-05,
      "loss": 1.0479,
      "step": 146
    },
    {
      "epoch": 0.009737678855325914,
      "grad_norm": 3.28620982170105,
      "learning_rate": 8.152763335422613e-05,
      "loss": 1.146,
      "step": 147
    },
    {
      "epoch": 0.00980392156862745,
      "grad_norm": 2.904519557952881,
      "learning_rate": 8.128070347473607e-05,
      "loss": 1.2052,
      "step": 148
    },
    {
      "epoch": 0.009870164281928988,
      "grad_norm": 2.705730676651001,
      "learning_rate": 8.103251361307119e-05,
      "loss": 1.2093,
      "step": 149
    },
    {
      "epoch": 0.009936406995230525,
      "grad_norm": 2.8267464637756348,
      "learning_rate": 8.07830737662829e-05,
      "loss": 1.0568,
      "step": 150
    },
    {
      "epoch": 0.010002649708532062,
      "grad_norm": 3.846031665802002,
      "learning_rate": 8.053239398177191e-05,
      "loss": 1.0338,
      "step": 151
    },
    {
      "epoch": 0.010068892421833599,
      "grad_norm": 3.5194196701049805,
      "learning_rate": 8.028048435688333e-05,
      "loss": 1.0629,
      "step": 152
    },
    {
      "epoch": 0.010135135135135136,
      "grad_norm": NaN,
      "learning_rate": 8.028048435688333e-05,
      "loss": 0.9882,
      "step": 153
    },
    {
      "epoch": 0.010201377848436673,
      "grad_norm": 4.410955429077148,
      "learning_rate": 8.002735503850016e-05,
      "loss": 1.0306,
      "step": 154
    },
    {
      "epoch": 0.01026762056173821,
      "grad_norm": 3.1042842864990234,
      "learning_rate": 7.97730162226344e-05,
      "loss": 0.971,
      "step": 155
    },
    {
      "epoch": 0.010333863275039745,
      "grad_norm": 2.9366629123687744,
      "learning_rate": 7.95174781540165e-05,
      "loss": 1.1321,
      "step": 156
    },
    {
      "epoch": 0.010400105988341282,
      "grad_norm": 2.575742721557617,
      "learning_rate": 7.926075112568259e-05,
      "loss": 0.9986,
      "step": 157
    },
    {
      "epoch": 0.010466348701642819,
      "grad_norm": 3.936211347579956,
      "learning_rate": 7.900284547855991e-05,
      "loss": 1.0993,
      "step": 158
    },
    {
      "epoch": 0.010532591414944356,
      "grad_norm": 3.5609424114227295,
      "learning_rate": 7.874377160105036e-05,
      "loss": 0.864,
      "step": 159
    },
    {
      "epoch": 0.010598834128245893,
      "grad_norm": 2.7518982887268066,
      "learning_rate": 7.848353992861195e-05,
      "loss": 1.0334,
      "step": 160
    },
    {
      "epoch": 0.01066507684154743,
      "grad_norm": 3.8861889839172363,
      "learning_rate": 7.822216094333847e-05,
      "loss": 1.2352,
      "step": 161
    },
    {
      "epoch": 0.010731319554848967,
      "grad_norm": 3.1392526626586914,
      "learning_rate": 7.795964517353735e-05,
      "loss": 1.0874,
      "step": 162
    },
    {
      "epoch": 0.010797562268150504,
      "grad_norm": 3.634960651397705,
      "learning_rate": 7.769600319330552e-05,
      "loss": 1.2544,
      "step": 163
    },
    {
      "epoch": 0.01086380498145204,
      "grad_norm": 2.7355165481567383,
      "learning_rate": 7.74312456221035e-05,
      "loss": 1.0776,
      "step": 164
    },
    {
      "epoch": 0.010930047694753578,
      "grad_norm": 2.546433687210083,
      "learning_rate": 7.716538312432766e-05,
      "loss": 1.1228,
      "step": 165
    },
    {
      "epoch": 0.010996290408055115,
      "grad_norm": 3.6321403980255127,
      "learning_rate": 7.689842640888063e-05,
      "loss": 0.9701,
      "step": 166
    },
    {
      "epoch": 0.011062533121356652,
      "grad_norm": 3.2757248878479004,
      "learning_rate": 7.663038622873999e-05,
      "loss": 1.0187,
      "step": 167
    },
    {
      "epoch": 0.011128775834658187,
      "grad_norm": 4.133677959442139,
      "learning_rate": 7.636127338052512e-05,
      "loss": 1.0939,
      "step": 168
    },
    {
      "epoch": 0.011195018547959724,
      "grad_norm": 3.4858860969543457,
      "learning_rate": 7.60910987040623e-05,
      "loss": 1.1639,
      "step": 169
    },
    {
      "epoch": 0.01126126126126126,
      "grad_norm": 3.3973159790039062,
      "learning_rate": 7.58198730819481e-05,
      "loss": 0.9735,
      "step": 170
    },
    {
      "epoch": 0.011327503974562798,
      "grad_norm": 3.4165470600128174,
      "learning_rate": 7.554760743911103e-05,
      "loss": 1.0428,
      "step": 171
    },
    {
      "epoch": 0.011393746687864335,
      "grad_norm": 2.838573455810547,
      "learning_rate": 7.52743127423715e-05,
      "loss": 1.1314,
      "step": 172
    },
    {
      "epoch": 0.011459989401165872,
      "grad_norm": 4.819404125213623,
      "learning_rate": 7.500000000000001e-05,
      "loss": 1.0144,
      "step": 173
    },
    {
      "epoch": 0.011526232114467409,
      "grad_norm": 2.6684818267822266,
      "learning_rate": 7.472468026127385e-05,
      "loss": 1.1102,
      "step": 174
    },
    {
      "epoch": 0.011592474827768946,
      "grad_norm": 2.8577935695648193,
      "learning_rate": 7.444836461603195e-05,
      "loss": 1.3194,
      "step": 175
    },
    {
      "epoch": 0.011658717541070483,
      "grad_norm": 3.140986442565918,
      "learning_rate": 7.417106419422819e-05,
      "loss": 1.217,
      "step": 176
    },
    {
      "epoch": 0.01172496025437202,
      "grad_norm": 3.443396806716919,
      "learning_rate": 7.389279016548316e-05,
      "loss": 1.1875,
      "step": 177
    },
    {
      "epoch": 0.011791202967673556,
      "grad_norm": 2.612710952758789,
      "learning_rate": 7.361355373863414e-05,
      "loss": 1.1958,
      "step": 178
    },
    {
      "epoch": 0.011857445680975093,
      "grad_norm": 3.0467050075531006,
      "learning_rate": 7.333336616128369e-05,
      "loss": 0.9334,
      "step": 179
    },
    {
      "epoch": 0.01192368839427663,
      "grad_norm": 4.09562349319458,
      "learning_rate": 7.305223871934657e-05,
      "loss": 0.8524,
      "step": 180
    },
    {
      "epoch": 0.011989931107578166,
      "grad_norm": 3.221140146255493,
      "learning_rate": 7.277018273659515e-05,
      "loss": 1.0936,
      "step": 181
    },
    {
      "epoch": 0.012056173820879703,
      "grad_norm": 3.082911252975464,
      "learning_rate": 7.24872095742033e-05,
      "loss": 1.17,
      "step": 182
    },
    {
      "epoch": 0.01212241653418124,
      "grad_norm": 3.4430413246154785,
      "learning_rate": 7.220333063028872e-05,
      "loss": 0.9395,
      "step": 183
    },
    {
      "epoch": 0.012188659247482777,
      "grad_norm": 3.4001548290252686,
      "learning_rate": 7.191855733945387e-05,
      "loss": 1.2077,
      "step": 184
    },
    {
      "epoch": 0.012254901960784314,
      "grad_norm": 3.1901586055755615,
      "learning_rate": 7.163290117232542e-05,
      "loss": 1.1448,
      "step": 185
    },
    {
      "epoch": 0.01232114467408585,
      "grad_norm": 3.381373643875122,
      "learning_rate": 7.13463736350921e-05,
      "loss": 1.0439,
      "step": 186
    },
    {
      "epoch": 0.012387387387387387,
      "grad_norm": 3.235003709793091,
      "learning_rate": 7.105898626904134e-05,
      "loss": 1.0607,
      "step": 187
    },
    {
      "epoch": 0.012453630100688924,
      "grad_norm": 3.330320358276367,
      "learning_rate": 7.077075065009433e-05,
      "loss": 0.9792,
      "step": 188
    },
    {
      "epoch": 0.012519872813990461,
      "grad_norm": 2.524749994277954,
      "learning_rate": 7.048167838833977e-05,
      "loss": 1.0369,
      "step": 189
    },
    {
      "epoch": 0.012586115527291998,
      "grad_norm": 3.304062604904175,
      "learning_rate": 7.019178112756624e-05,
      "loss": 1.0425,
      "step": 190
    },
    {
      "epoch": 0.012652358240593535,
      "grad_norm": 3.5333590507507324,
      "learning_rate": 6.990107054479312e-05,
      "loss": 1.2712,
      "step": 191
    },
    {
      "epoch": 0.012718600953895072,
      "grad_norm": 3.023491859436035,
      "learning_rate": 6.960955834980028e-05,
      "loss": 0.9613,
      "step": 192
    },
    {
      "epoch": 0.012784843667196608,
      "grad_norm": 3.522125720977783,
      "learning_rate": 6.931725628465643e-05,
      "loss": 1.0836,
      "step": 193
    },
    {
      "epoch": 0.012851086380498145,
      "grad_norm": 3.9891536235809326,
      "learning_rate": 6.902417612324615e-05,
      "loss": 1.3155,
      "step": 194
    },
    {
      "epoch": 0.012917329093799682,
      "grad_norm": 3.417726993560791,
      "learning_rate": 6.873032967079561e-05,
      "loss": 1.3886,
      "step": 195
    },
    {
      "epoch": 0.012983571807101218,
      "grad_norm": 2.602438449859619,
      "learning_rate": 6.843572876339705e-05,
      "loss": 1.0399,
      "step": 196
    },
    {
      "epoch": 0.013049814520402755,
      "grad_norm": 3.344975709915161,
      "learning_rate": 6.814038526753205e-05,
      "loss": 1.1015,
      "step": 197
    },
    {
      "epoch": 0.013116057233704292,
      "grad_norm": 2.7692296504974365,
      "learning_rate": 6.784431107959359e-05,
      "loss": 1.0401,
      "step": 198
    },
    {
      "epoch": 0.01318229994700583,
      "grad_norm": 3.4979803562164307,
      "learning_rate": 6.754751812540679e-05,
      "loss": 1.1515,
      "step": 199
    },
    {
      "epoch": 0.013248542660307366,
      "grad_norm": 2.6680681705474854,
      "learning_rate": 6.725001835974853e-05,
      "loss": 1.1091,
      "step": 200
    },
    {
      "epoch": 0.013314785373608903,
      "grad_norm": 3.0385169982910156,
      "learning_rate": 6.695182376586603e-05,
      "loss": 1.0292,
      "step": 201
    },
    {
      "epoch": 0.01338102808691044,
      "grad_norm": 3.3150277137756348,
      "learning_rate": 6.665294635499404e-05,
      "loss": 1.1928,
      "step": 202
    },
    {
      "epoch": 0.013447270800211977,
      "grad_norm": 3.5188493728637695,
      "learning_rate": 6.635339816587109e-05,
      "loss": 1.1318,
      "step": 203
    },
    {
      "epoch": 0.013513513513513514,
      "grad_norm": 2.7930448055267334,
      "learning_rate": 6.605319126425454e-05,
      "loss": 0.9974,
      "step": 204
    },
    {
      "epoch": 0.01357975622681505,
      "grad_norm": 2.9377574920654297,
      "learning_rate": 6.575233774243465e-05,
      "loss": 1.2034,
      "step": 205
    },
    {
      "epoch": 0.013645998940116586,
      "grad_norm": 3.25578236579895,
      "learning_rate": 6.545084971874738e-05,
      "loss": 1.1732,
      "step": 206
    },
    {
      "epoch": 0.013712241653418123,
      "grad_norm": 2.747596502304077,
      "learning_rate": 6.514873933708638e-05,
      "loss": 1.0755,
      "step": 207
    },
    {
      "epoch": 0.01377848436671966,
      "grad_norm": 2.6545355319976807,
      "learning_rate": 6.484601876641375e-05,
      "loss": 1.252,
      "step": 208
    },
    {
      "epoch": 0.013844727080021197,
      "grad_norm": 2.7165110111236572,
      "learning_rate": 6.454270020026995e-05,
      "loss": 1.0747,
      "step": 209
    },
    {
      "epoch": 0.013910969793322734,
      "grad_norm": 3.754575490951538,
      "learning_rate": 6.423879585628261e-05,
      "loss": 1.1255,
      "step": 210
    },
    {
      "epoch": 0.013977212506624271,
      "grad_norm": 3.1749327182769775,
      "learning_rate": 6.39343179756744e-05,
      "loss": 1.0677,
      "step": 211
    },
    {
      "epoch": 0.014043455219925808,
      "grad_norm": 3.1424500942230225,
      "learning_rate": 6.36292788227699e-05,
      "loss": 0.9287,
      "step": 212
    },
    {
      "epoch": 0.014109697933227345,
      "grad_norm": 2.6358635425567627,
      "learning_rate": 6.332369068450174e-05,
      "loss": 1.1417,
      "step": 213
    },
    {
      "epoch": 0.014175940646528882,
      "grad_norm": 3.1811203956604004,
      "learning_rate": 6.30175658699156e-05,
      "loss": 0.7852,
      "step": 214
    },
    {
      "epoch": 0.01424218335983042,
      "grad_norm": 2.6775519847869873,
      "learning_rate": 6.271091670967436e-05,
      "loss": 1.2041,
      "step": 215
    },
    {
      "epoch": 0.014308426073131956,
      "grad_norm": 2.410522699356079,
      "learning_rate": 6.240375555556145e-05,
      "loss": 0.871,
      "step": 216
    },
    {
      "epoch": 0.014374668786433493,
      "grad_norm": 2.776198625564575,
      "learning_rate": 6.209609477998338e-05,
      "loss": 1.2588,
      "step": 217
    },
    {
      "epoch": 0.014440911499735028,
      "grad_norm": 2.9820339679718018,
      "learning_rate": 6.178794677547137e-05,
      "loss": 1.1681,
      "step": 218
    },
    {
      "epoch": 0.014507154213036565,
      "grad_norm": 2.8462579250335693,
      "learning_rate": 6.147932395418205e-05,
      "loss": 1.1711,
      "step": 219
    },
    {
      "epoch": 0.014573396926338102,
      "grad_norm": 2.8808891773223877,
      "learning_rate": 6.117023874739772e-05,
      "loss": 1.0583,
      "step": 220
    },
    {
      "epoch": 0.01463963963963964,
      "grad_norm": 3.2114365100860596,
      "learning_rate": 6.0860703605025395e-05,
      "loss": 1.0383,
      "step": 221
    },
    {
      "epoch": 0.014705882352941176,
      "grad_norm": 2.736060380935669,
      "learning_rate": 6.05507309950955e-05,
      "loss": 1.0969,
      "step": 222
    },
    {
      "epoch": 0.014772125066242713,
      "grad_norm": 3.11415696144104,
      "learning_rate": 6.024033340325954e-05,
      "loss": 1.1705,
      "step": 223
    },
    {
      "epoch": 0.01483836777954425,
      "grad_norm": 2.8108973503112793,
      "learning_rate": 5.992952333228728e-05,
      "loss": 1.1343,
      "step": 224
    },
    {
      "epoch": 0.014904610492845787,
      "grad_norm": 3.597066640853882,
      "learning_rate": 5.9618313301563055e-05,
      "loss": 0.9326,
      "step": 225
    },
    {
      "epoch": 0.014970853206147324,
      "grad_norm": 2.9836792945861816,
      "learning_rate": 5.9306715846581506e-05,
      "loss": 0.9054,
      "step": 226
    },
    {
      "epoch": 0.015037095919448861,
      "grad_norm": 3.2471210956573486,
      "learning_rate": 5.8994743518442694e-05,
      "loss": 1.2807,
      "step": 227
    },
    {
      "epoch": 0.015103338632750398,
      "grad_norm": 3.138737916946411,
      "learning_rate": 5.868240888334653e-05,
      "loss": 0.8444,
      "step": 228
    },
    {
      "epoch": 0.015169581346051935,
      "grad_norm": 3.3165335655212402,
      "learning_rate": 5.836972452208654e-05,
      "loss": 1.0606,
      "step": 229
    },
    {
      "epoch": 0.01523582405935347,
      "grad_norm": 3.3207907676696777,
      "learning_rate": 5.805670302954321e-05,
      "loss": 0.9586,
      "step": 230
    },
    {
      "epoch": 0.015302066772655007,
      "grad_norm": 3.3062076568603516,
      "learning_rate": 5.7743357014176624e-05,
      "loss": 1.1893,
      "step": 231
    },
    {
      "epoch": 0.015368309485956544,
      "grad_norm": 2.8078105449676514,
      "learning_rate": 5.7429699097518585e-05,
      "loss": 1.0806,
      "step": 232
    },
    {
      "epoch": 0.015434552199258081,
      "grad_norm": 2.899343729019165,
      "learning_rate": 5.7115741913664264e-05,
      "loss": 1.1569,
      "step": 233
    },
    {
      "epoch": 0.015500794912559618,
      "grad_norm": 3.226118326187134,
      "learning_rate": 5.680149810876322e-05,
      "loss": 1.1287,
      "step": 234
    },
    {
      "epoch": 0.015567037625861155,
      "grad_norm": 2.956495523452759,
      "learning_rate": 5.6486980340510086e-05,
      "loss": 0.9077,
      "step": 235
    },
    {
      "epoch": 0.01563328033916269,
      "grad_norm": 3.407794952392578,
      "learning_rate": 5.617220127763474e-05,
      "loss": 0.9952,
      "step": 236
    },
    {
      "epoch": 0.015699523052464227,
      "grad_norm": 2.803701400756836,
      "learning_rate": 5.585717359939192e-05,
      "loss": 1.0333,
      "step": 237
    },
    {
      "epoch": 0.015765765765765764,
      "grad_norm": 2.8709299564361572,
      "learning_rate": 5.5541909995050554e-05,
      "loss": 1.0284,
      "step": 238
    },
    {
      "epoch": 0.0158320084790673,
      "grad_norm": 3.418093681335449,
      "learning_rate": 5.522642316338268e-05,
      "loss": 1.2408,
      "step": 239
    },
    {
      "epoch": 0.01589825119236884,
      "grad_norm": 3.020120143890381,
      "learning_rate": 5.4910725812151864e-05,
      "loss": 0.8973,
      "step": 240
    },
    {
      "epoch": 0.015964493905670375,
      "grad_norm": 2.859147787094116,
      "learning_rate": 5.4594830657601384e-05,
      "loss": 1.1254,
      "step": 241
    },
    {
      "epoch": 0.016030736618971912,
      "grad_norm": 2.5526840686798096,
      "learning_rate": 5.427875042394199e-05,
      "loss": 1.0587,
      "step": 242
    },
    {
      "epoch": 0.01609697933227345,
      "grad_norm": 2.6547365188598633,
      "learning_rate": 5.396249784283942e-05,
      "loss": 1.0351,
      "step": 243
    },
    {
      "epoch": 0.016163222045574986,
      "grad_norm": 3.0302789211273193,
      "learning_rate": 5.364608565290155e-05,
      "loss": 0.9653,
      "step": 244
    },
    {
      "epoch": 0.016229464758876523,
      "grad_norm": 2.6898632049560547,
      "learning_rate": 5.3329526599165204e-05,
      "loss": 1.0942,
      "step": 245
    },
    {
      "epoch": 0.01629570747217806,
      "grad_norm": 3.498012065887451,
      "learning_rate": 5.301283343258293e-05,
      "loss": 1.0225,
      "step": 246
    },
    {
      "epoch": 0.016361950185479597,
      "grad_norm": 2.8191730976104736,
      "learning_rate": 5.2696018909509306e-05,
      "loss": 0.9034,
      "step": 247
    },
    {
      "epoch": 0.016428192898781134,
      "grad_norm": 3.001528263092041,
      "learning_rate": 5.2379095791187124e-05,
      "loss": 1.0675,
      "step": 248
    },
    {
      "epoch": 0.01649443561208267,
      "grad_norm": 2.496105194091797,
      "learning_rate": 5.2062076843233366e-05,
      "loss": 0.9218,
      "step": 249
    },
    {
      "epoch": 0.016560678325384208,
      "grad_norm": 3.204658269882202,
      "learning_rate": 5.174497483512506e-05,
      "loss": 1.1221,
      "step": 250
    },
    {
      "epoch": 0.016626921038685745,
      "grad_norm": 3.5193898677825928,
      "learning_rate": 5.142780253968481e-05,
      "loss": 1.3175,
      "step": 251
    },
    {
      "epoch": 0.016693163751987282,
      "grad_norm": 3.5054750442504883,
      "learning_rate": 5.1110572732566475e-05,
      "loss": 1.305,
      "step": 252
    },
    {
      "epoch": 0.01675940646528882,
      "grad_norm": 3.196471691131592,
      "learning_rate": 5.0793298191740404e-05,
      "loss": 0.8932,
      "step": 253
    },
    {
      "epoch": 0.016825649178590356,
      "grad_norm": 3.170961380004883,
      "learning_rate": 5.047599169697884e-05,
      "loss": 1.3728,
      "step": 254
    },
    {
      "epoch": 0.016891891891891893,
      "grad_norm": 3.6696503162384033,
      "learning_rate": 5.015866602934112e-05,
      "loss": 1.0921,
      "step": 255
    },
    {
      "epoch": 0.01695813460519343,
      "grad_norm": 2.4134116172790527,
      "learning_rate": 4.984133397065889e-05,
      "loss": 1.0043,
      "step": 256
    },
    {
      "epoch": 0.017024377318494967,
      "grad_norm": 3.071329116821289,
      "learning_rate": 4.952400830302117e-05,
      "loss": 1.1727,
      "step": 257
    },
    {
      "epoch": 0.017090620031796504,
      "grad_norm": 2.9530293941497803,
      "learning_rate": 4.92067018082596e-05,
      "loss": 1.0273,
      "step": 258
    },
    {
      "epoch": 0.01715686274509804,
      "grad_norm": 3.093050718307495,
      "learning_rate": 4.888942726743353e-05,
      "loss": 1.0162,
      "step": 259
    },
    {
      "epoch": 0.017223105458399578,
      "grad_norm": 3.7697806358337402,
      "learning_rate": 4.85721974603152e-05,
      "loss": 0.7841,
      "step": 260
    }
  ],
  "logging_steps": 1,
  "max_steps": 500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.701486007676928e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
