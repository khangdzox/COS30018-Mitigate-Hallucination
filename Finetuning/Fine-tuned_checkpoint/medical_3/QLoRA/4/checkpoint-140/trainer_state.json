{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.009273979862215157,
  "eval_steps": 500,
  "global_step": 140,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 6.624271330153684e-05,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.2689,
      "step": 1
    },
    {
      "epoch": 0.00013248542660307367,
      "grad_norm": 3.935640573501587,
      "learning_rate": 2e-05,
      "loss": 2.3392,
      "step": 2
    },
    {
      "epoch": 0.0001987281399046105,
      "grad_norm": NaN,
      "learning_rate": 2e-05,
      "loss": 2.328,
      "step": 3
    },
    {
      "epoch": 0.00026497085320614734,
      "grad_norm": 3.172327995300293,
      "learning_rate": 4e-05,
      "loss": 2.23,
      "step": 4
    },
    {
      "epoch": 0.00033121356650768415,
      "grad_norm": 4.042605876922607,
      "learning_rate": 6e-05,
      "loss": 2.324,
      "step": 5
    },
    {
      "epoch": 0.000397456279809221,
      "grad_norm": 4.5806450843811035,
      "learning_rate": 8e-05,
      "loss": 2.3925,
      "step": 6
    },
    {
      "epoch": 0.0004636989931107578,
      "grad_norm": 3.348752498626709,
      "learning_rate": 0.0001,
      "loss": 2.1809,
      "step": 7
    },
    {
      "epoch": 0.0005299417064122947,
      "grad_norm": 3.5855343341827393,
      "learning_rate": 9.999899300364532e-05,
      "loss": 2.181,
      "step": 8
    },
    {
      "epoch": 0.0005961844197138314,
      "grad_norm": 3.9782562255859375,
      "learning_rate": 9.999597205514297e-05,
      "loss": 2.2029,
      "step": 9
    },
    {
      "epoch": 0.0006624271330153683,
      "grad_norm": 3.451857089996338,
      "learning_rate": 9.99909372761763e-05,
      "loss": 1.825,
      "step": 10
    },
    {
      "epoch": 0.0007286698463169052,
      "grad_norm": 3.3541646003723145,
      "learning_rate": 9.998388886954547e-05,
      "loss": 1.9844,
      "step": 11
    },
    {
      "epoch": 0.000794912559618442,
      "grad_norm": 4.102404594421387,
      "learning_rate": 9.997482711915927e-05,
      "loss": 2.1533,
      "step": 12
    },
    {
      "epoch": 0.0008611552729199788,
      "grad_norm": 5.836984634399414,
      "learning_rate": 9.996375239002369e-05,
      "loss": 1.9529,
      "step": 13
    },
    {
      "epoch": 0.0009273979862215156,
      "grad_norm": 4.671607971191406,
      "learning_rate": 9.99506651282272e-05,
      "loss": 1.7419,
      "step": 14
    },
    {
      "epoch": 0.0009936406995230524,
      "grad_norm": 6.9419403076171875,
      "learning_rate": 9.993556586092281e-05,
      "loss": 1.6356,
      "step": 15
    },
    {
      "epoch": 0.0010598834128245894,
      "grad_norm": 5.967659950256348,
      "learning_rate": 9.991845519630678e-05,
      "loss": 1.6832,
      "step": 16
    },
    {
      "epoch": 0.0011261261261261261,
      "grad_norm": 6.001219272613525,
      "learning_rate": 9.989933382359422e-05,
      "loss": 1.3467,
      "step": 17
    },
    {
      "epoch": 0.0011923688394276629,
      "grad_norm": 7.259174823760986,
      "learning_rate": 9.987820251299122e-05,
      "loss": 1.3676,
      "step": 18
    },
    {
      "epoch": 0.0012586115527291998,
      "grad_norm": 7.373693943023682,
      "learning_rate": 9.985506211566388e-05,
      "loss": 1.2509,
      "step": 19
    },
    {
      "epoch": 0.0013248542660307366,
      "grad_norm": 7.179003715515137,
      "learning_rate": 9.982991356370404e-05,
      "loss": 1.3077,
      "step": 20
    },
    {
      "epoch": 0.0013910969793322733,
      "grad_norm": 4.5587239265441895,
      "learning_rate": 9.98027578700917e-05,
      "loss": 1.5136,
      "step": 21
    },
    {
      "epoch": 0.0014573396926338103,
      "grad_norm": 4.268433570861816,
      "learning_rate": 9.977359612865423e-05,
      "loss": 1.2645,
      "step": 22
    },
    {
      "epoch": 0.001523582405935347,
      "grad_norm": 4.881389141082764,
      "learning_rate": 9.974242951402235e-05,
      "loss": 1.3434,
      "step": 23
    },
    {
      "epoch": 0.001589825119236884,
      "grad_norm": 4.863972187042236,
      "learning_rate": 9.970925928158274e-05,
      "loss": 1.3195,
      "step": 24
    },
    {
      "epoch": 0.0016560678325384208,
      "grad_norm": 17.17622947692871,
      "learning_rate": 9.967408676742751e-05,
      "loss": 1.3572,
      "step": 25
    },
    {
      "epoch": 0.0017223105458399576,
      "grad_norm": 3.2035300731658936,
      "learning_rate": 9.963691338830044e-05,
      "loss": 1.2448,
      "step": 26
    },
    {
      "epoch": 0.0017885532591414945,
      "grad_norm": 3.4512383937835693,
      "learning_rate": 9.959774064153977e-05,
      "loss": 1.2655,
      "step": 27
    },
    {
      "epoch": 0.0018547959724430313,
      "grad_norm": 2.893615245819092,
      "learning_rate": 9.955657010501806e-05,
      "loss": 1.232,
      "step": 28
    },
    {
      "epoch": 0.001921038685744568,
      "grad_norm": 3.7472167015075684,
      "learning_rate": 9.951340343707852e-05,
      "loss": 1.4194,
      "step": 29
    },
    {
      "epoch": 0.001987281399046105,
      "grad_norm": 3.8347768783569336,
      "learning_rate": 9.946824237646824e-05,
      "loss": 1.2648,
      "step": 30
    },
    {
      "epoch": 0.0020535241123476418,
      "grad_norm": 3.21195125579834,
      "learning_rate": 9.942108874226811e-05,
      "loss": 1.3137,
      "step": 31
    },
    {
      "epoch": 0.0021197668256491787,
      "grad_norm": 3.4692153930664062,
      "learning_rate": 9.937194443381972e-05,
      "loss": 1.1838,
      "step": 32
    },
    {
      "epoch": 0.0021860095389507153,
      "grad_norm": 3.0090458393096924,
      "learning_rate": 9.93208114306486e-05,
      "loss": 1.3544,
      "step": 33
    },
    {
      "epoch": 0.0022522522522522522,
      "grad_norm": 3.2744686603546143,
      "learning_rate": 9.926769179238466e-05,
      "loss": 1.3004,
      "step": 34
    },
    {
      "epoch": 0.002318494965553789,
      "grad_norm": 2.9766061305999756,
      "learning_rate": 9.921258765867919e-05,
      "loss": 1.2653,
      "step": 35
    },
    {
      "epoch": 0.0023847376788553257,
      "grad_norm": 3.0921878814697266,
      "learning_rate": 9.915550124911866e-05,
      "loss": 1.1329,
      "step": 36
    },
    {
      "epoch": 0.0024509803921568627,
      "grad_norm": 2.7303004264831543,
      "learning_rate": 9.909643486313533e-05,
      "loss": 1.089,
      "step": 37
    },
    {
      "epoch": 0.0025172231054583997,
      "grad_norm": 3.438199758529663,
      "learning_rate": 9.903539087991462e-05,
      "loss": 1.3225,
      "step": 38
    },
    {
      "epoch": 0.0025834658187599362,
      "grad_norm": 2.709054470062256,
      "learning_rate": 9.897237175829926e-05,
      "loss": 1.1871,
      "step": 39
    },
    {
      "epoch": 0.002649708532061473,
      "grad_norm": 2.9012386798858643,
      "learning_rate": 9.890738003669029e-05,
      "loss": 1.2922,
      "step": 40
    },
    {
      "epoch": 0.00271595124536301,
      "grad_norm": 3.2810704708099365,
      "learning_rate": 9.884041833294476e-05,
      "loss": 1.1932,
      "step": 41
    },
    {
      "epoch": 0.0027821939586645467,
      "grad_norm": 2.953472852706909,
      "learning_rate": 9.877148934427037e-05,
      "loss": 0.9832,
      "step": 42
    },
    {
      "epoch": 0.0028484366719660837,
      "grad_norm": 3.304927349090576,
      "learning_rate": 9.870059584711668e-05,
      "loss": 0.967,
      "step": 43
    },
    {
      "epoch": 0.0029146793852676206,
      "grad_norm": 3.002321720123291,
      "learning_rate": 9.862774069706346e-05,
      "loss": 1.1302,
      "step": 44
    },
    {
      "epoch": 0.0029809220985691576,
      "grad_norm": 3.673306465148926,
      "learning_rate": 9.855292682870551e-05,
      "loss": 1.1649,
      "step": 45
    },
    {
      "epoch": 0.003047164811870694,
      "grad_norm": 3.451751232147217,
      "learning_rate": 9.847615725553456e-05,
      "loss": 1.1961,
      "step": 46
    },
    {
      "epoch": 0.003113407525172231,
      "grad_norm": 3.4889540672302246,
      "learning_rate": 9.839743506981782e-05,
      "loss": 1.1033,
      "step": 47
    },
    {
      "epoch": 0.003179650238473768,
      "grad_norm": 3.8954901695251465,
      "learning_rate": 9.831676344247342e-05,
      "loss": 1.0559,
      "step": 48
    },
    {
      "epoch": 0.0032458929517753046,
      "grad_norm": 3.8720455169677734,
      "learning_rate": 9.82341456229428e-05,
      "loss": 1.2445,
      "step": 49
    },
    {
      "epoch": 0.0033121356650768416,
      "grad_norm": 2.5133635997772217,
      "learning_rate": 9.814958493905963e-05,
      "loss": 0.9382,
      "step": 50
    },
    {
      "epoch": 0.0033783783783783786,
      "grad_norm": 3.5231356620788574,
      "learning_rate": 9.806308479691595e-05,
      "loss": 1.2273,
      "step": 51
    },
    {
      "epoch": 0.003444621091679915,
      "grad_norm": 3.230523109436035,
      "learning_rate": 9.797464868072488e-05,
      "loss": 1.2495,
      "step": 52
    },
    {
      "epoch": 0.003510863804981452,
      "grad_norm": 3.12392520904541,
      "learning_rate": 9.788428015268027e-05,
      "loss": 1.2135,
      "step": 53
    },
    {
      "epoch": 0.003577106518282989,
      "grad_norm": 3.016922950744629,
      "learning_rate": 9.779198285281325e-05,
      "loss": 1.1439,
      "step": 54
    },
    {
      "epoch": 0.0036433492315845256,
      "grad_norm": 3.000917673110962,
      "learning_rate": 9.769776049884563e-05,
      "loss": 1.1964,
      "step": 55
    },
    {
      "epoch": 0.0037095919448860626,
      "grad_norm": 3.609166145324707,
      "learning_rate": 9.760161688604008e-05,
      "loss": 1.0649,
      "step": 56
    },
    {
      "epoch": 0.0037758346581875995,
      "grad_norm": 3.220571517944336,
      "learning_rate": 9.750355588704727e-05,
      "loss": 1.2922,
      "step": 57
    },
    {
      "epoch": 0.003842077371489136,
      "grad_norm": 3.3062751293182373,
      "learning_rate": 9.740358145174998e-05,
      "loss": 1.2174,
      "step": 58
    },
    {
      "epoch": 0.003908320084790673,
      "grad_norm": 3.251234769821167,
      "learning_rate": 9.730169760710386e-05,
      "loss": 1.2926,
      "step": 59
    },
    {
      "epoch": 0.00397456279809221,
      "grad_norm": 3.6804213523864746,
      "learning_rate": 9.719790845697533e-05,
      "loss": 0.9211,
      "step": 60
    },
    {
      "epoch": 0.0040408055113937465,
      "grad_norm": 3.72613787651062,
      "learning_rate": 9.709221818197624e-05,
      "loss": 1.13,
      "step": 61
    },
    {
      "epoch": 0.0041070482246952835,
      "grad_norm": 3.939169406890869,
      "learning_rate": 9.698463103929542e-05,
      "loss": 0.9656,
      "step": 62
    },
    {
      "epoch": 0.0041732909379968205,
      "grad_norm": 3.0876150131225586,
      "learning_rate": 9.687515136252731e-05,
      "loss": 1.1424,
      "step": 63
    },
    {
      "epoch": 0.0042395336512983575,
      "grad_norm": 3.3746180534362793,
      "learning_rate": 9.676378356149734e-05,
      "loss": 1.2594,
      "step": 64
    },
    {
      "epoch": 0.004305776364599894,
      "grad_norm": 4.579237461090088,
      "learning_rate": 9.665053212208426e-05,
      "loss": 1.1548,
      "step": 65
    },
    {
      "epoch": 0.0043720190779014305,
      "grad_norm": 3.825326919555664,
      "learning_rate": 9.653540160603956e-05,
      "loss": 1.2272,
      "step": 66
    },
    {
      "epoch": 0.0044382617912029675,
      "grad_norm": 3.9778435230255127,
      "learning_rate": 9.641839665080363e-05,
      "loss": 1.0409,
      "step": 67
    },
    {
      "epoch": 0.0045045045045045045,
      "grad_norm": 3.0924837589263916,
      "learning_rate": 9.629952196931901e-05,
      "loss": 1.0821,
      "step": 68
    },
    {
      "epoch": 0.0045707472178060414,
      "grad_norm": 3.241750955581665,
      "learning_rate": 9.617878234984055e-05,
      "loss": 1.0478,
      "step": 69
    },
    {
      "epoch": 0.004636989931107578,
      "grad_norm": 3.575681209564209,
      "learning_rate": 9.60561826557425e-05,
      "loss": 1.3172,
      "step": 70
    },
    {
      "epoch": 0.004703232644409115,
      "grad_norm": 3.798192262649536,
      "learning_rate": 9.593172782532268e-05,
      "loss": 1.1609,
      "step": 71
    },
    {
      "epoch": 0.0047694753577106515,
      "grad_norm": 3.1822009086608887,
      "learning_rate": 9.580542287160348e-05,
      "loss": 1.4767,
      "step": 72
    },
    {
      "epoch": 0.0048357180710121885,
      "grad_norm": 3.501284122467041,
      "learning_rate": 9.567727288213005e-05,
      "loss": 1.4139,
      "step": 73
    },
    {
      "epoch": 0.004901960784313725,
      "grad_norm": 2.887429714202881,
      "learning_rate": 9.554728301876526e-05,
      "loss": 1.0316,
      "step": 74
    },
    {
      "epoch": 0.004968203497615262,
      "grad_norm": 3.154911518096924,
      "learning_rate": 9.541545851748186e-05,
      "loss": 1.19,
      "step": 75
    },
    {
      "epoch": 0.005034446210916799,
      "grad_norm": 2.869253158569336,
      "learning_rate": 9.528180468815155e-05,
      "loss": 1.2745,
      "step": 76
    },
    {
      "epoch": 0.005100688924218336,
      "grad_norm": 3.471853017807007,
      "learning_rate": 9.514632691433107e-05,
      "loss": 1.0286,
      "step": 77
    },
    {
      "epoch": 0.0051669316375198724,
      "grad_norm": 3.389716148376465,
      "learning_rate": 9.50090306530454e-05,
      "loss": 1.0855,
      "step": 78
    },
    {
      "epoch": 0.005233174350821409,
      "grad_norm": 3.243983030319214,
      "learning_rate": 9.486992143456792e-05,
      "loss": 1.1931,
      "step": 79
    },
    {
      "epoch": 0.005299417064122946,
      "grad_norm": 3.344263792037964,
      "learning_rate": 9.472900486219769e-05,
      "loss": 1.2749,
      "step": 80
    },
    {
      "epoch": 0.005365659777424483,
      "grad_norm": 3.2355079650878906,
      "learning_rate": 9.458628661203367e-05,
      "loss": 0.9771,
      "step": 81
    },
    {
      "epoch": 0.00543190249072602,
      "grad_norm": 3.1826441287994385,
      "learning_rate": 9.444177243274618e-05,
      "loss": 1.244,
      "step": 82
    },
    {
      "epoch": 0.005498145204027557,
      "grad_norm": 3.1203207969665527,
      "learning_rate": 9.429546814534529e-05,
      "loss": 1.0937,
      "step": 83
    },
    {
      "epoch": 0.005564387917329093,
      "grad_norm": 3.6420516967773438,
      "learning_rate": 9.414737964294636e-05,
      "loss": 1.0575,
      "step": 84
    },
    {
      "epoch": 0.00563063063063063,
      "grad_norm": 2.949049949645996,
      "learning_rate": 9.399751289053267e-05,
      "loss": 0.9306,
      "step": 85
    },
    {
      "epoch": 0.005696873343932167,
      "grad_norm": 4.718625545501709,
      "learning_rate": 9.384587392471515e-05,
      "loss": 1.3255,
      "step": 86
    },
    {
      "epoch": 0.005763116057233704,
      "grad_norm": 3.3518567085266113,
      "learning_rate": 9.369246885348926e-05,
      "loss": 1.2166,
      "step": 87
    },
    {
      "epoch": 0.005829358770535241,
      "grad_norm": 4.522928714752197,
      "learning_rate": 9.353730385598887e-05,
      "loss": 1.0191,
      "step": 88
    },
    {
      "epoch": 0.005895601483836778,
      "grad_norm": 3.1650099754333496,
      "learning_rate": 9.338038518223747e-05,
      "loss": 0.8991,
      "step": 89
    },
    {
      "epoch": 0.005961844197138315,
      "grad_norm": 4.137946128845215,
      "learning_rate": 9.322171915289635e-05,
      "loss": 1.1083,
      "step": 90
    },
    {
      "epoch": 0.006028086910439851,
      "grad_norm": 3.100019931793213,
      "learning_rate": 9.306131215901003e-05,
      "loss": 1.1354,
      "step": 91
    },
    {
      "epoch": 0.006094329623741388,
      "grad_norm": 4.174485206604004,
      "learning_rate": 9.289917066174886e-05,
      "loss": 1.1759,
      "step": 92
    },
    {
      "epoch": 0.006160572337042925,
      "grad_norm": 2.924689531326294,
      "learning_rate": 9.273530119214868e-05,
      "loss": 1.1953,
      "step": 93
    },
    {
      "epoch": 0.006226815050344462,
      "grad_norm": 3.355476140975952,
      "learning_rate": 9.256971035084785e-05,
      "loss": 1.233,
      "step": 94
    },
    {
      "epoch": 0.006293057763645999,
      "grad_norm": 3.584958553314209,
      "learning_rate": 9.24024048078213e-05,
      "loss": 0.9755,
      "step": 95
    },
    {
      "epoch": 0.006359300476947536,
      "grad_norm": 2.8995909690856934,
      "learning_rate": 9.223339130211192e-05,
      "loss": 1.0963,
      "step": 96
    },
    {
      "epoch": 0.006425543190249072,
      "grad_norm": 3.1801187992095947,
      "learning_rate": 9.206267664155907e-05,
      "loss": 1.1805,
      "step": 97
    },
    {
      "epoch": 0.006491785903550609,
      "grad_norm": 2.646547794342041,
      "learning_rate": 9.189026770252436e-05,
      "loss": 1.259,
      "step": 98
    },
    {
      "epoch": 0.006558028616852146,
      "grad_norm": 3.100245952606201,
      "learning_rate": 9.171617142961477e-05,
      "loss": 1.2082,
      "step": 99
    },
    {
      "epoch": 0.006624271330153683,
      "grad_norm": 3.4374091625213623,
      "learning_rate": 9.154039483540273e-05,
      "loss": 0.9978,
      "step": 100
    },
    {
      "epoch": 0.00669051404345522,
      "grad_norm": 2.7903010845184326,
      "learning_rate": 9.136294500014386e-05,
      "loss": 1.043,
      "step": 101
    },
    {
      "epoch": 0.006756756756756757,
      "grad_norm": 3.8611390590667725,
      "learning_rate": 9.118382907149165e-05,
      "loss": 1.001,
      "step": 102
    },
    {
      "epoch": 0.006822999470058293,
      "grad_norm": 3.37994384765625,
      "learning_rate": 9.100305426420956e-05,
      "loss": 0.9132,
      "step": 103
    },
    {
      "epoch": 0.00688924218335983,
      "grad_norm": 2.945674180984497,
      "learning_rate": 9.082062785988049e-05,
      "loss": 0.9454,
      "step": 104
    },
    {
      "epoch": 0.006955484896661367,
      "grad_norm": 3.008054733276367,
      "learning_rate": 9.06365572066134e-05,
      "loss": 1.3524,
      "step": 105
    },
    {
      "epoch": 0.007021727609962904,
      "grad_norm": 3.6604371070861816,
      "learning_rate": 9.045084971874738e-05,
      "loss": 0.9391,
      "step": 106
    },
    {
      "epoch": 0.007087970323264441,
      "grad_norm": 3.0335724353790283,
      "learning_rate": 9.026351287655294e-05,
      "loss": 1.1299,
      "step": 107
    },
    {
      "epoch": 0.007154213036565978,
      "grad_norm": 4.3615593910217285,
      "learning_rate": 9.007455422593077e-05,
      "loss": 0.8973,
      "step": 108
    },
    {
      "epoch": 0.007220455749867514,
      "grad_norm": 3.213595151901245,
      "learning_rate": 8.988398137810777e-05,
      "loss": 1.1593,
      "step": 109
    },
    {
      "epoch": 0.007286698463169051,
      "grad_norm": 2.679920196533203,
      "learning_rate": 8.969180200933047e-05,
      "loss": 0.9451,
      "step": 110
    },
    {
      "epoch": 0.007352941176470588,
      "grad_norm": 3.1394662857055664,
      "learning_rate": 8.949802386055581e-05,
      "loss": 1.0178,
      "step": 111
    },
    {
      "epoch": 0.007419183889772125,
      "grad_norm": 2.8241398334503174,
      "learning_rate": 8.930265473713938e-05,
      "loss": 1.0239,
      "step": 112
    },
    {
      "epoch": 0.007485426603073662,
      "grad_norm": 2.8220155239105225,
      "learning_rate": 8.910570250852097e-05,
      "loss": 1.1167,
      "step": 113
    },
    {
      "epoch": 0.007551669316375199,
      "grad_norm": 3.1044211387634277,
      "learning_rate": 8.890717510790763e-05,
      "loss": 1.0398,
      "step": 114
    },
    {
      "epoch": 0.007617912029676735,
      "grad_norm": 3.0132229328155518,
      "learning_rate": 8.870708053195413e-05,
      "loss": 1.1759,
      "step": 115
    },
    {
      "epoch": 0.007684154742978272,
      "grad_norm": 2.792991876602173,
      "learning_rate": 8.850542684044078e-05,
      "loss": 1.2857,
      "step": 116
    },
    {
      "epoch": 0.007750397456279809,
      "grad_norm": 3.386343479156494,
      "learning_rate": 8.83022221559489e-05,
      "loss": 0.833,
      "step": 117
    },
    {
      "epoch": 0.007816640169581345,
      "grad_norm": 4.85018253326416,
      "learning_rate": 8.809747466353356e-05,
      "loss": 0.9716,
      "step": 118
    },
    {
      "epoch": 0.007882882882882882,
      "grad_norm": 3.574420928955078,
      "learning_rate": 8.789119261039385e-05,
      "loss": 0.9761,
      "step": 119
    },
    {
      "epoch": 0.00794912559618442,
      "grad_norm": 3.2426564693450928,
      "learning_rate": 8.768338430554082e-05,
      "loss": 1.1927,
      "step": 120
    },
    {
      "epoch": 0.008015368309485956,
      "grad_norm": 2.8880693912506104,
      "learning_rate": 8.74740581194627e-05,
      "loss": 1.1797,
      "step": 121
    },
    {
      "epoch": 0.008081611022787493,
      "grad_norm": 3.677746534347534,
      "learning_rate": 8.726322248378774e-05,
      "loss": 1.1132,
      "step": 122
    },
    {
      "epoch": 0.00814785373608903,
      "grad_norm": 4.071290493011475,
      "learning_rate": 8.705088589094459e-05,
      "loss": 1.2054,
      "step": 123
    },
    {
      "epoch": 0.008214096449390567,
      "grad_norm": 3.2814810276031494,
      "learning_rate": 8.683705689382024e-05,
      "loss": 1.1367,
      "step": 124
    },
    {
      "epoch": 0.008280339162692104,
      "grad_norm": 3.17580509185791,
      "learning_rate": 8.662174410541555e-05,
      "loss": 1.2001,
      "step": 125
    },
    {
      "epoch": 0.008346581875993641,
      "grad_norm": 2.702069044113159,
      "learning_rate": 8.640495619849821e-05,
      "loss": 1.1966,
      "step": 126
    },
    {
      "epoch": 0.008412824589295178,
      "grad_norm": 3.792174816131592,
      "learning_rate": 8.618670190525352e-05,
      "loss": 0.9687,
      "step": 127
    },
    {
      "epoch": 0.008479067302596715,
      "grad_norm": 3.31939959526062,
      "learning_rate": 8.596699001693255e-05,
      "loss": 0.92,
      "step": 128
    },
    {
      "epoch": 0.008545310015898252,
      "grad_norm": 3.4189999103546143,
      "learning_rate": 8.574582938349817e-05,
      "loss": 1.0359,
      "step": 129
    },
    {
      "epoch": 0.008611552729199789,
      "grad_norm": 4.046175956726074,
      "learning_rate": 8.552322891326846e-05,
      "loss": 1.1378,
      "step": 130
    },
    {
      "epoch": 0.008677795442501324,
      "grad_norm": 2.386441230773926,
      "learning_rate": 8.529919757255783e-05,
      "loss": 1.1806,
      "step": 131
    },
    {
      "epoch": 0.008744038155802861,
      "grad_norm": 3.603132724761963,
      "learning_rate": 8.507374438531607e-05,
      "loss": 1.1926,
      "step": 132
    },
    {
      "epoch": 0.008810280869104398,
      "grad_norm": 3.1990151405334473,
      "learning_rate": 8.484687843276467e-05,
      "loss": 1.039,
      "step": 133
    },
    {
      "epoch": 0.008876523582405935,
      "grad_norm": 3.0033857822418213,
      "learning_rate": 8.461860885303114e-05,
      "loss": 1.2236,
      "step": 134
    },
    {
      "epoch": 0.008942766295707472,
      "grad_norm": 2.9489827156066895,
      "learning_rate": 8.438894484078086e-05,
      "loss": 0.8662,
      "step": 135
    },
    {
      "epoch": 0.009009009009009009,
      "grad_norm": 3.539790153503418,
      "learning_rate": 8.415789564684673e-05,
      "loss": 1.3331,
      "step": 136
    },
    {
      "epoch": 0.009075251722310546,
      "grad_norm": 2.9523866176605225,
      "learning_rate": 8.392547057785661e-05,
      "loss": 1.1534,
      "step": 137
    },
    {
      "epoch": 0.009141494435612083,
      "grad_norm": 4.424468994140625,
      "learning_rate": 8.369167899585841e-05,
      "loss": 1.02,
      "step": 138
    },
    {
      "epoch": 0.00920773714891362,
      "grad_norm": 2.509406805038452,
      "learning_rate": 8.345653031794292e-05,
      "loss": 1.2234,
      "step": 139
    },
    {
      "epoch": 0.009273979862215157,
      "grad_norm": 3.0897953510284424,
      "learning_rate": 8.322003401586462e-05,
      "loss": 1.1343,
      "step": 140
    }
  ],
  "logging_steps": 1,
  "max_steps": 500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 9164373138800640.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
