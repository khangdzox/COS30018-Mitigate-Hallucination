{
  "best_metric": 0.9986802935600281,
  "best_model_checkpoint": "Finetuning/Fine-tuned_checkpoint/2/medical_3\\checkpoint-450",
  "epoch": 0.009845642209362111,
  "eval_steps": 25,
  "global_step": 450,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 2.187920490969358e-05,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.1489,
      "step": 1
    },
    {
      "epoch": 4.375840981938716e-05,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.2514,
      "step": 2
    },
    {
      "epoch": 6.563761472908074e-05,
      "grad_norm": 3.2709290981292725,
      "learning_rate": 4e-05,
      "loss": 2.3104,
      "step": 3
    },
    {
      "epoch": 8.751681963877433e-05,
      "grad_norm": 2.779268741607666,
      "learning_rate": 8e-05,
      "loss": 2.1807,
      "step": 4
    },
    {
      "epoch": 0.00010939602454846791,
      "grad_norm": 3.500542163848877,
      "learning_rate": 0.00012,
      "loss": 1.9871,
      "step": 5
    },
    {
      "epoch": 0.0001312752294581615,
      "grad_norm": NaN,
      "learning_rate": 0.00012,
      "loss": 2.0898,
      "step": 6
    },
    {
      "epoch": 0.00015315443436785507,
      "grad_norm": NaN,
      "learning_rate": 0.00012,
      "loss": 1.8829,
      "step": 7
    },
    {
      "epoch": 0.00017503363927754865,
      "grad_norm": NaN,
      "learning_rate": 0.00012,
      "loss": 2.1304,
      "step": 8
    },
    {
      "epoch": 0.00019691284418724223,
      "grad_norm": 27.946533203125,
      "learning_rate": 0.00016,
      "loss": 2.0801,
      "step": 9
    },
    {
      "epoch": 0.00021879204909693582,
      "grad_norm": 16.3977108001709,
      "learning_rate": 0.0002,
      "loss": 2.2192,
      "step": 10
    },
    {
      "epoch": 0.0002406712540066294,
      "grad_norm": 12.878182411193848,
      "learning_rate": 0.00019999798600729064,
      "loss": 2.2176,
      "step": 11
    },
    {
      "epoch": 0.000262550458916323,
      "grad_norm": 9.413625717163086,
      "learning_rate": 0.00019999194411028594,
      "loss": 2.1922,
      "step": 12
    },
    {
      "epoch": 0.00028442966382601656,
      "grad_norm": 9.80106258392334,
      "learning_rate": 0.0001999818745523526,
      "loss": 1.672,
      "step": 13
    },
    {
      "epoch": 0.00030630886873571014,
      "grad_norm": 20.085037231445312,
      "learning_rate": 0.00019996777773909093,
      "loss": 1.8428,
      "step": 14
    },
    {
      "epoch": 0.0003281880736454037,
      "grad_norm": 28.03141212463379,
      "learning_rate": 0.00019994965423831854,
      "loss": 1.8474,
      "step": 15
    },
    {
      "epoch": 0.0003500672785550973,
      "grad_norm": 4.915910243988037,
      "learning_rate": 0.00019992750478004738,
      "loss": 1.9661,
      "step": 16
    },
    {
      "epoch": 0.0003719464834647909,
      "grad_norm": 3.8566091060638428,
      "learning_rate": 0.0001999013302564544,
      "loss": 1.8644,
      "step": 17
    },
    {
      "epoch": 0.00039382568837448447,
      "grad_norm": 4.295337200164795,
      "learning_rate": 0.00019987113172184563,
      "loss": 1.6651,
      "step": 18
    },
    {
      "epoch": 0.00041570489328417805,
      "grad_norm": 5.446649551391602,
      "learning_rate": 0.00019983691039261357,
      "loss": 1.5854,
      "step": 19
    },
    {
      "epoch": 0.00043758409819387163,
      "grad_norm": 5.8035173416137695,
      "learning_rate": 0.00019979866764718843,
      "loss": 1.9175,
      "step": 20
    },
    {
      "epoch": 0.0004594633031035652,
      "grad_norm": 5.114415645599365,
      "learning_rate": 0.00019975640502598244,
      "loss": 1.299,
      "step": 21
    },
    {
      "epoch": 0.0004813425080132588,
      "grad_norm": 5.278352737426758,
      "learning_rate": 0.00019971012423132775,
      "loss": 1.191,
      "step": 22
    },
    {
      "epoch": 0.0005032217129229524,
      "grad_norm": 4.566121578216553,
      "learning_rate": 0.00019965982712740808,
      "loss": 1.4321,
      "step": 23
    },
    {
      "epoch": 0.000525100917832646,
      "grad_norm": 5.711122512817383,
      "learning_rate": 0.0001996055157401834,
      "loss": 1.1899,
      "step": 24
    },
    {
      "epoch": 0.0005469801227423396,
      "grad_norm": 5.113018035888672,
      "learning_rate": 0.00019954719225730847,
      "loss": 1.2518,
      "step": 25
    },
    {
      "epoch": 0.0005469801227423396,
      "eval_loss": 1.1922972202301025,
      "eval_runtime": 1914.1511,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 25
    },
    {
      "epoch": 0.0005688593276520331,
      "grad_norm": 4.588993549346924,
      "learning_rate": 0.0001994848590280447,
      "loss": 1.0657,
      "step": 26
    },
    {
      "epoch": 0.0005907385325617268,
      "grad_norm": 4.489006042480469,
      "learning_rate": 0.00019941851856316548,
      "loss": 1.2094,
      "step": 27
    },
    {
      "epoch": 0.0006126177374714203,
      "grad_norm": 5.219137191772461,
      "learning_rate": 0.00019934817353485501,
      "loss": 1.0861,
      "step": 28
    },
    {
      "epoch": 0.0006344969423811139,
      "grad_norm": 4.2921271324157715,
      "learning_rate": 0.00019927382677660088,
      "loss": 1.122,
      "step": 29
    },
    {
      "epoch": 0.0006563761472908074,
      "grad_norm": 4.807187557220459,
      "learning_rate": 0.00019919548128307954,
      "loss": 1.1372,
      "step": 30
    },
    {
      "epoch": 0.0006782553522005011,
      "grad_norm": 4.058358669281006,
      "learning_rate": 0.00019911314021003613,
      "loss": 0.8487,
      "step": 31
    },
    {
      "epoch": 0.0007001345571101946,
      "grad_norm": 3.881542444229126,
      "learning_rate": 0.00019902680687415705,
      "loss": 1.149,
      "step": 32
    },
    {
      "epoch": 0.0007220137620198882,
      "grad_norm": 4.037757873535156,
      "learning_rate": 0.00019893648475293648,
      "loss": 1.0259,
      "step": 33
    },
    {
      "epoch": 0.0007438929669295818,
      "grad_norm": 4.164768218994141,
      "learning_rate": 0.00019884217748453623,
      "loss": 1.0579,
      "step": 34
    },
    {
      "epoch": 0.0007657721718392754,
      "grad_norm": 3.8350396156311035,
      "learning_rate": 0.00019874388886763944,
      "loss": 0.9897,
      "step": 35
    },
    {
      "epoch": 0.0007876513767489689,
      "grad_norm": 3.524501085281372,
      "learning_rate": 0.0001986416228612972,
      "loss": 0.8777,
      "step": 36
    },
    {
      "epoch": 0.0008095305816586626,
      "grad_norm": 4.389486789703369,
      "learning_rate": 0.00019853538358476932,
      "loss": 1.0028,
      "step": 37
    },
    {
      "epoch": 0.0008314097865683561,
      "grad_norm": 5.2984418869018555,
      "learning_rate": 0.00019842517531735838,
      "loss": 0.8882,
      "step": 38
    },
    {
      "epoch": 0.0008532889914780497,
      "grad_norm": 4.486255645751953,
      "learning_rate": 0.00019831100249823733,
      "loss": 0.9041,
      "step": 39
    },
    {
      "epoch": 0.0008751681963877433,
      "grad_norm": 3.5563745498657227,
      "learning_rate": 0.00019819286972627066,
      "loss": 0.7836,
      "step": 40
    },
    {
      "epoch": 0.0008970474012974369,
      "grad_norm": 3.732107400894165,
      "learning_rate": 0.00019807078175982924,
      "loss": 0.8701,
      "step": 41
    },
    {
      "epoch": 0.0009189266062071304,
      "grad_norm": 4.825470924377441,
      "learning_rate": 0.00019794474351659852,
      "loss": 0.6458,
      "step": 42
    },
    {
      "epoch": 0.0009408058111168241,
      "grad_norm": 4.546532154083252,
      "learning_rate": 0.00019781476007338058,
      "loss": 0.5407,
      "step": 43
    },
    {
      "epoch": 0.0009626850160265176,
      "grad_norm": 3.5209569931030273,
      "learning_rate": 0.00019768083666588953,
      "loss": 0.4474,
      "step": 44
    },
    {
      "epoch": 0.0009845642209362112,
      "grad_norm": 2.735433578491211,
      "learning_rate": 0.00019754297868854073,
      "loss": 0.4174,
      "step": 45
    },
    {
      "epoch": 0.0010064434258459049,
      "grad_norm": 4.130967617034912,
      "learning_rate": 0.00019740119169423337,
      "loss": 0.5045,
      "step": 46
    },
    {
      "epoch": 0.0010283226307555983,
      "grad_norm": 3.451334238052368,
      "learning_rate": 0.00019725548139412692,
      "loss": 0.4736,
      "step": 47
    },
    {
      "epoch": 0.001050201835665292,
      "grad_norm": 3.0298266410827637,
      "learning_rate": 0.00019710585365741103,
      "loss": 0.4676,
      "step": 48
    },
    {
      "epoch": 0.0010720810405749855,
      "grad_norm": 4.125247955322266,
      "learning_rate": 0.00019695231451106912,
      "loss": 0.3847,
      "step": 49
    },
    {
      "epoch": 0.0010939602454846792,
      "grad_norm": 4.14768123626709,
      "learning_rate": 0.00019679487013963564,
      "loss": 0.4756,
      "step": 50
    },
    {
      "epoch": 0.0010939602454846792,
      "eval_loss": 1.0479252338409424,
      "eval_runtime": 2244.0112,
      "eval_samples_per_second": 0.373,
      "eval_steps_per_second": 0.373,
      "step": 50
    },
    {
      "epoch": 0.0011158394503943726,
      "grad_norm": 2.4684815406799316,
      "learning_rate": 0.00019663352688494684,
      "loss": 1.3828,
      "step": 51
    },
    {
      "epoch": 0.0011377186553040662,
      "grad_norm": 2.835707187652588,
      "learning_rate": 0.0001964682912458856,
      "loss": 1.2505,
      "step": 52
    },
    {
      "epoch": 0.0011595978602137599,
      "grad_norm": 3.27203106880188,
      "learning_rate": 0.00019629916987811926,
      "loss": 1.7888,
      "step": 53
    },
    {
      "epoch": 0.0011814770651234535,
      "grad_norm": 3.473505973815918,
      "learning_rate": 0.0001961261695938319,
      "loss": 1.3551,
      "step": 54
    },
    {
      "epoch": 0.001203356270033147,
      "grad_norm": 2.1574692726135254,
      "learning_rate": 0.00019594929736144976,
      "loss": 1.4639,
      "step": 55
    },
    {
      "epoch": 0.0012252354749428406,
      "grad_norm": 2.8092637062072754,
      "learning_rate": 0.00019576856030536054,
      "loss": 1.546,
      "step": 56
    },
    {
      "epoch": 0.0012471146798525342,
      "grad_norm": 3.09495210647583,
      "learning_rate": 0.0001955839657056265,
      "loss": 1.4857,
      "step": 57
    },
    {
      "epoch": 0.0012689938847622278,
      "grad_norm": 2.1254076957702637,
      "learning_rate": 0.00019539552099769126,
      "loss": 1.4344,
      "step": 58
    },
    {
      "epoch": 0.0012908730896719213,
      "grad_norm": 2.1704788208007812,
      "learning_rate": 0.00019520323377208017,
      "loss": 1.3843,
      "step": 59
    },
    {
      "epoch": 0.0013127522945816149,
      "grad_norm": 1.9811135530471802,
      "learning_rate": 0.00019500711177409454,
      "loss": 1.2643,
      "step": 60
    },
    {
      "epoch": 0.0013346314994913085,
      "grad_norm": 2.241314649581909,
      "learning_rate": 0.00019480716290349995,
      "loss": 1.4117,
      "step": 61
    },
    {
      "epoch": 0.0013565107044010022,
      "grad_norm": 2.4024972915649414,
      "learning_rate": 0.00019460339521420772,
      "loss": 1.2316,
      "step": 62
    },
    {
      "epoch": 0.0013783899093106956,
      "grad_norm": 2.691696882247925,
      "learning_rate": 0.00019439581691395067,
      "loss": 1.2826,
      "step": 63
    },
    {
      "epoch": 0.0014002691142203892,
      "grad_norm": 2.3235437870025635,
      "learning_rate": 0.00019418443636395248,
      "loss": 1.2188,
      "step": 64
    },
    {
      "epoch": 0.0014221483191300829,
      "grad_norm": 2.047386646270752,
      "learning_rate": 0.00019396926207859084,
      "loss": 1.5531,
      "step": 65
    },
    {
      "epoch": 0.0014440275240397765,
      "grad_norm": 2.65726900100708,
      "learning_rate": 0.00019375030272505463,
      "loss": 1.0757,
      "step": 66
    },
    {
      "epoch": 0.00146590672894947,
      "grad_norm": 2.378598928451538,
      "learning_rate": 0.00019352756712299468,
      "loss": 1.1644,
      "step": 67
    },
    {
      "epoch": 0.0014877859338591635,
      "grad_norm": 2.46109938621521,
      "learning_rate": 0.00019330106424416852,
      "loss": 1.2066,
      "step": 68
    },
    {
      "epoch": 0.0015096651387688572,
      "grad_norm": 2.7630972862243652,
      "learning_rate": 0.00019307080321207912,
      "loss": 1.2875,
      "step": 69
    },
    {
      "epoch": 0.0015315443436785508,
      "grad_norm": 2.292219638824463,
      "learning_rate": 0.00019283679330160726,
      "loss": 1.128,
      "step": 70
    },
    {
      "epoch": 0.0015534235485882442,
      "grad_norm": 2.4014084339141846,
      "learning_rate": 0.00019259904393863802,
      "loss": 1.1713,
      "step": 71
    },
    {
      "epoch": 0.0015753027534979379,
      "grad_norm": 2.270020008087158,
      "learning_rate": 0.0001923575646996811,
      "loss": 1.1165,
      "step": 72
    },
    {
      "epoch": 0.0015971819584076315,
      "grad_norm": 2.997513771057129,
      "learning_rate": 0.000192112365311485,
      "loss": 1.1613,
      "step": 73
    },
    {
      "epoch": 0.0016190611633173251,
      "grad_norm": 2.949875831604004,
      "learning_rate": 0.00019186345565064535,
      "loss": 1.2551,
      "step": 74
    },
    {
      "epoch": 0.0016409403682270186,
      "grad_norm": 2.4989330768585205,
      "learning_rate": 0.00019161084574320696,
      "loss": 1.1665,
      "step": 75
    },
    {
      "epoch": 0.0016409403682270186,
      "eval_loss": 1.0211350917816162,
      "eval_runtime": 2292.2752,
      "eval_samples_per_second": 0.365,
      "eval_steps_per_second": 0.365,
      "step": 75
    },
    {
      "epoch": 0.0016628195731367122,
      "grad_norm": 2.6845543384552,
      "learning_rate": 0.0001913545457642601,
      "loss": 1.1165,
      "step": 76
    },
    {
      "epoch": 0.0016846987780464058,
      "grad_norm": 2.6079254150390625,
      "learning_rate": 0.0001910945660375305,
      "loss": 1.1681,
      "step": 77
    },
    {
      "epoch": 0.0017065779829560995,
      "grad_norm": 2.4246103763580322,
      "learning_rate": 0.0001908309170349637,
      "loss": 1.0465,
      "step": 78
    },
    {
      "epoch": 0.0017284571878657929,
      "grad_norm": 2.8889334201812744,
      "learning_rate": 0.0001905636093763031,
      "loss": 0.9547,
      "step": 79
    },
    {
      "epoch": 0.0017503363927754865,
      "grad_norm": 3.004519462585449,
      "learning_rate": 0.00019029265382866214,
      "loss": 1.0678,
      "step": 80
    },
    {
      "epoch": 0.0017722155976851802,
      "grad_norm": 3.3404242992401123,
      "learning_rate": 0.0001900180613060908,
      "loss": 1.0148,
      "step": 81
    },
    {
      "epoch": 0.0017940948025948738,
      "grad_norm": 2.6319735050201416,
      "learning_rate": 0.00018973984286913584,
      "loss": 0.9487,
      "step": 82
    },
    {
      "epoch": 0.0018159740075045672,
      "grad_norm": 2.560392141342163,
      "learning_rate": 0.00018945800972439538,
      "loss": 0.9066,
      "step": 83
    },
    {
      "epoch": 0.0018378532124142608,
      "grad_norm": 2.362605571746826,
      "learning_rate": 0.00018917257322406734,
      "loss": 0.8231,
      "step": 84
    },
    {
      "epoch": 0.0018597324173239545,
      "grad_norm": 3.0136783123016357,
      "learning_rate": 0.00018888354486549237,
      "loss": 0.9301,
      "step": 85
    },
    {
      "epoch": 0.0018816116222336481,
      "grad_norm": 3.0647382736206055,
      "learning_rate": 0.00018859093629069058,
      "loss": 0.833,
      "step": 86
    },
    {
      "epoch": 0.0019034908271433415,
      "grad_norm": 2.915889263153076,
      "learning_rate": 0.00018829475928589271,
      "loss": 0.7995,
      "step": 87
    },
    {
      "epoch": 0.0019253700320530352,
      "grad_norm": 3.743215322494507,
      "learning_rate": 0.00018799502578106534,
      "loss": 0.6381,
      "step": 88
    },
    {
      "epoch": 0.0019472492369627288,
      "grad_norm": 3.00455641746521,
      "learning_rate": 0.0001876917478494303,
      "loss": 0.7331,
      "step": 89
    },
    {
      "epoch": 0.0019691284418724224,
      "grad_norm": 2.8673009872436523,
      "learning_rate": 0.00018738493770697852,
      "loss": 0.7309,
      "step": 90
    },
    {
      "epoch": 0.001991007646782116,
      "grad_norm": 3.5453310012817383,
      "learning_rate": 0.00018707460771197774,
      "loss": 0.6712,
      "step": 91
    },
    {
      "epoch": 0.0020128868516918097,
      "grad_norm": 3.176609754562378,
      "learning_rate": 0.00018676077036447494,
      "loss": 0.7036,
      "step": 92
    },
    {
      "epoch": 0.002034766056601503,
      "grad_norm": 2.8091795444488525,
      "learning_rate": 0.0001864434383057927,
      "loss": 0.5001,
      "step": 93
    },
    {
      "epoch": 0.0020566452615111966,
      "grad_norm": 2.55698561668396,
      "learning_rate": 0.00018612262431802007,
      "loss": 0.5897,
      "step": 94
    },
    {
      "epoch": 0.00207852446642089,
      "grad_norm": 2.899730682373047,
      "learning_rate": 0.00018579834132349772,
      "loss": 0.4938,
      "step": 95
    },
    {
      "epoch": 0.002100403671330584,
      "grad_norm": 3.249739170074463,
      "learning_rate": 0.00018547060238429736,
      "loss": 0.3385,
      "step": 96
    },
    {
      "epoch": 0.0021222828762402775,
      "grad_norm": 3.0253703594207764,
      "learning_rate": 0.0001851394207016957,
      "loss": 0.5308,
      "step": 97
    },
    {
      "epoch": 0.002144162081149971,
      "grad_norm": 3.144101858139038,
      "learning_rate": 0.0001848048096156426,
      "loss": 0.4482,
      "step": 98
    },
    {
      "epoch": 0.0021660412860596647,
      "grad_norm": 4.101083755493164,
      "learning_rate": 0.00018446678260422385,
      "loss": 0.6164,
      "step": 99
    },
    {
      "epoch": 0.0021879204909693584,
      "grad_norm": 3.0741498470306396,
      "learning_rate": 0.00018412535328311814,
      "loss": 0.4914,
      "step": 100
    },
    {
      "epoch": 0.0021879204909693584,
      "eval_loss": 1.0295677185058594,
      "eval_runtime": 2384.2426,
      "eval_samples_per_second": 0.351,
      "eval_steps_per_second": 0.351,
      "step": 100
    },
    {
      "epoch": 0.0022097996958790516,
      "grad_norm": 2.047893524169922,
      "learning_rate": 0.00018378053540504873,
      "loss": 1.5513,
      "step": 101
    },
    {
      "epoch": 0.002231678900788745,
      "grad_norm": 2.003300666809082,
      "learning_rate": 0.00018343234285922953,
      "loss": 1.373,
      "step": 102
    },
    {
      "epoch": 0.002253558105698439,
      "grad_norm": 2.0060677528381348,
      "learning_rate": 0.00018308078967080546,
      "loss": 1.4835,
      "step": 103
    },
    {
      "epoch": 0.0022754373106081325,
      "grad_norm": 1.9966665506362915,
      "learning_rate": 0.00018272589000028772,
      "loss": 1.5559,
      "step": 104
    },
    {
      "epoch": 0.002297316515517826,
      "grad_norm": 2.153258800506592,
      "learning_rate": 0.0001823676581429833,
      "loss": 1.4547,
      "step": 105
    },
    {
      "epoch": 0.0023191957204275197,
      "grad_norm": 1.9949474334716797,
      "learning_rate": 0.00018200610852841913,
      "loss": 1.5239,
      "step": 106
    },
    {
      "epoch": 0.0023410749253372134,
      "grad_norm": 1.9800158739089966,
      "learning_rate": 0.00018164125571976098,
      "loss": 1.4176,
      "step": 107
    },
    {
      "epoch": 0.002362954130246907,
      "grad_norm": 1.7942959070205688,
      "learning_rate": 0.0001812731144132268,
      "loss": 1.3128,
      "step": 108
    },
    {
      "epoch": 0.0023848333351566002,
      "grad_norm": 2.085221767425537,
      "learning_rate": 0.00018090169943749476,
      "loss": 1.3837,
      "step": 109
    },
    {
      "epoch": 0.002406712540066294,
      "grad_norm": 2.0194644927978516,
      "learning_rate": 0.00018052702575310588,
      "loss": 1.2042,
      "step": 110
    },
    {
      "epoch": 0.0024285917449759875,
      "grad_norm": 1.767561435699463,
      "learning_rate": 0.00018014910845186153,
      "loss": 1.4426,
      "step": 111
    },
    {
      "epoch": 0.002450470949885681,
      "grad_norm": 2.0302302837371826,
      "learning_rate": 0.00017976796275621555,
      "loss": 1.1742,
      "step": 112
    },
    {
      "epoch": 0.0024723501547953748,
      "grad_norm": 2.2055411338806152,
      "learning_rate": 0.00017938360401866093,
      "loss": 1.278,
      "step": 113
    },
    {
      "epoch": 0.0024942293597050684,
      "grad_norm": 2.5468649864196777,
      "learning_rate": 0.00017899604772111163,
      "loss": 1.3092,
      "step": 114
    },
    {
      "epoch": 0.002516108564614762,
      "grad_norm": 2.0420081615448,
      "learning_rate": 0.00017860530947427875,
      "loss": 1.1001,
      "step": 115
    },
    {
      "epoch": 0.0025379877695244557,
      "grad_norm": 1.9514058828353882,
      "learning_rate": 0.00017821140501704194,
      "loss": 1.2121,
      "step": 116
    },
    {
      "epoch": 0.002559866974434149,
      "grad_norm": 2.1732685565948486,
      "learning_rate": 0.00017781435021581527,
      "loss": 1.1049,
      "step": 117
    },
    {
      "epoch": 0.0025817461793438425,
      "grad_norm": 2.1666769981384277,
      "learning_rate": 0.00017741416106390826,
      "loss": 1.0371,
      "step": 118
    },
    {
      "epoch": 0.002603625384253536,
      "grad_norm": 2.6952311992645264,
      "learning_rate": 0.00017701085368088156,
      "loss": 1.1353,
      "step": 119
    },
    {
      "epoch": 0.0026255045891632298,
      "grad_norm": 1.9632771015167236,
      "learning_rate": 0.0001766044443118978,
      "loss": 0.9534,
      "step": 120
    },
    {
      "epoch": 0.0026473837940729234,
      "grad_norm": 2.292632579803467,
      "learning_rate": 0.0001761949493270671,
      "loss": 1.0353,
      "step": 121
    },
    {
      "epoch": 0.002669262998982617,
      "grad_norm": 2.140517234802246,
      "learning_rate": 0.0001757823852207877,
      "loss": 1.0749,
      "step": 122
    },
    {
      "epoch": 0.0026911422038923107,
      "grad_norm": 3.025057554244995,
      "learning_rate": 0.00017536676861108164,
      "loss": 1.2436,
      "step": 123
    },
    {
      "epoch": 0.0027130214088020043,
      "grad_norm": 2.670936107635498,
      "learning_rate": 0.0001749481162389254,
      "loss": 1.0804,
      "step": 124
    },
    {
      "epoch": 0.0027349006137116975,
      "grad_norm": 1.8179781436920166,
      "learning_rate": 0.00017452644496757547,
      "loss": 1.0845,
      "step": 125
    },
    {
      "epoch": 0.0027349006137116975,
      "eval_loss": 1.0170316696166992,
      "eval_runtime": 2345.7791,
      "eval_samples_per_second": 0.357,
      "eval_steps_per_second": 0.357,
      "step": 125
    },
    {
      "epoch": 0.002756779818621391,
      "grad_norm": 3.1400606632232666,
      "learning_rate": 0.00017410177178188918,
      "loss": 1.2351,
      "step": 126
    },
    {
      "epoch": 0.002778659023531085,
      "grad_norm": 2.2635045051574707,
      "learning_rate": 0.0001736741137876405,
      "loss": 0.9059,
      "step": 127
    },
    {
      "epoch": 0.0028005382284407784,
      "grad_norm": 2.3612606525421143,
      "learning_rate": 0.0001732434882108311,
      "loss": 0.8582,
      "step": 128
    },
    {
      "epoch": 0.002822417433350472,
      "grad_norm": 2.61826229095459,
      "learning_rate": 0.00017280991239699642,
      "loss": 0.9309,
      "step": 129
    },
    {
      "epoch": 0.0028442966382601657,
      "grad_norm": 2.4500739574432373,
      "learning_rate": 0.00017237340381050703,
      "loss": 0.8815,
      "step": 130
    },
    {
      "epoch": 0.0028661758431698593,
      "grad_norm": 2.6421356201171875,
      "learning_rate": 0.0001719339800338651,
      "loss": 0.8395,
      "step": 131
    },
    {
      "epoch": 0.002888055048079553,
      "grad_norm": 2.155247449874878,
      "learning_rate": 0.00017149165876699635,
      "loss": 0.8199,
      "step": 132
    },
    {
      "epoch": 0.002909934252989246,
      "grad_norm": 1.975793480873108,
      "learning_rate": 0.0001710464578265369,
      "loss": 0.7158,
      "step": 133
    },
    {
      "epoch": 0.00293181345789894,
      "grad_norm": 2.2563371658325195,
      "learning_rate": 0.00017059839514511565,
      "loss": 0.7996,
      "step": 134
    },
    {
      "epoch": 0.0029536926628086334,
      "grad_norm": 2.2849698066711426,
      "learning_rate": 0.00017014748877063214,
      "loss": 0.6947,
      "step": 135
    },
    {
      "epoch": 0.002975571867718327,
      "grad_norm": 3.1464266777038574,
      "learning_rate": 0.00016969375686552935,
      "loss": 0.8948,
      "step": 136
    },
    {
      "epoch": 0.0029974510726280207,
      "grad_norm": 2.3216614723205566,
      "learning_rate": 0.00016923721770606228,
      "loss": 0.6492,
      "step": 137
    },
    {
      "epoch": 0.0030193302775377144,
      "grad_norm": 2.4590539932250977,
      "learning_rate": 0.0001687778896815617,
      "loss": 0.6872,
      "step": 138
    },
    {
      "epoch": 0.003041209482447408,
      "grad_norm": 2.5943121910095215,
      "learning_rate": 0.00016831579129369346,
      "loss": 0.6785,
      "step": 139
    },
    {
      "epoch": 0.0030630886873571016,
      "grad_norm": 3.1107752323150635,
      "learning_rate": 0.00016785094115571322,
      "loss": 0.7062,
      "step": 140
    },
    {
      "epoch": 0.003084967892266795,
      "grad_norm": 2.4394242763519287,
      "learning_rate": 0.00016738335799171682,
      "loss": 0.5371,
      "step": 141
    },
    {
      "epoch": 0.0031068470971764885,
      "grad_norm": 3.405695915222168,
      "learning_rate": 0.00016691306063588583,
      "loss": 0.7447,
      "step": 142
    },
    {
      "epoch": 0.003128726302086182,
      "grad_norm": 2.860955238342285,
      "learning_rate": 0.00016644006803172924,
      "loss": 0.5884,
      "step": 143
    },
    {
      "epoch": 0.0031506055069958757,
      "grad_norm": 4.096427917480469,
      "learning_rate": 0.00016596439923132015,
      "loss": 0.7599,
      "step": 144
    },
    {
      "epoch": 0.0031724847119055694,
      "grad_norm": 2.120732545852661,
      "learning_rate": 0.00016548607339452853,
      "loss": 0.4696,
      "step": 145
    },
    {
      "epoch": 0.003194363916815263,
      "grad_norm": 2.5159990787506104,
      "learning_rate": 0.00016500510978824926,
      "loss": 0.5496,
      "step": 146
    },
    {
      "epoch": 0.0032162431217249566,
      "grad_norm": 2.4202635288238525,
      "learning_rate": 0.0001645215277856263,
      "loss": 0.5653,
      "step": 147
    },
    {
      "epoch": 0.0032381223266346503,
      "grad_norm": 3.990581750869751,
      "learning_rate": 0.00016403534686527225,
      "loss": 0.4697,
      "step": 148
    },
    {
      "epoch": 0.0032600015315443435,
      "grad_norm": 1.9455926418304443,
      "learning_rate": 0.00016354658661048364,
      "loss": 0.4656,
      "step": 149
    },
    {
      "epoch": 0.003281880736454037,
      "grad_norm": 2.953429937362671,
      "learning_rate": 0.00016305526670845226,
      "loss": 0.5323,
      "step": 150
    },
    {
      "epoch": 0.003281880736454037,
      "eval_loss": 1.0263699293136597,
      "eval_runtime": 2022.1062,
      "eval_samples_per_second": 0.414,
      "eval_steps_per_second": 0.414,
      "step": 150
    },
    {
      "epoch": 0.0033037599413637308,
      "grad_norm": 2.1393024921417236,
      "learning_rate": 0.00016256140694947214,
      "loss": 1.6442,
      "step": 151
    },
    {
      "epoch": 0.0033256391462734244,
      "grad_norm": 2.887155055999756,
      "learning_rate": 0.00016206502722614238,
      "loss": 1.3616,
      "step": 152
    },
    {
      "epoch": 0.003347518351183118,
      "grad_norm": 1.8749501705169678,
      "learning_rate": 0.0001615661475325658,
      "loss": 1.6493,
      "step": 153
    },
    {
      "epoch": 0.0033693975560928117,
      "grad_norm": 2.1255173683166504,
      "learning_rate": 0.00016106478796354382,
      "loss": 1.329,
      "step": 154
    },
    {
      "epoch": 0.0033912767610025053,
      "grad_norm": 2.4779014587402344,
      "learning_rate": 0.00016056096871376667,
      "loss": 1.4879,
      "step": 155
    },
    {
      "epoch": 0.003413155965912199,
      "grad_norm": 2.176313877105713,
      "learning_rate": 0.00016005471007700031,
      "loss": 1.2862,
      "step": 156
    },
    {
      "epoch": 0.003435035170821892,
      "grad_norm": 3.133812665939331,
      "learning_rate": 0.0001595460324452688,
      "loss": 1.2171,
      "step": 157
    },
    {
      "epoch": 0.0034569143757315858,
      "grad_norm": 1.80746591091156,
      "learning_rate": 0.000159034956308033,
      "loss": 1.3368,
      "step": 158
    },
    {
      "epoch": 0.0034787935806412794,
      "grad_norm": 2.0994222164154053,
      "learning_rate": 0.00015852150225136518,
      "loss": 1.3704,
      "step": 159
    },
    {
      "epoch": 0.003500672785550973,
      "grad_norm": 2.343292713165283,
      "learning_rate": 0.00015800569095711982,
      "loss": 1.399,
      "step": 160
    },
    {
      "epoch": 0.0035225519904606667,
      "grad_norm": 2.077725410461426,
      "learning_rate": 0.00015748754320210072,
      "loss": 1.2064,
      "step": 161
    },
    {
      "epoch": 0.0035444311953703603,
      "grad_norm": 1.805177092552185,
      "learning_rate": 0.0001569670798572239,
      "loss": 1.2201,
      "step": 162
    },
    {
      "epoch": 0.003566310400280054,
      "grad_norm": 1.9531844854354858,
      "learning_rate": 0.00015644432188667695,
      "loss": 1.2113,
      "step": 163
    },
    {
      "epoch": 0.0035881896051897476,
      "grad_norm": 1.6639947891235352,
      "learning_rate": 0.0001559192903470747,
      "loss": 1.3234,
      "step": 164
    },
    {
      "epoch": 0.003610068810099441,
      "grad_norm": 2.222898006439209,
      "learning_rate": 0.00015539200638661104,
      "loss": 1.064,
      "step": 165
    },
    {
      "epoch": 0.0036319480150091344,
      "grad_norm": 2.164314031600952,
      "learning_rate": 0.000154862491244207,
      "loss": 1.4272,
      "step": 166
    },
    {
      "epoch": 0.003653827219918828,
      "grad_norm": 3.3964216709136963,
      "learning_rate": 0.00015433076624865531,
      "loss": 1.2594,
      "step": 167
    },
    {
      "epoch": 0.0036757064248285217,
      "grad_norm": 1.6932060718536377,
      "learning_rate": 0.00015379685281776125,
      "loss": 0.9795,
      "step": 168
    },
    {
      "epoch": 0.0036975856297382153,
      "grad_norm": 2.466364622116089,
      "learning_rate": 0.00015326077245747999,
      "loss": 1.0687,
      "step": 169
    },
    {
      "epoch": 0.003719464834647909,
      "grad_norm": 2.3859012126922607,
      "learning_rate": 0.00015272254676105025,
      "loss": 1.2019,
      "step": 170
    },
    {
      "epoch": 0.0037413440395576026,
      "grad_norm": 2.3799631595611572,
      "learning_rate": 0.0001521821974081246,
      "loss": 1.3411,
      "step": 171
    },
    {
      "epoch": 0.0037632232444672962,
      "grad_norm": 2.5653069019317627,
      "learning_rate": 0.0001516397461638962,
      "loss": 1.0896,
      "step": 172
    },
    {
      "epoch": 0.0037851024493769894,
      "grad_norm": 2.5625128746032715,
      "learning_rate": 0.00015109521487822206,
      "loss": 1.1308,
      "step": 173
    },
    {
      "epoch": 0.003806981654286683,
      "grad_norm": 2.1904702186584473,
      "learning_rate": 0.000150548625484743,
      "loss": 0.8308,
      "step": 174
    },
    {
      "epoch": 0.0038288608591963767,
      "grad_norm": 2.1738123893737793,
      "learning_rate": 0.00015000000000000001,
      "loss": 0.9327,
      "step": 175
    },
    {
      "epoch": 0.0038288608591963767,
      "eval_loss": 1.0102177858352661,
      "eval_runtime": 2043.698,
      "eval_samples_per_second": 0.41,
      "eval_steps_per_second": 0.41,
      "step": 175
    },
    {
      "epoch": 0.0038507400641060703,
      "grad_norm": 2.1486189365386963,
      "learning_rate": 0.0001494493605225477,
      "loss": 0.8329,
      "step": 176
    },
    {
      "epoch": 0.003872619269015764,
      "grad_norm": 2.224435329437256,
      "learning_rate": 0.0001488967292320639,
      "loss": 1.1457,
      "step": 177
    },
    {
      "epoch": 0.0038944984739254576,
      "grad_norm": 2.3640012741088867,
      "learning_rate": 0.00014834212838845637,
      "loss": 0.9384,
      "step": 178
    },
    {
      "epoch": 0.003916377678835151,
      "grad_norm": 2.3616738319396973,
      "learning_rate": 0.00014778558033096633,
      "loss": 0.968,
      "step": 179
    },
    {
      "epoch": 0.003938256883744845,
      "grad_norm": 2.9090635776519775,
      "learning_rate": 0.0001472271074772683,
      "loss": 0.8382,
      "step": 180
    },
    {
      "epoch": 0.003960136088654538,
      "grad_norm": 2.6539201736450195,
      "learning_rate": 0.00014666673232256738,
      "loss": 0.7864,
      "step": 181
    },
    {
      "epoch": 0.003982015293564232,
      "grad_norm": 2.3968169689178467,
      "learning_rate": 0.00014610447743869314,
      "loss": 0.6355,
      "step": 182
    },
    {
      "epoch": 0.004003894498473925,
      "grad_norm": 2.634657382965088,
      "learning_rate": 0.0001455403654731903,
      "loss": 0.9017,
      "step": 183
    },
    {
      "epoch": 0.004025773703383619,
      "grad_norm": 2.9040074348449707,
      "learning_rate": 0.0001449744191484066,
      "loss": 0.8115,
      "step": 184
    },
    {
      "epoch": 0.004047652908293313,
      "grad_norm": 2.451035737991333,
      "learning_rate": 0.00014440666126057744,
      "loss": 0.7758,
      "step": 185
    },
    {
      "epoch": 0.004069532113203006,
      "grad_norm": 2.430938959121704,
      "learning_rate": 0.00014383711467890774,
      "loss": 0.8412,
      "step": 186
    },
    {
      "epoch": 0.0040914113181127,
      "grad_norm": 3.020045042037964,
      "learning_rate": 0.00014326580234465085,
      "loss": 0.7734,
      "step": 187
    },
    {
      "epoch": 0.004113290523022393,
      "grad_norm": 3.8700296878814697,
      "learning_rate": 0.0001426927472701842,
      "loss": 0.9098,
      "step": 188
    },
    {
      "epoch": 0.004135169727932087,
      "grad_norm": 1.986897587776184,
      "learning_rate": 0.00014211797253808268,
      "loss": 0.4828,
      "step": 189
    },
    {
      "epoch": 0.00415704893284178,
      "grad_norm": 2.6167354583740234,
      "learning_rate": 0.00014154150130018866,
      "loss": 0.4839,
      "step": 190
    },
    {
      "epoch": 0.0041789281377514744,
      "grad_norm": 2.4971776008605957,
      "learning_rate": 0.00014096335677667954,
      "loss": 0.5831,
      "step": 191
    },
    {
      "epoch": 0.004200807342661168,
      "grad_norm": 3.629246711730957,
      "learning_rate": 0.00014038356225513248,
      "loss": 0.6916,
      "step": 192
    },
    {
      "epoch": 0.004222686547570861,
      "grad_norm": 3.7974228858947754,
      "learning_rate": 0.00013980214108958624,
      "loss": 0.4834,
      "step": 193
    },
    {
      "epoch": 0.004244565752480555,
      "grad_norm": 3.1534810066223145,
      "learning_rate": 0.00013921911669960055,
      "loss": 0.5248,
      "step": 194
    },
    {
      "epoch": 0.004266444957390248,
      "grad_norm": 1.9292908906936646,
      "learning_rate": 0.00013863451256931287,
      "loss": 0.3412,
      "step": 195
    },
    {
      "epoch": 0.004288324162299942,
      "grad_norm": 2.455993413925171,
      "learning_rate": 0.0001380483522464923,
      "loss": 0.5754,
      "step": 196
    },
    {
      "epoch": 0.004310203367209635,
      "grad_norm": 3.127329111099243,
      "learning_rate": 0.00013746065934159123,
      "loss": 0.5141,
      "step": 197
    },
    {
      "epoch": 0.0043320825721193295,
      "grad_norm": 3.6233246326446533,
      "learning_rate": 0.0001368714575267941,
      "loss": 0.5456,
      "step": 198
    },
    {
      "epoch": 0.004353961777029023,
      "grad_norm": 2.5969443321228027,
      "learning_rate": 0.0001362807705350641,
      "loss": 0.4715,
      "step": 199
    },
    {
      "epoch": 0.004375840981938717,
      "grad_norm": 2.122073173522949,
      "learning_rate": 0.00013568862215918717,
      "loss": 0.4502,
      "step": 200
    },
    {
      "epoch": 0.004375840981938717,
      "eval_loss": 1.0119068622589111,
      "eval_runtime": 2060.4284,
      "eval_samples_per_second": 0.406,
      "eval_steps_per_second": 0.406,
      "step": 200
    },
    {
      "epoch": 0.00439772018684841,
      "grad_norm": 2.100381374359131,
      "learning_rate": 0.00013509503625081358,
      "loss": 1.6707,
      "step": 201
    },
    {
      "epoch": 0.004419599391758103,
      "grad_norm": 1.854717493057251,
      "learning_rate": 0.00013450003671949706,
      "loss": 1.5099,
      "step": 202
    },
    {
      "epoch": 0.004441478596667797,
      "grad_norm": 2.0012567043304443,
      "learning_rate": 0.00013390364753173206,
      "loss": 1.3675,
      "step": 203
    },
    {
      "epoch": 0.00446335780157749,
      "grad_norm": 1.7509772777557373,
      "learning_rate": 0.00013330589270998808,
      "loss": 1.2602,
      "step": 204
    },
    {
      "epoch": 0.0044852370064871845,
      "grad_norm": 2.069669008255005,
      "learning_rate": 0.00013270679633174218,
      "loss": 1.5031,
      "step": 205
    },
    {
      "epoch": 0.004507116211396878,
      "grad_norm": 1.851431131362915,
      "learning_rate": 0.00013210638252850908,
      "loss": 1.2242,
      "step": 206
    },
    {
      "epoch": 0.004528995416306572,
      "grad_norm": 2.854668378829956,
      "learning_rate": 0.0001315046754848693,
      "loss": 1.1869,
      "step": 207
    },
    {
      "epoch": 0.004550874621216265,
      "grad_norm": 1.7559220790863037,
      "learning_rate": 0.00013090169943749476,
      "loss": 1.0646,
      "step": 208
    },
    {
      "epoch": 0.004572753826125958,
      "grad_norm": 2.1576266288757324,
      "learning_rate": 0.00013029747867417276,
      "loss": 1.5068,
      "step": 209
    },
    {
      "epoch": 0.004594633031035652,
      "grad_norm": 1.8369613885879517,
      "learning_rate": 0.0001296920375328275,
      "loss": 1.3125,
      "step": 210
    },
    {
      "epoch": 0.004616512235945345,
      "grad_norm": 1.7737456560134888,
      "learning_rate": 0.0001290854004005399,
      "loss": 1.086,
      "step": 211
    },
    {
      "epoch": 0.0046383914408550395,
      "grad_norm": 2.1841211318969727,
      "learning_rate": 0.00012847759171256523,
      "loss": 1.2898,
      "step": 212
    },
    {
      "epoch": 0.004660270645764733,
      "grad_norm": 2.783679723739624,
      "learning_rate": 0.0001278686359513488,
      "loss": 1.2587,
      "step": 213
    },
    {
      "epoch": 0.004682149850674427,
      "grad_norm": 1.7687182426452637,
      "learning_rate": 0.0001272585576455398,
      "loss": 1.3263,
      "step": 214
    },
    {
      "epoch": 0.00470402905558412,
      "grad_norm": 1.9682635068893433,
      "learning_rate": 0.00012664738136900348,
      "loss": 1.1423,
      "step": 215
    },
    {
      "epoch": 0.004725908260493814,
      "grad_norm": 1.73717200756073,
      "learning_rate": 0.0001260351317398312,
      "loss": 1.1171,
      "step": 216
    },
    {
      "epoch": 0.004747787465403507,
      "grad_norm": 1.6289108991622925,
      "learning_rate": 0.00012542183341934872,
      "loss": 1.0802,
      "step": 217
    },
    {
      "epoch": 0.0047696666703132004,
      "grad_norm": 2.13883900642395,
      "learning_rate": 0.0001248075111111229,
      "loss": 0.9732,
      "step": 218
    },
    {
      "epoch": 0.0047915458752228945,
      "grad_norm": 2.2125933170318604,
      "learning_rate": 0.00012419218955996676,
      "loss": 0.9819,
      "step": 219
    },
    {
      "epoch": 0.004813425080132588,
      "grad_norm": 2.029482841491699,
      "learning_rate": 0.00012357589355094275,
      "loss": 1.1952,
      "step": 220
    },
    {
      "epoch": 0.004835304285042282,
      "grad_norm": 2.7746028900146484,
      "learning_rate": 0.0001229586479083641,
      "loss": 1.3261,
      "step": 221
    },
    {
      "epoch": 0.004857183489951975,
      "grad_norm": 2.0585696697235107,
      "learning_rate": 0.00012234047749479544,
      "loss": 0.9645,
      "step": 222
    },
    {
      "epoch": 0.004879062694861669,
      "grad_norm": 2.5084571838378906,
      "learning_rate": 0.00012172140721005079,
      "loss": 1.0772,
      "step": 223
    },
    {
      "epoch": 0.004900941899771362,
      "grad_norm": 1.847245216369629,
      "learning_rate": 0.000121101461990191,
      "loss": 1.0717,
      "step": 224
    },
    {
      "epoch": 0.0049228211046810555,
      "grad_norm": 2.0576205253601074,
      "learning_rate": 0.00012048066680651908,
      "loss": 0.7095,
      "step": 225
    },
    {
      "epoch": 0.0049228211046810555,
      "eval_loss": 1.0100913047790527,
      "eval_runtime": 2041.3087,
      "eval_samples_per_second": 0.41,
      "eval_steps_per_second": 0.41,
      "step": 225
    },
    {
      "epoch": 0.0049447003095907495,
      "grad_norm": 2.527238368988037,
      "learning_rate": 0.00011985904666457455,
      "loss": 1.0365,
      "step": 226
    },
    {
      "epoch": 0.004966579514500443,
      "grad_norm": 2.4983737468719482,
      "learning_rate": 0.00011923662660312611,
      "loss": 1.0625,
      "step": 227
    },
    {
      "epoch": 0.004988458719410137,
      "grad_norm": 2.5282070636749268,
      "learning_rate": 0.00011861343169316301,
      "loss": 0.8804,
      "step": 228
    },
    {
      "epoch": 0.00501033792431983,
      "grad_norm": 2.2000861167907715,
      "learning_rate": 0.00011798948703688539,
      "loss": 0.9255,
      "step": 229
    },
    {
      "epoch": 0.005032217129229524,
      "grad_norm": 2.0931503772735596,
      "learning_rate": 0.00011736481776669306,
      "loss": 0.947,
      "step": 230
    },
    {
      "epoch": 0.005054096334139217,
      "grad_norm": 2.193046808242798,
      "learning_rate": 0.00011673944904417308,
      "loss": 0.9635,
      "step": 231
    },
    {
      "epoch": 0.005075975539048911,
      "grad_norm": 2.5529487133026123,
      "learning_rate": 0.00011611340605908642,
      "loss": 1.0039,
      "step": 232
    },
    {
      "epoch": 0.0050978547439586045,
      "grad_norm": 2.6898205280303955,
      "learning_rate": 0.00011548671402835325,
      "loss": 0.7438,
      "step": 233
    },
    {
      "epoch": 0.005119733948868298,
      "grad_norm": 3.0761213302612305,
      "learning_rate": 0.00011485939819503717,
      "loss": 0.9677,
      "step": 234
    },
    {
      "epoch": 0.005141613153777992,
      "grad_norm": 2.2238330841064453,
      "learning_rate": 0.00011423148382732853,
      "loss": 0.7824,
      "step": 235
    },
    {
      "epoch": 0.005163492358687685,
      "grad_norm": 2.240037441253662,
      "learning_rate": 0.00011360299621752644,
      "loss": 0.6899,
      "step": 236
    },
    {
      "epoch": 0.005185371563597379,
      "grad_norm": 2.174140691757202,
      "learning_rate": 0.00011297396068102017,
      "loss": 0.7468,
      "step": 237
    },
    {
      "epoch": 0.005207250768507072,
      "grad_norm": 2.1373889446258545,
      "learning_rate": 0.00011234440255526948,
      "loss": 0.6043,
      "step": 238
    },
    {
      "epoch": 0.005229129973416766,
      "grad_norm": 2.327693462371826,
      "learning_rate": 0.00011171434719878384,
      "loss": 0.6554,
      "step": 239
    },
    {
      "epoch": 0.0052510091783264596,
      "grad_norm": 3.228789806365967,
      "learning_rate": 0.00011108381999010111,
      "loss": 0.6614,
      "step": 240
    },
    {
      "epoch": 0.005272888383236154,
      "grad_norm": 2.4611475467681885,
      "learning_rate": 0.00011045284632676536,
      "loss": 0.5144,
      "step": 241
    },
    {
      "epoch": 0.005294767588145847,
      "grad_norm": 3.708246946334839,
      "learning_rate": 0.00010982145162430373,
      "loss": 0.628,
      "step": 242
    },
    {
      "epoch": 0.00531664679305554,
      "grad_norm": 3.8016910552978516,
      "learning_rate": 0.00010918966131520277,
      "loss": 0.5706,
      "step": 243
    },
    {
      "epoch": 0.005338525997965234,
      "grad_norm": 2.656146764755249,
      "learning_rate": 0.00010855750084788398,
      "loss": 0.648,
      "step": 244
    },
    {
      "epoch": 0.005360405202874927,
      "grad_norm": 3.203629732131958,
      "learning_rate": 0.00010792499568567884,
      "loss": 0.6072,
      "step": 245
    },
    {
      "epoch": 0.005382284407784621,
      "grad_norm": 3.5270371437072754,
      "learning_rate": 0.0001072921713058031,
      "loss": 0.5934,
      "step": 246
    },
    {
      "epoch": 0.005404163612694315,
      "grad_norm": 3.237220287322998,
      "learning_rate": 0.00010665905319833041,
      "loss": 0.5463,
      "step": 247
    },
    {
      "epoch": 0.005426042817604009,
      "grad_norm": 3.6706695556640625,
      "learning_rate": 0.00010602566686516586,
      "loss": 0.4587,
      "step": 248
    },
    {
      "epoch": 0.005447922022513702,
      "grad_norm": 2.720128059387207,
      "learning_rate": 0.00010539203781901861,
      "loss": 0.4919,
      "step": 249
    },
    {
      "epoch": 0.005469801227423395,
      "grad_norm": 3.179784059524536,
      "learning_rate": 0.00010475819158237425,
      "loss": 0.587,
      "step": 250
    },
    {
      "epoch": 0.005469801227423395,
      "eval_loss": 1.0056504011154175,
      "eval_runtime": 2045.6789,
      "eval_samples_per_second": 0.409,
      "eval_steps_per_second": 0.409,
      "step": 250
    },
    {
      "epoch": 0.005491680432333089,
      "grad_norm": 1.702817440032959,
      "learning_rate": 0.00010412415368646673,
      "loss": 1.4535,
      "step": 251
    },
    {
      "epoch": 0.005513559637242782,
      "grad_norm": 1.5473411083221436,
      "learning_rate": 0.00010348994967025012,
      "loss": 1.3771,
      "step": 252
    },
    {
      "epoch": 0.005535438842152476,
      "grad_norm": 2.764439821243286,
      "learning_rate": 0.00010285560507936961,
      "loss": 1.6902,
      "step": 253
    },
    {
      "epoch": 0.00555731804706217,
      "grad_norm": 1.7280420064926147,
      "learning_rate": 0.00010222114546513295,
      "loss": 1.3864,
      "step": 254
    },
    {
      "epoch": 0.005579197251971864,
      "grad_norm": 1.8132249116897583,
      "learning_rate": 0.00010158659638348081,
      "loss": 1.291,
      "step": 255
    },
    {
      "epoch": 0.005601076456881557,
      "grad_norm": 2.213444232940674,
      "learning_rate": 0.00010095198339395769,
      "loss": 1.6253,
      "step": 256
    },
    {
      "epoch": 0.005622955661791251,
      "grad_norm": 2.0258429050445557,
      "learning_rate": 0.00010031733205868224,
      "loss": 1.3965,
      "step": 257
    },
    {
      "epoch": 0.005644834866700944,
      "grad_norm": 1.864917278289795,
      "learning_rate": 9.968266794131777e-05,
      "loss": 1.1432,
      "step": 258
    },
    {
      "epoch": 0.005666714071610637,
      "grad_norm": 1.667617678642273,
      "learning_rate": 9.904801660604234e-05,
      "loss": 1.3403,
      "step": 259
    },
    {
      "epoch": 0.005688593276520331,
      "grad_norm": 1.847839593887329,
      "learning_rate": 9.84134036165192e-05,
      "loss": 0.8391,
      "step": 260
    },
    {
      "epoch": 0.005710472481430025,
      "grad_norm": 1.5734866857528687,
      "learning_rate": 9.777885453486706e-05,
      "loss": 1.2188,
      "step": 261
    },
    {
      "epoch": 0.005732351686339719,
      "grad_norm": 1.9410783052444458,
      "learning_rate": 9.71443949206304e-05,
      "loss": 1.2588,
      "step": 262
    },
    {
      "epoch": 0.005754230891249412,
      "grad_norm": 1.9070533514022827,
      "learning_rate": 9.651005032974994e-05,
      "loss": 1.0091,
      "step": 263
    },
    {
      "epoch": 0.005776110096159106,
      "grad_norm": 2.174211263656616,
      "learning_rate": 9.587584631353329e-05,
      "loss": 1.3081,
      "step": 264
    },
    {
      "epoch": 0.005797989301068799,
      "grad_norm": 1.8562030792236328,
      "learning_rate": 9.524180841762577e-05,
      "loss": 1.1745,
      "step": 265
    },
    {
      "epoch": 0.005819868505978492,
      "grad_norm": 1.9879131317138672,
      "learning_rate": 9.460796218098143e-05,
      "loss": 1.1408,
      "step": 266
    },
    {
      "epoch": 0.005841747710888186,
      "grad_norm": 2.0576367378234863,
      "learning_rate": 9.397433313483416e-05,
      "loss": 1.2917,
      "step": 267
    },
    {
      "epoch": 0.00586362691579788,
      "grad_norm": 1.7986656427383423,
      "learning_rate": 9.334094680166962e-05,
      "loss": 0.9851,
      "step": 268
    },
    {
      "epoch": 0.005885506120707574,
      "grad_norm": 2.6142239570617676,
      "learning_rate": 9.270782869419694e-05,
      "loss": 0.9755,
      "step": 269
    },
    {
      "epoch": 0.005907385325617267,
      "grad_norm": 2.655205488204956,
      "learning_rate": 9.207500431432115e-05,
      "loss": 1.0464,
      "step": 270
    },
    {
      "epoch": 0.005929264530526961,
      "grad_norm": 1.937177300453186,
      "learning_rate": 9.144249915211605e-05,
      "loss": 0.9535,
      "step": 271
    },
    {
      "epoch": 0.005951143735436654,
      "grad_norm": 2.0596463680267334,
      "learning_rate": 9.081033868479727e-05,
      "loss": 1.1053,
      "step": 272
    },
    {
      "epoch": 0.005973022940346348,
      "grad_norm": 2.7733404636383057,
      "learning_rate": 9.01785483756963e-05,
      "loss": 1.1704,
      "step": 273
    },
    {
      "epoch": 0.0059949021452560414,
      "grad_norm": 2.398871421813965,
      "learning_rate": 8.954715367323468e-05,
      "loss": 1.0349,
      "step": 274
    },
    {
      "epoch": 0.006016781350165735,
      "grad_norm": 2.280219078063965,
      "learning_rate": 8.891618000989891e-05,
      "loss": 0.871,
      "step": 275
    },
    {
      "epoch": 0.006016781350165735,
      "eval_loss": 1.0035606622695923,
      "eval_runtime": 2047.4712,
      "eval_samples_per_second": 0.409,
      "eval_steps_per_second": 0.409,
      "step": 275
    },
    {
      "epoch": 0.006038660555075429,
      "grad_norm": 2.44294810295105,
      "learning_rate": 8.828565280121617e-05,
      "loss": 0.9217,
      "step": 276
    },
    {
      "epoch": 0.006060539759985122,
      "grad_norm": 2.2066566944122314,
      "learning_rate": 8.765559744473053e-05,
      "loss": 0.8132,
      "step": 277
    },
    {
      "epoch": 0.006082418964894816,
      "grad_norm": 2.2377102375030518,
      "learning_rate": 8.702603931897982e-05,
      "loss": 0.6959,
      "step": 278
    },
    {
      "epoch": 0.006104298169804509,
      "grad_norm": 2.273106575012207,
      "learning_rate": 8.639700378247361e-05,
      "loss": 0.9064,
      "step": 279
    },
    {
      "epoch": 0.006126177374714203,
      "grad_norm": 2.9397037029266357,
      "learning_rate": 8.57685161726715e-05,
      "loss": 1.0536,
      "step": 280
    },
    {
      "epoch": 0.0061480565796238965,
      "grad_norm": 2.8179335594177246,
      "learning_rate": 8.514060180496285e-05,
      "loss": 0.9664,
      "step": 281
    },
    {
      "epoch": 0.00616993578453359,
      "grad_norm": 2.504295587539673,
      "learning_rate": 8.451328597164679e-05,
      "loss": 0.8033,
      "step": 282
    },
    {
      "epoch": 0.006191814989443284,
      "grad_norm": 2.0903098583221436,
      "learning_rate": 8.38865939409136e-05,
      "loss": 0.8285,
      "step": 283
    },
    {
      "epoch": 0.006213694194352977,
      "grad_norm": 2.4072699546813965,
      "learning_rate": 8.326055095582694e-05,
      "loss": 0.7928,
      "step": 284
    },
    {
      "epoch": 0.006235573399262671,
      "grad_norm": 2.590024948120117,
      "learning_rate": 8.263518223330697e-05,
      "loss": 0.6305,
      "step": 285
    },
    {
      "epoch": 0.006257452604172364,
      "grad_norm": 3.521517276763916,
      "learning_rate": 8.201051296311462e-05,
      "loss": 0.6634,
      "step": 286
    },
    {
      "epoch": 0.006279331809082058,
      "grad_norm": 2.358081340789795,
      "learning_rate": 8.1386568306837e-05,
      "loss": 0.6834,
      "step": 287
    },
    {
      "epoch": 0.0063012110139917515,
      "grad_norm": 1.9242448806762695,
      "learning_rate": 8.076337339687394e-05,
      "loss": 0.4992,
      "step": 288
    },
    {
      "epoch": 0.0063230902189014455,
      "grad_norm": 2.6089489459991455,
      "learning_rate": 8.014095333542548e-05,
      "loss": 0.5506,
      "step": 289
    },
    {
      "epoch": 0.006344969423811139,
      "grad_norm": 1.8825725317001343,
      "learning_rate": 7.951933319348095e-05,
      "loss": 0.4337,
      "step": 290
    },
    {
      "epoch": 0.006366848628720832,
      "grad_norm": 2.347763776779175,
      "learning_rate": 7.889853800980904e-05,
      "loss": 0.603,
      "step": 291
    },
    {
      "epoch": 0.006388727833630526,
      "grad_norm": 2.0081677436828613,
      "learning_rate": 7.827859278994925e-05,
      "loss": 0.4539,
      "step": 292
    },
    {
      "epoch": 0.006410607038540219,
      "grad_norm": 2.2024800777435303,
      "learning_rate": 7.765952250520459e-05,
      "loss": 0.4806,
      "step": 293
    },
    {
      "epoch": 0.006432486243449913,
      "grad_norm": 2.358978271484375,
      "learning_rate": 7.704135209163589e-05,
      "loss": 0.5088,
      "step": 294
    },
    {
      "epoch": 0.0064543654483596065,
      "grad_norm": 2.57924747467041,
      "learning_rate": 7.642410644905726e-05,
      "loss": 0.4691,
      "step": 295
    },
    {
      "epoch": 0.0064762446532693006,
      "grad_norm": 2.8522422313690186,
      "learning_rate": 7.580781044003324e-05,
      "loss": 0.376,
      "step": 296
    },
    {
      "epoch": 0.006498123858178994,
      "grad_norm": 2.757258415222168,
      "learning_rate": 7.519248888887716e-05,
      "loss": 0.4888,
      "step": 297
    },
    {
      "epoch": 0.006520003063088687,
      "grad_norm": 3.2115137577056885,
      "learning_rate": 7.457816658065134e-05,
      "loss": 0.4889,
      "step": 298
    },
    {
      "epoch": 0.006541882267998381,
      "grad_norm": 3.043586492538452,
      "learning_rate": 7.39648682601688e-05,
      "loss": 0.5196,
      "step": 299
    },
    {
      "epoch": 0.006563761472908074,
      "grad_norm": 2.7563977241516113,
      "learning_rate": 7.335261863099651e-05,
      "loss": 0.3927,
      "step": 300
    },
    {
      "epoch": 0.006563761472908074,
      "eval_loss": 1.005658507347107,
      "eval_runtime": 2525.9934,
      "eval_samples_per_second": 0.331,
      "eval_steps_per_second": 0.331,
      "step": 300
    },
    {
      "epoch": 0.006585640677817768,
      "grad_norm": 1.656569242477417,
      "learning_rate": 7.274144235446023e-05,
      "loss": 1.6816,
      "step": 301
    },
    {
      "epoch": 0.0066075198827274615,
      "grad_norm": 1.5884077548980713,
      "learning_rate": 7.213136404865124e-05,
      "loss": 1.4448,
      "step": 302
    },
    {
      "epoch": 0.006629399087637156,
      "grad_norm": 1.4064958095550537,
      "learning_rate": 7.152240828743477e-05,
      "loss": 1.2638,
      "step": 303
    },
    {
      "epoch": 0.006651278292546849,
      "grad_norm": 1.8525580167770386,
      "learning_rate": 7.09145995994601e-05,
      "loss": 1.3085,
      "step": 304
    },
    {
      "epoch": 0.006673157497456543,
      "grad_norm": 1.757436752319336,
      "learning_rate": 7.030796246717255e-05,
      "loss": 1.2665,
      "step": 305
    },
    {
      "epoch": 0.006695036702366236,
      "grad_norm": 2.228156089782715,
      "learning_rate": 6.970252132582728e-05,
      "loss": 1.254,
      "step": 306
    },
    {
      "epoch": 0.006716915907275929,
      "grad_norm": 2.1743247509002686,
      "learning_rate": 6.909830056250527e-05,
      "loss": 1.4102,
      "step": 307
    },
    {
      "epoch": 0.006738795112185623,
      "grad_norm": 1.8606452941894531,
      "learning_rate": 6.849532451513074e-05,
      "loss": 1.2786,
      "step": 308
    },
    {
      "epoch": 0.0067606743170953165,
      "grad_norm": 1.845127820968628,
      "learning_rate": 6.789361747149093e-05,
      "loss": 1.3093,
      "step": 309
    },
    {
      "epoch": 0.006782553522005011,
      "grad_norm": 2.000415802001953,
      "learning_rate": 6.729320366825784e-05,
      "loss": 1.1157,
      "step": 310
    },
    {
      "epoch": 0.006804432726914704,
      "grad_norm": 2.126692533493042,
      "learning_rate": 6.669410729001193e-05,
      "loss": 1.3458,
      "step": 311
    },
    {
      "epoch": 0.006826311931824398,
      "grad_norm": 2.0188801288604736,
      "learning_rate": 6.609635246826794e-05,
      "loss": 1.2723,
      "step": 312
    },
    {
      "epoch": 0.006848191136734091,
      "grad_norm": 2.6371796131134033,
      "learning_rate": 6.549996328050296e-05,
      "loss": 1.4314,
      "step": 313
    },
    {
      "epoch": 0.006870070341643784,
      "grad_norm": 1.9445394277572632,
      "learning_rate": 6.490496374918647e-05,
      "loss": 1.0299,
      "step": 314
    },
    {
      "epoch": 0.006891949546553478,
      "grad_norm": 1.7599563598632812,
      "learning_rate": 6.431137784081282e-05,
      "loss": 1.1659,
      "step": 315
    },
    {
      "epoch": 0.0069138287514631715,
      "grad_norm": 2.203550338745117,
      "learning_rate": 6.371922946493591e-05,
      "loss": 1.0712,
      "step": 316
    },
    {
      "epoch": 0.006935707956372866,
      "grad_norm": 2.017775297164917,
      "learning_rate": 6.312854247320595e-05,
      "loss": 0.9529,
      "step": 317
    },
    {
      "epoch": 0.006957587161282559,
      "grad_norm": 2.2350380420684814,
      "learning_rate": 6.25393406584088e-05,
      "loss": 1.0303,
      "step": 318
    },
    {
      "epoch": 0.006979466366192253,
      "grad_norm": 2.0651214122772217,
      "learning_rate": 6.19516477535077e-05,
      "loss": 0.9416,
      "step": 319
    },
    {
      "epoch": 0.007001345571101946,
      "grad_norm": 1.8475967645645142,
      "learning_rate": 6.136548743068713e-05,
      "loss": 0.9698,
      "step": 320
    },
    {
      "epoch": 0.00702322477601164,
      "grad_norm": 4.0742034912109375,
      "learning_rate": 6.078088330039945e-05,
      "loss": 1.111,
      "step": 321
    },
    {
      "epoch": 0.007045103980921333,
      "grad_norm": 2.1027302742004395,
      "learning_rate": 6.019785891041381e-05,
      "loss": 0.8477,
      "step": 322
    },
    {
      "epoch": 0.0070669831858310266,
      "grad_norm": 1.884430170059204,
      "learning_rate": 5.9616437744867535e-05,
      "loss": 1.0797,
      "step": 323
    },
    {
      "epoch": 0.007088862390740721,
      "grad_norm": 1.9785232543945312,
      "learning_rate": 5.9036643223320475e-05,
      "loss": 0.8002,
      "step": 324
    },
    {
      "epoch": 0.007110741595650414,
      "grad_norm": 3.312437057495117,
      "learning_rate": 5.845849869981137e-05,
      "loss": 1.0582,
      "step": 325
    },
    {
      "epoch": 0.007110741595650414,
      "eval_loss": 1.000714898109436,
      "eval_runtime": 3362.7948,
      "eval_samples_per_second": 0.249,
      "eval_steps_per_second": 0.249,
      "step": 325
    },
    {
      "epoch": 0.007132620800560108,
      "grad_norm": 2.622080087661743,
      "learning_rate": 5.788202746191734e-05,
      "loss": 0.9659,
      "step": 326
    },
    {
      "epoch": 0.007154500005469801,
      "grad_norm": 1.8870490789413452,
      "learning_rate": 5.7307252729815833e-05,
      "loss": 0.787,
      "step": 327
    },
    {
      "epoch": 0.007176379210379495,
      "grad_norm": 2.179854393005371,
      "learning_rate": 5.6734197655349156e-05,
      "loss": 0.9381,
      "step": 328
    },
    {
      "epoch": 0.007198258415289188,
      "grad_norm": 2.577857732772827,
      "learning_rate": 5.616288532109225e-05,
      "loss": 0.8738,
      "step": 329
    },
    {
      "epoch": 0.007220137620198882,
      "grad_norm": 3.2158851623535156,
      "learning_rate": 5.559333873942259e-05,
      "loss": 0.8038,
      "step": 330
    },
    {
      "epoch": 0.007242016825108576,
      "grad_norm": 2.0847840309143066,
      "learning_rate": 5.5025580851593436e-05,
      "loss": 0.8016,
      "step": 331
    },
    {
      "epoch": 0.007263896030018269,
      "grad_norm": 2.1622791290283203,
      "learning_rate": 5.445963452680973e-05,
      "loss": 0.7461,
      "step": 332
    },
    {
      "epoch": 0.007285775234927963,
      "grad_norm": 3.2952165603637695,
      "learning_rate": 5.38955225613069e-05,
      "loss": 0.972,
      "step": 333
    },
    {
      "epoch": 0.007307654439837656,
      "grad_norm": 2.87129807472229,
      "learning_rate": 5.333326767743263e-05,
      "loss": 0.8553,
      "step": 334
    },
    {
      "epoch": 0.00732953364474735,
      "grad_norm": 2.8267598152160645,
      "learning_rate": 5.277289252273174e-05,
      "loss": 0.7859,
      "step": 335
    },
    {
      "epoch": 0.007351412849657043,
      "grad_norm": 2.714458465576172,
      "learning_rate": 5.221441966903371e-05,
      "loss": 0.6944,
      "step": 336
    },
    {
      "epoch": 0.0073732920545667375,
      "grad_norm": 2.2087454795837402,
      "learning_rate": 5.1657871611543605e-05,
      "loss": 0.6193,
      "step": 337
    },
    {
      "epoch": 0.007395171259476431,
      "grad_norm": 2.9056169986724854,
      "learning_rate": 5.110327076793613e-05,
      "loss": 0.6752,
      "step": 338
    },
    {
      "epoch": 0.007417050464386124,
      "grad_norm": 3.0631539821624756,
      "learning_rate": 5.055063947745233e-05,
      "loss": 0.7054,
      "step": 339
    },
    {
      "epoch": 0.007438929669295818,
      "grad_norm": 2.9411771297454834,
      "learning_rate": 5.000000000000002e-05,
      "loss": 0.5425,
      "step": 340
    },
    {
      "epoch": 0.007460808874205511,
      "grad_norm": 2.0313539505004883,
      "learning_rate": 4.945137451525707e-05,
      "loss": 0.5528,
      "step": 341
    },
    {
      "epoch": 0.007482688079115205,
      "grad_norm": 2.156536102294922,
      "learning_rate": 4.890478512177795e-05,
      "loss": 0.4655,
      "step": 342
    },
    {
      "epoch": 0.007504567284024898,
      "grad_norm": 2.141179084777832,
      "learning_rate": 4.836025383610382e-05,
      "loss": 0.4006,
      "step": 343
    },
    {
      "epoch": 0.0075264464889345925,
      "grad_norm": 2.1046013832092285,
      "learning_rate": 4.7817802591875426e-05,
      "loss": 0.4368,
      "step": 344
    },
    {
      "epoch": 0.007548325693844286,
      "grad_norm": 2.7510762214660645,
      "learning_rate": 4.727745323894976e-05,
      "loss": 0.5201,
      "step": 345
    },
    {
      "epoch": 0.007570204898753979,
      "grad_norm": 2.6333107948303223,
      "learning_rate": 4.673922754252002e-05,
      "loss": 0.3972,
      "step": 346
    },
    {
      "epoch": 0.007592084103663673,
      "grad_norm": 1.883894443511963,
      "learning_rate": 4.620314718223876e-05,
      "loss": 0.3792,
      "step": 347
    },
    {
      "epoch": 0.007613963308573366,
      "grad_norm": 2.832202911376953,
      "learning_rate": 4.566923375134472e-05,
      "loss": 0.4722,
      "step": 348
    },
    {
      "epoch": 0.00763584251348306,
      "grad_norm": 2.0017240047454834,
      "learning_rate": 4.513750875579303e-05,
      "loss": 0.3901,
      "step": 349
    },
    {
      "epoch": 0.007657721718392753,
      "grad_norm": 3.123565196990967,
      "learning_rate": 4.4607993613388976e-05,
      "loss": 0.6031,
      "step": 350
    },
    {
      "epoch": 0.007657721718392753,
      "eval_loss": 0.999909520149231,
      "eval_runtime": 2048.1144,
      "eval_samples_per_second": 0.409,
      "eval_steps_per_second": 0.409,
      "step": 350
    },
    {
      "epoch": 0.0076796009233024475,
      "grad_norm": 1.442865252494812,
      "learning_rate": 4.4080709652925336e-05,
      "loss": 1.5591,
      "step": 351
    },
    {
      "epoch": 0.007701480128212141,
      "grad_norm": 1.7502624988555908,
      "learning_rate": 4.355567811332311e-05,
      "loss": 1.6198,
      "step": 352
    },
    {
      "epoch": 0.007723359333121835,
      "grad_norm": 1.3770827054977417,
      "learning_rate": 4.3032920142776125e-05,
      "loss": 1.4065,
      "step": 353
    },
    {
      "epoch": 0.007745238538031528,
      "grad_norm": 2.177210569381714,
      "learning_rate": 4.251245679789928e-05,
      "loss": 1.7435,
      "step": 354
    },
    {
      "epoch": 0.007767117742941221,
      "grad_norm": 1.8911025524139404,
      "learning_rate": 4.19943090428802e-05,
      "loss": 1.2972,
      "step": 355
    },
    {
      "epoch": 0.007788996947850915,
      "grad_norm": 1.7135956287384033,
      "learning_rate": 4.147849774863488e-05,
      "loss": 1.3061,
      "step": 356
    },
    {
      "epoch": 0.007810876152760608,
      "grad_norm": 1.8210240602493286,
      "learning_rate": 4.096504369196704e-05,
      "loss": 1.4118,
      "step": 357
    },
    {
      "epoch": 0.007832755357670302,
      "grad_norm": 1.5980275869369507,
      "learning_rate": 4.045396755473121e-05,
      "loss": 1.1914,
      "step": 358
    },
    {
      "epoch": 0.007854634562579996,
      "grad_norm": 1.7343734502792358,
      "learning_rate": 3.994528992299971e-05,
      "loss": 1.1117,
      "step": 359
    },
    {
      "epoch": 0.00787651376748969,
      "grad_norm": 2.1653451919555664,
      "learning_rate": 3.943903128623335e-05,
      "loss": 1.2628,
      "step": 360
    },
    {
      "epoch": 0.007898392972399384,
      "grad_norm": 2.0278310775756836,
      "learning_rate": 3.893521203645618e-05,
      "loss": 1.2383,
      "step": 361
    },
    {
      "epoch": 0.007920272177309076,
      "grad_norm": 1.7438740730285645,
      "learning_rate": 3.843385246743417e-05,
      "loss": 1.0902,
      "step": 362
    },
    {
      "epoch": 0.00794215138221877,
      "grad_norm": 1.986527681350708,
      "learning_rate": 3.7934972773857634e-05,
      "loss": 1.2483,
      "step": 363
    },
    {
      "epoch": 0.007964030587128464,
      "grad_norm": 2.0457987785339355,
      "learning_rate": 3.7438593050527845e-05,
      "loss": 1.0614,
      "step": 364
    },
    {
      "epoch": 0.007985909792038157,
      "grad_norm": 2.0793516635894775,
      "learning_rate": 3.694473329154778e-05,
      "loss": 1.2387,
      "step": 365
    },
    {
      "epoch": 0.00800778899694785,
      "grad_norm": 2.307730197906494,
      "learning_rate": 3.645341338951639e-05,
      "loss": 1.0251,
      "step": 366
    },
    {
      "epoch": 0.008029668201857545,
      "grad_norm": 1.9568785429000854,
      "learning_rate": 3.5964653134727776e-05,
      "loss": 1.113,
      "step": 367
    },
    {
      "epoch": 0.008051547406767239,
      "grad_norm": 2.4474165439605713,
      "learning_rate": 3.547847221437372e-05,
      "loss": 1.0298,
      "step": 368
    },
    {
      "epoch": 0.008073426611676931,
      "grad_norm": 2.1192216873168945,
      "learning_rate": 3.4994890211750754e-05,
      "loss": 1.0451,
      "step": 369
    },
    {
      "epoch": 0.008095305816586625,
      "grad_norm": 2.3250648975372314,
      "learning_rate": 3.45139266054715e-05,
      "loss": 1.1636,
      "step": 370
    },
    {
      "epoch": 0.00811718502149632,
      "grad_norm": 2.4348390102386475,
      "learning_rate": 3.4035600768679855e-05,
      "loss": 1.0394,
      "step": 371
    },
    {
      "epoch": 0.008139064226406012,
      "grad_norm": 2.457401752471924,
      "learning_rate": 3.3559931968270753e-05,
      "loss": 0.9672,
      "step": 372
    },
    {
      "epoch": 0.008160943431315706,
      "grad_norm": 2.284877061843872,
      "learning_rate": 3.308693936411421e-05,
      "loss": 1.1054,
      "step": 373
    },
    {
      "epoch": 0.0081828226362254,
      "grad_norm": 2.184863805770874,
      "learning_rate": 3.2616642008283213e-05,
      "loss": 1.004,
      "step": 374
    },
    {
      "epoch": 0.008204701841135094,
      "grad_norm": 2.3890492916107178,
      "learning_rate": 3.21490588442868e-05,
      "loss": 1.0933,
      "step": 375
    },
    {
      "epoch": 0.008204701841135094,
      "eval_loss": 1.0015242099761963,
      "eval_runtime": 2109.22,
      "eval_samples_per_second": 0.397,
      "eval_steps_per_second": 0.397,
      "step": 375
    },
    {
      "epoch": 0.008226581046044786,
      "grad_norm": 2.025757312774658,
      "learning_rate": 3.1684208706306574e-05,
      "loss": 0.7833,
      "step": 376
    },
    {
      "epoch": 0.00824846025095448,
      "grad_norm": 2.5050532817840576,
      "learning_rate": 3.122211031843831e-05,
      "loss": 1.1401,
      "step": 377
    },
    {
      "epoch": 0.008270339455864174,
      "grad_norm": 2.594789743423462,
      "learning_rate": 3.076278229393773e-05,
      "loss": 0.9521,
      "step": 378
    },
    {
      "epoch": 0.008292218660773867,
      "grad_norm": 1.9983664751052856,
      "learning_rate": 3.030624313447067e-05,
      "loss": 1.0422,
      "step": 379
    },
    {
      "epoch": 0.00831409786568356,
      "grad_norm": 2.1198644638061523,
      "learning_rate": 2.9852511229367865e-05,
      "loss": 0.9687,
      "step": 380
    },
    {
      "epoch": 0.008335977070593255,
      "grad_norm": 2.347486734390259,
      "learning_rate": 2.9401604854884357e-05,
      "loss": 0.92,
      "step": 381
    },
    {
      "epoch": 0.008357856275502949,
      "grad_norm": 2.2873458862304688,
      "learning_rate": 2.8953542173463133e-05,
      "loss": 0.9743,
      "step": 382
    },
    {
      "epoch": 0.008379735480412641,
      "grad_norm": 2.797316789627075,
      "learning_rate": 2.8508341233003654e-05,
      "loss": 0.8656,
      "step": 383
    },
    {
      "epoch": 0.008401614685322335,
      "grad_norm": 1.8931509256362915,
      "learning_rate": 2.8066019966134904e-05,
      "loss": 0.734,
      "step": 384
    },
    {
      "epoch": 0.00842349389023203,
      "grad_norm": 2.014939308166504,
      "learning_rate": 2.7626596189492983e-05,
      "loss": 0.5928,
      "step": 385
    },
    {
      "epoch": 0.008445373095141722,
      "grad_norm": 2.454859733581543,
      "learning_rate": 2.719008760300359e-05,
      "loss": 0.7503,
      "step": 386
    },
    {
      "epoch": 0.008467252300051416,
      "grad_norm": 2.3540098667144775,
      "learning_rate": 2.6756511789168915e-05,
      "loss": 0.7318,
      "step": 387
    },
    {
      "epoch": 0.00848913150496111,
      "grad_norm": 2.9678258895874023,
      "learning_rate": 2.6325886212359498e-05,
      "loss": 0.7094,
      "step": 388
    },
    {
      "epoch": 0.008511010709870804,
      "grad_norm": 2.215604543685913,
      "learning_rate": 2.589822821811083e-05,
      "loss": 0.6393,
      "step": 389
    },
    {
      "epoch": 0.008532889914780496,
      "grad_norm": 2.7122485637664795,
      "learning_rate": 2.5473555032424533e-05,
      "loss": 0.6139,
      "step": 390
    },
    {
      "epoch": 0.00855476911969019,
      "grad_norm": 2.1079201698303223,
      "learning_rate": 2.5051883761074614e-05,
      "loss": 0.5918,
      "step": 391
    },
    {
      "epoch": 0.008576648324599884,
      "grad_norm": 2.3411448001861572,
      "learning_rate": 2.4633231388918378e-05,
      "loss": 0.4963,
      "step": 392
    },
    {
      "epoch": 0.008598527529509578,
      "grad_norm": 2.444171667098999,
      "learning_rate": 2.4217614779212315e-05,
      "loss": 0.416,
      "step": 393
    },
    {
      "epoch": 0.00862040673441927,
      "grad_norm": 2.0639753341674805,
      "learning_rate": 2.3805050672932928e-05,
      "loss": 0.5142,
      "step": 394
    },
    {
      "epoch": 0.008642285939328965,
      "grad_norm": 2.118741035461426,
      "learning_rate": 2.339555568810221e-05,
      "loss": 0.4379,
      "step": 395
    },
    {
      "epoch": 0.008664165144238659,
      "grad_norm": 2.2282183170318604,
      "learning_rate": 2.2989146319118425e-05,
      "loss": 0.3962,
      "step": 396
    },
    {
      "epoch": 0.008686044349148351,
      "grad_norm": 2.029334306716919,
      "learning_rate": 2.2585838936091754e-05,
      "loss": 0.5348,
      "step": 397
    },
    {
      "epoch": 0.008707923554058045,
      "grad_norm": 2.447338581085205,
      "learning_rate": 2.2185649784184746e-05,
      "loss": 0.4434,
      "step": 398
    },
    {
      "epoch": 0.00872980275896774,
      "grad_norm": 2.1002566814422607,
      "learning_rate": 2.178859498295809e-05,
      "loss": 0.3932,
      "step": 399
    },
    {
      "epoch": 0.008751681963877433,
      "grad_norm": 1.7954540252685547,
      "learning_rate": 2.139469052572127e-05,
      "loss": 0.4493,
      "step": 400
    },
    {
      "epoch": 0.008751681963877433,
      "eval_loss": 1.0000619888305664,
      "eval_runtime": 2481.8506,
      "eval_samples_per_second": 0.337,
      "eval_steps_per_second": 0.337,
      "step": 400
    },
    {
      "epoch": 0.008773561168787126,
      "grad_norm": 1.685921311378479,
      "learning_rate": 2.1003952278888382e-05,
      "loss": 1.6747,
      "step": 401
    },
    {
      "epoch": 0.00879544037369682,
      "grad_norm": 1.6280176639556885,
      "learning_rate": 2.0616395981339075e-05,
      "loss": 1.3384,
      "step": 402
    },
    {
      "epoch": 0.008817319578606514,
      "grad_norm": 1.853163242340088,
      "learning_rate": 2.0232037243784475e-05,
      "loss": 1.1665,
      "step": 403
    },
    {
      "epoch": 0.008839198783516206,
      "grad_norm": 1.5039806365966797,
      "learning_rate": 1.985089154813846e-05,
      "loss": 1.2022,
      "step": 404
    },
    {
      "epoch": 0.0088610779884259,
      "grad_norm": 2.0080740451812744,
      "learning_rate": 1.947297424689414e-05,
      "loss": 1.2822,
      "step": 405
    },
    {
      "epoch": 0.008882957193335594,
      "grad_norm": 2.1490914821624756,
      "learning_rate": 1.9098300562505266e-05,
      "loss": 1.2904,
      "step": 406
    },
    {
      "epoch": 0.008904836398245288,
      "grad_norm": 1.790208339691162,
      "learning_rate": 1.8726885586773212e-05,
      "loss": 1.2424,
      "step": 407
    },
    {
      "epoch": 0.00892671560315498,
      "grad_norm": 2.2298481464385986,
      "learning_rate": 1.835874428023905e-05,
      "loss": 1.3951,
      "step": 408
    },
    {
      "epoch": 0.008948594808064675,
      "grad_norm": 2.098416328430176,
      "learning_rate": 1.7993891471580893e-05,
      "loss": 1.2828,
      "step": 409
    },
    {
      "epoch": 0.008970474012974369,
      "grad_norm": 1.8565698862075806,
      "learning_rate": 1.763234185701673e-05,
      "loss": 1.1504,
      "step": 410
    },
    {
      "epoch": 0.008992353217884061,
      "grad_norm": 1.9450656175613403,
      "learning_rate": 1.7274109999712295e-05,
      "loss": 1.0994,
      "step": 411
    },
    {
      "epoch": 0.009014232422793755,
      "grad_norm": 1.8794082403182983,
      "learning_rate": 1.6919210329194533e-05,
      "loss": 1.1825,
      "step": 412
    },
    {
      "epoch": 0.00903611162770345,
      "grad_norm": 1.8893122673034668,
      "learning_rate": 1.6567657140770475e-05,
      "loss": 1.0804,
      "step": 413
    },
    {
      "epoch": 0.009057990832613144,
      "grad_norm": 2.4556703567504883,
      "learning_rate": 1.621946459495127e-05,
      "loss": 1.0699,
      "step": 414
    },
    {
      "epoch": 0.009079870037522836,
      "grad_norm": 1.9377635717391968,
      "learning_rate": 1.587464671688187e-05,
      "loss": 1.0118,
      "step": 415
    },
    {
      "epoch": 0.00910174924243253,
      "grad_norm": 1.9297547340393066,
      "learning_rate": 1.553321739577619e-05,
      "loss": 1.1294,
      "step": 416
    },
    {
      "epoch": 0.009123628447342224,
      "grad_norm": 2.0228824615478516,
      "learning_rate": 1.5195190384357404e-05,
      "loss": 0.9834,
      "step": 417
    },
    {
      "epoch": 0.009145507652251916,
      "grad_norm": 2.2386703491210938,
      "learning_rate": 1.4860579298304312e-05,
      "loss": 1.1385,
      "step": 418
    },
    {
      "epoch": 0.00916738685716161,
      "grad_norm": 2.046353340148926,
      "learning_rate": 1.4529397615702656e-05,
      "loss": 0.9959,
      "step": 419
    },
    {
      "epoch": 0.009189266062071304,
      "grad_norm": 2.2949371337890625,
      "learning_rate": 1.4201658676502294e-05,
      "loss": 1.0872,
      "step": 420
    },
    {
      "epoch": 0.009211145266980999,
      "grad_norm": 2.040724039077759,
      "learning_rate": 1.3877375681979943e-05,
      "loss": 1.1492,
      "step": 421
    },
    {
      "epoch": 0.00923302447189069,
      "grad_norm": 2.0195488929748535,
      "learning_rate": 1.3556561694207338e-05,
      "loss": 0.9367,
      "step": 422
    },
    {
      "epoch": 0.009254903676800385,
      "grad_norm": 2.098896026611328,
      "learning_rate": 1.3239229635525074e-05,
      "loss": 1.0662,
      "step": 423
    },
    {
      "epoch": 0.009276782881710079,
      "grad_norm": 2.326597213745117,
      "learning_rate": 1.2925392288022298e-05,
      "loss": 0.9764,
      "step": 424
    },
    {
      "epoch": 0.009298662086619773,
      "grad_norm": 2.102693796157837,
      "learning_rate": 1.2615062293021507e-05,
      "loss": 0.9465,
      "step": 425
    },
    {
      "epoch": 0.009298662086619773,
      "eval_loss": 0.998818576335907,
      "eval_runtime": 2314.5235,
      "eval_samples_per_second": 0.362,
      "eval_steps_per_second": 0.362,
      "step": 425
    },
    {
      "epoch": 0.009320541291529465,
      "grad_norm": 2.5758793354034424,
      "learning_rate": 1.230825215056971e-05,
      "loss": 0.9958,
      "step": 426
    },
    {
      "epoch": 0.00934242049643916,
      "grad_norm": 1.941099762916565,
      "learning_rate": 1.2004974218934695e-05,
      "loss": 0.9218,
      "step": 427
    },
    {
      "epoch": 0.009364299701348854,
      "grad_norm": 2.7641751766204834,
      "learning_rate": 1.1705240714107302e-05,
      "loss": 1.0143,
      "step": 428
    },
    {
      "epoch": 0.009386178906258546,
      "grad_norm": 2.4476540088653564,
      "learning_rate": 1.1409063709309442e-05,
      "loss": 0.9924,
      "step": 429
    },
    {
      "epoch": 0.00940805811116824,
      "grad_norm": 2.8888657093048096,
      "learning_rate": 1.1116455134507664e-05,
      "loss": 0.8245,
      "step": 430
    },
    {
      "epoch": 0.009429937316077934,
      "grad_norm": 2.477165460586548,
      "learning_rate": 1.082742677593267e-05,
      "loss": 1.0374,
      "step": 431
    },
    {
      "epoch": 0.009451816520987628,
      "grad_norm": 2.4441776275634766,
      "learning_rate": 1.054199027560463e-05,
      "loss": 0.7228,
      "step": 432
    },
    {
      "epoch": 0.00947369572589732,
      "grad_norm": 2.3560984134674072,
      "learning_rate": 1.026015713086418e-05,
      "loss": 0.6514,
      "step": 433
    },
    {
      "epoch": 0.009495574930807014,
      "grad_norm": 2.343258857727051,
      "learning_rate": 9.98193869390922e-06,
      "loss": 0.808,
      "step": 434
    },
    {
      "epoch": 0.009517454135716709,
      "grad_norm": 2.3667995929718018,
      "learning_rate": 9.707346171337894e-06,
      "loss": 0.7945,
      "step": 435
    },
    {
      "epoch": 0.009539333340626401,
      "grad_norm": 2.8864121437072754,
      "learning_rate": 9.436390623696911e-06,
      "loss": 0.8246,
      "step": 436
    },
    {
      "epoch": 0.009561212545536095,
      "grad_norm": 2.563746213912964,
      "learning_rate": 9.16908296503628e-06,
      "loss": 0.6783,
      "step": 437
    },
    {
      "epoch": 0.009583091750445789,
      "grad_norm": 1.9059044122695923,
      "learning_rate": 8.905433962469489e-06,
      "loss": 0.5461,
      "step": 438
    },
    {
      "epoch": 0.009604970955355483,
      "grad_norm": 2.4938528537750244,
      "learning_rate": 8.645454235739903e-06,
      "loss": 0.6735,
      "step": 439
    },
    {
      "epoch": 0.009626850160265175,
      "grad_norm": 2.2794852256774902,
      "learning_rate": 8.38915425679304e-06,
      "loss": 0.6688,
      "step": 440
    },
    {
      "epoch": 0.00964872936517487,
      "grad_norm": 2.3621878623962402,
      "learning_rate": 8.13654434935467e-06,
      "loss": 0.4574,
      "step": 441
    },
    {
      "epoch": 0.009670608570084564,
      "grad_norm": 2.5767862796783447,
      "learning_rate": 7.887634688515e-06,
      "loss": 0.5478,
      "step": 442
    },
    {
      "epoch": 0.009692487774994256,
      "grad_norm": 2.564307689666748,
      "learning_rate": 7.642435300318907e-06,
      "loss": 0.5514,
      "step": 443
    },
    {
      "epoch": 0.00971436697990395,
      "grad_norm": 2.390059471130371,
      "learning_rate": 7.400956061361974e-06,
      "loss": 0.4515,
      "step": 444
    },
    {
      "epoch": 0.009736246184813644,
      "grad_norm": 2.0385568141937256,
      "learning_rate": 7.163206698392744e-06,
      "loss": 0.3894,
      "step": 445
    },
    {
      "epoch": 0.009758125389723338,
      "grad_norm": 2.9452784061431885,
      "learning_rate": 6.929196787920899e-06,
      "loss": 0.632,
      "step": 446
    },
    {
      "epoch": 0.00978000459463303,
      "grad_norm": 1.9925888776779175,
      "learning_rate": 6.698935755831492e-06,
      "loss": 0.5057,
      "step": 447
    },
    {
      "epoch": 0.009801883799542725,
      "grad_norm": 3.627941131591797,
      "learning_rate": 6.472432877005341e-06,
      "loss": 0.417,
      "step": 448
    },
    {
      "epoch": 0.009823763004452419,
      "grad_norm": 2.857243061065674,
      "learning_rate": 6.2496972749453766e-06,
      "loss": 0.405,
      "step": 449
    },
    {
      "epoch": 0.009845642209362111,
      "grad_norm": 2.6988790035247803,
      "learning_rate": 6.030737921409169e-06,
      "loss": 0.5196,
      "step": 450
    },
    {
      "epoch": 0.009845642209362111,
      "eval_loss": 0.9986802935600281,
      "eval_runtime": 2057.8641,
      "eval_samples_per_second": 0.407,
      "eval_steps_per_second": 0.407,
      "step": 450
    }
  ],
  "logging_steps": 1,
  "max_steps": 500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.9502259443712e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
