{
  "best_metric": 1.0056504011154175,
  "best_model_checkpoint": "Finetuning/Fine-tuned_checkpoint/2/medical_3\\checkpoint-250",
  "epoch": 0.005469801227423395,
  "eval_steps": 25,
  "global_step": 250,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 2.187920490969358e-05,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.1489,
      "step": 1
    },
    {
      "epoch": 4.375840981938716e-05,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.2514,
      "step": 2
    },
    {
      "epoch": 6.563761472908074e-05,
      "grad_norm": 3.2709290981292725,
      "learning_rate": 4e-05,
      "loss": 2.3104,
      "step": 3
    },
    {
      "epoch": 8.751681963877433e-05,
      "grad_norm": 2.779268741607666,
      "learning_rate": 8e-05,
      "loss": 2.1807,
      "step": 4
    },
    {
      "epoch": 0.00010939602454846791,
      "grad_norm": 3.500542163848877,
      "learning_rate": 0.00012,
      "loss": 1.9871,
      "step": 5
    },
    {
      "epoch": 0.0001312752294581615,
      "grad_norm": NaN,
      "learning_rate": 0.00012,
      "loss": 2.0898,
      "step": 6
    },
    {
      "epoch": 0.00015315443436785507,
      "grad_norm": NaN,
      "learning_rate": 0.00012,
      "loss": 1.8829,
      "step": 7
    },
    {
      "epoch": 0.00017503363927754865,
      "grad_norm": NaN,
      "learning_rate": 0.00012,
      "loss": 2.1304,
      "step": 8
    },
    {
      "epoch": 0.00019691284418724223,
      "grad_norm": 27.946533203125,
      "learning_rate": 0.00016,
      "loss": 2.0801,
      "step": 9
    },
    {
      "epoch": 0.00021879204909693582,
      "grad_norm": 16.3977108001709,
      "learning_rate": 0.0002,
      "loss": 2.2192,
      "step": 10
    },
    {
      "epoch": 0.0002406712540066294,
      "grad_norm": 12.878182411193848,
      "learning_rate": 0.00019999798600729064,
      "loss": 2.2176,
      "step": 11
    },
    {
      "epoch": 0.000262550458916323,
      "grad_norm": 9.413625717163086,
      "learning_rate": 0.00019999194411028594,
      "loss": 2.1922,
      "step": 12
    },
    {
      "epoch": 0.00028442966382601656,
      "grad_norm": 9.80106258392334,
      "learning_rate": 0.0001999818745523526,
      "loss": 1.672,
      "step": 13
    },
    {
      "epoch": 0.00030630886873571014,
      "grad_norm": 20.085037231445312,
      "learning_rate": 0.00019996777773909093,
      "loss": 1.8428,
      "step": 14
    },
    {
      "epoch": 0.0003281880736454037,
      "grad_norm": 28.03141212463379,
      "learning_rate": 0.00019994965423831854,
      "loss": 1.8474,
      "step": 15
    },
    {
      "epoch": 0.0003500672785550973,
      "grad_norm": 4.915910243988037,
      "learning_rate": 0.00019992750478004738,
      "loss": 1.9661,
      "step": 16
    },
    {
      "epoch": 0.0003719464834647909,
      "grad_norm": 3.8566091060638428,
      "learning_rate": 0.0001999013302564544,
      "loss": 1.8644,
      "step": 17
    },
    {
      "epoch": 0.00039382568837448447,
      "grad_norm": 4.295337200164795,
      "learning_rate": 0.00019987113172184563,
      "loss": 1.6651,
      "step": 18
    },
    {
      "epoch": 0.00041570489328417805,
      "grad_norm": 5.446649551391602,
      "learning_rate": 0.00019983691039261357,
      "loss": 1.5854,
      "step": 19
    },
    {
      "epoch": 0.00043758409819387163,
      "grad_norm": 5.8035173416137695,
      "learning_rate": 0.00019979866764718843,
      "loss": 1.9175,
      "step": 20
    },
    {
      "epoch": 0.0004594633031035652,
      "grad_norm": 5.114415645599365,
      "learning_rate": 0.00019975640502598244,
      "loss": 1.299,
      "step": 21
    },
    {
      "epoch": 0.0004813425080132588,
      "grad_norm": 5.278352737426758,
      "learning_rate": 0.00019971012423132775,
      "loss": 1.191,
      "step": 22
    },
    {
      "epoch": 0.0005032217129229524,
      "grad_norm": 4.566121578216553,
      "learning_rate": 0.00019965982712740808,
      "loss": 1.4321,
      "step": 23
    },
    {
      "epoch": 0.000525100917832646,
      "grad_norm": 5.711122512817383,
      "learning_rate": 0.0001996055157401834,
      "loss": 1.1899,
      "step": 24
    },
    {
      "epoch": 0.0005469801227423396,
      "grad_norm": 5.113018035888672,
      "learning_rate": 0.00019954719225730847,
      "loss": 1.2518,
      "step": 25
    },
    {
      "epoch": 0.0005469801227423396,
      "eval_loss": 1.1922972202301025,
      "eval_runtime": 1914.1511,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 25
    },
    {
      "epoch": 0.0005688593276520331,
      "grad_norm": 4.588993549346924,
      "learning_rate": 0.0001994848590280447,
      "loss": 1.0657,
      "step": 26
    },
    {
      "epoch": 0.0005907385325617268,
      "grad_norm": 4.489006042480469,
      "learning_rate": 0.00019941851856316548,
      "loss": 1.2094,
      "step": 27
    },
    {
      "epoch": 0.0006126177374714203,
      "grad_norm": 5.219137191772461,
      "learning_rate": 0.00019934817353485501,
      "loss": 1.0861,
      "step": 28
    },
    {
      "epoch": 0.0006344969423811139,
      "grad_norm": 4.2921271324157715,
      "learning_rate": 0.00019927382677660088,
      "loss": 1.122,
      "step": 29
    },
    {
      "epoch": 0.0006563761472908074,
      "grad_norm": 4.807187557220459,
      "learning_rate": 0.00019919548128307954,
      "loss": 1.1372,
      "step": 30
    },
    {
      "epoch": 0.0006782553522005011,
      "grad_norm": 4.058358669281006,
      "learning_rate": 0.00019911314021003613,
      "loss": 0.8487,
      "step": 31
    },
    {
      "epoch": 0.0007001345571101946,
      "grad_norm": 3.881542444229126,
      "learning_rate": 0.00019902680687415705,
      "loss": 1.149,
      "step": 32
    },
    {
      "epoch": 0.0007220137620198882,
      "grad_norm": 4.037757873535156,
      "learning_rate": 0.00019893648475293648,
      "loss": 1.0259,
      "step": 33
    },
    {
      "epoch": 0.0007438929669295818,
      "grad_norm": 4.164768218994141,
      "learning_rate": 0.00019884217748453623,
      "loss": 1.0579,
      "step": 34
    },
    {
      "epoch": 0.0007657721718392754,
      "grad_norm": 3.8350396156311035,
      "learning_rate": 0.00019874388886763944,
      "loss": 0.9897,
      "step": 35
    },
    {
      "epoch": 0.0007876513767489689,
      "grad_norm": 3.524501085281372,
      "learning_rate": 0.0001986416228612972,
      "loss": 0.8777,
      "step": 36
    },
    {
      "epoch": 0.0008095305816586626,
      "grad_norm": 4.389486789703369,
      "learning_rate": 0.00019853538358476932,
      "loss": 1.0028,
      "step": 37
    },
    {
      "epoch": 0.0008314097865683561,
      "grad_norm": 5.2984418869018555,
      "learning_rate": 0.00019842517531735838,
      "loss": 0.8882,
      "step": 38
    },
    {
      "epoch": 0.0008532889914780497,
      "grad_norm": 4.486255645751953,
      "learning_rate": 0.00019831100249823733,
      "loss": 0.9041,
      "step": 39
    },
    {
      "epoch": 0.0008751681963877433,
      "grad_norm": 3.5563745498657227,
      "learning_rate": 0.00019819286972627066,
      "loss": 0.7836,
      "step": 40
    },
    {
      "epoch": 0.0008970474012974369,
      "grad_norm": 3.732107400894165,
      "learning_rate": 0.00019807078175982924,
      "loss": 0.8701,
      "step": 41
    },
    {
      "epoch": 0.0009189266062071304,
      "grad_norm": 4.825470924377441,
      "learning_rate": 0.00019794474351659852,
      "loss": 0.6458,
      "step": 42
    },
    {
      "epoch": 0.0009408058111168241,
      "grad_norm": 4.546532154083252,
      "learning_rate": 0.00019781476007338058,
      "loss": 0.5407,
      "step": 43
    },
    {
      "epoch": 0.0009626850160265176,
      "grad_norm": 3.5209569931030273,
      "learning_rate": 0.00019768083666588953,
      "loss": 0.4474,
      "step": 44
    },
    {
      "epoch": 0.0009845642209362112,
      "grad_norm": 2.735433578491211,
      "learning_rate": 0.00019754297868854073,
      "loss": 0.4174,
      "step": 45
    },
    {
      "epoch": 0.0010064434258459049,
      "grad_norm": 4.130967617034912,
      "learning_rate": 0.00019740119169423337,
      "loss": 0.5045,
      "step": 46
    },
    {
      "epoch": 0.0010283226307555983,
      "grad_norm": 3.451334238052368,
      "learning_rate": 0.00019725548139412692,
      "loss": 0.4736,
      "step": 47
    },
    {
      "epoch": 0.001050201835665292,
      "grad_norm": 3.0298266410827637,
      "learning_rate": 0.00019710585365741103,
      "loss": 0.4676,
      "step": 48
    },
    {
      "epoch": 0.0010720810405749855,
      "grad_norm": 4.125247955322266,
      "learning_rate": 0.00019695231451106912,
      "loss": 0.3847,
      "step": 49
    },
    {
      "epoch": 0.0010939602454846792,
      "grad_norm": 4.14768123626709,
      "learning_rate": 0.00019679487013963564,
      "loss": 0.4756,
      "step": 50
    },
    {
      "epoch": 0.0010939602454846792,
      "eval_loss": 1.0479252338409424,
      "eval_runtime": 2244.0112,
      "eval_samples_per_second": 0.373,
      "eval_steps_per_second": 0.373,
      "step": 50
    },
    {
      "epoch": 0.0011158394503943726,
      "grad_norm": 2.4684815406799316,
      "learning_rate": 0.00019663352688494684,
      "loss": 1.3828,
      "step": 51
    },
    {
      "epoch": 0.0011377186553040662,
      "grad_norm": 2.835707187652588,
      "learning_rate": 0.0001964682912458856,
      "loss": 1.2505,
      "step": 52
    },
    {
      "epoch": 0.0011595978602137599,
      "grad_norm": 3.27203106880188,
      "learning_rate": 0.00019629916987811926,
      "loss": 1.7888,
      "step": 53
    },
    {
      "epoch": 0.0011814770651234535,
      "grad_norm": 3.473505973815918,
      "learning_rate": 0.0001961261695938319,
      "loss": 1.3551,
      "step": 54
    },
    {
      "epoch": 0.001203356270033147,
      "grad_norm": 2.1574692726135254,
      "learning_rate": 0.00019594929736144976,
      "loss": 1.4639,
      "step": 55
    },
    {
      "epoch": 0.0012252354749428406,
      "grad_norm": 2.8092637062072754,
      "learning_rate": 0.00019576856030536054,
      "loss": 1.546,
      "step": 56
    },
    {
      "epoch": 0.0012471146798525342,
      "grad_norm": 3.09495210647583,
      "learning_rate": 0.0001955839657056265,
      "loss": 1.4857,
      "step": 57
    },
    {
      "epoch": 0.0012689938847622278,
      "grad_norm": 2.1254076957702637,
      "learning_rate": 0.00019539552099769126,
      "loss": 1.4344,
      "step": 58
    },
    {
      "epoch": 0.0012908730896719213,
      "grad_norm": 2.1704788208007812,
      "learning_rate": 0.00019520323377208017,
      "loss": 1.3843,
      "step": 59
    },
    {
      "epoch": 0.0013127522945816149,
      "grad_norm": 1.9811135530471802,
      "learning_rate": 0.00019500711177409454,
      "loss": 1.2643,
      "step": 60
    },
    {
      "epoch": 0.0013346314994913085,
      "grad_norm": 2.241314649581909,
      "learning_rate": 0.00019480716290349995,
      "loss": 1.4117,
      "step": 61
    },
    {
      "epoch": 0.0013565107044010022,
      "grad_norm": 2.4024972915649414,
      "learning_rate": 0.00019460339521420772,
      "loss": 1.2316,
      "step": 62
    },
    {
      "epoch": 0.0013783899093106956,
      "grad_norm": 2.691696882247925,
      "learning_rate": 0.00019439581691395067,
      "loss": 1.2826,
      "step": 63
    },
    {
      "epoch": 0.0014002691142203892,
      "grad_norm": 2.3235437870025635,
      "learning_rate": 0.00019418443636395248,
      "loss": 1.2188,
      "step": 64
    },
    {
      "epoch": 0.0014221483191300829,
      "grad_norm": 2.047386646270752,
      "learning_rate": 0.00019396926207859084,
      "loss": 1.5531,
      "step": 65
    },
    {
      "epoch": 0.0014440275240397765,
      "grad_norm": 2.65726900100708,
      "learning_rate": 0.00019375030272505463,
      "loss": 1.0757,
      "step": 66
    },
    {
      "epoch": 0.00146590672894947,
      "grad_norm": 2.378598928451538,
      "learning_rate": 0.00019352756712299468,
      "loss": 1.1644,
      "step": 67
    },
    {
      "epoch": 0.0014877859338591635,
      "grad_norm": 2.46109938621521,
      "learning_rate": 0.00019330106424416852,
      "loss": 1.2066,
      "step": 68
    },
    {
      "epoch": 0.0015096651387688572,
      "grad_norm": 2.7630972862243652,
      "learning_rate": 0.00019307080321207912,
      "loss": 1.2875,
      "step": 69
    },
    {
      "epoch": 0.0015315443436785508,
      "grad_norm": 2.292219638824463,
      "learning_rate": 0.00019283679330160726,
      "loss": 1.128,
      "step": 70
    },
    {
      "epoch": 0.0015534235485882442,
      "grad_norm": 2.4014084339141846,
      "learning_rate": 0.00019259904393863802,
      "loss": 1.1713,
      "step": 71
    },
    {
      "epoch": 0.0015753027534979379,
      "grad_norm": 2.270020008087158,
      "learning_rate": 0.0001923575646996811,
      "loss": 1.1165,
      "step": 72
    },
    {
      "epoch": 0.0015971819584076315,
      "grad_norm": 2.997513771057129,
      "learning_rate": 0.000192112365311485,
      "loss": 1.1613,
      "step": 73
    },
    {
      "epoch": 0.0016190611633173251,
      "grad_norm": 2.949875831604004,
      "learning_rate": 0.00019186345565064535,
      "loss": 1.2551,
      "step": 74
    },
    {
      "epoch": 0.0016409403682270186,
      "grad_norm": 2.4989330768585205,
      "learning_rate": 0.00019161084574320696,
      "loss": 1.1665,
      "step": 75
    },
    {
      "epoch": 0.0016409403682270186,
      "eval_loss": 1.0211350917816162,
      "eval_runtime": 2292.2752,
      "eval_samples_per_second": 0.365,
      "eval_steps_per_second": 0.365,
      "step": 75
    },
    {
      "epoch": 0.0016628195731367122,
      "grad_norm": 2.6845543384552,
      "learning_rate": 0.0001913545457642601,
      "loss": 1.1165,
      "step": 76
    },
    {
      "epoch": 0.0016846987780464058,
      "grad_norm": 2.6079254150390625,
      "learning_rate": 0.0001910945660375305,
      "loss": 1.1681,
      "step": 77
    },
    {
      "epoch": 0.0017065779829560995,
      "grad_norm": 2.4246103763580322,
      "learning_rate": 0.0001908309170349637,
      "loss": 1.0465,
      "step": 78
    },
    {
      "epoch": 0.0017284571878657929,
      "grad_norm": 2.8889334201812744,
      "learning_rate": 0.0001905636093763031,
      "loss": 0.9547,
      "step": 79
    },
    {
      "epoch": 0.0017503363927754865,
      "grad_norm": 3.004519462585449,
      "learning_rate": 0.00019029265382866214,
      "loss": 1.0678,
      "step": 80
    },
    {
      "epoch": 0.0017722155976851802,
      "grad_norm": 3.3404242992401123,
      "learning_rate": 0.0001900180613060908,
      "loss": 1.0148,
      "step": 81
    },
    {
      "epoch": 0.0017940948025948738,
      "grad_norm": 2.6319735050201416,
      "learning_rate": 0.00018973984286913584,
      "loss": 0.9487,
      "step": 82
    },
    {
      "epoch": 0.0018159740075045672,
      "grad_norm": 2.560392141342163,
      "learning_rate": 0.00018945800972439538,
      "loss": 0.9066,
      "step": 83
    },
    {
      "epoch": 0.0018378532124142608,
      "grad_norm": 2.362605571746826,
      "learning_rate": 0.00018917257322406734,
      "loss": 0.8231,
      "step": 84
    },
    {
      "epoch": 0.0018597324173239545,
      "grad_norm": 3.0136783123016357,
      "learning_rate": 0.00018888354486549237,
      "loss": 0.9301,
      "step": 85
    },
    {
      "epoch": 0.0018816116222336481,
      "grad_norm": 3.0647382736206055,
      "learning_rate": 0.00018859093629069058,
      "loss": 0.833,
      "step": 86
    },
    {
      "epoch": 0.0019034908271433415,
      "grad_norm": 2.915889263153076,
      "learning_rate": 0.00018829475928589271,
      "loss": 0.7995,
      "step": 87
    },
    {
      "epoch": 0.0019253700320530352,
      "grad_norm": 3.743215322494507,
      "learning_rate": 0.00018799502578106534,
      "loss": 0.6381,
      "step": 88
    },
    {
      "epoch": 0.0019472492369627288,
      "grad_norm": 3.00455641746521,
      "learning_rate": 0.0001876917478494303,
      "loss": 0.7331,
      "step": 89
    },
    {
      "epoch": 0.0019691284418724224,
      "grad_norm": 2.8673009872436523,
      "learning_rate": 0.00018738493770697852,
      "loss": 0.7309,
      "step": 90
    },
    {
      "epoch": 0.001991007646782116,
      "grad_norm": 3.5453310012817383,
      "learning_rate": 0.00018707460771197774,
      "loss": 0.6712,
      "step": 91
    },
    {
      "epoch": 0.0020128868516918097,
      "grad_norm": 3.176609754562378,
      "learning_rate": 0.00018676077036447494,
      "loss": 0.7036,
      "step": 92
    },
    {
      "epoch": 0.002034766056601503,
      "grad_norm": 2.8091795444488525,
      "learning_rate": 0.0001864434383057927,
      "loss": 0.5001,
      "step": 93
    },
    {
      "epoch": 0.0020566452615111966,
      "grad_norm": 2.55698561668396,
      "learning_rate": 0.00018612262431802007,
      "loss": 0.5897,
      "step": 94
    },
    {
      "epoch": 0.00207852446642089,
      "grad_norm": 2.899730682373047,
      "learning_rate": 0.00018579834132349772,
      "loss": 0.4938,
      "step": 95
    },
    {
      "epoch": 0.002100403671330584,
      "grad_norm": 3.249739170074463,
      "learning_rate": 0.00018547060238429736,
      "loss": 0.3385,
      "step": 96
    },
    {
      "epoch": 0.0021222828762402775,
      "grad_norm": 3.0253703594207764,
      "learning_rate": 0.0001851394207016957,
      "loss": 0.5308,
      "step": 97
    },
    {
      "epoch": 0.002144162081149971,
      "grad_norm": 3.144101858139038,
      "learning_rate": 0.0001848048096156426,
      "loss": 0.4482,
      "step": 98
    },
    {
      "epoch": 0.0021660412860596647,
      "grad_norm": 4.101083755493164,
      "learning_rate": 0.00018446678260422385,
      "loss": 0.6164,
      "step": 99
    },
    {
      "epoch": 0.0021879204909693584,
      "grad_norm": 3.0741498470306396,
      "learning_rate": 0.00018412535328311814,
      "loss": 0.4914,
      "step": 100
    },
    {
      "epoch": 0.0021879204909693584,
      "eval_loss": 1.0295677185058594,
      "eval_runtime": 2384.2426,
      "eval_samples_per_second": 0.351,
      "eval_steps_per_second": 0.351,
      "step": 100
    },
    {
      "epoch": 0.0022097996958790516,
      "grad_norm": 2.047893524169922,
      "learning_rate": 0.00018378053540504873,
      "loss": 1.5513,
      "step": 101
    },
    {
      "epoch": 0.002231678900788745,
      "grad_norm": 2.003300666809082,
      "learning_rate": 0.00018343234285922953,
      "loss": 1.373,
      "step": 102
    },
    {
      "epoch": 0.002253558105698439,
      "grad_norm": 2.0060677528381348,
      "learning_rate": 0.00018308078967080546,
      "loss": 1.4835,
      "step": 103
    },
    {
      "epoch": 0.0022754373106081325,
      "grad_norm": 1.9966665506362915,
      "learning_rate": 0.00018272589000028772,
      "loss": 1.5559,
      "step": 104
    },
    {
      "epoch": 0.002297316515517826,
      "grad_norm": 2.153258800506592,
      "learning_rate": 0.0001823676581429833,
      "loss": 1.4547,
      "step": 105
    },
    {
      "epoch": 0.0023191957204275197,
      "grad_norm": 1.9949474334716797,
      "learning_rate": 0.00018200610852841913,
      "loss": 1.5239,
      "step": 106
    },
    {
      "epoch": 0.0023410749253372134,
      "grad_norm": 1.9800158739089966,
      "learning_rate": 0.00018164125571976098,
      "loss": 1.4176,
      "step": 107
    },
    {
      "epoch": 0.002362954130246907,
      "grad_norm": 1.7942959070205688,
      "learning_rate": 0.0001812731144132268,
      "loss": 1.3128,
      "step": 108
    },
    {
      "epoch": 0.0023848333351566002,
      "grad_norm": 2.085221767425537,
      "learning_rate": 0.00018090169943749476,
      "loss": 1.3837,
      "step": 109
    },
    {
      "epoch": 0.002406712540066294,
      "grad_norm": 2.0194644927978516,
      "learning_rate": 0.00018052702575310588,
      "loss": 1.2042,
      "step": 110
    },
    {
      "epoch": 0.0024285917449759875,
      "grad_norm": 1.767561435699463,
      "learning_rate": 0.00018014910845186153,
      "loss": 1.4426,
      "step": 111
    },
    {
      "epoch": 0.002450470949885681,
      "grad_norm": 2.0302302837371826,
      "learning_rate": 0.00017976796275621555,
      "loss": 1.1742,
      "step": 112
    },
    {
      "epoch": 0.0024723501547953748,
      "grad_norm": 2.2055411338806152,
      "learning_rate": 0.00017938360401866093,
      "loss": 1.278,
      "step": 113
    },
    {
      "epoch": 0.0024942293597050684,
      "grad_norm": 2.5468649864196777,
      "learning_rate": 0.00017899604772111163,
      "loss": 1.3092,
      "step": 114
    },
    {
      "epoch": 0.002516108564614762,
      "grad_norm": 2.0420081615448,
      "learning_rate": 0.00017860530947427875,
      "loss": 1.1001,
      "step": 115
    },
    {
      "epoch": 0.0025379877695244557,
      "grad_norm": 1.9514058828353882,
      "learning_rate": 0.00017821140501704194,
      "loss": 1.2121,
      "step": 116
    },
    {
      "epoch": 0.002559866974434149,
      "grad_norm": 2.1732685565948486,
      "learning_rate": 0.00017781435021581527,
      "loss": 1.1049,
      "step": 117
    },
    {
      "epoch": 0.0025817461793438425,
      "grad_norm": 2.1666769981384277,
      "learning_rate": 0.00017741416106390826,
      "loss": 1.0371,
      "step": 118
    },
    {
      "epoch": 0.002603625384253536,
      "grad_norm": 2.6952311992645264,
      "learning_rate": 0.00017701085368088156,
      "loss": 1.1353,
      "step": 119
    },
    {
      "epoch": 0.0026255045891632298,
      "grad_norm": 1.9632771015167236,
      "learning_rate": 0.0001766044443118978,
      "loss": 0.9534,
      "step": 120
    },
    {
      "epoch": 0.0026473837940729234,
      "grad_norm": 2.292632579803467,
      "learning_rate": 0.0001761949493270671,
      "loss": 1.0353,
      "step": 121
    },
    {
      "epoch": 0.002669262998982617,
      "grad_norm": 2.140517234802246,
      "learning_rate": 0.0001757823852207877,
      "loss": 1.0749,
      "step": 122
    },
    {
      "epoch": 0.0026911422038923107,
      "grad_norm": 3.025057554244995,
      "learning_rate": 0.00017536676861108164,
      "loss": 1.2436,
      "step": 123
    },
    {
      "epoch": 0.0027130214088020043,
      "grad_norm": 2.670936107635498,
      "learning_rate": 0.0001749481162389254,
      "loss": 1.0804,
      "step": 124
    },
    {
      "epoch": 0.0027349006137116975,
      "grad_norm": 1.8179781436920166,
      "learning_rate": 0.00017452644496757547,
      "loss": 1.0845,
      "step": 125
    },
    {
      "epoch": 0.0027349006137116975,
      "eval_loss": 1.0170316696166992,
      "eval_runtime": 2345.7791,
      "eval_samples_per_second": 0.357,
      "eval_steps_per_second": 0.357,
      "step": 125
    },
    {
      "epoch": 0.002756779818621391,
      "grad_norm": 3.1400606632232666,
      "learning_rate": 0.00017410177178188918,
      "loss": 1.2351,
      "step": 126
    },
    {
      "epoch": 0.002778659023531085,
      "grad_norm": 2.2635045051574707,
      "learning_rate": 0.0001736741137876405,
      "loss": 0.9059,
      "step": 127
    },
    {
      "epoch": 0.0028005382284407784,
      "grad_norm": 2.3612606525421143,
      "learning_rate": 0.0001732434882108311,
      "loss": 0.8582,
      "step": 128
    },
    {
      "epoch": 0.002822417433350472,
      "grad_norm": 2.61826229095459,
      "learning_rate": 0.00017280991239699642,
      "loss": 0.9309,
      "step": 129
    },
    {
      "epoch": 0.0028442966382601657,
      "grad_norm": 2.4500739574432373,
      "learning_rate": 0.00017237340381050703,
      "loss": 0.8815,
      "step": 130
    },
    {
      "epoch": 0.0028661758431698593,
      "grad_norm": 2.6421356201171875,
      "learning_rate": 0.0001719339800338651,
      "loss": 0.8395,
      "step": 131
    },
    {
      "epoch": 0.002888055048079553,
      "grad_norm": 2.155247449874878,
      "learning_rate": 0.00017149165876699635,
      "loss": 0.8199,
      "step": 132
    },
    {
      "epoch": 0.002909934252989246,
      "grad_norm": 1.975793480873108,
      "learning_rate": 0.0001710464578265369,
      "loss": 0.7158,
      "step": 133
    },
    {
      "epoch": 0.00293181345789894,
      "grad_norm": 2.2563371658325195,
      "learning_rate": 0.00017059839514511565,
      "loss": 0.7996,
      "step": 134
    },
    {
      "epoch": 0.0029536926628086334,
      "grad_norm": 2.2849698066711426,
      "learning_rate": 0.00017014748877063214,
      "loss": 0.6947,
      "step": 135
    },
    {
      "epoch": 0.002975571867718327,
      "grad_norm": 3.1464266777038574,
      "learning_rate": 0.00016969375686552935,
      "loss": 0.8948,
      "step": 136
    },
    {
      "epoch": 0.0029974510726280207,
      "grad_norm": 2.3216614723205566,
      "learning_rate": 0.00016923721770606228,
      "loss": 0.6492,
      "step": 137
    },
    {
      "epoch": 0.0030193302775377144,
      "grad_norm": 2.4590539932250977,
      "learning_rate": 0.0001687778896815617,
      "loss": 0.6872,
      "step": 138
    },
    {
      "epoch": 0.003041209482447408,
      "grad_norm": 2.5943121910095215,
      "learning_rate": 0.00016831579129369346,
      "loss": 0.6785,
      "step": 139
    },
    {
      "epoch": 0.0030630886873571016,
      "grad_norm": 3.1107752323150635,
      "learning_rate": 0.00016785094115571322,
      "loss": 0.7062,
      "step": 140
    },
    {
      "epoch": 0.003084967892266795,
      "grad_norm": 2.4394242763519287,
      "learning_rate": 0.00016738335799171682,
      "loss": 0.5371,
      "step": 141
    },
    {
      "epoch": 0.0031068470971764885,
      "grad_norm": 3.405695915222168,
      "learning_rate": 0.00016691306063588583,
      "loss": 0.7447,
      "step": 142
    },
    {
      "epoch": 0.003128726302086182,
      "grad_norm": 2.860955238342285,
      "learning_rate": 0.00016644006803172924,
      "loss": 0.5884,
      "step": 143
    },
    {
      "epoch": 0.0031506055069958757,
      "grad_norm": 4.096427917480469,
      "learning_rate": 0.00016596439923132015,
      "loss": 0.7599,
      "step": 144
    },
    {
      "epoch": 0.0031724847119055694,
      "grad_norm": 2.120732545852661,
      "learning_rate": 0.00016548607339452853,
      "loss": 0.4696,
      "step": 145
    },
    {
      "epoch": 0.003194363916815263,
      "grad_norm": 2.5159990787506104,
      "learning_rate": 0.00016500510978824926,
      "loss": 0.5496,
      "step": 146
    },
    {
      "epoch": 0.0032162431217249566,
      "grad_norm": 2.4202635288238525,
      "learning_rate": 0.0001645215277856263,
      "loss": 0.5653,
      "step": 147
    },
    {
      "epoch": 0.0032381223266346503,
      "grad_norm": 3.990581750869751,
      "learning_rate": 0.00016403534686527225,
      "loss": 0.4697,
      "step": 148
    },
    {
      "epoch": 0.0032600015315443435,
      "grad_norm": 1.9455926418304443,
      "learning_rate": 0.00016354658661048364,
      "loss": 0.4656,
      "step": 149
    },
    {
      "epoch": 0.003281880736454037,
      "grad_norm": 2.953429937362671,
      "learning_rate": 0.00016305526670845226,
      "loss": 0.5323,
      "step": 150
    },
    {
      "epoch": 0.003281880736454037,
      "eval_loss": 1.0263699293136597,
      "eval_runtime": 2022.1062,
      "eval_samples_per_second": 0.414,
      "eval_steps_per_second": 0.414,
      "step": 150
    },
    {
      "epoch": 0.0033037599413637308,
      "grad_norm": 2.1393024921417236,
      "learning_rate": 0.00016256140694947214,
      "loss": 1.6442,
      "step": 151
    },
    {
      "epoch": 0.0033256391462734244,
      "grad_norm": 2.887155055999756,
      "learning_rate": 0.00016206502722614238,
      "loss": 1.3616,
      "step": 152
    },
    {
      "epoch": 0.003347518351183118,
      "grad_norm": 1.8749501705169678,
      "learning_rate": 0.0001615661475325658,
      "loss": 1.6493,
      "step": 153
    },
    {
      "epoch": 0.0033693975560928117,
      "grad_norm": 2.1255173683166504,
      "learning_rate": 0.00016106478796354382,
      "loss": 1.329,
      "step": 154
    },
    {
      "epoch": 0.0033912767610025053,
      "grad_norm": 2.4779014587402344,
      "learning_rate": 0.00016056096871376667,
      "loss": 1.4879,
      "step": 155
    },
    {
      "epoch": 0.003413155965912199,
      "grad_norm": 2.176313877105713,
      "learning_rate": 0.00016005471007700031,
      "loss": 1.2862,
      "step": 156
    },
    {
      "epoch": 0.003435035170821892,
      "grad_norm": 3.133812665939331,
      "learning_rate": 0.0001595460324452688,
      "loss": 1.2171,
      "step": 157
    },
    {
      "epoch": 0.0034569143757315858,
      "grad_norm": 1.80746591091156,
      "learning_rate": 0.000159034956308033,
      "loss": 1.3368,
      "step": 158
    },
    {
      "epoch": 0.0034787935806412794,
      "grad_norm": 2.0994222164154053,
      "learning_rate": 0.00015852150225136518,
      "loss": 1.3704,
      "step": 159
    },
    {
      "epoch": 0.003500672785550973,
      "grad_norm": 2.343292713165283,
      "learning_rate": 0.00015800569095711982,
      "loss": 1.399,
      "step": 160
    },
    {
      "epoch": 0.0035225519904606667,
      "grad_norm": 2.077725410461426,
      "learning_rate": 0.00015748754320210072,
      "loss": 1.2064,
      "step": 161
    },
    {
      "epoch": 0.0035444311953703603,
      "grad_norm": 1.805177092552185,
      "learning_rate": 0.0001569670798572239,
      "loss": 1.2201,
      "step": 162
    },
    {
      "epoch": 0.003566310400280054,
      "grad_norm": 1.9531844854354858,
      "learning_rate": 0.00015644432188667695,
      "loss": 1.2113,
      "step": 163
    },
    {
      "epoch": 0.0035881896051897476,
      "grad_norm": 1.6639947891235352,
      "learning_rate": 0.0001559192903470747,
      "loss": 1.3234,
      "step": 164
    },
    {
      "epoch": 0.003610068810099441,
      "grad_norm": 2.222898006439209,
      "learning_rate": 0.00015539200638661104,
      "loss": 1.064,
      "step": 165
    },
    {
      "epoch": 0.0036319480150091344,
      "grad_norm": 2.164314031600952,
      "learning_rate": 0.000154862491244207,
      "loss": 1.4272,
      "step": 166
    },
    {
      "epoch": 0.003653827219918828,
      "grad_norm": 3.3964216709136963,
      "learning_rate": 0.00015433076624865531,
      "loss": 1.2594,
      "step": 167
    },
    {
      "epoch": 0.0036757064248285217,
      "grad_norm": 1.6932060718536377,
      "learning_rate": 0.00015379685281776125,
      "loss": 0.9795,
      "step": 168
    },
    {
      "epoch": 0.0036975856297382153,
      "grad_norm": 2.466364622116089,
      "learning_rate": 0.00015326077245747999,
      "loss": 1.0687,
      "step": 169
    },
    {
      "epoch": 0.003719464834647909,
      "grad_norm": 2.3859012126922607,
      "learning_rate": 0.00015272254676105025,
      "loss": 1.2019,
      "step": 170
    },
    {
      "epoch": 0.0037413440395576026,
      "grad_norm": 2.3799631595611572,
      "learning_rate": 0.0001521821974081246,
      "loss": 1.3411,
      "step": 171
    },
    {
      "epoch": 0.0037632232444672962,
      "grad_norm": 2.5653069019317627,
      "learning_rate": 0.0001516397461638962,
      "loss": 1.0896,
      "step": 172
    },
    {
      "epoch": 0.0037851024493769894,
      "grad_norm": 2.5625128746032715,
      "learning_rate": 0.00015109521487822206,
      "loss": 1.1308,
      "step": 173
    },
    {
      "epoch": 0.003806981654286683,
      "grad_norm": 2.1904702186584473,
      "learning_rate": 0.000150548625484743,
      "loss": 0.8308,
      "step": 174
    },
    {
      "epoch": 0.0038288608591963767,
      "grad_norm": 2.1738123893737793,
      "learning_rate": 0.00015000000000000001,
      "loss": 0.9327,
      "step": 175
    },
    {
      "epoch": 0.0038288608591963767,
      "eval_loss": 1.0102177858352661,
      "eval_runtime": 2043.698,
      "eval_samples_per_second": 0.41,
      "eval_steps_per_second": 0.41,
      "step": 175
    },
    {
      "epoch": 0.0038507400641060703,
      "grad_norm": 2.1486189365386963,
      "learning_rate": 0.0001494493605225477,
      "loss": 0.8329,
      "step": 176
    },
    {
      "epoch": 0.003872619269015764,
      "grad_norm": 2.224435329437256,
      "learning_rate": 0.0001488967292320639,
      "loss": 1.1457,
      "step": 177
    },
    {
      "epoch": 0.0038944984739254576,
      "grad_norm": 2.3640012741088867,
      "learning_rate": 0.00014834212838845637,
      "loss": 0.9384,
      "step": 178
    },
    {
      "epoch": 0.003916377678835151,
      "grad_norm": 2.3616738319396973,
      "learning_rate": 0.00014778558033096633,
      "loss": 0.968,
      "step": 179
    },
    {
      "epoch": 0.003938256883744845,
      "grad_norm": 2.9090635776519775,
      "learning_rate": 0.0001472271074772683,
      "loss": 0.8382,
      "step": 180
    },
    {
      "epoch": 0.003960136088654538,
      "grad_norm": 2.6539201736450195,
      "learning_rate": 0.00014666673232256738,
      "loss": 0.7864,
      "step": 181
    },
    {
      "epoch": 0.003982015293564232,
      "grad_norm": 2.3968169689178467,
      "learning_rate": 0.00014610447743869314,
      "loss": 0.6355,
      "step": 182
    },
    {
      "epoch": 0.004003894498473925,
      "grad_norm": 2.634657382965088,
      "learning_rate": 0.0001455403654731903,
      "loss": 0.9017,
      "step": 183
    },
    {
      "epoch": 0.004025773703383619,
      "grad_norm": 2.9040074348449707,
      "learning_rate": 0.0001449744191484066,
      "loss": 0.8115,
      "step": 184
    },
    {
      "epoch": 0.004047652908293313,
      "grad_norm": 2.451035737991333,
      "learning_rate": 0.00014440666126057744,
      "loss": 0.7758,
      "step": 185
    },
    {
      "epoch": 0.004069532113203006,
      "grad_norm": 2.430938959121704,
      "learning_rate": 0.00014383711467890774,
      "loss": 0.8412,
      "step": 186
    },
    {
      "epoch": 0.0040914113181127,
      "grad_norm": 3.020045042037964,
      "learning_rate": 0.00014326580234465085,
      "loss": 0.7734,
      "step": 187
    },
    {
      "epoch": 0.004113290523022393,
      "grad_norm": 3.8700296878814697,
      "learning_rate": 0.0001426927472701842,
      "loss": 0.9098,
      "step": 188
    },
    {
      "epoch": 0.004135169727932087,
      "grad_norm": 1.986897587776184,
      "learning_rate": 0.00014211797253808268,
      "loss": 0.4828,
      "step": 189
    },
    {
      "epoch": 0.00415704893284178,
      "grad_norm": 2.6167354583740234,
      "learning_rate": 0.00014154150130018866,
      "loss": 0.4839,
      "step": 190
    },
    {
      "epoch": 0.0041789281377514744,
      "grad_norm": 2.4971776008605957,
      "learning_rate": 0.00014096335677667954,
      "loss": 0.5831,
      "step": 191
    },
    {
      "epoch": 0.004200807342661168,
      "grad_norm": 3.629246711730957,
      "learning_rate": 0.00014038356225513248,
      "loss": 0.6916,
      "step": 192
    },
    {
      "epoch": 0.004222686547570861,
      "grad_norm": 3.7974228858947754,
      "learning_rate": 0.00013980214108958624,
      "loss": 0.4834,
      "step": 193
    },
    {
      "epoch": 0.004244565752480555,
      "grad_norm": 3.1534810066223145,
      "learning_rate": 0.00013921911669960055,
      "loss": 0.5248,
      "step": 194
    },
    {
      "epoch": 0.004266444957390248,
      "grad_norm": 1.9292908906936646,
      "learning_rate": 0.00013863451256931287,
      "loss": 0.3412,
      "step": 195
    },
    {
      "epoch": 0.004288324162299942,
      "grad_norm": 2.455993413925171,
      "learning_rate": 0.0001380483522464923,
      "loss": 0.5754,
      "step": 196
    },
    {
      "epoch": 0.004310203367209635,
      "grad_norm": 3.127329111099243,
      "learning_rate": 0.00013746065934159123,
      "loss": 0.5141,
      "step": 197
    },
    {
      "epoch": 0.0043320825721193295,
      "grad_norm": 3.6233246326446533,
      "learning_rate": 0.0001368714575267941,
      "loss": 0.5456,
      "step": 198
    },
    {
      "epoch": 0.004353961777029023,
      "grad_norm": 2.5969443321228027,
      "learning_rate": 0.0001362807705350641,
      "loss": 0.4715,
      "step": 199
    },
    {
      "epoch": 0.004375840981938717,
      "grad_norm": 2.122073173522949,
      "learning_rate": 0.00013568862215918717,
      "loss": 0.4502,
      "step": 200
    },
    {
      "epoch": 0.004375840981938717,
      "eval_loss": 1.0119068622589111,
      "eval_runtime": 2060.4284,
      "eval_samples_per_second": 0.406,
      "eval_steps_per_second": 0.406,
      "step": 200
    },
    {
      "epoch": 0.00439772018684841,
      "grad_norm": 2.100381374359131,
      "learning_rate": 0.00013509503625081358,
      "loss": 1.6707,
      "step": 201
    },
    {
      "epoch": 0.004419599391758103,
      "grad_norm": 1.854717493057251,
      "learning_rate": 0.00013450003671949706,
      "loss": 1.5099,
      "step": 202
    },
    {
      "epoch": 0.004441478596667797,
      "grad_norm": 2.0012567043304443,
      "learning_rate": 0.00013390364753173206,
      "loss": 1.3675,
      "step": 203
    },
    {
      "epoch": 0.00446335780157749,
      "grad_norm": 1.7509772777557373,
      "learning_rate": 0.00013330589270998808,
      "loss": 1.2602,
      "step": 204
    },
    {
      "epoch": 0.0044852370064871845,
      "grad_norm": 2.069669008255005,
      "learning_rate": 0.00013270679633174218,
      "loss": 1.5031,
      "step": 205
    },
    {
      "epoch": 0.004507116211396878,
      "grad_norm": 1.851431131362915,
      "learning_rate": 0.00013210638252850908,
      "loss": 1.2242,
      "step": 206
    },
    {
      "epoch": 0.004528995416306572,
      "grad_norm": 2.854668378829956,
      "learning_rate": 0.0001315046754848693,
      "loss": 1.1869,
      "step": 207
    },
    {
      "epoch": 0.004550874621216265,
      "grad_norm": 1.7559220790863037,
      "learning_rate": 0.00013090169943749476,
      "loss": 1.0646,
      "step": 208
    },
    {
      "epoch": 0.004572753826125958,
      "grad_norm": 2.1576266288757324,
      "learning_rate": 0.00013029747867417276,
      "loss": 1.5068,
      "step": 209
    },
    {
      "epoch": 0.004594633031035652,
      "grad_norm": 1.8369613885879517,
      "learning_rate": 0.0001296920375328275,
      "loss": 1.3125,
      "step": 210
    },
    {
      "epoch": 0.004616512235945345,
      "grad_norm": 1.7737456560134888,
      "learning_rate": 0.0001290854004005399,
      "loss": 1.086,
      "step": 211
    },
    {
      "epoch": 0.0046383914408550395,
      "grad_norm": 2.1841211318969727,
      "learning_rate": 0.00012847759171256523,
      "loss": 1.2898,
      "step": 212
    },
    {
      "epoch": 0.004660270645764733,
      "grad_norm": 2.783679723739624,
      "learning_rate": 0.0001278686359513488,
      "loss": 1.2587,
      "step": 213
    },
    {
      "epoch": 0.004682149850674427,
      "grad_norm": 1.7687182426452637,
      "learning_rate": 0.0001272585576455398,
      "loss": 1.3263,
      "step": 214
    },
    {
      "epoch": 0.00470402905558412,
      "grad_norm": 1.9682635068893433,
      "learning_rate": 0.00012664738136900348,
      "loss": 1.1423,
      "step": 215
    },
    {
      "epoch": 0.004725908260493814,
      "grad_norm": 1.73717200756073,
      "learning_rate": 0.0001260351317398312,
      "loss": 1.1171,
      "step": 216
    },
    {
      "epoch": 0.004747787465403507,
      "grad_norm": 1.6289108991622925,
      "learning_rate": 0.00012542183341934872,
      "loss": 1.0802,
      "step": 217
    },
    {
      "epoch": 0.0047696666703132004,
      "grad_norm": 2.13883900642395,
      "learning_rate": 0.0001248075111111229,
      "loss": 0.9732,
      "step": 218
    },
    {
      "epoch": 0.0047915458752228945,
      "grad_norm": 2.2125933170318604,
      "learning_rate": 0.00012419218955996676,
      "loss": 0.9819,
      "step": 219
    },
    {
      "epoch": 0.004813425080132588,
      "grad_norm": 2.029482841491699,
      "learning_rate": 0.00012357589355094275,
      "loss": 1.1952,
      "step": 220
    },
    {
      "epoch": 0.004835304285042282,
      "grad_norm": 2.7746028900146484,
      "learning_rate": 0.0001229586479083641,
      "loss": 1.3261,
      "step": 221
    },
    {
      "epoch": 0.004857183489951975,
      "grad_norm": 2.0585696697235107,
      "learning_rate": 0.00012234047749479544,
      "loss": 0.9645,
      "step": 222
    },
    {
      "epoch": 0.004879062694861669,
      "grad_norm": 2.5084571838378906,
      "learning_rate": 0.00012172140721005079,
      "loss": 1.0772,
      "step": 223
    },
    {
      "epoch": 0.004900941899771362,
      "grad_norm": 1.847245216369629,
      "learning_rate": 0.000121101461990191,
      "loss": 1.0717,
      "step": 224
    },
    {
      "epoch": 0.0049228211046810555,
      "grad_norm": 2.0576205253601074,
      "learning_rate": 0.00012048066680651908,
      "loss": 0.7095,
      "step": 225
    },
    {
      "epoch": 0.0049228211046810555,
      "eval_loss": 1.0100913047790527,
      "eval_runtime": 2041.3087,
      "eval_samples_per_second": 0.41,
      "eval_steps_per_second": 0.41,
      "step": 225
    },
    {
      "epoch": 0.0049447003095907495,
      "grad_norm": 2.527238368988037,
      "learning_rate": 0.00011985904666457455,
      "loss": 1.0365,
      "step": 226
    },
    {
      "epoch": 0.004966579514500443,
      "grad_norm": 2.4983737468719482,
      "learning_rate": 0.00011923662660312611,
      "loss": 1.0625,
      "step": 227
    },
    {
      "epoch": 0.004988458719410137,
      "grad_norm": 2.5282070636749268,
      "learning_rate": 0.00011861343169316301,
      "loss": 0.8804,
      "step": 228
    },
    {
      "epoch": 0.00501033792431983,
      "grad_norm": 2.2000861167907715,
      "learning_rate": 0.00011798948703688539,
      "loss": 0.9255,
      "step": 229
    },
    {
      "epoch": 0.005032217129229524,
      "grad_norm": 2.0931503772735596,
      "learning_rate": 0.00011736481776669306,
      "loss": 0.947,
      "step": 230
    },
    {
      "epoch": 0.005054096334139217,
      "grad_norm": 2.193046808242798,
      "learning_rate": 0.00011673944904417308,
      "loss": 0.9635,
      "step": 231
    },
    {
      "epoch": 0.005075975539048911,
      "grad_norm": 2.5529487133026123,
      "learning_rate": 0.00011611340605908642,
      "loss": 1.0039,
      "step": 232
    },
    {
      "epoch": 0.0050978547439586045,
      "grad_norm": 2.6898205280303955,
      "learning_rate": 0.00011548671402835325,
      "loss": 0.7438,
      "step": 233
    },
    {
      "epoch": 0.005119733948868298,
      "grad_norm": 3.0761213302612305,
      "learning_rate": 0.00011485939819503717,
      "loss": 0.9677,
      "step": 234
    },
    {
      "epoch": 0.005141613153777992,
      "grad_norm": 2.2238330841064453,
      "learning_rate": 0.00011423148382732853,
      "loss": 0.7824,
      "step": 235
    },
    {
      "epoch": 0.005163492358687685,
      "grad_norm": 2.240037441253662,
      "learning_rate": 0.00011360299621752644,
      "loss": 0.6899,
      "step": 236
    },
    {
      "epoch": 0.005185371563597379,
      "grad_norm": 2.174140691757202,
      "learning_rate": 0.00011297396068102017,
      "loss": 0.7468,
      "step": 237
    },
    {
      "epoch": 0.005207250768507072,
      "grad_norm": 2.1373889446258545,
      "learning_rate": 0.00011234440255526948,
      "loss": 0.6043,
      "step": 238
    },
    {
      "epoch": 0.005229129973416766,
      "grad_norm": 2.327693462371826,
      "learning_rate": 0.00011171434719878384,
      "loss": 0.6554,
      "step": 239
    },
    {
      "epoch": 0.0052510091783264596,
      "grad_norm": 3.228789806365967,
      "learning_rate": 0.00011108381999010111,
      "loss": 0.6614,
      "step": 240
    },
    {
      "epoch": 0.005272888383236154,
      "grad_norm": 2.4611475467681885,
      "learning_rate": 0.00011045284632676536,
      "loss": 0.5144,
      "step": 241
    },
    {
      "epoch": 0.005294767588145847,
      "grad_norm": 3.708246946334839,
      "learning_rate": 0.00010982145162430373,
      "loss": 0.628,
      "step": 242
    },
    {
      "epoch": 0.00531664679305554,
      "grad_norm": 3.8016910552978516,
      "learning_rate": 0.00010918966131520277,
      "loss": 0.5706,
      "step": 243
    },
    {
      "epoch": 0.005338525997965234,
      "grad_norm": 2.656146764755249,
      "learning_rate": 0.00010855750084788398,
      "loss": 0.648,
      "step": 244
    },
    {
      "epoch": 0.005360405202874927,
      "grad_norm": 3.203629732131958,
      "learning_rate": 0.00010792499568567884,
      "loss": 0.6072,
      "step": 245
    },
    {
      "epoch": 0.005382284407784621,
      "grad_norm": 3.5270371437072754,
      "learning_rate": 0.0001072921713058031,
      "loss": 0.5934,
      "step": 246
    },
    {
      "epoch": 0.005404163612694315,
      "grad_norm": 3.237220287322998,
      "learning_rate": 0.00010665905319833041,
      "loss": 0.5463,
      "step": 247
    },
    {
      "epoch": 0.005426042817604009,
      "grad_norm": 3.6706695556640625,
      "learning_rate": 0.00010602566686516586,
      "loss": 0.4587,
      "step": 248
    },
    {
      "epoch": 0.005447922022513702,
      "grad_norm": 2.720128059387207,
      "learning_rate": 0.00010539203781901861,
      "loss": 0.4919,
      "step": 249
    },
    {
      "epoch": 0.005469801227423395,
      "grad_norm": 3.179784059524536,
      "learning_rate": 0.00010475819158237425,
      "loss": 0.587,
      "step": 250
    },
    {
      "epoch": 0.005469801227423395,
      "eval_loss": 1.0056504011154175,
      "eval_runtime": 2045.6789,
      "eval_samples_per_second": 0.409,
      "eval_steps_per_second": 0.409,
      "step": 250
    }
  ],
  "logging_steps": 1,
  "max_steps": 500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.108293971730432e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
