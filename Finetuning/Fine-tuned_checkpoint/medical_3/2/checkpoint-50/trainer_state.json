{
  "best_metric": 1.0479252338409424,
  "best_model_checkpoint": "Finetuning/Fine-tuned_checkpoint/2/medical_3\\checkpoint-50",
  "epoch": 0.0010939602454846792,
  "eval_steps": 25,
  "global_step": 50,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 2.187920490969358e-05,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.1489,
      "step": 1
    },
    {
      "epoch": 4.375840981938716e-05,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.2514,
      "step": 2
    },
    {
      "epoch": 6.563761472908074e-05,
      "grad_norm": 3.2709290981292725,
      "learning_rate": 4e-05,
      "loss": 2.3104,
      "step": 3
    },
    {
      "epoch": 8.751681963877433e-05,
      "grad_norm": 2.779268741607666,
      "learning_rate": 8e-05,
      "loss": 2.1807,
      "step": 4
    },
    {
      "epoch": 0.00010939602454846791,
      "grad_norm": 3.500542163848877,
      "learning_rate": 0.00012,
      "loss": 1.9871,
      "step": 5
    },
    {
      "epoch": 0.0001312752294581615,
      "grad_norm": NaN,
      "learning_rate": 0.00012,
      "loss": 2.0898,
      "step": 6
    },
    {
      "epoch": 0.00015315443436785507,
      "grad_norm": NaN,
      "learning_rate": 0.00012,
      "loss": 1.8829,
      "step": 7
    },
    {
      "epoch": 0.00017503363927754865,
      "grad_norm": NaN,
      "learning_rate": 0.00012,
      "loss": 2.1304,
      "step": 8
    },
    {
      "epoch": 0.00019691284418724223,
      "grad_norm": 27.946533203125,
      "learning_rate": 0.00016,
      "loss": 2.0801,
      "step": 9
    },
    {
      "epoch": 0.00021879204909693582,
      "grad_norm": 16.3977108001709,
      "learning_rate": 0.0002,
      "loss": 2.2192,
      "step": 10
    },
    {
      "epoch": 0.0002406712540066294,
      "grad_norm": 12.878182411193848,
      "learning_rate": 0.00019999798600729064,
      "loss": 2.2176,
      "step": 11
    },
    {
      "epoch": 0.000262550458916323,
      "grad_norm": 9.413625717163086,
      "learning_rate": 0.00019999194411028594,
      "loss": 2.1922,
      "step": 12
    },
    {
      "epoch": 0.00028442966382601656,
      "grad_norm": 9.80106258392334,
      "learning_rate": 0.0001999818745523526,
      "loss": 1.672,
      "step": 13
    },
    {
      "epoch": 0.00030630886873571014,
      "grad_norm": 20.085037231445312,
      "learning_rate": 0.00019996777773909093,
      "loss": 1.8428,
      "step": 14
    },
    {
      "epoch": 0.0003281880736454037,
      "grad_norm": 28.03141212463379,
      "learning_rate": 0.00019994965423831854,
      "loss": 1.8474,
      "step": 15
    },
    {
      "epoch": 0.0003500672785550973,
      "grad_norm": 4.915910243988037,
      "learning_rate": 0.00019992750478004738,
      "loss": 1.9661,
      "step": 16
    },
    {
      "epoch": 0.0003719464834647909,
      "grad_norm": 3.8566091060638428,
      "learning_rate": 0.0001999013302564544,
      "loss": 1.8644,
      "step": 17
    },
    {
      "epoch": 0.00039382568837448447,
      "grad_norm": 4.295337200164795,
      "learning_rate": 0.00019987113172184563,
      "loss": 1.6651,
      "step": 18
    },
    {
      "epoch": 0.00041570489328417805,
      "grad_norm": 5.446649551391602,
      "learning_rate": 0.00019983691039261357,
      "loss": 1.5854,
      "step": 19
    },
    {
      "epoch": 0.00043758409819387163,
      "grad_norm": 5.8035173416137695,
      "learning_rate": 0.00019979866764718843,
      "loss": 1.9175,
      "step": 20
    },
    {
      "epoch": 0.0004594633031035652,
      "grad_norm": 5.114415645599365,
      "learning_rate": 0.00019975640502598244,
      "loss": 1.299,
      "step": 21
    },
    {
      "epoch": 0.0004813425080132588,
      "grad_norm": 5.278352737426758,
      "learning_rate": 0.00019971012423132775,
      "loss": 1.191,
      "step": 22
    },
    {
      "epoch": 0.0005032217129229524,
      "grad_norm": 4.566121578216553,
      "learning_rate": 0.00019965982712740808,
      "loss": 1.4321,
      "step": 23
    },
    {
      "epoch": 0.000525100917832646,
      "grad_norm": 5.711122512817383,
      "learning_rate": 0.0001996055157401834,
      "loss": 1.1899,
      "step": 24
    },
    {
      "epoch": 0.0005469801227423396,
      "grad_norm": 5.113018035888672,
      "learning_rate": 0.00019954719225730847,
      "loss": 1.2518,
      "step": 25
    },
    {
      "epoch": 0.0005469801227423396,
      "eval_loss": 1.1922972202301025,
      "eval_runtime": 1914.1511,
      "eval_samples_per_second": 0.437,
      "eval_steps_per_second": 0.437,
      "step": 25
    },
    {
      "epoch": 0.0005688593276520331,
      "grad_norm": 4.588993549346924,
      "learning_rate": 0.0001994848590280447,
      "loss": 1.0657,
      "step": 26
    },
    {
      "epoch": 0.0005907385325617268,
      "grad_norm": 4.489006042480469,
      "learning_rate": 0.00019941851856316548,
      "loss": 1.2094,
      "step": 27
    },
    {
      "epoch": 0.0006126177374714203,
      "grad_norm": 5.219137191772461,
      "learning_rate": 0.00019934817353485501,
      "loss": 1.0861,
      "step": 28
    },
    {
      "epoch": 0.0006344969423811139,
      "grad_norm": 4.2921271324157715,
      "learning_rate": 0.00019927382677660088,
      "loss": 1.122,
      "step": 29
    },
    {
      "epoch": 0.0006563761472908074,
      "grad_norm": 4.807187557220459,
      "learning_rate": 0.00019919548128307954,
      "loss": 1.1372,
      "step": 30
    },
    {
      "epoch": 0.0006782553522005011,
      "grad_norm": 4.058358669281006,
      "learning_rate": 0.00019911314021003613,
      "loss": 0.8487,
      "step": 31
    },
    {
      "epoch": 0.0007001345571101946,
      "grad_norm": 3.881542444229126,
      "learning_rate": 0.00019902680687415705,
      "loss": 1.149,
      "step": 32
    },
    {
      "epoch": 0.0007220137620198882,
      "grad_norm": 4.037757873535156,
      "learning_rate": 0.00019893648475293648,
      "loss": 1.0259,
      "step": 33
    },
    {
      "epoch": 0.0007438929669295818,
      "grad_norm": 4.164768218994141,
      "learning_rate": 0.00019884217748453623,
      "loss": 1.0579,
      "step": 34
    },
    {
      "epoch": 0.0007657721718392754,
      "grad_norm": 3.8350396156311035,
      "learning_rate": 0.00019874388886763944,
      "loss": 0.9897,
      "step": 35
    },
    {
      "epoch": 0.0007876513767489689,
      "grad_norm": 3.524501085281372,
      "learning_rate": 0.0001986416228612972,
      "loss": 0.8777,
      "step": 36
    },
    {
      "epoch": 0.0008095305816586626,
      "grad_norm": 4.389486789703369,
      "learning_rate": 0.00019853538358476932,
      "loss": 1.0028,
      "step": 37
    },
    {
      "epoch": 0.0008314097865683561,
      "grad_norm": 5.2984418869018555,
      "learning_rate": 0.00019842517531735838,
      "loss": 0.8882,
      "step": 38
    },
    {
      "epoch": 0.0008532889914780497,
      "grad_norm": 4.486255645751953,
      "learning_rate": 0.00019831100249823733,
      "loss": 0.9041,
      "step": 39
    },
    {
      "epoch": 0.0008751681963877433,
      "grad_norm": 3.5563745498657227,
      "learning_rate": 0.00019819286972627066,
      "loss": 0.7836,
      "step": 40
    },
    {
      "epoch": 0.0008970474012974369,
      "grad_norm": 3.732107400894165,
      "learning_rate": 0.00019807078175982924,
      "loss": 0.8701,
      "step": 41
    },
    {
      "epoch": 0.0009189266062071304,
      "grad_norm": 4.825470924377441,
      "learning_rate": 0.00019794474351659852,
      "loss": 0.6458,
      "step": 42
    },
    {
      "epoch": 0.0009408058111168241,
      "grad_norm": 4.546532154083252,
      "learning_rate": 0.00019781476007338058,
      "loss": 0.5407,
      "step": 43
    },
    {
      "epoch": 0.0009626850160265176,
      "grad_norm": 3.5209569931030273,
      "learning_rate": 0.00019768083666588953,
      "loss": 0.4474,
      "step": 44
    },
    {
      "epoch": 0.0009845642209362112,
      "grad_norm": 2.735433578491211,
      "learning_rate": 0.00019754297868854073,
      "loss": 0.4174,
      "step": 45
    },
    {
      "epoch": 0.0010064434258459049,
      "grad_norm": 4.130967617034912,
      "learning_rate": 0.00019740119169423337,
      "loss": 0.5045,
      "step": 46
    },
    {
      "epoch": 0.0010283226307555983,
      "grad_norm": 3.451334238052368,
      "learning_rate": 0.00019725548139412692,
      "loss": 0.4736,
      "step": 47
    },
    {
      "epoch": 0.001050201835665292,
      "grad_norm": 3.0298266410827637,
      "learning_rate": 0.00019710585365741103,
      "loss": 0.4676,
      "step": 48
    },
    {
      "epoch": 0.0010720810405749855,
      "grad_norm": 4.125247955322266,
      "learning_rate": 0.00019695231451106912,
      "loss": 0.3847,
      "step": 49
    },
    {
      "epoch": 0.0010939602454846792,
      "grad_norm": 4.14768123626709,
      "learning_rate": 0.00019679487013963564,
      "loss": 0.4756,
      "step": 50
    },
    {
      "epoch": 0.0010939602454846792,
      "eval_loss": 1.0479252338409424,
      "eval_runtime": 2244.0112,
      "eval_samples_per_second": 0.373,
      "eval_steps_per_second": 0.373,
      "step": 50
    }
  ],
  "logging_steps": 1,
  "max_steps": 500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2297668345036800.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
