{
  "best_metric": 1.0610764026641846,
  "best_model_checkpoint": "Finetuning/Fine-tuned_checkpoint/medical_3\\checkpoint-160",
  "epoch": 0.003500672785550973,
  "eval_steps": 20,
  "global_step": 160,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 2.187920490969358e-05,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.3,
      "step": 1
    },
    {
      "epoch": 4.375840981938716e-05,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.134,
      "step": 2
    },
    {
      "epoch": 6.563761472908074e-05,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.2576,
      "step": 3
    },
    {
      "epoch": 8.751681963877433e-05,
      "grad_norm": 3.582977533340454,
      "learning_rate": 4e-05,
      "loss": 2.0945,
      "step": 4
    },
    {
      "epoch": 0.00010939602454846791,
      "grad_norm": 3.747514247894287,
      "learning_rate": 8e-05,
      "loss": 1.8236,
      "step": 5
    },
    {
      "epoch": 0.0001312752294581615,
      "grad_norm": 3.6239430904388428,
      "learning_rate": 0.00012,
      "loss": 1.8854,
      "step": 6
    },
    {
      "epoch": 0.00015315443436785507,
      "grad_norm": NaN,
      "learning_rate": 0.00012,
      "loss": 2.0524,
      "step": 7
    },
    {
      "epoch": 0.00017503363927754865,
      "grad_norm": 3.4393887519836426,
      "learning_rate": 0.00016,
      "loss": 2.0364,
      "step": 8
    },
    {
      "epoch": 0.00019691284418724223,
      "grad_norm": NaN,
      "learning_rate": 0.00016,
      "loss": 2.0879,
      "step": 9
    },
    {
      "epoch": 0.00021879204909693582,
      "grad_norm": 10.043916702270508,
      "learning_rate": 0.0002,
      "loss": 2.2239,
      "step": 10
    },
    {
      "epoch": 0.0002406712540066294,
      "grad_norm": 4.174642086029053,
      "learning_rate": 0.0001995959595959596,
      "loss": 2.11,
      "step": 11
    },
    {
      "epoch": 0.000262550458916323,
      "grad_norm": 5.000838279724121,
      "learning_rate": 0.0001991919191919192,
      "loss": 2.2663,
      "step": 12
    },
    {
      "epoch": 0.00028442966382601656,
      "grad_norm": 10.119012832641602,
      "learning_rate": 0.00019878787878787878,
      "loss": 2.1576,
      "step": 13
    },
    {
      "epoch": 0.00030630886873571014,
      "grad_norm": 6.535268783569336,
      "learning_rate": 0.00019838383838383837,
      "loss": 2.1494,
      "step": 14
    },
    {
      "epoch": 0.0003281880736454037,
      "grad_norm": 7.976975917816162,
      "learning_rate": 0.000197979797979798,
      "loss": 1.7698,
      "step": 15
    },
    {
      "epoch": 0.0003500672785550973,
      "grad_norm": NaN,
      "learning_rate": 0.000197979797979798,
      "loss": 1.6329,
      "step": 16
    },
    {
      "epoch": 0.0003719464834647909,
      "grad_norm": 7.913826942443848,
      "learning_rate": 0.0001975757575757576,
      "loss": 1.5018,
      "step": 17
    },
    {
      "epoch": 0.00039382568837448447,
      "grad_norm": NaN,
      "learning_rate": 0.0001975757575757576,
      "loss": 1.7756,
      "step": 18
    },
    {
      "epoch": 0.00041570489328417805,
      "grad_norm": 28.892213821411133,
      "learning_rate": 0.0001971717171717172,
      "loss": 1.4315,
      "step": 19
    },
    {
      "epoch": 0.00043758409819387163,
      "grad_norm": 30.55006217956543,
      "learning_rate": 0.00019676767676767677,
      "loss": 1.7918,
      "step": 20
    },
    {
      "epoch": 0.00043758409819387163,
      "eval_loss": 1.5775611400604248,
      "eval_runtime": 1207.3533,
      "eval_samples_per_second": 0.347,
      "eval_steps_per_second": 0.347,
      "step": 20
    },
    {
      "epoch": 0.0004594633031035652,
      "grad_norm": 16.522403717041016,
      "learning_rate": 0.00019636363636363636,
      "loss": 1.5203,
      "step": 21
    },
    {
      "epoch": 0.0004813425080132588,
      "grad_norm": 13.282205581665039,
      "learning_rate": 0.00019595959595959596,
      "loss": 1.8017,
      "step": 22
    },
    {
      "epoch": 0.0005032217129229524,
      "grad_norm": 9.916335105895996,
      "learning_rate": 0.00019555555555555556,
      "loss": 1.4519,
      "step": 23
    },
    {
      "epoch": 0.000525100917832646,
      "grad_norm": 6.676116466522217,
      "learning_rate": 0.00019515151515151516,
      "loss": 1.202,
      "step": 24
    },
    {
      "epoch": 0.0005469801227423396,
      "grad_norm": 6.057275295257568,
      "learning_rate": 0.00019474747474747476,
      "loss": 1.2828,
      "step": 25
    },
    {
      "epoch": 0.0005688593276520331,
      "grad_norm": 7.44789981842041,
      "learning_rate": 0.00019434343434343435,
      "loss": 1.0523,
      "step": 26
    },
    {
      "epoch": 0.0005907385325617268,
      "grad_norm": 6.50221061706543,
      "learning_rate": 0.00019393939393939395,
      "loss": 1.2899,
      "step": 27
    },
    {
      "epoch": 0.0006126177374714203,
      "grad_norm": 6.900063991546631,
      "learning_rate": 0.00019353535353535355,
      "loss": 1.2485,
      "step": 28
    },
    {
      "epoch": 0.0006344969423811139,
      "grad_norm": 6.068929672241211,
      "learning_rate": 0.00019313131313131315,
      "loss": 1.0439,
      "step": 29
    },
    {
      "epoch": 0.0006563761472908074,
      "grad_norm": 7.316030025482178,
      "learning_rate": 0.00019272727272727274,
      "loss": 1.2418,
      "step": 30
    },
    {
      "epoch": 0.0006782553522005011,
      "grad_norm": 5.879920959472656,
      "learning_rate": 0.00019232323232323232,
      "loss": 1.1358,
      "step": 31
    },
    {
      "epoch": 0.0007001345571101946,
      "grad_norm": 5.274486064910889,
      "learning_rate": 0.00019191919191919191,
      "loss": 0.9103,
      "step": 32
    },
    {
      "epoch": 0.0007220137620198882,
      "grad_norm": 5.948697566986084,
      "learning_rate": 0.0001915151515151515,
      "loss": 0.7902,
      "step": 33
    },
    {
      "epoch": 0.0007438929669295818,
      "grad_norm": 7.261547565460205,
      "learning_rate": 0.00019111111111111114,
      "loss": 1.0552,
      "step": 34
    },
    {
      "epoch": 0.0007657721718392754,
      "grad_norm": 6.084803104400635,
      "learning_rate": 0.00019070707070707073,
      "loss": 0.8835,
      "step": 35
    },
    {
      "epoch": 0.0007876513767489689,
      "grad_norm": 7.025145053863525,
      "learning_rate": 0.0001903030303030303,
      "loss": 0.9343,
      "step": 36
    },
    {
      "epoch": 0.0008095305816586626,
      "grad_norm": 7.298849582672119,
      "learning_rate": 0.0001898989898989899,
      "loss": 0.8876,
      "step": 37
    },
    {
      "epoch": 0.0008314097865683561,
      "grad_norm": 4.5801215171813965,
      "learning_rate": 0.0001894949494949495,
      "loss": 0.6371,
      "step": 38
    },
    {
      "epoch": 0.0008532889914780497,
      "grad_norm": 6.978671073913574,
      "learning_rate": 0.0001890909090909091,
      "loss": 0.9304,
      "step": 39
    },
    {
      "epoch": 0.0008751681963877433,
      "grad_norm": 5.251047134399414,
      "learning_rate": 0.0001886868686868687,
      "loss": 0.7401,
      "step": 40
    },
    {
      "epoch": 0.0008751681963877433,
      "eval_loss": 1.1186294555664062,
      "eval_runtime": 1247.1505,
      "eval_samples_per_second": 0.336,
      "eval_steps_per_second": 0.336,
      "step": 40
    },
    {
      "epoch": 0.0008970474012974369,
      "grad_norm": 4.654073715209961,
      "learning_rate": 0.0001882828282828283,
      "loss": 0.6543,
      "step": 41
    },
    {
      "epoch": 0.0009189266062071304,
      "grad_norm": 4.651910781860352,
      "learning_rate": 0.0001878787878787879,
      "loss": 0.6413,
      "step": 42
    },
    {
      "epoch": 0.0009408058111168241,
      "grad_norm": 4.910744667053223,
      "learning_rate": 0.0001874747474747475,
      "loss": 0.5807,
      "step": 43
    },
    {
      "epoch": 0.0009626850160265176,
      "grad_norm": 6.588287830352783,
      "learning_rate": 0.0001870707070707071,
      "loss": 0.7039,
      "step": 44
    },
    {
      "epoch": 0.0009845642209362112,
      "grad_norm": 5.525004863739014,
      "learning_rate": 0.0001866666666666667,
      "loss": 0.4344,
      "step": 45
    },
    {
      "epoch": 0.0010064434258459049,
      "grad_norm": 9.238670349121094,
      "learning_rate": 0.00018626262626262628,
      "loss": 0.6316,
      "step": 46
    },
    {
      "epoch": 0.0010283226307555983,
      "grad_norm": 7.444749355316162,
      "learning_rate": 0.00018585858585858586,
      "loss": 0.5477,
      "step": 47
    },
    {
      "epoch": 0.001050201835665292,
      "grad_norm": 5.9416422843933105,
      "learning_rate": 0.00018545454545454545,
      "loss": 0.609,
      "step": 48
    },
    {
      "epoch": 0.0010720810405749855,
      "grad_norm": 6.438233852386475,
      "learning_rate": 0.00018505050505050505,
      "loss": 0.4394,
      "step": 49
    },
    {
      "epoch": 0.0010939602454846792,
      "grad_norm": 8.061407089233398,
      "learning_rate": 0.00018464646464646465,
      "loss": 0.4812,
      "step": 50
    },
    {
      "epoch": 0.0011158394503943726,
      "grad_norm": 4.240417003631592,
      "learning_rate": 0.00018424242424242427,
      "loss": 1.8898,
      "step": 51
    },
    {
      "epoch": 0.0011377186553040662,
      "grad_norm": 3.834597110748291,
      "learning_rate": 0.00018383838383838384,
      "loss": 1.5857,
      "step": 52
    },
    {
      "epoch": 0.0011595978602137599,
      "grad_norm": 3.5741536617279053,
      "learning_rate": 0.00018343434343434344,
      "loss": 1.7066,
      "step": 53
    },
    {
      "epoch": 0.0011814770651234535,
      "grad_norm": 4.111266613006592,
      "learning_rate": 0.00018303030303030304,
      "loss": 1.395,
      "step": 54
    },
    {
      "epoch": 0.001203356270033147,
      "grad_norm": 3.1388490200042725,
      "learning_rate": 0.00018262626262626264,
      "loss": 1.4979,
      "step": 55
    },
    {
      "epoch": 0.0012252354749428406,
      "grad_norm": 3.169434070587158,
      "learning_rate": 0.00018222222222222224,
      "loss": 1.4344,
      "step": 56
    },
    {
      "epoch": 0.0012471146798525342,
      "grad_norm": 3.5518546104431152,
      "learning_rate": 0.00018181818181818183,
      "loss": 1.3921,
      "step": 57
    },
    {
      "epoch": 0.0012689938847622278,
      "grad_norm": 2.809762477874756,
      "learning_rate": 0.0001814141414141414,
      "loss": 1.1816,
      "step": 58
    },
    {
      "epoch": 0.0012908730896719213,
      "grad_norm": 2.9568984508514404,
      "learning_rate": 0.00018101010101010103,
      "loss": 1.3195,
      "step": 59
    },
    {
      "epoch": 0.0013127522945816149,
      "grad_norm": 3.265488386154175,
      "learning_rate": 0.00018060606060606063,
      "loss": 1.5435,
      "step": 60
    },
    {
      "epoch": 0.0013127522945816149,
      "eval_loss": 1.083312749862671,
      "eval_runtime": 1206.0657,
      "eval_samples_per_second": 0.347,
      "eval_steps_per_second": 0.347,
      "step": 60
    },
    {
      "epoch": 0.0013346314994913085,
      "grad_norm": 4.071633338928223,
      "learning_rate": 0.00018020202020202023,
      "loss": 1.5087,
      "step": 61
    },
    {
      "epoch": 0.0013565107044010022,
      "grad_norm": 2.7864012718200684,
      "learning_rate": 0.0001797979797979798,
      "loss": 1.2848,
      "step": 62
    },
    {
      "epoch": 0.0013783899093106956,
      "grad_norm": 3.2151615619659424,
      "learning_rate": 0.0001793939393939394,
      "loss": 1.5074,
      "step": 63
    },
    {
      "epoch": 0.0014002691142203892,
      "grad_norm": 3.9442641735076904,
      "learning_rate": 0.000178989898989899,
      "loss": 1.2842,
      "step": 64
    },
    {
      "epoch": 0.0014221483191300829,
      "grad_norm": 4.133578300476074,
      "learning_rate": 0.0001785858585858586,
      "loss": 1.4328,
      "step": 65
    },
    {
      "epoch": 0.0014440275240397765,
      "grad_norm": 3.8823354244232178,
      "learning_rate": 0.0001781818181818182,
      "loss": 1.3915,
      "step": 66
    },
    {
      "epoch": 0.00146590672894947,
      "grad_norm": 3.128021001815796,
      "learning_rate": 0.00017777777777777779,
      "loss": 1.2119,
      "step": 67
    },
    {
      "epoch": 0.0014877859338591635,
      "grad_norm": 3.8006370067596436,
      "learning_rate": 0.00017737373737373738,
      "loss": 1.1112,
      "step": 68
    },
    {
      "epoch": 0.0015096651387688572,
      "grad_norm": 3.0185561180114746,
      "learning_rate": 0.00017696969696969698,
      "loss": 1.2158,
      "step": 69
    },
    {
      "epoch": 0.0015315443436785508,
      "grad_norm": 3.7873857021331787,
      "learning_rate": 0.00017656565656565658,
      "loss": 1.1746,
      "step": 70
    },
    {
      "epoch": 0.0015534235485882442,
      "grad_norm": 3.1916403770446777,
      "learning_rate": 0.00017616161616161618,
      "loss": 1.1164,
      "step": 71
    },
    {
      "epoch": 0.0015753027534979379,
      "grad_norm": 3.7392940521240234,
      "learning_rate": 0.00017575757575757578,
      "loss": 0.9906,
      "step": 72
    },
    {
      "epoch": 0.0015971819584076315,
      "grad_norm": 3.4577906131744385,
      "learning_rate": 0.00017535353535353535,
      "loss": 1.2396,
      "step": 73
    },
    {
      "epoch": 0.0016190611633173251,
      "grad_norm": 3.020916700363159,
      "learning_rate": 0.00017494949494949494,
      "loss": 0.9811,
      "step": 74
    },
    {
      "epoch": 0.0016409403682270186,
      "grad_norm": 4.235615253448486,
      "learning_rate": 0.00017454545454545454,
      "loss": 1.1704,
      "step": 75
    },
    {
      "epoch": 0.0016628195731367122,
      "grad_norm": 3.181392192840576,
      "learning_rate": 0.00017414141414141414,
      "loss": 1.2832,
      "step": 76
    },
    {
      "epoch": 0.0016846987780464058,
      "grad_norm": 3.735889196395874,
      "learning_rate": 0.00017373737373737377,
      "loss": 1.0175,
      "step": 77
    },
    {
      "epoch": 0.0017065779829560995,
      "grad_norm": 3.641462564468384,
      "learning_rate": 0.00017333333333333334,
      "loss": 1.0523,
      "step": 78
    },
    {
      "epoch": 0.0017284571878657929,
      "grad_norm": 3.7880380153656006,
      "learning_rate": 0.00017292929292929293,
      "loss": 1.0719,
      "step": 79
    },
    {
      "epoch": 0.0017503363927754865,
      "grad_norm": 4.743274688720703,
      "learning_rate": 0.00017252525252525253,
      "loss": 1.0136,
      "step": 80
    },
    {
      "epoch": 0.0017503363927754865,
      "eval_loss": 1.0778734683990479,
      "eval_runtime": 989.9713,
      "eval_samples_per_second": 0.423,
      "eval_steps_per_second": 0.423,
      "step": 80
    },
    {
      "epoch": 0.0017722155976851802,
      "grad_norm": 3.7915987968444824,
      "learning_rate": 0.00017212121212121213,
      "loss": 0.909,
      "step": 81
    },
    {
      "epoch": 0.0017940948025948738,
      "grad_norm": 4.286474227905273,
      "learning_rate": 0.00017171717171717173,
      "loss": 0.9834,
      "step": 82
    },
    {
      "epoch": 0.0018159740075045672,
      "grad_norm": 4.193707466125488,
      "learning_rate": 0.00017131313131313133,
      "loss": 0.9174,
      "step": 83
    },
    {
      "epoch": 0.0018378532124142608,
      "grad_norm": 3.6510438919067383,
      "learning_rate": 0.0001709090909090909,
      "loss": 0.755,
      "step": 84
    },
    {
      "epoch": 0.0018597324173239545,
      "grad_norm": 3.7754054069519043,
      "learning_rate": 0.00017050505050505052,
      "loss": 0.6863,
      "step": 85
    },
    {
      "epoch": 0.0018816116222336481,
      "grad_norm": 4.033400058746338,
      "learning_rate": 0.00017010101010101012,
      "loss": 0.8197,
      "step": 86
    },
    {
      "epoch": 0.0019034908271433415,
      "grad_norm": 4.03004789352417,
      "learning_rate": 0.00016969696969696972,
      "loss": 0.7402,
      "step": 87
    },
    {
      "epoch": 0.0019253700320530352,
      "grad_norm": 5.7920966148376465,
      "learning_rate": 0.00016929292929292932,
      "loss": 0.7261,
      "step": 88
    },
    {
      "epoch": 0.0019472492369627288,
      "grad_norm": 5.998499393463135,
      "learning_rate": 0.00016888888888888889,
      "loss": 0.6546,
      "step": 89
    },
    {
      "epoch": 0.0019691284418724224,
      "grad_norm": 4.057008743286133,
      "learning_rate": 0.00016848484848484848,
      "loss": 0.6322,
      "step": 90
    },
    {
      "epoch": 0.001991007646782116,
      "grad_norm": 3.8783164024353027,
      "learning_rate": 0.00016808080808080808,
      "loss": 0.5848,
      "step": 91
    },
    {
      "epoch": 0.0020128868516918097,
      "grad_norm": 4.726592063903809,
      "learning_rate": 0.00016767676767676768,
      "loss": 0.5252,
      "step": 92
    },
    {
      "epoch": 0.002034766056601503,
      "grad_norm": 4.214262008666992,
      "learning_rate": 0.00016727272727272728,
      "loss": 0.5345,
      "step": 93
    },
    {
      "epoch": 0.0020566452615111966,
      "grad_norm": 3.671370267868042,
      "learning_rate": 0.00016686868686868688,
      "loss": 0.5801,
      "step": 94
    },
    {
      "epoch": 0.00207852446642089,
      "grad_norm": 3.4273715019226074,
      "learning_rate": 0.00016646464646464647,
      "loss": 0.4902,
      "step": 95
    },
    {
      "epoch": 0.002100403671330584,
      "grad_norm": 4.14777946472168,
      "learning_rate": 0.00016606060606060607,
      "loss": 0.5774,
      "step": 96
    },
    {
      "epoch": 0.0021222828762402775,
      "grad_norm": 4.871885299682617,
      "learning_rate": 0.00016565656565656567,
      "loss": 0.4951,
      "step": 97
    },
    {
      "epoch": 0.002144162081149971,
      "grad_norm": 5.4186692237854,
      "learning_rate": 0.00016525252525252527,
      "loss": 0.5824,
      "step": 98
    },
    {
      "epoch": 0.0021660412860596647,
      "grad_norm": 3.454399347305298,
      "learning_rate": 0.00016484848484848487,
      "loss": 0.431,
      "step": 99
    },
    {
      "epoch": 0.0021879204909693584,
      "grad_norm": 4.710813999176025,
      "learning_rate": 0.00016444444444444444,
      "loss": 0.5383,
      "step": 100
    },
    {
      "epoch": 0.0021879204909693584,
      "eval_loss": 1.08467435836792,
      "eval_runtime": 1157.1673,
      "eval_samples_per_second": 0.362,
      "eval_steps_per_second": 0.362,
      "step": 100
    },
    {
      "epoch": 0.0022097996958790516,
      "grad_norm": 2.853144407272339,
      "learning_rate": 0.00016404040404040403,
      "loss": 1.4011,
      "step": 101
    },
    {
      "epoch": 0.002231678900788745,
      "grad_norm": 2.8249521255493164,
      "learning_rate": 0.00016363636363636366,
      "loss": 1.5769,
      "step": 102
    },
    {
      "epoch": 0.002253558105698439,
      "grad_norm": 2.3686609268188477,
      "learning_rate": 0.00016323232323232326,
      "loss": 1.6052,
      "step": 103
    },
    {
      "epoch": 0.0022754373106081325,
      "grad_norm": 2.9171142578125,
      "learning_rate": 0.00016282828282828283,
      "loss": 1.441,
      "step": 104
    },
    {
      "epoch": 0.002297316515517826,
      "grad_norm": 2.3063971996307373,
      "learning_rate": 0.00016242424242424243,
      "loss": 1.3995,
      "step": 105
    },
    {
      "epoch": 0.0023191957204275197,
      "grad_norm": 3.0006494522094727,
      "learning_rate": 0.00016202020202020202,
      "loss": 1.5298,
      "step": 106
    },
    {
      "epoch": 0.0023410749253372134,
      "grad_norm": 3.122603416442871,
      "learning_rate": 0.00016161616161616162,
      "loss": 1.5058,
      "step": 107
    },
    {
      "epoch": 0.002362954130246907,
      "grad_norm": 2.6224844455718994,
      "learning_rate": 0.00016121212121212122,
      "loss": 1.6639,
      "step": 108
    },
    {
      "epoch": 0.0023848333351566002,
      "grad_norm": 2.936767101287842,
      "learning_rate": 0.00016080808080808082,
      "loss": 1.4193,
      "step": 109
    },
    {
      "epoch": 0.002406712540066294,
      "grad_norm": 2.4151980876922607,
      "learning_rate": 0.0001604040404040404,
      "loss": 1.3721,
      "step": 110
    },
    {
      "epoch": 0.0024285917449759875,
      "grad_norm": 2.1034464836120605,
      "learning_rate": 0.00016,
      "loss": 1.2986,
      "step": 111
    },
    {
      "epoch": 0.002450470949885681,
      "grad_norm": 2.6494686603546143,
      "learning_rate": 0.0001595959595959596,
      "loss": 1.2483,
      "step": 112
    },
    {
      "epoch": 0.0024723501547953748,
      "grad_norm": 2.6644647121429443,
      "learning_rate": 0.0001591919191919192,
      "loss": 1.2991,
      "step": 113
    },
    {
      "epoch": 0.0024942293597050684,
      "grad_norm": 2.961926221847534,
      "learning_rate": 0.0001587878787878788,
      "loss": 1.2564,
      "step": 114
    },
    {
      "epoch": 0.002516108564614762,
      "grad_norm": 2.454287528991699,
      "learning_rate": 0.00015838383838383838,
      "loss": 1.327,
      "step": 115
    },
    {
      "epoch": 0.0025379877695244557,
      "grad_norm": 2.856566905975342,
      "learning_rate": 0.00015797979797979798,
      "loss": 1.0797,
      "step": 116
    },
    {
      "epoch": 0.002559866974434149,
      "grad_norm": 3.308616876602173,
      "learning_rate": 0.00015757575757575757,
      "loss": 1.236,
      "step": 117
    },
    {
      "epoch": 0.0025817461793438425,
      "grad_norm": 3.6130104064941406,
      "learning_rate": 0.00015717171717171717,
      "loss": 1.1725,
      "step": 118
    },
    {
      "epoch": 0.002603625384253536,
      "grad_norm": 3.3420894145965576,
      "learning_rate": 0.0001567676767676768,
      "loss": 1.0999,
      "step": 119
    },
    {
      "epoch": 0.0026255045891632298,
      "grad_norm": 3.749969720840454,
      "learning_rate": 0.00015636363636363637,
      "loss": 1.2844,
      "step": 120
    },
    {
      "epoch": 0.0026255045891632298,
      "eval_loss": 1.074223518371582,
      "eval_runtime": 1285.6475,
      "eval_samples_per_second": 0.326,
      "eval_steps_per_second": 0.326,
      "step": 120
    },
    {
      "epoch": 0.0026473837940729234,
      "grad_norm": 3.2028071880340576,
      "learning_rate": 0.00015595959595959597,
      "loss": 1.0457,
      "step": 121
    },
    {
      "epoch": 0.002669262998982617,
      "grad_norm": 3.0654754638671875,
      "learning_rate": 0.00015555555555555556,
      "loss": 1.0361,
      "step": 122
    },
    {
      "epoch": 0.0026911422038923107,
      "grad_norm": 3.2505292892456055,
      "learning_rate": 0.00015515151515151516,
      "loss": 1.1189,
      "step": 123
    },
    {
      "epoch": 0.0027130214088020043,
      "grad_norm": 3.7952983379364014,
      "learning_rate": 0.00015474747474747476,
      "loss": 1.2202,
      "step": 124
    },
    {
      "epoch": 0.0027349006137116975,
      "grad_norm": 3.5331673622131348,
      "learning_rate": 0.00015434343434343436,
      "loss": 1.0622,
      "step": 125
    },
    {
      "epoch": 0.002756779818621391,
      "grad_norm": 3.791264772415161,
      "learning_rate": 0.00015393939393939393,
      "loss": 1.0674,
      "step": 126
    },
    {
      "epoch": 0.002778659023531085,
      "grad_norm": 3.621905565261841,
      "learning_rate": 0.00015353535353535353,
      "loss": 0.9996,
      "step": 127
    },
    {
      "epoch": 0.0028005382284407784,
      "grad_norm": 3.7815229892730713,
      "learning_rate": 0.00015313131313131315,
      "loss": 1.0512,
      "step": 128
    },
    {
      "epoch": 0.002822417433350472,
      "grad_norm": 3.3097989559173584,
      "learning_rate": 0.00015272727272727275,
      "loss": 1.0251,
      "step": 129
    },
    {
      "epoch": 0.0028442966382601657,
      "grad_norm": 4.104879379272461,
      "learning_rate": 0.00015232323232323235,
      "loss": 0.8195,
      "step": 130
    },
    {
      "epoch": 0.0028661758431698593,
      "grad_norm": 4.4640278816223145,
      "learning_rate": 0.00015191919191919192,
      "loss": 0.7767,
      "step": 131
    },
    {
      "epoch": 0.002888055048079553,
      "grad_norm": 4.035533905029297,
      "learning_rate": 0.00015151515151515152,
      "loss": 0.9547,
      "step": 132
    },
    {
      "epoch": 0.002909934252989246,
      "grad_norm": 3.889815092086792,
      "learning_rate": 0.0001511111111111111,
      "loss": 0.6884,
      "step": 133
    },
    {
      "epoch": 0.00293181345789894,
      "grad_norm": 3.688404083251953,
      "learning_rate": 0.0001507070707070707,
      "loss": 0.9177,
      "step": 134
    },
    {
      "epoch": 0.0029536926628086334,
      "grad_norm": 3.4736905097961426,
      "learning_rate": 0.0001503030303030303,
      "loss": 0.7686,
      "step": 135
    },
    {
      "epoch": 0.002975571867718327,
      "grad_norm": 3.99497652053833,
      "learning_rate": 0.0001498989898989899,
      "loss": 0.8811,
      "step": 136
    },
    {
      "epoch": 0.0029974510726280207,
      "grad_norm": 4.97185754776001,
      "learning_rate": 0.0001494949494949495,
      "loss": 0.8895,
      "step": 137
    },
    {
      "epoch": 0.0030193302775377144,
      "grad_norm": 3.418463945388794,
      "learning_rate": 0.0001490909090909091,
      "loss": 0.5909,
      "step": 138
    },
    {
      "epoch": 0.003041209482447408,
      "grad_norm": 3.456327199935913,
      "learning_rate": 0.0001486868686868687,
      "loss": 0.6432,
      "step": 139
    },
    {
      "epoch": 0.0030630886873571016,
      "grad_norm": 5.200439929962158,
      "learning_rate": 0.0001482828282828283,
      "loss": 0.6963,
      "step": 140
    },
    {
      "epoch": 0.0030630886873571016,
      "eval_loss": 1.0772480964660645,
      "eval_runtime": 1290.3941,
      "eval_samples_per_second": 0.325,
      "eval_steps_per_second": 0.325,
      "step": 140
    },
    {
      "epoch": 0.003084967892266795,
      "grad_norm": 4.516968727111816,
      "learning_rate": 0.0001478787878787879,
      "loss": 0.6698,
      "step": 141
    },
    {
      "epoch": 0.0031068470971764885,
      "grad_norm": 3.0375325679779053,
      "learning_rate": 0.00014747474747474747,
      "loss": 0.5314,
      "step": 142
    },
    {
      "epoch": 0.003128726302086182,
      "grad_norm": 4.42278528213501,
      "learning_rate": 0.00014707070707070706,
      "loss": 0.7276,
      "step": 143
    },
    {
      "epoch": 0.0031506055069958757,
      "grad_norm": 5.266684055328369,
      "learning_rate": 0.00014666666666666666,
      "loss": 0.6405,
      "step": 144
    },
    {
      "epoch": 0.0031724847119055694,
      "grad_norm": 4.321359157562256,
      "learning_rate": 0.0001462626262626263,
      "loss": 0.5172,
      "step": 145
    },
    {
      "epoch": 0.003194363916815263,
      "grad_norm": 4.264169692993164,
      "learning_rate": 0.00014585858585858586,
      "loss": 0.568,
      "step": 146
    },
    {
      "epoch": 0.0032162431217249566,
      "grad_norm": 5.165587902069092,
      "learning_rate": 0.00014545454545454546,
      "loss": 0.4741,
      "step": 147
    },
    {
      "epoch": 0.0032381223266346503,
      "grad_norm": 3.804166555404663,
      "learning_rate": 0.00014505050505050505,
      "loss": 0.4898,
      "step": 148
    },
    {
      "epoch": 0.0032600015315443435,
      "grad_norm": 3.928715229034424,
      "learning_rate": 0.00014464646464646465,
      "loss": 0.4071,
      "step": 149
    },
    {
      "epoch": 0.003281880736454037,
      "grad_norm": 3.7104098796844482,
      "learning_rate": 0.00014424242424242425,
      "loss": 0.5414,
      "step": 150
    },
    {
      "epoch": 0.0033037599413637308,
      "grad_norm": 2.637075185775757,
      "learning_rate": 0.00014383838383838385,
      "loss": 1.5114,
      "step": 151
    },
    {
      "epoch": 0.0033256391462734244,
      "grad_norm": 2.3697123527526855,
      "learning_rate": 0.00014343434343434342,
      "loss": 1.7502,
      "step": 152
    },
    {
      "epoch": 0.003347518351183118,
      "grad_norm": 2.353017807006836,
      "learning_rate": 0.00014303030303030304,
      "loss": 1.2037,
      "step": 153
    },
    {
      "epoch": 0.0033693975560928117,
      "grad_norm": 2.5830557346343994,
      "learning_rate": 0.00014262626262626264,
      "loss": 1.5081,
      "step": 154
    },
    {
      "epoch": 0.0033912767610025053,
      "grad_norm": 3.009612798690796,
      "learning_rate": 0.00014222222222222224,
      "loss": 1.4589,
      "step": 155
    },
    {
      "epoch": 0.003413155965912199,
      "grad_norm": 2.4310996532440186,
      "learning_rate": 0.00014181818181818184,
      "loss": 1.5467,
      "step": 156
    },
    {
      "epoch": 0.003435035170821892,
      "grad_norm": 2.2948780059814453,
      "learning_rate": 0.0001414141414141414,
      "loss": 1.2685,
      "step": 157
    },
    {
      "epoch": 0.0034569143757315858,
      "grad_norm": 2.5796425342559814,
      "learning_rate": 0.000141010101010101,
      "loss": 1.2207,
      "step": 158
    },
    {
      "epoch": 0.0034787935806412794,
      "grad_norm": 2.6796839237213135,
      "learning_rate": 0.0001406060606060606,
      "loss": 1.3855,
      "step": 159
    },
    {
      "epoch": 0.003500672785550973,
      "grad_norm": 2.5676474571228027,
      "learning_rate": 0.0001402020202020202,
      "loss": 1.2576,
      "step": 160
    },
    {
      "epoch": 0.003500672785550973,
      "eval_loss": 1.0610764026641846,
      "eval_runtime": 1293.8927,
      "eval_samples_per_second": 0.324,
      "eval_steps_per_second": 0.324,
      "step": 160
    }
  ],
  "logging_steps": 1,
  "max_steps": 500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 40,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7090124636307456.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
