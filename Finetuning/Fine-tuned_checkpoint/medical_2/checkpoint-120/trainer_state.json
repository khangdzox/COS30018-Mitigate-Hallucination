{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.03900853311661926,
  "eval_steps": 500,
  "global_step": 120,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0003250711093051605,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.08,
      "step": 1
    },
    {
      "epoch": 0.000650142218610321,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 1.4591,
      "step": 2
    },
    {
      "epoch": 0.0009752133279154815,
      "grad_norm": 3.1998820304870605,
      "learning_rate": 2e-05,
      "loss": 1.6253,
      "step": 3
    },
    {
      "epoch": 0.001300284437220642,
      "grad_norm": 2.9948737621307373,
      "learning_rate": 4e-05,
      "loss": 1.6721,
      "step": 4
    },
    {
      "epoch": 0.0016253555465258025,
      "grad_norm": 3.1418697834014893,
      "learning_rate": 6e-05,
      "loss": 1.5395,
      "step": 5
    },
    {
      "epoch": 0.001950426655830963,
      "grad_norm": 3.2573678493499756,
      "learning_rate": 8e-05,
      "loss": 1.7849,
      "step": 6
    },
    {
      "epoch": 0.0022754977651361233,
      "grad_norm": 3.1392688751220703,
      "learning_rate": 0.0001,
      "loss": 1.5148,
      "step": 7
    },
    {
      "epoch": 0.002600568874441284,
      "grad_norm": 3.4964191913604736,
      "learning_rate": 9.91304347826087e-05,
      "loss": 1.7256,
      "step": 8
    },
    {
      "epoch": 0.0029256399837464444,
      "grad_norm": 2.8406357765197754,
      "learning_rate": 9.82608695652174e-05,
      "loss": 1.4819,
      "step": 9
    },
    {
      "epoch": 0.003250711093051605,
      "grad_norm": 2.8371129035949707,
      "learning_rate": 9.739130434782609e-05,
      "loss": 1.6265,
      "step": 10
    },
    {
      "epoch": 0.0035757822023567655,
      "grad_norm": 2.9346745014190674,
      "learning_rate": 9.652173913043479e-05,
      "loss": 1.6113,
      "step": 11
    },
    {
      "epoch": 0.003900853311661926,
      "grad_norm": 3.8821520805358887,
      "learning_rate": 9.565217391304348e-05,
      "loss": 1.6219,
      "step": 12
    },
    {
      "epoch": 0.0042259244209670865,
      "grad_norm": 2.58050274848938,
      "learning_rate": 9.478260869565218e-05,
      "loss": 1.7487,
      "step": 13
    },
    {
      "epoch": 0.004550995530272247,
      "grad_norm": 2.78120493888855,
      "learning_rate": 9.391304347826087e-05,
      "loss": 1.4048,
      "step": 14
    },
    {
      "epoch": 0.004876066639577408,
      "grad_norm": 2.944776773452759,
      "learning_rate": 9.304347826086957e-05,
      "loss": 1.5187,
      "step": 15
    },
    {
      "epoch": 0.005201137748882568,
      "grad_norm": NaN,
      "learning_rate": 9.304347826086957e-05,
      "loss": 1.4219,
      "step": 16
    },
    {
      "epoch": 0.005526208858187729,
      "grad_norm": 3.3512513637542725,
      "learning_rate": 9.217391304347827e-05,
      "loss": 1.6285,
      "step": 17
    },
    {
      "epoch": 0.005851279967492889,
      "grad_norm": 4.108438968658447,
      "learning_rate": 9.130434782608696e-05,
      "loss": 1.6834,
      "step": 18
    },
    {
      "epoch": 0.00617635107679805,
      "grad_norm": 8.739580154418945,
      "learning_rate": 9.043478260869566e-05,
      "loss": 1.6192,
      "step": 19
    },
    {
      "epoch": 0.00650142218610321,
      "grad_norm": 5.452498435974121,
      "learning_rate": 8.956521739130435e-05,
      "loss": 1.8468,
      "step": 20
    },
    {
      "epoch": 0.006826493295408371,
      "grad_norm": 5.304111003875732,
      "learning_rate": 8.869565217391305e-05,
      "loss": 1.8753,
      "step": 21
    },
    {
      "epoch": 0.007151564404713531,
      "grad_norm": 5.397887706756592,
      "learning_rate": 8.782608695652174e-05,
      "loss": 1.5483,
      "step": 22
    },
    {
      "epoch": 0.007476635514018692,
      "grad_norm": 4.446691989898682,
      "learning_rate": 8.695652173913044e-05,
      "loss": 2.0464,
      "step": 23
    },
    {
      "epoch": 0.007801706623323852,
      "grad_norm": 5.63054895401001,
      "learning_rate": 8.608695652173914e-05,
      "loss": 1.5185,
      "step": 24
    },
    {
      "epoch": 0.008126777732629013,
      "grad_norm": 4.2936110496521,
      "learning_rate": 8.521739130434783e-05,
      "loss": 1.9341,
      "step": 25
    },
    {
      "epoch": 0.008451848841934173,
      "grad_norm": 6.7118096351623535,
      "learning_rate": 8.434782608695653e-05,
      "loss": 1.5375,
      "step": 26
    },
    {
      "epoch": 0.008776919951239333,
      "grad_norm": 6.351090431213379,
      "learning_rate": 8.347826086956521e-05,
      "loss": 1.7483,
      "step": 27
    },
    {
      "epoch": 0.009101991060544493,
      "grad_norm": 6.118259906768799,
      "learning_rate": 8.260869565217392e-05,
      "loss": 1.8245,
      "step": 28
    },
    {
      "epoch": 0.009427062169849655,
      "grad_norm": 6.602056980133057,
      "learning_rate": 8.173913043478262e-05,
      "loss": 1.7244,
      "step": 29
    },
    {
      "epoch": 0.009752133279154815,
      "grad_norm": 9.263007164001465,
      "learning_rate": 8.086956521739131e-05,
      "loss": 1.7546,
      "step": 30
    },
    {
      "epoch": 0.010077204388459975,
      "grad_norm": 11.271392822265625,
      "learning_rate": 8e-05,
      "loss": 1.9388,
      "step": 31
    },
    {
      "epoch": 0.010402275497765135,
      "grad_norm": 11.910871505737305,
      "learning_rate": 7.91304347826087e-05,
      "loss": 1.8086,
      "step": 32
    },
    {
      "epoch": 0.010727346607070297,
      "grad_norm": 10.223878860473633,
      "learning_rate": 7.82608695652174e-05,
      "loss": 1.6812,
      "step": 33
    },
    {
      "epoch": 0.011052417716375457,
      "grad_norm": 9.884183883666992,
      "learning_rate": 7.73913043478261e-05,
      "loss": 1.6771,
      "step": 34
    },
    {
      "epoch": 0.011377488825680617,
      "grad_norm": 12.381192207336426,
      "learning_rate": 7.652173913043479e-05,
      "loss": 1.713,
      "step": 35
    },
    {
      "epoch": 0.011702559934985778,
      "grad_norm": 9.68319034576416,
      "learning_rate": 7.565217391304347e-05,
      "loss": 1.8486,
      "step": 36
    },
    {
      "epoch": 0.01202763104429094,
      "grad_norm": 10.927685737609863,
      "learning_rate": 7.478260869565218e-05,
      "loss": 1.6444,
      "step": 37
    },
    {
      "epoch": 0.0123527021535961,
      "grad_norm": 10.451171875,
      "learning_rate": 7.391304347826086e-05,
      "loss": 1.5174,
      "step": 38
    },
    {
      "epoch": 0.01267777326290126,
      "grad_norm": 13.722479820251465,
      "learning_rate": 7.304347826086957e-05,
      "loss": 1.4828,
      "step": 39
    },
    {
      "epoch": 0.01300284437220642,
      "grad_norm": 11.974713325500488,
      "learning_rate": 7.217391304347827e-05,
      "loss": 1.7899,
      "step": 40
    },
    {
      "epoch": 0.01332791548151158,
      "grad_norm": 16.966033935546875,
      "learning_rate": 7.130434782608696e-05,
      "loss": 1.5923,
      "step": 41
    },
    {
      "epoch": 0.013652986590816742,
      "grad_norm": 14.759374618530273,
      "learning_rate": 7.043478260869566e-05,
      "loss": 1.5056,
      "step": 42
    },
    {
      "epoch": 0.013978057700121902,
      "grad_norm": 22.450876235961914,
      "learning_rate": 6.956521739130436e-05,
      "loss": 0.9514,
      "step": 43
    },
    {
      "epoch": 0.014303128809427062,
      "grad_norm": 13.286316871643066,
      "learning_rate": 6.869565217391305e-05,
      "loss": 1.5462,
      "step": 44
    },
    {
      "epoch": 0.014628199918732222,
      "grad_norm": 17.08991241455078,
      "learning_rate": 6.782608695652173e-05,
      "loss": 1.6532,
      "step": 45
    },
    {
      "epoch": 0.014953271028037384,
      "grad_norm": 13.111824035644531,
      "learning_rate": 6.695652173913044e-05,
      "loss": 1.7479,
      "step": 46
    },
    {
      "epoch": 0.015278342137342544,
      "grad_norm": 11.647292137145996,
      "learning_rate": 6.608695652173912e-05,
      "loss": 1.6129,
      "step": 47
    },
    {
      "epoch": 0.015603413246647704,
      "grad_norm": 18.04575538635254,
      "learning_rate": 6.521739130434783e-05,
      "loss": 1.1876,
      "step": 48
    },
    {
      "epoch": 0.015928484355952864,
      "grad_norm": 13.766936302185059,
      "learning_rate": 6.434782608695652e-05,
      "loss": 1.3907,
      "step": 49
    },
    {
      "epoch": 0.016253555465258026,
      "grad_norm": 14.083051681518555,
      "learning_rate": 6.347826086956523e-05,
      "loss": 1.4865,
      "step": 50
    },
    {
      "epoch": 0.016578626574563184,
      "grad_norm": 5.161220073699951,
      "learning_rate": 6.260869565217392e-05,
      "loss": 1.0494,
      "step": 51
    },
    {
      "epoch": 0.016903697683868346,
      "grad_norm": 4.058265686035156,
      "learning_rate": 6.173913043478262e-05,
      "loss": 1.4853,
      "step": 52
    },
    {
      "epoch": 0.017228768793173508,
      "grad_norm": 3.2624385356903076,
      "learning_rate": 6.086956521739131e-05,
      "loss": 1.1625,
      "step": 53
    },
    {
      "epoch": 0.017553839902478666,
      "grad_norm": 3.039238452911377,
      "learning_rate": 6e-05,
      "loss": 1.2179,
      "step": 54
    },
    {
      "epoch": 0.017878911011783828,
      "grad_norm": 2.963388442993164,
      "learning_rate": 5.9130434782608704e-05,
      "loss": 1.2492,
      "step": 55
    },
    {
      "epoch": 0.018203982121088987,
      "grad_norm": 3.1252002716064453,
      "learning_rate": 5.826086956521739e-05,
      "loss": 1.444,
      "step": 56
    },
    {
      "epoch": 0.01852905323039415,
      "grad_norm": 3.8899929523468018,
      "learning_rate": 5.739130434782609e-05,
      "loss": 1.2269,
      "step": 57
    },
    {
      "epoch": 0.01885412433969931,
      "grad_norm": 3.3702940940856934,
      "learning_rate": 5.652173913043478e-05,
      "loss": 1.337,
      "step": 58
    },
    {
      "epoch": 0.01917919544900447,
      "grad_norm": 3.612416982650757,
      "learning_rate": 5.565217391304348e-05,
      "loss": 1.2134,
      "step": 59
    },
    {
      "epoch": 0.01950426655830963,
      "grad_norm": 4.880900859832764,
      "learning_rate": 5.478260869565217e-05,
      "loss": 1.1759,
      "step": 60
    },
    {
      "epoch": 0.019829337667614792,
      "grad_norm": 4.004622936248779,
      "learning_rate": 5.391304347826087e-05,
      "loss": 1.1094,
      "step": 61
    },
    {
      "epoch": 0.02015440877691995,
      "grad_norm": 3.8476173877716064,
      "learning_rate": 5.3043478260869574e-05,
      "loss": 1.192,
      "step": 62
    },
    {
      "epoch": 0.020479479886225112,
      "grad_norm": 4.368276596069336,
      "learning_rate": 5.217391304347826e-05,
      "loss": 1.1627,
      "step": 63
    },
    {
      "epoch": 0.02080455099553027,
      "grad_norm": 5.630763053894043,
      "learning_rate": 5.1304347826086966e-05,
      "loss": 0.9637,
      "step": 64
    },
    {
      "epoch": 0.021129622104835433,
      "grad_norm": 4.74536657333374,
      "learning_rate": 5.0434782608695655e-05,
      "loss": 1.1778,
      "step": 65
    },
    {
      "epoch": 0.021454693214140595,
      "grad_norm": 5.717198848724365,
      "learning_rate": 4.956521739130435e-05,
      "loss": 1.4252,
      "step": 66
    },
    {
      "epoch": 0.021779764323445753,
      "grad_norm": 4.421367645263672,
      "learning_rate": 4.8695652173913046e-05,
      "loss": 1.4348,
      "step": 67
    },
    {
      "epoch": 0.022104835432750915,
      "grad_norm": 9.500694274902344,
      "learning_rate": 4.782608695652174e-05,
      "loss": 0.6538,
      "step": 68
    },
    {
      "epoch": 0.022429906542056073,
      "grad_norm": 4.924030303955078,
      "learning_rate": 4.695652173913044e-05,
      "loss": 1.4647,
      "step": 69
    },
    {
      "epoch": 0.022754977651361235,
      "grad_norm": 5.065474510192871,
      "learning_rate": 4.608695652173913e-05,
      "loss": 1.2753,
      "step": 70
    },
    {
      "epoch": 0.023080048760666397,
      "grad_norm": 5.137774467468262,
      "learning_rate": 4.521739130434783e-05,
      "loss": 1.5689,
      "step": 71
    },
    {
      "epoch": 0.023405119869971555,
      "grad_norm": 4.898194313049316,
      "learning_rate": 4.4347826086956525e-05,
      "loss": 1.2593,
      "step": 72
    },
    {
      "epoch": 0.023730190979276717,
      "grad_norm": 5.318073749542236,
      "learning_rate": 4.347826086956522e-05,
      "loss": 1.2791,
      "step": 73
    },
    {
      "epoch": 0.02405526208858188,
      "grad_norm": 6.076398849487305,
      "learning_rate": 4.2608695652173916e-05,
      "loss": 1.4711,
      "step": 74
    },
    {
      "epoch": 0.024380333197887037,
      "grad_norm": 6.104296684265137,
      "learning_rate": 4.1739130434782605e-05,
      "loss": 1.3401,
      "step": 75
    },
    {
      "epoch": 0.0247054043071922,
      "grad_norm": 5.85785436630249,
      "learning_rate": 4.086956521739131e-05,
      "loss": 1.4259,
      "step": 76
    },
    {
      "epoch": 0.025030475416497357,
      "grad_norm": 6.041687965393066,
      "learning_rate": 4e-05,
      "loss": 1.4218,
      "step": 77
    },
    {
      "epoch": 0.02535554652580252,
      "grad_norm": 5.4369635581970215,
      "learning_rate": 3.91304347826087e-05,
      "loss": 1.3524,
      "step": 78
    },
    {
      "epoch": 0.02568061763510768,
      "grad_norm": 4.758640289306641,
      "learning_rate": 3.8260869565217395e-05,
      "loss": 1.3735,
      "step": 79
    },
    {
      "epoch": 0.02600568874441284,
      "grad_norm": 5.537770748138428,
      "learning_rate": 3.739130434782609e-05,
      "loss": 1.5163,
      "step": 80
    },
    {
      "epoch": 0.026330759853718,
      "grad_norm": 7.859096050262451,
      "learning_rate": 3.6521739130434786e-05,
      "loss": 1.2194,
      "step": 81
    },
    {
      "epoch": 0.02665583096302316,
      "grad_norm": 6.037172794342041,
      "learning_rate": 3.565217391304348e-05,
      "loss": 1.6329,
      "step": 82
    },
    {
      "epoch": 0.02698090207232832,
      "grad_norm": 5.853726387023926,
      "learning_rate": 3.478260869565218e-05,
      "loss": 1.5252,
      "step": 83
    },
    {
      "epoch": 0.027305973181633483,
      "grad_norm": 5.801245212554932,
      "learning_rate": 3.3913043478260867e-05,
      "loss": 1.4412,
      "step": 84
    },
    {
      "epoch": 0.02763104429093864,
      "grad_norm": 6.06510591506958,
      "learning_rate": 3.304347826086956e-05,
      "loss": 1.2259,
      "step": 85
    },
    {
      "epoch": 0.027956115400243804,
      "grad_norm": 7.038747787475586,
      "learning_rate": 3.217391304347826e-05,
      "loss": 1.5642,
      "step": 86
    },
    {
      "epoch": 0.028281186509548965,
      "grad_norm": 11.784317016601562,
      "learning_rate": 3.130434782608696e-05,
      "loss": 1.4661,
      "step": 87
    },
    {
      "epoch": 0.028606257618854124,
      "grad_norm": 7.379680156707764,
      "learning_rate": 3.0434782608695656e-05,
      "loss": 1.3205,
      "step": 88
    },
    {
      "epoch": 0.028931328728159286,
      "grad_norm": 9.882616996765137,
      "learning_rate": 2.9565217391304352e-05,
      "loss": 1.6223,
      "step": 89
    },
    {
      "epoch": 0.029256399837464444,
      "grad_norm": 10.066666603088379,
      "learning_rate": 2.8695652173913044e-05,
      "loss": 1.2183,
      "step": 90
    },
    {
      "epoch": 0.029581470946769606,
      "grad_norm": 10.235254287719727,
      "learning_rate": 2.782608695652174e-05,
      "loss": 1.3564,
      "step": 91
    },
    {
      "epoch": 0.029906542056074768,
      "grad_norm": 8.2982816696167,
      "learning_rate": 2.6956521739130436e-05,
      "loss": 1.2896,
      "step": 92
    },
    {
      "epoch": 0.030231613165379926,
      "grad_norm": 9.4310884475708,
      "learning_rate": 2.608695652173913e-05,
      "loss": 1.6456,
      "step": 93
    },
    {
      "epoch": 0.030556684274685088,
      "grad_norm": 10.073590278625488,
      "learning_rate": 2.5217391304347827e-05,
      "loss": 1.6745,
      "step": 94
    },
    {
      "epoch": 0.030881755383990246,
      "grad_norm": 8.501212120056152,
      "learning_rate": 2.4347826086956523e-05,
      "loss": 1.0882,
      "step": 95
    },
    {
      "epoch": 0.031206826493295408,
      "grad_norm": 8.793200492858887,
      "learning_rate": 2.347826086956522e-05,
      "loss": 0.851,
      "step": 96
    },
    {
      "epoch": 0.03153189760260057,
      "grad_norm": 10.490819931030273,
      "learning_rate": 2.2608695652173914e-05,
      "loss": 1.3494,
      "step": 97
    },
    {
      "epoch": 0.03185696871190573,
      "grad_norm": 15.46264362335205,
      "learning_rate": 2.173913043478261e-05,
      "loss": 1.1971,
      "step": 98
    },
    {
      "epoch": 0.03218203982121089,
      "grad_norm": 13.817658424377441,
      "learning_rate": 2.0869565217391303e-05,
      "loss": 1.3729,
      "step": 99
    },
    {
      "epoch": 0.03250711093051605,
      "grad_norm": 14.645284652709961,
      "learning_rate": 2e-05,
      "loss": 1.2767,
      "step": 100
    },
    {
      "epoch": 0.03283218203982121,
      "grad_norm": 3.3646512031555176,
      "learning_rate": 1.9130434782608697e-05,
      "loss": 1.4994,
      "step": 101
    },
    {
      "epoch": 0.03315725314912637,
      "grad_norm": 3.7383344173431396,
      "learning_rate": 1.8260869565217393e-05,
      "loss": 1.2209,
      "step": 102
    },
    {
      "epoch": 0.033482324258431534,
      "grad_norm": 3.2974982261657715,
      "learning_rate": 1.739130434782609e-05,
      "loss": 1.3453,
      "step": 103
    },
    {
      "epoch": 0.03380739536773669,
      "grad_norm": 3.9776597023010254,
      "learning_rate": 1.652173913043478e-05,
      "loss": 1.3519,
      "step": 104
    },
    {
      "epoch": 0.03413246647704185,
      "grad_norm": 4.372646808624268,
      "learning_rate": 1.565217391304348e-05,
      "loss": 0.9886,
      "step": 105
    },
    {
      "epoch": 0.034457537586347016,
      "grad_norm": 4.657804489135742,
      "learning_rate": 1.4782608695652176e-05,
      "loss": 1.0112,
      "step": 106
    },
    {
      "epoch": 0.034782608695652174,
      "grad_norm": 4.408566951751709,
      "learning_rate": 1.391304347826087e-05,
      "loss": 1.0175,
      "step": 107
    },
    {
      "epoch": 0.03510767980495733,
      "grad_norm": 3.579554557800293,
      "learning_rate": 1.3043478260869566e-05,
      "loss": 1.2249,
      "step": 108
    },
    {
      "epoch": 0.0354327509142625,
      "grad_norm": 4.512207984924316,
      "learning_rate": 1.2173913043478261e-05,
      "loss": 0.936,
      "step": 109
    },
    {
      "epoch": 0.035757822023567656,
      "grad_norm": 5.890448570251465,
      "learning_rate": 1.1304347826086957e-05,
      "loss": 0.7927,
      "step": 110
    },
    {
      "epoch": 0.036082893132872815,
      "grad_norm": 4.720961093902588,
      "learning_rate": 1.0434782608695651e-05,
      "loss": 0.8639,
      "step": 111
    },
    {
      "epoch": 0.03640796424217797,
      "grad_norm": 4.552245616912842,
      "learning_rate": 9.565217391304349e-06,
      "loss": 1.1343,
      "step": 112
    },
    {
      "epoch": 0.03673303535148314,
      "grad_norm": 4.197638988494873,
      "learning_rate": 8.695652173913044e-06,
      "loss": 1.1934,
      "step": 113
    },
    {
      "epoch": 0.0370581064607883,
      "grad_norm": 5.143686771392822,
      "learning_rate": 7.82608695652174e-06,
      "loss": 0.829,
      "step": 114
    },
    {
      "epoch": 0.037383177570093455,
      "grad_norm": 4.840649604797363,
      "learning_rate": 6.956521739130435e-06,
      "loss": 1.2324,
      "step": 115
    },
    {
      "epoch": 0.03770824867939862,
      "grad_norm": 5.697801113128662,
      "learning_rate": 6.086956521739131e-06,
      "loss": 1.0661,
      "step": 116
    },
    {
      "epoch": 0.03803331978870378,
      "grad_norm": 5.00540018081665,
      "learning_rate": 5.217391304347826e-06,
      "loss": 1.3314,
      "step": 117
    },
    {
      "epoch": 0.03835839089800894,
      "grad_norm": 5.853672981262207,
      "learning_rate": 4.347826086956522e-06,
      "loss": 1.2907,
      "step": 118
    },
    {
      "epoch": 0.0386834620073141,
      "grad_norm": 5.03692102432251,
      "learning_rate": 3.4782608695652175e-06,
      "loss": 1.497,
      "step": 119
    },
    {
      "epoch": 0.03900853311661926,
      "grad_norm": 4.755803108215332,
      "learning_rate": 2.608695652173913e-06,
      "loss": 1.2695,
      "step": 120
    }
  ],
  "logging_steps": 1,
  "max_steps": 120,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6106038068994048.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
